[
    {
        "cwe": "CWE-362",
        "func_name": "torvalds/fib6_add_rt2node",
        "score": 0.8025215864181519,
        "func_before": "static int fib6_add_rt2node(struct fib6_node *fn, struct fib6_info *rt,\n\t\t\t    struct nl_info *info,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\tstruct fib6_info *iter = NULL;\n\tstruct fib6_info __rcu **ins;\n\tstruct fib6_info __rcu **fallback_ins = NULL;\n\tint replace = (info->nlh &&\n\t\t       (info->nlh->nlmsg_flags & NLM_F_REPLACE));\n\tint add = (!info->nlh ||\n\t\t   (info->nlh->nlmsg_flags & NLM_F_CREATE));\n\tint found = 0;\n\tbool rt_can_ecmp = rt6_qualify_for_ecmp(rt);\n\tbool notify_sibling_rt = false;\n\tu16 nlflags = NLM_F_EXCL;\n\tint err;\n\n\tif (info->nlh && (info->nlh->nlmsg_flags & NLM_F_APPEND))\n\t\tnlflags |= NLM_F_APPEND;\n\n\tins = &fn->leaf;\n\n\tfor (iter = leaf; iter;\n\t     iter = rcu_dereference_protected(iter->fib6_next,\n\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock))) {\n\t\t/*\n\t\t *\tSearch for duplicates\n\t\t */\n\n\t\tif (iter->fib6_metric == rt->fib6_metric) {\n\t\t\t/*\n\t\t\t *\tSame priority level\n\t\t\t */\n\t\t\tif (info->nlh &&\n\t\t\t    (info->nlh->nlmsg_flags & NLM_F_EXCL))\n\t\t\t\treturn -EEXIST;\n\n\t\t\tnlflags &= ~NLM_F_EXCL;\n\t\t\tif (replace) {\n\t\t\t\tif (rt_can_ecmp == rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\tfound++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tfallback_ins = fallback_ins ?: ins;\n\t\t\t\tgoto next_iter;\n\t\t\t}\n\n\t\t\tif (rt6_duplicate_nexthop(iter, rt)) {\n\t\t\t\tif (rt->fib6_nsiblings)\n\t\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tif (!(iter->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (!(rt->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\tfib6_clean_expires_locked(iter);\n\t\t\t\telse\n\t\t\t\t\tfib6_set_expires_locked(iter,\n\t\t\t\t\t\t\t\trt->expires);\n\n\t\t\t\tif (rt->fib6_pmtu)\n\t\t\t\t\tfib6_metric_set(iter, RTAX_MTU,\n\t\t\t\t\t\t\trt->fib6_pmtu);\n\t\t\t\treturn -EEXIST;\n\t\t\t}\n\t\t\t/* If we have the same destination and the same metric,\n\t\t\t * but not the same gateway, then the route we try to\n\t\t\t * add is sibling to this route, increment our counter\n\t\t\t * of siblings, and later we will add our route to the\n\t\t\t * list.\n\t\t\t * Only static routes (which don't have flag\n\t\t\t * RTF_EXPIRES) are used for ECMPv6.\n\t\t\t *\n\t\t\t * To avoid long list, we only had siblings if the\n\t\t\t * route have a gateway.\n\t\t\t */\n\t\t\tif (rt_can_ecmp &&\n\t\t\t    rt6_qualify_for_ecmp(iter))\n\t\t\t\trt->fib6_nsiblings++;\n\t\t}\n\n\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\tbreak;\n\nnext_iter:\n\t\tins = &iter->fib6_next;\n\t}\n\n\tif (fallback_ins && !found) {\n\t\t/* No matching route with same ecmp-able-ness found, replace\n\t\t * first matching route\n\t\t */\n\t\tins = fallback_ins;\n\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\tfound++;\n\t}\n\n\t/* Reset round-robin state, if necessary */\n\tif (ins == &fn->leaf)\n\t\tfn->rr_ptr = NULL;\n\n\t/* Link this route to others same route. */\n\tif (rt->fib6_nsiblings) {\n\t\tunsigned int fib6_nsiblings;\n\t\tstruct fib6_info *sibling, *temp_sibling;\n\n\t\t/* Find the first route that have the same metric */\n\t\tsibling = leaf;\n\t\tnotify_sibling_rt = true;\n\t\twhile (sibling) {\n\t\t\tif (sibling->fib6_metric == rt->fib6_metric &&\n\t\t\t    rt6_qualify_for_ecmp(sibling)) {\n\t\t\t\tlist_add_tail(&rt->fib6_siblings,\n\t\t\t\t\t      &sibling->fib6_siblings);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsibling = rcu_dereference_protected(sibling->fib6_next,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\tnotify_sibling_rt = false;\n\t\t}\n\t\t/* For each sibling in the list, increment the counter of\n\t\t * siblings. BUG() if counters does not match, list of siblings\n\t\t * is broken!\n\t\t */\n\t\tfib6_nsiblings = 0;\n\t\tlist_for_each_entry_safe(sibling, temp_sibling,\n\t\t\t\t\t &rt->fib6_siblings, fib6_siblings) {\n\t\t\tsibling->fib6_nsiblings++;\n\t\t\tBUG_ON(sibling->fib6_nsiblings != rt->fib6_nsiblings);\n\t\t\tfib6_nsiblings++;\n\t\t}\n\t\tBUG_ON(fib6_nsiblings != rt->fib6_nsiblings);\n\t\trt6_multipath_rebalance(temp_sibling);\n\t}\n\n\t/*\n\t *\tinsert node\n\t */\n\tif (!replace) {\n\t\tif (!add)\n\t\t\tpr_warn(\"NLM_F_CREATE should be set when creating new route\\n\");\n\nadd:\n\t\tnlflags |= NLM_F_CREATE;\n\n\t\t/* The route should only be notified if it is the first\n\t\t * route in the node or if it is added as a sibling\n\t\t * route to the first route in the node.\n\t\t */\n\t\tif (!info->skip_notify_kernel &&\n\t\t    (notify_sibling_rt || ins == &fn->leaf)) {\n\t\t\tenum fib_event_type fib_event;\n\n\t\t\tif (notify_sibling_rt)\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_APPEND;\n\t\t\telse\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_REPLACE;\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tfib_event, rt,\n\t\t\t\t\t\t\textack);\n\t\t\tif (err) {\n\t\t\t\tstruct fib6_info *sibling, *next_sibling;\n\n\t\t\t\t/* If the route has siblings, then it first\n\t\t\t\t * needs to be unlinked from them.\n\t\t\t\t */\n\t\t\t\tif (!rt->fib6_nsiblings)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlist_for_each_entry_safe(sibling, next_sibling,\n\t\t\t\t\t\t\t &rt->fib6_siblings,\n\t\t\t\t\t\t\t fib6_siblings)\n\t\t\t\t\tsibling->fib6_nsiblings--;\n\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tlist_del_init(&rt->fib6_siblings);\n\t\t\t\trt6_multipath_rebalance(next_sibling);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\trcu_assign_pointer(rt->fib6_next, iter);\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, nlflags);\n\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries++;\n\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\n\t} else {\n\t\tint nsiblings;\n\n\t\tif (!found) {\n\t\t\tif (add)\n\t\t\t\tgoto add;\n\t\t\tpr_warn(\"NLM_F_REPLACE set, but no existing node found!\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tif (!info->skip_notify_kernel && ins == &fn->leaf) {\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tFIB_EVENT_ENTRY_REPLACE,\n\t\t\t\t\t\t\trt, extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trt->fib6_next = iter->fib6_next;\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, NLM_F_REPLACE);\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\t\tnsiblings = iter->fib6_nsiblings;\n\t\titer->fib6_node = NULL;\n\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\tfn->rr_ptr = NULL;\n\t\tfib6_info_release(iter);\n\n\t\tif (nsiblings) {\n\t\t\t/* Replacing an ECMP route, remove all siblings */\n\t\t\tins = &rt->fib6_next;\n\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\twhile (iter) {\n\t\t\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\t\t\tbreak;\n\t\t\t\tif (rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\t*ins = iter->fib6_next;\n\t\t\t\t\titer->fib6_node = NULL;\n\t\t\t\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\t\t\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\t\t\t\tfn->rr_ptr = NULL;\n\t\t\t\t\tfib6_info_release(iter);\n\t\t\t\t\tnsiblings--;\n\t\t\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries--;\n\t\t\t\t} else {\n\t\t\t\t\tins = &iter->fib6_next;\n\t\t\t\t}\n\t\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\t}\n\t\t\tWARN_ON(nsiblings != 0);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func_after": "static int fib6_add_rt2node(struct fib6_node *fn, struct fib6_info *rt,\n\t\t\t    struct nl_info *info,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fib6_info *leaf = rcu_dereference_protected(fn->leaf,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\tstruct fib6_info *iter = NULL;\n\tstruct fib6_info __rcu **ins;\n\tstruct fib6_info __rcu **fallback_ins = NULL;\n\tint replace = (info->nlh &&\n\t\t       (info->nlh->nlmsg_flags & NLM_F_REPLACE));\n\tint add = (!info->nlh ||\n\t\t   (info->nlh->nlmsg_flags & NLM_F_CREATE));\n\tint found = 0;\n\tbool rt_can_ecmp = rt6_qualify_for_ecmp(rt);\n\tbool notify_sibling_rt = false;\n\tu16 nlflags = NLM_F_EXCL;\n\tint err;\n\n\tif (info->nlh && (info->nlh->nlmsg_flags & NLM_F_APPEND))\n\t\tnlflags |= NLM_F_APPEND;\n\n\tins = &fn->leaf;\n\n\tfor (iter = leaf; iter;\n\t     iter = rcu_dereference_protected(iter->fib6_next,\n\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock))) {\n\t\t/*\n\t\t *\tSearch for duplicates\n\t\t */\n\n\t\tif (iter->fib6_metric == rt->fib6_metric) {\n\t\t\t/*\n\t\t\t *\tSame priority level\n\t\t\t */\n\t\t\tif (info->nlh &&\n\t\t\t    (info->nlh->nlmsg_flags & NLM_F_EXCL))\n\t\t\t\treturn -EEXIST;\n\n\t\t\tnlflags &= ~NLM_F_EXCL;\n\t\t\tif (replace) {\n\t\t\t\tif (rt_can_ecmp == rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\tfound++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tfallback_ins = fallback_ins ?: ins;\n\t\t\t\tgoto next_iter;\n\t\t\t}\n\n\t\t\tif (rt6_duplicate_nexthop(iter, rt)) {\n\t\t\t\tif (rt->fib6_nsiblings)\n\t\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tif (!(iter->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\tif (!(rt->fib6_flags & RTF_EXPIRES))\n\t\t\t\t\tfib6_clean_expires(iter);\n\t\t\t\telse\n\t\t\t\t\tfib6_set_expires(iter, rt->expires);\n\n\t\t\t\tif (rt->fib6_pmtu)\n\t\t\t\t\tfib6_metric_set(iter, RTAX_MTU,\n\t\t\t\t\t\t\trt->fib6_pmtu);\n\t\t\t\treturn -EEXIST;\n\t\t\t}\n\t\t\t/* If we have the same destination and the same metric,\n\t\t\t * but not the same gateway, then the route we try to\n\t\t\t * add is sibling to this route, increment our counter\n\t\t\t * of siblings, and later we will add our route to the\n\t\t\t * list.\n\t\t\t * Only static routes (which don't have flag\n\t\t\t * RTF_EXPIRES) are used for ECMPv6.\n\t\t\t *\n\t\t\t * To avoid long list, we only had siblings if the\n\t\t\t * route have a gateway.\n\t\t\t */\n\t\t\tif (rt_can_ecmp &&\n\t\t\t    rt6_qualify_for_ecmp(iter))\n\t\t\t\trt->fib6_nsiblings++;\n\t\t}\n\n\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\tbreak;\n\nnext_iter:\n\t\tins = &iter->fib6_next;\n\t}\n\n\tif (fallback_ins && !found) {\n\t\t/* No matching route with same ecmp-able-ness found, replace\n\t\t * first matching route\n\t\t */\n\t\tins = fallback_ins;\n\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\tfound++;\n\t}\n\n\t/* Reset round-robin state, if necessary */\n\tif (ins == &fn->leaf)\n\t\tfn->rr_ptr = NULL;\n\n\t/* Link this route to others same route. */\n\tif (rt->fib6_nsiblings) {\n\t\tunsigned int fib6_nsiblings;\n\t\tstruct fib6_info *sibling, *temp_sibling;\n\n\t\t/* Find the first route that have the same metric */\n\t\tsibling = leaf;\n\t\tnotify_sibling_rt = true;\n\t\twhile (sibling) {\n\t\t\tif (sibling->fib6_metric == rt->fib6_metric &&\n\t\t\t    rt6_qualify_for_ecmp(sibling)) {\n\t\t\t\tlist_add_tail(&rt->fib6_siblings,\n\t\t\t\t\t      &sibling->fib6_siblings);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsibling = rcu_dereference_protected(sibling->fib6_next,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\tnotify_sibling_rt = false;\n\t\t}\n\t\t/* For each sibling in the list, increment the counter of\n\t\t * siblings. BUG() if counters does not match, list of siblings\n\t\t * is broken!\n\t\t */\n\t\tfib6_nsiblings = 0;\n\t\tlist_for_each_entry_safe(sibling, temp_sibling,\n\t\t\t\t\t &rt->fib6_siblings, fib6_siblings) {\n\t\t\tsibling->fib6_nsiblings++;\n\t\t\tBUG_ON(sibling->fib6_nsiblings != rt->fib6_nsiblings);\n\t\t\tfib6_nsiblings++;\n\t\t}\n\t\tBUG_ON(fib6_nsiblings != rt->fib6_nsiblings);\n\t\trt6_multipath_rebalance(temp_sibling);\n\t}\n\n\t/*\n\t *\tinsert node\n\t */\n\tif (!replace) {\n\t\tif (!add)\n\t\t\tpr_warn(\"NLM_F_CREATE should be set when creating new route\\n\");\n\nadd:\n\t\tnlflags |= NLM_F_CREATE;\n\n\t\t/* The route should only be notified if it is the first\n\t\t * route in the node or if it is added as a sibling\n\t\t * route to the first route in the node.\n\t\t */\n\t\tif (!info->skip_notify_kernel &&\n\t\t    (notify_sibling_rt || ins == &fn->leaf)) {\n\t\t\tenum fib_event_type fib_event;\n\n\t\t\tif (notify_sibling_rt)\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_APPEND;\n\t\t\telse\n\t\t\t\tfib_event = FIB_EVENT_ENTRY_REPLACE;\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tfib_event, rt,\n\t\t\t\t\t\t\textack);\n\t\t\tif (err) {\n\t\t\t\tstruct fib6_info *sibling, *next_sibling;\n\n\t\t\t\t/* If the route has siblings, then it first\n\t\t\t\t * needs to be unlinked from them.\n\t\t\t\t */\n\t\t\t\tif (!rt->fib6_nsiblings)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlist_for_each_entry_safe(sibling, next_sibling,\n\t\t\t\t\t\t\t &rt->fib6_siblings,\n\t\t\t\t\t\t\t fib6_siblings)\n\t\t\t\t\tsibling->fib6_nsiblings--;\n\t\t\t\trt->fib6_nsiblings = 0;\n\t\t\t\tlist_del_init(&rt->fib6_siblings);\n\t\t\t\trt6_multipath_rebalance(next_sibling);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\trcu_assign_pointer(rt->fib6_next, iter);\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, nlflags);\n\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries++;\n\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\n\t} else {\n\t\tint nsiblings;\n\n\t\tif (!found) {\n\t\t\tif (add)\n\t\t\t\tgoto add;\n\t\t\tpr_warn(\"NLM_F_REPLACE set, but no existing node found!\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tif (!info->skip_notify_kernel && ins == &fn->leaf) {\n\t\t\terr = call_fib6_entry_notifiers(info->nl_net,\n\t\t\t\t\t\t\tFIB_EVENT_ENTRY_REPLACE,\n\t\t\t\t\t\t\trt, extack);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tfib6_info_hold(rt);\n\t\trcu_assign_pointer(rt->fib6_node, fn);\n\t\trt->fib6_next = iter->fib6_next;\n\t\trcu_assign_pointer(*ins, rt);\n\t\tif (!info->skip_notify)\n\t\t\tinet6_rt_notify(RTM_NEWROUTE, rt, info, NLM_F_REPLACE);\n\t\tif (!(fn->fn_flags & RTN_RTINFO)) {\n\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_route_nodes++;\n\t\t\tfn->fn_flags |= RTN_RTINFO;\n\t\t}\n\t\tnsiblings = iter->fib6_nsiblings;\n\t\titer->fib6_node = NULL;\n\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\tfn->rr_ptr = NULL;\n\t\tfib6_info_release(iter);\n\n\t\tif (nsiblings) {\n\t\t\t/* Replacing an ECMP route, remove all siblings */\n\t\t\tins = &rt->fib6_next;\n\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t    lockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\twhile (iter) {\n\t\t\t\tif (iter->fib6_metric > rt->fib6_metric)\n\t\t\t\t\tbreak;\n\t\t\t\tif (rt6_qualify_for_ecmp(iter)) {\n\t\t\t\t\t*ins = iter->fib6_next;\n\t\t\t\t\titer->fib6_node = NULL;\n\t\t\t\t\tfib6_purge_rt(iter, fn, info->nl_net);\n\t\t\t\t\tif (rcu_access_pointer(fn->rr_ptr) == iter)\n\t\t\t\t\t\tfn->rr_ptr = NULL;\n\t\t\t\t\tfib6_info_release(iter);\n\t\t\t\t\tnsiblings--;\n\t\t\t\t\tinfo->nl_net->ipv6.rt6_stats->fib_rt_entries--;\n\t\t\t\t} else {\n\t\t\t\t\tins = &iter->fib6_next;\n\t\t\t\t}\n\t\t\t\titer = rcu_dereference_protected(*ins,\n\t\t\t\t\tlockdep_is_held(&rt->fib6_table->tb6_lock));\n\t\t\t}\n\t\t\tWARN_ON(nsiblings != 0);\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "description": "A race condition exists in the Linux Kernel where an unauthenticated attacker from an adjacent network could exploit it by sending an ICMPv6 router advertisement packet, potentially leading to arbitrary code execution.",
        "commit": "This vulnerability involves a revert of a specific commit due to race conditions related to the management of the `expires` field in a `fib6_info` structure. The original commit introduced issues where the garbage collection (gc) process could start before the entry was added to the gc list and the timer value was set, potentially leading to a use-after-free (UAF) condition. The recommended approach is to revert the commit and address the problem in a future release."
    },
    {
        "cwe": "CWE-667",
        "func_name": "torvalds/pipe_to_file",
        "score": 0.7980558276176453,
        "func_before": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tpage_cache_release(page);\n\tunlock_page(page);\nout_ret:\n\treturn ret;\n}",
        "func_after": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out_release;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tunlock_page(page);\nout_release:\n\tpage_cache_release(page);\nout_ret:\n\treturn ret;\n}",
        "description": "In the splice subsystem of the Linux kernel prior to version 2.6.22.2, there is an issue where the `add_to_page_cache_lru` function's failure is not adequately managed. As a result, the system attempts to unlock a page that was not previously locked, leading to a denial of service condition characterized by a kernel BUG and potential system crash. This vulnerability can be exploited by local users using tools such as fio for I/O operations.",
        "commit": "If the `add_to_page_cache_lru()` function fails, the page remains unlocked. However, the subsequent code in `splice` jumps to an error path that attempts to release and unlock the page, leading to a `BUG()` in `unlock_page()`. This issue can be resolved by introducing an additional label that solely handles the page release without attempting to unlock it. This bug was triggered on EL5 by Gurudas Pai using the fio tool."
    },
    {
        "cwe": "CWE-662",
        "func_name": "torvalds/do_fontx_ioctl",
        "score": 0.771639883518219,
        "func_before": "static inline int do_fontx_ioctl(int cmd,\n\t\tstruct consolefontdesc __user *user_cfd,\n\t\tstruct console_font_op *op)\n{\n\tstruct consolefontdesc cfdarg;\n\tint i;\n\n\tif (copy_from_user(&cfdarg, user_cfd, sizeof(struct consolefontdesc)))\n\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase PIO_FONTX:\n\t\top->op = KD_FONT_OP_SET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\treturn con_font_op(vc_cons[fg_console].d, op);\n\tcase GIO_FONTX: {\n\t\top->op = KD_FONT_OP_GET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\ti = con_font_op(vc_cons[fg_console].d, op);\n\t\tif (i)\n\t\t\treturn i;\n\t\tcfdarg.charheight = op->height;\n\t\tcfdarg.charcount = op->charcount;\n\t\tif (copy_to_user(user_cfd, &cfdarg, sizeof(struct consolefontdesc)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}",
        "func_after": "static inline int do_fontx_ioctl(struct vc_data *vc, int cmd,\n\t\tstruct consolefontdesc __user *user_cfd,\n\t\tstruct console_font_op *op)\n{\n\tstruct consolefontdesc cfdarg;\n\tint i;\n\n\tif (copy_from_user(&cfdarg, user_cfd, sizeof(struct consolefontdesc)))\n\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase PIO_FONTX:\n\t\top->op = KD_FONT_OP_SET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\treturn con_font_op(vc, op);\n\n\tcase GIO_FONTX:\n\t\top->op = KD_FONT_OP_GET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\ti = con_font_op(vc, op);\n\t\tif (i)\n\t\t\treturn i;\n\t\tcfdarg.charheight = op->height;\n\t\tcfdarg.charcount = op->charcount;\n\t\tif (copy_to_user(user_cfd, &cfdarg, sizeof(struct consolefontdesc)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}",
        "description": "A flaw was identified in the Linux Kernel where access to a global variable used for managing the foreground console is not adequately synchronized, resulting in a use-after-free error within the function responsible for font operations.",
        "commit": "Some font-related terminal I/O control operations previously utilized the current foreground virtual console (VC) for their execution. This practice has been discontinued to address a data race condition involving the `fg_console` variable. Notably, both Michael Ellerman and Jiri Slaby have observed that these I/O control operations are deprecated and should have been removed earlier. They suggest that most systems now use the `KDFONTOP` ioctl instead. Additionally, Michael notes that BusyBox's `loadfont` program transitioned to using `KDFONTOP` precisely due to this bug, which was identified approximately 12 years ago."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.7973527908325195,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    },
    {
        "cwe": "CWE-326",
        "func_name": "torvalds/__ipv6_select_ident",
        "score": 0.7715237140655518,
        "func_before": "static u32 __ipv6_select_ident(struct net *net, u32 hashrnd,\n\t\t\t       const struct in6_addr *dst,\n\t\t\t       const struct in6_addr *src)\n{\n\tu32 hash, id;\n\n\thash = __ipv6_addr_jhash(dst, hashrnd);\n\thash = __ipv6_addr_jhash(src, hash);\n\thash ^= net_hash_mix(net);\n\n\t/* Treat id of 0 as unset and if we get 0 back from ip_idents_reserve,\n\t * set the hight order instead thus minimizing possible future\n\t * collisions.\n\t */\n\tid = ip_idents_reserve(hash, 1);\n\tif (unlikely(!id))\n\t\tid = 1 << 31;\n\n\treturn id;\n}",
        "func_after": "static u32 __ipv6_select_ident(struct net *net,\n\t\t\t       const struct in6_addr *dst,\n\t\t\t       const struct in6_addr *src)\n{\n\tconst struct {\n\t\tstruct in6_addr dst;\n\t\tstruct in6_addr src;\n\t} __aligned(SIPHASH_ALIGNMENT) combined = {\n\t\t.dst = *dst,\n\t\t.src = *src,\n\t};\n\tu32 hash, id;\n\n\t/* Note the following code is not safe, but this is okay. */\n\tif (unlikely(siphash_key_is_zero(&net->ipv4.ip_id_key)))\n\t\tget_random_bytes(&net->ipv4.ip_id_key,\n\t\t\t\t sizeof(net->ipv4.ip_id_key));\n\n\thash = siphash(&combined, sizeof(combined), &net->ipv4.ip_id_key);\n\n\t/* Treat id of 0 as unset and if we get 0 back from ip_idents_reserve,\n\t * set the hight order instead thus minimizing possible future\n\t * collisions.\n\t */\n\tid = ip_idents_reserve(hash, 1);\n\tif (unlikely(!id))\n\t\tid = 1 << 31;\n\n\treturn id;\n}",
        "description": "In versions of the Linux kernel prior to 5.1.7, an attacker can track devices by analyzing the IP ID values generated by the kernel for connectionless protocols such as UDP and ICMP. By sending traffic to multiple destination IP addresses, it is possible to induce hash collisions within the counter array, potentially revealing the hashing key through enumeration. This vulnerability can be exploited by hosting a specially crafted web page that leverages technologies like WebRTC or gQUIC to direct UDP traffic to IP addresses controlled by the attacker.",
        "commit": "The IP ID generation mechanism in the networking stack is deemed insufficiently secure due to potential vulnerabilities that could be exploited by attackers. Despite recent improvements such as the introduction of a 64-bit key and the Jenkins hash function, these measures are still considered risky. Therefore, it is recommended to transition to using siphash with its 128-bit keys to enhance the security of IP ID generation."
    }
]