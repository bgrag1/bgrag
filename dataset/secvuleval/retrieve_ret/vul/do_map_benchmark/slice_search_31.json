[
    {
        "cwe": "CWE-367",
        "func_name": "torvalds/enter_svm_guest_mode",
        "score": 0.7683156132698059,
        "func_before": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "func_after": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "description": "A use-after-free vulnerability exists in the Linux kernel prior to version 5.11.12 within the AMD KVM SVM nested virtualization module. This flaw allows an AMD KVM guest to potentially bypass access controls on host OS Model-Specific Registers (MSRs) when multiple nested guests are present. The issue arises due to a Time-of-Check to Time-of-Use (TOCTOU) race condition involving a VMCB12 double fetch in the `nested_svm_vmrun` function.",
        "commit": "To avoid race conditions between checking and using nested VMCB controls, ensuring that the VMRUN intercept is consistently reflected to the nested hypervisor rather than being processed by the host is crucial. Without this patch, there is a risk that `svm->nested.hsave` could point to the MSR permission bitmap for nested guests, leading to potential vulnerabilities such as CVE-2021-29657."
    },
    {
        "cwe": "CWE-190",
        "func_name": "torvalds/ring_buffer_resize",
        "score": 0.8006622791290283,
        "func_before": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tsize = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tsize *= BUF_PAGE_SIZE;\n\n\t/* we need a minimum of two pages */\n\tif (size < BUF_PAGE_SIZE * 2)\n\t\tsize = BUF_PAGE_SIZE * 2;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}",
        "func_after": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/* we need a minimum of two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tsize = nr_pages * BUF_PAGE_SIZE;\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}",
        "description": "The `ring_buffer_resize` function in the profiling subsystem of the Linux kernel, prior to version 4.6.1, mishandles specific integer calculations. This flaw enables local users to escalate their privileges by writing to the `/sys/kernel/debug/tracing/buffer_size_kb` file.",
        "commit": "A vulnerability was identified in the ring buffer resizing functionality within the Linux kernel. Specifically, if the size passed to the `ring_buffer_resize()` function exceeds `MAX_LONG - BUF_PAGE_SIZE`, the `DIV_ROUND_UP()` operation will incorrectly return zero. This occurs because the intermediate calculation during the resizing process leads to an overflow, resulting in an incorrect number of buffer pages being allocated. Consequently, the logic fails, potentially causing a kernel crash. To address this issue, the redundant use of `DIV_ROUND_UP()` should be cleaned up, ensuring proper handling of large sizes and preventing overflow."
    },
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7456039190292358,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-667",
        "func_name": "torvalds/pipe_to_file",
        "score": 0.7969195246696472,
        "func_before": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tpage_cache_release(page);\n\tunlock_page(page);\nout_ret:\n\treturn ret;\n}",
        "func_after": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out_release;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tunlock_page(page);\nout_release:\n\tpage_cache_release(page);\nout_ret:\n\treturn ret;\n}",
        "description": "In the splice subsystem of the Linux kernel prior to version 2.6.22.2, there is an issue where the `add_to_page_cache_lru` function's failure is not adequately managed. As a result, the system attempts to unlock a page that was not previously locked, leading to a denial of service condition characterized by a kernel BUG and potential system crash. This vulnerability can be exploited by local users using tools such as fio for I/O operations.",
        "commit": "If the `add_to_page_cache_lru()` function fails, the page remains unlocked. However, the subsequent code in `splice` jumps to an error path that attempts to release and unlock the page, leading to a `BUG()` in `unlock_page()`. This issue can be resolved by introducing an additional label that solely handles the page release without attempting to unlock it. This bug was triggered on EL5 by Gurudas Pai using the fio tool."
    },
    {
        "cwe": "CWE-662",
        "func_name": "torvalds/do_fontx_ioctl",
        "score": 0.7717898488044739,
        "func_before": "static inline int do_fontx_ioctl(int cmd,\n\t\tstruct consolefontdesc __user *user_cfd,\n\t\tstruct console_font_op *op)\n{\n\tstruct consolefontdesc cfdarg;\n\tint i;\n\n\tif (copy_from_user(&cfdarg, user_cfd, sizeof(struct consolefontdesc)))\n\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase PIO_FONTX:\n\t\top->op = KD_FONT_OP_SET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\treturn con_font_op(vc_cons[fg_console].d, op);\n\tcase GIO_FONTX: {\n\t\top->op = KD_FONT_OP_GET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\ti = con_font_op(vc_cons[fg_console].d, op);\n\t\tif (i)\n\t\t\treturn i;\n\t\tcfdarg.charheight = op->height;\n\t\tcfdarg.charcount = op->charcount;\n\t\tif (copy_to_user(user_cfd, &cfdarg, sizeof(struct consolefontdesc)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}",
        "func_after": "static inline int do_fontx_ioctl(struct vc_data *vc, int cmd,\n\t\tstruct consolefontdesc __user *user_cfd,\n\t\tstruct console_font_op *op)\n{\n\tstruct consolefontdesc cfdarg;\n\tint i;\n\n\tif (copy_from_user(&cfdarg, user_cfd, sizeof(struct consolefontdesc)))\n\t\treturn -EFAULT;\n\n\tswitch (cmd) {\n\tcase PIO_FONTX:\n\t\top->op = KD_FONT_OP_SET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\treturn con_font_op(vc, op);\n\n\tcase GIO_FONTX:\n\t\top->op = KD_FONT_OP_GET;\n\t\top->flags = KD_FONT_FLAG_OLD;\n\t\top->width = 8;\n\t\top->height = cfdarg.charheight;\n\t\top->charcount = cfdarg.charcount;\n\t\top->data = cfdarg.chardata;\n\t\ti = con_font_op(vc, op);\n\t\tif (i)\n\t\t\treturn i;\n\t\tcfdarg.charheight = op->height;\n\t\tcfdarg.charcount = op->charcount;\n\t\tif (copy_to_user(user_cfd, &cfdarg, sizeof(struct consolefontdesc)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}",
        "description": "A flaw was identified in the Linux Kernel where access to a global variable used for managing the foreground console is not adequately synchronized, resulting in a use-after-free error within the function responsible for font operations.",
        "commit": "Some font-related terminal I/O control operations previously utilized the current foreground virtual console (VC) for their execution. This practice has been discontinued to address a data race condition involving the `fg_console` variable. Notably, both Michael Ellerman and Jiri Slaby have observed that these I/O control operations are deprecated and should have been removed earlier. They suggest that most systems now use the `KDFONTOP` ioctl instead. Additionally, Michael notes that BusyBox's `loadfont` program transitioned to using `KDFONTOP` precisely due to this bug, which was identified approximately 12 years ago."
    }
]