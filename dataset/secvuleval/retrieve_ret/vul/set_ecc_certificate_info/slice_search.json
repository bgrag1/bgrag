[
    {
        "cwe": "CWE-672",
        "func_name": "torvalds/get_gate_page",
        "score": 0.7803981304168701,
        "func_before": "static int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tif (pgd_none(*pgd))\n\t\treturn -EFAULT;\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn -EFAULT;\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn -EFAULT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tif (unlikely(!try_get_page(*page))) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}",
        "func_after": "static int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tif (pgd_none(*pgd))\n\t\treturn -EFAULT;\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn -EFAULT;\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn -EFAULT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tif (unlikely(!try_grab_page(*page, gup_flags))) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}",
        "description": "\"In the Linux kernel versions 5.7.x and 5.8.x prior to 5.8.7, a privilege escalation vulnerability exists due to incorrect reference counting of the struct page backing the vsyscall page, resulting in a refcount underflow. This issue can be exploited by any 64-bit process that has access to ptrace() or process_vm_readv() functions.\"",
        "commit": "It was discovered that gate pages were overlooked during the conversion from `get` to `pin_user_pages()`, leading to reference count imbalances. This issue can be reliably reproduced by running the x86 selftests with `vsyscall=emulate` enabled (the default setting). The problem arises because `pin_user_pages()` uses a \"bias\" value, manipulating the reference count by 1024 instead of 1, which is used by `get_user_pages()`. Gate pages, such as the vsyscall page, are typically part of the kernel image but are mapped to userspace, allowing access through interfaces using `get/pin_user_pages()`. The reference count of these kernel pages is adjusted similarly to normal user pages on the get/pin side to ensure consistency on the put/unpin side. However, `get_gate_page()` uses `try_get_page()`, which increments the reference count by 1, not 1024, even when called in the `pin_user_pages()` path. This results in a reference count discrepancy of 1023 when `unpin_user_pages()` is eventually called. The fix involves using `try_grab_page()` instead of `try_get_page()` and passing the appropriate flags to respect `FOLL_PIN`. This bug has been present since the introduction of `FOLL_PIN` support in commit 3faa52c03f44, which first appeared in the 5.7 release."
    },
    {
        "cwe": "CWE-189",
        "func_name": "torvalds/i915_gem_execbuffer2",
        "score": 0.7783063650131226,
        "func_before": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "func_after": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1 ||\n\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "description": "An integer overflow vulnerability exists within the `i915_gem_execbuffer2` function of the DRM subsystem in the Linux kernel prior to version 3.3.5 on 32-bit platforms. This flaw allows local users to trigger a denial of service through an out-of-bounds write or potentially cause other unspecified impacts via a specially crafted ioctl call.",
        "commit": "A vulnerability was identified in the i915 driver of the DRM subsystem within the Linux kernel, where a large `args->buffer_count` value provided by userspace through an ioctl call could cause an integer overflow on 32-bit systems. This overflow affects the calculation of the allocation size for temporary execution buffers, potentially leading to out-of-bounds memory access. This issue arose due to changes introduced in commit 8408c282, which attempted to optimize buffer allocation by first trying a normal large `kmalloc`."
    },
    {
        "cwe": "CWE-330",
        "func_name": "torvalds/sfb_enqueue",
        "score": 0.7884313464164734,
        "func_before": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = jhash_1word(salt, q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(skb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "func_after": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = siphash_1u32(salt, &q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, &q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    &q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(skb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "description": "The flow_dissector feature in the Linux kernel from version 4.3 up to but not including 5.3.10 suffers from a device tracking vulnerability, identified as CID-55667441c84f. This vulnerability arises due to the reliance on a 32-bit hashrnd value as a secret for the auto flowlabel of a UDP IPv6 packet. Additionally, the use of jhash instead of siphash exacerbates the issue. Since the hashrnd value remains constant from the time of system boot, it can be deduced by an attacker, thereby compromising the intended security measures. This problem is present in the net/core/flow_dissector.c file and related components.",
        "commit": "The vulnerability involves the use of a 32-bit secret in generating auto flowlabels for UDP IPv6 packets, which can be inferred by attackers to identify devices or users. The secret is initialized only at boot time and is used in conjunction with the jhash function to create flow labels that are predictable. This predictability poses a significant privacy risk. The proposed solution is to switch from using jhash to a cryptographically strong pseudo-random function like siphash, similar to changes made in the IP ID generator. This switch aims to enhance security by making the flow label generation process less predictable and thereby reducing the risk of device/user identification."
    },
    {
        "cwe": "CWE-347",
        "func_name": "rpm-software-management/pgpPrtParams",
        "score": 0.7723578214645386,
        "func_before": "int pgpPrtParams(const uint8_t * pkts, size_t pktlen, unsigned int pkttype,\n\t\t pgpDigParams * ret)\n{\n    const uint8_t *p = pkts;\n    const uint8_t *pend = pkts + pktlen;\n    pgpDigParams digp = NULL;\n    struct pgpPkt pkt;\n    int rc = -1; /* assume failure */\n\n    while (p < pend) {\n\tif (decodePkt(p, (pend - p), &pkt))\n\t    break;\n\n\tif (digp == NULL) {\n\t    if (pkttype && pkt.tag != pkttype) {\n\t\tbreak;\n\t    } else {\n\t\tdigp = pgpDigParamsNew(pkt.tag);\n\t    }\n\t}\n\n\tif (pgpPrtPkt(&pkt, digp))\n\t    break;\n\n\tp += (pkt.body - pkt.head) + pkt.blen;\n\tif (pkttype == PGPTAG_SIGNATURE)\n\t    break;\n    }\n\n    rc = (digp && (p == pend)) ? 0 : -1;\n\n    if (ret && rc == 0) {\n\t*ret = digp;\n    } else {\n\tpgpDigParamsFree(digp);\n    }\n    return rc;\n}",
        "func_after": "int pgpPrtParams(const uint8_t * pkts, size_t pktlen, unsigned int pkttype,\n\t\t pgpDigParams * ret)\n{\n    const uint8_t *p = pkts;\n    const uint8_t *pend = pkts + pktlen;\n    pgpDigParams digp = NULL;\n    pgpDigParams selfsig = NULL;\n    int i = 0;\n    int alloced = 16; /* plenty for normal cases */\n    struct pgpPkt *all = xmalloc(alloced * sizeof(*all));\n    int rc = -1; /* assume failure */\n    int expect = 0;\n    int prevtag = 0;\n\n    while (p < pend) {\n\tstruct pgpPkt *pkt = &all[i];\n\tif (decodePkt(p, (pend - p), pkt))\n\t    break;\n\n\tif (digp == NULL) {\n\t    if (pkttype && pkt->tag != pkttype) {\n\t\tbreak;\n\t    } else {\n\t\tdigp = pgpDigParamsNew(pkt->tag);\n\t    }\n\t}\n\n\tif (expect) {\n\t    if (pkt->tag != expect)\n\t\tbreak;\n\t    selfsig = pgpDigParamsNew(pkt->tag);\n\t}\n\n\tif (pgpPrtPkt(pkt, selfsig ? selfsig : digp))\n\t    break;\n\n\tif (selfsig) {\n\t    /* subkeys must be followed by binding signature */\n\t    if (prevtag == PGPTAG_PUBLIC_SUBKEY) {\n\t\tif (selfsig->sigtype != PGPSIGTYPE_SUBKEY_BINDING)\n\t\t    break;\n\t    }\n\n\t    int xx = pgpVerifySelf(digp, selfsig, all, i);\n\n\t    selfsig = pgpDigParamsFree(selfsig);\n\t    if (xx)\n\t\tbreak;\n\t    expect = 0;\n\t}\n\n\tif (pkt->tag == PGPTAG_PUBLIC_SUBKEY)\n\t    expect = PGPTAG_SIGNATURE;\n\tprevtag = pkt->tag;\n\n\ti++;\n\tp += (pkt->body - pkt->head) + pkt->blen;\n\tif (pkttype == PGPTAG_SIGNATURE)\n\t    break;\n\n\tif (alloced <= i) {\n\t    alloced *= 2;\n\t    all = xrealloc(all, alloced * sizeof(*all));\n\t}\n    }\n\n    rc = (digp && (p == pend) && expect == 0) ? 0 : -1;\n\n    free(all);\n    if (ret && rc == 0) {\n\t*ret = digp;\n    } else {\n\tpgpDigParamsFree(digp);\n    }\n    return rc;\n}",
        "description": "There is a flaw in RPM's signature functionality where it does not verify the binding signature of subkeys before importing them. This can allow an attacker to add or trick another party into adding a malicious subkey to a legitimate public key, potentially causing RPM to trust a malicious signature. The primary impact of this flaw is on data integrity. Exploitation requires compromising an RPM repository or convincing an administrator to install an untrusted RPM or public key. It is recommended to only use RPMs and public keys from trusted sources.",
        "commit": "To enhance the applicability of the given vulnerability knowledge across different scenarios, we can abstract and generalize it as follows:\n\n**Abstracted and Generalized Description:**\n\n\"A vulnerability was identified in the parsing mechanism of PGP public keys, where the implementation lacked enforcement of subkey binding signatures as mandated by the OpenPGP RFC. To address this, a workaround was introduced to ensure that all subkeys are validated by a binding signature from the primary key. This solution involves storing raw packets internally during decoding to facilitate access to previous elements for validating ordering and data integrity. Additionally, test cases were added to handle manipulated keys that previously could be imported successfully. This fix aims to improve the robustness of the parser without altering the API to maximize compatibility with older versions.\""
    },
    {
        "cwe": "CWE-254",
        "func_name": "android/impeg2d_dec_d_slice",
        "score": 0.7776155471801758,
        "func_before": "IMPEG2D_ERROR_CODES_T impeg2d_dec_d_slice(dec_state_t *ps_dec)\n{\n    UWORD32 i;\n    yuv_buf_t *ps_cur_frm_buf  = &ps_dec->s_cur_frm_buf;\n\n    stream_t   *ps_stream       = &ps_dec->s_bit_stream;\n    UWORD8   *pu1_vld_buf;\n\n    WORD16 i2_dc_diff;\n    UWORD32 u4_frame_width = ps_dec->u2_frame_width;\n    UWORD32 u4_frm_offset = 0;\n    if(ps_dec->u2_picture_structure != FRAME_PICTURE)\n    {\n        u4_frame_width <<= 1;\n        if(ps_dec->u2_picture_structure == BOTTOM_FIELD)\n        {\n            u4_frm_offset = ps_dec->u2_frame_width;\n        }\n    }\n\n    do\n    {\n\n        UWORD32 u4_x_offset, u4_y_offset;\n        UWORD32 u4_blk_pos;\n        WORD16 i2_dc_val;\n\n        UWORD32 u4_dst_x_offset     = u4_frm_offset + (ps_dec->u2_mb_x << 4);\n        UWORD32 u4_dst_y_offset     = (ps_dec->u2_mb_y << 4) * u4_frame_width;\n        UWORD8 *pu1_vld_buf8        = ps_cur_frm_buf->pu1_y + u4_dst_x_offset + u4_dst_y_offset;\n        UWORD32 u4_dst_wd           = u4_frame_width;\n        /*------------------------------------------------------------------*/\n        /* Discard the Macroblock stuffing in case of MPEG-1 stream         */\n        /*------------------------------------------------------------------*/\n        while(impeg2d_bit_stream_nxt(ps_stream,MB_STUFFING_CODE_LEN) == MB_STUFFING_CODE)\n            impeg2d_bit_stream_flush(ps_stream,MB_STUFFING_CODE_LEN);\n\n        /*------------------------------------------------------------------*/\n        /* Flush 2 bits from bitstream [MB_Type and MacroBlockAddrIncrement]*/\n        /*------------------------------------------------------------------*/\n        impeg2d_bit_stream_flush(ps_stream,1);\n\n        if(impeg2d_bit_stream_get(ps_stream, 1) != 0x01)\n        {\n            /* Ignore and continue decoding. */\n        }\n\n        /* Process LUMA blocks of the MB */\n        for(i = 0; i < NUM_LUMA_BLKS; ++i)\n        {\n\n            u4_x_offset    = gai2_impeg2_blk_x_off[i];\n            u4_y_offset    = gai2_impeg2_blk_y_off_frm[i] ;\n            u4_blk_pos     = (u4_y_offset * u4_dst_wd) + u4_x_offset;\n            pu1_vld_buf     = pu1_vld_buf8 + u4_blk_pos;\n\n            i2_dc_diff = impeg2d_get_luma_dc_diff(ps_stream);\n            i2_dc_val = ps_dec->u2_def_dc_pred[Y_LUMA] + i2_dc_diff;\n            ps_dec->u2_def_dc_pred[Y_LUMA] = i2_dc_val;\n            i2_dc_val = CLIP_U8(i2_dc_val);\n\n            ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n        }\n\n\n\n        /* Process U block of the MB */\n\n        u4_dst_x_offset                >>= 1;\n        u4_dst_y_offset                >>= 2;\n        u4_dst_wd                      >>= 1;\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_u + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[U_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[U_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n\n        /* Process V block of the MB */\n\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_v + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[V_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[V_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n        /* Common MB processing Steps */\n\n\n        ps_dec->u2_num_mbs_left--;\n        ps_dec->u2_mb_x++;\n\n        if(ps_dec->s_bit_stream.u4_offset > ps_dec->s_bit_stream.u4_max_offset)\n        {\n            return IMPEG2D_BITSTREAM_BUFF_EXCEEDED_ERR;\n        }\n        else if (ps_dec->u2_mb_x == ps_dec->u2_num_horiz_mb)\n        {\n            ps_dec->u2_mb_x = 0;\n            ps_dec->u2_mb_y++;\n\n        }\n\n        /* Flush end of macro block */\n        impeg2d_bit_stream_flush(ps_stream,1);\n    }\n    while(ps_dec->u2_num_mbs_left != 0 && impeg2d_bit_stream_nxt(&ps_dec->s_bit_stream,23) != 0x0);\n    return (IMPEG2D_ERROR_CODES_T)IVD_ERROR_NONE;\n}",
        "func_after": "IMPEG2D_ERROR_CODES_T impeg2d_dec_d_slice(dec_state_t *ps_dec)\n{\n    UWORD32 i;\n    yuv_buf_t *ps_cur_frm_buf  = &ps_dec->s_cur_frm_buf;\n\n    stream_t   *ps_stream       = &ps_dec->s_bit_stream;\n    UWORD8   *pu1_vld_buf;\n\n    WORD16 i2_dc_diff;\n    UWORD32 u4_frame_width = ps_dec->u2_frame_width;\n    UWORD32 u4_frm_offset = 0;\n    if(ps_dec->u2_picture_structure != FRAME_PICTURE)\n    {\n        u4_frame_width <<= 1;\n        if(ps_dec->u2_picture_structure == BOTTOM_FIELD)\n        {\n            u4_frm_offset = ps_dec->u2_frame_width;\n        }\n    }\n\n    do\n    {\n\n        UWORD32 u4_x_offset, u4_y_offset;\n        UWORD32 u4_blk_pos;\n        WORD16 i2_dc_val;\n\n        UWORD32 u4_dst_x_offset     = u4_frm_offset + (ps_dec->u2_mb_x << 4);\n        UWORD32 u4_dst_y_offset     = (ps_dec->u2_mb_y << 4) * u4_frame_width;\n        UWORD8 *pu1_vld_buf8        = ps_cur_frm_buf->pu1_y + u4_dst_x_offset + u4_dst_y_offset;\n        UWORD32 u4_dst_wd           = u4_frame_width;\n        /*------------------------------------------------------------------*/\n        /* Discard the Macroblock stuffing in case of MPEG-1 stream         */\n        /*------------------------------------------------------------------*/\n        while(impeg2d_bit_stream_nxt(ps_stream,MB_STUFFING_CODE_LEN) == MB_STUFFING_CODE &&\n                ps_stream->u4_offset < ps_stream->u4_max_offset)\n            impeg2d_bit_stream_flush(ps_stream,MB_STUFFING_CODE_LEN);\n\n        /*------------------------------------------------------------------*/\n        /* Flush 2 bits from bitstream [MB_Type and MacroBlockAddrIncrement]*/\n        /*------------------------------------------------------------------*/\n        impeg2d_bit_stream_flush(ps_stream,1);\n\n        if(impeg2d_bit_stream_get(ps_stream, 1) != 0x01)\n        {\n            /* Ignore and continue decoding. */\n        }\n\n        /* Process LUMA blocks of the MB */\n        for(i = 0; i < NUM_LUMA_BLKS; ++i)\n        {\n\n            u4_x_offset    = gai2_impeg2_blk_x_off[i];\n            u4_y_offset    = gai2_impeg2_blk_y_off_frm[i] ;\n            u4_blk_pos     = (u4_y_offset * u4_dst_wd) + u4_x_offset;\n            pu1_vld_buf     = pu1_vld_buf8 + u4_blk_pos;\n\n            i2_dc_diff = impeg2d_get_luma_dc_diff(ps_stream);\n            i2_dc_val = ps_dec->u2_def_dc_pred[Y_LUMA] + i2_dc_diff;\n            ps_dec->u2_def_dc_pred[Y_LUMA] = i2_dc_val;\n            i2_dc_val = CLIP_U8(i2_dc_val);\n\n            ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n        }\n\n\n\n        /* Process U block of the MB */\n\n        u4_dst_x_offset                >>= 1;\n        u4_dst_y_offset                >>= 2;\n        u4_dst_wd                      >>= 1;\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_u + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[U_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[U_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n\n        /* Process V block of the MB */\n\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_v + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[V_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[V_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n        /* Common MB processing Steps */\n\n\n        ps_dec->u2_num_mbs_left--;\n        ps_dec->u2_mb_x++;\n\n        if(ps_dec->s_bit_stream.u4_offset > ps_dec->s_bit_stream.u4_max_offset)\n        {\n            return IMPEG2D_BITSTREAM_BUFF_EXCEEDED_ERR;\n        }\n        else if (ps_dec->u2_mb_x == ps_dec->u2_num_horiz_mb)\n        {\n            ps_dec->u2_mb_x = 0;\n            ps_dec->u2_mb_y++;\n\n        }\n\n        /* Flush end of macro block */\n        impeg2d_bit_stream_flush(ps_stream,1);\n    }\n    while(ps_dec->u2_num_mbs_left != 0 && impeg2d_bit_stream_nxt(&ps_dec->s_bit_stream,23) != 0x0);\n    return (IMPEG2D_ERROR_CODES_T)IVD_ERROR_NONE;\n}",
        "description": "libstagefright in Android versions prior to March 1, 2016, contains a vulnerability that allows attackers to gain unauthorized access to sensitive information by manipulating Bitstream data. This flaw enables attackers to potentially bypass an unspecified security mechanism, such as Signature or SignatureOrSystem access, through crafted input.",
        "commit": "Fixed bit stream access to ensure that reads do not exceed the allocated size."
    }
]