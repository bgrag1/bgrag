[
    {
        "cwe": "CWE-476",
        "func_name": "ImageMagick/LoadOpenCLDevices",
        "score": 0.7818822264671326,
        "func_before": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireQuantumMemory(length,\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "func_after": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireCriticalMemory(length*\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "description": "An issue was discovered in ImageMagick where a NULL pointer dereference vulnerability exists in the function responsible for loading OpenCL devices. This flaw allows attackers to trigger a denial of service through the use of a specially crafted file.",
        "commit": "It was discovered that ImageMagick, a popular image processing library, contains a vulnerability related to improper handling of certain image formats, potentially leading to buffer overflows or other memory corruption issues."
    },
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7311745882034302,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-193",
        "func_name": "torvalds/ext4_ext_insert_extent",
        "score": 0.7880982756614685,
        "func_before": "int ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\tint flags = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCK) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (flag & EXT4_GET_BLOCKS_PUNCH_OUT_EXT)\n\t\tflags = EXT4_MB_USE_ROOT_BLOCKS;\n\terr = ext4_ext_create_new_leaf(handle, inode, flags, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}",
        "func_after": "int ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\tint flags = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCKS) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\tif (flag & EXT4_GET_BLOCKS_PUNCH_OUT_EXT)\n\t\tflags = EXT4_MB_USE_ROOT_BLOCKS;\n\terr = ext4_ext_create_new_leaf(handle, inode, flags, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}",
        "description": "Multiple off-by-one errors in the ext4 subsystem of the Linux kernel, prior to a specific release candidate, enable local users to trigger a denial of service (resulting in BUG_ON and system crashes) through write operations on sparse files in extent format, particularly when the block number corresponds to the maximum possible 32-bit unsigned integer.",
        "commit": "A vulnerability was identified in the ext4 file system where writing to the last block (2^32-1) of a sparse file in extent format triggers a BUG_ON condition in the ext4_ext_put_gap_in_cache() function. The root cause is that the maximum bytes (s_maxbytes) are set such that the block at s_maxbytes fits into a 32-bit on-disk extent format, but the extent structure stores start block number and length in blocks, requiring EXT_MAX_BLOCK + 1 to cover the entire extent range. To resolve this issue without altering the struct ext4_extent members' meanings, s_maxbytes should be reduced by one filesystem block. Additionally, the commit renames EXT_MAX_BLOCK to EXT_MAX_BLOCKS and adjusts its usage to represent the maximum number of blocks in an extent, addressing inconsistencies in its application throughout the codebase. This bug can be reproduced by sequentially writing to the second-to-last and last blocks of a sparse file using the dd command."
    },
    {
        "cwe": "CWE-129",
        "func_name": "admesh/stl_fix_normal_directions",
        "score": 0.7639997601509094,
        "func_before": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "func_after": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1 &&\n         stl->neighbors_start[facet_num].neighbor[j] < stl->stats.number_of_facets*sizeof(char)) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "description": "An improper array index validation vulnerability exists in the stl_fix_normal_directions functionality of ADMesh. A specially-crafted STL file can lead to a heap buffer overflow. An attacker can exploit this by providing a malicious file.",
        "commit": "The vulnerability involves a check for the `neighbor_index` within the `stl_check_normal_vector` function. This fix addresses an issue identified in ticket #60."
    },
    {
        "cwe": "CWE-763",
        "func_name": "facebook/read",
        "score": 0.7682517170906067,
        "func_before": "void read(Protocol_* iprot, const StructInfo& structInfo, void* object) {\n  DCHECK(object);\n  ProtocolReaderStructReadState<Protocol_> readState;\n  readState.readStructBegin(iprot);\n\n  if (UNLIKELY(structInfo.unionExt != nullptr)) {\n    readState.fieldId = 0;\n    readState.readFieldBegin(iprot);\n    if (readState.atStop()) {\n      structInfo.unionExt->clear(object);\n      readState.readStructEnd(iprot);\n      return;\n    }\n    const auto* fieldInfo = findFieldInfo(iprot, readState, structInfo);\n    // Found it.\n    if (fieldInfo) {\n      void* unionVal = getMember(*fieldInfo, object);\n      // Default construct and placement new into the member union.\n      structInfo.unionExt->initMember[fieldInfo - structInfo.fieldInfos](\n          unionVal);\n      read(iprot, *fieldInfo->typeInfo, readState, unionVal);\n      const_cast<FieldID&>(activeUnionMemberId(\n          object, structInfo.unionExt->unionTypeOffset)) = fieldInfo->id;\n    } else {\n      skip(iprot, readState);\n    }\n    readState.readFieldEnd(iprot);\n    readState.readFieldBegin(iprot);\n    if (UNLIKELY(!readState.atStop())) {\n      TProtocolException::throwUnionMissingStop();\n    }\n    readState.readStructEnd(iprot);\n    return;\n  }\n\n  // Define out of loop to call advanceToNextField after the loop ends.\n  FieldID prevFieldId = 0;\n\n  // The index of the expected field in the struct layout.\n  std::int16_t index = 0;\n\n  // Every time advanceToNextField reports a field mismatch, either because the\n  // field is missing or if the serialized fields are not sorted (protocols\n  // don't guarantee a specific field order), we search for the field info\n  // matching the read bytes. Then we resume from the one past the found field\n  // to reduce the number of scans we have to do if the fields are sorted which\n  // is a common case. When we increment index past the number of fields we\n  // utilize the same search logic with a field info of type TType::T_STOP.\n  for (;; ++index) {\n    auto* fieldInfo = index < structInfo.numFields\n        ? &structInfo.fieldInfos[index]\n        : &kStopMarker;\n    // Try to match the next field in order against the current bytes.\n    if (UNLIKELY(!readState.advanceToNextField(\n            iprot, prevFieldId, fieldInfo->id, fieldInfo->typeInfo->type))) {\n      // Loop to skip until we find a match for both field id/name and type.\n      for (;;) {\n        readState.afterAdvanceFailure(iprot);\n        if (readState.atStop()) {\n          // Already at stop, return immediately.\n          readState.readStructEnd(iprot);\n          return;\n        }\n        fieldInfo = findFieldInfo(iprot, readState, structInfo);\n        // Found it.\n        if (fieldInfo) {\n          // Set the index to the field next in order to the found field.\n          index = fieldInfo - structInfo.fieldInfos;\n          break;\n        }\n        skip(iprot, readState);\n      }\n    } else if (UNLIKELY(index >= structInfo.numFields)) {\n      // We are at stop and have tried all of the fields, so return.\n      readState.readStructEnd(iprot);\n      return;\n    }\n    // Id and type are what we expect, try read.\n    prevFieldId = fieldInfo->id;\n    read(iprot, *fieldInfo->typeInfo, readState, getMember(*fieldInfo, object));\n    if (fieldInfo->issetOffset > 0) {\n      const_cast<bool&>(fieldIsSet(object, fieldInfo->issetOffset)) = true;\n    }\n  }\n}",
        "func_after": "void read(Protocol_* iprot, const StructInfo& structInfo, void* object) {\n  DCHECK(object);\n  ProtocolReaderStructReadState<Protocol_> readState;\n  readState.readStructBegin(iprot);\n\n  if (UNLIKELY(structInfo.unionExt != nullptr)) {\n    readState.fieldId = 0;\n    readState.readFieldBegin(iprot);\n    if (readState.atStop()) {\n      structInfo.unionExt->clear(object);\n      readState.readStructEnd(iprot);\n      return;\n    }\n    if (const auto* fieldInfo = findFieldInfo(iprot, readState, structInfo)) {\n      auto& activeId = const_cast<int&>(getActiveId(object, structInfo));\n      if (activeId != 0) {\n        structInfo.unionExt->clear(object);\n      }\n      void* value = getMember(*fieldInfo, object);\n      structInfo.unionExt->initMember[fieldInfo - structInfo.fieldInfos](value);\n      read(iprot, *fieldInfo->typeInfo, readState, value);\n      activeId = fieldInfo->id;\n    } else {\n      skip(iprot, readState);\n    }\n    readState.readFieldEnd(iprot);\n    readState.readFieldBegin(iprot);\n    if (UNLIKELY(!readState.atStop())) {\n      TProtocolException::throwUnionMissingStop();\n    }\n    readState.readStructEnd(iprot);\n    return;\n  }\n\n  // Define out of loop to call advanceToNextField after the loop ends.\n  FieldID prevFieldId = 0;\n\n  // The index of the expected field in the struct layout.\n  std::int16_t index = 0;\n\n  // Every time advanceToNextField reports a field mismatch, either because the\n  // field is missing or if the serialized fields are not sorted (protocols\n  // don't guarantee a specific field order), we search for the field info\n  // matching the read bytes. Then we resume from the one past the found field\n  // to reduce the number of scans we have to do if the fields are sorted which\n  // is a common case. When we increment index past the number of fields we\n  // utilize the same search logic with a field info of type TType::T_STOP.\n  for (;; ++index) {\n    auto* fieldInfo = index < structInfo.numFields\n        ? &structInfo.fieldInfos[index]\n        : &kStopMarker;\n    // Try to match the next field in order against the current bytes.\n    if (UNLIKELY(!readState.advanceToNextField(\n            iprot, prevFieldId, fieldInfo->id, fieldInfo->typeInfo->type))) {\n      // Loop to skip until we find a match for both field id/name and type.\n      for (;;) {\n        readState.afterAdvanceFailure(iprot);\n        if (readState.atStop()) {\n          // Already at stop, return immediately.\n          readState.readStructEnd(iprot);\n          return;\n        }\n        fieldInfo = findFieldInfo(iprot, readState, structInfo);\n        // Found it.\n        if (fieldInfo) {\n          // Set the index to the field next in order to the found field.\n          index = fieldInfo - structInfo.fieldInfos;\n          break;\n        }\n        skip(iprot, readState);\n      }\n    } else if (UNLIKELY(index >= structInfo.numFields)) {\n      // We are at stop and have tried all of the fields, so return.\n      readState.readStructEnd(iprot);\n      return;\n    }\n    // Id and type are what we expect, try read.\n    prevFieldId = fieldInfo->id;\n    read(iprot, *fieldInfo->typeInfo, readState, getMember(*fieldInfo, object));\n    if (fieldInfo->issetOffset > 0) {\n      const_cast<bool&>(fieldIsSet(object, fieldInfo->issetOffset)) = true;\n    }\n  }\n}",
        "description": "An invalid free operation within Thrift's table-based serialization can lead to application crashes or enable code execution and other unintended consequences. This vulnerability impacts Facebook Thrift versions prior to v2021.02.22.00.",
        "commit": "It was discovered that the table-based serializer improperly handled invalid union data, leading to potential memory leaks and other undesirable effects. Specifically, when duplicate union data was encountered, the previous active member of the union was overwritten without invoking the destructor of the old object. Additionally, if the second piece of data was incomplete, the incorrect destructor was called during stack unwinding, resulting in segmentation faults, data corruption, or other issues. The fix involves clearing the union if there is an active member and correcting the type of the data member that holds the active field ID."
    }
]