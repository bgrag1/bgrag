[
    {
        "cwe": "CWE-440",
        "func_name": "eclipse-openj9/Java_java_lang_invoke_InterfaceHandle_convertITableIndexToVTableIndex",
        "score": 0.7913997769355774,
        "func_before": "JNIEXPORT jint JNICALL Java_java_lang_invoke_InterfaceHandle_convertITableIndexToVTableIndex\n  (JNIEnv *env, jclass InterfaceMethodHandle, jlong interfaceArg, jint itableIndex, jlong receiverClassArg)\n   {\n   J9Class  *interfaceClass = (J9Class*)(intptr_t)interfaceArg;\n   J9Class  *receiverClass  = (J9Class*)(intptr_t)receiverClassArg;\n   J9ITable *itableEntry;\n   for (itableEntry = (J9ITable*)receiverClass->iTable; itableEntry; itableEntry = itableEntry->next)\n      if (itableEntry->interfaceClass == interfaceClass)\n         break;\n   TR_ASSERT(itableEntry, \"Shouldn't call convertITableIndexToVTableIndex without first ensuring the receiver implements the interface\");\n   UDATA *itableArray = (UDATA*)(itableEntry+1);\n   return (itableArray[itableIndex] - sizeof(J9Class))/sizeof(intptr_t);\n#if 0 // TODO:JSR292: We probably want to do something more like this instead, so it will properly handle exceptional cases\n   struct\n      {\n      UDATA interfaceClass;\n      UDATA methodIndex;\n      } indexAndLiterals =\n      {\n      (UDATA)interfaceClass,\n      (UDATA)itableIndex\n      };\n   J9Object *receiver = (J9Object*)receiverArg;\n   return (jint)jitLookupInterfaceMethod(receiver->clazz, &indexAndLiterals, 0);\n#endif\n   }",
        "func_after": "JNIEXPORT jint JNICALL Java_java_lang_invoke_InterfaceHandle_convertITableIndexToVTableIndex\n  (JNIEnv *env, jclass InterfaceMethodHandle, jlong interfaceArg, jint itableIndex, jlong receiverClassArg)\n   {\n   J9Class  *interfaceClass = (J9Class*)(intptr_t)interfaceArg;\n   J9Class  *receiverClass  = (J9Class*)(intptr_t)receiverClassArg;\n   J9ITable *itableEntry;\n   for (itableEntry = (J9ITable*)receiverClass->iTable; itableEntry; itableEntry = itableEntry->next)\n      if (itableEntry->interfaceClass == interfaceClass)\n         break;\n   TR_ASSERT(itableEntry, \"Shouldn't call convertITableIndexToVTableIndex without first ensuring the receiver implements the interface\");\n   UDATA *itableArray = (UDATA*)(itableEntry+1);\n   UDATA vTableOffset = itableArray[itableIndex];\n   J9Method *method = *(J9Method**)((UDATA)receiverClass + vTableOffset);\n   if ((J9_ROM_METHOD_FROM_RAM_METHOD(method)->modifiers & J9AccPublic) == 0)\n      return -1;\n\n   return (vTableOffset - sizeof(J9Class))/sizeof(intptr_t);\n#if 0 // TODO:JSR292: We probably want to do something more like this instead, so it will properly handle exceptional cases\n   struct\n      {\n      UDATA interfaceClass;\n      UDATA methodIndex;\n      } indexAndLiterals =\n      {\n      (UDATA)interfaceClass,\n      (UDATA)itableIndex\n      };\n   J9Object *receiver = (J9Object*)receiverArg;\n   return (jint)jitLookupInterfaceMethod(receiver->clazz, &indexAndLiterals, 0);\n#endif\n   }",
        "description": "In versions of Eclipse Openj9 prior to 0.29.0, the Java Virtual Machine (JVM) fails to throw an IllegalAccessError when MethodHandles are used to invoke methods that are not accessible through interfaces.",
        "commit": "The vulnerability involves a situation where an `IllegalAccessError` (IAE) is not being thrown appropriately when an `InterfaceHandle` encounters a non-public method during dispatch. The expected behavior is for the dispatch mechanism implemented by `InterfaceHandle` to mimic that of `invokeinterface`, which throws an `IAE` when attempting to access non-public methods. This discrepancy allows unauthorized access to non-public methods, potentially leading to security vulnerabilities."
    },
    {
        "cwe": "CWE-330",
        "func_name": "torvalds/sfb_enqueue",
        "score": 0.8158020377159119,
        "func_before": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = jhash_1word(salt, q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(skb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "func_after": "static int sfb_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\n\tstruct sfb_sched_data *q = qdisc_priv(sch);\n\tstruct Qdisc *child = q->qdisc;\n\tstruct tcf_proto *fl;\n\tint i;\n\tu32 p_min = ~0;\n\tu32 minqlen = ~0;\n\tu32 r, sfbhash;\n\tu32 slot = q->slot;\n\tint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\n\tif (unlikely(sch->q.qlen >= q->limit)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.queuedrop++;\n\t\tgoto drop;\n\t}\n\n\tif (q->rehash_interval > 0) {\n\t\tunsigned long limit = q->rehash_time + q->rehash_interval;\n\n\t\tif (unlikely(time_after(jiffies, limit))) {\n\t\t\tsfb_swap_slot(q);\n\t\t\tq->rehash_time = jiffies;\n\t\t} else if (unlikely(!q->double_buffering && q->warmup_time > 0 &&\n\t\t\t\t    time_after(jiffies, limit - q->warmup_time))) {\n\t\t\tq->double_buffering = true;\n\t\t}\n\t}\n\n\tfl = rcu_dereference_bh(q->filter_list);\n\tif (fl) {\n\t\tu32 salt;\n\n\t\t/* If using external classifiers, get result and record it. */\n\t\tif (!sfb_classify(skb, fl, &ret, &salt))\n\t\t\tgoto other_drop;\n\t\tsfbhash = siphash_1u32(salt, &q->bins[slot].perturbation);\n\t} else {\n\t\tsfbhash = skb_get_hash_perturb(skb, &q->bins[slot].perturbation);\n\t}\n\n\n\tif (!sfbhash)\n\t\tsfbhash = 1;\n\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\tif (b->qlen == 0)\n\t\t\tdecrement_prob(b, q);\n\t\telse if (b->qlen >= q->bin_size)\n\t\t\tincrement_prob(b, q);\n\t\tif (minqlen > b->qlen)\n\t\t\tminqlen = b->qlen;\n\t\tif (p_min > b->p_mark)\n\t\t\tp_min = b->p_mark;\n\t}\n\n\tslot ^= 1;\n\tsfb_skb_cb(skb)->hashes[slot] = 0;\n\n\tif (unlikely(minqlen >= q->max)) {\n\t\tqdisc_qstats_overlimit(sch);\n\t\tq->stats.bucketdrop++;\n\t\tgoto drop;\n\t}\n\n\tif (unlikely(p_min >= SFB_MAX_PROB)) {\n\t\t/* Inelastic flow */\n\t\tif (q->double_buffering) {\n\t\t\tsfbhash = skb_get_hash_perturb(skb,\n\t\t\t    &q->bins[slot].perturbation);\n\t\t\tif (!sfbhash)\n\t\t\t\tsfbhash = 1;\n\t\t\tsfb_skb_cb(skb)->hashes[slot] = sfbhash;\n\n\t\t\tfor (i = 0; i < SFB_LEVELS; i++) {\n\t\t\t\tu32 hash = sfbhash & SFB_BUCKET_MASK;\n\t\t\t\tstruct sfb_bucket *b = &q->bins[slot].bins[i][hash];\n\n\t\t\t\tsfbhash >>= SFB_BUCKET_SHIFT;\n\t\t\t\tif (b->qlen == 0)\n\t\t\t\t\tdecrement_prob(b, q);\n\t\t\t\telse if (b->qlen >= q->bin_size)\n\t\t\t\t\tincrement_prob(b, q);\n\t\t\t}\n\t\t}\n\t\tif (sfb_rate_limit(skb, q)) {\n\t\t\tqdisc_qstats_overlimit(sch);\n\t\t\tq->stats.penaltydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t\tgoto enqueue;\n\t}\n\n\tr = prandom_u32() & SFB_MAX_PROB;\n\n\tif (unlikely(r < p_min)) {\n\t\tif (unlikely(p_min > SFB_MAX_PROB / 2)) {\n\t\t\t/* If we're marking that many packets, then either\n\t\t\t * this flow is unresponsive, or we're badly congested.\n\t\t\t * In either case, we want to start dropping packets.\n\t\t\t */\n\t\t\tif (r < (p_min - SFB_MAX_PROB / 2) * 2) {\n\t\t\t\tq->stats.earlydrop++;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t}\n\t\tif (INET_ECN_set_ce(skb)) {\n\t\t\tq->stats.marked++;\n\t\t} else {\n\t\t\tq->stats.earlydrop++;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nenqueue:\n\tret = qdisc_enqueue(skb, child, to_free);\n\tif (likely(ret == NET_XMIT_SUCCESS)) {\n\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\tsch->q.qlen++;\n\t\tincrement_qlen(skb, q);\n\t} else if (net_xmit_drop_count(ret)) {\n\t\tq->stats.childdrop++;\n\t\tqdisc_qstats_drop(sch);\n\t}\n\treturn ret;\n\ndrop:\n\tqdisc_drop(skb, sch, to_free);\n\treturn NET_XMIT_CN;\nother_drop:\n\tif (ret & __NET_XMIT_BYPASS)\n\t\tqdisc_qstats_drop(sch);\n\tkfree_skb(skb);\n\treturn ret;\n}",
        "description": "The flow_dissector feature in the Linux kernel from version 4.3 up to but not including 5.3.10 suffers from a device tracking vulnerability, identified as CID-55667441c84f. This vulnerability arises due to the reliance on a 32-bit hashrnd value as a secret for the auto flowlabel of a UDP IPv6 packet. Additionally, the use of jhash instead of siphash exacerbates the issue. Since the hashrnd value remains constant from the time of system boot, it can be deduced by an attacker, thereby compromising the intended security measures. This problem is present in the net/core/flow_dissector.c file and related components.",
        "commit": "The vulnerability involves the use of a 32-bit secret in generating auto flowlabels for UDP IPv6 packets, which can be inferred by attackers to identify devices or users. The secret is initialized only at boot time and is used in conjunction with the jhash function to create flow labels that are predictable. This predictability poses a significant privacy risk. The proposed solution is to switch from using jhash to a cryptographically strong pseudo-random function like siphash, similar to changes made in the IP ID generator. This switch aims to enhance security by making the flow label generation process less predictable and thereby reducing the risk of device/user identification."
    },
    {
        "cwe": "CWE-367",
        "func_name": "torvalds/enter_svm_guest_mode",
        "score": 0.7893915176391602,
        "func_before": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "func_after": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "description": "A use-after-free vulnerability exists in the Linux kernel prior to version 5.11.12 within the AMD KVM SVM nested virtualization module. This flaw allows an AMD KVM guest to potentially bypass access controls on host OS Model-Specific Registers (MSRs) when multiple nested guests are present. The issue arises due to a Time-of-Check to Time-of-Use (TOCTOU) race condition involving a VMCB12 double fetch in the `nested_svm_vmrun` function.",
        "commit": "To avoid race conditions between checking and using nested VMCB controls, ensuring that the VMRUN intercept is consistently reflected to the nested hypervisor rather than being processed by the host is crucial. Without this patch, there is a risk that `svm->nested.hsave` could point to the MSR permission bitmap for nested guests, leading to potential vulnerabilities such as CVE-2021-29657."
    },
    {
        "cwe": "CWE-732",
        "func_name": "torvalds/read_mem",
        "score": 0.8118821978569031,
        "func_before": "static ssize_t read_mem(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t read, sz;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn 0;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\tread = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\tif (sz > 0) {\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tcount -= sz;\n\t\t\tread += sz;\n\t\t}\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tunsigned long remaining;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\tif (!range_is_allowed(p >> PAGE_SHIFT, count))\n\t\t\treturn -EPERM;\n\n\t\t/*\n\t\t * On ia64 if a page has been mapped somewhere as uncached, then\n\t\t * it must also be accessed uncached by the kernel or data\n\t\t * corruption may occur.\n\t\t */\n\t\tptr = xlate_dev_mem_ptr(p);\n\t\tif (!ptr)\n\t\t\treturn -EFAULT;\n\n\t\tremaining = copy_to_user(buf, ptr, sz);\n\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\tif (remaining)\n\t\t\treturn -EFAULT;\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\tread += sz;\n\t}\n\n\t*ppos += read;\n\treturn read;\n}",
        "func_after": "static ssize_t read_mem(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t read, sz;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn 0;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\tread = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\tif (sz > 0) {\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tcount -= sz;\n\t\t\tread += sz;\n\t\t}\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tunsigned long remaining;\n\t\tint allowed;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\tallowed = page_is_allowed(p >> PAGE_SHIFT);\n\t\tif (!allowed)\n\t\t\treturn -EPERM;\n\t\tif (allowed == 2) {\n\t\t\t/* Show zeros for restricted memory. */\n\t\t\tremaining = clear_user(buf, sz);\n\t\t} else {\n\t\t\t/*\n\t\t\t * On ia64 if a page has been mapped somewhere as\n\t\t\t * uncached, then it must also be accessed uncached\n\t\t\t * by the kernel or data corruption may occur.\n\t\t\t */\n\t\t\tptr = xlate_dev_mem_ptr(p);\n\t\t\tif (!ptr)\n\t\t\t\treturn -EFAULT;\n\n\t\t\tremaining = copy_to_user(buf, ptr, sz);\n\n\t\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\t}\n\n\t\tif (remaining)\n\t\t\treturn -EFAULT;\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\tread += sz;\n\t}\n\n\t*ppos += read;\n\treturn read;\n}",
        "description": "The memory management (mm) subsystem in the Linux kernel up to version 3.2 inadequately enforces the CONFIG_STRICT_DEVMEM protection mechanism. This failure enables local users to access and modify kernel memory locations within the first megabyte, effectively circumventing slab-allocation access restrictions by utilizing an application that opens the /dev/mem file.",
        "commit": "Under the configuration option `CONFIG_STRICT_DEVMEM`, reading System RAM through `/dev/mem` is prohibited in the Linux kernel. On x86 architectures, however, the first 1MB of memory was traditionally allowed for BIOS and other purposes, irrespective of whether it contained actual System RAM. This could lead to situations where heap allocations ended up in the low 1MB of RAM, which could then be accessed and potentially exposed by tools like `x86info` or `dd`. To mitigate this, the kernel now reads back zeros for System RAM areas within the low 1MB range instead of allowing such accesses unconditionally. Further enhancements are required to apply this restriction to `mmap` operations, although currently, `mmap` does not go through the usercopy mechanism, thus avoiding potential kernel oopses due to hardened usercopy."
    },
    {
        "cwe": "CWE-863",
        "func_name": "torvalds/internal_get_user_pages_fast",
        "score": 0.8148948550224304,
        "func_before": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "func_after": "static int internal_get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\t\t\tunsigned int gup_flags,\n\t\t\t\t\tstruct page **pages)\n{\n\tunsigned long addr, len, end;\n\tint nr_pinned = 0, ret = 0;\n\n\tif (WARN_ON_ONCE(gup_flags & ~(FOLL_WRITE | FOLL_LONGTERM |\n\t\t\t\t       FOLL_FORCE | FOLL_PIN | FOLL_GET)))\n\t\treturn -EINVAL;\n\n\tstart = untagged_addr(start) & PAGE_MASK;\n\taddr = start;\n\tlen = (unsigned long) nr_pages << PAGE_SHIFT;\n\tend = start + len;\n\n\tif (end <= start)\n\t\treturn 0;\n\tif (unlikely(!access_ok((void __user *)start, len)))\n\t\treturn -EFAULT;\n\n\t/*\n\t * The FAST_GUP case requires FOLL_WRITE even for pure reads,\n\t * because get_user_pages() may need to cause an early COW in\n\t * order to avoid confusing the normal COW routines. So only\n\t * targets that are already writable are safe to do by just\n\t * looking at the page tables.\n\t */\n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP) &&\n\t    gup_fast_permitted(start, end)) {\n\t\tlocal_irq_disable();\n\t\tgup_pgd_range(addr, end, gup_flags | FOLL_WRITE, pages, &nr_pinned);\n\t\tlocal_irq_enable();\n\t\tret = nr_pinned;\n\t}\n\n\tif (nr_pinned < nr_pages) {\n\t\t/* Try to get the remaining pages with get_user_pages */\n\t\tstart += nr_pinned << PAGE_SHIFT;\n\t\tpages += nr_pinned;\n\n\t\tret = __gup_longterm_unlocked(start, nr_pages - nr_pinned,\n\t\t\t\t\t      gup_flags, pages);\n\n\t\t/* Have to be a bit careful with return values */\n\t\tif (nr_pinned > 0) {\n\t\t\tif (ret < 0)\n\t\t\t\tret = nr_pinned;\n\t\t\telse\n\t\t\t\tret += nr_pinned;\n\t\t}\n\t}\n\n\treturn ret;\n}",
        "description": "An issue was discovered in the Linux kernel prior to version 5.7.3, affecting the memory management components located in mm/gup.c and mm/huge_memory.c. The get_user_pages (gup) implementation, when utilized for copy-on-write pages, fails to correctly handle read operation semantics, potentially resulting in unintended write access.",
        "commit": "The vulnerability involves the `get_user_pages()` function in the Linux kernel, which can lead to ambiguous behavior when dealing with copy-on-write (COW) pages. Specifically, the function may return a page pointer that is no longer associated with the original virtual memory area (VM) due to potential COW events. This ambiguity arises because the direction and timing of COW events are undefined, and a page can be unmapped by the thread that performed the `get_user_pages()` call, especially under memory pressure.\n\nTo mitigate this issue, the kernel introduces a change to force a COW event by setting the `FOLL_WRITE` flag when `FOLL_GET` or `FOLL_PIN` is used on a COW mapping. This ensures that the page is properly isolated from other VMs, preventing unintended access or control. However, this change affects the behavior of `get_user_pages_fast()`, which now refuses to follow read-only pages due to the potential need for COW breaking, leading to slower processing in such cases.\n\nThe current semantics are as follows:\n- `__get_user_pages_fast()`: No changes; it does not break COW unless explicitly asked.\n- `get_user_pages_fast()`: Refuses to follow read-only pages, requiring the slow path for COW breaking.\n- `get_user_pages()`: Forces COW breaking for COW mappings when using `FOLL_GET` or `FOLL_PIN`.\n\nThis change aims to clarify the semantics of `get_user_pages()` and reduce the risk of subtle bugs related to COW behavior. While this addresses the ambiguity, it is noted that true shared mappings will still allow pages to change under the user, meaning `get_user_pages()` does not guarantee a \"stable\" page."
    }
]