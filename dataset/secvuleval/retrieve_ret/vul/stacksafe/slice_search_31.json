[
    {
        "cwe": "CWE-665",
        "func_name": "torvalds/__skb_flow_dissect",
        "score": 0.7975707650184631,
        "func_before": "bool __skb_flow_dissect(const struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen)\n{\n\tstruct flow_dissector_key_control *key_control;\n\tstruct flow_dissector_key_basic *key_basic;\n\tstruct flow_dissector_key_addrs *key_addrs;\n\tstruct flow_dissector_key_ports *key_ports;\n\tstruct flow_dissector_key_tags *key_tags;\n\tstruct flow_dissector_key_keyid *key_keyid;\n\tu8 ip_proto = 0;\n\n\tif (!data) {\n\t\tdata = skb->data;\n\t\tproto = skb->protocol;\n\t\tnhoff = skb_network_offset(skb);\n\t\thlen = skb_headlen(skb);\n\t}\n\n\t/* It is ensured by skb_flow_dissector_init() that control key will\n\t * be always present.\n\t */\n\tkey_control = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_CONTROL,\n\t\t\t\t\t\ttarget_container);\n\n\t/* It is ensured by skb_flow_dissector_init() that basic key will\n\t * be always present.\n\t */\n\tkey_basic = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t      FLOW_DISSECTOR_KEY_BASIC,\n\t\t\t\t\t      target_container);\n\n\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_ETH_ADDRS)) {\n\t\tstruct ethhdr *eth = eth_hdr(skb);\n\t\tstruct flow_dissector_key_eth_addrs *key_eth_addrs;\n\n\t\tkey_eth_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t  FLOW_DISSECTOR_KEY_ETH_ADDRS,\n\t\t\t\t\t\t\t  target_container);\n\t\tmemcpy(key_eth_addrs, &eth->h_dest, sizeof(*key_eth_addrs));\n\t}\n\nagain:\n\tswitch (proto) {\n\tcase htons(ETH_P_IP): {\n\t\tconst struct iphdr *iph;\n\t\tstruct iphdr _iph;\nip:\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph || iph->ihl < 5)\n\t\t\treturn false;\n\t\tnhoff += iph->ihl * 4;\n\n\t\tip_proto = iph->protocol;\n\t\tif (ip_is_fragment(iph))\n\t\t\tip_proto = 0;\n\n\t\tif (!skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t FLOW_DISSECTOR_KEY_IPV4_ADDRS))\n\t\t\tbreak;\n\n\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t      FLOW_DISSECTOR_KEY_IPV4_ADDRS, target_container);\n\t\tmemcpy(&key_addrs->v4addrs, &iph->saddr,\n\t\t       sizeof(key_addrs->v4addrs));\n\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_IPV6): {\n\t\tconst struct ipv6hdr *iph;\n\t\tstruct ipv6hdr _iph;\n\t\t__be32 flow_label;\n\nipv6:\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph)\n\t\t\treturn false;\n\n\t\tip_proto = iph->nexthdr;\n\t\tnhoff += sizeof(struct ipv6hdr);\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_IPV6_ADDRS)) {\n\t\t\tstruct flow_dissector_key_ipv6_addrs *key_ipv6_addrs;\n\n\t\t\tkey_ipv6_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t   FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t\t\t\t\t\t\t   target_container);\n\n\t\t\tmemcpy(key_ipv6_addrs, &iph->saddr, sizeof(*key_ipv6_addrs));\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t\t}\n\n\t\tflow_label = ip6_flowlabel(iph);\n\t\tif (flow_label) {\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_FLOW_LABEL)) {\n\t\t\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_FLOW_LABEL,\n\t\t\t\t\t\t\t\t     target_container);\n\t\t\t\tkey_tags->flow_label = ntohl(flow_label);\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_8021AD):\n\tcase htons(ETH_P_8021Q): {\n\t\tconst struct vlan_hdr *vlan;\n\t\tstruct vlan_hdr _vlan;\n\n\t\tvlan = __skb_header_pointer(skb, nhoff, sizeof(_vlan), data, hlen, &_vlan);\n\t\tif (!vlan)\n\t\t\treturn false;\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_VLANID)) {\n\t\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_VLANID,\n\t\t\t\t\t\t\t     target_container);\n\n\t\t\tkey_tags->vlan_id = skb_vlan_tag_get_id(skb);\n\t\t}\n\n\t\tproto = vlan->h_vlan_encapsulated_proto;\n\t\tnhoff += sizeof(*vlan);\n\t\tgoto again;\n\t}\n\tcase htons(ETH_P_PPP_SES): {\n\t\tstruct {\n\t\t\tstruct pppoe_hdr hdr;\n\t\t\t__be16 proto;\n\t\t} *hdr, _hdr;\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\treturn false;\n\t\tproto = hdr->proto;\n\t\tnhoff += PPPOE_SES_HLEN;\n\t\tswitch (proto) {\n\t\tcase htons(PPP_IP):\n\t\t\tgoto ip;\n\t\tcase htons(PPP_IPV6):\n\t\t\tgoto ipv6;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\tcase htons(ETH_P_TIPC): {\n\t\tstruct {\n\t\t\t__be32 pre[3];\n\t\t\t__be32 srcnode;\n\t\t} *hdr, _hdr;\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\treturn false;\n\t\tkey_basic->n_proto = proto;\n\t\tkey_control->thoff = (u16)nhoff;\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_TIPC_ADDRS)) {\n\t\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_TIPC_ADDRS,\n\t\t\t\t\t\t\t      target_container);\n\t\t\tkey_addrs->tipcaddrs.srcnode = hdr->srcnode;\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_TIPC_ADDRS;\n\t\t}\n\t\treturn true;\n\t}\n\n\tcase htons(ETH_P_MPLS_UC):\n\tcase htons(ETH_P_MPLS_MC): {\n\t\tstruct mpls_label *hdr, _hdr[2];\nmpls:\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,\n\t\t\t\t\t   hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\treturn false;\n\n\t\tif ((ntohl(hdr[0].entry) & MPLS_LS_LABEL_MASK) >>\n\t\t     MPLS_LS_LABEL_SHIFT == MPLS_LABEL_ENTROPY) {\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t\tFLOW_DISSECTOR_KEY_MPLS_ENTROPY)) {\n\t\t\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_MPLS_ENTROPY,\n\t\t\t\t\t\t\t\t      target_container);\n\t\t\t\tkey_keyid->keyid = hdr[1].entry &\n\t\t\t\t\thtonl(MPLS_LS_LABEL_MASK);\n\t\t\t}\n\n\t\t\tkey_basic->n_proto = proto;\n\t\t\tkey_basic->ip_proto = ip_proto;\n\t\t\tkey_control->thoff = (u16)nhoff;\n\n\t\t\treturn true;\n\t\t}\n\n\t\treturn true;\n\t}\n\n\tcase htons(ETH_P_FCOE):\n\t\tkey_control->thoff = (u16)(nhoff + FCOE_HEADER_LEN);\n\t\t/* fall through */\n\tdefault:\n\t\treturn false;\n\t}\n\nip_proto_again:\n\tswitch (ip_proto) {\n\tcase IPPROTO_GRE: {\n\t\tstruct gre_hdr {\n\t\t\t__be16 flags;\n\t\t\t__be16 proto;\n\t\t} *hdr, _hdr;\n\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\treturn false;\n\t\t/*\n\t\t * Only look inside GRE if version zero and no\n\t\t * routing\n\t\t */\n\t\tif (hdr->flags & (GRE_VERSION | GRE_ROUTING))\n\t\t\tbreak;\n\n\t\tproto = hdr->proto;\n\t\tnhoff += 4;\n\t\tif (hdr->flags & GRE_CSUM)\n\t\t\tnhoff += 4;\n\t\tif (hdr->flags & GRE_KEY) {\n\t\t\tconst __be32 *keyid;\n\t\t\t__be32 _keyid;\n\n\t\t\tkeyid = __skb_header_pointer(skb, nhoff, sizeof(_keyid),\n\t\t\t\t\t\t     data, hlen, &_keyid);\n\n\t\t\tif (!keyid)\n\t\t\t\treturn false;\n\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t\tFLOW_DISSECTOR_KEY_GRE_KEYID)) {\n\t\t\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_GRE_KEYID,\n\t\t\t\t\t\t\t\t      target_container);\n\t\t\t\tkey_keyid->keyid = *keyid;\n\t\t\t}\n\t\t\tnhoff += 4;\n\t\t}\n\t\tif (hdr->flags & GRE_SEQ)\n\t\t\tnhoff += 4;\n\t\tif (proto == htons(ETH_P_TEB)) {\n\t\t\tconst struct ethhdr *eth;\n\t\t\tstruct ethhdr _eth;\n\n\t\t\teth = __skb_header_pointer(skb, nhoff,\n\t\t\t\t\t\t   sizeof(_eth),\n\t\t\t\t\t\t   data, hlen, &_eth);\n\t\t\tif (!eth)\n\t\t\t\treturn false;\n\t\t\tproto = eth->h_proto;\n\t\t\tnhoff += sizeof(*eth);\n\t\t}\n\t\tgoto again;\n\t}\n\tcase NEXTHDR_HOP:\n\tcase NEXTHDR_ROUTING:\n\tcase NEXTHDR_DEST: {\n\t\tu8 _opthdr[2], *opthdr;\n\n\t\tif (proto != htons(ETH_P_IPV6))\n\t\t\tbreak;\n\n\t\topthdr = __skb_header_pointer(skb, nhoff, sizeof(_opthdr),\n\t\t\t\t\t      data, hlen, &_opthdr);\n\t\tif (!opthdr)\n\t\t\treturn false;\n\n\t\tip_proto = opthdr[0];\n\t\tnhoff += (opthdr[1] + 1) << 3;\n\n\t\tgoto ip_proto_again;\n\t}\n\tcase IPPROTO_IPIP:\n\t\tproto = htons(ETH_P_IP);\n\t\tgoto ip;\n\tcase IPPROTO_IPV6:\n\t\tproto = htons(ETH_P_IPV6);\n\t\tgoto ipv6;\n\tcase IPPROTO_MPLS:\n\t\tproto = htons(ETH_P_MPLS_UC);\n\t\tgoto mpls;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tkey_basic->n_proto = proto;\n\tkey_basic->ip_proto = ip_proto;\n\tkey_control->thoff = (u16)nhoff;\n\n\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_PORTS)) {\n\t\tkey_ports = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_PORTS,\n\t\t\t\t\t\t      target_container);\n\t\tkey_ports->ports = __skb_flow_get_ports(skb, nhoff, ip_proto,\n\t\t\t\t\t\t\tdata, hlen);\n\t}\n\n\treturn true;\n}",
        "func_after": "bool __skb_flow_dissect(const struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container,\n\t\t\tvoid *data, __be16 proto, int nhoff, int hlen)\n{\n\tstruct flow_dissector_key_control *key_control;\n\tstruct flow_dissector_key_basic *key_basic;\n\tstruct flow_dissector_key_addrs *key_addrs;\n\tstruct flow_dissector_key_ports *key_ports;\n\tstruct flow_dissector_key_tags *key_tags;\n\tstruct flow_dissector_key_keyid *key_keyid;\n\tu8 ip_proto = 0;\n\tbool ret = false;\n\n\tif (!data) {\n\t\tdata = skb->data;\n\t\tproto = skb->protocol;\n\t\tnhoff = skb_network_offset(skb);\n\t\thlen = skb_headlen(skb);\n\t}\n\n\t/* It is ensured by skb_flow_dissector_init() that control key will\n\t * be always present.\n\t */\n\tkey_control = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_CONTROL,\n\t\t\t\t\t\ttarget_container);\n\n\t/* It is ensured by skb_flow_dissector_init() that basic key will\n\t * be always present.\n\t */\n\tkey_basic = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t      FLOW_DISSECTOR_KEY_BASIC,\n\t\t\t\t\t      target_container);\n\n\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_ETH_ADDRS)) {\n\t\tstruct ethhdr *eth = eth_hdr(skb);\n\t\tstruct flow_dissector_key_eth_addrs *key_eth_addrs;\n\n\t\tkey_eth_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t  FLOW_DISSECTOR_KEY_ETH_ADDRS,\n\t\t\t\t\t\t\t  target_container);\n\t\tmemcpy(key_eth_addrs, &eth->h_dest, sizeof(*key_eth_addrs));\n\t}\n\nagain:\n\tswitch (proto) {\n\tcase htons(ETH_P_IP): {\n\t\tconst struct iphdr *iph;\n\t\tstruct iphdr _iph;\nip:\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph || iph->ihl < 5)\n\t\t\tgoto out_bad;\n\t\tnhoff += iph->ihl * 4;\n\n\t\tip_proto = iph->protocol;\n\t\tif (ip_is_fragment(iph))\n\t\t\tip_proto = 0;\n\n\t\tif (!skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t FLOW_DISSECTOR_KEY_IPV4_ADDRS))\n\t\t\tbreak;\n\n\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t      FLOW_DISSECTOR_KEY_IPV4_ADDRS, target_container);\n\t\tmemcpy(&key_addrs->v4addrs, &iph->saddr,\n\t\t       sizeof(key_addrs->v4addrs));\n\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_IPV6): {\n\t\tconst struct ipv6hdr *iph;\n\t\tstruct ipv6hdr _iph;\n\t\t__be32 flow_label;\n\nipv6:\n\t\tiph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);\n\t\tif (!iph)\n\t\t\tgoto out_bad;\n\n\t\tip_proto = iph->nexthdr;\n\t\tnhoff += sizeof(struct ipv6hdr);\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_IPV6_ADDRS)) {\n\t\t\tstruct flow_dissector_key_ipv6_addrs *key_ipv6_addrs;\n\n\t\t\tkey_ipv6_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t   FLOW_DISSECTOR_KEY_IPV6_ADDRS,\n\t\t\t\t\t\t\t\t   target_container);\n\n\t\t\tmemcpy(key_ipv6_addrs, &iph->saddr, sizeof(*key_ipv6_addrs));\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t\t}\n\n\t\tflow_label = ip6_flowlabel(iph);\n\t\tif (flow_label) {\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\tFLOW_DISSECTOR_KEY_FLOW_LABEL)) {\n\t\t\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_FLOW_LABEL,\n\t\t\t\t\t\t\t\t     target_container);\n\t\t\t\tkey_tags->flow_label = ntohl(flow_label);\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\t}\n\tcase htons(ETH_P_8021AD):\n\tcase htons(ETH_P_8021Q): {\n\t\tconst struct vlan_hdr *vlan;\n\t\tstruct vlan_hdr _vlan;\n\n\t\tvlan = __skb_header_pointer(skb, nhoff, sizeof(_vlan), data, hlen, &_vlan);\n\t\tif (!vlan)\n\t\t\tgoto out_bad;\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_VLANID)) {\n\t\t\tkey_tags = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t     FLOW_DISSECTOR_KEY_VLANID,\n\t\t\t\t\t\t\t     target_container);\n\n\t\t\tkey_tags->vlan_id = skb_vlan_tag_get_id(skb);\n\t\t}\n\n\t\tproto = vlan->h_vlan_encapsulated_proto;\n\t\tnhoff += sizeof(*vlan);\n\t\tgoto again;\n\t}\n\tcase htons(ETH_P_PPP_SES): {\n\t\tstruct {\n\t\t\tstruct pppoe_hdr hdr;\n\t\t\t__be16 proto;\n\t\t} *hdr, _hdr;\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\tgoto out_bad;\n\t\tproto = hdr->proto;\n\t\tnhoff += PPPOE_SES_HLEN;\n\t\tswitch (proto) {\n\t\tcase htons(PPP_IP):\n\t\t\tgoto ip;\n\t\tcase htons(PPP_IPV6):\n\t\t\tgoto ipv6;\n\t\tdefault:\n\t\t\tgoto out_bad;\n\t\t}\n\t}\n\tcase htons(ETH_P_TIPC): {\n\t\tstruct {\n\t\t\t__be32 pre[3];\n\t\t\t__be32 srcnode;\n\t\t} *hdr, _hdr;\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\tgoto out_bad;\n\n\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\tFLOW_DISSECTOR_KEY_TIPC_ADDRS)) {\n\t\t\tkey_addrs = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_TIPC_ADDRS,\n\t\t\t\t\t\t\t      target_container);\n\t\t\tkey_addrs->tipcaddrs.srcnode = hdr->srcnode;\n\t\t\tkey_control->addr_type = FLOW_DISSECTOR_KEY_TIPC_ADDRS;\n\t\t}\n\t\tgoto out_good;\n\t}\n\n\tcase htons(ETH_P_MPLS_UC):\n\tcase htons(ETH_P_MPLS_MC): {\n\t\tstruct mpls_label *hdr, _hdr[2];\nmpls:\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,\n\t\t\t\t\t   hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\tgoto out_bad;\n\n\t\tif ((ntohl(hdr[0].entry) & MPLS_LS_LABEL_MASK) >>\n\t\t     MPLS_LS_LABEL_SHIFT == MPLS_LABEL_ENTROPY) {\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t\tFLOW_DISSECTOR_KEY_MPLS_ENTROPY)) {\n\t\t\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_MPLS_ENTROPY,\n\t\t\t\t\t\t\t\t      target_container);\n\t\t\t\tkey_keyid->keyid = hdr[1].entry &\n\t\t\t\t\thtonl(MPLS_LS_LABEL_MASK);\n\t\t\t}\n\n\t\t\tgoto out_good;\n\t\t}\n\n\t\tgoto out_good;\n\t}\n\n\tcase htons(ETH_P_FCOE):\n\t\tkey_control->thoff = (u16)(nhoff + FCOE_HEADER_LEN);\n\t\t/* fall through */\n\tdefault:\n\t\tgoto out_bad;\n\t}\n\nip_proto_again:\n\tswitch (ip_proto) {\n\tcase IPPROTO_GRE: {\n\t\tstruct gre_hdr {\n\t\t\t__be16 flags;\n\t\t\t__be16 proto;\n\t\t} *hdr, _hdr;\n\n\t\thdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);\n\t\tif (!hdr)\n\t\t\tgoto out_bad;\n\t\t/*\n\t\t * Only look inside GRE if version zero and no\n\t\t * routing\n\t\t */\n\t\tif (hdr->flags & (GRE_VERSION | GRE_ROUTING))\n\t\t\tbreak;\n\n\t\tproto = hdr->proto;\n\t\tnhoff += 4;\n\t\tif (hdr->flags & GRE_CSUM)\n\t\t\tnhoff += 4;\n\t\tif (hdr->flags & GRE_KEY) {\n\t\t\tconst __be32 *keyid;\n\t\t\t__be32 _keyid;\n\n\t\t\tkeyid = __skb_header_pointer(skb, nhoff, sizeof(_keyid),\n\t\t\t\t\t\t     data, hlen, &_keyid);\n\n\t\t\tif (!keyid)\n\t\t\t\tgoto out_bad;\n\n\t\t\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\t\t\tFLOW_DISSECTOR_KEY_GRE_KEYID)) {\n\t\t\t\tkey_keyid = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_GRE_KEYID,\n\t\t\t\t\t\t\t\t      target_container);\n\t\t\t\tkey_keyid->keyid = *keyid;\n\t\t\t}\n\t\t\tnhoff += 4;\n\t\t}\n\t\tif (hdr->flags & GRE_SEQ)\n\t\t\tnhoff += 4;\n\t\tif (proto == htons(ETH_P_TEB)) {\n\t\t\tconst struct ethhdr *eth;\n\t\t\tstruct ethhdr _eth;\n\n\t\t\teth = __skb_header_pointer(skb, nhoff,\n\t\t\t\t\t\t   sizeof(_eth),\n\t\t\t\t\t\t   data, hlen, &_eth);\n\t\t\tif (!eth)\n\t\t\t\tgoto out_bad;\n\t\t\tproto = eth->h_proto;\n\t\t\tnhoff += sizeof(*eth);\n\t\t}\n\t\tgoto again;\n\t}\n\tcase NEXTHDR_HOP:\n\tcase NEXTHDR_ROUTING:\n\tcase NEXTHDR_DEST: {\n\t\tu8 _opthdr[2], *opthdr;\n\n\t\tif (proto != htons(ETH_P_IPV6))\n\t\t\tbreak;\n\n\t\topthdr = __skb_header_pointer(skb, nhoff, sizeof(_opthdr),\n\t\t\t\t\t      data, hlen, &_opthdr);\n\t\tif (!opthdr)\n\t\t\tgoto out_bad;\n\n\t\tip_proto = opthdr[0];\n\t\tnhoff += (opthdr[1] + 1) << 3;\n\n\t\tgoto ip_proto_again;\n\t}\n\tcase IPPROTO_IPIP:\n\t\tproto = htons(ETH_P_IP);\n\t\tgoto ip;\n\tcase IPPROTO_IPV6:\n\t\tproto = htons(ETH_P_IPV6);\n\t\tgoto ipv6;\n\tcase IPPROTO_MPLS:\n\t\tproto = htons(ETH_P_MPLS_UC);\n\t\tgoto mpls;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (skb_flow_dissector_uses_key(flow_dissector,\n\t\t\t\t\tFLOW_DISSECTOR_KEY_PORTS)) {\n\t\tkey_ports = skb_flow_dissector_target(flow_dissector,\n\t\t\t\t\t\t      FLOW_DISSECTOR_KEY_PORTS,\n\t\t\t\t\t\t      target_container);\n\t\tkey_ports->ports = __skb_flow_get_ports(skb, nhoff, ip_proto,\n\t\t\t\t\t\t\tdata, hlen);\n\t}\n\nout_good:\n\tret = true;\n\nout_bad:\n\tkey_basic->n_proto = proto;\n\tkey_basic->ip_proto = ip_proto;\n\tkey_control->thoff = (u16)nhoff;\n\n\treturn ret;\n}",
        "description": "The `__skb_flow_dissect` function in the Linux kernel, prior to version 4.3, does not guarantee the initialization of certain protocol fields (`n_proto`, `ip_proto`, and `thoff`). This oversight enables remote attackers to exploit a single crafted MPLS packet to trigger a denial of service (system crash) or potentially execute arbitrary code.",
        "commit": "The vulnerability involves a function responsible for dissecting network flows, where instead of returning immediately upon encountering a parsing failure, it jumps to cleanup code. This approach ensures that even in the event of a failure, some protocol values are set in the key_control structure, although there may still be valid information in the key_tags that was established before the error occurred."
    },
    {
        "cwe": "CWE-189",
        "func_name": "torvalds/i915_gem_execbuffer2",
        "score": 0.7832946181297302,
        "func_before": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "func_after": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1 ||\n\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "description": "An integer overflow vulnerability exists within the `i915_gem_execbuffer2` function of the DRM subsystem in the Linux kernel prior to version 3.3.5 on 32-bit platforms. This flaw allows local users to trigger a denial of service through an out-of-bounds write or potentially cause other unspecified impacts via a specially crafted ioctl call.",
        "commit": "A vulnerability was identified in the i915 driver of the DRM subsystem within the Linux kernel, where a large `args->buffer_count` value provided by userspace through an ioctl call could cause an integer overflow on 32-bit systems. This overflow affects the calculation of the allocation size for temporary execution buffers, potentially leading to out-of-bounds memory access. This issue arose due to changes introduced in commit 8408c282, which attempted to optimize buffer allocation by first trying a normal large `kmalloc`."
    },
    {
        "cwe": "CWE-681",
        "func_name": "FreeRDP/update_recv_secondary_order",
        "score": 0.791766881942749,
        "func_before": "static BOOL update_recv_secondary_order(rdpUpdate* update, wStream* s, BYTE flags)\n{\n\tBOOL rc = FALSE;\n\tsize_t start, end, diff;\n\tBYTE orderType;\n\tUINT16 extraFlags;\n\tUINT16 orderLength;\n\trdpContext* context = update->context;\n\trdpSettings* settings = context->settings;\n\trdpSecondaryUpdate* secondary = update->secondary;\n\tconst char* name;\n\n\tif (Stream_GetRemainingLength(s) < 5)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) < 5\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, orderLength); /* orderLength (2 bytes) */\n\tStream_Read_UINT16(s, extraFlags);  /* extraFlags (2 bytes) */\n\tStream_Read_UINT8(s, orderType);    /* orderType (1 byte) */\n\tif (Stream_GetRemainingLength(s) < orderLength + 7U)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) %\" PRIuz \" < %\" PRIu16,\n\t\t           Stream_GetRemainingLength(s), orderLength + 7);\n\t\treturn FALSE;\n\t}\n\n\tstart = Stream_GetPosition(s);\n\tname = secondary_order_string(orderType);\n\tWLog_Print(update->log, WLOG_DEBUG, \"Secondary Drawing Order %s\", name);\n\n\tif (!check_secondary_order_supported(update->log, settings, orderType, name))\n\t\treturn FALSE;\n\n\tswitch (orderType)\n\t{\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED:\n\t\tcase ORDER_TYPE_CACHE_BITMAP_COMPRESSED:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_CACHE_BITMAP_COMPRESSED);\n\t\t\tCACHE_BITMAP_ORDER* order =\n\t\t\t    update_read_cache_bitmap_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmap, context, order);\n\t\t\t\tfree_cache_bitmap_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED_V2:\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V2:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_BITMAP_COMPRESSED_V2);\n\t\t\tCACHE_BITMAP_V2_ORDER* order =\n\t\t\t    update_read_cache_bitmap_v2_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV2, context, order);\n\t\t\t\tfree_cache_bitmap_v2_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V3:\n\t\t{\n\t\t\tCACHE_BITMAP_V3_ORDER* order = update_read_cache_bitmap_v3_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV3, context, order);\n\t\t\t\tfree_cache_bitmap_v3_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_COLOR_TABLE:\n\t\t{\n\t\t\tCACHE_COLOR_TABLE_ORDER* order =\n\t\t\t    update_read_cache_color_table_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheColorTable, context, order);\n\t\t\t\tfree_cache_color_table_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_GLYPH:\n\t\t{\n\t\t\tswitch (settings->GlyphSupportLevel)\n\t\t\t{\n\t\t\t\tcase GLYPH_SUPPORT_PARTIAL:\n\t\t\t\tcase GLYPH_SUPPORT_FULL:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_ORDER* order = update_read_cache_glyph_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyph, context, order);\n\t\t\t\t\t\tfree_cache_glyph_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_ENCODE:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_V2_ORDER* order =\n\t\t\t\t\t    update_read_cache_glyph_v2_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyphV2, context, order);\n\t\t\t\t\t\tfree_cache_glyph_v2_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_NONE:\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_BRUSH:\n\t\t\t/* [MS-RDPEGDI] 2.2.2.2.1.2.7 Cache Brush (CACHE_BRUSH_ORDER) */\n\t\t\t{\n\t\t\t\tCACHE_BRUSH_ORDER* order = update_read_cache_brush_order(update, s, extraFlags);\n\n\t\t\t\tif (order)\n\t\t\t\t{\n\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBrush, context, order);\n\t\t\t\t\tfree_cache_brush_order(context, order);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY ORDER %s not supported\", name);\n\t\t\tbreak;\n\t}\n\n\tif (!rc)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"SECONDARY ORDER %s failed\", name);\n\t}\n\n\tstart += orderLength + 7;\n\tend = Stream_GetPosition(s);\n\tif (start > end)\n\t{\n\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes too much\",\n\t\t           name, end - start);\n\t\treturn FALSE;\n\t}\n\tdiff = start - end;\n\tif (diff > 0)\n\t{\n\t\tWLog_Print(update->log, WLOG_DEBUG,\n\t\t           \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes short, skipping\", name, diff);\n\t\tStream_Seek(s, diff);\n\t}\n\treturn rc;\n}",
        "func_after": "static BOOL update_recv_secondary_order(rdpUpdate* update, wStream* s, BYTE flags)\n{\n\tBOOL rc = FALSE;\n\tsize_t start, end, diff;\n\tBYTE orderType;\n\tUINT16 extraFlags;\n\tUINT16 orderLength;\n\trdpContext* context = update->context;\n\trdpSettings* settings = context->settings;\n\trdpSecondaryUpdate* secondary = update->secondary;\n\tconst char* name;\n\n\tif (Stream_GetRemainingLength(s) < 5)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) < 5\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, orderLength); /* orderLength (2 bytes) */\n\tStream_Read_UINT16(s, extraFlags);  /* extraFlags (2 bytes) */\n\tStream_Read_UINT8(s, orderType);    /* orderType (1 byte) */\n\tif (Stream_GetRemainingLength(s) < orderLength + 7U)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) %\" PRIuz \" < %\" PRIu16,\n\t\t           Stream_GetRemainingLength(s), orderLength + 7);\n\t\treturn FALSE;\n\t}\n\n\tstart = Stream_GetPosition(s);\n\tname = secondary_order_string(orderType);\n\tWLog_Print(update->log, WLOG_DEBUG, \"Secondary Drawing Order %s\", name);\n\n\tif (!check_secondary_order_supported(update->log, settings, orderType, name))\n\t\treturn FALSE;\n\n\tswitch (orderType)\n\t{\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED:\n\t\tcase ORDER_TYPE_CACHE_BITMAP_COMPRESSED:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_CACHE_BITMAP_COMPRESSED);\n\t\t\tCACHE_BITMAP_ORDER* order =\n\t\t\t    update_read_cache_bitmap_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmap, context, order);\n\t\t\t\tfree_cache_bitmap_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED_V2:\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V2:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_BITMAP_COMPRESSED_V2);\n\t\t\tCACHE_BITMAP_V2_ORDER* order =\n\t\t\t    update_read_cache_bitmap_v2_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV2, context, order);\n\t\t\t\tfree_cache_bitmap_v2_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V3:\n\t\t{\n\t\t\tCACHE_BITMAP_V3_ORDER* order = update_read_cache_bitmap_v3_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV3, context, order);\n\t\t\t\tfree_cache_bitmap_v3_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_COLOR_TABLE:\n\t\t{\n\t\t\tCACHE_COLOR_TABLE_ORDER* order =\n\t\t\t    update_read_cache_color_table_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheColorTable, context, order);\n\t\t\t\tfree_cache_color_table_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_GLYPH:\n\t\t{\n\t\t\tswitch (settings->GlyphSupportLevel)\n\t\t\t{\n\t\t\t\tcase GLYPH_SUPPORT_PARTIAL:\n\t\t\t\tcase GLYPH_SUPPORT_FULL:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_ORDER* order = update_read_cache_glyph_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyph, context, order);\n\t\t\t\t\t\tfree_cache_glyph_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_ENCODE:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_V2_ORDER* order =\n\t\t\t\t\t    update_read_cache_glyph_v2_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyphV2, context, order);\n\t\t\t\t\t\tfree_cache_glyph_v2_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_NONE:\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_BRUSH:\n\t\t\t/* [MS-RDPEGDI] 2.2.2.2.1.2.7 Cache Brush (CACHE_BRUSH_ORDER) */\n\t\t\t{\n\t\t\t\tCACHE_BRUSH_ORDER* order = update_read_cache_brush_order(update, s, extraFlags);\n\n\t\t\t\tif (order)\n\t\t\t\t{\n\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBrush, context, order);\n\t\t\t\t\tfree_cache_brush_order(context, order);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY ORDER %s not supported\", name);\n\t\t\tbreak;\n\t}\n\n\tif (!rc)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"SECONDARY ORDER %s failed\", name);\n\t}\n\n\tstart += orderLength + 7;\n\tend = Stream_GetPosition(s);\n\tif (start > end)\n\t{\n\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes too much\",\n\t\t           name, end - start);\n\t\treturn FALSE;\n\t}\n\tdiff = end - start;\n\tif (diff > 0)\n\t{\n\t\tWLog_Print(update->log, WLOG_DEBUG,\n\t\t           \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes short, skipping\", name, diff);\n\t\tif (!Stream_SafeSeek(s, diff))\n\t\t\treturn FALSE;\n\t}\n\treturn rc;\n}",
        "description": "In FreeRDP versions prior to 2.1.2, there exists an integer casting vulnerability within the `update_recv_secondary_order` function. This vulnerability affects all clients configured with both `+glyph-cache` and `relax-order-checks` options. This issue has been addressed in version 2.1.2.",
        "commit": "An out-of-bounds (OOB) read vulnerability was addressed in the `update_recv_secondary_order` function. This issue was identified and reported by @antonio-morales."
    },
    {
        "cwe": "CWE-704",
        "func_name": "torvalds/sctp_make_strreset_req",
        "score": 0.7939796447753906,
        "func_before": "struct sctp_chunk *sctp_make_strreset_req(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__u16 stream_num, __be16 *stream_list,\n\t\t\t\t\tbool out, bool in)\n{\n\t__u16 stream_len = stream_num * sizeof(__u16);\n\tstruct sctp_strreset_outreq outreq;\n\tstruct sctp_strreset_inreq inreq;\n\tstruct sctp_chunk *retval;\n\t__u16 outlen, inlen;\n\n\toutlen = (sizeof(outreq) + stream_len) * out;\n\tinlen = (sizeof(inreq) + stream_len) * in;\n\n\tretval = sctp_make_reconf(asoc, outlen + inlen);\n\tif (!retval)\n\t\treturn NULL;\n\n\tif (outlen) {\n\t\toutreq.param_hdr.type = SCTP_PARAM_RESET_OUT_REQUEST;\n\t\toutreq.param_hdr.length = htons(outlen);\n\t\toutreq.request_seq = htonl(asoc->strreset_outseq);\n\t\toutreq.response_seq = htonl(asoc->strreset_inseq - 1);\n\t\toutreq.send_reset_at_tsn = htonl(asoc->next_tsn - 1);\n\n\t\tsctp_addto_chunk(retval, sizeof(outreq), &outreq);\n\n\t\tif (stream_len)\n\t\t\tsctp_addto_chunk(retval, stream_len, stream_list);\n\t}\n\n\tif (inlen) {\n\t\tinreq.param_hdr.type = SCTP_PARAM_RESET_IN_REQUEST;\n\t\tinreq.param_hdr.length = htons(inlen);\n\t\tinreq.request_seq = htonl(asoc->strreset_outseq + out);\n\n\t\tsctp_addto_chunk(retval, sizeof(inreq), &inreq);\n\n\t\tif (stream_len)\n\t\t\tsctp_addto_chunk(retval, stream_len, stream_list);\n\t}\n\n\treturn retval;\n}",
        "func_after": "struct sctp_chunk *sctp_make_strreset_req(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__u16 stream_num, __be16 *stream_list,\n\t\t\t\t\tbool out, bool in)\n{\n\t__u16 stream_len = stream_num * sizeof(__u16);\n\tstruct sctp_strreset_outreq outreq;\n\tstruct sctp_strreset_inreq inreq;\n\tstruct sctp_chunk *retval;\n\t__u16 outlen, inlen;\n\n\toutlen = (sizeof(outreq) + stream_len) * out;\n\tinlen = (sizeof(inreq) + stream_len) * in;\n\n\tretval = sctp_make_reconf(asoc, SCTP_PAD4(outlen) + SCTP_PAD4(inlen));\n\tif (!retval)\n\t\treturn NULL;\n\n\tif (outlen) {\n\t\toutreq.param_hdr.type = SCTP_PARAM_RESET_OUT_REQUEST;\n\t\toutreq.param_hdr.length = htons(outlen);\n\t\toutreq.request_seq = htonl(asoc->strreset_outseq);\n\t\toutreq.response_seq = htonl(asoc->strreset_inseq - 1);\n\t\toutreq.send_reset_at_tsn = htonl(asoc->next_tsn - 1);\n\n\t\tsctp_addto_chunk(retval, sizeof(outreq), &outreq);\n\n\t\tif (stream_len)\n\t\t\tsctp_addto_chunk(retval, stream_len, stream_list);\n\t}\n\n\tif (inlen) {\n\t\tinreq.param_hdr.type = SCTP_PARAM_RESET_IN_REQUEST;\n\t\tinreq.param_hdr.length = htons(inlen);\n\t\tinreq.request_seq = htonl(asoc->strreset_outseq + out);\n\n\t\tsctp_addto_chunk(retval, sizeof(inreq), &inreq);\n\n\t\tif (stream_len)\n\t\t\tsctp_addto_chunk(retval, stream_len, stream_list);\n\t}\n\n\treturn retval;\n}",
        "description": "A flaw exists in the SCTP network protocol within the Linux kernel, where the `sctp_make_strreset_req` function may allocate insufficient buffer space. When an attempt is made to utilize more buffer than what has been allocated, it triggers a `BUG_ON` condition, resulting in a denial of service (DOS) for local users.",
        "commit": "The `sctp_make_strreset_req()` function repeatedly invokes `sctp_addto_chunk()`, which inherently accounts for padding with each call. Although `inreq` and `outreq` are already 4-byte aligned, the payload is not. The use of `SCTP_PAD4(a + b)` (implicitly done by `_sctp_make_chunk()`) differs from `SCTP_PAD4(a) + SCTP_PAD4(b)`, leading to insufficient padding calculation. This discrepancy potentially results in attempting to use more buffer space than allocated, triggering a `BUG_ON`."
    },
    {
        "cwe": "CWE-668",
        "func_name": "xen-project/sh_guess_wrmap",
        "score": 0.7652614712715149,
        "func_before": "static int sh_guess_wrmap(struct vcpu *v, unsigned long vaddr, mfn_t gmfn)\n/* Look up this vaddr in the current shadow and see if it's a writeable\n * mapping of this gmfn.  If so, remove it.  Returns 1 if it worked. */\n{\n    struct domain *d = v->domain;\n    shadow_l1e_t sl1e, *sl1p;\n    shadow_l2e_t *sl2p;\n    shadow_l3e_t *sl3p;\n#if SHADOW_PAGING_LEVELS >= 4\n    shadow_l4e_t *sl4p;\n#endif\n    mfn_t sl1mfn;\n    int r;\n\n    /* Carefully look in the shadow linear map for the l1e we expect */\n#if SHADOW_PAGING_LEVELS >= 4\n    sl4p = sh_linear_l4_table(v) + shadow_l4_linear_offset(vaddr);\n    if ( !(shadow_l4e_get_flags(*sl4p) & _PAGE_PRESENT) )\n        return 0;\n    sl3p = sh_linear_l3_table(v) + shadow_l3_linear_offset(vaddr);\n    if ( !(shadow_l3e_get_flags(*sl3p) & _PAGE_PRESENT) )\n        return 0;\n#else /* SHADOW_PAGING_LEVELS == 3 */\n    sl3p = ((shadow_l3e_t *) v->arch.paging.shadow.l3table)\n        + shadow_l3_linear_offset(vaddr);\n    if ( !(shadow_l3e_get_flags(*sl3p) & _PAGE_PRESENT) )\n        return 0;\n#endif\n    sl2p = sh_linear_l2_table(v) + shadow_l2_linear_offset(vaddr);\n    if ( !(shadow_l2e_get_flags(*sl2p) & _PAGE_PRESENT) )\n        return 0;\n    sl1p = sh_linear_l1_table(v) + shadow_l1_linear_offset(vaddr);\n    sl1e = *sl1p;\n    if ( ((shadow_l1e_get_flags(sl1e) & (_PAGE_PRESENT|_PAGE_RW))\n          != (_PAGE_PRESENT|_PAGE_RW))\n         || (mfn_x(shadow_l1e_get_mfn(sl1e)) != mfn_x(gmfn)) )\n        return 0;\n\n    /* Found it!  Need to remove its write permissions. */\n    sl1mfn = shadow_l2e_get_mfn(*sl2p);\n    sl1e = shadow_l1e_remove_flags(sl1e, _PAGE_RW);\n    r = shadow_set_l1e(d, sl1p, sl1e, p2m_ram_rw, sl1mfn);\n    if ( r & SHADOW_SET_ERROR ) {\n        /* Can only currently happen if we found a grant-mapped\n         * page.  Just make the guess fail. */\n        return 0;\n    }\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_WRMAP_GUESS_FOUND);\n    return 1;\n}",
        "func_after": "static int sh_guess_wrmap(struct vcpu *v, unsigned long vaddr, mfn_t gmfn)\n/* Look up this vaddr in the current shadow and see if it's a writeable\n * mapping of this gmfn.  If so, remove it.  Returns 1 if it worked. */\n{\n    struct domain *d = v->domain;\n    shadow_l1e_t sl1e, *sl1p;\n    shadow_l2e_t *sl2p;\n    shadow_l3e_t *sl3p;\n#if SHADOW_PAGING_LEVELS >= 4\n    shadow_l4e_t *sl4p;\n#endif\n    mfn_t sl1mfn;\n    int r;\n\n    /* Carefully look in the shadow linear map for the l1e we expect */\n#if SHADOW_PAGING_LEVELS >= 4\n    /* Is a shadow linear map is installed in the first place? */\n    sl4p  = v->arch.paging.shadow.guest_vtable;\n    sl4p += shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START);\n    if ( !(shadow_l4e_get_flags(*sl4p) & _PAGE_PRESENT) )\n        return 0;\n    sl4p = sh_linear_l4_table(v) + shadow_l4_linear_offset(vaddr);\n    if ( !(shadow_l4e_get_flags(*sl4p) & _PAGE_PRESENT) )\n        return 0;\n    sl3p = sh_linear_l3_table(v) + shadow_l3_linear_offset(vaddr);\n    if ( !(shadow_l3e_get_flags(*sl3p) & _PAGE_PRESENT) )\n        return 0;\n#else /* SHADOW_PAGING_LEVELS == 3 */\n    sl3p = ((shadow_l3e_t *) v->arch.paging.shadow.l3table)\n        + shadow_l3_linear_offset(vaddr);\n    if ( !(shadow_l3e_get_flags(*sl3p) & _PAGE_PRESENT) )\n        return 0;\n#endif\n    sl2p = sh_linear_l2_table(v) + shadow_l2_linear_offset(vaddr);\n    if ( !(shadow_l2e_get_flags(*sl2p) & _PAGE_PRESENT) )\n        return 0;\n    sl1p = sh_linear_l1_table(v) + shadow_l1_linear_offset(vaddr);\n    sl1e = *sl1p;\n    if ( ((shadow_l1e_get_flags(sl1e) & (_PAGE_PRESENT|_PAGE_RW))\n          != (_PAGE_PRESENT|_PAGE_RW))\n         || (mfn_x(shadow_l1e_get_mfn(sl1e)) != mfn_x(gmfn)) )\n        return 0;\n\n    /* Found it!  Need to remove its write permissions. */\n    sl1mfn = shadow_l2e_get_mfn(*sl2p);\n    sl1e = shadow_l1e_remove_flags(sl1e, _PAGE_RW);\n    r = shadow_set_l1e(d, sl1p, sl1e, p2m_ram_rw, sl1mfn);\n    if ( r & SHADOW_SET_ERROR ) {\n        /* Can only currently happen if we found a grant-mapped\n         * page.  Just make the guess fail. */\n        return 0;\n    }\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_WRMAP_GUESS_FOUND);\n    return 1;\n}",
        "description": "An issue was discovered in Xen versions up to 4.9.x, where x86 HVM guest OS users could potentially cause a denial of service (hypervisor crash) or escalate privileges due to improper handling of self-linear shadow mappings in translated guests.",
        "commit": "A vulnerability was identified in the x86/shadow component of the Xen hypervisor, specifically within the `sh_guess_wrmap()` function. The fix for XSA-243/CVE-2017-15592 introduced a change in behavior for `sh_guest_wrmap()`, which now needs to handle cases where no shadow linear mapping is present. The issue arises because `guest_vtable` maps the guest's page table rather than Xen's page table, leading to incorrect checks for the presence of a shadow linear slot. As a result, a shadow HVM vCPU switching to 4-level paging mode with an L4 page table containing a mapping that aliases Xen's `SH_LINEAR_PT_VIRT_START` can pass the safety check when it should fail. Consequently, Xen encounters a page fault due to the missing mapping. This problem is part of XSA-243."
    }
]