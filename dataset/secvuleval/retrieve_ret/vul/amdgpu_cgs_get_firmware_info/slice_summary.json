[
    {
        "cwe": "CWE-1284",
        "func_name": "Samsung/crypto_bignum_allocate",
        "score": 0.7881394028663635,
        "func_before": "struct bignum *crypto_bignum_allocate(size_t size_bits)\n{\n\tsize_t sz = mpa_StaticVarSizeInU32(size_bits) *\tsizeof(uint32_t);\n\tstruct mpa_numbase_struct *bn = calloc(1, sz);\n\n\tif (!bn)\n\t\treturn NULL;\n\tbn->alloc = sz - MPA_NUMBASE_METADATA_SIZE_IN_U32 * sizeof(uint32_t);\n\treturn (struct bignum *)bn;\n}",
        "func_after": "struct bignum *crypto_bignum_allocate(size_t size_bits)\n{\n\tsize_t sz = mpa_StaticVarSizeInU32(size_bits) *\tsizeof(uint32_t);\n\t// struct mpa_numbase_struct *bn = calloc(1, sz);\n\tstruct mpa_numbase_struct *bn= TEE_Malloc(sz, TEE_MALLOC_FILL_ZERO);\n\tif (!bn)\n\t\treturn NULL;\n\tbn->alloc = sz - MPA_NUMBASE_METADATA_SIZE_IN_U32 * sizeof(uint32_t);\n\treturn (struct bignum *)bn;\n}",
        "description": "The function `tee_obj_free` in Samsung mTower through version 0.3.0 enables a trusted application to cause a Denial of Service (DoS) by calling the function `TEE_AllocateOperation` with a disrupted heap layout, which is associated with `utee_cryp_obj_alloc`.",
        "commit": "A vulnerability has been addressed in a software system, specifically identified by CVE-2022-40761."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.8151554465293884,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    },
    {
        "cwe": "CWE-203",
        "func_name": "barebox/check_passwd",
        "score": 0.8057494759559631,
        "func_before": "static int check_passwd(unsigned char *passwd, size_t length)\n{\n\tstruct digest *d = NULL;\n\tunsigned char *passwd1_sum;\n\tunsigned char *passwd2_sum;\n\tint ret = 0;\n\tint hash_len;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\thash_len = PBKDF2_LENGTH;\n\t} else {\n\t\td = digest_alloc(PASSWD_SUM);\n\t\tif (!d) {\n\t\t\tpr_err(\"No such digest: %s\\n\",\n\t\t\t       PASSWD_SUM ? PASSWD_SUM : \"NULL\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\thash_len = digest_length(d);\n\t}\n\n\tpasswd1_sum = calloc(hash_len * 2, sizeof(unsigned char));\n\tif (!passwd1_sum)\n\t\treturn -ENOMEM;\n\n\tpasswd2_sum = passwd1_sum + hash_len;\n\n\tif (is_passwd_env_enable())\n\t\tret = read_env_passwd(passwd2_sum, hash_len);\n\telse if (is_passwd_default_enable())\n\t\tret = read_default_passwd(passwd2_sum, hash_len);\n\telse\n\t\tret = -EINVAL;\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\tchar *key = passwd2_sum + PBKDF2_SALT_LEN;\n\t\tchar *salt = passwd2_sum;\n\t\tint keylen = PBKDF2_LENGTH - PBKDF2_SALT_LEN;\n\n\t\tret = pkcs5_pbkdf2_hmac_sha1(passwd, length, salt,\n\t\t\tPBKDF2_SALT_LEN, PBKDF2_COUNT, keylen, passwd1_sum);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (strncmp(passwd1_sum, key, keylen) == 0)\n\t\t\tret = 1;\n\t} else {\n\t\tret = digest_digest(d, passwd, length, passwd1_sum);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (strncmp(passwd1_sum, passwd2_sum, hash_len) == 0)\n\t\t\tret = 1;\n\t}\n\nerr:\n\tfree(passwd1_sum);\n\tdigest_free(d);\n\n\treturn ret;\n}",
        "func_after": "static int check_passwd(unsigned char *passwd, size_t length)\n{\n\tstruct digest *d = NULL;\n\tunsigned char *passwd1_sum;\n\tunsigned char *passwd2_sum;\n\tint ret = 0;\n\tint hash_len;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\thash_len = PBKDF2_LENGTH;\n\t} else {\n\t\td = digest_alloc(PASSWD_SUM);\n\t\tif (!d) {\n\t\t\tpr_err(\"No such digest: %s\\n\",\n\t\t\t       PASSWD_SUM ? PASSWD_SUM : \"NULL\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\thash_len = digest_length(d);\n\t}\n\n\tpasswd1_sum = calloc(hash_len * 2, sizeof(unsigned char));\n\tif (!passwd1_sum)\n\t\treturn -ENOMEM;\n\n\tpasswd2_sum = passwd1_sum + hash_len;\n\n\tif (is_passwd_env_enable())\n\t\tret = read_env_passwd(passwd2_sum, hash_len);\n\telse if (is_passwd_default_enable())\n\t\tret = read_default_passwd(passwd2_sum, hash_len);\n\telse\n\t\tret = -EINVAL;\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\tchar *key = passwd2_sum + PBKDF2_SALT_LEN;\n\t\tchar *salt = passwd2_sum;\n\t\tint keylen = PBKDF2_LENGTH - PBKDF2_SALT_LEN;\n\n\t\tret = pkcs5_pbkdf2_hmac_sha1(passwd, length, salt,\n\t\t\tPBKDF2_SALT_LEN, PBKDF2_COUNT, keylen, passwd1_sum);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (!crypto_memneq(passwd1_sum, key, keylen))\n\t\t\tret = 1;\n\t} else {\n\t\tret = digest_digest(d, passwd, length, passwd1_sum);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (!crypto_memneq(passwd1_sum, passwd2_sum, hash_len))\n\t\t\tret = 1;\n\t}\n\nerr:\n\tfree(passwd1_sum);\n\tdigest_free(d);\n\n\treturn ret;\n}",
        "description": "The Pengutronix barebox, up to version 2021.07.0, suffers from a vulnerability where timing information is leaked due to the use of `strncmp` during hash comparisons.",
        "commit": "Cryptographic verifications should employ a time-constant comparison method to prevent attackers from inferring secret information by observing system behavior. Therefore, it is recommended to use functions like `crypto_memneq()` instead of `memcmp()` for comparing sensitive data such as password hashes."
    },
    {
        "cwe": "CWE-674",
        "func_name": "the-tcpdump-group/bgp_update_print",
        "score": 0.8107725381851196,
        "func_before": "static void\nbgp_update_print(netdissect_options *ndo,\n                 const u_char *dat, int length)\n{\n\tstruct bgp bgp;\n\tconst u_char *p;\n\tint withdrawn_routes_len;\n\tint len;\n\tint i;\n\n\tND_TCHECK2(dat[0], BGP_SIZE);\n\tif (length < BGP_SIZE)\n\t\tgoto trunc;\n\tmemcpy(&bgp, dat, BGP_SIZE);\n\tp = dat + BGP_SIZE;\t/*XXX*/\n\tlength -= BGP_SIZE;\n\n\t/* Unfeasible routes */\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\twithdrawn_routes_len = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\tif (withdrawn_routes_len) {\n\t\t/*\n\t\t * Without keeping state from the original NLRI message,\n\t\t * it's not possible to tell if this a v4 or v6 route,\n\t\t * so only try to decode it if we're not v6 enabled.\n\t         */\n\t\tND_TCHECK2(p[0], withdrawn_routes_len);\n\t\tif (length < withdrawn_routes_len)\n\t\t\tgoto trunc;\n\t\tND_PRINT((ndo, \"\\n\\t  Withdrawn routes: %d bytes\", withdrawn_routes_len));\n\t\tp += withdrawn_routes_len;\n\t\tlength -= withdrawn_routes_len;\n\t}\n\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\tlen = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\n        if (withdrawn_routes_len == 0 && len == 0 && length == 0) {\n            /* No withdrawn routes, no path attributes, no NLRI */\n            ND_PRINT((ndo, \"\\n\\t  End-of-Rib Marker (empty NLRI)\"));\n            return;\n        }\n\n\tif (len) {\n\t\t/* do something more useful!*/\n\t\twhile (len) {\n\t\t\tint aflags, atype, alenlen, alen;\n\n\t\t\tND_TCHECK2(p[0], 2);\n\t\t\tif (len < 2)\n\t\t\t    goto trunc;\n\t\t\tif (length < 2)\n\t\t\t    goto trunc;\n\t\t\taflags = *p;\n\t\t\tatype = *(p + 1);\n\t\t\tp += 2;\n\t\t\tlen -= 2;\n\t\t\tlength -= 2;\n\t\t\talenlen = bgp_attr_lenlen(aflags, p);\n\t\t\tND_TCHECK2(p[0], alenlen);\n\t\t\tif (len < alenlen)\n\t\t\t    goto trunc;\n\t\t\tif (length < alenlen)\n\t\t\t    goto trunc;\n\t\t\talen = bgp_attr_len(aflags, p);\n\t\t\tp += alenlen;\n\t\t\tlen -= alenlen;\n\t\t\tlength -= alenlen;\n\n\t\t\tND_PRINT((ndo, \"\\n\\t  %s (%u), length: %u\",\n                              tok2str(bgp_attr_values, \"Unknown Attribute\",\n\t\t\t\t\t atype),\n                              atype,\n                              alen));\n\n\t\t\tif (aflags) {\n\t\t\t\tND_PRINT((ndo, \", Flags [%s%s%s%s\",\n\t\t\t\t\taflags & 0x80 ? \"O\" : \"\",\n\t\t\t\t\taflags & 0x40 ? \"T\" : \"\",\n\t\t\t\t\taflags & 0x20 ? \"P\" : \"\",\n\t\t\t\t\taflags & 0x10 ? \"E\" : \"\"));\n\t\t\t\tif (aflags & 0xf)\n\t\t\t\t\tND_PRINT((ndo, \"+%x\", aflags & 0xf));\n\t\t\t\tND_PRINT((ndo, \"]: \"));\n\t\t\t}\n\t\t\tif (len < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (length < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (!bgp_attr_print(ndo, atype, p, alen))\n\t\t\t\tgoto trunc;\n\t\t\tp += alen;\n\t\t\tlen -= alen;\n\t\t\tlength -= alen;\n\t\t}\n\t}\n\n\tif (length) {\n\t\t/*\n\t\t * XXX - what if they're using the \"Advertisement of\n\t\t * Multiple Paths in BGP\" feature:\n\t\t *\n\t\t * https://datatracker.ietf.org/doc/draft-ietf-idr-add-paths/\n\t\t *\n\t\t * http://tools.ietf.org/html/draft-ietf-idr-add-paths-06\n\t\t */\n\t\tND_PRINT((ndo, \"\\n\\t  Updated routes:\"));\n\t\twhile (length) {\n\t\t\tchar buf[MAXHOSTNAMELEN + 100];\n\t\t\ti = decode_prefix4(ndo, p, length, buf, sizeof(buf));\n\t\t\tif (i == -1) {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    (illegal prefix length)\"));\n\t\t\t\tbreak;\n\t\t\t} else if (i == -2)\n\t\t\t\tgoto trunc;\n\t\t\telse if (i == -3)\n\t\t\t\tgoto trunc; /* bytes left, but not enough */\n\t\t\telse {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    %s\", buf));\n\t\t\t\tp += i;\n\t\t\t\tlength -= i;\n\t\t\t}\n\t\t}\n\t}\n\treturn;\ntrunc:\n\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "func_after": "static void\nbgp_update_print(netdissect_options *ndo,\n                 const u_char *dat, int length)\n{\n\tstruct bgp bgp;\n\tconst u_char *p;\n\tint withdrawn_routes_len;\n\tint len;\n\tint i;\n\n\tND_TCHECK2(dat[0], BGP_SIZE);\n\tif (length < BGP_SIZE)\n\t\tgoto trunc;\n\tmemcpy(&bgp, dat, BGP_SIZE);\n\tp = dat + BGP_SIZE;\t/*XXX*/\n\tlength -= BGP_SIZE;\n\n\t/* Unfeasible routes */\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\twithdrawn_routes_len = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\tif (withdrawn_routes_len) {\n\t\t/*\n\t\t * Without keeping state from the original NLRI message,\n\t\t * it's not possible to tell if this a v4 or v6 route,\n\t\t * so only try to decode it if we're not v6 enabled.\n\t         */\n\t\tND_TCHECK2(p[0], withdrawn_routes_len);\n\t\tif (length < withdrawn_routes_len)\n\t\t\tgoto trunc;\n\t\tND_PRINT((ndo, \"\\n\\t  Withdrawn routes: %d bytes\", withdrawn_routes_len));\n\t\tp += withdrawn_routes_len;\n\t\tlength -= withdrawn_routes_len;\n\t}\n\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\tlen = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\n        if (withdrawn_routes_len == 0 && len == 0 && length == 0) {\n            /* No withdrawn routes, no path attributes, no NLRI */\n            ND_PRINT((ndo, \"\\n\\t  End-of-Rib Marker (empty NLRI)\"));\n            return;\n        }\n\n\tif (len) {\n\t\t/* do something more useful!*/\n\t\twhile (len) {\n\t\t\tint aflags, atype, alenlen, alen;\n\n\t\t\tND_TCHECK2(p[0], 2);\n\t\t\tif (len < 2)\n\t\t\t    goto trunc;\n\t\t\tif (length < 2)\n\t\t\t    goto trunc;\n\t\t\taflags = *p;\n\t\t\tatype = *(p + 1);\n\t\t\tp += 2;\n\t\t\tlen -= 2;\n\t\t\tlength -= 2;\n\t\t\talenlen = bgp_attr_lenlen(aflags, p);\n\t\t\tND_TCHECK2(p[0], alenlen);\n\t\t\tif (len < alenlen)\n\t\t\t    goto trunc;\n\t\t\tif (length < alenlen)\n\t\t\t    goto trunc;\n\t\t\talen = bgp_attr_len(aflags, p);\n\t\t\tp += alenlen;\n\t\t\tlen -= alenlen;\n\t\t\tlength -= alenlen;\n\n\t\t\tND_PRINT((ndo, \"\\n\\t  %s (%u), length: %u\",\n                              tok2str(bgp_attr_values, \"Unknown Attribute\",\n\t\t\t\t\t atype),\n                              atype,\n                              alen));\n\n\t\t\tif (aflags) {\n\t\t\t\tND_PRINT((ndo, \", Flags [%s%s%s%s\",\n\t\t\t\t\taflags & 0x80 ? \"O\" : \"\",\n\t\t\t\t\taflags & 0x40 ? \"T\" : \"\",\n\t\t\t\t\taflags & 0x20 ? \"P\" : \"\",\n\t\t\t\t\taflags & 0x10 ? \"E\" : \"\"));\n\t\t\t\tif (aflags & 0xf)\n\t\t\t\t\tND_PRINT((ndo, \"+%x\", aflags & 0xf));\n\t\t\t\tND_PRINT((ndo, \"]: \"));\n\t\t\t}\n\t\t\tif (len < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (length < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (!bgp_attr_print(ndo, atype, p, alen, 0))\n\t\t\t\tgoto trunc;\n\t\t\tp += alen;\n\t\t\tlen -= alen;\n\t\t\tlength -= alen;\n\t\t}\n\t}\n\n\tif (length) {\n\t\t/*\n\t\t * XXX - what if they're using the \"Advertisement of\n\t\t * Multiple Paths in BGP\" feature:\n\t\t *\n\t\t * https://datatracker.ietf.org/doc/draft-ietf-idr-add-paths/\n\t\t *\n\t\t * http://tools.ietf.org/html/draft-ietf-idr-add-paths-06\n\t\t */\n\t\tND_PRINT((ndo, \"\\n\\t  Updated routes:\"));\n\t\twhile (length) {\n\t\t\tchar buf[MAXHOSTNAMELEN + 100];\n\t\t\ti = decode_prefix4(ndo, p, length, buf, sizeof(buf));\n\t\t\tif (i == -1) {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    (illegal prefix length)\"));\n\t\t\t\tbreak;\n\t\t\t} else if (i == -2)\n\t\t\t\tgoto trunc;\n\t\t\telse if (i == -3)\n\t\t\t\tgoto trunc; /* bytes left, but not enough */\n\t\t\telse {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    %s\", buf));\n\t\t\t\tp += i;\n\t\t\t\tlength -= i;\n\t\t\t}\n\t\t}\n\t}\n\treturn;\ntrunc:\n\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "description": "The BGP parser in tcpdump, prior to version 4.9.3, suffers from stack consumption due to unlimited recursion within the `bgp_attr_print()` function.",
        "commit": "A function related to BGP attribute printing within the Linux kernel, specifically in version 4.9.3, was modified to prevent stack exhaustion. This was achieved by enforcing a recursion limit on the `bgp_attr_print()` function. The vulnerability was identified through a code audit conducted by Include Security under the Mozilla SOS program in 2018."
    },
    {
        "cwe": "CWE-770",
        "func_name": "binutils-gdb/_bfd_elf_slurp_version_tables",
        "score": 0.808039665222168,
        "func_before": "bfd_boolean\n_bfd_elf_slurp_version_tables (bfd *abfd, bfd_boolean default_imported_symver)\n{\n  bfd_byte *contents = NULL;\n  unsigned int freeidx = 0;\n\n  if (elf_dynverref (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verneed *everneed;\n      Elf_Internal_Verneed *iverneed;\n      unsigned int i;\n      bfd_byte *contents_end;\n\n      hdr = &elf_tdata (abfd)->dynverref_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verneed))\n\t{\nerror_return_bad_verref:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_r invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\nerror_return_verref:\n\t  elf_tdata (abfd)->verref = NULL;\n\t  elf_tdata (abfd)->cverrefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verref;\n\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verref;\n\n      elf_tdata (abfd)->verref = (Elf_Internal_Verneed *)\n\tbfd_zalloc2 (abfd, hdr->sh_info, sizeof (Elf_Internal_Verneed));\n\n      if (elf_tdata (abfd)->verref == NULL)\n\tgoto error_return_verref;\n\n      BFD_ASSERT (sizeof (Elf_External_Verneed)\n\t\t  == sizeof (Elf_External_Vernaux));\n      contents_end = contents + hdr->sh_size - sizeof (Elf_External_Verneed);\n      everneed = (Elf_External_Verneed *) contents;\n      iverneed = elf_tdata (abfd)->verref;\n      for (i = 0; i < hdr->sh_info; i++, iverneed++)\n\t{\n\t  Elf_External_Vernaux *evernaux;\n\t  Elf_Internal_Vernaux *ivernaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verneed_in (abfd, everneed, iverneed);\n\n\t  iverneed->vn_bfd = abfd;\n\n\t  iverneed->vn_filename =\n\t    bfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t     iverneed->vn_file);\n\t  if (iverneed->vn_filename == NULL)\n\t    goto error_return_bad_verref;\n\n\t  if (iverneed->vn_cnt == 0)\n\t    iverneed->vn_auxptr = NULL;\n\t  else\n\t    {\n\t      iverneed->vn_auxptr = (struct elf_internal_vernaux *)\n                  bfd_alloc2 (abfd, iverneed->vn_cnt,\n                              sizeof (Elf_Internal_Vernaux));\n\t      if (iverneed->vn_auxptr == NULL)\n\t\tgoto error_return_verref;\n\t    }\n\n\t  if (iverneed->vn_aux\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  evernaux = ((Elf_External_Vernaux *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_aux));\n\t  ivernaux = iverneed->vn_auxptr;\n\t  for (j = 0; j < iverneed->vn_cnt; j++, ivernaux++)\n\t    {\n\t      _bfd_elf_swap_vernaux_in (abfd, evernaux, ivernaux);\n\n\t      ivernaux->vna_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t ivernaux->vna_name);\n\t      if (ivernaux->vna_nodename == NULL)\n\t\tgoto error_return_bad_verref;\n\n\t      if (ivernaux->vna_other > freeidx)\n\t\tfreeidx = ivernaux->vna_other;\n\n\t      ivernaux->vna_nextptr = NULL;\n\t      if (ivernaux->vna_next == 0)\n\t\t{\n\t\t  iverneed->vn_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverneed->vn_cnt)\n\t\tivernaux->vna_nextptr = ivernaux + 1;\n\n\t      if (ivernaux->vna_next\n\t\t  > (size_t) (contents_end - (bfd_byte *) evernaux))\n\t\tgoto error_return_bad_verref;\n\n\t      evernaux = ((Elf_External_Vernaux *)\n\t\t\t  ((bfd_byte *) evernaux + ivernaux->vna_next));\n\t    }\n\n\t  iverneed->vn_nextref = NULL;\n\t  if (iverneed->vn_next == 0)\n\t    break;\n\t  if (i + 1 < hdr->sh_info)\n\t    iverneed->vn_nextref = iverneed + 1;\n\n\t  if (iverneed->vn_next\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  everneed = ((Elf_External_Verneed *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_next));\n\t}\n      elf_tdata (abfd)->cverrefs = i;\n\n      free (contents);\n      contents = NULL;\n    }\n\n  if (elf_dynverdef (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verdef *everdef;\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdef *iverdefarr;\n      Elf_Internal_Verdef iverdefmem;\n      unsigned int i;\n      unsigned int maxidx;\n      bfd_byte *contents_end_def, *contents_end_aux;\n\n      hdr = &elf_tdata (abfd)->dynverdef_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verdef))\n\t{\n\terror_return_bad_verdef:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_d invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\n\terror_return_verdef:\n\t  elf_tdata (abfd)->verdef = NULL;\n\t  elf_tdata (abfd)->cverdefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verdef;\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verdef;\n\n      BFD_ASSERT (sizeof (Elf_External_Verdef)\n\t\t  >= sizeof (Elf_External_Verdaux));\n      contents_end_def = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdef);\n      contents_end_aux = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdaux);\n\n      /* We know the number of entries in the section but not the maximum\n\t index.  Therefore we have to run through all entries and find\n\t the maximum.  */\n      everdef = (Elf_External_Verdef *) contents;\n      maxidx = 0;\n      for (i = 0; i < hdr->sh_info; ++i)\n\t{\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) == 0)\n\t    goto error_return_bad_verdef;\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) > maxidx)\n\t    maxidx = iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION);\n\n\t  if (iverdefmem.vd_next == 0)\n\t    break;\n\n\t  if (iverdefmem.vd_next\n\t      > (size_t) (contents_end_def - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdefmem.vd_next));\n\t}\n\n      if (default_imported_symver)\n\t{\n\t  if (freeidx > maxidx)\n\t    maxidx = ++freeidx;\n\t  else\n\t    freeidx = ++maxidx;\n\t}\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n\tbfd_zalloc2 (abfd, maxidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return_verdef;\n\n      elf_tdata (abfd)->cverdefs = maxidx;\n\n      everdef = (Elf_External_Verdef *) contents;\n      iverdefarr = elf_tdata (abfd)->verdef;\n      for (i = 0; i < hdr->sh_info; i++)\n\t{\n\t  Elf_External_Verdaux *everdaux;\n\t  Elf_Internal_Verdaux *iverdaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & VERSYM_VERSION) == 0)\n\t    goto error_return_bad_verdef;\n\n\t  iverdef = &iverdefarr[(iverdefmem.vd_ndx & VERSYM_VERSION) - 1];\n\t  memcpy (iverdef, &iverdefmem, offsetof (Elf_Internal_Verdef, vd_bfd));\n\n\t  iverdef->vd_bfd = abfd;\n\n\t  if (iverdef->vd_cnt == 0)\n\t    iverdef->vd_auxptr = NULL;\n\t  else\n\t    {\n\t      iverdef->vd_auxptr = (struct elf_internal_verdaux *)\n                  bfd_alloc2 (abfd, iverdef->vd_cnt,\n                              sizeof (Elf_Internal_Verdaux));\n\t      if (iverdef->vd_auxptr == NULL)\n\t\tgoto error_return_verdef;\n\t    }\n\n\t  if (iverdef->vd_aux\n\t      > (size_t) (contents_end_aux - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdaux = ((Elf_External_Verdaux *)\n\t\t      ((bfd_byte *) everdef + iverdef->vd_aux));\n\t  iverdaux = iverdef->vd_auxptr;\n\t  for (j = 0; j < iverdef->vd_cnt; j++, iverdaux++)\n\t    {\n\t      _bfd_elf_swap_verdaux_in (abfd, everdaux, iverdaux);\n\n\t      iverdaux->vda_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t iverdaux->vda_name);\n\t      if (iverdaux->vda_nodename == NULL)\n\t\tgoto error_return_bad_verdef;\n\n\t      iverdaux->vda_nextptr = NULL;\n\t      if (iverdaux->vda_next == 0)\n\t\t{\n\t\t  iverdef->vd_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverdef->vd_cnt)\n\t\tiverdaux->vda_nextptr = iverdaux + 1;\n\n\t      if (iverdaux->vda_next\n\t\t  > (size_t) (contents_end_aux - (bfd_byte *) everdaux))\n\t\tgoto error_return_bad_verdef;\n\n\t      everdaux = ((Elf_External_Verdaux *)\n\t\t\t  ((bfd_byte *) everdaux + iverdaux->vda_next));\n\t    }\n\n\t  iverdef->vd_nodename = NULL;\n\t  if (iverdef->vd_cnt)\n\t    iverdef->vd_nodename = iverdef->vd_auxptr->vda_nodename;\n\n\t  iverdef->vd_nextdef = NULL;\n\t  if (iverdef->vd_next == 0)\n\t    break;\n\t  if ((size_t) (iverdef - iverdefarr) + 1 < maxidx)\n\t    iverdef->vd_nextdef = iverdef + 1;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdef->vd_next));\n\t}\n\n      free (contents);\n      contents = NULL;\n    }\n  else if (default_imported_symver)\n    {\n      if (freeidx < 3)\n\tfreeidx = 3;\n      else\n\tfreeidx++;\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n          bfd_zalloc2 (abfd, freeidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return;\n\n      elf_tdata (abfd)->cverdefs = freeidx;\n    }\n\n  /* Create a default version based on the soname.  */\n  if (default_imported_symver)\n    {\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdaux *iverdaux;\n\n      iverdef = &elf_tdata (abfd)->verdef[freeidx - 1];\n\n      iverdef->vd_version = VER_DEF_CURRENT;\n      iverdef->vd_flags = 0;\n      iverdef->vd_ndx = freeidx;\n      iverdef->vd_cnt = 1;\n\n      iverdef->vd_bfd = abfd;\n\n      iverdef->vd_nodename = bfd_elf_get_dt_soname (abfd);\n      if (iverdef->vd_nodename == NULL)\n\tgoto error_return_verdef;\n      iverdef->vd_nextdef = NULL;\n      iverdef->vd_auxptr = ((struct elf_internal_verdaux *)\n\t\t\t    bfd_zalloc (abfd, sizeof (Elf_Internal_Verdaux)));\n      if (iverdef->vd_auxptr == NULL)\n\tgoto error_return_verdef;\n\n      iverdaux = iverdef->vd_auxptr;\n      iverdaux->vda_nodename = iverdef->vd_nodename;\n    }\n\n  return TRUE;\n\n error_return:\n  if (contents != NULL)\n    free (contents);\n  return FALSE;\n}",
        "func_after": "bfd_boolean\n_bfd_elf_slurp_version_tables (bfd *abfd, bfd_boolean default_imported_symver)\n{\n  bfd_byte *contents = NULL;\n  unsigned int freeidx = 0;\n\n  if (elf_dynverref (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verneed *everneed;\n      Elf_Internal_Verneed *iverneed;\n      unsigned int i;\n      bfd_byte *contents_end;\n\n      hdr = &elf_tdata (abfd)->dynverref_hdr;\n\n      if (hdr->sh_info == 0\n\t  || hdr->sh_info > hdr->sh_size / sizeof (Elf_External_Verneed))\n\t{\nerror_return_bad_verref:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_r invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\nerror_return_verref:\n\t  elf_tdata (abfd)->verref = NULL;\n\t  elf_tdata (abfd)->cverrefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verref;\n\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verref;\n\n      elf_tdata (abfd)->verref = (Elf_Internal_Verneed *)\n\tbfd_alloc2 (abfd, hdr->sh_info, sizeof (Elf_Internal_Verneed));\n\n      if (elf_tdata (abfd)->verref == NULL)\n\tgoto error_return_verref;\n\n      BFD_ASSERT (sizeof (Elf_External_Verneed)\n\t\t  == sizeof (Elf_External_Vernaux));\n      contents_end = contents + hdr->sh_size - sizeof (Elf_External_Verneed);\n      everneed = (Elf_External_Verneed *) contents;\n      iverneed = elf_tdata (abfd)->verref;\n      for (i = 0; i < hdr->sh_info; i++, iverneed++)\n\t{\n\t  Elf_External_Vernaux *evernaux;\n\t  Elf_Internal_Vernaux *ivernaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verneed_in (abfd, everneed, iverneed);\n\n\t  iverneed->vn_bfd = abfd;\n\n\t  iverneed->vn_filename =\n\t    bfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t     iverneed->vn_file);\n\t  if (iverneed->vn_filename == NULL)\n\t    goto error_return_bad_verref;\n\n\t  if (iverneed->vn_cnt == 0)\n\t    iverneed->vn_auxptr = NULL;\n\t  else\n\t    {\n\t      iverneed->vn_auxptr = (struct elf_internal_vernaux *)\n                  bfd_alloc2 (abfd, iverneed->vn_cnt,\n                              sizeof (Elf_Internal_Vernaux));\n\t      if (iverneed->vn_auxptr == NULL)\n\t\tgoto error_return_verref;\n\t    }\n\n\t  if (iverneed->vn_aux\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  evernaux = ((Elf_External_Vernaux *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_aux));\n\t  ivernaux = iverneed->vn_auxptr;\n\t  for (j = 0; j < iverneed->vn_cnt; j++, ivernaux++)\n\t    {\n\t      _bfd_elf_swap_vernaux_in (abfd, evernaux, ivernaux);\n\n\t      ivernaux->vna_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t ivernaux->vna_name);\n\t      if (ivernaux->vna_nodename == NULL)\n\t\tgoto error_return_bad_verref;\n\n\t      if (ivernaux->vna_other > freeidx)\n\t\tfreeidx = ivernaux->vna_other;\n\n\t      ivernaux->vna_nextptr = NULL;\n\t      if (ivernaux->vna_next == 0)\n\t\t{\n\t\t  iverneed->vn_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverneed->vn_cnt)\n\t\tivernaux->vna_nextptr = ivernaux + 1;\n\n\t      if (ivernaux->vna_next\n\t\t  > (size_t) (contents_end - (bfd_byte *) evernaux))\n\t\tgoto error_return_bad_verref;\n\n\t      evernaux = ((Elf_External_Vernaux *)\n\t\t\t  ((bfd_byte *) evernaux + ivernaux->vna_next));\n\t    }\n\n\t  iverneed->vn_nextref = NULL;\n\t  if (iverneed->vn_next == 0)\n\t    break;\n\t  if (i + 1 < hdr->sh_info)\n\t    iverneed->vn_nextref = iverneed + 1;\n\n\t  if (iverneed->vn_next\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  everneed = ((Elf_External_Verneed *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_next));\n\t}\n      elf_tdata (abfd)->cverrefs = i;\n\n      free (contents);\n      contents = NULL;\n    }\n\n  if (elf_dynverdef (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verdef *everdef;\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdef *iverdefarr;\n      Elf_Internal_Verdef iverdefmem;\n      unsigned int i;\n      unsigned int maxidx;\n      bfd_byte *contents_end_def, *contents_end_aux;\n\n      hdr = &elf_tdata (abfd)->dynverdef_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verdef))\n\t{\n\terror_return_bad_verdef:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_d invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\n\terror_return_verdef:\n\t  elf_tdata (abfd)->verdef = NULL;\n\t  elf_tdata (abfd)->cverdefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verdef;\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verdef;\n\n      BFD_ASSERT (sizeof (Elf_External_Verdef)\n\t\t  >= sizeof (Elf_External_Verdaux));\n      contents_end_def = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdef);\n      contents_end_aux = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdaux);\n\n      /* We know the number of entries in the section but not the maximum\n\t index.  Therefore we have to run through all entries and find\n\t the maximum.  */\n      everdef = (Elf_External_Verdef *) contents;\n      maxidx = 0;\n      for (i = 0; i < hdr->sh_info; ++i)\n\t{\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) == 0)\n\t    goto error_return_bad_verdef;\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) > maxidx)\n\t    maxidx = iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION);\n\n\t  if (iverdefmem.vd_next == 0)\n\t    break;\n\n\t  if (iverdefmem.vd_next\n\t      > (size_t) (contents_end_def - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdefmem.vd_next));\n\t}\n\n      if (default_imported_symver)\n\t{\n\t  if (freeidx > maxidx)\n\t    maxidx = ++freeidx;\n\t  else\n\t    freeidx = ++maxidx;\n\t}\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n\tbfd_zalloc2 (abfd, maxidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return_verdef;\n\n      elf_tdata (abfd)->cverdefs = maxidx;\n\n      everdef = (Elf_External_Verdef *) contents;\n      iverdefarr = elf_tdata (abfd)->verdef;\n      for (i = 0; i < hdr->sh_info; i++)\n\t{\n\t  Elf_External_Verdaux *everdaux;\n\t  Elf_Internal_Verdaux *iverdaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & VERSYM_VERSION) == 0)\n\t    goto error_return_bad_verdef;\n\n\t  iverdef = &iverdefarr[(iverdefmem.vd_ndx & VERSYM_VERSION) - 1];\n\t  memcpy (iverdef, &iverdefmem, offsetof (Elf_Internal_Verdef, vd_bfd));\n\n\t  iverdef->vd_bfd = abfd;\n\n\t  if (iverdef->vd_cnt == 0)\n\t    iverdef->vd_auxptr = NULL;\n\t  else\n\t    {\n\t      iverdef->vd_auxptr = (struct elf_internal_verdaux *)\n                  bfd_alloc2 (abfd, iverdef->vd_cnt,\n                              sizeof (Elf_Internal_Verdaux));\n\t      if (iverdef->vd_auxptr == NULL)\n\t\tgoto error_return_verdef;\n\t    }\n\n\t  if (iverdef->vd_aux\n\t      > (size_t) (contents_end_aux - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdaux = ((Elf_External_Verdaux *)\n\t\t      ((bfd_byte *) everdef + iverdef->vd_aux));\n\t  iverdaux = iverdef->vd_auxptr;\n\t  for (j = 0; j < iverdef->vd_cnt; j++, iverdaux++)\n\t    {\n\t      _bfd_elf_swap_verdaux_in (abfd, everdaux, iverdaux);\n\n\t      iverdaux->vda_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t iverdaux->vda_name);\n\t      if (iverdaux->vda_nodename == NULL)\n\t\tgoto error_return_bad_verdef;\n\n\t      iverdaux->vda_nextptr = NULL;\n\t      if (iverdaux->vda_next == 0)\n\t\t{\n\t\t  iverdef->vd_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverdef->vd_cnt)\n\t\tiverdaux->vda_nextptr = iverdaux + 1;\n\n\t      if (iverdaux->vda_next\n\t\t  > (size_t) (contents_end_aux - (bfd_byte *) everdaux))\n\t\tgoto error_return_bad_verdef;\n\n\t      everdaux = ((Elf_External_Verdaux *)\n\t\t\t  ((bfd_byte *) everdaux + iverdaux->vda_next));\n\t    }\n\n\t  iverdef->vd_nodename = NULL;\n\t  if (iverdef->vd_cnt)\n\t    iverdef->vd_nodename = iverdef->vd_auxptr->vda_nodename;\n\n\t  iverdef->vd_nextdef = NULL;\n\t  if (iverdef->vd_next == 0)\n\t    break;\n\t  if ((size_t) (iverdef - iverdefarr) + 1 < maxidx)\n\t    iverdef->vd_nextdef = iverdef + 1;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdef->vd_next));\n\t}\n\n      free (contents);\n      contents = NULL;\n    }\n  else if (default_imported_symver)\n    {\n      if (freeidx < 3)\n\tfreeidx = 3;\n      else\n\tfreeidx++;\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n          bfd_zalloc2 (abfd, freeidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return;\n\n      elf_tdata (abfd)->cverdefs = freeidx;\n    }\n\n  /* Create a default version based on the soname.  */\n  if (default_imported_symver)\n    {\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdaux *iverdaux;\n\n      iverdef = &elf_tdata (abfd)->verdef[freeidx - 1];\n\n      iverdef->vd_version = VER_DEF_CURRENT;\n      iverdef->vd_flags = 0;\n      iverdef->vd_ndx = freeidx;\n      iverdef->vd_cnt = 1;\n\n      iverdef->vd_bfd = abfd;\n\n      iverdef->vd_nodename = bfd_elf_get_dt_soname (abfd);\n      if (iverdef->vd_nodename == NULL)\n\tgoto error_return_verdef;\n      iverdef->vd_nextdef = NULL;\n      iverdef->vd_auxptr = ((struct elf_internal_verdaux *)\n\t\t\t    bfd_zalloc (abfd, sizeof (Elf_Internal_Verdaux)));\n      if (iverdef->vd_auxptr == NULL)\n\tgoto error_return_verdef;\n\n      iverdaux = iverdef->vd_auxptr;\n      iverdaux->vda_nodename = iverdef->vd_nodename;\n    }\n\n  return TRUE;\n\n error_return:\n  if (contents != NULL)\n    free (contents);\n  return FALSE;\n}",
        "description": "The _bfd_elf_slurp_version_tables function in the BFD library, part of GNU Binutils 2.29, is susceptible to a denial of service attack. This vulnerability arises from excessive memory allocation when processing a specially crafted ELF file, leading to potential application crashes.",
        "commit": "The vulnerability involves a memory allocation issue within the ELF (Executable and Linkable Format) handling module of a binary file format library. Specifically, the code performs a sanity check on the size of the `SHT_GNU_verneed` section, ensuring it meets certain minimum requirements related to the number of version entries (`sh_info`). Additionally, since the code either fully initializes all fields of the version entries or exits with an error, there is no need to zero-initialize the allocated memory for these entries. The fix includes modifying the `_bfd_elf_slurp_version_tables` function to test the `sh_info` field of the `SHT_GNU_verneed` section for validity and to avoid zero-initializing the memory allocated for version references."
    }
]