[
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.8148666024208069,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    },
    {
        "cwe": "CWE-121",
        "func_name": "libtiff/TIFFReadCustomDirectory",
        "score": 0.8119214773178101,
        "func_before": "int\nTIFFReadCustomDirectory(TIFF* tif, toff_t diroff,\n\t\t\tconst TIFFFieldArray* infoarray)\n{\n\tstatic const char module[] = \"TIFFReadCustomDirectory\";\n\tTIFFDirEntry* dir;\n\tuint16_t dircount;\n\tTIFFDirEntry* dp;\n\tuint16_t di;\n\tconst TIFFField* fip;\n\tuint32_t fii;\n        (*tif->tif_cleanup)(tif);   /* cleanup any previous compression state */\n\t_TIFFSetupFields(tif, infoarray);\n\tdircount=TIFFFetchDirectory(tif,diroff,&dir,NULL);\n\tif (!dircount)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata,module,\n\t\t    \"Failed to read custom directory at offset %\" PRIu64,diroff);\n\t\treturn 0;\n\t}\n\tTIFFFreeDirectory(tif);\n\t_TIFFmemset(&tif->tif_dir, 0, sizeof(TIFFDirectory));\n\tTIFFReadDirectoryCheckOrder(tif,dir,dircount);\n\tfor (di=0, dp=dir; di<dircount; di++, dp++)\n\t{\n\t\tTIFFReadDirectoryFindFieldInfo(tif,dp->tdir_tag,&fii);\n\t\tif (fii == FAILED_FII)\n\t\t{\n\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t    \"Unknown field with tag %\"PRIu16\" (0x%\"PRIx16\") encountered\",\n\t\t\t    dp->tdir_tag, dp->tdir_tag);\n\t\t\tif (!_TIFFMergeFields(tif, _TIFFCreateAnonField(tif,\n\t\t\t\t\t\tdp->tdir_tag,\n\t\t\t\t\t\t(TIFFDataType) dp->tdir_type),\n\t\t\t\t\t     1)) {\n\t\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t\t    \"Registering anonymous field with tag %\"PRIu16\" (0x%\"PRIx16\") failed\",\n\t\t\t\t    dp->tdir_tag, dp->tdir_tag);\n\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t} else {\n\t\t\t\tTIFFReadDirectoryFindFieldInfo(tif,dp->tdir_tag,&fii);\n\t\t\t\tassert( fii != FAILED_FII );\n\t\t\t}\n\t\t}\n\t\tif (!dp->tdir_ignore)\n\t\t{\n\t\t\tfip=tif->tif_fields[fii];\n\t\t\tif (fip->field_bit==FIELD_IGNORE)\n\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\telse\n\t\t\t{\n\t\t\t\t/* check data type */\n\t\t\t\twhile ((fip->field_type!=TIFF_ANY)&&(fip->field_type!=dp->tdir_type))\n\t\t\t\t{\n\t\t\t\t\tfii++;\n\t\t\t\t\tif ((fii==tif->tif_nfields)||\n\t\t\t\t\t    (tif->tif_fields[fii]->field_tag!=(uint32_t)dp->tdir_tag))\n\t\t\t\t\t{\n\t\t\t\t\t\tfii=0xFFFF;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tfip=tif->tif_fields[fii];\n\t\t\t\t}\n\t\t\t\tif (fii==0xFFFF)\n\t\t\t\t{\n\t\t\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t\t\t    \"Wrong data type %\"PRIu16\" for \\\"%s\\\"; tag ignored\",\n\t\t\t\t\t    dp->tdir_type,fip->field_name);\n\t\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\t/* check count if known in advance */\n\t\t\t\t\tif ((fip->field_readcount!=TIFF_VARIABLE)&&\n\t\t\t\t\t    (fip->field_readcount!=TIFF_VARIABLE2))\n\t\t\t\t\t{\n\t\t\t\t\t\tuint32_t expected;\n\t\t\t\t\t\tif (fip->field_readcount==TIFF_SPP)\n\t\t\t\t\t\t\texpected=(uint32_t)tif->tif_dir.td_samplesperpixel;\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\texpected=(uint32_t)fip->field_readcount;\n\t\t\t\t\t\tif (!CheckDirCount(tif,dp,expected))\n\t\t\t\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!dp->tdir_ignore) {\n\t\t\t\tswitch (dp->tdir_tag) \n\t\t\t\t{\n\t\t\t\t\tcase EXIFTAG_SUBJECTDISTANCE:\n                        if(fip->field_name != NULL && strncmp(fip->field_name, \"Tag \", 4) != 0 ) {\n                            /* should only be called on a Exif directory */\n                            /* when exifFields[] is active */\n                            (void)TIFFFetchSubjectDistance(tif, dp);\n                        }\n                        else {\n                            (void)TIFFFetchNormalTag(tif, dp, TRUE);\n                        }\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t(void)TIFFFetchNormalTag(tif, dp, TRUE);\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} /*-- if (!dp->tdir_ignore) */\n\t\t}\n\t}\n\tif (dir)\n\t\t_TIFFfree(dir);\n\treturn 1;\n}",
        "func_after": "int\nTIFFReadCustomDirectory(TIFF* tif, toff_t diroff,\n\t\t\tconst TIFFFieldArray* infoarray)\n{\n\tstatic const char module[] = \"TIFFReadCustomDirectory\";\n\tTIFFDirEntry* dir;\n\tuint16_t dircount;\n\tTIFFDirEntry* dp;\n\tuint16_t di;\n\tconst TIFFField* fip;\n\tuint32_t fii;\n        (*tif->tif_cleanup)(tif);   /* cleanup any previous compression state */\n\t_TIFFSetupFields(tif, infoarray);\n\tdircount=TIFFFetchDirectory(tif,diroff,&dir,NULL);\n\tif (!dircount)\n\t{\n\t\tTIFFErrorExt(tif->tif_clientdata,module,\n\t\t    \"Failed to read custom directory at offset %\" PRIu64,diroff);\n\t\treturn 0;\n\t}\n\tTIFFFreeDirectory(tif);\n\t_TIFFmemset(&tif->tif_dir, 0, sizeof(TIFFDirectory));\n\tTIFFReadDirectoryCheckOrder(tif,dir,dircount);\n\tfor (di=0, dp=dir; di<dircount; di++, dp++)\n\t{\n\t\tTIFFReadDirectoryFindFieldInfo(tif,dp->tdir_tag,&fii);\n\t\tif (fii == FAILED_FII)\n\t\t{\n\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t    \"Unknown field with tag %\"PRIu16\" (0x%\"PRIx16\") encountered\",\n\t\t\t    dp->tdir_tag, dp->tdir_tag);\n\t\t\tif (!_TIFFMergeFields(tif, _TIFFCreateAnonField(tif,\n\t\t\t\t\t\tdp->tdir_tag,\n\t\t\t\t\t\t(TIFFDataType) dp->tdir_type),\n\t\t\t\t\t     1)) {\n\t\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t\t    \"Registering anonymous field with tag %\"PRIu16\" (0x%\"PRIx16\") failed\",\n\t\t\t\t    dp->tdir_tag, dp->tdir_tag);\n\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t} else {\n\t\t\t\tTIFFReadDirectoryFindFieldInfo(tif,dp->tdir_tag,&fii);\n\t\t\t\tassert( fii != FAILED_FII );\n\t\t\t}\n\t\t}\n\t\tif (!dp->tdir_ignore)\n\t\t{\n\t\t\tfip=tif->tif_fields[fii];\n\t\t\tif (fip->field_bit==FIELD_IGNORE)\n\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\telse\n\t\t\t{\n\t\t\t\t/* check data type */\n\t\t\t\twhile ((fip->field_type!=TIFF_ANY)&&(fip->field_type!=dp->tdir_type))\n\t\t\t\t{\n\t\t\t\t\tfii++;\n\t\t\t\t\tif ((fii==tif->tif_nfields)||\n\t\t\t\t\t    (tif->tif_fields[fii]->field_tag!=(uint32_t)dp->tdir_tag))\n\t\t\t\t\t{\n\t\t\t\t\t\tfii=0xFFFF;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tfip=tif->tif_fields[fii];\n\t\t\t\t}\n\t\t\t\tif (fii==0xFFFF)\n\t\t\t\t{\n\t\t\t\t\tTIFFWarningExt(tif->tif_clientdata, module,\n\t\t\t\t\t    \"Wrong data type %\"PRIu16\" for \\\"%s\\\"; tag ignored\",\n\t\t\t\t\t    dp->tdir_type,fip->field_name);\n\t\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\t/* check count if known in advance */\n\t\t\t\t\tif ((fip->field_readcount!=TIFF_VARIABLE)&&\n\t\t\t\t\t    (fip->field_readcount!=TIFF_VARIABLE2))\n\t\t\t\t\t{\n\t\t\t\t\t\tuint32_t expected;\n\t\t\t\t\t\tif (fip->field_readcount==TIFF_SPP)\n\t\t\t\t\t\t\texpected=(uint32_t)tif->tif_dir.td_samplesperpixel;\n\t\t\t\t\t\telse\n\t\t\t\t\t\t\texpected=(uint32_t)fip->field_readcount;\n\t\t\t\t\t\tif (!CheckDirCount(tif,dp,expected))\n\t\t\t\t\t\t\tdp->tdir_ignore = TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!dp->tdir_ignore) {\n\t\t\t\tswitch (dp->tdir_tag) \n\t\t\t\t{\n\t\t\t\t\tcase EXIFTAG_SUBJECTDISTANCE:\n                        if(!TIFFFieldIsAnonymous(fip)) {\n                            /* should only be called on a Exif directory */\n                            /* when exifFields[] is active */\n                            (void)TIFFFetchSubjectDistance(tif, dp);\n                        }\n                        else {\n                            (void)TIFFFetchNormalTag(tif, dp, TRUE);\n                        }\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t(void)TIFFFetchNormalTag(tif, dp, TRUE);\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} /*-- if (!dp->tdir_ignore) */\n\t\t}\n\t}\n\tif (dir)\n\t\t_TIFFfree(dir);\n\treturn 1;\n}",
        "description": "A stack buffer overflow vulnerability exists in the `main()` function of Libtiffs' `tiffcp.c`. This flaw enables an attacker to exploit it by providing a specially crafted TIFF file to the `tiffcp` tool, resulting in a stack buffer overflow. This can lead to memory corruption and ultimately cause a crash, leading to a denial of service.",
        "commit": "The addition of an extra flag for handling anonymous or unknown tags."
    },
    {
        "cwe": "CWE-416",
        "func_name": "torvalds/ipt_do_table",
        "score": 0.8172488808631897,
        "func_before": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = READ_ONCE(table->private); /* Address dependency. */\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "func_after": "unsigned int\nipt_do_table(struct sk_buff *skb,\n\t     const struct nf_hook_state *state,\n\t     struct xt_table *table)\n{\n\tunsigned int hook = state->hook;\n\tstatic const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));\n\tconst struct iphdr *ip;\n\t/* Initializing verdict to NF_DROP keeps gcc happy. */\n\tunsigned int verdict = NF_DROP;\n\tconst char *indev, *outdev;\n\tconst void *table_base;\n\tstruct ipt_entry *e, **jumpstack;\n\tunsigned int stackidx, cpu;\n\tconst struct xt_table_info *private;\n\tstruct xt_action_param acpar;\n\tunsigned int addend;\n\n\t/* Initialization */\n\tstackidx = 0;\n\tip = ip_hdr(skb);\n\tindev = state->in ? state->in->name : nulldevname;\n\toutdev = state->out ? state->out->name : nulldevname;\n\t/* We handle fragments by dealing with the first fragment as\n\t * if it was a normal packet.  All other fragments are treated\n\t * normally, except that they will NEVER match rules that ask\n\t * things we don't know, ie. tcp syn flag or ports).  If the\n\t * rule is also a fragment-specific rule, non-fragments won't\n\t * match it. */\n\tacpar.fragoff = ntohs(ip->frag_off) & IP_OFFSET;\n\tacpar.thoff   = ip_hdrlen(skb);\n\tacpar.hotdrop = false;\n\tacpar.state   = state;\n\n\tWARN_ON(!(table->valid_hooks & (1 << hook)));\n\tlocal_bh_disable();\n\taddend = xt_write_recseq_begin();\n\tprivate = rcu_access_pointer(table->private);\n\tcpu        = smp_processor_id();\n\ttable_base = private->entries;\n\tjumpstack  = (struct ipt_entry **)private->jumpstack[cpu];\n\n\t/* Switch to alternate jumpstack if we're being invoked via TEE.\n\t * TEE issues XT_CONTINUE verdict on original skb so we must not\n\t * clobber the jumpstack.\n\t *\n\t * For recursion via REJECT or SYNPROXY the stack will be clobbered\n\t * but it is no problem since absolute verdict is issued by these.\n\t */\n\tif (static_key_false(&xt_tee_enabled))\n\t\tjumpstack += private->stacksize * __this_cpu_read(nf_skb_duplicated);\n\n\te = get_entry(table_base, private->hook_entry[hook]);\n\n\tdo {\n\t\tconst struct xt_entry_target *t;\n\t\tconst struct xt_entry_match *ematch;\n\t\tstruct xt_counters *counter;\n\n\t\tWARN_ON(!e);\n\t\tif (!ip_packet_match(ip, indev, outdev,\n\t\t    &e->ip, acpar.fragoff)) {\n no_match:\n\t\t\te = ipt_next_entry(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\txt_ematch_foreach(ematch, e) {\n\t\t\tacpar.match     = ematch->u.kernel.match;\n\t\t\tacpar.matchinfo = ematch->data;\n\t\t\tif (!acpar.match->match(skb, &acpar))\n\t\t\t\tgoto no_match;\n\t\t}\n\n\t\tcounter = xt_get_this_cpu_counter(&e->counters);\n\t\tADD_COUNTER(*counter, skb->len, 1);\n\n\t\tt = ipt_get_target_c(e);\n\t\tWARN_ON(!t->u.kernel.target);\n\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)\n\t\t/* The packet is traced: log it */\n\t\tif (unlikely(skb->nf_trace))\n\t\t\ttrace_packet(state->net, skb, hook, state->in,\n\t\t\t\t     state->out, table->name, private, e);\n#endif\n\t\t/* Standard target? */\n\t\tif (!t->u.kernel.target->target) {\n\t\t\tint v;\n\n\t\t\tv = ((struct xt_standard_target *)t)->verdict;\n\t\t\tif (v < 0) {\n\t\t\t\t/* Pop from stack? */\n\t\t\t\tif (v != XT_RETURN) {\n\t\t\t\t\tverdict = (unsigned int)(-v) - 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (stackidx == 0) {\n\t\t\t\t\te = get_entry(table_base,\n\t\t\t\t\t    private->underflow[hook]);\n\t\t\t\t} else {\n\t\t\t\t\te = jumpstack[--stackidx];\n\t\t\t\t\te = ipt_next_entry(e);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (table_base + v != ipt_next_entry(e) &&\n\t\t\t    !(e->ip.flags & IPT_F_GOTO)) {\n\t\t\t\tif (unlikely(stackidx >= private->stacksize)) {\n\t\t\t\t\tverdict = NF_DROP;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tjumpstack[stackidx++] = e;\n\t\t\t}\n\n\t\t\te = get_entry(table_base, v);\n\t\t\tcontinue;\n\t\t}\n\n\t\tacpar.target   = t->u.kernel.target;\n\t\tacpar.targinfo = t->data;\n\n\t\tverdict = t->u.kernel.target->target(skb, &acpar);\n\t\tif (verdict == XT_CONTINUE) {\n\t\t\t/* Target might have changed stuff. */\n\t\t\tip = ip_hdr(skb);\n\t\t\te = ipt_next_entry(e);\n\t\t} else {\n\t\t\t/* Verdict */\n\t\t\tbreak;\n\t\t}\n\t} while (!acpar.hotdrop);\n\n\txt_write_recseq_end(addend);\n\tlocal_bh_enable();\n\n\tif (acpar.hotdrop)\n\t\treturn NF_DROP;\n\telse return verdict;\n}",
        "description": "An issue was discovered in the netfilter component of the Linux kernel prior to version 5.10. This vulnerability involves a use-after-free condition in the packet processing context due to improper handling of per-CPU sequence counts during concurrent iptables rule replacements. This flaw could be exploited by users with the CAP_NET_ADMIN capability in an unprivileged namespace.",
        "commit": "When performing concurrent iptables rules replacement, the system checks the per-CPU sequence count after assigning new information. This sequence count is intended to synchronize with the packet path without using explicit locking. If packets are currently using the old table information, the sequence count is incremented to an odd value and then to an even value after processing. After assigning the new table information, a write memory barrier is executed to ensure all CPUs see the latest value. If the packet path starts with the old table information, the sequence counter remains odd, and the iptables replacement waits until the sequence count is even before freeing the old table information. However, if the CPU delays executing the new table information assignment and memory barrier, another CPU's packet path may still use the old table information. This delay can lead to a use-after-free error in the packet processing context, resulting in a kernel NULL pointer dereference. To fix this issue, either enforce instruction ordering after the new table information assignment or switch to RCU for synchronization."
    },
    {
        "cwe": "CWE-385",
        "func_name": "openssl/ecdsa_sign_setup",
        "score": 0.8109728693962097,
        "func_before": "static int ecdsa_sign_setup(EC_KEY *eckey, BN_CTX *ctx_in,\n\t\t\t\t\tBIGNUM **kinvp, BIGNUM **rp,\n\t\t\t\t\tconst unsigned char *dgst, int dlen)\n{\n\tBN_CTX   *ctx = NULL;\n\tBIGNUM\t *k = NULL, *r = NULL, *order = NULL, *X = NULL;\n\tEC_POINT *tmp_point=NULL;\n\tconst EC_GROUP *group;\n\tint \t ret = 0;\n\n\tif (eckey == NULL || (group = EC_KEY_get0_group(eckey)) == NULL)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_PASSED_NULL_PARAMETER);\n\t\treturn 0;\n\t}\n\n\tif (ctx_in == NULL) \n\t{\n\t\tif ((ctx = BN_CTX_new()) == NULL)\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_MALLOC_FAILURE);\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse\n\t\tctx = ctx_in;\n\n\tk     = BN_new();\t/* this value is later returned in *kinvp */\n\tr     = BN_new();\t/* this value is later returned in *rp    */\n\torder = BN_new();\n\tX     = BN_new();\n\tif (!k || !r || !order || !X)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_MALLOC_FAILURE);\n\t\tgoto err;\n\t}\n\tif ((tmp_point = EC_POINT_new(group)) == NULL)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\tgoto err;\n\t}\n\tif (!EC_GROUP_get_order(group, order, ctx))\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\tgoto err;\n\t}\n\n#ifdef OPENSSL_FIPS\n\tif (!fips_check_ec_prng(eckey))\n\t\tgoto err;\n#endif\n\t\n\tdo\n\t{\n\t\t/* get random k */\t\n\t\tdo\n#ifndef OPENSSL_NO_SHA512\n\t\t\tif (dgst != NULL)\n\t\t\t{\n\t\t\t\tif (!BN_generate_dsa_nonce(k, order, EC_KEY_get0_private_key(eckey),\n\t\t\t\t\t\t\t   dgst, dlen, ctx))\n\t\t\t\t\t{\n\t\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,\n\t\t\t\t\t\t ECDSA_R_RANDOM_NUMBER_GENERATION_FAILED);\n\t\t\t\t\tgoto err;\n\t\t\t\t\t}\n\t\t\t}\n\t\t\telse\n#endif\n\t\t\t{\n\t\t\t\tif (!BN_rand_range(k, order))\n\t\t\t\t{\n\t\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,\n\t\t\t\t\t\t ECDSA_R_RANDOM_NUMBER_GENERATION_FAILED);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t}\n\t\twhile (BN_is_zero(k));\n\n\t\t/* We do not want timing information to leak the length of k,\n\t\t * so we compute G*k using an equivalent scalar of fixed\n\t\t * bit-length. */\n\n\t\tif (!BN_add(k, k, order)) goto err;\n\t\tif (BN_num_bits(k) <= BN_num_bits(order))\n\t\t\tif (!BN_add(k, k, order)) goto err;\n\n\t\t/* compute r the x-coordinate of generator * k */\n\t\tif (!EC_POINT_mul(group, tmp_point, k, NULL, NULL, ctx))\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\t\tgoto err;\n\t\t}\n\t\tif (EC_METHOD_get_field_type(EC_GROUP_method_of(group)) == NID_X9_62_prime_field)\n\t\t{\n\t\t\tif (!EC_POINT_get_affine_coordinates_GFp(group,\n\t\t\t\ttmp_point, X, NULL, ctx))\n\t\t\t{\n\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_EC_LIB);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n#ifndef OPENSSL_NO_EC2M\n\t\telse /* NID_X9_62_characteristic_two_field */\n\t\t{\n\t\t\tif (!EC_POINT_get_affine_coordinates_GF2m(group,\n\t\t\t\ttmp_point, X, NULL, ctx))\n\t\t\t{\n\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_EC_LIB);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n#endif\n\t\tif (!BN_nnmod(r, X, order, ctx))\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\n\t\t}\n\t}\n\twhile (BN_is_zero(r));\n\n\t/* compute the inverse of k */\n\tif (!BN_mod_inverse(k, k, order, ctx))\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\tgoto err;\t\n\t}\n\t/* clear old values if necessary */\n\tif (*rp != NULL)\n\t\tBN_clear_free(*rp);\n\tif (*kinvp != NULL) \n\t\tBN_clear_free(*kinvp);\n\t/* save the pre-computed values  */\n\t*rp    = r;\n\t*kinvp = k;\n\tret = 1;\nerr:\n\tif (!ret)\n\t{\n\t\tif (k != NULL) BN_clear_free(k);\n\t\tif (r != NULL) BN_clear_free(r);\n\t}\n\tif (ctx_in == NULL) \n\t\tBN_CTX_free(ctx);\n\tif (order != NULL)\n\t\tBN_free(order);\n\tif (tmp_point != NULL) \n\t\tEC_POINT_free(tmp_point);\n\tif (X)\n\t\tBN_clear_free(X);\n\treturn(ret);\n}",
        "func_after": "static int ecdsa_sign_setup(EC_KEY *eckey, BN_CTX *ctx_in,\n\t\t\t\t\tBIGNUM **kinvp, BIGNUM **rp,\n\t\t\t\t\tconst unsigned char *dgst, int dlen)\n{\n\tBN_CTX   *ctx = NULL;\n\tBIGNUM\t *k = NULL, *r = NULL, *order = NULL, *X = NULL;\n\tEC_POINT *tmp_point=NULL;\n\tconst EC_GROUP *group;\n\tint \t ret = 0;\n\n\tif (eckey == NULL || (group = EC_KEY_get0_group(eckey)) == NULL)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_PASSED_NULL_PARAMETER);\n\t\treturn 0;\n\t}\n\n\tif (ctx_in == NULL) \n\t{\n\t\tif ((ctx = BN_CTX_new()) == NULL)\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_MALLOC_FAILURE);\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse\n\t\tctx = ctx_in;\n\n\tk     = BN_new();\t/* this value is later returned in *kinvp */\n\tr     = BN_new();\t/* this value is later returned in *rp    */\n\torder = BN_new();\n\tX     = BN_new();\n\tif (!k || !r || !order || !X)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_MALLOC_FAILURE);\n\t\tgoto err;\n\t}\n\tif ((tmp_point = EC_POINT_new(group)) == NULL)\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\tgoto err;\n\t}\n\tif (!EC_GROUP_get_order(group, order, ctx))\n\t{\n\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\tgoto err;\n\t}\n\n#ifdef OPENSSL_FIPS\n\tif (!fips_check_ec_prng(eckey))\n\t\tgoto err;\n#endif\n\t\n\tdo\n\t{\n\t\t/* get random k */\t\n\t\tdo\n#ifndef OPENSSL_NO_SHA512\n\t\t\tif (dgst != NULL)\n\t\t\t{\n\t\t\t\tif (!BN_generate_dsa_nonce(k, order, EC_KEY_get0_private_key(eckey),\n\t\t\t\t\t\t\t   dgst, dlen, ctx))\n\t\t\t\t\t{\n\t\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,\n\t\t\t\t\t\t ECDSA_R_RANDOM_NUMBER_GENERATION_FAILED);\n\t\t\t\t\tgoto err;\n\t\t\t\t\t}\n\t\t\t}\n\t\t\telse\n#endif\n\t\t\t{\n\t\t\t\tif (!BN_rand_range(k, order))\n\t\t\t\t{\n\t\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,\n\t\t\t\t\t\t ECDSA_R_RANDOM_NUMBER_GENERATION_FAILED);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t}\n\t\twhile (BN_is_zero(k));\n\n\t\t/* We do not want timing information to leak the length of k,\n\t\t * so we compute G*k using an equivalent scalar of fixed\n\t\t * bit-length. */\n\n\t\tif (!BN_add(k, k, order)) goto err;\n\t\tif (BN_num_bits(k) <= BN_num_bits(order))\n\t\t\tif (!BN_add(k, k, order)) goto err;\n\n\t\t/* compute r the x-coordinate of generator * k */\n\t\tif (!EC_POINT_mul(group, tmp_point, k, NULL, NULL, ctx))\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_EC_LIB);\n\t\t\tgoto err;\n\t\t}\n\t\tif (EC_METHOD_get_field_type(EC_GROUP_method_of(group)) == NID_X9_62_prime_field)\n\t\t{\n\t\t\tif (!EC_POINT_get_affine_coordinates_GFp(group,\n\t\t\t\ttmp_point, X, NULL, ctx))\n\t\t\t{\n\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_EC_LIB);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n#ifndef OPENSSL_NO_EC2M\n\t\telse /* NID_X9_62_characteristic_two_field */\n\t\t{\n\t\t\tif (!EC_POINT_get_affine_coordinates_GF2m(group,\n\t\t\t\ttmp_point, X, NULL, ctx))\n\t\t\t{\n\t\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP,ERR_R_EC_LIB);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n#endif\n\t\tif (!BN_nnmod(r, X, order, ctx))\n\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\n\t\t}\n\t}\n\twhile (BN_is_zero(r));\n\n\t/* compute the inverse of k */\n\tif (EC_GROUP_get_mont_data(group) != NULL)\n\t\t{\n\t\t/* We want inverse in constant time, therefore we utilize the\n\t\t * fact order must be prime and use Fermats Little Theorem\n\t\t * instead. */\n\t\tif (!BN_set_word(X, 2) )\n\t\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\n\t\t\t}\n\t\tif (!BN_mod_sub(X, order, X, order, ctx))\n\t\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\n\t\t\t}\n\t\tBN_set_flags(X, BN_FLG_CONSTTIME);\n\t\tif (!BN_mod_exp_mont_consttime(k, k, X, order, ctx, EC_GROUP_get_mont_data(group)))\n\t\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\telse\n\t\t{\n\t\tif (!BN_mod_inverse(k, k, order, ctx))\n\t\t\t{\n\t\t\tECDSAerr(ECDSA_F_ECDSA_SIGN_SETUP, ERR_R_BN_LIB);\n\t\t\tgoto err;\t\n\t\t\t}\n\t\t}\n\n\t/* clear old values if necessary */\n\tif (*rp != NULL)\n\t\tBN_clear_free(*rp);\n\tif (*kinvp != NULL) \n\t\tBN_clear_free(*kinvp);\n\t/* save the pre-computed values  */\n\t*rp    = r;\n\t*kinvp = k;\n\tret = 1;\nerr:\n\tif (!ret)\n\t{\n\t\tif (k != NULL) BN_clear_free(k);\n\t\tif (r != NULL) BN_clear_free(r);\n\t}\n\tif (ctx_in == NULL) \n\t\tBN_CTX_free(ctx);\n\tif (order != NULL)\n\t\tBN_free(order);\n\tif (tmp_point != NULL) \n\t\tEC_POINT_free(tmp_point);\n\tif (X)\n\t\tBN_clear_free(X);\n\treturn(ret);\n}",
        "description": "A timing attack vulnerability was identified in OpenSSL versions prior to 1.0.1u, enabling a malicious user with local access to potentially recover ECDSA P-256 private keys through the exploitation of timing variations in cryptographic operations.",
        "commit": "The vulnerability knowledge has been abstracted and generalized as follows:\n\n\"The implementation of ECDSA includes an option to utilize BN_mod_exp_mont_consttime, a constant-time modular exponentiation function. This enhancement aims to improve the security of ECDSA operations.\""
    },
    {
        "cwe": "CWE-74",
        "func_name": "flatpak/flatpak_run_add_environment_args",
        "score": 0.8076599836349487,
        "func_before": "gboolean\nflatpak_run_add_environment_args (FlatpakBwrap    *bwrap,\n                                  const char      *app_info_path,\n                                  FlatpakRunFlags  flags,\n                                  const char      *app_id,\n                                  FlatpakContext  *context,\n                                  GFile           *app_id_dir,\n                                  GPtrArray       *previous_app_id_dirs,\n                                  FlatpakExports **exports_out,\n                                  GCancellable    *cancellable,\n                                  GError         **error)\n{\n  g_autoptr(GError) my_error = NULL;\n  g_autoptr(FlatpakExports) exports = NULL;\n  g_autoptr(FlatpakBwrap) proxy_arg_bwrap = flatpak_bwrap_new (flatpak_bwrap_empty_env);\n  gboolean has_wayland = FALSE;\n  gboolean allow_x11 = FALSE;\n\n  if ((context->shares & FLATPAK_CONTEXT_SHARED_IPC) == 0)\n    {\n      g_debug (\"Disallowing ipc access\");\n      flatpak_bwrap_add_args (bwrap, \"--unshare-ipc\", NULL);\n    }\n\n  if ((context->shares & FLATPAK_CONTEXT_SHARED_NETWORK) == 0)\n    {\n      g_debug (\"Disallowing network access\");\n      flatpak_bwrap_add_args (bwrap, \"--unshare-net\", NULL);\n    }\n\n  if (context->devices & FLATPAK_CONTEXT_DEVICE_ALL)\n    {\n      flatpak_bwrap_add_args (bwrap,\n                              \"--dev-bind\", \"/dev\", \"/dev\",\n                              NULL);\n      /* Don't expose the host /dev/shm, just the device nodes, unless explicitly allowed */\n      if (g_file_test (\"/dev/shm\", G_FILE_TEST_IS_DIR))\n        {\n          if ((context->devices & FLATPAK_CONTEXT_DEVICE_SHM) == 0)\n            flatpak_bwrap_add_args (bwrap,\n                                    \"--tmpfs\", \"/dev/shm\",\n                                    NULL);\n        }\n      else if (g_file_test (\"/dev/shm\", G_FILE_TEST_IS_SYMLINK))\n        {\n          g_autofree char *link = flatpak_readlink (\"/dev/shm\", NULL);\n\n          /* On debian (with sysv init) the host /dev/shm is a symlink to /run/shm, so we can't\n             mount on top of it. */\n          if (g_strcmp0 (link, \"/run/shm\") == 0)\n            {\n              if (context->devices & FLATPAK_CONTEXT_DEVICE_SHM &&\n                  g_file_test (\"/run/shm\", G_FILE_TEST_IS_DIR))\n                flatpak_bwrap_add_args (bwrap,\n                                        \"--bind\", \"/run/shm\", \"/run/shm\",\n                                        NULL);\n              else\n                flatpak_bwrap_add_args (bwrap,\n                                        \"--dir\", \"/run/shm\",\n                                        NULL);\n            }\n          else\n            g_warning (\"Unexpected /dev/shm symlink %s\", link);\n        }\n    }\n  else\n    {\n      flatpak_bwrap_add_args (bwrap,\n                              \"--dev\", \"/dev\",\n                              NULL);\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_DRI)\n        {\n          g_debug (\"Allowing dri access\");\n          int i;\n          char *dri_devices[] = {\n            \"/dev/dri\",\n            /* mali */\n            \"/dev/mali\",\n            \"/dev/mali0\",\n            \"/dev/umplock\",\n            /* nvidia */\n            \"/dev/nvidiactl\",\n            \"/dev/nvidia-modeset\",\n            /* nvidia OpenCL/CUDA */\n            \"/dev/nvidia-uvm\",\n            \"/dev/nvidia-uvm-tools\",\n          };\n\n          for (i = 0; i < G_N_ELEMENTS (dri_devices); i++)\n            {\n              if (g_file_test (dri_devices[i], G_FILE_TEST_EXISTS))\n                flatpak_bwrap_add_args (bwrap, \"--dev-bind\", dri_devices[i], dri_devices[i], NULL);\n            }\n\n          /* Each Nvidia card gets its own device.\n             This is a fairly arbitrary limit but ASUS sells mining boards supporting 20 in theory. */\n          char nvidia_dev[14]; /* /dev/nvidia plus up to 2 digits */\n          for (i = 0; i < 20; i++)\n            {\n              g_snprintf (nvidia_dev, sizeof (nvidia_dev), \"/dev/nvidia%d\", i);\n              if (g_file_test (nvidia_dev, G_FILE_TEST_EXISTS))\n                flatpak_bwrap_add_args (bwrap, \"--dev-bind\", nvidia_dev, nvidia_dev, NULL);\n            }\n        }\n\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_KVM)\n        {\n          g_debug (\"Allowing kvm access\");\n          if (g_file_test (\"/dev/kvm\", G_FILE_TEST_EXISTS))\n            flatpak_bwrap_add_args (bwrap, \"--dev-bind\", \"/dev/kvm\", \"/dev/kvm\", NULL);\n        }\n\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_SHM)\n        {\n          /* This is a symlink to /run/shm on debian, so bind to real target */\n          g_autofree char *real_dev_shm = realpath (\"/dev/shm\", NULL);\n\n          g_debug (\"Allowing /dev/shm access (as %s)\", real_dev_shm);\n          if (real_dev_shm != NULL)\n              flatpak_bwrap_add_args (bwrap, \"--bind\", real_dev_shm, \"/dev/shm\", NULL);\n        }\n    }\n\n  flatpak_context_append_bwrap_filesystem (context, bwrap, app_id, app_id_dir, previous_app_id_dirs, &exports);\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_WAYLAND)\n    {\n      g_debug (\"Allowing wayland access\");\n      has_wayland = flatpak_run_add_wayland_args (bwrap);\n    }\n\n  if ((context->sockets & FLATPAK_CONTEXT_SOCKET_FALLBACK_X11) != 0)\n    allow_x11 = !has_wayland;\n  else\n    allow_x11 = (context->sockets & FLATPAK_CONTEXT_SOCKET_X11) != 0;\n\n  flatpak_run_add_x11_args (bwrap, allow_x11);\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_SSH_AUTH)\n    {\n      flatpak_run_add_ssh_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_PULSEAUDIO)\n    {\n      g_debug (\"Allowing pulseaudio access\");\n      flatpak_run_add_pulseaudio_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_PCSC)\n    {\n      flatpak_run_add_pcsc_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_CUPS)\n    {\n      flatpak_run_add_cups_args (bwrap);\n    }\n\n  flatpak_run_add_session_dbus_args (bwrap, proxy_arg_bwrap, context, flags, app_id);\n  flatpak_run_add_system_dbus_args (bwrap, proxy_arg_bwrap, context, flags);\n  flatpak_run_add_a11y_dbus_args (bwrap, proxy_arg_bwrap, context, flags);\n\n  if (g_environ_getenv (bwrap->envp, \"LD_LIBRARY_PATH\") != NULL)\n    {\n      /* LD_LIBRARY_PATH is overridden for setuid helper, so pass it as cmdline arg */\n      flatpak_bwrap_add_args (bwrap,\n                              \"--setenv\", \"LD_LIBRARY_PATH\", g_environ_getenv (bwrap->envp, \"LD_LIBRARY_PATH\"),\n                              NULL);\n      flatpak_bwrap_unset_env (bwrap, \"LD_LIBRARY_PATH\");\n    }\n\n  if (g_environ_getenv (bwrap->envp, \"TMPDIR\") != NULL)\n    {\n      /* TMPDIR is overridden for setuid helper, so pass it as cmdline arg */\n      flatpak_bwrap_add_args (bwrap,\n                              \"--setenv\", \"TMPDIR\", g_environ_getenv (bwrap->envp, \"TMPDIR\"),\n                              NULL);\n      flatpak_bwrap_unset_env (bwrap, \"TMPDIR\");\n    }\n\n  /* Must run this before spawning the dbus proxy, to ensure it\n     ends up in the app cgroup */\n  if (!flatpak_run_in_transient_unit (app_id, &my_error))\n    {\n      /* We still run along even if we don't get a cgroup, as nothing\n         really depends on it. Its just nice to have */\n      g_debug (\"Failed to run in transient scope: %s\", my_error->message);\n      g_clear_error (&my_error);\n    }\n\n  if (!flatpak_bwrap_is_empty (proxy_arg_bwrap) &&\n      !start_dbus_proxy (bwrap, proxy_arg_bwrap, app_info_path, error))\n    return FALSE;\n\n  if (exports_out)\n    *exports_out = g_steal_pointer (&exports);\n\n  return TRUE;\n}",
        "func_after": "gboolean\nflatpak_run_add_environment_args (FlatpakBwrap    *bwrap,\n                                  const char      *app_info_path,\n                                  FlatpakRunFlags  flags,\n                                  const char      *app_id,\n                                  FlatpakContext  *context,\n                                  GFile           *app_id_dir,\n                                  GPtrArray       *previous_app_id_dirs,\n                                  FlatpakExports **exports_out,\n                                  GCancellable    *cancellable,\n                                  GError         **error)\n{\n  g_autoptr(GError) my_error = NULL;\n  g_autoptr(FlatpakExports) exports = NULL;\n  g_autoptr(FlatpakBwrap) proxy_arg_bwrap = flatpak_bwrap_new (flatpak_bwrap_empty_env);\n  gboolean has_wayland = FALSE;\n  gboolean allow_x11 = FALSE;\n\n  if ((context->shares & FLATPAK_CONTEXT_SHARED_IPC) == 0)\n    {\n      g_debug (\"Disallowing ipc access\");\n      flatpak_bwrap_add_args (bwrap, \"--unshare-ipc\", NULL);\n    }\n\n  if ((context->shares & FLATPAK_CONTEXT_SHARED_NETWORK) == 0)\n    {\n      g_debug (\"Disallowing network access\");\n      flatpak_bwrap_add_args (bwrap, \"--unshare-net\", NULL);\n    }\n\n  if (context->devices & FLATPAK_CONTEXT_DEVICE_ALL)\n    {\n      flatpak_bwrap_add_args (bwrap,\n                              \"--dev-bind\", \"/dev\", \"/dev\",\n                              NULL);\n      /* Don't expose the host /dev/shm, just the device nodes, unless explicitly allowed */\n      if (g_file_test (\"/dev/shm\", G_FILE_TEST_IS_DIR))\n        {\n          if ((context->devices & FLATPAK_CONTEXT_DEVICE_SHM) == 0)\n            flatpak_bwrap_add_args (bwrap,\n                                    \"--tmpfs\", \"/dev/shm\",\n                                    NULL);\n        }\n      else if (g_file_test (\"/dev/shm\", G_FILE_TEST_IS_SYMLINK))\n        {\n          g_autofree char *link = flatpak_readlink (\"/dev/shm\", NULL);\n\n          /* On debian (with sysv init) the host /dev/shm is a symlink to /run/shm, so we can't\n             mount on top of it. */\n          if (g_strcmp0 (link, \"/run/shm\") == 0)\n            {\n              if (context->devices & FLATPAK_CONTEXT_DEVICE_SHM &&\n                  g_file_test (\"/run/shm\", G_FILE_TEST_IS_DIR))\n                flatpak_bwrap_add_args (bwrap,\n                                        \"--bind\", \"/run/shm\", \"/run/shm\",\n                                        NULL);\n              else\n                flatpak_bwrap_add_args (bwrap,\n                                        \"--dir\", \"/run/shm\",\n                                        NULL);\n            }\n          else\n            g_warning (\"Unexpected /dev/shm symlink %s\", link);\n        }\n    }\n  else\n    {\n      flatpak_bwrap_add_args (bwrap,\n                              \"--dev\", \"/dev\",\n                              NULL);\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_DRI)\n        {\n          g_debug (\"Allowing dri access\");\n          int i;\n          char *dri_devices[] = {\n            \"/dev/dri\",\n            /* mali */\n            \"/dev/mali\",\n            \"/dev/mali0\",\n            \"/dev/umplock\",\n            /* nvidia */\n            \"/dev/nvidiactl\",\n            \"/dev/nvidia-modeset\",\n            /* nvidia OpenCL/CUDA */\n            \"/dev/nvidia-uvm\",\n            \"/dev/nvidia-uvm-tools\",\n          };\n\n          for (i = 0; i < G_N_ELEMENTS (dri_devices); i++)\n            {\n              if (g_file_test (dri_devices[i], G_FILE_TEST_EXISTS))\n                flatpak_bwrap_add_args (bwrap, \"--dev-bind\", dri_devices[i], dri_devices[i], NULL);\n            }\n\n          /* Each Nvidia card gets its own device.\n             This is a fairly arbitrary limit but ASUS sells mining boards supporting 20 in theory. */\n          char nvidia_dev[14]; /* /dev/nvidia plus up to 2 digits */\n          for (i = 0; i < 20; i++)\n            {\n              g_snprintf (nvidia_dev, sizeof (nvidia_dev), \"/dev/nvidia%d\", i);\n              if (g_file_test (nvidia_dev, G_FILE_TEST_EXISTS))\n                flatpak_bwrap_add_args (bwrap, \"--dev-bind\", nvidia_dev, nvidia_dev, NULL);\n            }\n        }\n\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_KVM)\n        {\n          g_debug (\"Allowing kvm access\");\n          if (g_file_test (\"/dev/kvm\", G_FILE_TEST_EXISTS))\n            flatpak_bwrap_add_args (bwrap, \"--dev-bind\", \"/dev/kvm\", \"/dev/kvm\", NULL);\n        }\n\n      if (context->devices & FLATPAK_CONTEXT_DEVICE_SHM)\n        {\n          /* This is a symlink to /run/shm on debian, so bind to real target */\n          g_autofree char *real_dev_shm = realpath (\"/dev/shm\", NULL);\n\n          g_debug (\"Allowing /dev/shm access (as %s)\", real_dev_shm);\n          if (real_dev_shm != NULL)\n              flatpak_bwrap_add_args (bwrap, \"--bind\", real_dev_shm, \"/dev/shm\", NULL);\n        }\n    }\n\n  flatpak_context_append_bwrap_filesystem (context, bwrap, app_id, app_id_dir, previous_app_id_dirs, &exports);\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_WAYLAND)\n    {\n      g_debug (\"Allowing wayland access\");\n      has_wayland = flatpak_run_add_wayland_args (bwrap);\n    }\n\n  if ((context->sockets & FLATPAK_CONTEXT_SOCKET_FALLBACK_X11) != 0)\n    allow_x11 = !has_wayland;\n  else\n    allow_x11 = (context->sockets & FLATPAK_CONTEXT_SOCKET_X11) != 0;\n\n  flatpak_run_add_x11_args (bwrap, allow_x11);\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_SSH_AUTH)\n    {\n      flatpak_run_add_ssh_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_PULSEAUDIO)\n    {\n      g_debug (\"Allowing pulseaudio access\");\n      flatpak_run_add_pulseaudio_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_PCSC)\n    {\n      flatpak_run_add_pcsc_args (bwrap);\n    }\n\n  if (context->sockets & FLATPAK_CONTEXT_SOCKET_CUPS)\n    {\n      flatpak_run_add_cups_args (bwrap);\n    }\n\n  flatpak_run_add_session_dbus_args (bwrap, proxy_arg_bwrap, context, flags, app_id);\n  flatpak_run_add_system_dbus_args (bwrap, proxy_arg_bwrap, context, flags);\n  flatpak_run_add_a11y_dbus_args (bwrap, proxy_arg_bwrap, context, flags);\n\n  /* Must run this before spawning the dbus proxy, to ensure it\n     ends up in the app cgroup */\n  if (!flatpak_run_in_transient_unit (app_id, &my_error))\n    {\n      /* We still run along even if we don't get a cgroup, as nothing\n         really depends on it. Its just nice to have */\n      g_debug (\"Failed to run in transient scope: %s\", my_error->message);\n      g_clear_error (&my_error);\n    }\n\n  if (!flatpak_bwrap_is_empty (proxy_arg_bwrap) &&\n      !start_dbus_proxy (bwrap, proxy_arg_bwrap, app_info_path, error))\n    return FALSE;\n\n  if (exports_out)\n    *exports_out = g_steal_pointer (&exports);\n\n  return TRUE;\n}",
        "description": "A vulnerability was identified in the Flatpak portal service (`flatpak-portal`), allowing sandboxed applications to execute arbitrary code on the host system, effectively enabling a sandbox escape. This issue affects versions of Flatpak from 0.11.4 up to but not including fixed versions 1.8.5 and 1.10.0. The Flatpak portal service facilitates launching subprocesses within new sandbox instances, either with the same or more restrictive security settings compared to the calling application. Vulnerable versions pass caller-specified environment variables to non-sandboxed processes on the host system, particularly to the `flatpak run` command used for launching new sandbox instances. Malicious or compromised applications could exploit this by setting environment variables trusted by the `flatpak run` command, thereby executing arbitrary code outside the sandbox. As a temporary workaround, disabling the `flatpak-portal` service mitigates the vulnerability but may prevent many Flatpak applications from functioning correctly. This issue has been resolved in versions 1.8.5 and 1.10.0.",
        "commit": "The vulnerability involves converting all environment variables into arguments for `bwrap`, a tool used to create isolated execution environments. This conversion helps prevent certain environment variables from being filtered out by a setuid `bwrap`. Additionally, it ensures that even if these variables come from an untrusted source, they cannot be used to inject arbitrary code into a non-setuid `bwrap` through mechanisms like `LD_PRELOAD`. By bundling these variables into a `memfd` or temporary file, they are not included in `argv`, making them inaccessible to processes running under a different user ID. This measure is crucial for protecting sensitive information such as tokens or other secrets."
    }
]