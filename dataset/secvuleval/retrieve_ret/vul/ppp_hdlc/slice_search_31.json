[
    {
        "cwe": "CWE-770",
        "func_name": "binutils-gdb/_bfd_elf_slurp_version_tables",
        "score": 0.7576576471328735,
        "func_before": "bfd_boolean\n_bfd_elf_slurp_version_tables (bfd *abfd, bfd_boolean default_imported_symver)\n{\n  bfd_byte *contents = NULL;\n  unsigned int freeidx = 0;\n\n  if (elf_dynverref (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verneed *everneed;\n      Elf_Internal_Verneed *iverneed;\n      unsigned int i;\n      bfd_byte *contents_end;\n\n      hdr = &elf_tdata (abfd)->dynverref_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verneed))\n\t{\nerror_return_bad_verref:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_r invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\nerror_return_verref:\n\t  elf_tdata (abfd)->verref = NULL;\n\t  elf_tdata (abfd)->cverrefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verref;\n\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verref;\n\n      elf_tdata (abfd)->verref = (Elf_Internal_Verneed *)\n\tbfd_zalloc2 (abfd, hdr->sh_info, sizeof (Elf_Internal_Verneed));\n\n      if (elf_tdata (abfd)->verref == NULL)\n\tgoto error_return_verref;\n\n      BFD_ASSERT (sizeof (Elf_External_Verneed)\n\t\t  == sizeof (Elf_External_Vernaux));\n      contents_end = contents + hdr->sh_size - sizeof (Elf_External_Verneed);\n      everneed = (Elf_External_Verneed *) contents;\n      iverneed = elf_tdata (abfd)->verref;\n      for (i = 0; i < hdr->sh_info; i++, iverneed++)\n\t{\n\t  Elf_External_Vernaux *evernaux;\n\t  Elf_Internal_Vernaux *ivernaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verneed_in (abfd, everneed, iverneed);\n\n\t  iverneed->vn_bfd = abfd;\n\n\t  iverneed->vn_filename =\n\t    bfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t     iverneed->vn_file);\n\t  if (iverneed->vn_filename == NULL)\n\t    goto error_return_bad_verref;\n\n\t  if (iverneed->vn_cnt == 0)\n\t    iverneed->vn_auxptr = NULL;\n\t  else\n\t    {\n\t      iverneed->vn_auxptr = (struct elf_internal_vernaux *)\n                  bfd_alloc2 (abfd, iverneed->vn_cnt,\n                              sizeof (Elf_Internal_Vernaux));\n\t      if (iverneed->vn_auxptr == NULL)\n\t\tgoto error_return_verref;\n\t    }\n\n\t  if (iverneed->vn_aux\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  evernaux = ((Elf_External_Vernaux *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_aux));\n\t  ivernaux = iverneed->vn_auxptr;\n\t  for (j = 0; j < iverneed->vn_cnt; j++, ivernaux++)\n\t    {\n\t      _bfd_elf_swap_vernaux_in (abfd, evernaux, ivernaux);\n\n\t      ivernaux->vna_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t ivernaux->vna_name);\n\t      if (ivernaux->vna_nodename == NULL)\n\t\tgoto error_return_bad_verref;\n\n\t      if (ivernaux->vna_other > freeidx)\n\t\tfreeidx = ivernaux->vna_other;\n\n\t      ivernaux->vna_nextptr = NULL;\n\t      if (ivernaux->vna_next == 0)\n\t\t{\n\t\t  iverneed->vn_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverneed->vn_cnt)\n\t\tivernaux->vna_nextptr = ivernaux + 1;\n\n\t      if (ivernaux->vna_next\n\t\t  > (size_t) (contents_end - (bfd_byte *) evernaux))\n\t\tgoto error_return_bad_verref;\n\n\t      evernaux = ((Elf_External_Vernaux *)\n\t\t\t  ((bfd_byte *) evernaux + ivernaux->vna_next));\n\t    }\n\n\t  iverneed->vn_nextref = NULL;\n\t  if (iverneed->vn_next == 0)\n\t    break;\n\t  if (i + 1 < hdr->sh_info)\n\t    iverneed->vn_nextref = iverneed + 1;\n\n\t  if (iverneed->vn_next\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  everneed = ((Elf_External_Verneed *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_next));\n\t}\n      elf_tdata (abfd)->cverrefs = i;\n\n      free (contents);\n      contents = NULL;\n    }\n\n  if (elf_dynverdef (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verdef *everdef;\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdef *iverdefarr;\n      Elf_Internal_Verdef iverdefmem;\n      unsigned int i;\n      unsigned int maxidx;\n      bfd_byte *contents_end_def, *contents_end_aux;\n\n      hdr = &elf_tdata (abfd)->dynverdef_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verdef))\n\t{\n\terror_return_bad_verdef:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_d invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\n\terror_return_verdef:\n\t  elf_tdata (abfd)->verdef = NULL;\n\t  elf_tdata (abfd)->cverdefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verdef;\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verdef;\n\n      BFD_ASSERT (sizeof (Elf_External_Verdef)\n\t\t  >= sizeof (Elf_External_Verdaux));\n      contents_end_def = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdef);\n      contents_end_aux = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdaux);\n\n      /* We know the number of entries in the section but not the maximum\n\t index.  Therefore we have to run through all entries and find\n\t the maximum.  */\n      everdef = (Elf_External_Verdef *) contents;\n      maxidx = 0;\n      for (i = 0; i < hdr->sh_info; ++i)\n\t{\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) == 0)\n\t    goto error_return_bad_verdef;\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) > maxidx)\n\t    maxidx = iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION);\n\n\t  if (iverdefmem.vd_next == 0)\n\t    break;\n\n\t  if (iverdefmem.vd_next\n\t      > (size_t) (contents_end_def - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdefmem.vd_next));\n\t}\n\n      if (default_imported_symver)\n\t{\n\t  if (freeidx > maxidx)\n\t    maxidx = ++freeidx;\n\t  else\n\t    freeidx = ++maxidx;\n\t}\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n\tbfd_zalloc2 (abfd, maxidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return_verdef;\n\n      elf_tdata (abfd)->cverdefs = maxidx;\n\n      everdef = (Elf_External_Verdef *) contents;\n      iverdefarr = elf_tdata (abfd)->verdef;\n      for (i = 0; i < hdr->sh_info; i++)\n\t{\n\t  Elf_External_Verdaux *everdaux;\n\t  Elf_Internal_Verdaux *iverdaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & VERSYM_VERSION) == 0)\n\t    goto error_return_bad_verdef;\n\n\t  iverdef = &iverdefarr[(iverdefmem.vd_ndx & VERSYM_VERSION) - 1];\n\t  memcpy (iverdef, &iverdefmem, offsetof (Elf_Internal_Verdef, vd_bfd));\n\n\t  iverdef->vd_bfd = abfd;\n\n\t  if (iverdef->vd_cnt == 0)\n\t    iverdef->vd_auxptr = NULL;\n\t  else\n\t    {\n\t      iverdef->vd_auxptr = (struct elf_internal_verdaux *)\n                  bfd_alloc2 (abfd, iverdef->vd_cnt,\n                              sizeof (Elf_Internal_Verdaux));\n\t      if (iverdef->vd_auxptr == NULL)\n\t\tgoto error_return_verdef;\n\t    }\n\n\t  if (iverdef->vd_aux\n\t      > (size_t) (contents_end_aux - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdaux = ((Elf_External_Verdaux *)\n\t\t      ((bfd_byte *) everdef + iverdef->vd_aux));\n\t  iverdaux = iverdef->vd_auxptr;\n\t  for (j = 0; j < iverdef->vd_cnt; j++, iverdaux++)\n\t    {\n\t      _bfd_elf_swap_verdaux_in (abfd, everdaux, iverdaux);\n\n\t      iverdaux->vda_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t iverdaux->vda_name);\n\t      if (iverdaux->vda_nodename == NULL)\n\t\tgoto error_return_bad_verdef;\n\n\t      iverdaux->vda_nextptr = NULL;\n\t      if (iverdaux->vda_next == 0)\n\t\t{\n\t\t  iverdef->vd_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverdef->vd_cnt)\n\t\tiverdaux->vda_nextptr = iverdaux + 1;\n\n\t      if (iverdaux->vda_next\n\t\t  > (size_t) (contents_end_aux - (bfd_byte *) everdaux))\n\t\tgoto error_return_bad_verdef;\n\n\t      everdaux = ((Elf_External_Verdaux *)\n\t\t\t  ((bfd_byte *) everdaux + iverdaux->vda_next));\n\t    }\n\n\t  iverdef->vd_nodename = NULL;\n\t  if (iverdef->vd_cnt)\n\t    iverdef->vd_nodename = iverdef->vd_auxptr->vda_nodename;\n\n\t  iverdef->vd_nextdef = NULL;\n\t  if (iverdef->vd_next == 0)\n\t    break;\n\t  if ((size_t) (iverdef - iverdefarr) + 1 < maxidx)\n\t    iverdef->vd_nextdef = iverdef + 1;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdef->vd_next));\n\t}\n\n      free (contents);\n      contents = NULL;\n    }\n  else if (default_imported_symver)\n    {\n      if (freeidx < 3)\n\tfreeidx = 3;\n      else\n\tfreeidx++;\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n          bfd_zalloc2 (abfd, freeidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return;\n\n      elf_tdata (abfd)->cverdefs = freeidx;\n    }\n\n  /* Create a default version based on the soname.  */\n  if (default_imported_symver)\n    {\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdaux *iverdaux;\n\n      iverdef = &elf_tdata (abfd)->verdef[freeidx - 1];\n\n      iverdef->vd_version = VER_DEF_CURRENT;\n      iverdef->vd_flags = 0;\n      iverdef->vd_ndx = freeidx;\n      iverdef->vd_cnt = 1;\n\n      iverdef->vd_bfd = abfd;\n\n      iverdef->vd_nodename = bfd_elf_get_dt_soname (abfd);\n      if (iverdef->vd_nodename == NULL)\n\tgoto error_return_verdef;\n      iverdef->vd_nextdef = NULL;\n      iverdef->vd_auxptr = ((struct elf_internal_verdaux *)\n\t\t\t    bfd_zalloc (abfd, sizeof (Elf_Internal_Verdaux)));\n      if (iverdef->vd_auxptr == NULL)\n\tgoto error_return_verdef;\n\n      iverdaux = iverdef->vd_auxptr;\n      iverdaux->vda_nodename = iverdef->vd_nodename;\n    }\n\n  return TRUE;\n\n error_return:\n  if (contents != NULL)\n    free (contents);\n  return FALSE;\n}",
        "func_after": "bfd_boolean\n_bfd_elf_slurp_version_tables (bfd *abfd, bfd_boolean default_imported_symver)\n{\n  bfd_byte *contents = NULL;\n  unsigned int freeidx = 0;\n\n  if (elf_dynverref (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verneed *everneed;\n      Elf_Internal_Verneed *iverneed;\n      unsigned int i;\n      bfd_byte *contents_end;\n\n      hdr = &elf_tdata (abfd)->dynverref_hdr;\n\n      if (hdr->sh_info == 0\n\t  || hdr->sh_info > hdr->sh_size / sizeof (Elf_External_Verneed))\n\t{\nerror_return_bad_verref:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_r invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\nerror_return_verref:\n\t  elf_tdata (abfd)->verref = NULL;\n\t  elf_tdata (abfd)->cverrefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verref;\n\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verref;\n\n      elf_tdata (abfd)->verref = (Elf_Internal_Verneed *)\n\tbfd_alloc2 (abfd, hdr->sh_info, sizeof (Elf_Internal_Verneed));\n\n      if (elf_tdata (abfd)->verref == NULL)\n\tgoto error_return_verref;\n\n      BFD_ASSERT (sizeof (Elf_External_Verneed)\n\t\t  == sizeof (Elf_External_Vernaux));\n      contents_end = contents + hdr->sh_size - sizeof (Elf_External_Verneed);\n      everneed = (Elf_External_Verneed *) contents;\n      iverneed = elf_tdata (abfd)->verref;\n      for (i = 0; i < hdr->sh_info; i++, iverneed++)\n\t{\n\t  Elf_External_Vernaux *evernaux;\n\t  Elf_Internal_Vernaux *ivernaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verneed_in (abfd, everneed, iverneed);\n\n\t  iverneed->vn_bfd = abfd;\n\n\t  iverneed->vn_filename =\n\t    bfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t     iverneed->vn_file);\n\t  if (iverneed->vn_filename == NULL)\n\t    goto error_return_bad_verref;\n\n\t  if (iverneed->vn_cnt == 0)\n\t    iverneed->vn_auxptr = NULL;\n\t  else\n\t    {\n\t      iverneed->vn_auxptr = (struct elf_internal_vernaux *)\n                  bfd_alloc2 (abfd, iverneed->vn_cnt,\n                              sizeof (Elf_Internal_Vernaux));\n\t      if (iverneed->vn_auxptr == NULL)\n\t\tgoto error_return_verref;\n\t    }\n\n\t  if (iverneed->vn_aux\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  evernaux = ((Elf_External_Vernaux *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_aux));\n\t  ivernaux = iverneed->vn_auxptr;\n\t  for (j = 0; j < iverneed->vn_cnt; j++, ivernaux++)\n\t    {\n\t      _bfd_elf_swap_vernaux_in (abfd, evernaux, ivernaux);\n\n\t      ivernaux->vna_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t ivernaux->vna_name);\n\t      if (ivernaux->vna_nodename == NULL)\n\t\tgoto error_return_bad_verref;\n\n\t      if (ivernaux->vna_other > freeidx)\n\t\tfreeidx = ivernaux->vna_other;\n\n\t      ivernaux->vna_nextptr = NULL;\n\t      if (ivernaux->vna_next == 0)\n\t\t{\n\t\t  iverneed->vn_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverneed->vn_cnt)\n\t\tivernaux->vna_nextptr = ivernaux + 1;\n\n\t      if (ivernaux->vna_next\n\t\t  > (size_t) (contents_end - (bfd_byte *) evernaux))\n\t\tgoto error_return_bad_verref;\n\n\t      evernaux = ((Elf_External_Vernaux *)\n\t\t\t  ((bfd_byte *) evernaux + ivernaux->vna_next));\n\t    }\n\n\t  iverneed->vn_nextref = NULL;\n\t  if (iverneed->vn_next == 0)\n\t    break;\n\t  if (i + 1 < hdr->sh_info)\n\t    iverneed->vn_nextref = iverneed + 1;\n\n\t  if (iverneed->vn_next\n\t      > (size_t) (contents_end - (bfd_byte *) everneed))\n\t    goto error_return_bad_verref;\n\n\t  everneed = ((Elf_External_Verneed *)\n\t\t      ((bfd_byte *) everneed + iverneed->vn_next));\n\t}\n      elf_tdata (abfd)->cverrefs = i;\n\n      free (contents);\n      contents = NULL;\n    }\n\n  if (elf_dynverdef (abfd) != 0)\n    {\n      Elf_Internal_Shdr *hdr;\n      Elf_External_Verdef *everdef;\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdef *iverdefarr;\n      Elf_Internal_Verdef iverdefmem;\n      unsigned int i;\n      unsigned int maxidx;\n      bfd_byte *contents_end_def, *contents_end_aux;\n\n      hdr = &elf_tdata (abfd)->dynverdef_hdr;\n\n      if (hdr->sh_info == 0 || hdr->sh_size < sizeof (Elf_External_Verdef))\n\t{\n\terror_return_bad_verdef:\n\t  _bfd_error_handler\n\t    (_(\"%B: .gnu.version_d invalid entry\"), abfd);\n\t  bfd_set_error (bfd_error_bad_value);\n\terror_return_verdef:\n\t  elf_tdata (abfd)->verdef = NULL;\n\t  elf_tdata (abfd)->cverdefs = 0;\n\t  goto error_return;\n\t}\n\n      contents = (bfd_byte *) bfd_malloc (hdr->sh_size);\n      if (contents == NULL)\n\tgoto error_return_verdef;\n      if (bfd_seek (abfd, hdr->sh_offset, SEEK_SET) != 0\n\t  || bfd_bread (contents, hdr->sh_size, abfd) != hdr->sh_size)\n\tgoto error_return_verdef;\n\n      BFD_ASSERT (sizeof (Elf_External_Verdef)\n\t\t  >= sizeof (Elf_External_Verdaux));\n      contents_end_def = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdef);\n      contents_end_aux = contents + hdr->sh_size\n\t\t\t - sizeof (Elf_External_Verdaux);\n\n      /* We know the number of entries in the section but not the maximum\n\t index.  Therefore we have to run through all entries and find\n\t the maximum.  */\n      everdef = (Elf_External_Verdef *) contents;\n      maxidx = 0;\n      for (i = 0; i < hdr->sh_info; ++i)\n\t{\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) == 0)\n\t    goto error_return_bad_verdef;\n\t  if ((iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION)) > maxidx)\n\t    maxidx = iverdefmem.vd_ndx & ((unsigned) VERSYM_VERSION);\n\n\t  if (iverdefmem.vd_next == 0)\n\t    break;\n\n\t  if (iverdefmem.vd_next\n\t      > (size_t) (contents_end_def - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdefmem.vd_next));\n\t}\n\n      if (default_imported_symver)\n\t{\n\t  if (freeidx > maxidx)\n\t    maxidx = ++freeidx;\n\t  else\n\t    freeidx = ++maxidx;\n\t}\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n\tbfd_zalloc2 (abfd, maxidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return_verdef;\n\n      elf_tdata (abfd)->cverdefs = maxidx;\n\n      everdef = (Elf_External_Verdef *) contents;\n      iverdefarr = elf_tdata (abfd)->verdef;\n      for (i = 0; i < hdr->sh_info; i++)\n\t{\n\t  Elf_External_Verdaux *everdaux;\n\t  Elf_Internal_Verdaux *iverdaux;\n\t  unsigned int j;\n\n\t  _bfd_elf_swap_verdef_in (abfd, everdef, &iverdefmem);\n\n\t  if ((iverdefmem.vd_ndx & VERSYM_VERSION) == 0)\n\t    goto error_return_bad_verdef;\n\n\t  iverdef = &iverdefarr[(iverdefmem.vd_ndx & VERSYM_VERSION) - 1];\n\t  memcpy (iverdef, &iverdefmem, offsetof (Elf_Internal_Verdef, vd_bfd));\n\n\t  iverdef->vd_bfd = abfd;\n\n\t  if (iverdef->vd_cnt == 0)\n\t    iverdef->vd_auxptr = NULL;\n\t  else\n\t    {\n\t      iverdef->vd_auxptr = (struct elf_internal_verdaux *)\n                  bfd_alloc2 (abfd, iverdef->vd_cnt,\n                              sizeof (Elf_Internal_Verdaux));\n\t      if (iverdef->vd_auxptr == NULL)\n\t\tgoto error_return_verdef;\n\t    }\n\n\t  if (iverdef->vd_aux\n\t      > (size_t) (contents_end_aux - (bfd_byte *) everdef))\n\t    goto error_return_bad_verdef;\n\n\t  everdaux = ((Elf_External_Verdaux *)\n\t\t      ((bfd_byte *) everdef + iverdef->vd_aux));\n\t  iverdaux = iverdef->vd_auxptr;\n\t  for (j = 0; j < iverdef->vd_cnt; j++, iverdaux++)\n\t    {\n\t      _bfd_elf_swap_verdaux_in (abfd, everdaux, iverdaux);\n\n\t      iverdaux->vda_nodename =\n\t\tbfd_elf_string_from_elf_section (abfd, hdr->sh_link,\n\t\t\t\t\t\t iverdaux->vda_name);\n\t      if (iverdaux->vda_nodename == NULL)\n\t\tgoto error_return_bad_verdef;\n\n\t      iverdaux->vda_nextptr = NULL;\n\t      if (iverdaux->vda_next == 0)\n\t\t{\n\t\t  iverdef->vd_cnt = j + 1;\n\t\t  break;\n\t\t}\n\t      if (j + 1 < iverdef->vd_cnt)\n\t\tiverdaux->vda_nextptr = iverdaux + 1;\n\n\t      if (iverdaux->vda_next\n\t\t  > (size_t) (contents_end_aux - (bfd_byte *) everdaux))\n\t\tgoto error_return_bad_verdef;\n\n\t      everdaux = ((Elf_External_Verdaux *)\n\t\t\t  ((bfd_byte *) everdaux + iverdaux->vda_next));\n\t    }\n\n\t  iverdef->vd_nodename = NULL;\n\t  if (iverdef->vd_cnt)\n\t    iverdef->vd_nodename = iverdef->vd_auxptr->vda_nodename;\n\n\t  iverdef->vd_nextdef = NULL;\n\t  if (iverdef->vd_next == 0)\n\t    break;\n\t  if ((size_t) (iverdef - iverdefarr) + 1 < maxidx)\n\t    iverdef->vd_nextdef = iverdef + 1;\n\n\t  everdef = ((Elf_External_Verdef *)\n\t\t     ((bfd_byte *) everdef + iverdef->vd_next));\n\t}\n\n      free (contents);\n      contents = NULL;\n    }\n  else if (default_imported_symver)\n    {\n      if (freeidx < 3)\n\tfreeidx = 3;\n      else\n\tfreeidx++;\n\n      elf_tdata (abfd)->verdef = (Elf_Internal_Verdef *)\n          bfd_zalloc2 (abfd, freeidx, sizeof (Elf_Internal_Verdef));\n      if (elf_tdata (abfd)->verdef == NULL)\n\tgoto error_return;\n\n      elf_tdata (abfd)->cverdefs = freeidx;\n    }\n\n  /* Create a default version based on the soname.  */\n  if (default_imported_symver)\n    {\n      Elf_Internal_Verdef *iverdef;\n      Elf_Internal_Verdaux *iverdaux;\n\n      iverdef = &elf_tdata (abfd)->verdef[freeidx - 1];\n\n      iverdef->vd_version = VER_DEF_CURRENT;\n      iverdef->vd_flags = 0;\n      iverdef->vd_ndx = freeidx;\n      iverdef->vd_cnt = 1;\n\n      iverdef->vd_bfd = abfd;\n\n      iverdef->vd_nodename = bfd_elf_get_dt_soname (abfd);\n      if (iverdef->vd_nodename == NULL)\n\tgoto error_return_verdef;\n      iverdef->vd_nextdef = NULL;\n      iverdef->vd_auxptr = ((struct elf_internal_verdaux *)\n\t\t\t    bfd_zalloc (abfd, sizeof (Elf_Internal_Verdaux)));\n      if (iverdef->vd_auxptr == NULL)\n\tgoto error_return_verdef;\n\n      iverdaux = iverdef->vd_auxptr;\n      iverdaux->vda_nodename = iverdef->vd_nodename;\n    }\n\n  return TRUE;\n\n error_return:\n  if (contents != NULL)\n    free (contents);\n  return FALSE;\n}",
        "description": "The _bfd_elf_slurp_version_tables function in the BFD library, part of GNU Binutils 2.29, is susceptible to a denial of service attack. This vulnerability arises from excessive memory allocation when processing a specially crafted ELF file, leading to potential application crashes.",
        "commit": "The vulnerability involves a memory allocation issue within the ELF (Executable and Linkable Format) handling module of a binary file format library. Specifically, the code performs a sanity check on the size of the `SHT_GNU_verneed` section, ensuring it meets certain minimum requirements related to the number of version entries (`sh_info`). Additionally, since the code either fully initializes all fields of the version entries or exits with an error, there is no need to zero-initialize the allocated memory for these entries. The fix includes modifying the `_bfd_elf_slurp_version_tables` function to test the `sh_info` field of the `SHT_GNU_verneed` section for validity and to avoid zero-initializing the memory allocated for version references."
    },
    {
        "cwe": "CWE-125",
        "func_name": "the-tcpdump-group/nfsreq_print_noaddr",
        "score": 0.7667955756187439,
        "func_before": "void\nnfsreq_print_noaddr(netdissect_options *ndo,\n                    register const u_char *bp, u_int length,\n                    register const u_char *bp2)\n{\n\tregister const struct sunrpc_msg *rp;\n\tregister const uint32_t *dp;\n\tnfs_type type;\n\tint v3;\n\tuint32_t proc;\n\tuint32_t access_flags;\n\tstruct nfsv3_sattr sa3;\n\n\tND_PRINT((ndo, \"%d\", length));\n\tnfserr = 0;\t\t/* assume no error */\n\trp = (const struct sunrpc_msg *)bp;\n\n\tif (!xid_map_enter(ndo, rp, bp2))\t/* record proc number for later on */\n\t\tgoto trunc;\n\n\tv3 = (EXTRACT_32BITS(&rp->rm_call.cb_vers) == NFS_VER3);\n\tproc = EXTRACT_32BITS(&rp->rm_call.cb_proc);\n\n\tif (!v3 && proc < NFS_NPROCS)\n\t\tproc =  nfsv3_procid[proc];\n\n\tND_PRINT((ndo, \" %s\", tok2str(nfsproc_str, \"proc-%u\", proc)));\n\tswitch (proc) {\n\n\tcase NFSPROC_GETATTR:\n\tcase NFSPROC_SETATTR:\n\tcase NFSPROC_READLINK:\n\tcase NFSPROC_FSSTAT:\n\tcase NFSPROC_FSINFO:\n\tcase NFSPROC_PATHCONF:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    parsefh(ndo, dp, v3) != NULL)\n\t\t\treturn;\n\t\tbreak;\n\n\tcase NFSPROC_LOOKUP:\n\tcase NFSPROC_CREATE:\n\tcase NFSPROC_MKDIR:\n\tcase NFSPROC_REMOVE:\n\tcase NFSPROC_RMDIR:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    parsefhn(ndo, dp, v3) != NULL)\n\t\t\treturn;\n\t\tbreak;\n\n\tcase NFSPROC_ACCESS:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[0]);\n\t\t\taccess_flags = EXTRACT_32BITS(&dp[0]);\n\t\t\tif (access_flags & ~NFSV3ACCESS_FULL) {\n\t\t\t\t/* NFSV3ACCESS definitions aren't up to date */\n\t\t\t\tND_PRINT((ndo, \" %04x\", access_flags));\n\t\t\t} else if ((access_flags & NFSV3ACCESS_FULL) == NFSV3ACCESS_FULL) {\n\t\t\t\tND_PRINT((ndo, \" NFS_ACCESS_FULL\"));\n\t\t\t} else {\n\t\t\t\tchar separator = ' ';\n\t\t\t\tif (access_flags & NFSV3ACCESS_READ) {\n\t\t\t\t\tND_PRINT((ndo, \" NFS_ACCESS_READ\"));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_LOOKUP) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_LOOKUP\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_MODIFY) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_MODIFY\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_EXTEND) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_EXTEND\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_DELETE) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_DELETE\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_EXECUTE)\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_EXECUTE\", separator));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READ:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[2]);\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRIu64,\n\t\t\t\t       EXTRACT_32BITS(&dp[2]),\n\t\t\t\t       EXTRACT_64BITS(&dp[0])));\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %u\",\n\t\t\t\t    EXTRACT_32BITS(&dp[1]),\n\t\t\t\t    EXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_WRITE:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[2]);\n\t\t\t\tND_PRINT((ndo, \" %u (%u) bytes @ %\" PRIu64,\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[4]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\t\tif (ndo->ndo_vflag) {\n\t\t\t\t\tdp += 3;\n\t\t\t\t\tND_TCHECK(dp[0]);\n\t\t\t\t\tND_PRINT((ndo, \" <%s>\",\n\t\t\t\t\t\ttok2str(nfsv3_writemodes,\n\t\t\t\t\t\t\tNULL, EXTRACT_32BITS(dp))));\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[3]);\n\t\t\t\tND_PRINT((ndo, \" %u (%u) bytes @ %u (%u)\",\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[3]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[1]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_SYMLINK:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (v3 && (dp = parse_sattr3(ndo, dp, &sa3)) == NULL)\n\t\t\t\tbreak;\n\t\t\tif (parsefn(ndo, dp) == NULL)\n\t\t\t\tbreak;\n\t\t\tif (v3 && ndo->ndo_vflag)\n\t\t\t\tprint_sattr3(ndo, &sa3, ndo->ndo_vflag);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_MKNOD:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(*dp);\n\t\t\ttype = (nfs_type)EXTRACT_32BITS(dp);\n\t\t\tdp++;\n\t\t\tif ((dp = parse_sattr3(ndo, dp, &sa3)) == NULL)\n\t\t\t\tbreak;\n\t\t\tND_PRINT((ndo, \" %s\", tok2str(type2str, \"unk-ft %d\", type)));\n\t\t\tif (ndo->ndo_vflag && (type == NFCHR || type == NFBLK)) {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\tND_PRINT((ndo, \" %u/%u\",\n\t\t\t\t       EXTRACT_32BITS(&dp[0]),\n\t\t\t\t       EXTRACT_32BITS(&dp[1])));\n\t\t\t\tdp += 2;\n\t\t\t}\n\t\t\tif (ndo->ndo_vflag)\n\t\t\t\tprint_sattr3(ndo, &sa3, ndo->ndo_vflag);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_RENAME:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (parsefhn(ndo, dp, v3) != NULL)\n\t\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_LINK:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (parsefhn(ndo, dp, v3) != NULL)\n\t\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READDIR:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[4]);\n\t\t\t\t/*\n\t\t\t\t * We shouldn't really try to interpret the\n\t\t\t\t * offset cookie here.\n\t\t\t\t */\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRId64,\n\t\t\t\t    EXTRACT_32BITS(&dp[4]),\n\t\t\t\t    EXTRACT_64BITS(&dp[0])));\n\t\t\t\tif (ndo->ndo_vflag)\n\t\t\t\t\tND_PRINT((ndo, \" verf %08x%08x\", dp[2], dp[3]));\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\t/*\n\t\t\t\t * Print the offset as signed, since -1 is\n\t\t\t\t * common, but offsets > 2^31 aren't.\n\t\t\t\t */\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %d\",\n\t\t\t\t    EXTRACT_32BITS(&dp[1]),\n\t\t\t\t    EXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READDIRPLUS:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[4]);\n\t\t\t/*\n\t\t\t * We don't try to interpret the offset\n\t\t\t * cookie here.\n\t\t\t */\n\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRId64,\n\t\t\t\tEXTRACT_32BITS(&dp[4]),\n\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\tif (ndo->ndo_vflag) {\n\t\t\t\tND_TCHECK(dp[5]);\n\t\t\t\tND_PRINT((ndo, \" max %u verf %08x%08x\",\n\t\t\t\t       EXTRACT_32BITS(&dp[5]), dp[2], dp[3]));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_COMMIT:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[2]);\n\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRIu64,\n\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\ntrunc:\n\tif (!nfserr)\n\t\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "func_after": "void\nnfsreq_print_noaddr(netdissect_options *ndo,\n                    register const u_char *bp, u_int length,\n                    register const u_char *bp2)\n{\n\tregister const struct sunrpc_msg *rp;\n\tregister const uint32_t *dp;\n\tnfs_type type;\n\tint v3;\n\tuint32_t proc;\n\tuint32_t access_flags;\n\tstruct nfsv3_sattr sa3;\n\n\tND_PRINT((ndo, \"%d\", length));\n\tnfserr = 0;\t\t/* assume no error */\n\trp = (const struct sunrpc_msg *)bp;\n\n\tif (!xid_map_enter(ndo, rp, bp2))\t/* record proc number for later on */\n\t\tgoto trunc;\n\n\tv3 = (EXTRACT_32BITS(&rp->rm_call.cb_vers) == NFS_VER3);\n\tproc = EXTRACT_32BITS(&rp->rm_call.cb_proc);\n\n\tif (!v3 && proc < NFS_NPROCS)\n\t\tproc =  nfsv3_procid[proc];\n\n\tND_PRINT((ndo, \" %s\", tok2str(nfsproc_str, \"proc-%u\", proc)));\n\tswitch (proc) {\n\n\tcase NFSPROC_GETATTR:\n\tcase NFSPROC_SETATTR:\n\tcase NFSPROC_READLINK:\n\tcase NFSPROC_FSSTAT:\n\tcase NFSPROC_FSINFO:\n\tcase NFSPROC_PATHCONF:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    parsefh(ndo, dp, v3) != NULL)\n\t\t\treturn;\n\t\tbreak;\n\n\tcase NFSPROC_LOOKUP:\n\tcase NFSPROC_CREATE:\n\tcase NFSPROC_MKDIR:\n\tcase NFSPROC_REMOVE:\n\tcase NFSPROC_RMDIR:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    parsefhn(ndo, dp, v3) != NULL)\n\t\t\treturn;\n\t\tbreak;\n\n\tcase NFSPROC_ACCESS:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[0]);\n\t\t\taccess_flags = EXTRACT_32BITS(&dp[0]);\n\t\t\tif (access_flags & ~NFSV3ACCESS_FULL) {\n\t\t\t\t/* NFSV3ACCESS definitions aren't up to date */\n\t\t\t\tND_PRINT((ndo, \" %04x\", access_flags));\n\t\t\t} else if ((access_flags & NFSV3ACCESS_FULL) == NFSV3ACCESS_FULL) {\n\t\t\t\tND_PRINT((ndo, \" NFS_ACCESS_FULL\"));\n\t\t\t} else {\n\t\t\t\tchar separator = ' ';\n\t\t\t\tif (access_flags & NFSV3ACCESS_READ) {\n\t\t\t\t\tND_PRINT((ndo, \" NFS_ACCESS_READ\"));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_LOOKUP) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_LOOKUP\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_MODIFY) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_MODIFY\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_EXTEND) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_EXTEND\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_DELETE) {\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_DELETE\", separator));\n\t\t\t\t\tseparator = '|';\n\t\t\t\t}\n\t\t\t\tif (access_flags & NFSV3ACCESS_EXECUTE)\n\t\t\t\t\tND_PRINT((ndo, \"%cNFS_ACCESS_EXECUTE\", separator));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READ:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[2]);\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRIu64,\n\t\t\t\t       EXTRACT_32BITS(&dp[2]),\n\t\t\t\t       EXTRACT_64BITS(&dp[0])));\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %u\",\n\t\t\t\t    EXTRACT_32BITS(&dp[1]),\n\t\t\t\t    EXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_WRITE:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[4]);\n\t\t\t\tND_PRINT((ndo, \" %u (%u) bytes @ %\" PRIu64,\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[4]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\t\tif (ndo->ndo_vflag) {\n\t\t\t\t\tND_PRINT((ndo, \" <%s>\",\n\t\t\t\t\t\ttok2str(nfsv3_writemodes,\n\t\t\t\t\t\t\tNULL, EXTRACT_32BITS(&dp[3]))));\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[3]);\n\t\t\t\tND_PRINT((ndo, \" %u (%u) bytes @ %u (%u)\",\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[3]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[1]),\n\t\t\t\t\t\tEXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_SYMLINK:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (v3 && (dp = parse_sattr3(ndo, dp, &sa3)) == NULL)\n\t\t\t\tbreak;\n\t\t\tif (parsefn(ndo, dp) == NULL)\n\t\t\t\tbreak;\n\t\t\tif (v3 && ndo->ndo_vflag)\n\t\t\t\tprint_sattr3(ndo, &sa3, ndo->ndo_vflag);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_MKNOD:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(*dp);\n\t\t\ttype = (nfs_type)EXTRACT_32BITS(dp);\n\t\t\tdp++;\n\t\t\tif ((dp = parse_sattr3(ndo, dp, &sa3)) == NULL)\n\t\t\t\tbreak;\n\t\t\tND_PRINT((ndo, \" %s\", tok2str(type2str, \"unk-ft %d\", type)));\n\t\t\tif (ndo->ndo_vflag && (type == NFCHR || type == NFBLK)) {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\tND_PRINT((ndo, \" %u/%u\",\n\t\t\t\t       EXTRACT_32BITS(&dp[0]),\n\t\t\t\t       EXTRACT_32BITS(&dp[1])));\n\t\t\t\tdp += 2;\n\t\t\t}\n\t\t\tif (ndo->ndo_vflag)\n\t\t\t\tprint_sattr3(ndo, &sa3, ndo->ndo_vflag);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_RENAME:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefhn(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (parsefhn(ndo, dp, v3) != NULL)\n\t\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_LINK:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_PRINT((ndo, \" ->\"));\n\t\t\tif (parsefhn(ndo, dp, v3) != NULL)\n\t\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READDIR:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tif (v3) {\n\t\t\t\tND_TCHECK(dp[4]);\n\t\t\t\t/*\n\t\t\t\t * We shouldn't really try to interpret the\n\t\t\t\t * offset cookie here.\n\t\t\t\t */\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRId64,\n\t\t\t\t    EXTRACT_32BITS(&dp[4]),\n\t\t\t\t    EXTRACT_64BITS(&dp[0])));\n\t\t\t\tif (ndo->ndo_vflag)\n\t\t\t\t\tND_PRINT((ndo, \" verf %08x%08x\", dp[2], dp[3]));\n\t\t\t} else {\n\t\t\t\tND_TCHECK(dp[1]);\n\t\t\t\t/*\n\t\t\t\t * Print the offset as signed, since -1 is\n\t\t\t\t * common, but offsets > 2^31 aren't.\n\t\t\t\t */\n\t\t\t\tND_PRINT((ndo, \" %u bytes @ %d\",\n\t\t\t\t    EXTRACT_32BITS(&dp[1]),\n\t\t\t\t    EXTRACT_32BITS(&dp[0])));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_READDIRPLUS:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[4]);\n\t\t\t/*\n\t\t\t * We don't try to interpret the offset\n\t\t\t * cookie here.\n\t\t\t */\n\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRId64,\n\t\t\t\tEXTRACT_32BITS(&dp[4]),\n\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\tif (ndo->ndo_vflag) {\n\t\t\t\tND_TCHECK(dp[5]);\n\t\t\t\tND_PRINT((ndo, \" max %u verf %08x%08x\",\n\t\t\t\t       EXTRACT_32BITS(&dp[5]), dp[2], dp[3]));\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tcase NFSPROC_COMMIT:\n\t\tif ((dp = parsereq(ndo, rp, length)) != NULL &&\n\t\t    (dp = parsefh(ndo, dp, v3)) != NULL) {\n\t\t\tND_TCHECK(dp[2]);\n\t\t\tND_PRINT((ndo, \" %u bytes @ %\" PRIu64,\n\t\t\t\tEXTRACT_32BITS(&dp[2]),\n\t\t\t\tEXTRACT_64BITS(&dp[0])));\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n\ntrunc:\n\tif (!nfserr)\n\t\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "description": "The NFS parser in tcpdump, prior to version 4.9.2, suffers from a buffer over-read issue within the interp_reply function.",
        "commit": "The NFSv3 WRITE procedure lacked proper bounds checking, specifically for the length of the opaque data being written. This oversight allowed for a buffer over-read vulnerability. Additionally, the code was updated to ensure that the entire `ar_stat` field is present in the captured data and to clarify the handling of the \"stable\" argument by removing redundant checks. A test case was included to verify the fix for checking before fetching the \"access\" part of the NFSv3 ACCESS results."
    },
    {
        "cwe": "CWE-835",
        "func_name": "torvalds/schedule",
        "score": 0.7629362344741821,
        "func_before": "asmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\trelease_kernel_lock(prev);\nneed_resched_nonpreemptible:\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\tclear_tsk_need_resched(prev);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If a worker is going to sleep, notify and\n\t\t\t * ask workqueue whether it wants to wake up a\n\t\t\t * task to maintain concurrency.  If so, wake\n\t\t\t * up the task.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\n\tif (likely(prev != next)) {\n\t\tsched_info_switch(prev, next);\n\t\tperf_event_task_sched_out(prev, next);\n\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tif (unlikely(reacquire_kernel_lock(prev)))\n\t\tgoto need_resched_nonpreemptible;\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}",
        "func_after": "asmlinkage void __sched schedule(void)\n{\n\tstruct task_struct *prev, *next;\n\tunsigned long *switch_count;\n\tstruct rq *rq;\n\tint cpu;\n\nneed_resched:\n\tpreempt_disable();\n\tcpu = smp_processor_id();\n\trq = cpu_rq(cpu);\n\trcu_note_context_switch(cpu);\n\tprev = rq->curr;\n\n\trelease_kernel_lock(prev);\nneed_resched_nonpreemptible:\n\n\tschedule_debug(prev);\n\n\tif (sched_feat(HRTICK))\n\t\thrtick_clear(rq);\n\n\traw_spin_lock_irq(&rq->lock);\n\n\tswitch_count = &prev->nivcsw;\n\tif (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {\n\t\tif (unlikely(signal_pending_state(prev->state, prev))) {\n\t\t\tprev->state = TASK_RUNNING;\n\t\t} else {\n\t\t\t/*\n\t\t\t * If a worker is going to sleep, notify and\n\t\t\t * ask workqueue whether it wants to wake up a\n\t\t\t * task to maintain concurrency.  If so, wake\n\t\t\t * up the task.\n\t\t\t */\n\t\t\tif (prev->flags & PF_WQ_WORKER) {\n\t\t\t\tstruct task_struct *to_wakeup;\n\n\t\t\t\tto_wakeup = wq_worker_sleeping(prev, cpu);\n\t\t\t\tif (to_wakeup)\n\t\t\t\t\ttry_to_wake_up_local(to_wakeup);\n\t\t\t}\n\t\t\tdeactivate_task(rq, prev, DEQUEUE_SLEEP);\n\t\t}\n\t\tswitch_count = &prev->nvcsw;\n\t}\n\n\tpre_schedule(rq, prev);\n\n\tif (unlikely(!rq->nr_running))\n\t\tidle_balance(cpu, rq);\n\n\tput_prev_task(rq, prev);\n\tnext = pick_next_task(rq);\n\tclear_tsk_need_resched(prev);\n\trq->skip_clock_update = 0;\n\n\tif (likely(prev != next)) {\n\t\tsched_info_switch(prev, next);\n\t\tperf_event_task_sched_out(prev, next);\n\n\t\trq->nr_switches++;\n\t\trq->curr = next;\n\t\t++*switch_count;\n\t\tWARN_ON_ONCE(test_tsk_need_resched(next));\n\n\t\tcontext_switch(rq, prev, next); /* unlocks the rq */\n\t\t/*\n\t\t * The context switch have flipped the stack from under us\n\t\t * and restored the local variables which were saved when\n\t\t * this task called schedule() in the past. prev == current\n\t\t * is still correct, but it can be moved to another cpu/rq.\n\t\t */\n\t\tcpu = smp_processor_id();\n\t\trq = cpu_rq(cpu);\n\t} else\n\t\traw_spin_unlock_irq(&rq->lock);\n\n\tpost_schedule(rq);\n\n\tif (unlikely(reacquire_kernel_lock(prev)))\n\t\tgoto need_resched_nonpreemptible;\n\n\tpreempt_enable_no_resched();\n\tif (need_resched())\n\t\tgoto need_resched;\n}",
        "description": "The Linux kernel prior to version 2.6.37 fails to correctly implement a specific clock-update optimization. This deficiency enables local users to trigger a denial of service (system hang) by executing an application that runs code in an infinite loop.",
        "commit": "The `idle_balance()` function in the scheduler module of the Linux kernel inadvertently leaves tasks vulnerable to `set_tsk_need_resched()` by dropping and retaking the runqueue lock (`rq->lock`). To address this issue, the optimization should be made robust by clearing the need for rescheduling after returning from balancing and in `setup_thread_stack()`. Additionally, the logic for the `skip_clock_update` optimization needs to be adjusted to ensure it correctly handles cases where a sleeping task is woken up before it successfully deschedules. This adjustment involves checking if the current task has been dequeued before setting the flag to avoid unnecessary clock updates. Finally, the flag should be cleared unconditionally in the `schedule()` function rather than conditionally in `put_prev_task()`."
    },
    {
        "cwe": "CWE-674",
        "func_name": "the-tcpdump-group/bgp_update_print",
        "score": 0.7613220810890198,
        "func_before": "static void\nbgp_update_print(netdissect_options *ndo,\n                 const u_char *dat, int length)\n{\n\tstruct bgp bgp;\n\tconst u_char *p;\n\tint withdrawn_routes_len;\n\tint len;\n\tint i;\n\n\tND_TCHECK2(dat[0], BGP_SIZE);\n\tif (length < BGP_SIZE)\n\t\tgoto trunc;\n\tmemcpy(&bgp, dat, BGP_SIZE);\n\tp = dat + BGP_SIZE;\t/*XXX*/\n\tlength -= BGP_SIZE;\n\n\t/* Unfeasible routes */\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\twithdrawn_routes_len = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\tif (withdrawn_routes_len) {\n\t\t/*\n\t\t * Without keeping state from the original NLRI message,\n\t\t * it's not possible to tell if this a v4 or v6 route,\n\t\t * so only try to decode it if we're not v6 enabled.\n\t         */\n\t\tND_TCHECK2(p[0], withdrawn_routes_len);\n\t\tif (length < withdrawn_routes_len)\n\t\t\tgoto trunc;\n\t\tND_PRINT((ndo, \"\\n\\t  Withdrawn routes: %d bytes\", withdrawn_routes_len));\n\t\tp += withdrawn_routes_len;\n\t\tlength -= withdrawn_routes_len;\n\t}\n\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\tlen = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\n        if (withdrawn_routes_len == 0 && len == 0 && length == 0) {\n            /* No withdrawn routes, no path attributes, no NLRI */\n            ND_PRINT((ndo, \"\\n\\t  End-of-Rib Marker (empty NLRI)\"));\n            return;\n        }\n\n\tif (len) {\n\t\t/* do something more useful!*/\n\t\twhile (len) {\n\t\t\tint aflags, atype, alenlen, alen;\n\n\t\t\tND_TCHECK2(p[0], 2);\n\t\t\tif (len < 2)\n\t\t\t    goto trunc;\n\t\t\tif (length < 2)\n\t\t\t    goto trunc;\n\t\t\taflags = *p;\n\t\t\tatype = *(p + 1);\n\t\t\tp += 2;\n\t\t\tlen -= 2;\n\t\t\tlength -= 2;\n\t\t\talenlen = bgp_attr_lenlen(aflags, p);\n\t\t\tND_TCHECK2(p[0], alenlen);\n\t\t\tif (len < alenlen)\n\t\t\t    goto trunc;\n\t\t\tif (length < alenlen)\n\t\t\t    goto trunc;\n\t\t\talen = bgp_attr_len(aflags, p);\n\t\t\tp += alenlen;\n\t\t\tlen -= alenlen;\n\t\t\tlength -= alenlen;\n\n\t\t\tND_PRINT((ndo, \"\\n\\t  %s (%u), length: %u\",\n                              tok2str(bgp_attr_values, \"Unknown Attribute\",\n\t\t\t\t\t atype),\n                              atype,\n                              alen));\n\n\t\t\tif (aflags) {\n\t\t\t\tND_PRINT((ndo, \", Flags [%s%s%s%s\",\n\t\t\t\t\taflags & 0x80 ? \"O\" : \"\",\n\t\t\t\t\taflags & 0x40 ? \"T\" : \"\",\n\t\t\t\t\taflags & 0x20 ? \"P\" : \"\",\n\t\t\t\t\taflags & 0x10 ? \"E\" : \"\"));\n\t\t\t\tif (aflags & 0xf)\n\t\t\t\t\tND_PRINT((ndo, \"+%x\", aflags & 0xf));\n\t\t\t\tND_PRINT((ndo, \"]: \"));\n\t\t\t}\n\t\t\tif (len < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (length < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (!bgp_attr_print(ndo, atype, p, alen))\n\t\t\t\tgoto trunc;\n\t\t\tp += alen;\n\t\t\tlen -= alen;\n\t\t\tlength -= alen;\n\t\t}\n\t}\n\n\tif (length) {\n\t\t/*\n\t\t * XXX - what if they're using the \"Advertisement of\n\t\t * Multiple Paths in BGP\" feature:\n\t\t *\n\t\t * https://datatracker.ietf.org/doc/draft-ietf-idr-add-paths/\n\t\t *\n\t\t * http://tools.ietf.org/html/draft-ietf-idr-add-paths-06\n\t\t */\n\t\tND_PRINT((ndo, \"\\n\\t  Updated routes:\"));\n\t\twhile (length) {\n\t\t\tchar buf[MAXHOSTNAMELEN + 100];\n\t\t\ti = decode_prefix4(ndo, p, length, buf, sizeof(buf));\n\t\t\tif (i == -1) {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    (illegal prefix length)\"));\n\t\t\t\tbreak;\n\t\t\t} else if (i == -2)\n\t\t\t\tgoto trunc;\n\t\t\telse if (i == -3)\n\t\t\t\tgoto trunc; /* bytes left, but not enough */\n\t\t\telse {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    %s\", buf));\n\t\t\t\tp += i;\n\t\t\t\tlength -= i;\n\t\t\t}\n\t\t}\n\t}\n\treturn;\ntrunc:\n\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "func_after": "static void\nbgp_update_print(netdissect_options *ndo,\n                 const u_char *dat, int length)\n{\n\tstruct bgp bgp;\n\tconst u_char *p;\n\tint withdrawn_routes_len;\n\tint len;\n\tint i;\n\n\tND_TCHECK2(dat[0], BGP_SIZE);\n\tif (length < BGP_SIZE)\n\t\tgoto trunc;\n\tmemcpy(&bgp, dat, BGP_SIZE);\n\tp = dat + BGP_SIZE;\t/*XXX*/\n\tlength -= BGP_SIZE;\n\n\t/* Unfeasible routes */\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\twithdrawn_routes_len = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\tif (withdrawn_routes_len) {\n\t\t/*\n\t\t * Without keeping state from the original NLRI message,\n\t\t * it's not possible to tell if this a v4 or v6 route,\n\t\t * so only try to decode it if we're not v6 enabled.\n\t         */\n\t\tND_TCHECK2(p[0], withdrawn_routes_len);\n\t\tif (length < withdrawn_routes_len)\n\t\t\tgoto trunc;\n\t\tND_PRINT((ndo, \"\\n\\t  Withdrawn routes: %d bytes\", withdrawn_routes_len));\n\t\tp += withdrawn_routes_len;\n\t\tlength -= withdrawn_routes_len;\n\t}\n\n\tND_TCHECK2(p[0], 2);\n\tif (length < 2)\n\t\tgoto trunc;\n\tlen = EXTRACT_16BITS(p);\n\tp += 2;\n\tlength -= 2;\n\n        if (withdrawn_routes_len == 0 && len == 0 && length == 0) {\n            /* No withdrawn routes, no path attributes, no NLRI */\n            ND_PRINT((ndo, \"\\n\\t  End-of-Rib Marker (empty NLRI)\"));\n            return;\n        }\n\n\tif (len) {\n\t\t/* do something more useful!*/\n\t\twhile (len) {\n\t\t\tint aflags, atype, alenlen, alen;\n\n\t\t\tND_TCHECK2(p[0], 2);\n\t\t\tif (len < 2)\n\t\t\t    goto trunc;\n\t\t\tif (length < 2)\n\t\t\t    goto trunc;\n\t\t\taflags = *p;\n\t\t\tatype = *(p + 1);\n\t\t\tp += 2;\n\t\t\tlen -= 2;\n\t\t\tlength -= 2;\n\t\t\talenlen = bgp_attr_lenlen(aflags, p);\n\t\t\tND_TCHECK2(p[0], alenlen);\n\t\t\tif (len < alenlen)\n\t\t\t    goto trunc;\n\t\t\tif (length < alenlen)\n\t\t\t    goto trunc;\n\t\t\talen = bgp_attr_len(aflags, p);\n\t\t\tp += alenlen;\n\t\t\tlen -= alenlen;\n\t\t\tlength -= alenlen;\n\n\t\t\tND_PRINT((ndo, \"\\n\\t  %s (%u), length: %u\",\n                              tok2str(bgp_attr_values, \"Unknown Attribute\",\n\t\t\t\t\t atype),\n                              atype,\n                              alen));\n\n\t\t\tif (aflags) {\n\t\t\t\tND_PRINT((ndo, \", Flags [%s%s%s%s\",\n\t\t\t\t\taflags & 0x80 ? \"O\" : \"\",\n\t\t\t\t\taflags & 0x40 ? \"T\" : \"\",\n\t\t\t\t\taflags & 0x20 ? \"P\" : \"\",\n\t\t\t\t\taflags & 0x10 ? \"E\" : \"\"));\n\t\t\t\tif (aflags & 0xf)\n\t\t\t\t\tND_PRINT((ndo, \"+%x\", aflags & 0xf));\n\t\t\t\tND_PRINT((ndo, \"]: \"));\n\t\t\t}\n\t\t\tif (len < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (length < alen)\n\t\t\t\tgoto trunc;\n\t\t\tif (!bgp_attr_print(ndo, atype, p, alen, 0))\n\t\t\t\tgoto trunc;\n\t\t\tp += alen;\n\t\t\tlen -= alen;\n\t\t\tlength -= alen;\n\t\t}\n\t}\n\n\tif (length) {\n\t\t/*\n\t\t * XXX - what if they're using the \"Advertisement of\n\t\t * Multiple Paths in BGP\" feature:\n\t\t *\n\t\t * https://datatracker.ietf.org/doc/draft-ietf-idr-add-paths/\n\t\t *\n\t\t * http://tools.ietf.org/html/draft-ietf-idr-add-paths-06\n\t\t */\n\t\tND_PRINT((ndo, \"\\n\\t  Updated routes:\"));\n\t\twhile (length) {\n\t\t\tchar buf[MAXHOSTNAMELEN + 100];\n\t\t\ti = decode_prefix4(ndo, p, length, buf, sizeof(buf));\n\t\t\tif (i == -1) {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    (illegal prefix length)\"));\n\t\t\t\tbreak;\n\t\t\t} else if (i == -2)\n\t\t\t\tgoto trunc;\n\t\t\telse if (i == -3)\n\t\t\t\tgoto trunc; /* bytes left, but not enough */\n\t\t\telse {\n\t\t\t\tND_PRINT((ndo, \"\\n\\t    %s\", buf));\n\t\t\t\tp += i;\n\t\t\t\tlength -= i;\n\t\t\t}\n\t\t}\n\t}\n\treturn;\ntrunc:\n\tND_PRINT((ndo, \"%s\", tstr));\n}",
        "description": "The BGP parser in tcpdump, prior to version 4.9.3, suffers from stack consumption due to unlimited recursion within the `bgp_attr_print()` function.",
        "commit": "A function related to BGP attribute printing within the Linux kernel, specifically in version 4.9.3, was modified to prevent stack exhaustion. This was achieved by enforcing a recursion limit on the `bgp_attr_print()` function. The vulnerability was identified through a code audit conducted by Include Security under the Mozilla SOS program in 2018."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.764909029006958,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    }
]