[
    {
        "cwe": "CWE-668",
        "func_name": "xen-project/sh_install_xen_entries_in_l4",
        "score": 0.6813329458236694,
        "func_before": "void sh_install_xen_entries_in_l4(struct domain *d, mfn_t gl4mfn, mfn_t sl4mfn)\n{\n    shadow_l4e_t *sl4e;\n    unsigned int slots;\n\n    sl4e = map_domain_page(sl4mfn);\n    BUILD_BUG_ON(sizeof (l4_pgentry_t) != sizeof (shadow_l4e_t));\n\n    /* Copy the common Xen mappings from the idle domain */\n    slots = (shadow_mode_external(d)\n             ? ROOT_PAGETABLE_XEN_SLOTS\n             : ROOT_PAGETABLE_PV_XEN_SLOTS);\n    memcpy(&sl4e[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           &idle_pg_table[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           slots * sizeof(l4_pgentry_t));\n\n    /* Install the per-domain mappings for this domain */\n    sl4e[shadow_l4_table_offset(PERDOMAIN_VIRT_START)] =\n        shadow_l4e_from_mfn(page_to_mfn(d->arch.perdomain_l3_pg),\n                            __PAGE_HYPERVISOR_RW);\n\n    if ( !shadow_mode_external(d) && !is_pv_32bit_domain(d) &&\n         !VM_ASSIST(d, m2p_strict) )\n    {\n        /* open coded zap_ro_mpt(mfn_x(sl4mfn)): */\n        sl4e[shadow_l4_table_offset(RO_MPT_VIRT_START)] = shadow_l4e_empty();\n    }\n\n    /* Shadow linear mapping for 4-level shadows.  N.B. for 3-level\n     * shadows on 64-bit xen, this linear mapping is later replaced by the\n     * monitor pagetable structure, which is built in make_monitor_table\n     * and maintained by sh_update_linear_entries. */\n    sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n        shadow_l4e_from_mfn(sl4mfn, __PAGE_HYPERVISOR_RW);\n\n    /* Self linear mapping.  */\n    if ( shadow_mode_translate(d) && !shadow_mode_external(d) )\n    {\n        // linear tables may not be used with translated PV guests\n        sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n            shadow_l4e_empty();\n    }\n    else\n    {\n        sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n            shadow_l4e_from_mfn(gl4mfn, __PAGE_HYPERVISOR_RW);\n    }\n\n    unmap_domain_page(sl4e);\n}",
        "func_after": "void sh_install_xen_entries_in_l4(struct domain *d, mfn_t gl4mfn, mfn_t sl4mfn)\n{\n    shadow_l4e_t *sl4e;\n    unsigned int slots;\n\n    sl4e = map_domain_page(sl4mfn);\n    BUILD_BUG_ON(sizeof (l4_pgentry_t) != sizeof (shadow_l4e_t));\n\n    /* Copy the common Xen mappings from the idle domain */\n    slots = (shadow_mode_external(d)\n             ? ROOT_PAGETABLE_XEN_SLOTS\n             : ROOT_PAGETABLE_PV_XEN_SLOTS);\n    memcpy(&sl4e[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           &idle_pg_table[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           slots * sizeof(l4_pgentry_t));\n\n    /* Install the per-domain mappings for this domain */\n    sl4e[shadow_l4_table_offset(PERDOMAIN_VIRT_START)] =\n        shadow_l4e_from_mfn(page_to_mfn(d->arch.perdomain_l3_pg),\n                            __PAGE_HYPERVISOR_RW);\n\n    if ( !shadow_mode_external(d) && !is_pv_32bit_domain(d) &&\n         !VM_ASSIST(d, m2p_strict) )\n    {\n        /* open coded zap_ro_mpt(mfn_x(sl4mfn)): */\n        sl4e[shadow_l4_table_offset(RO_MPT_VIRT_START)] = shadow_l4e_empty();\n    }\n\n    /*\n     * Linear mapping slots:\n     *\n     * Calling this function with gl4mfn == sl4mfn is used to construct a\n     * monitor table for translated domains.  In this case, gl4mfn forms the\n     * self-linear mapping (i.e. not pointing into the translated domain), and\n     * the shadow-linear slot is skipped.  The shadow-linear slot is either\n     * filled when constructing lower level monitor tables, or via\n     * sh_update_cr3() for 4-level guests.\n     *\n     * Calling this function with gl4mfn != sl4mfn is used for non-translated\n     * guests, where the shadow-linear slot is actually self-linear, and the\n     * guest-linear slot points into the guests view of its pagetables.\n     */\n    if ( shadow_mode_translate(d) )\n    {\n        ASSERT(mfn_eq(gl4mfn, sl4mfn));\n\n        sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n            shadow_l4e_empty();\n    }\n    else\n    {\n        ASSERT(!mfn_eq(gl4mfn, sl4mfn));\n\n        sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n            shadow_l4e_from_mfn(sl4mfn, __PAGE_HYPERVISOR_RW);\n    }\n\n    sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n        shadow_l4e_from_mfn(gl4mfn, __PAGE_HYPERVISOR_RW);\n\n    unmap_domain_page(sl4e);\n}",
        "description": "An issue was discovered in Xen versions up to 4.9.x, where x86 HVM guest OS users could potentially cause a denial of service (hypervisor crash) or escalate privileges due to improper handling of self-linear shadow mappings in translated guests.",
        "commit": "When setting up a monitor table for 4-level translated guests in the x86/shadow module, avoid creating self-linear shadow mappings. These mappings can confuse the writable heuristic logic, causing it to follow Xen's mappings instead of the guests' shadows. As a result, the function `sh_guess_wrmap()` must handle cases where no shadow-linear mapping is present, which happens each time a vCPU switches to 4-level paging from a different mode. Appropriate shadow-linear slots are inserted into the monitor table either during the construction of lower-level tables or by `sh_update_cr3()`. Additionally, clarify the safety of other mappings; although they may appear unsafe, creating guest-linear mappings for translated domains is correct because they are self-linear and do not point into the translated domain. Remove a redundant clause for guests that are not external translators. This addresses issue XSA-243."
    },
    {
        "cwe": "CWE-19",
        "func_name": "torvalds/xfs_attr_rmtval_set",
        "score": 0.7204322814941406,
        "func_before": "int\nxfs_attr_rmtval_set(\n\tstruct xfs_da_args\t*args)\n{\n\tstruct xfs_inode\t*dp = args->dp;\n\tstruct xfs_mount\t*mp = dp->i_mount;\n\tstruct xfs_bmbt_irec\tmap;\n\txfs_dablk_t\t\tlblkno;\n\txfs_fileoff_t\t\tlfileoff = 0;\n\t__uint8_t\t\t*src = args->value;\n\tint\t\t\tblkcnt;\n\tint\t\t\tvaluelen;\n\tint\t\t\tnmap;\n\tint\t\t\terror;\n\tint\t\t\toffset = 0;\n\n\ttrace_xfs_attr_rmtval_set(args);\n\n\t/*\n\t * Find a \"hole\" in the attribute address space large enough for\n\t * us to drop the new attribute's value into. Because CRC enable\n\t * attributes have headers, we can't just do a straight byte to FSB\n\t * conversion and have to take the header space into account.\n\t */\n\tblkcnt = xfs_attr3_rmt_blocks(mp, args->valuelen);\n\terror = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,\n\t\t\t\t\t\t   XFS_ATTR_FORK);\n\tif (error)\n\t\treturn error;\n\n\targs->rmtblkno = lblkno = (xfs_dablk_t)lfileoff;\n\targs->rmtblkcnt = blkcnt;\n\n\t/*\n\t * Roll through the \"value\", allocating blocks on disk as required.\n\t */\n\twhile (blkcnt > 0) {\n\t\tint\tcommitted;\n\n\t\t/*\n\t\t * Allocate a single extent, up to the size of the value.\n\t\t */\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_write(args->trans, dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t  blkcnt,\n\t\t\t\t  XFS_BMAPI_ATTRFORK | XFS_BMAPI_METADATA,\n\t\t\t\t  args->firstblock, args->total, &map, &nmap,\n\t\t\t\t  args->flist);\n\t\tif (!error) {\n\t\t\terror = xfs_bmap_finish(&args->trans, args->flist,\n\t\t\t\t\t\t&committed);\n\t\t}\n\t\tif (error) {\n\t\t\tASSERT(committed);\n\t\t\targs->trans = NULL;\n\t\t\txfs_bmap_cancel(args->flist);\n\t\t\treturn(error);\n\t\t}\n\n\t\t/*\n\t\t * bmap_finish() may have committed the last trans and started\n\t\t * a new one.  We need the inode to be in all transactions.\n\t\t */\n\t\tif (committed)\n\t\t\txfs_trans_ijoin(args->trans, dp, 0);\n\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\n\t\t/*\n\t\t * Start the next trans in the chain.\n\t\t */\n\t\terror = xfs_trans_roll(&args->trans, dp);\n\t\tif (error)\n\t\t\treturn (error);\n\t}\n\n\t/*\n\t * Roll through the \"value\", copying the attribute value to the\n\t * already-allocated blocks.  Blocks are written synchronously\n\t * so that we can know they are all on disk before we turn off\n\t * the INCOMPLETE flag.\n\t */\n\tlblkno = args->rmtblkno;\n\tblkcnt = args->rmtblkcnt;\n\tvaluelen = args->valuelen;\n\twhile (valuelen > 0) {\n\t\tstruct xfs_buf\t*bp;\n\t\txfs_daddr_t\tdblkno;\n\t\tint\t\tdblkcnt;\n\n\t\tASSERT(blkcnt > 0);\n\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_read(dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t       blkcnt, &map, &nmap,\n\t\t\t\t       XFS_BMAPI_ATTRFORK);\n\t\tif (error)\n\t\t\treturn(error);\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\n\t\tdblkno = XFS_FSB_TO_DADDR(mp, map.br_startblock),\n\t\tdblkcnt = XFS_FSB_TO_BB(mp, map.br_blockcount);\n\n\t\tbp = xfs_buf_get(mp->m_ddev_targp, dblkno, dblkcnt, 0);\n\t\tif (!bp)\n\t\t\treturn ENOMEM;\n\t\tbp->b_ops = &xfs_attr3_rmt_buf_ops;\n\n\t\txfs_attr_rmtval_copyin(mp, bp, args->dp->i_ino, &offset,\n\t\t\t\t       &valuelen, &src);\n\n\t\terror = xfs_bwrite(bp);\t/* GROT: NOTE: synchronous write */\n\t\txfs_buf_relse(bp);\n\t\tif (error)\n\t\t\treturn error;\n\n\n\t\t/* roll attribute extent map forwards */\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\t}\n\tASSERT(valuelen == 0);\n\treturn 0;\n}",
        "func_after": "int\nxfs_attr_rmtval_set(\n\tstruct xfs_da_args\t*args)\n{\n\tstruct xfs_inode\t*dp = args->dp;\n\tstruct xfs_mount\t*mp = dp->i_mount;\n\tstruct xfs_bmbt_irec\tmap;\n\txfs_dablk_t\t\tlblkno;\n\txfs_fileoff_t\t\tlfileoff = 0;\n\t__uint8_t\t\t*src = args->value;\n\tint\t\t\tblkcnt;\n\tint\t\t\tvaluelen;\n\tint\t\t\tnmap;\n\tint\t\t\terror;\n\tint\t\t\toffset = 0;\n\n\ttrace_xfs_attr_rmtval_set(args);\n\n\t/*\n\t * Find a \"hole\" in the attribute address space large enough for\n\t * us to drop the new attribute's value into. Because CRC enable\n\t * attributes have headers, we can't just do a straight byte to FSB\n\t * conversion and have to take the header space into account.\n\t */\n\tblkcnt = xfs_attr3_rmt_blocks(mp, args->rmtvaluelen);\n\terror = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,\n\t\t\t\t\t\t   XFS_ATTR_FORK);\n\tif (error)\n\t\treturn error;\n\n\targs->rmtblkno = lblkno = (xfs_dablk_t)lfileoff;\n\targs->rmtblkcnt = blkcnt;\n\n\t/*\n\t * Roll through the \"value\", allocating blocks on disk as required.\n\t */\n\twhile (blkcnt > 0) {\n\t\tint\tcommitted;\n\n\t\t/*\n\t\t * Allocate a single extent, up to the size of the value.\n\t\t */\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_write(args->trans, dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t  blkcnt,\n\t\t\t\t  XFS_BMAPI_ATTRFORK | XFS_BMAPI_METADATA,\n\t\t\t\t  args->firstblock, args->total, &map, &nmap,\n\t\t\t\t  args->flist);\n\t\tif (!error) {\n\t\t\terror = xfs_bmap_finish(&args->trans, args->flist,\n\t\t\t\t\t\t&committed);\n\t\t}\n\t\tif (error) {\n\t\t\tASSERT(committed);\n\t\t\targs->trans = NULL;\n\t\t\txfs_bmap_cancel(args->flist);\n\t\t\treturn(error);\n\t\t}\n\n\t\t/*\n\t\t * bmap_finish() may have committed the last trans and started\n\t\t * a new one.  We need the inode to be in all transactions.\n\t\t */\n\t\tif (committed)\n\t\t\txfs_trans_ijoin(args->trans, dp, 0);\n\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\n\t\t/*\n\t\t * Start the next trans in the chain.\n\t\t */\n\t\terror = xfs_trans_roll(&args->trans, dp);\n\t\tif (error)\n\t\t\treturn (error);\n\t}\n\n\t/*\n\t * Roll through the \"value\", copying the attribute value to the\n\t * already-allocated blocks.  Blocks are written synchronously\n\t * so that we can know they are all on disk before we turn off\n\t * the INCOMPLETE flag.\n\t */\n\tlblkno = args->rmtblkno;\n\tblkcnt = args->rmtblkcnt;\n\tvaluelen = args->rmtvaluelen;\n\twhile (valuelen > 0) {\n\t\tstruct xfs_buf\t*bp;\n\t\txfs_daddr_t\tdblkno;\n\t\tint\t\tdblkcnt;\n\n\t\tASSERT(blkcnt > 0);\n\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_read(dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t       blkcnt, &map, &nmap,\n\t\t\t\t       XFS_BMAPI_ATTRFORK);\n\t\tif (error)\n\t\t\treturn(error);\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\n\t\tdblkno = XFS_FSB_TO_DADDR(mp, map.br_startblock),\n\t\tdblkcnt = XFS_FSB_TO_BB(mp, map.br_blockcount);\n\n\t\tbp = xfs_buf_get(mp->m_ddev_targp, dblkno, dblkcnt, 0);\n\t\tif (!bp)\n\t\t\treturn ENOMEM;\n\t\tbp->b_ops = &xfs_attr3_rmt_buf_ops;\n\n\t\txfs_attr_rmtval_copyin(mp, bp, args->dp->i_ino, &offset,\n\t\t\t\t       &valuelen, &src);\n\n\t\terror = xfs_bwrite(bp);\t/* GROT: NOTE: synchronous write */\n\t\txfs_buf_relse(bp);\n\t\tif (error)\n\t\t\treturn error;\n\n\n\t\t/* roll attribute extent map forwards */\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\t}\n\tASSERT(valuelen == 0);\n\treturn 0;\n}",
        "description": "The XFS implementation in the Linux kernel prior to version 3.15 improperly utilizes an outdated size value during remote attribute replacement, enabling local users to trigger a denial of service through transaction overruns and data corruption, or potentially escalate their privileges by exploiting XFS filesystem access.",
        "commit": "A vulnerability in the XFS filesystem allows for a remote attribute overwrite, leading to a transaction overrun. During remote attribute lookups, the length of the attribute is passed in the `xfs_da_args` structure to ensure correct CRC calculations and validity checks. However, this inadvertently modifies the `args->valuelen` parameter when replacing a remote attribute. Specifically, the lookup operation overwrites `args->valuelen` with the length of the existing remote attribute, causing the new attribute to be created with the incorrect size. If the new attribute is significantly smaller than the old one, this results in a transaction overrun and an assertion failure in debug kernels. The fix involves maintaining separate lengths for the remote attribute value and the attribute value within the `xfs_da_args` structure. Additionally, ensuring that remote block contexts saved for later renames are properly reset to avoid confusing the state of the attribute to be removed with the new attribute's state."
    },
    {
        "cwe": "CWE-426",
        "func_name": "openbsd/main",
        "score": 0.7127208709716797,
        "func_before": "int\nmain(int ac, char **av)\n{\n\tint c_flag = 0, d_flag = 0, D_flag = 0, k_flag = 0, s_flag = 0;\n\tint sock, fd, ch, result, saved_errno;\n\tu_int nalloc;\n\tchar *shell, *format, *pidstr, *agentsocket = NULL;\n\tfd_set *readsetp = NULL, *writesetp = NULL;\n\tstruct rlimit rlim;\n\textern int optind;\n\textern char *optarg;\n\tpid_t pid;\n\tchar pidstrbuf[1 + 3 * sizeof pid];\n\tstruct timeval *tvp = NULL;\n\tsize_t len;\n\tmode_t prev_mask;\n\n\tssh_malloc_init();\t/* must be called before any mallocs */\n\t/* Ensure that fds 0, 1 and 2 are open or directed to /dev/null */\n\tsanitise_stdfd();\n\n\t/* drop */\n\tsetegid(getgid());\n\tsetgid(getgid());\n\n#ifdef WITH_OPENSSL\n\tOpenSSL_add_all_algorithms();\n#endif\n\n\twhile ((ch = getopt(ac, av, \"cDdksE:a:t:\")) != -1) {\n\t\tswitch (ch) {\n\t\tcase 'E':\n\t\t\tfingerprint_hash = ssh_digest_alg_by_name(optarg);\n\t\t\tif (fingerprint_hash == -1)\n\t\t\t\tfatal(\"Invalid hash algorithm \\\"%s\\\"\", optarg);\n\t\t\tbreak;\n\t\tcase 'c':\n\t\t\tif (s_flag)\n\t\t\t\tusage();\n\t\t\tc_flag++;\n\t\t\tbreak;\n\t\tcase 'k':\n\t\t\tk_flag++;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tif (c_flag)\n\t\t\t\tusage();\n\t\t\ts_flag++;\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tif (d_flag || D_flag)\n\t\t\t\tusage();\n\t\t\td_flag++;\n\t\t\tbreak;\n\t\tcase 'D':\n\t\t\tif (d_flag || D_flag)\n\t\t\t\tusage();\n\t\t\tD_flag++;\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tagentsocket = optarg;\n\t\t\tbreak;\n\t\tcase 't':\n\t\t\tif ((lifetime = convtime(optarg)) == -1) {\n\t\t\t\tfprintf(stderr, \"Invalid lifetime\\n\");\n\t\t\t\tusage();\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t}\n\t}\n\tac -= optind;\n\tav += optind;\n\n\tif (ac > 0 && (c_flag || k_flag || s_flag || d_flag || D_flag))\n\t\tusage();\n\n\tif (ac == 0 && !c_flag && !s_flag) {\n\t\tshell = getenv(\"SHELL\");\n\t\tif (shell != NULL && (len = strlen(shell)) > 2 &&\n\t\t    strncmp(shell + len - 3, \"csh\", 3) == 0)\n\t\t\tc_flag = 1;\n\t}\n\tif (k_flag) {\n\t\tconst char *errstr = NULL;\n\n\t\tpidstr = getenv(SSH_AGENTPID_ENV_NAME);\n\t\tif (pidstr == NULL) {\n\t\t\tfprintf(stderr, \"%s not set, cannot kill agent\\n\",\n\t\t\t    SSH_AGENTPID_ENV_NAME);\n\t\t\texit(1);\n\t\t}\n\t\tpid = (int)strtonum(pidstr, 2, INT_MAX, &errstr);\n\t\tif (errstr) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s=\\\"%s\\\", which is not a good PID: %s\\n\",\n\t\t\t    SSH_AGENTPID_ENV_NAME, pidstr, errstr);\n\t\t\texit(1);\n\t\t}\n\t\tif (kill(pid, SIGTERM) == -1) {\n\t\t\tperror(\"kill\");\n\t\t\texit(1);\n\t\t}\n\t\tformat = c_flag ? \"unsetenv %s;\\n\" : \"unset %s;\\n\";\n\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME);\n\t\tprintf(format, SSH_AGENTPID_ENV_NAME);\n\t\tprintf(\"echo Agent pid %ld killed;\\n\", (long)pid);\n\t\texit(0);\n\t}\n\tparent_pid = getpid();\n\n\tif (agentsocket == NULL) {\n\t\t/* Create private directory for agent socket */\n\t\tmktemp_proto(socket_dir, sizeof(socket_dir));\n\t\tif (mkdtemp(socket_dir) == NULL) {\n\t\t\tperror(\"mkdtemp: private socket dir\");\n\t\t\texit(1);\n\t\t}\n\t\tsnprintf(socket_name, sizeof socket_name, \"%s/agent.%ld\", socket_dir,\n\t\t    (long)parent_pid);\n\t} else {\n\t\t/* Try to use specified agent socket */\n\t\tsocket_dir[0] = '\\0';\n\t\tstrlcpy(socket_name, agentsocket, sizeof socket_name);\n\t}\n\n\t/*\n\t * Create socket early so it will exist before command gets run from\n\t * the parent.\n\t */\n\tprev_mask = umask(0177);\n\tsock = unix_listener(socket_name, SSH_LISTEN_BACKLOG, 0);\n\tif (sock < 0) {\n\t\t/* XXX - unix_listener() calls error() not perror() */\n\t\t*socket_name = '\\0'; /* Don't unlink any existing file */\n\t\tcleanup_exit(1);\n\t}\n\tumask(prev_mask);\n\n\t/*\n\t * Fork, and have the parent execute the command, if any, or present\n\t * the socket data.  The child continues as the authentication agent.\n\t */\n\tif (D_flag || d_flag) {\n\t\tlog_init(__progname,\n\t\t    d_flag ? SYSLOG_LEVEL_DEBUG3 : SYSLOG_LEVEL_INFO,\n\t\t    SYSLOG_FACILITY_AUTH, 1);\n\t\tformat = c_flag ? \"setenv %s %s;\\n\" : \"%s=%s; export %s;\\n\";\n\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME, socket_name,\n\t\t    SSH_AUTHSOCKET_ENV_NAME);\n\t\tprintf(\"echo Agent pid %ld;\\n\", (long)parent_pid);\n\t\tfflush(stdout);\n\t\tgoto skip;\n\t}\n\tpid = fork();\n\tif (pid == -1) {\n\t\tperror(\"fork\");\n\t\tcleanup_exit(1);\n\t}\n\tif (pid != 0) {\t\t/* Parent - execute the given command. */\n\t\tclose(sock);\n\t\tsnprintf(pidstrbuf, sizeof pidstrbuf, \"%ld\", (long)pid);\n\t\tif (ac == 0) {\n\t\t\tformat = c_flag ? \"setenv %s %s;\\n\" : \"%s=%s; export %s;\\n\";\n\t\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME, socket_name,\n\t\t\t    SSH_AUTHSOCKET_ENV_NAME);\n\t\t\tprintf(format, SSH_AGENTPID_ENV_NAME, pidstrbuf,\n\t\t\t    SSH_AGENTPID_ENV_NAME);\n\t\t\tprintf(\"echo Agent pid %ld;\\n\", (long)pid);\n\t\t\texit(0);\n\t\t}\n\t\tif (setenv(SSH_AUTHSOCKET_ENV_NAME, socket_name, 1) == -1 ||\n\t\t    setenv(SSH_AGENTPID_ENV_NAME, pidstrbuf, 1) == -1) {\n\t\t\tperror(\"setenv\");\n\t\t\texit(1);\n\t\t}\n\t\texecvp(av[0], av);\n\t\tperror(av[0]);\n\t\texit(1);\n\t}\n\t/* child */\n\tlog_init(__progname, SYSLOG_LEVEL_INFO, SYSLOG_FACILITY_AUTH, 0);\n\n\tif (setsid() == -1) {\n\t\terror(\"setsid: %s\", strerror(errno));\n\t\tcleanup_exit(1);\n\t}\n\n\t(void)chdir(\"/\");\n\tif ((fd = open(_PATH_DEVNULL, O_RDWR, 0)) != -1) {\n\t\t/* XXX might close listen socket */\n\t\t(void)dup2(fd, STDIN_FILENO);\n\t\t(void)dup2(fd, STDOUT_FILENO);\n\t\t(void)dup2(fd, STDERR_FILENO);\n\t\tif (fd > 2)\n\t\t\tclose(fd);\n\t}\n\n\t/* deny core dumps, since memory contains unencrypted private keys */\n\trlim.rlim_cur = rlim.rlim_max = 0;\n\tif (setrlimit(RLIMIT_CORE, &rlim) < 0) {\n\t\terror(\"setrlimit RLIMIT_CORE: %s\", strerror(errno));\n\t\tcleanup_exit(1);\n\t}\n\nskip:\n\n\tcleanup_pid = getpid();\n\n#ifdef ENABLE_PKCS11\n\tpkcs11_init(0);\n#endif\n\tnew_socket(AUTH_SOCKET, sock);\n\tif (ac > 0)\n\t\tparent_alive_interval = 10;\n\tidtab_init();\n\tsignal(SIGPIPE, SIG_IGN);\n\tsignal(SIGINT, (d_flag | D_flag) ? cleanup_handler : SIG_IGN);\n\tsignal(SIGHUP, cleanup_handler);\n\tsignal(SIGTERM, cleanup_handler);\n\tnalloc = 0;\n\n\tif (pledge(\"stdio cpath unix id proc exec\", NULL) == -1)\n\t\tfatal(\"%s: pledge: %s\", __progname, strerror(errno));\n\n\twhile (1) {\n\t\tprepare_select(&readsetp, &writesetp, &max_fd, &nalloc, &tvp);\n\t\tresult = select(max_fd + 1, readsetp, writesetp, NULL, tvp);\n\t\tsaved_errno = errno;\n\t\tif (parent_alive_interval != 0)\n\t\t\tcheck_parent_exists();\n\t\t(void) reaper();\t/* remove expired keys */\n\t\tif (result < 0) {\n\t\t\tif (saved_errno == EINTR)\n\t\t\t\tcontinue;\n\t\t\tfatal(\"select: %s\", strerror(saved_errno));\n\t\t} else if (result > 0)\n\t\t\tafter_select(readsetp, writesetp);\n\t}\n\t/* NOTREACHED */\n}",
        "func_after": "int\nmain(int ac, char **av)\n{\n\tint c_flag = 0, d_flag = 0, D_flag = 0, k_flag = 0, s_flag = 0;\n\tint sock, fd, ch, result, saved_errno;\n\tu_int nalloc;\n\tchar *shell, *format, *pidstr, *agentsocket = NULL;\n\tfd_set *readsetp = NULL, *writesetp = NULL;\n\tstruct rlimit rlim;\n\textern int optind;\n\textern char *optarg;\n\tpid_t pid;\n\tchar pidstrbuf[1 + 3 * sizeof pid];\n\tstruct timeval *tvp = NULL;\n\tsize_t len;\n\tmode_t prev_mask;\n\n\tssh_malloc_init();\t/* must be called before any mallocs */\n\t/* Ensure that fds 0, 1 and 2 are open or directed to /dev/null */\n\tsanitise_stdfd();\n\n\t/* drop */\n\tsetegid(getgid());\n\tsetgid(getgid());\n\n#ifdef WITH_OPENSSL\n\tOpenSSL_add_all_algorithms();\n#endif\n\n\twhile ((ch = getopt(ac, av, \"cDdksE:a:P:t:\")) != -1) {\n\t\tswitch (ch) {\n\t\tcase 'E':\n\t\t\tfingerprint_hash = ssh_digest_alg_by_name(optarg);\n\t\t\tif (fingerprint_hash == -1)\n\t\t\t\tfatal(\"Invalid hash algorithm \\\"%s\\\"\", optarg);\n\t\t\tbreak;\n\t\tcase 'c':\n\t\t\tif (s_flag)\n\t\t\t\tusage();\n\t\t\tc_flag++;\n\t\t\tbreak;\n\t\tcase 'k':\n\t\t\tk_flag++;\n\t\t\tbreak;\n\t\tcase 'P':\n\t\t\tif (pkcs11_whitelist != NULL)\n\t\t\t\tfatal(\"-P option already specified\");\n\t\t\tpkcs11_whitelist = xstrdup(optarg);\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tif (c_flag)\n\t\t\t\tusage();\n\t\t\ts_flag++;\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tif (d_flag || D_flag)\n\t\t\t\tusage();\n\t\t\td_flag++;\n\t\t\tbreak;\n\t\tcase 'D':\n\t\t\tif (d_flag || D_flag)\n\t\t\t\tusage();\n\t\t\tD_flag++;\n\t\t\tbreak;\n\t\tcase 'a':\n\t\t\tagentsocket = optarg;\n\t\t\tbreak;\n\t\tcase 't':\n\t\t\tif ((lifetime = convtime(optarg)) == -1) {\n\t\t\t\tfprintf(stderr, \"Invalid lifetime\\n\");\n\t\t\t\tusage();\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t}\n\t}\n\tac -= optind;\n\tav += optind;\n\n\tif (ac > 0 && (c_flag || k_flag || s_flag || d_flag || D_flag))\n\t\tusage();\n\n\tif (pkcs11_whitelist == NULL)\n\t\tpkcs11_whitelist = xstrdup(DEFAULT_PKCS11_WHITELIST);\n\n\tif (ac == 0 && !c_flag && !s_flag) {\n\t\tshell = getenv(\"SHELL\");\n\t\tif (shell != NULL && (len = strlen(shell)) > 2 &&\n\t\t    strncmp(shell + len - 3, \"csh\", 3) == 0)\n\t\t\tc_flag = 1;\n\t}\n\tif (k_flag) {\n\t\tconst char *errstr = NULL;\n\n\t\tpidstr = getenv(SSH_AGENTPID_ENV_NAME);\n\t\tif (pidstr == NULL) {\n\t\t\tfprintf(stderr, \"%s not set, cannot kill agent\\n\",\n\t\t\t    SSH_AGENTPID_ENV_NAME);\n\t\t\texit(1);\n\t\t}\n\t\tpid = (int)strtonum(pidstr, 2, INT_MAX, &errstr);\n\t\tif (errstr) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s=\\\"%s\\\", which is not a good PID: %s\\n\",\n\t\t\t    SSH_AGENTPID_ENV_NAME, pidstr, errstr);\n\t\t\texit(1);\n\t\t}\n\t\tif (kill(pid, SIGTERM) == -1) {\n\t\t\tperror(\"kill\");\n\t\t\texit(1);\n\t\t}\n\t\tformat = c_flag ? \"unsetenv %s;\\n\" : \"unset %s;\\n\";\n\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME);\n\t\tprintf(format, SSH_AGENTPID_ENV_NAME);\n\t\tprintf(\"echo Agent pid %ld killed;\\n\", (long)pid);\n\t\texit(0);\n\t}\n\tparent_pid = getpid();\n\n\tif (agentsocket == NULL) {\n\t\t/* Create private directory for agent socket */\n\t\tmktemp_proto(socket_dir, sizeof(socket_dir));\n\t\tif (mkdtemp(socket_dir) == NULL) {\n\t\t\tperror(\"mkdtemp: private socket dir\");\n\t\t\texit(1);\n\t\t}\n\t\tsnprintf(socket_name, sizeof socket_name, \"%s/agent.%ld\", socket_dir,\n\t\t    (long)parent_pid);\n\t} else {\n\t\t/* Try to use specified agent socket */\n\t\tsocket_dir[0] = '\\0';\n\t\tstrlcpy(socket_name, agentsocket, sizeof socket_name);\n\t}\n\n\t/*\n\t * Create socket early so it will exist before command gets run from\n\t * the parent.\n\t */\n\tprev_mask = umask(0177);\n\tsock = unix_listener(socket_name, SSH_LISTEN_BACKLOG, 0);\n\tif (sock < 0) {\n\t\t/* XXX - unix_listener() calls error() not perror() */\n\t\t*socket_name = '\\0'; /* Don't unlink any existing file */\n\t\tcleanup_exit(1);\n\t}\n\tumask(prev_mask);\n\n\t/*\n\t * Fork, and have the parent execute the command, if any, or present\n\t * the socket data.  The child continues as the authentication agent.\n\t */\n\tif (D_flag || d_flag) {\n\t\tlog_init(__progname,\n\t\t    d_flag ? SYSLOG_LEVEL_DEBUG3 : SYSLOG_LEVEL_INFO,\n\t\t    SYSLOG_FACILITY_AUTH, 1);\n\t\tformat = c_flag ? \"setenv %s %s;\\n\" : \"%s=%s; export %s;\\n\";\n\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME, socket_name,\n\t\t    SSH_AUTHSOCKET_ENV_NAME);\n\t\tprintf(\"echo Agent pid %ld;\\n\", (long)parent_pid);\n\t\tfflush(stdout);\n\t\tgoto skip;\n\t}\n\tpid = fork();\n\tif (pid == -1) {\n\t\tperror(\"fork\");\n\t\tcleanup_exit(1);\n\t}\n\tif (pid != 0) {\t\t/* Parent - execute the given command. */\n\t\tclose(sock);\n\t\tsnprintf(pidstrbuf, sizeof pidstrbuf, \"%ld\", (long)pid);\n\t\tif (ac == 0) {\n\t\t\tformat = c_flag ? \"setenv %s %s;\\n\" : \"%s=%s; export %s;\\n\";\n\t\t\tprintf(format, SSH_AUTHSOCKET_ENV_NAME, socket_name,\n\t\t\t    SSH_AUTHSOCKET_ENV_NAME);\n\t\t\tprintf(format, SSH_AGENTPID_ENV_NAME, pidstrbuf,\n\t\t\t    SSH_AGENTPID_ENV_NAME);\n\t\t\tprintf(\"echo Agent pid %ld;\\n\", (long)pid);\n\t\t\texit(0);\n\t\t}\n\t\tif (setenv(SSH_AUTHSOCKET_ENV_NAME, socket_name, 1) == -1 ||\n\t\t    setenv(SSH_AGENTPID_ENV_NAME, pidstrbuf, 1) == -1) {\n\t\t\tperror(\"setenv\");\n\t\t\texit(1);\n\t\t}\n\t\texecvp(av[0], av);\n\t\tperror(av[0]);\n\t\texit(1);\n\t}\n\t/* child */\n\tlog_init(__progname, SYSLOG_LEVEL_INFO, SYSLOG_FACILITY_AUTH, 0);\n\n\tif (setsid() == -1) {\n\t\terror(\"setsid: %s\", strerror(errno));\n\t\tcleanup_exit(1);\n\t}\n\n\t(void)chdir(\"/\");\n\tif ((fd = open(_PATH_DEVNULL, O_RDWR, 0)) != -1) {\n\t\t/* XXX might close listen socket */\n\t\t(void)dup2(fd, STDIN_FILENO);\n\t\t(void)dup2(fd, STDOUT_FILENO);\n\t\t(void)dup2(fd, STDERR_FILENO);\n\t\tif (fd > 2)\n\t\t\tclose(fd);\n\t}\n\n\t/* deny core dumps, since memory contains unencrypted private keys */\n\trlim.rlim_cur = rlim.rlim_max = 0;\n\tif (setrlimit(RLIMIT_CORE, &rlim) < 0) {\n\t\terror(\"setrlimit RLIMIT_CORE: %s\", strerror(errno));\n\t\tcleanup_exit(1);\n\t}\n\nskip:\n\n\tcleanup_pid = getpid();\n\n#ifdef ENABLE_PKCS11\n\tpkcs11_init(0);\n#endif\n\tnew_socket(AUTH_SOCKET, sock);\n\tif (ac > 0)\n\t\tparent_alive_interval = 10;\n\tidtab_init();\n\tsignal(SIGPIPE, SIG_IGN);\n\tsignal(SIGINT, (d_flag | D_flag) ? cleanup_handler : SIG_IGN);\n\tsignal(SIGHUP, cleanup_handler);\n\tsignal(SIGTERM, cleanup_handler);\n\tnalloc = 0;\n\n\tif (pledge(\"stdio rpath cpath unix id proc exec\", NULL) == -1)\n\t\tfatal(\"%s: pledge: %s\", __progname, strerror(errno));\n\n\twhile (1) {\n\t\tprepare_select(&readsetp, &writesetp, &max_fd, &nalloc, &tvp);\n\t\tresult = select(max_fd + 1, readsetp, writesetp, NULL, tvp);\n\t\tsaved_errno = errno;\n\t\tif (parent_alive_interval != 0)\n\t\t\tcheck_parent_exists();\n\t\t(void) reaper();\t/* remove expired keys */\n\t\tif (result < 0) {\n\t\t\tif (saved_errno == EINTR)\n\t\t\t\tcontinue;\n\t\t\tfatal(\"select: %s\", strerror(saved_errno));\n\t\t} else if (result > 0)\n\t\t\tafter_select(readsetp, writesetp);\n\t}\n\t/* NOTREACHED */\n}",
        "description": "An untrusted search path vulnerability exists in the ssh-agent component of OpenSSH versions prior to 7.4. This flaw enables remote attackers to execute arbitrary local PKCS#11 modules by exploiting control over a forwarded agent-socket.",
        "commit": "The addition of a whitelist mechanism to restrict the paths from which `ssh-agent` can load PKCS#11 modules via `ssh-pkcs11-helper`. This enhancement ensures that only specified paths are permitted, thereby enhancing security by preventing unauthorized access to PKCS#11 modules."
    },
    {
        "cwe": "CWE-436",
        "func_name": "xen-project/sh_page_fault",
        "score": 0.715745747089386,
        "func_before": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && (mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying) )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "func_after": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn) && is_hvm_domain(d) &&\n         mfn_to_page(gmfn)->pagetable_dying )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "description": "An issue was discovered in Xen versions up to 4.11.x, where x86 PV guest OS users could potentially cause a denial of service (host OS crash) or escalate their privileges to those of the host OS due to an interpretation conflict involving a union data structure related to shadow paging. This problem arose from an incorrect fix applied to address CVE-2017-15595.",
        "commit": "The vulnerability involves modifying the `struct page_info` in the x86/shadow module by reducing the size of the `shadow_flags` field to 16 bits to prevent overlap with the `linear_pt_count` field used in PV domains. To address this change, a new, HVM-only field named `pagetable_dying` was introduced to handle the functionality previously managed by the upper 16 bits of `shadow_flags`. The accesses to `shadow_flags` in functions like `shadow_{pro,de}mote()` were changed to non-atomic, non-bitops operations because atomic operations such as `{test,set,clear}_bit()` are not supported on `uint16_t` fields. This modification is justified by the fact that all updates to `shadow_flags` should occur while holding the paging lock, and other updates use bitwise operations (`|=`, `&=`), which inherently provide some level of atomicity. This change is part of the XSA-280 patch set."
    },
    {
        "cwe": "CWE-17",
        "func_name": "torvalds/udf_find_entry",
        "score": 0.7297837734222412,
        "func_before": "static struct fileIdentDesc *udf_find_entry(struct inode *dir,\n\t\t\t\t\t    const struct qstr *child,\n\t\t\t\t\t    struct udf_fileident_bh *fibh,\n\t\t\t\t\t    struct fileIdentDesc *cfi)\n{\n\tstruct fileIdentDesc *fi = NULL;\n\tloff_t f_pos;\n\tint block, flen;\n\tunsigned char *fname = NULL;\n\tunsigned char *nameptr;\n\tuint8_t lfi;\n\tuint16_t liu;\n\tloff_t size;\n\tstruct kernel_lb_addr eloc;\n\tuint32_t elen;\n\tsector_t offset;\n\tstruct extent_position epos = {};\n\tstruct udf_inode_info *dinfo = UDF_I(dir);\n\tint isdotdot = child->len == 2 &&\n\t\tchild->name[0] == '.' && child->name[1] == '.';\n\n\tsize = udf_ext0_offset(dir) + dir->i_size;\n\tf_pos = udf_ext0_offset(dir);\n\n\tfibh->sbh = fibh->ebh = NULL;\n\tfibh->soffset = fibh->eoffset = f_pos & (dir->i_sb->s_blocksize - 1);\n\tif (dinfo->i_alloc_type != ICBTAG_FLAG_AD_IN_ICB) {\n\t\tif (inode_bmap(dir, f_pos >> dir->i_sb->s_blocksize_bits, &epos,\n\t\t    &eloc, &elen, &offset) != (EXT_RECORDED_ALLOCATED >> 30))\n\t\t\tgoto out_err;\n\t\tblock = udf_get_lb_pblock(dir->i_sb, &eloc, offset);\n\t\tif ((++offset << dir->i_sb->s_blocksize_bits) < elen) {\n\t\t\tif (dinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT)\n\t\t\t\tepos.offset -= sizeof(struct short_ad);\n\t\t\telse if (dinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG)\n\t\t\t\tepos.offset -= sizeof(struct long_ad);\n\t\t} else\n\t\t\toffset = 0;\n\n\t\tfibh->sbh = fibh->ebh = udf_tread(dir->i_sb, block);\n\t\tif (!fibh->sbh)\n\t\t\tgoto out_err;\n\t}\n\n\tfname = kmalloc(UDF_NAME_LEN, GFP_NOFS);\n\tif (!fname)\n\t\tgoto out_err;\n\n\twhile (f_pos < size) {\n\t\tfi = udf_fileident_read(dir, &f_pos, fibh, cfi, &epos, &eloc,\n\t\t\t\t\t&elen, &offset);\n\t\tif (!fi)\n\t\t\tgoto out_err;\n\n\t\tliu = le16_to_cpu(cfi->lengthOfImpUse);\n\t\tlfi = cfi->lengthFileIdent;\n\n\t\tif (fibh->sbh == fibh->ebh) {\n\t\t\tnameptr = fi->fileIdent + liu;\n\t\t} else {\n\t\t\tint poffset;\t/* Unpaded ending offset */\n\n\t\t\tpoffset = fibh->soffset + sizeof(struct fileIdentDesc) +\n\t\t\t\t\tliu + lfi;\n\n\t\t\tif (poffset >= lfi)\n\t\t\t\tnameptr = (uint8_t *)(fibh->ebh->b_data +\n\t\t\t\t\t\t      poffset - lfi);\n\t\t\telse {\n\t\t\t\tnameptr = fname;\n\t\t\t\tmemcpy(nameptr, fi->fileIdent + liu,\n\t\t\t\t\tlfi - poffset);\n\t\t\t\tmemcpy(nameptr + lfi - poffset,\n\t\t\t\t\tfibh->ebh->b_data, poffset);\n\t\t\t}\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_DELETED) != 0) {\n\t\t\tif (!UDF_QUERY_FLAG(dir->i_sb, UDF_FLAG_UNDELETE))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_HIDDEN) != 0) {\n\t\t\tif (!UDF_QUERY_FLAG(dir->i_sb, UDF_FLAG_UNHIDE))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_PARENT) &&\n\t\t    isdotdot)\n\t\t\tgoto out_ok;\n\n\t\tif (!lfi)\n\t\t\tcontinue;\n\n\t\tflen = udf_get_filename(dir->i_sb, nameptr, fname, lfi);\n\t\tif (flen && udf_match(flen, fname, child->len, child->name))\n\t\t\tgoto out_ok;\n\t}\n\nout_err:\n\tfi = NULL;\n\tif (fibh->sbh != fibh->ebh)\n\t\tbrelse(fibh->ebh);\n\tbrelse(fibh->sbh);\nout_ok:\n\tbrelse(epos.bh);\n\tkfree(fname);\n\n\treturn fi;\n}",
        "func_after": "static struct fileIdentDesc *udf_find_entry(struct inode *dir,\n\t\t\t\t\t    const struct qstr *child,\n\t\t\t\t\t    struct udf_fileident_bh *fibh,\n\t\t\t\t\t    struct fileIdentDesc *cfi)\n{\n\tstruct fileIdentDesc *fi = NULL;\n\tloff_t f_pos;\n\tint block, flen;\n\tunsigned char *fname = NULL;\n\tunsigned char *nameptr;\n\tuint8_t lfi;\n\tuint16_t liu;\n\tloff_t size;\n\tstruct kernel_lb_addr eloc;\n\tuint32_t elen;\n\tsector_t offset;\n\tstruct extent_position epos = {};\n\tstruct udf_inode_info *dinfo = UDF_I(dir);\n\tint isdotdot = child->len == 2 &&\n\t\tchild->name[0] == '.' && child->name[1] == '.';\n\n\tsize = udf_ext0_offset(dir) + dir->i_size;\n\tf_pos = udf_ext0_offset(dir);\n\n\tfibh->sbh = fibh->ebh = NULL;\n\tfibh->soffset = fibh->eoffset = f_pos & (dir->i_sb->s_blocksize - 1);\n\tif (dinfo->i_alloc_type != ICBTAG_FLAG_AD_IN_ICB) {\n\t\tif (inode_bmap(dir, f_pos >> dir->i_sb->s_blocksize_bits, &epos,\n\t\t    &eloc, &elen, &offset) != (EXT_RECORDED_ALLOCATED >> 30))\n\t\t\tgoto out_err;\n\t\tblock = udf_get_lb_pblock(dir->i_sb, &eloc, offset);\n\t\tif ((++offset << dir->i_sb->s_blocksize_bits) < elen) {\n\t\t\tif (dinfo->i_alloc_type == ICBTAG_FLAG_AD_SHORT)\n\t\t\t\tepos.offset -= sizeof(struct short_ad);\n\t\t\telse if (dinfo->i_alloc_type == ICBTAG_FLAG_AD_LONG)\n\t\t\t\tepos.offset -= sizeof(struct long_ad);\n\t\t} else\n\t\t\toffset = 0;\n\n\t\tfibh->sbh = fibh->ebh = udf_tread(dir->i_sb, block);\n\t\tif (!fibh->sbh)\n\t\t\tgoto out_err;\n\t}\n\n\tfname = kmalloc(UDF_NAME_LEN, GFP_NOFS);\n\tif (!fname)\n\t\tgoto out_err;\n\n\twhile (f_pos < size) {\n\t\tfi = udf_fileident_read(dir, &f_pos, fibh, cfi, &epos, &eloc,\n\t\t\t\t\t&elen, &offset);\n\t\tif (!fi)\n\t\t\tgoto out_err;\n\n\t\tliu = le16_to_cpu(cfi->lengthOfImpUse);\n\t\tlfi = cfi->lengthFileIdent;\n\n\t\tif (fibh->sbh == fibh->ebh) {\n\t\t\tnameptr = fi->fileIdent + liu;\n\t\t} else {\n\t\t\tint poffset;\t/* Unpaded ending offset */\n\n\t\t\tpoffset = fibh->soffset + sizeof(struct fileIdentDesc) +\n\t\t\t\t\tliu + lfi;\n\n\t\t\tif (poffset >= lfi)\n\t\t\t\tnameptr = (uint8_t *)(fibh->ebh->b_data +\n\t\t\t\t\t\t      poffset - lfi);\n\t\t\telse {\n\t\t\t\tnameptr = fname;\n\t\t\t\tmemcpy(nameptr, fi->fileIdent + liu,\n\t\t\t\t\tlfi - poffset);\n\t\t\t\tmemcpy(nameptr + lfi - poffset,\n\t\t\t\t\tfibh->ebh->b_data, poffset);\n\t\t\t}\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_DELETED) != 0) {\n\t\t\tif (!UDF_QUERY_FLAG(dir->i_sb, UDF_FLAG_UNDELETE))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_HIDDEN) != 0) {\n\t\t\tif (!UDF_QUERY_FLAG(dir->i_sb, UDF_FLAG_UNHIDE))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif ((cfi->fileCharacteristics & FID_FILE_CHAR_PARENT) &&\n\t\t    isdotdot)\n\t\t\tgoto out_ok;\n\n\t\tif (!lfi)\n\t\t\tcontinue;\n\n\t\tflen = udf_get_filename(dir->i_sb, nameptr, lfi, fname,\n\t\t\t\t\tUDF_NAME_LEN);\n\t\tif (flen && udf_match(flen, fname, child->len, child->name))\n\t\t\tgoto out_ok;\n\t}\n\nout_err:\n\tfi = NULL;\n\tif (fibh->sbh != fibh->ebh)\n\t\tbrelse(fibh->ebh);\n\tbrelse(fibh->sbh);\nout_ok:\n\tbrelse(epos.bh);\n\tkfree(fname);\n\n\treturn fi;\n}",
        "description": "The UDF filesystem implementation in the Linux kernel prior to version 3.18.2 does not verify that sufficient space is allocated for storing a symlink target's name along with a trailing null character. This oversight enables local users to potentially access sensitive information through a specially crafted filesystem image.",
        "commit": "The vulnerability involves a failure to verify whether the resolved path from reading a symbolic link fits within the allocated buffer size. This oversight occurs during the process of reading symlinks, where the code does not account for potential encoding conversions that could increase the path length. Consequently, there is a risk that the path may exceed the buffer capacity, leading to buffer overflow issues."
    }
]