[
    {
        "cwe": "CWE-189",
        "func_name": "torvalds/i915_gem_execbuffer2",
        "score": 0.7717556357383728,
        "func_before": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "func_after": "int\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\n\tint ret;\n\n\tif (args->buffer_count < 1 ||\n\t    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {\n\t\tDRM_DEBUG(\"execbuf2 with %d buffers\\n\", args->buffer_count);\n\t\treturn -EINVAL;\n\t}\n\n\texec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\n\t\t\t     GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\n\tif (exec2_list == NULL)\n\t\texec2_list = drm_malloc_ab(sizeof(*exec2_list),\n\t\t\t\t\t   args->buffer_count);\n\tif (exec2_list == NULL) {\n\t\tDRM_DEBUG(\"Failed to allocate exec list for %d buffers\\n\",\n\t\t\t  args->buffer_count);\n\t\treturn -ENOMEM;\n\t}\n\tret = copy_from_user(exec2_list,\n\t\t\t     (struct drm_i915_relocation_entry __user *)\n\t\t\t     (uintptr_t) args->buffers_ptr,\n\t\t\t     sizeof(*exec2_list) * args->buffer_count);\n\tif (ret != 0) {\n\t\tDRM_DEBUG(\"copy %d exec entries failed %d\\n\",\n\t\t\t  args->buffer_count, ret);\n\t\tdrm_free_large(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\tret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\n\tif (!ret) {\n\t\t/* Copy the new buffer offsets back to the user's exec list. */\n\t\tret = copy_to_user((struct drm_i915_relocation_entry __user *)\n\t\t\t\t   (uintptr_t) args->buffers_ptr,\n\t\t\t\t   exec2_list,\n\t\t\t\t   sizeof(*exec2_list) * args->buffer_count);\n\t\tif (ret) {\n\t\t\tret = -EFAULT;\n\t\t\tDRM_DEBUG(\"failed to copy %d exec entries \"\n\t\t\t\t  \"back to user (%d)\\n\",\n\t\t\t\t  args->buffer_count, ret);\n\t\t}\n\t}\n\n\tdrm_free_large(exec2_list);\n\treturn ret;\n}",
        "description": "An integer overflow vulnerability exists within the `i915_gem_execbuffer2` function of the DRM subsystem in the Linux kernel prior to version 3.3.5 on 32-bit platforms. This flaw allows local users to trigger a denial of service through an out-of-bounds write or potentially cause other unspecified impacts via a specially crafted ioctl call.",
        "commit": "A vulnerability was identified in the i915 driver of the DRM subsystem within the Linux kernel, where a large `args->buffer_count` value provided by userspace through an ioctl call could cause an integer overflow on 32-bit systems. This overflow affects the calculation of the allocation size for temporary execution buffers, potentially leading to out-of-bounds memory access. This issue arose due to changes introduced in commit 8408c282, which attempted to optimize buffer allocation by first trying a normal large `kmalloc`."
    },
    {
        "cwe": "CWE-672",
        "func_name": "torvalds/get_gate_page",
        "score": 0.7710798382759094,
        "func_before": "static int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tif (pgd_none(*pgd))\n\t\treturn -EFAULT;\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn -EFAULT;\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn -EFAULT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tif (unlikely(!try_get_page(*page))) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}",
        "func_after": "static int get_gate_page(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int gup_flags, struct vm_area_struct **vma,\n\t\tstruct page **page)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret = -EFAULT;\n\n\t/* user gate pages are read-only */\n\tif (gup_flags & FOLL_WRITE)\n\t\treturn -EFAULT;\n\tif (address > TASK_SIZE)\n\t\tpgd = pgd_offset_k(address);\n\telse\n\t\tpgd = pgd_offset_gate(mm, address);\n\tif (pgd_none(*pgd))\n\t\treturn -EFAULT;\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn -EFAULT;\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn -EFAULT;\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn -EFAULT;\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tpte = pte_offset_map(pmd, address);\n\tif (pte_none(*pte))\n\t\tgoto unmap;\n\t*vma = get_gate_vma(mm);\n\tif (!page)\n\t\tgoto out;\n\t*page = vm_normal_page(*vma, address, *pte);\n\tif (!*page) {\n\t\tif ((gup_flags & FOLL_DUMP) || !is_zero_pfn(pte_pfn(*pte)))\n\t\t\tgoto unmap;\n\t\t*page = pte_page(*pte);\n\t}\n\tif (unlikely(!try_grab_page(*page, gup_flags))) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\nout:\n\tret = 0;\nunmap:\n\tpte_unmap(pte);\n\treturn ret;\n}",
        "description": "\"In the Linux kernel versions 5.7.x and 5.8.x prior to 5.8.7, a privilege escalation vulnerability exists due to incorrect reference counting of the struct page backing the vsyscall page, resulting in a refcount underflow. This issue can be exploited by any 64-bit process that has access to ptrace() or process_vm_readv() functions.\"",
        "commit": "It was discovered that gate pages were overlooked during the conversion from `get` to `pin_user_pages()`, leading to reference count imbalances. This issue can be reliably reproduced by running the x86 selftests with `vsyscall=emulate` enabled (the default setting). The problem arises because `pin_user_pages()` uses a \"bias\" value, manipulating the reference count by 1024 instead of 1, which is used by `get_user_pages()`. Gate pages, such as the vsyscall page, are typically part of the kernel image but are mapped to userspace, allowing access through interfaces using `get/pin_user_pages()`. The reference count of these kernel pages is adjusted similarly to normal user pages on the get/pin side to ensure consistency on the put/unpin side. However, `get_gate_page()` uses `try_get_page()`, which increments the reference count by 1, not 1024, even when called in the `pin_user_pages()` path. This results in a reference count discrepancy of 1023 when `unpin_user_pages()` is eventually called. The fix involves using `try_grab_page()` instead of `try_get_page()` and passing the appropriate flags to respect `FOLL_PIN`. This bug has been present since the introduction of `FOLL_PIN` support in commit 3faa52c03f44, which first appeared in the 5.7 release."
    },
    {
        "cwe": "CWE-129",
        "func_name": "admesh/stl_fix_normal_directions",
        "score": 0.761562168598175,
        "func_before": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "func_after": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1 &&\n         stl->neighbors_start[facet_num].neighbor[j] < stl->stats.number_of_facets*sizeof(char)) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "description": "An improper array index validation vulnerability exists in the stl_fix_normal_directions functionality of ADMesh. A specially-crafted STL file can lead to a heap buffer overflow. An attacker can exploit this by providing a malicious file.",
        "commit": "The vulnerability involves a check for the `neighbor_index` within the `stl_check_normal_vector` function. This fix addresses an issue identified in ticket #60."
    },
    {
        "cwe": "CWE-668",
        "func_name": "xen-project/sh_install_xen_entries_in_l4",
        "score": 0.750722348690033,
        "func_before": "void sh_install_xen_entries_in_l4(struct domain *d, mfn_t gl4mfn, mfn_t sl4mfn)\n{\n    shadow_l4e_t *sl4e;\n    unsigned int slots;\n\n    sl4e = map_domain_page(sl4mfn);\n    BUILD_BUG_ON(sizeof (l4_pgentry_t) != sizeof (shadow_l4e_t));\n\n    /* Copy the common Xen mappings from the idle domain */\n    slots = (shadow_mode_external(d)\n             ? ROOT_PAGETABLE_XEN_SLOTS\n             : ROOT_PAGETABLE_PV_XEN_SLOTS);\n    memcpy(&sl4e[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           &idle_pg_table[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           slots * sizeof(l4_pgentry_t));\n\n    /* Install the per-domain mappings for this domain */\n    sl4e[shadow_l4_table_offset(PERDOMAIN_VIRT_START)] =\n        shadow_l4e_from_mfn(page_to_mfn(d->arch.perdomain_l3_pg),\n                            __PAGE_HYPERVISOR_RW);\n\n    if ( !shadow_mode_external(d) && !is_pv_32bit_domain(d) &&\n         !VM_ASSIST(d, m2p_strict) )\n    {\n        /* open coded zap_ro_mpt(mfn_x(sl4mfn)): */\n        sl4e[shadow_l4_table_offset(RO_MPT_VIRT_START)] = shadow_l4e_empty();\n    }\n\n    /* Shadow linear mapping for 4-level shadows.  N.B. for 3-level\n     * shadows on 64-bit xen, this linear mapping is later replaced by the\n     * monitor pagetable structure, which is built in make_monitor_table\n     * and maintained by sh_update_linear_entries. */\n    sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n        shadow_l4e_from_mfn(sl4mfn, __PAGE_HYPERVISOR_RW);\n\n    /* Self linear mapping.  */\n    if ( shadow_mode_translate(d) && !shadow_mode_external(d) )\n    {\n        // linear tables may not be used with translated PV guests\n        sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n            shadow_l4e_empty();\n    }\n    else\n    {\n        sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n            shadow_l4e_from_mfn(gl4mfn, __PAGE_HYPERVISOR_RW);\n    }\n\n    unmap_domain_page(sl4e);\n}",
        "func_after": "void sh_install_xen_entries_in_l4(struct domain *d, mfn_t gl4mfn, mfn_t sl4mfn)\n{\n    shadow_l4e_t *sl4e;\n    unsigned int slots;\n\n    sl4e = map_domain_page(sl4mfn);\n    BUILD_BUG_ON(sizeof (l4_pgentry_t) != sizeof (shadow_l4e_t));\n\n    /* Copy the common Xen mappings from the idle domain */\n    slots = (shadow_mode_external(d)\n             ? ROOT_PAGETABLE_XEN_SLOTS\n             : ROOT_PAGETABLE_PV_XEN_SLOTS);\n    memcpy(&sl4e[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           &idle_pg_table[ROOT_PAGETABLE_FIRST_XEN_SLOT],\n           slots * sizeof(l4_pgentry_t));\n\n    /* Install the per-domain mappings for this domain */\n    sl4e[shadow_l4_table_offset(PERDOMAIN_VIRT_START)] =\n        shadow_l4e_from_mfn(page_to_mfn(d->arch.perdomain_l3_pg),\n                            __PAGE_HYPERVISOR_RW);\n\n    if ( !shadow_mode_external(d) && !is_pv_32bit_domain(d) &&\n         !VM_ASSIST(d, m2p_strict) )\n    {\n        /* open coded zap_ro_mpt(mfn_x(sl4mfn)): */\n        sl4e[shadow_l4_table_offset(RO_MPT_VIRT_START)] = shadow_l4e_empty();\n    }\n\n    /*\n     * Linear mapping slots:\n     *\n     * Calling this function with gl4mfn == sl4mfn is used to construct a\n     * monitor table for translated domains.  In this case, gl4mfn forms the\n     * self-linear mapping (i.e. not pointing into the translated domain), and\n     * the shadow-linear slot is skipped.  The shadow-linear slot is either\n     * filled when constructing lower level monitor tables, or via\n     * sh_update_cr3() for 4-level guests.\n     *\n     * Calling this function with gl4mfn != sl4mfn is used for non-translated\n     * guests, where the shadow-linear slot is actually self-linear, and the\n     * guest-linear slot points into the guests view of its pagetables.\n     */\n    if ( shadow_mode_translate(d) )\n    {\n        ASSERT(mfn_eq(gl4mfn, sl4mfn));\n\n        sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n            shadow_l4e_empty();\n    }\n    else\n    {\n        ASSERT(!mfn_eq(gl4mfn, sl4mfn));\n\n        sl4e[shadow_l4_table_offset(SH_LINEAR_PT_VIRT_START)] =\n            shadow_l4e_from_mfn(sl4mfn, __PAGE_HYPERVISOR_RW);\n    }\n\n    sl4e[shadow_l4_table_offset(LINEAR_PT_VIRT_START)] =\n        shadow_l4e_from_mfn(gl4mfn, __PAGE_HYPERVISOR_RW);\n\n    unmap_domain_page(sl4e);\n}",
        "description": "An issue was discovered in Xen versions up to 4.9.x, where x86 HVM guest OS users could potentially cause a denial of service (hypervisor crash) or escalate privileges due to improper handling of self-linear shadow mappings in translated guests.",
        "commit": "When setting up a monitor table for 4-level translated guests in the x86/shadow module, avoid creating self-linear shadow mappings. These mappings can confuse the writable heuristic logic, causing it to follow Xen's mappings instead of the guests' shadows. As a result, the function `sh_guess_wrmap()` must handle cases where no shadow-linear mapping is present, which happens each time a vCPU switches to 4-level paging from a different mode. Appropriate shadow-linear slots are inserted into the monitor table either during the construction of lower-level tables or by `sh_update_cr3()`. Additionally, clarify the safety of other mappings; although they may appear unsafe, creating guest-linear mappings for translated domains is correct because they are self-linear and do not point into the translated domain. Remove a redundant clause for guests that are not external translators. This addresses issue XSA-243."
    },
    {
        "cwe": "CWE-697",
        "func_name": "cvxopt/symbolic",
        "score": 0.7704260349273682,
        "func_before": "static PyObject* symbolic(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *A;\n    cholmod_sparse *Ac = NULL;\n    cholmod_factor *L;\n    matrix *P=NULL;\n#if PY_MAJOR_VERSION >= 3\n    int uplo_='L';\n#endif\n    char uplo='L';\n    int n;\n    char *kwlist[] = {\"A\", \"p\", \"uplo\", NULL};\n\n    if (!set_options()) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|OC\", kwlist, &A,\n        &P, &uplo_)) return NULL;\n    uplo = (char) uplo_;\n#else\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|Oc\", kwlist, &A,\n        &P, &uplo)) return NULL;\n#endif\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a square sparse matrix\");\n    n = SP_NROWS(A);\n\n    if (P) {\n        if (!Matrix_Check(P) || MAT_ID(P) != INT) err_int_mtrx(\"p\");\n        if (MAT_LGT(P) != n) err_buf_len(\"p\");\n        if (!CHOL(check_perm)(P->buffer, n, n, &Common))\n            PY_ERR(PyExc_ValueError, \"p is not a valid permutation\");\n    }\n    if (uplo != 'U' && uplo != 'L') err_char(\"uplo\", \"'L', 'U'\");\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    L = CHOL(analyze_p)(Ac, P ? MAT_BUFI(P): NULL, NULL, 0, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status != CHOLMOD_OK){\n        if (Common.status == CHOLMOD_OUT_OF_MEMORY)\n            return PyErr_NoMemory();\n        else{\n            PyErr_SetString(PyExc_ValueError, \"symbolic factorization \"\n                \"failed\");\n            return NULL;\n        }\n    }\n#if PY_MAJOR_VERSION >= 3\n    return (PyObject *) PyCapsule_New((void *) L, SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n        (PyCapsule_Destructor) &cvxopt_free_cholmod_factor);\n#else\n    return (PyObject *) PyCObject_FromVoidPtrAndDesc((void *) L,\n        SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n\tcvxopt_free_cholmod_factor);\n#endif\n}",
        "func_after": "static PyObject* symbolic(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *A;\n    cholmod_sparse *Ac = NULL;\n    cholmod_factor *L;\n    matrix *P=NULL;\n#if PY_MAJOR_VERSION >= 3\n    int uplo_='L';\n#endif\n    char uplo='L';\n    int n;\n    char *kwlist[] = {\"A\", \"p\", \"uplo\", NULL};\n\n    if (!set_options()) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|OC\", kwlist, &A,\n        &P, &uplo_)) return NULL;\n    uplo = (char) uplo_;\n#else\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|Oc\", kwlist, &A,\n        &P, &uplo)) return NULL;\n#endif\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a square sparse matrix\");\n    n = SP_NROWS(A);\n\n    if (P) {\n        if (!Matrix_Check(P) || MAT_ID(P) != INT) err_int_mtrx(\"p\");\n        if (MAT_LGT(P) != n) err_buf_len(\"p\");\n        if (!CHOL(check_perm)(P->buffer, n, n, &Common))\n            PY_ERR(PyExc_ValueError, \"p is not a valid permutation\");\n    }\n    if (uplo != 'U' && uplo != 'L') err_char(\"uplo\", \"'L', 'U'\");\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    L = CHOL(analyze_p)(Ac, P ? MAT_BUFI(P): NULL, NULL, 0, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status != CHOLMOD_OK){\n        if (Common.status == CHOLMOD_OUT_OF_MEMORY)\n            return PyErr_NoMemory();\n        else{\n            PyErr_SetString(PyExc_ValueError, \"symbolic factorization \"\n                \"failed\");\n            return NULL;\n        }\n    }\n#if PY_MAJOR_VERSION >= 3\n    return (PyObject *) PyCapsule_New((void *) L, SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  strCFDL : strCFDU) :\n        (uplo == 'L' ?  strCFZL : strCFZU),\n        (PyCapsule_Destructor) &cvxopt_free_cholmod_factor);\n#else\n    return (PyObject *) PyCObject_FromVoidPtrAndDesc((void *) L,\n        SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  strCFDL : strCFDU) :\n        (uplo == 'L' ?  strCFZL : strCFZU),\n\tcvxopt_free_cholmod_factor);\n#endif\n}",
        "description": "An incomplete string comparison vulnerability exists in the cvxopt library versions up to 1.2.6 within certain APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve). This flaw enables attackers to carry out Denial of Service (DoS) attacks by constructing fake Capsule objects.",
        "commit": "This update resolves an unspecified issue identified by #193, updates the associated documentation, and increments the version number to 1.2.7."
    }
]