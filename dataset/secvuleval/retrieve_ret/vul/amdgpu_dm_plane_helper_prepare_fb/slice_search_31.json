[
    {
        "cwe": "CWE-670",
        "func_name": "xen-project/port_is_valid",
        "score": 0.7388275265693665,
        "func_before": "static inline bool_t port_is_valid(struct domain *d, unsigned int p)\n{\n    return p < read_atomic(&d->valid_evtchns);\n}",
        "func_after": "static inline bool_t port_is_valid(struct domain *d, unsigned int p)\n{\n    if ( p >= read_atomic(&d->valid_evtchns) )\n        return false;\n\n    /*\n     * The caller will usually access the event channel afterwards and\n     * may be done without taking the per-domain lock. The barrier is\n     * going in pair the smp_wmb() barrier in evtchn_allocate_port().\n     */\n    smp_rmb();\n\n    return true;\n}",
        "description": "An issue was discovered in Xen through version 4.14.x, where memory barriers are absent during the access or allocation of event channels. Event channels control structures can be accessed without locks as long as the port is deemed valid. However, the absence of an appropriate memory barrier (such as smp_*mb()) allows the compiler and CPU to reorder memory accesses. This could enable a malicious guest to trigger a hypervisor crash, leading to a Denial of Service (DoS). Additionally, information leaks and privilege escalations cannot be ruled out. The vulnerability affects all versions of Xen, with the likelihood of exploitation depending on the CPU and compiler used to build Xen. The exact impact varies based on the compiler's code generation options and the CPU's ability to reorder memory accesses. It is recommended to consult the CPU vendor for guidance on potential vulnerabilities on Arm systems, while x86 systems are only vulnerable if a compiler performs reordering.",
        "commit": "It was discovered that the Xen hypervisor's event channel management lacked appropriate memory barriers during both allocation and access operations. Specifically, while the allocation of an event channel bucket is protected by a per-domain lock, accessing the bucket can occur without the lock being held, relying instead on the `port_is_valid()` function to ensure the port has an associated structure. However, due to potential compiler and processor reordering of memory accesses, there is a risk that updates to `d->valid_evtchns` could occur before the new bucket is fully allocated. To mitigate this issue, memory barriers were added: a write memory barrier during allocation and a read memory barrier when checking if the port is valid. This addresses a reordering problem that could lead to unintended behavior."
    },
    {
        "cwe": "CWE-476",
        "func_name": "ImageMagick/LoadOpenCLDevices",
        "score": 0.8150505423545837,
        "func_before": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireQuantumMemory(length,\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "func_after": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireCriticalMemory(length*\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "description": "An issue was discovered in ImageMagick where a NULL pointer dereference vulnerability exists in the function responsible for loading OpenCL devices. This flaw allows attackers to trigger a denial of service through the use of a specially crafted file.",
        "commit": "It was discovered that ImageMagick, a popular image processing library, contains a vulnerability related to improper handling of certain image formats, potentially leading to buffer overflows or other memory corruption issues."
    },
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7619717717170715,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-129",
        "func_name": "admesh/stl_fix_normal_directions",
        "score": 0.7977562546730042,
        "func_before": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "func_after": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1 &&\n         stl->neighbors_start[facet_num].neighbor[j] < stl->stats.number_of_facets*sizeof(char)) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "description": "An improper array index validation vulnerability exists in the stl_fix_normal_directions functionality of ADMesh. A specially-crafted STL file can lead to a heap buffer overflow. An attacker can exploit this by providing a malicious file.",
        "commit": "The vulnerability involves a check for the `neighbor_index` within the `stl_check_normal_vector` function. This fix addresses an issue identified in ticket #60."
    },
    {
        "cwe": "CWE-388",
        "func_name": "torvalds/handle_exception",
        "score": 0.8173779249191284,
        "func_before": "static int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif ((intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR)\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_no_device(intr_info)) {\n\t\tvmx_fpu_activate(vcpu);\n\t\treturn 1;\n\t}\n\n\tif (is_invalid_opcode(intr_info)) {\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\ter = emulate_instruction(vcpu, EMULTYPE_TRAP_UD);\n\t\tif (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\t/* EPT won't cause page fault directly */\n\t\tBUG_ON(enable_ept);\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\ttrace_kvm_page_fault(cr2, error_code);\n\n\t\tif (kvm_event_needs_reinjection(vcpu))\n\t\t\tkvm_mmu_unprotect_page_virt(vcpu, cr2);\n\t\treturn kvm_mmu_page_fault(vcpu, cr2, error_code, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (!(dr6 & ~DR6_RESERVED)) /* icebp */\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "func_after": "static int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif (is_nmi(intr_info))\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_no_device(intr_info)) {\n\t\tvmx_fpu_activate(vcpu);\n\t\treturn 1;\n\t}\n\n\tif (is_invalid_opcode(intr_info)) {\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\ter = emulate_instruction(vcpu, EMULTYPE_TRAP_UD);\n\t\tif (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\t/* EPT won't cause page fault directly */\n\t\tBUG_ON(enable_ept);\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\ttrace_kvm_page_fault(cr2, error_code);\n\n\t\tif (kvm_event_needs_reinjection(vcpu))\n\t\t\tkvm_mmu_unprotect_page_virt(vcpu, cr2);\n\t\treturn kvm_mmu_page_fault(vcpu, cr2, error_code, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (!(dr6 & ~DR6_RESERVED)) /* icebp */\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "description": "The Linux kernel through version 4.9 improperly manages the #BP (Breakpoint) and #OF (Overflow) exceptions in the arch/x86/kvm/vmx.c file. This mismanagement allows guest operating system users to trigger a denial of service (resulting in a guest OS crash) by refusing to handle an exception thrown by an L2 guest.",
        "commit": "In the KVM (Kernel-based Virtual Machine) subsystem, specifically within the nested virtualization (nVMX) feature, there is an enhancement that allows the Level 1 (L1) hypervisor to intercept software exceptions such as Breakpoint (#BP) and Overflow (#OF). Previously, only hardware exceptions were forwarded to L1; now, when Level 2 (L2) exits to Level 0 (L0) due to an \"exception or NMI,\" these software exceptions should be handled by L1 if L1 has requested interception."
    }
]