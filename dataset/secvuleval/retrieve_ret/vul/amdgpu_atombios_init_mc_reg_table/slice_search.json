[
    {
        "cwe": "CWE-682",
        "func_name": "torvalds/sctp_make_init",
        "score": 0.7975322604179382,
        "func_before": "struct sctp_chunk *sctp_make_init(const struct sctp_association *asoc,\n\t\t\t     const struct sctp_bind_addr *bp,\n\t\t\t     gfp_t gfp, int vparam_len)\n{\n\tsctp_inithdr_t init;\n\tunion sctp_params addrs;\n\tsize_t chunksize;\n\tstruct sctp_chunk *retval = NULL;\n\tint num_types, addrs_len = 0;\n\tstruct sctp_sock *sp;\n\tsctp_supported_addrs_param_t sat;\n\t__be16 types[2];\n\tsctp_adaptation_ind_param_t aiparam;\n\tsctp_supported_ext_param_t ext_param;\n\tint num_ext = 0;\n\t__u8 extensions[3];\n\tsctp_paramhdr_t *auth_chunks = NULL,\n\t\t\t*auth_hmacs = NULL;\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 1: The INIT chunks can contain multiple addresses that\n\t * can be IPv4 and/or IPv6 in any combination.\n\t */\n\tretval = NULL;\n\n\t/* Convert the provided bind address list to raw format. */\n\taddrs = sctp_bind_addrs_to_raw(bp, &addrs_len, gfp);\n\n\tinit.init_tag\t\t   = htonl(asoc->c.my_vtag);\n\tinit.a_rwnd\t\t   = htonl(asoc->rwnd);\n\tinit.num_outbound_streams  = htons(asoc->c.sinit_num_ostreams);\n\tinit.num_inbound_streams   = htons(asoc->c.sinit_max_instreams);\n\tinit.initial_tsn\t   = htonl(asoc->c.initial_tsn);\n\n\t/* How many address types are needed? */\n\tsp = sctp_sk(asoc->base.sk);\n\tnum_types = sp->pf->supported_addrs(sp, types);\n\n\tchunksize = sizeof(init) + addrs_len + SCTP_SAT_LEN(num_types);\n\tchunksize += sizeof(ecap_param);\n\n\tif (sctp_prsctp_enable)\n\t\tchunksize += sizeof(prsctp_param);\n\n\t/* ADDIP: Section 4.2.7:\n\t *  An implementation supporting this extension [ADDIP] MUST list\n\t *  the ASCONF,the ASCONF-ACK, and the AUTH  chunks in its INIT and\n\t *  INIT-ACK parameters.\n\t */\n\tif (sctp_addip_enable) {\n\t\textensions[num_ext] = SCTP_CID_ASCONF;\n\t\textensions[num_ext+1] = SCTP_CID_ASCONF_ACK;\n\t\tnum_ext += 2;\n\t}\n\n\tif (sp->adaptation_ind)\n\t\tchunksize += sizeof(aiparam);\n\n\tchunksize += vparam_len;\n\n\t/* Account for AUTH related parameters */\n\tif (sctp_auth_enable) {\n\t\t/* Add random parameter length*/\n\t\tchunksize += sizeof(asoc->c.auth_random);\n\n\t\t/* Add HMACS parameter length if any were defined */\n\t\tauth_hmacs = (sctp_paramhdr_t *)asoc->c.auth_hmacs;\n\t\tif (auth_hmacs->length)\n\t\t\tchunksize += ntohs(auth_hmacs->length);\n\t\telse\n\t\t\tauth_hmacs = NULL;\n\n\t\t/* Add CHUNKS parameter length */\n\t\tauth_chunks = (sctp_paramhdr_t *)asoc->c.auth_chunks;\n\t\tif (auth_chunks->length)\n\t\t\tchunksize += ntohs(auth_chunks->length);\n\t\telse\n\t\t\tauth_chunks = NULL;\n\n\t\textensions[num_ext] = SCTP_CID_AUTH;\n\t\tnum_ext += 1;\n\t}\n\n\t/* If we have any extensions to report, account for that */\n\tif (num_ext)\n\t\tchunksize += sizeof(sctp_supported_ext_param_t) + num_ext;\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 3: An INIT chunk MUST NOT contain more than one Host\n\t * Name address parameter. Moreover, the sender of the INIT\n\t * MUST NOT combine any other address types with the Host Name\n\t * address in the INIT. The receiver of INIT MUST ignore any\n\t * other address types if the Host Name address parameter is\n\t * present in the received INIT chunk.\n\t *\n\t * PLEASE DO NOT FIXME [This version does not support Host Name.]\n\t */\n\n\tretval = sctp_make_chunk(asoc, SCTP_CID_INIT, 0, chunksize);\n\tif (!retval)\n\t\tgoto nodata;\n\n\tretval->subh.init_hdr =\n\t\tsctp_addto_chunk(retval, sizeof(init), &init);\n\tretval->param_hdr.v =\n\t\tsctp_addto_chunk(retval, addrs_len, addrs.v);\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 4: This parameter, when present, specifies all the\n\t * address types the sending endpoint can support. The absence\n\t * of this parameter indicates that the sending endpoint can\n\t * support any address type.\n\t */\n\tsat.param_hdr.type = SCTP_PARAM_SUPPORTED_ADDRESS_TYPES;\n\tsat.param_hdr.length = htons(SCTP_SAT_LEN(num_types));\n\tsctp_addto_chunk(retval, sizeof(sat), &sat);\n\tsctp_addto_chunk(retval, num_types * sizeof(__u16), &types);\n\n\tsctp_addto_chunk(retval, sizeof(ecap_param), &ecap_param);\n\n\t/* Add the supported extensions parameter.  Be nice and add this\n\t * fist before addiding the parameters for the extensions themselves\n\t */\n\tif (num_ext) {\n\t\text_param.param_hdr.type = SCTP_PARAM_SUPPORTED_EXT;\n\t\text_param.param_hdr.length =\n\t\t\t    htons(sizeof(sctp_supported_ext_param_t) + num_ext);\n\t\tsctp_addto_chunk(retval, sizeof(sctp_supported_ext_param_t),\n\t\t\t\t&ext_param);\n\t\tsctp_addto_param(retval, num_ext, extensions);\n\t}\n\n\tif (sctp_prsctp_enable)\n\t\tsctp_addto_chunk(retval, sizeof(prsctp_param), &prsctp_param);\n\n\tif (sp->adaptation_ind) {\n\t\taiparam.param_hdr.type = SCTP_PARAM_ADAPTATION_LAYER_IND;\n\t\taiparam.param_hdr.length = htons(sizeof(aiparam));\n\t\taiparam.adaptation_ind = htonl(sp->adaptation_ind);\n\t\tsctp_addto_chunk(retval, sizeof(aiparam), &aiparam);\n\t}\n\n\t/* Add SCTP-AUTH chunks to the parameter list */\n\tif (sctp_auth_enable) {\n\t\tsctp_addto_chunk(retval, sizeof(asoc->c.auth_random),\n\t\t\t\t asoc->c.auth_random);\n\t\tif (auth_hmacs)\n\t\t\tsctp_addto_chunk(retval, ntohs(auth_hmacs->length),\n\t\t\t\t\tauth_hmacs);\n\t\tif (auth_chunks)\n\t\t\tsctp_addto_chunk(retval, ntohs(auth_chunks->length),\n\t\t\t\t\tauth_chunks);\n\t}\nnodata:\n\tkfree(addrs.v);\n\treturn retval;\n}",
        "func_after": "struct sctp_chunk *sctp_make_init(const struct sctp_association *asoc,\n\t\t\t     const struct sctp_bind_addr *bp,\n\t\t\t     gfp_t gfp, int vparam_len)\n{\n\tsctp_inithdr_t init;\n\tunion sctp_params addrs;\n\tsize_t chunksize;\n\tstruct sctp_chunk *retval = NULL;\n\tint num_types, addrs_len = 0;\n\tstruct sctp_sock *sp;\n\tsctp_supported_addrs_param_t sat;\n\t__be16 types[2];\n\tsctp_adaptation_ind_param_t aiparam;\n\tsctp_supported_ext_param_t ext_param;\n\tint num_ext = 0;\n\t__u8 extensions[3];\n\tsctp_paramhdr_t *auth_chunks = NULL,\n\t\t\t*auth_hmacs = NULL;\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 1: The INIT chunks can contain multiple addresses that\n\t * can be IPv4 and/or IPv6 in any combination.\n\t */\n\tretval = NULL;\n\n\t/* Convert the provided bind address list to raw format. */\n\taddrs = sctp_bind_addrs_to_raw(bp, &addrs_len, gfp);\n\n\tinit.init_tag\t\t   = htonl(asoc->c.my_vtag);\n\tinit.a_rwnd\t\t   = htonl(asoc->rwnd);\n\tinit.num_outbound_streams  = htons(asoc->c.sinit_num_ostreams);\n\tinit.num_inbound_streams   = htons(asoc->c.sinit_max_instreams);\n\tinit.initial_tsn\t   = htonl(asoc->c.initial_tsn);\n\n\t/* How many address types are needed? */\n\tsp = sctp_sk(asoc->base.sk);\n\tnum_types = sp->pf->supported_addrs(sp, types);\n\n\tchunksize = sizeof(init) + addrs_len;\n\tchunksize += WORD_ROUND(SCTP_SAT_LEN(num_types));\n\tchunksize += sizeof(ecap_param);\n\n\tif (sctp_prsctp_enable)\n\t\tchunksize += sizeof(prsctp_param);\n\n\t/* ADDIP: Section 4.2.7:\n\t *  An implementation supporting this extension [ADDIP] MUST list\n\t *  the ASCONF,the ASCONF-ACK, and the AUTH  chunks in its INIT and\n\t *  INIT-ACK parameters.\n\t */\n\tif (sctp_addip_enable) {\n\t\textensions[num_ext] = SCTP_CID_ASCONF;\n\t\textensions[num_ext+1] = SCTP_CID_ASCONF_ACK;\n\t\tnum_ext += 2;\n\t}\n\n\tif (sp->adaptation_ind)\n\t\tchunksize += sizeof(aiparam);\n\n\tchunksize += vparam_len;\n\n\t/* Account for AUTH related parameters */\n\tif (sctp_auth_enable) {\n\t\t/* Add random parameter length*/\n\t\tchunksize += sizeof(asoc->c.auth_random);\n\n\t\t/* Add HMACS parameter length if any were defined */\n\t\tauth_hmacs = (sctp_paramhdr_t *)asoc->c.auth_hmacs;\n\t\tif (auth_hmacs->length)\n\t\t\tchunksize += WORD_ROUND(ntohs(auth_hmacs->length));\n\t\telse\n\t\t\tauth_hmacs = NULL;\n\n\t\t/* Add CHUNKS parameter length */\n\t\tauth_chunks = (sctp_paramhdr_t *)asoc->c.auth_chunks;\n\t\tif (auth_chunks->length)\n\t\t\tchunksize += WORD_ROUND(ntohs(auth_chunks->length));\n\t\telse\n\t\t\tauth_chunks = NULL;\n\n\t\textensions[num_ext] = SCTP_CID_AUTH;\n\t\tnum_ext += 1;\n\t}\n\n\t/* If we have any extensions to report, account for that */\n\tif (num_ext)\n\t\tchunksize += WORD_ROUND(sizeof(sctp_supported_ext_param_t) +\n\t\t\t\t\tnum_ext);\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 3: An INIT chunk MUST NOT contain more than one Host\n\t * Name address parameter. Moreover, the sender of the INIT\n\t * MUST NOT combine any other address types with the Host Name\n\t * address in the INIT. The receiver of INIT MUST ignore any\n\t * other address types if the Host Name address parameter is\n\t * present in the received INIT chunk.\n\t *\n\t * PLEASE DO NOT FIXME [This version does not support Host Name.]\n\t */\n\n\tretval = sctp_make_chunk(asoc, SCTP_CID_INIT, 0, chunksize);\n\tif (!retval)\n\t\tgoto nodata;\n\n\tretval->subh.init_hdr =\n\t\tsctp_addto_chunk(retval, sizeof(init), &init);\n\tretval->param_hdr.v =\n\t\tsctp_addto_chunk(retval, addrs_len, addrs.v);\n\n\t/* RFC 2960 3.3.2 Initiation (INIT) (1)\n\t *\n\t * Note 4: This parameter, when present, specifies all the\n\t * address types the sending endpoint can support. The absence\n\t * of this parameter indicates that the sending endpoint can\n\t * support any address type.\n\t */\n\tsat.param_hdr.type = SCTP_PARAM_SUPPORTED_ADDRESS_TYPES;\n\tsat.param_hdr.length = htons(SCTP_SAT_LEN(num_types));\n\tsctp_addto_chunk(retval, sizeof(sat), &sat);\n\tsctp_addto_chunk(retval, num_types * sizeof(__u16), &types);\n\n\tsctp_addto_chunk(retval, sizeof(ecap_param), &ecap_param);\n\n\t/* Add the supported extensions parameter.  Be nice and add this\n\t * fist before addiding the parameters for the extensions themselves\n\t */\n\tif (num_ext) {\n\t\text_param.param_hdr.type = SCTP_PARAM_SUPPORTED_EXT;\n\t\text_param.param_hdr.length =\n\t\t\t    htons(sizeof(sctp_supported_ext_param_t) + num_ext);\n\t\tsctp_addto_chunk(retval, sizeof(sctp_supported_ext_param_t),\n\t\t\t\t&ext_param);\n\t\tsctp_addto_param(retval, num_ext, extensions);\n\t}\n\n\tif (sctp_prsctp_enable)\n\t\tsctp_addto_chunk(retval, sizeof(prsctp_param), &prsctp_param);\n\n\tif (sp->adaptation_ind) {\n\t\taiparam.param_hdr.type = SCTP_PARAM_ADAPTATION_LAYER_IND;\n\t\taiparam.param_hdr.length = htons(sizeof(aiparam));\n\t\taiparam.adaptation_ind = htonl(sp->adaptation_ind);\n\t\tsctp_addto_chunk(retval, sizeof(aiparam), &aiparam);\n\t}\n\n\t/* Add SCTP-AUTH chunks to the parameter list */\n\tif (sctp_auth_enable) {\n\t\tsctp_addto_chunk(retval, sizeof(asoc->c.auth_random),\n\t\t\t\t asoc->c.auth_random);\n\t\tif (auth_hmacs)\n\t\t\tsctp_addto_chunk(retval, ntohs(auth_hmacs->length),\n\t\t\t\t\tauth_hmacs);\n\t\tif (auth_chunks)\n\t\t\tsctp_addto_chunk(retval, ntohs(auth_chunks->length),\n\t\t\t\t\tauth_chunks);\n\t}\nnodata:\n\tkfree(addrs.v);\n\treturn retval;\n}",
        "description": "In the Linux kernel prior to version 2.6.34, when both addip_enable and auth_enable are enabled, the function responsible for creating SCTP chunks does not account for the amount of zero padding during the calculation of chunk lengths for INIT and INIT ACK chunks. This oversight allows remote attackers to exploit this vulnerability by sending specially crafted packets, resulting in a denial of service (OOPS).",
        "commit": "When calculating the INIT/INIT-ACK chunk length, it is essential to consider not only the length of the parameters but also the zero padding length of these parameters, such as the AUTH HMACS parameter and CHUNKS parameter. Failing to account for the zero padding length can lead to a kernel panic, specifically an `skb_over_panic` error, which occurs when attempting to write beyond the buffer's allocated space. This issue arises within the SCTP protocol implementation in the Linux kernel, potentially affecting network communication stability and security."
    },
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7432342767715454,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-908",
        "func_name": "php/gdImageCreateFromXbm",
        "score": 0.7875237464904785,
        "func_before": "gdImagePtr gdImageCreateFromXbm(FILE * fd)\n{\n\tchar fline[MAX_XBM_LINE_SIZE];\n\tchar iname[MAX_XBM_LINE_SIZE];\n\tchar *type;\n\tint value;\n\tunsigned int width = 0, height = 0;\n\tint fail = 0;\n\tint max_bit = 0;\n\n\tgdImagePtr im;\n\tint bytes = 0, i;\n\tint bit, x = 0, y = 0;\n\tint ch;\n\tchar h[8];\n\tunsigned int b;\n\n\trewind(fd);\n\twhile (fgets(fline, MAX_XBM_LINE_SIZE, fd)) {\n\t\tfline[MAX_XBM_LINE_SIZE-1] = '\\0';\n\t\tif (strlen(fline) == MAX_XBM_LINE_SIZE-1) {\n\t\t\treturn 0;\n\t\t}\n\t\tif (sscanf(fline, \"#define %s %d\", iname, &value) == 2) {\n\t\t\tif (!(type = strrchr(iname, '_'))) {\n\t\t\t\ttype = iname;\n\t\t\t} else {\n\t\t\t\ttype++;\n\t\t\t}\n\n\t\t\tif (!strcmp(\"width\", type)) {\n\t\t\t\twidth = (unsigned int) value;\n\t\t\t}\n\t\t\tif (!strcmp(\"height\", type)) {\n\t\t\t\theight = (unsigned int) value;\n\t\t\t}\n\t\t} else {\n\t\t\tif ( sscanf(fline, \"static unsigned char %s = {\", iname) == 1\n\t\t\t  || sscanf(fline, \"static char %s = {\", iname) == 1)\n\t\t\t{\n\t\t\t\tmax_bit = 128;\n\t\t\t} else if (sscanf(fline, \"static unsigned short %s = {\", iname) == 1\n\t\t\t\t\t|| sscanf(fline, \"static short %s = {\", iname) == 1)\n\t\t\t{\n\t\t\t\tmax_bit = 32768;\n\t\t\t}\n\t\t\tif (max_bit) {\n\t\t\t\tbytes = (width + 7) / 8 * height;\n\t\t\t\tif (!bytes) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tif (!(type = strrchr(iname, '_'))) {\n\t\t\t\t\ttype = iname;\n\t\t\t\t} else {\n\t\t\t\t\ttype++;\n\t\t\t\t}\n\t\t\t\tif (!strcmp(\"bits[]\", type)) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n \t\t}\n\t}\n\tif (!bytes || !max_bit) {\n\t\treturn 0;\n\t}\n\n\tif(!(im = gdImageCreate(width, height))) {\n\t\treturn 0;\n\t}\n\tgdImageColorAllocate(im, 255, 255, 255);\n\tgdImageColorAllocate(im, 0, 0, 0);\n\th[2] = '\\0';\n\th[4] = '\\0';\n\tfor (i = 0; i < bytes; i++) {\n\t\twhile (1) {\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tfail = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ch == 'x') {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (fail) {\n\t\t\tbreak;\n\t\t}\n\t\t/* Get hex value */\n\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\tbreak;\n\t\t}\n\t\th[0] = ch;\n\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\tbreak;\n\t\t}\n\t\th[1] = ch;\n\t\tif (max_bit == 32768) {\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\th[2] = ch;\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\th[3] = ch;\n\t\t}\n\t\tsscanf(h, \"%x\", &b);\n\t\tfor (bit = 1; bit <= max_bit; bit = bit << 1) {\n\t\t\tgdImageSetPixel(im, x++, y, (b & bit) ? 1 : 0);\n\t\t\tif (x == im->sx) {\n\t\t\t\tx = 0;\n\t\t\t\ty++;\n\t\t\t\tif (y == im->sy) {\n\t\t\t\t\treturn im;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tphp_gd_error(\"EOF before image was complete\");\n\tgdImageDestroy(im);\n\treturn 0;\n}",
        "func_after": "gdImagePtr gdImageCreateFromXbm(FILE * fd)\n{\n\tchar fline[MAX_XBM_LINE_SIZE];\n\tchar iname[MAX_XBM_LINE_SIZE];\n\tchar *type;\n\tint value;\n\tunsigned int width = 0, height = 0;\n\tint fail = 0;\n\tint max_bit = 0;\n\n\tgdImagePtr im;\n\tint bytes = 0, i;\n\tint bit, x = 0, y = 0;\n\tint ch;\n\tchar h[8];\n\tunsigned int b;\n\n\trewind(fd);\n\twhile (fgets(fline, MAX_XBM_LINE_SIZE, fd)) {\n\t\tfline[MAX_XBM_LINE_SIZE-1] = '\\0';\n\t\tif (strlen(fline) == MAX_XBM_LINE_SIZE-1) {\n\t\t\treturn 0;\n\t\t}\n\t\tif (sscanf(fline, \"#define %s %d\", iname, &value) == 2) {\n\t\t\tif (!(type = strrchr(iname, '_'))) {\n\t\t\t\ttype = iname;\n\t\t\t} else {\n\t\t\t\ttype++;\n\t\t\t}\n\n\t\t\tif (!strcmp(\"width\", type)) {\n\t\t\t\twidth = (unsigned int) value;\n\t\t\t}\n\t\t\tif (!strcmp(\"height\", type)) {\n\t\t\t\theight = (unsigned int) value;\n\t\t\t}\n\t\t} else {\n\t\t\tif ( sscanf(fline, \"static unsigned char %s = {\", iname) == 1\n\t\t\t  || sscanf(fline, \"static char %s = {\", iname) == 1)\n\t\t\t{\n\t\t\t\tmax_bit = 128;\n\t\t\t} else if (sscanf(fline, \"static unsigned short %s = {\", iname) == 1\n\t\t\t\t\t|| sscanf(fline, \"static short %s = {\", iname) == 1)\n\t\t\t{\n\t\t\t\tmax_bit = 32768;\n\t\t\t}\n\t\t\tif (max_bit) {\n\t\t\t\tbytes = (width + 7) / 8 * height;\n\t\t\t\tif (!bytes) {\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tif (!(type = strrchr(iname, '_'))) {\n\t\t\t\t\ttype = iname;\n\t\t\t\t} else {\n\t\t\t\t\ttype++;\n\t\t\t\t}\n\t\t\t\tif (!strcmp(\"bits[]\", type)) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n \t\t}\n\t}\n\tif (!bytes || !max_bit) {\n\t\treturn 0;\n\t}\n\n\tif(!(im = gdImageCreate(width, height))) {\n\t\treturn 0;\n\t}\n\tgdImageColorAllocate(im, 255, 255, 255);\n\tgdImageColorAllocate(im, 0, 0, 0);\n\th[2] = '\\0';\n\th[4] = '\\0';\n\tfor (i = 0; i < bytes; i++) {\n\t\twhile (1) {\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tfail = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ch == 'x') {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (fail) {\n\t\t\tbreak;\n\t\t}\n\t\t/* Get hex value */\n\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\tbreak;\n\t\t}\n\t\th[0] = ch;\n\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\tbreak;\n\t\t}\n\t\th[1] = ch;\n\t\tif (max_bit == 32768) {\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\th[2] = ch;\n\t\t\tif ((ch=getc(fd)) == EOF) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\th[3] = ch;\n\t\t}\n\t\tif (sscanf(h, \"%x\", &b) != 1) {\n\t\t\tphp_gd_error(\"invalid XBM\");\n\t\t\tgdImageDestroy(im);\n\t\t\treturn 0;\n\t\t}\n\t\tfor (bit = 1; bit <= max_bit; bit = bit << 1) {\n\t\t\tgdImageSetPixel(im, x++, y, (b & bit) ? 1 : 0);\n\t\t\tif (x == im->sx) {\n\t\t\t\tx = 0;\n\t\t\t\ty++;\n\t\t\t\tif (y == im->sy) {\n\t\t\t\t\treturn im;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tphp_gd_error(\"EOF before image was complete\");\n\tgdImageDestroy(im);\n\treturn 0;\n}",
        "description": "When utilizing the gdImageCreateFromXbm() function within the GD Graphics Library (LibGD) version 2.2.5, as employed in the PHP GD extension across various PHP versions, it is feasible to provide data that results in the function accessing an uninitialized variable. This scenario can potentially expose stack contents that were previously left by other code, leading to information disclosure vulnerabilities.",
        "commit": "It was discovered that the `gdImageCreateFromXbm` function contains an uninitialized read issue. To address this, it is essential to verify that the `sscanf()` function successfully reads a hexadecimal value and to terminate the process otherwise."
    },
    {
        "cwe": "CWE-476",
        "func_name": "ImageMagick/LoadOpenCLDevices",
        "score": 0.79816073179245,
        "func_before": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireQuantumMemory(length,\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "func_after": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireCriticalMemory(length*\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "description": "An issue was discovered in ImageMagick where a NULL pointer dereference vulnerability exists in the function responsible for loading OpenCL devices. This flaw allows attackers to trigger a denial of service through the use of a specially crafted file.",
        "commit": "It was discovered that ImageMagick, a popular image processing library, contains a vulnerability related to improper handling of certain image formats, potentially leading to buffer overflows or other memory corruption issues."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.800971269607544,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    }
]