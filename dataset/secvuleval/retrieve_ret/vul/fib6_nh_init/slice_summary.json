[
    {
        "cwe": "CWE-20",
        "func_name": "torvalds/cifs_lookup",
        "score": 0.773457407951355,
        "func_before": "struct dentry *\ncifs_lookup(struct inode *parent_dir_inode, struct dentry *direntry,\n\t    struct nameidata *nd)\n{\n\tint xid;\n\tint rc = 0; /* to get around spurious gcc warning, set to zero here */\n\t__u32 oplock = enable_oplocks ? REQ_OPLOCK : 0;\n\t__u16 fileHandle = 0;\n\tbool posix_open = false;\n\tstruct cifs_sb_info *cifs_sb;\n\tstruct tcon_link *tlink;\n\tstruct cifs_tcon *pTcon;\n\tstruct cifsFileInfo *cfile;\n\tstruct inode *newInode = NULL;\n\tchar *full_path = NULL;\n\tstruct file *filp;\n\n\txid = GetXid();\n\n\tcFYI(1, \"parent inode = 0x%p name is: %s and dentry = 0x%p\",\n\t      parent_dir_inode, direntry->d_name.name, direntry);\n\n\t/* check whether path exists */\n\n\tcifs_sb = CIFS_SB(parent_dir_inode->i_sb);\n\ttlink = cifs_sb_tlink(cifs_sb);\n\tif (IS_ERR(tlink)) {\n\t\tFreeXid(xid);\n\t\treturn (struct dentry *)tlink;\n\t}\n\tpTcon = tlink_tcon(tlink);\n\n\t/*\n\t * Don't allow the separator character in a path component.\n\t * The VFS will not allow \"/\", but \"\\\" is allowed by posix.\n\t */\n\tif (!(cifs_sb->mnt_cifs_flags & CIFS_MOUNT_POSIX_PATHS)) {\n\t\tint i;\n\t\tfor (i = 0; i < direntry->d_name.len; i++)\n\t\t\tif (direntry->d_name.name[i] == '\\\\') {\n\t\t\t\tcFYI(1, \"Invalid file name\");\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\t}\n\n\t/*\n\t * O_EXCL: optimize away the lookup, but don't hash the dentry. Let\n\t * the VFS handle the create.\n\t */\n\tif (nd && (nd->flags & LOOKUP_EXCL)) {\n\t\td_instantiate(direntry, NULL);\n\t\trc = 0;\n\t\tgoto lookup_out;\n\t}\n\n\t/* can not grab the rename sem here since it would\n\tdeadlock in the cases (beginning of sys_rename itself)\n\tin which we already have the sb rename sem */\n\tfull_path = build_path_from_dentry(direntry);\n\tif (full_path == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto lookup_out;\n\t}\n\n\tif (direntry->d_inode != NULL) {\n\t\tcFYI(1, \"non-NULL inode in lookup\");\n\t} else {\n\t\tcFYI(1, \"NULL inode in lookup\");\n\t}\n\tcFYI(1, \"Full path: %s inode = 0x%p\", full_path, direntry->d_inode);\n\n\t/* Posix open is only called (at lookup time) for file create now.\n\t * For opens (rather than creates), because we do not know if it\n\t * is a file or directory yet, and current Samba no longer allows\n\t * us to do posix open on dirs, we could end up wasting an open call\n\t * on what turns out to be a dir. For file opens, we wait to call posix\n\t * open till cifs_open.  It could be added here (lookup) in the future\n\t * but the performance tradeoff of the extra network request when EISDIR\n\t * or EACCES is returned would have to be weighed against the 50%\n\t * reduction in network traffic in the other paths.\n\t */\n\tif (pTcon->unix_ext) {\n\t\tif (nd && !(nd->flags & LOOKUP_DIRECTORY) &&\n\t\t     (nd->flags & LOOKUP_OPEN) && !pTcon->broken_posix_open &&\n\t\t     (nd->intent.open.file->f_flags & O_CREAT)) {\n\t\t\trc = cifs_posix_open(full_path, &newInode,\n\t\t\t\t\tparent_dir_inode->i_sb,\n\t\t\t\t\tnd->intent.open.create_mode,\n\t\t\t\t\tnd->intent.open.file->f_flags, &oplock,\n\t\t\t\t\t&fileHandle, xid);\n\t\t\t/*\n\t\t\t * The check below works around a bug in POSIX\n\t\t\t * open in samba versions 3.3.1 and earlier where\n\t\t\t * open could incorrectly fail with invalid parameter.\n\t\t\t * If either that or op not supported returned, follow\n\t\t\t * the normal lookup.\n\t\t\t */\n\t\t\tif ((rc == 0) || (rc == -ENOENT))\n\t\t\t\tposix_open = true;\n\t\t\telse if ((rc == -EINVAL) || (rc != -EOPNOTSUPP))\n\t\t\t\tpTcon->broken_posix_open = true;\n\t\t}\n\t\tif (!posix_open)\n\t\t\trc = cifs_get_inode_info_unix(&newInode, full_path,\n\t\t\t\t\t\tparent_dir_inode->i_sb, xid);\n\t} else\n\t\trc = cifs_get_inode_info(&newInode, full_path, NULL,\n\t\t\t\tparent_dir_inode->i_sb, xid, NULL);\n\n\tif ((rc == 0) && (newInode != NULL)) {\n\t\td_add(direntry, newInode);\n\t\tif (posix_open) {\n\t\t\tfilp = lookup_instantiate_filp(nd, direntry,\n\t\t\t\t\t\t       generic_file_open);\n\t\t\tif (IS_ERR(filp)) {\n\t\t\t\trc = PTR_ERR(filp);\n\t\t\t\tCIFSSMBClose(xid, pTcon, fileHandle);\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\n\t\t\tcfile = cifs_new_fileinfo(fileHandle, filp, tlink,\n\t\t\t\t\t\t  oplock);\n\t\t\tif (cfile == NULL) {\n\t\t\t\tfput(filp);\n\t\t\t\tCIFSSMBClose(xid, pTcon, fileHandle);\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\t\t}\n\t\t/* since paths are not looked up by component - the parent\n\t\t   directories are presumed to be good here */\n\t\trenew_parental_timestamps(direntry);\n\n\t} else if (rc == -ENOENT) {\n\t\trc = 0;\n\t\tdirentry->d_time = jiffies;\n\t\td_add(direntry, NULL);\n\t/*\tif it was once a directory (but how can we tell?) we could do\n\t\tshrink_dcache_parent(direntry); */\n\t} else if (rc != -EACCES) {\n\t\tcERROR(1, \"Unexpected lookup error %d\", rc);\n\t\t/* We special case check for Access Denied - since that\n\t\tis a common return code */\n\t}\n\nlookup_out:\n\tkfree(full_path);\n\tcifs_put_tlink(tlink);\n\tFreeXid(xid);\n\treturn ERR_PTR(rc);\n}",
        "func_after": "struct dentry *\ncifs_lookup(struct inode *parent_dir_inode, struct dentry *direntry,\n\t    struct nameidata *nd)\n{\n\tint xid;\n\tint rc = 0; /* to get around spurious gcc warning, set to zero here */\n\t__u32 oplock = enable_oplocks ? REQ_OPLOCK : 0;\n\t__u16 fileHandle = 0;\n\tbool posix_open = false;\n\tstruct cifs_sb_info *cifs_sb;\n\tstruct tcon_link *tlink;\n\tstruct cifs_tcon *pTcon;\n\tstruct cifsFileInfo *cfile;\n\tstruct inode *newInode = NULL;\n\tchar *full_path = NULL;\n\tstruct file *filp;\n\n\txid = GetXid();\n\n\tcFYI(1, \"parent inode = 0x%p name is: %s and dentry = 0x%p\",\n\t      parent_dir_inode, direntry->d_name.name, direntry);\n\n\t/* check whether path exists */\n\n\tcifs_sb = CIFS_SB(parent_dir_inode->i_sb);\n\ttlink = cifs_sb_tlink(cifs_sb);\n\tif (IS_ERR(tlink)) {\n\t\tFreeXid(xid);\n\t\treturn (struct dentry *)tlink;\n\t}\n\tpTcon = tlink_tcon(tlink);\n\n\t/*\n\t * Don't allow the separator character in a path component.\n\t * The VFS will not allow \"/\", but \"\\\" is allowed by posix.\n\t */\n\tif (!(cifs_sb->mnt_cifs_flags & CIFS_MOUNT_POSIX_PATHS)) {\n\t\tint i;\n\t\tfor (i = 0; i < direntry->d_name.len; i++)\n\t\t\tif (direntry->d_name.name[i] == '\\\\') {\n\t\t\t\tcFYI(1, \"Invalid file name\");\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\t}\n\n\t/*\n\t * O_EXCL: optimize away the lookup, but don't hash the dentry. Let\n\t * the VFS handle the create.\n\t */\n\tif (nd && (nd->flags & LOOKUP_EXCL)) {\n\t\td_instantiate(direntry, NULL);\n\t\trc = 0;\n\t\tgoto lookup_out;\n\t}\n\n\t/* can not grab the rename sem here since it would\n\tdeadlock in the cases (beginning of sys_rename itself)\n\tin which we already have the sb rename sem */\n\tfull_path = build_path_from_dentry(direntry);\n\tif (full_path == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto lookup_out;\n\t}\n\n\tif (direntry->d_inode != NULL) {\n\t\tcFYI(1, \"non-NULL inode in lookup\");\n\t} else {\n\t\tcFYI(1, \"NULL inode in lookup\");\n\t}\n\tcFYI(1, \"Full path: %s inode = 0x%p\", full_path, direntry->d_inode);\n\n\t/* Posix open is only called (at lookup time) for file create now.\n\t * For opens (rather than creates), because we do not know if it\n\t * is a file or directory yet, and current Samba no longer allows\n\t * us to do posix open on dirs, we could end up wasting an open call\n\t * on what turns out to be a dir. For file opens, we wait to call posix\n\t * open till cifs_open.  It could be added here (lookup) in the future\n\t * but the performance tradeoff of the extra network request when EISDIR\n\t * or EACCES is returned would have to be weighed against the 50%\n\t * reduction in network traffic in the other paths.\n\t */\n\tif (pTcon->unix_ext) {\n\t\tif (nd && !(nd->flags & LOOKUP_DIRECTORY) &&\n\t\t     (nd->flags & LOOKUP_OPEN) && !pTcon->broken_posix_open &&\n\t\t     (nd->intent.open.file->f_flags & O_CREAT)) {\n\t\t\trc = cifs_posix_open(full_path, &newInode,\n\t\t\t\t\tparent_dir_inode->i_sb,\n\t\t\t\t\tnd->intent.open.create_mode,\n\t\t\t\t\tnd->intent.open.file->f_flags, &oplock,\n\t\t\t\t\t&fileHandle, xid);\n\t\t\t/*\n\t\t\t * The check below works around a bug in POSIX\n\t\t\t * open in samba versions 3.3.1 and earlier where\n\t\t\t * open could incorrectly fail with invalid parameter.\n\t\t\t * If either that or op not supported returned, follow\n\t\t\t * the normal lookup.\n\t\t\t */\n\t\t\tswitch (rc) {\n\t\t\tcase 0:\n\t\t\t\t/*\n\t\t\t\t * The server may allow us to open things like\n\t\t\t\t * FIFOs, but the client isn't set up to deal\n\t\t\t\t * with that. If it's not a regular file, just\n\t\t\t\t * close it and proceed as if it were a normal\n\t\t\t\t * lookup.\n\t\t\t\t */\n\t\t\t\tif (newInode && !S_ISREG(newInode->i_mode)) {\n\t\t\t\t\tCIFSSMBClose(xid, pTcon, fileHandle);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tcase -ENOENT:\n\t\t\t\tposix_open = true;\n\t\t\tcase -EOPNOTSUPP:\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tpTcon->broken_posix_open = true;\n\t\t\t}\n\t\t}\n\t\tif (!posix_open)\n\t\t\trc = cifs_get_inode_info_unix(&newInode, full_path,\n\t\t\t\t\t\tparent_dir_inode->i_sb, xid);\n\t} else\n\t\trc = cifs_get_inode_info(&newInode, full_path, NULL,\n\t\t\t\tparent_dir_inode->i_sb, xid, NULL);\n\n\tif ((rc == 0) && (newInode != NULL)) {\n\t\td_add(direntry, newInode);\n\t\tif (posix_open) {\n\t\t\tfilp = lookup_instantiate_filp(nd, direntry,\n\t\t\t\t\t\t       generic_file_open);\n\t\t\tif (IS_ERR(filp)) {\n\t\t\t\trc = PTR_ERR(filp);\n\t\t\t\tCIFSSMBClose(xid, pTcon, fileHandle);\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\n\t\t\tcfile = cifs_new_fileinfo(fileHandle, filp, tlink,\n\t\t\t\t\t\t  oplock);\n\t\t\tif (cfile == NULL) {\n\t\t\t\tfput(filp);\n\t\t\t\tCIFSSMBClose(xid, pTcon, fileHandle);\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto lookup_out;\n\t\t\t}\n\t\t}\n\t\t/* since paths are not looked up by component - the parent\n\t\t   directories are presumed to be good here */\n\t\trenew_parental_timestamps(direntry);\n\n\t} else if (rc == -ENOENT) {\n\t\trc = 0;\n\t\tdirentry->d_time = jiffies;\n\t\td_add(direntry, NULL);\n\t/*\tif it was once a directory (but how can we tell?) we could do\n\t\tshrink_dcache_parent(direntry); */\n\t} else if (rc != -EACCES) {\n\t\tcERROR(1, \"Unexpected lookup error %d\", rc);\n\t\t/* We special case check for Access Denied - since that\n\t\tis a common return code */\n\t}\n\nlookup_out:\n\tkfree(full_path);\n\tcifs_put_tlink(tlink);\n\tFreeXid(xid);\n\treturn ERR_PTR(rc);\n}",
        "description": "The `cifs_lookup` function in the CIFS directory handling module of the Linux kernel, prior to version 3.2.10, permits local users to trigger a denial of service (OOPS) condition by attempting to access a special file, such as a FIFO.",
        "commit": "The CIFS code attempts to open files during lookup under specific conditions. However, if the file turned out to be a FIFO or another special file type, the open file handle would be leaked, resulting in a dentry reference count mismatch and causing an oops error during unmount. This issue has been resolved by ensuring that the file handle on the server is closed if the file is not a regular file. Additionally, the code has been refactored to use a switch statement instead of a complex if-else structure."
    },
    {
        "cwe": "CWE-90",
        "func_name": "krb5/krb5_ldap_put_principal",
        "score": 0.7564836740493774,
        "func_before": "krb5_error_code\nkrb5_ldap_put_principal(krb5_context context, krb5_db_entry *entry,\n                        char **db_args)\n{\n    int                         l=0, kerberos_principal_object_type=0;\n    unsigned int                ntrees=0, tre=0;\n    krb5_error_code             st=0, tempst=0;\n    LDAP                        *ld=NULL;\n    LDAPMessage                 *result=NULL, *ent=NULL;\n    char                        **subtreelist = NULL;\n    char                        *user=NULL, *subtree=NULL, *principal_dn=NULL;\n    char                        **values=NULL, *strval[10]={NULL}, errbuf[1024];\n    char                        *filtuser=NULL;\n    struct berval               **bersecretkey=NULL;\n    LDAPMod                     **mods=NULL;\n    krb5_boolean                create_standalone=FALSE;\n    krb5_boolean                krb_identity_exists=FALSE, establish_links=FALSE;\n    char                        *standalone_principal_dn=NULL;\n    krb5_tl_data                *tl_data=NULL;\n    krb5_key_data               **keys=NULL;\n    kdb5_dal_handle             *dal_handle=NULL;\n    krb5_ldap_context           *ldap_context=NULL;\n    krb5_ldap_server_handle     *ldap_server_handle=NULL;\n    osa_princ_ent_rec           princ_ent = {0};\n    xargs_t                     xargs = {0};\n    char                        *polname = NULL;\n    OPERATION optype;\n    krb5_boolean                found_entry = FALSE;\n\n    /* Clear the global error string */\n    krb5_clear_error_message(context);\n\n    SETUP_CONTEXT();\n    if (ldap_context->lrparams == NULL || ldap_context->container_dn == NULL)\n        return EINVAL;\n\n    /* get ldap handle */\n    GET_HANDLE();\n\n    if (!is_principal_in_realm(ldap_context, entry->princ)) {\n        st = EINVAL;\n        k5_setmsg(context, st,\n                  _(\"Principal does not belong to the default realm\"));\n        goto cleanup;\n    }\n\n    /* get the principal information to act on */\n    if (((st=krb5_unparse_name(context, entry->princ, &user)) != 0) ||\n        ((st=krb5_ldap_unparse_principal_name(user)) != 0))\n        goto cleanup;\n    filtuser = ldap_filter_correct(user);\n    if (filtuser == NULL) {\n        st = ENOMEM;\n        goto cleanup;\n    }\n\n    /* Identity the type of operation, it can be\n     * add principal or modify principal.\n     * hack if the entry->mask has KRB_PRINCIPAL flag set\n     * then it is a add operation\n     */\n    if (entry->mask & KADM5_PRINCIPAL)\n        optype = ADD_PRINCIPAL;\n    else\n        optype = MODIFY_PRINCIPAL;\n\n    if (((st=krb5_get_princ_type(context, entry, &kerberos_principal_object_type)) != 0) ||\n        ((st=krb5_get_userdn(context, entry, &principal_dn)) != 0))\n        goto cleanup;\n\n    if ((st=process_db_args(context, db_args, &xargs, optype)) != 0)\n        goto cleanup;\n\n    if (entry->mask & KADM5_LOAD) {\n        unsigned int     tree = 0;\n        int              numlentries = 0;\n        char             *filter = NULL;\n\n        /*  A load operation is special, will do a mix-in (add krbprinc\n         *  attrs to a non-krb object entry) if an object exists with a\n         *  matching krbprincipalname attribute so try to find existing\n         *  object and set principal_dn.  This assumes that the\n         *  krbprincipalname attribute is unique (only one object entry has\n         *  a particular krbprincipalname attribute).\n         */\n        if (asprintf(&filter, FILTER\"%s))\", filtuser) < 0) {\n            filter = NULL;\n            st = ENOMEM;\n            goto cleanup;\n        }\n\n        /* get the current subtree list */\n        if ((st = krb5_get_subtree_info(ldap_context, &subtreelist, &ntrees)) != 0)\n            goto cleanup;\n\n        found_entry = FALSE;\n        /* search for entry with matching krbprincipalname attribute */\n        for (tree = 0; found_entry == FALSE && tree < ntrees; ++tree) {\n            if (principal_dn == NULL) {\n                LDAP_SEARCH_1(subtreelist[tree], ldap_context->lrparams->search_scope, filter, principal_attributes, IGNORE_STATUS);\n            } else {\n                /* just look for entry with principal_dn */\n                LDAP_SEARCH_1(principal_dn, LDAP_SCOPE_BASE, filter, principal_attributes, IGNORE_STATUS);\n            }\n            if (st == LDAP_SUCCESS) {\n                numlentries = ldap_count_entries(ld, result);\n                if (numlentries > 1) {\n                    free(filter);\n                    st = EINVAL;\n                    k5_setmsg(context, st,\n                              _(\"operation can not continue, more than one \"\n                                \"entry with principal name \\\"%s\\\" found\"),\n                              user);\n                    goto cleanup;\n                } else if (numlentries == 1) {\n                    found_entry = TRUE;\n                    if (principal_dn == NULL) {\n                        ent = ldap_first_entry(ld, result);\n                        if (ent != NULL) {\n                            /* setting principal_dn will cause that entry to be modified further down */\n                            if ((principal_dn = ldap_get_dn(ld, ent)) == NULL) {\n                                ldap_get_option (ld, LDAP_OPT_RESULT_CODE, &st);\n                                st = set_ldap_error (context, st, 0);\n                                free(filter);\n                                goto cleanup;\n                            }\n                        }\n                    }\n                }\n            } else if (st != LDAP_NO_SUCH_OBJECT) {\n                /* could not perform search, return with failure */\n                st = set_ldap_error (context, st, 0);\n                free(filter);\n                goto cleanup;\n            }\n            ldap_msgfree(result);\n            result = NULL;\n            /*\n             * If it isn't found then assume a standalone princ entry is to\n             * be created.\n             */\n        } /* end for (tree = 0; principal_dn == ... */\n\n        free(filter);\n\n        if (found_entry == FALSE && principal_dn != NULL) {\n            /*\n             * if principal_dn is null then there is code further down to\n             * deal with setting standalone_principal_dn.  Also note that\n             * this will set create_standalone true for\n             * non-mix-in entries which is okay if loading from a dump.\n             */\n            create_standalone = TRUE;\n            standalone_principal_dn = strdup(principal_dn);\n            CHECK_NULL(standalone_principal_dn);\n        }\n    } /* end if (entry->mask & KADM5_LOAD */\n\n    /* time to generate the DN information with the help of\n     * containerdn, principalcontainerreference or\n     * realmcontainerdn information\n     */\n    if (principal_dn == NULL && xargs.dn == NULL) { /* creation of standalone principal */\n        /* get the subtree information */\n        if (entry->princ->length == 2 && entry->princ->data[0].length == strlen(\"krbtgt\") &&\n            strncmp(entry->princ->data[0].data, \"krbtgt\", entry->princ->data[0].length) == 0) {\n            /* if the principal is a inter-realm principal, always created in the realm container */\n            subtree = strdup(ldap_context->lrparams->realmdn);\n        } else if (xargs.containerdn) {\n            if ((st=checkattributevalue(ld, xargs.containerdn, NULL, NULL, NULL)) != 0) {\n                if (st == KRB5_KDB_NOENTRY || st == KRB5_KDB_CONSTRAINT_VIOLATION) {\n                    int ost = st;\n                    st = EINVAL;\n                    k5_wrapmsg(context, ost, st, _(\"'%s' not found\"),\n                               xargs.containerdn);\n                }\n                goto cleanup;\n            }\n            subtree = strdup(xargs.containerdn);\n        } else if (ldap_context->lrparams->containerref && strlen(ldap_context->lrparams->containerref) != 0) {\n            /*\n             * Here the subtree should be changed with\n             * principalcontainerreference attribute value\n             */\n            subtree = strdup(ldap_context->lrparams->containerref);\n        } else {\n            subtree = strdup(ldap_context->lrparams->realmdn);\n        }\n        CHECK_NULL(subtree);\n\n        if (asprintf(&standalone_principal_dn, \"krbprincipalname=%s,%s\",\n                     filtuser, subtree) < 0)\n            standalone_principal_dn = NULL;\n        CHECK_NULL(standalone_principal_dn);\n        /*\n         * free subtree when you are done using the subtree\n         * set the boolean create_standalone to TRUE\n         */\n        create_standalone = TRUE;\n        free(subtree);\n        subtree = NULL;\n    }\n\n    /*\n     * If the DN information is presented by the user, time to\n     * validate the input to ensure that the DN falls under\n     * any of the subtrees\n     */\n    if (xargs.dn_from_kbd == TRUE) {\n        /* make sure the DN falls in the subtree */\n        int              dnlen=0, subtreelen=0;\n        char             *dn=NULL;\n        krb5_boolean     outofsubtree=TRUE;\n\n        if (xargs.dn != NULL) {\n            dn = xargs.dn;\n        } else if (xargs.linkdn != NULL) {\n            dn = xargs.linkdn;\n        } else if (standalone_principal_dn != NULL) {\n            /*\n             * Even though the standalone_principal_dn is constructed\n             * within this function, there is the containerdn input\n             * from the user that can become part of the it.\n             */\n            dn = standalone_principal_dn;\n        }\n\n        /* Get the current subtree list if we haven't already done so. */\n        if (subtreelist == NULL) {\n            st = krb5_get_subtree_info(ldap_context, &subtreelist, &ntrees);\n            if (st)\n                goto cleanup;\n        }\n\n        for (tre=0; tre<ntrees; ++tre) {\n            if (subtreelist[tre] == NULL || strlen(subtreelist[tre]) == 0) {\n                outofsubtree = FALSE;\n                break;\n            } else {\n                dnlen = strlen (dn);\n                subtreelen = strlen(subtreelist[tre]);\n                if ((dnlen >= subtreelen) && (strcasecmp((dn + dnlen - subtreelen), subtreelist[tre]) == 0)) {\n                    outofsubtree = FALSE;\n                    break;\n                }\n            }\n        }\n\n        if (outofsubtree == TRUE) {\n            st = EINVAL;\n            k5_setmsg(context, st, _(\"DN is out of the realm subtree\"));\n            goto cleanup;\n        }\n\n        /*\n         * dn value will be set either by dn, linkdn or the standalone_principal_dn\n         * In the first 2 cases, the dn should be existing and in the last case we\n         * are supposed to create the ldap object. so the below should not be\n         * executed for the last case.\n         */\n\n        if (standalone_principal_dn == NULL) {\n            /*\n             * If the ldap object is missing, this results in an error.\n             */\n\n            /*\n             * Search for krbprincipalname attribute here.\n             * This is to find if a kerberos identity is already present\n             * on the ldap object, in which case adding a kerberos identity\n             * on the ldap object should result in an error.\n             */\n            char  *attributes[]={\"krbticketpolicyreference\", \"krbprincipalname\", NULL};\n\n            ldap_msgfree(result);\n            result = NULL;\n            LDAP_SEARCH_1(dn, LDAP_SCOPE_BASE, 0, attributes, IGNORE_STATUS);\n            if (st == LDAP_SUCCESS) {\n                ent = ldap_first_entry(ld, result);\n                if (ent != NULL) {\n                    if ((values=ldap_get_values(ld, ent, \"krbticketpolicyreference\")) != NULL) {\n                        ldap_value_free(values);\n                    }\n\n                    if ((values=ldap_get_values(ld, ent, \"krbprincipalname\")) != NULL) {\n                        krb_identity_exists = TRUE;\n                        ldap_value_free(values);\n                    }\n                }\n            } else {\n                st = set_ldap_error(context, st, OP_SEARCH);\n                goto cleanup;\n            }\n        }\n    }\n\n    /*\n     * If xargs.dn is set then the request is to add a\n     * kerberos principal on a ldap object, but if\n     * there is one already on the ldap object this\n     * should result in an error.\n     */\n\n    if (xargs.dn != NULL && krb_identity_exists == TRUE) {\n        st = EINVAL;\n        snprintf(errbuf, sizeof(errbuf),\n                 _(\"ldap object is already kerberized\"));\n        k5_setmsg(context, st, \"%s\", errbuf);\n        goto cleanup;\n    }\n\n    if (xargs.linkdn != NULL) {\n        /*\n         * link information can be changed using modprinc.\n         * However, link information can be changed only on the\n         * standalone kerberos principal objects. A standalone\n         * kerberos principal object is of type krbprincipal\n         * structural objectclass.\n         *\n         * NOTE: kerberos principals on an ldap object can't be\n         * linked to other ldap objects.\n         */\n        if (optype == MODIFY_PRINCIPAL &&\n            kerberos_principal_object_type != KDB_STANDALONE_PRINCIPAL_OBJECT) {\n            st = EINVAL;\n            snprintf(errbuf, sizeof(errbuf),\n                     _(\"link information can not be set/updated as the \"\n                       \"kerberos principal belongs to an ldap object\"));\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n        /*\n         * Check the link information. If there is already a link\n         * existing then this operation is not allowed.\n         */\n        {\n            char **linkdns=NULL;\n            int  j=0;\n\n            if ((st=krb5_get_linkdn(context, entry, &linkdns)) != 0) {\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"Failed getting object references\"));\n                k5_setmsg(context, st, \"%s\", errbuf);\n                goto cleanup;\n            }\n            if (linkdns != NULL) {\n                st = EINVAL;\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"kerberos principal is already linked to a ldap \"\n                           \"object\"));\n                k5_setmsg(context, st, \"%s\", errbuf);\n                for (j=0; linkdns[j] != NULL; ++j)\n                    free (linkdns[j]);\n                free (linkdns);\n                goto cleanup;\n            }\n        }\n\n        establish_links = TRUE;\n    }\n\n    if (entry->mask & KADM5_LAST_SUCCESS) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->last_success)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastSuccessfulAuth\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_LAST_FAILED) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->last_failed)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastFailedAuth\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free(strval[0]);\n    }\n\n    if (entry->mask & KADM5_FAIL_AUTH_COUNT) {\n        krb5_kvno fail_auth_count;\n\n        fail_auth_count = entry->fail_auth_count;\n        if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT)\n            fail_auth_count++;\n\n        st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                       LDAP_MOD_REPLACE,\n                                       fail_auth_count);\n        if (st != 0)\n            goto cleanup;\n    } else if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT) {\n        int attr_mask = 0;\n        krb5_boolean has_fail_count;\n\n        /* Check if the krbLoginFailedCount attribute exists.  (Through\n         * krb5 1.8.1, it wasn't set in new entries.) */\n        st = krb5_get_attributes_mask(context, entry, &attr_mask);\n        if (st != 0)\n            goto cleanup;\n        has_fail_count = ((attr_mask & KDB_FAIL_AUTH_COUNT_ATTR) != 0);\n\n        /*\n         * If the client library and server supports RFC 4525,\n         * then use it to increment by one the value of the\n         * krbLoginFailedCount attribute. Otherwise, assert the\n         * (provided) old value by deleting it before adding.\n         */\n#ifdef LDAP_MOD_INCREMENT\n        if (ldap_server_handle->server_info->modify_increment &&\n            has_fail_count) {\n            st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                           LDAP_MOD_INCREMENT, 1);\n            if (st != 0)\n                goto cleanup;\n        } else {\n#endif /* LDAP_MOD_INCREMENT */\n            if (has_fail_count) {\n                st = krb5_add_int_mem_ldap_mod(&mods,\n                                               \"krbLoginFailedCount\",\n                                               LDAP_MOD_DELETE,\n                                               entry->fail_auth_count);\n                if (st != 0)\n                    goto cleanup;\n            }\n            st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                           LDAP_MOD_ADD,\n                                           entry->fail_auth_count + 1);\n            if (st != 0)\n                goto cleanup;\n#ifdef LDAP_MOD_INCREMENT\n        }\n#endif\n    } else if (optype == ADD_PRINCIPAL) {\n        /* Initialize krbLoginFailedCount in new entries to help avoid a\n         * race during the first failed login. */\n        st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                       LDAP_MOD_ADD, 0);\n    }\n\n    if (entry->mask & KADM5_MAX_LIFE) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbmaxticketlife\", LDAP_MOD_REPLACE, entry->max_life)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_MAX_RLIFE) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbmaxrenewableage\", LDAP_MOD_REPLACE,\n                                          entry->max_renewable_life)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_ATTRIBUTES) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbticketflags\", LDAP_MOD_REPLACE,\n                                          entry->attributes)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_PRINCIPAL) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = user;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbprincipalname\", LDAP_MOD_REPLACE, strval)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_PRINC_EXPIRE_TIME) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->expiration)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbprincipalexpiration\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_PW_EXPIRATION) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->pw_expiration)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpasswordexpiration\",\n                                          LDAP_MOD_REPLACE,\n                                          strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_POLICY || entry->mask & KADM5_KEY_HIST) {\n        memset(&princ_ent, 0, sizeof(princ_ent));\n        for (tl_data=entry->tl_data; tl_data; tl_data=tl_data->tl_data_next) {\n            if (tl_data->tl_data_type == KRB5_TL_KADM_DATA) {\n                if ((st = krb5_lookup_tl_kadm_data(tl_data, &princ_ent)) != 0) {\n                    goto cleanup;\n                }\n                break;\n            }\n        }\n    }\n\n    if (entry->mask & KADM5_POLICY) {\n        if (princ_ent.aux_attributes & KADM5_POLICY) {\n            memset(strval, 0, sizeof(strval));\n            if ((st = krb5_ldap_name_to_policydn (context, princ_ent.policy, &polname)) != 0)\n                goto cleanup;\n            strval[0] = polname;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_REPLACE, strval)) != 0)\n                goto cleanup;\n        } else {\n            st = EINVAL;\n            k5_setmsg(context, st, \"Password policy value null\");\n            goto cleanup;\n        }\n    } else if (entry->mask & KADM5_LOAD && found_entry == TRUE) {\n        /*\n         * a load is special in that existing entries must have attrs that\n         * removed.\n         */\n\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_REPLACE, NULL)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_POLICY_CLR) {\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_DELETE, NULL)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_KEY_HIST) {\n        bersecretkey = krb5_encode_histkey(&princ_ent);\n        if (bersecretkey == NULL) {\n            st = ENOMEM;\n            goto cleanup;\n        }\n\n        st = krb5_add_ber_mem_ldap_mod(&mods, \"krbpwdhistory\",\n                                       LDAP_MOD_REPLACE | LDAP_MOD_BVALUES,\n                                       bersecretkey);\n        if (st != 0)\n            goto cleanup;\n        free_berdata(bersecretkey);\n        bersecretkey = NULL;\n    }\n\n    if (entry->mask & KADM5_KEY_DATA || entry->mask & KADM5_KVNO) {\n        krb5_kvno mkvno;\n\n        if ((st=krb5_dbe_lookup_mkvno(context, entry, &mkvno)) != 0)\n            goto cleanup;\n        bersecretkey = krb5_encode_krbsecretkey (entry->key_data,\n                                                 entry->n_key_data, mkvno);\n\n        if (bersecretkey == NULL) {\n            st = ENOMEM;\n            goto cleanup;\n        }\n        /* An empty list of bervals is only accepted for modify operations,\n         * not add operations. */\n        if (bersecretkey[0] != NULL || !create_standalone) {\n            st = krb5_add_ber_mem_ldap_mod(&mods, \"krbprincipalkey\",\n                                           LDAP_MOD_REPLACE | LDAP_MOD_BVALUES,\n                                           bersecretkey);\n            if (st != 0)\n                goto cleanup;\n        }\n\n        if (!(entry->mask & KADM5_PRINCIPAL)) {\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0]=getstringtime(entry->pw_expiration)) == NULL)\n                goto cleanup;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods,\n                                              \"krbpasswordexpiration\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n\n        /* Update last password change whenever a new key is set */\n        {\n            krb5_timestamp last_pw_changed;\n            if ((st=krb5_dbe_lookup_last_pwd_change(context, entry,\n                                                    &last_pw_changed)) != 0)\n                goto cleanup;\n\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0] = getstringtime(last_pw_changed)) == NULL)\n                goto cleanup;\n\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastPwdChange\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n\n    } /* Modify Key data ends here */\n\n    /* Auth indicators will also be stored in krbExtraData when processing\n     * tl_data. */\n    st = update_ldap_mod_auth_ind(context, entry, &mods);\n    if (st != 0)\n        goto cleanup;\n\n    /* Set tl_data */\n    if (entry->tl_data != NULL) {\n        int count = 0;\n        struct berval **ber_tl_data = NULL;\n        krb5_tl_data *ptr;\n        krb5_timestamp unlock_time;\n        for (ptr = entry->tl_data; ptr != NULL; ptr = ptr->tl_data_next) {\n            if (ptr->tl_data_type == KRB5_TL_LAST_PWD_CHANGE\n#ifdef SECURID\n                || ptr->tl_data_type == KRB5_TL_DB_ARGS\n#endif\n                || ptr->tl_data_type == KRB5_TL_KADM_DATA\n                || ptr->tl_data_type == KDB_TL_USER_INFO\n                || ptr->tl_data_type == KRB5_TL_CONSTRAINED_DELEGATION_ACL\n                || ptr->tl_data_type == KRB5_TL_LAST_ADMIN_UNLOCK)\n                continue;\n            count++;\n        }\n        if (count != 0) {\n            int j;\n            ber_tl_data = (struct berval **) calloc (count + 1,\n                                                     sizeof (struct berval*));\n            if (ber_tl_data == NULL) {\n                st = ENOMEM;\n                goto cleanup;\n            }\n            for (j = 0, ptr = entry->tl_data; ptr != NULL; ptr = ptr->tl_data_next) {\n                /* Ignore tl_data that are stored in separate directory\n                 * attributes */\n                if (ptr->tl_data_type == KRB5_TL_LAST_PWD_CHANGE\n#ifdef SECURID\n                    || ptr->tl_data_type == KRB5_TL_DB_ARGS\n#endif\n                    || ptr->tl_data_type == KRB5_TL_KADM_DATA\n                    || ptr->tl_data_type == KDB_TL_USER_INFO\n                    || ptr->tl_data_type == KRB5_TL_CONSTRAINED_DELEGATION_ACL\n                    || ptr->tl_data_type == KRB5_TL_LAST_ADMIN_UNLOCK)\n                    continue;\n                if ((st = tl_data2berval (ptr, &ber_tl_data[j])) != 0)\n                    break;\n                j++;\n            }\n            if (st == 0) {\n                ber_tl_data[count] = NULL;\n                st=krb5_add_ber_mem_ldap_mod(&mods, \"krbExtraData\",\n                                             LDAP_MOD_REPLACE |\n                                             LDAP_MOD_BVALUES, ber_tl_data);\n            }\n            free_berdata(ber_tl_data);\n            if (st != 0)\n                goto cleanup;\n        }\n        if ((st=krb5_dbe_lookup_last_admin_unlock(context, entry,\n                                                  &unlock_time)) != 0)\n            goto cleanup;\n        if (unlock_time != 0) {\n            /* Update last admin unlock */\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0] = getstringtime(unlock_time)) == NULL)\n                goto cleanup;\n\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastAdminUnlock\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n    }\n\n    /* Directory specific attribute */\n    if (xargs.tktpolicydn != NULL) {\n        int tmask=0;\n\n        if (strlen(xargs.tktpolicydn) != 0) {\n            st = checkattributevalue(ld, xargs.tktpolicydn, \"objectclass\", policyclass, &tmask);\n            CHECK_CLASS_VALIDITY(st, tmask, _(\"ticket policy object value: \"));\n\n            strval[0] = xargs.tktpolicydn;\n            strval[1] = NULL;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbticketpolicyreference\", LDAP_MOD_REPLACE, strval)) != 0)\n                goto cleanup;\n\n        } else {\n            /* if xargs.tktpolicydn is a empty string, then delete\n             * already existing krbticketpolicyreference attr */\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbticketpolicyreference\", LDAP_MOD_DELETE, NULL)) != 0)\n                goto cleanup;\n        }\n\n    }\n\n    if (establish_links == TRUE) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = xargs.linkdn;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbObjectReferences\", LDAP_MOD_REPLACE, strval)) != 0)\n            goto cleanup;\n    }\n\n    /*\n     * in case mods is NULL then return\n     * not sure but can happen in a modprinc\n     * so no need to return an error\n     * addprinc will at least have the principal name\n     * and the keys passed in\n     */\n    if (mods == NULL)\n        goto cleanup;\n\n    if (create_standalone == TRUE) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = \"krbprincipal\";\n        strval[1] = \"krbprincipalaux\";\n        strval[2] = \"krbTicketPolicyAux\";\n\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"objectclass\", LDAP_MOD_ADD, strval)) != 0)\n            goto cleanup;\n\n        st = ldap_add_ext_s(ld, standalone_principal_dn, mods, NULL, NULL);\n        if (st == LDAP_ALREADY_EXISTS && entry->mask & KADM5_LOAD) {\n            /* a load operation must replace an existing entry */\n            st = ldap_delete_ext_s(ld, standalone_principal_dn, NULL, NULL);\n            if (st != LDAP_SUCCESS) {\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"Principal delete failed (trying to replace \"\n                           \"entry): %s\"), ldap_err2string(st));\n                st = translate_ldap_error (st, OP_ADD);\n                k5_setmsg(context, st, \"%s\", errbuf);\n                goto cleanup;\n            } else {\n                st = ldap_add_ext_s(ld, standalone_principal_dn, mods, NULL, NULL);\n            }\n        }\n        if (st != LDAP_SUCCESS) {\n            snprintf(errbuf, sizeof(errbuf), _(\"Principal add failed: %s\"),\n                     ldap_err2string(st));\n            st = translate_ldap_error (st, OP_ADD);\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n    } else {\n        /*\n         * Here existing ldap object is modified and can be related\n         * to any attribute, so always ensure that the ldap\n         * object is extended with all the kerberos related\n         * objectclasses so that there are no constraint\n         * violations.\n         */\n        {\n            char *attrvalues[] = {\"krbprincipalaux\", \"krbTicketPolicyAux\", NULL};\n            int p, q, r=0, amask=0;\n\n            if ((st=checkattributevalue(ld, (xargs.dn) ? xargs.dn : principal_dn,\n                                        \"objectclass\", attrvalues, &amask)) != 0)\n                goto cleanup;\n\n            memset(strval, 0, sizeof(strval));\n            for (p=1, q=0; p<=2; p<<=1, ++q) {\n                if ((p & amask) == 0)\n                    strval[r++] = attrvalues[q];\n            }\n            if (r != 0) {\n                if ((st=krb5_add_str_mem_ldap_mod(&mods, \"objectclass\", LDAP_MOD_ADD, strval)) != 0)\n                    goto cleanup;\n            }\n        }\n        if (xargs.dn != NULL)\n            st=ldap_modify_ext_s(ld, xargs.dn, mods, NULL, NULL);\n        else\n            st = ldap_modify_ext_s(ld, principal_dn, mods, NULL, NULL);\n\n        if (st != LDAP_SUCCESS) {\n            snprintf(errbuf, sizeof(errbuf), _(\"User modification failed: %s\"),\n                     ldap_err2string(st));\n            st = translate_ldap_error (st, OP_MOD);\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n\n        if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT)\n            entry->fail_auth_count++;\n    }\n\ncleanup:\n    if (user)\n        free(user);\n\n    if (filtuser)\n        free(filtuser);\n\n    free_xargs(xargs);\n\n    if (standalone_principal_dn)\n        free(standalone_principal_dn);\n\n    if (principal_dn)\n        free (principal_dn);\n\n    if (polname != NULL)\n        free(polname);\n\n    for (tre = 0; tre < ntrees; tre++)\n        free(subtreelist[tre]);\n    free(subtreelist);\n\n    if (subtree)\n        free (subtree);\n\n    if (bersecretkey) {\n        for (l=0; bersecretkey[l]; ++l) {\n            if (bersecretkey[l]->bv_val)\n                free (bersecretkey[l]->bv_val);\n            free (bersecretkey[l]);\n        }\n        free (bersecretkey);\n    }\n\n    if (keys)\n        free (keys);\n\n    ldap_mods_free(mods, 1);\n    ldap_osa_free_princ_ent(&princ_ent);\n    ldap_msgfree(result);\n    krb5_ldap_put_handle_to_pool(ldap_context, ldap_server_handle);\n    return(st);\n}",
        "func_after": "krb5_error_code\nkrb5_ldap_put_principal(krb5_context context, krb5_db_entry *entry,\n                        char **db_args)\n{\n    int                         l=0, kerberos_principal_object_type=0;\n    unsigned int                ntrees=0, tre=0;\n    krb5_error_code             st=0, tempst=0;\n    LDAP                        *ld=NULL;\n    LDAPMessage                 *result=NULL, *ent=NULL;\n    char                        **subtreelist = NULL;\n    char                        *user=NULL, *subtree=NULL, *principal_dn=NULL;\n    char                        *strval[10]={NULL}, errbuf[1024];\n    char                        *filtuser=NULL;\n    struct berval               **bersecretkey=NULL;\n    LDAPMod                     **mods=NULL;\n    krb5_boolean                create_standalone=FALSE;\n    krb5_boolean                establish_links=FALSE;\n    char                        *standalone_principal_dn=NULL;\n    krb5_tl_data                *tl_data=NULL;\n    krb5_key_data               **keys=NULL;\n    kdb5_dal_handle             *dal_handle=NULL;\n    krb5_ldap_context           *ldap_context=NULL;\n    krb5_ldap_server_handle     *ldap_server_handle=NULL;\n    osa_princ_ent_rec           princ_ent = {0};\n    xargs_t                     xargs = {0};\n    char                        *polname = NULL;\n    OPERATION optype;\n    krb5_boolean                found_entry = FALSE;\n\n    /* Clear the global error string */\n    krb5_clear_error_message(context);\n\n    SETUP_CONTEXT();\n    if (ldap_context->lrparams == NULL || ldap_context->container_dn == NULL)\n        return EINVAL;\n\n    /* get ldap handle */\n    GET_HANDLE();\n\n    if (!is_principal_in_realm(ldap_context, entry->princ)) {\n        st = EINVAL;\n        k5_setmsg(context, st,\n                  _(\"Principal does not belong to the default realm\"));\n        goto cleanup;\n    }\n\n    /* get the principal information to act on */\n    if (((st=krb5_unparse_name(context, entry->princ, &user)) != 0) ||\n        ((st=krb5_ldap_unparse_principal_name(user)) != 0))\n        goto cleanup;\n    filtuser = ldap_filter_correct(user);\n    if (filtuser == NULL) {\n        st = ENOMEM;\n        goto cleanup;\n    }\n\n    /* Identity the type of operation, it can be\n     * add principal or modify principal.\n     * hack if the entry->mask has KRB_PRINCIPAL flag set\n     * then it is a add operation\n     */\n    if (entry->mask & KADM5_PRINCIPAL)\n        optype = ADD_PRINCIPAL;\n    else\n        optype = MODIFY_PRINCIPAL;\n\n    if (((st=krb5_get_princ_type(context, entry, &kerberos_principal_object_type)) != 0) ||\n        ((st=krb5_get_userdn(context, entry, &principal_dn)) != 0))\n        goto cleanup;\n\n    if ((st=process_db_args(context, db_args, &xargs, optype)) != 0)\n        goto cleanup;\n\n    if (entry->mask & KADM5_LOAD) {\n        unsigned int     tree = 0;\n        int              numlentries = 0;\n        char             *filter = NULL;\n\n        /*  A load operation is special, will do a mix-in (add krbprinc\n         *  attrs to a non-krb object entry) if an object exists with a\n         *  matching krbprincipalname attribute so try to find existing\n         *  object and set principal_dn.  This assumes that the\n         *  krbprincipalname attribute is unique (only one object entry has\n         *  a particular krbprincipalname attribute).\n         */\n        if (asprintf(&filter, FILTER\"%s))\", filtuser) < 0) {\n            filter = NULL;\n            st = ENOMEM;\n            goto cleanup;\n        }\n\n        /* get the current subtree list */\n        if ((st = krb5_get_subtree_info(ldap_context, &subtreelist, &ntrees)) != 0)\n            goto cleanup;\n\n        found_entry = FALSE;\n        /* search for entry with matching krbprincipalname attribute */\n        for (tree = 0; found_entry == FALSE && tree < ntrees; ++tree) {\n            if (principal_dn == NULL) {\n                LDAP_SEARCH_1(subtreelist[tree], ldap_context->lrparams->search_scope, filter, principal_attributes, IGNORE_STATUS);\n            } else {\n                /* just look for entry with principal_dn */\n                LDAP_SEARCH_1(principal_dn, LDAP_SCOPE_BASE, filter, principal_attributes, IGNORE_STATUS);\n            }\n            if (st == LDAP_SUCCESS) {\n                numlentries = ldap_count_entries(ld, result);\n                if (numlentries > 1) {\n                    free(filter);\n                    st = EINVAL;\n                    k5_setmsg(context, st,\n                              _(\"operation can not continue, more than one \"\n                                \"entry with principal name \\\"%s\\\" found\"),\n                              user);\n                    goto cleanup;\n                } else if (numlentries == 1) {\n                    found_entry = TRUE;\n                    if (principal_dn == NULL) {\n                        ent = ldap_first_entry(ld, result);\n                        if (ent != NULL) {\n                            /* setting principal_dn will cause that entry to be modified further down */\n                            if ((principal_dn = ldap_get_dn(ld, ent)) == NULL) {\n                                ldap_get_option (ld, LDAP_OPT_RESULT_CODE, &st);\n                                st = set_ldap_error (context, st, 0);\n                                free(filter);\n                                goto cleanup;\n                            }\n                        }\n                    }\n                }\n            } else if (st != LDAP_NO_SUCH_OBJECT) {\n                /* could not perform search, return with failure */\n                st = set_ldap_error (context, st, 0);\n                free(filter);\n                goto cleanup;\n            }\n            ldap_msgfree(result);\n            result = NULL;\n            /*\n             * If it isn't found then assume a standalone princ entry is to\n             * be created.\n             */\n        } /* end for (tree = 0; principal_dn == ... */\n\n        free(filter);\n\n        if (found_entry == FALSE && principal_dn != NULL) {\n            /*\n             * if principal_dn is null then there is code further down to\n             * deal with setting standalone_principal_dn.  Also note that\n             * this will set create_standalone true for\n             * non-mix-in entries which is okay if loading from a dump.\n             */\n            create_standalone = TRUE;\n            standalone_principal_dn = strdup(principal_dn);\n            CHECK_NULL(standalone_principal_dn);\n        }\n    } /* end if (entry->mask & KADM5_LOAD */\n\n    /* time to generate the DN information with the help of\n     * containerdn, principalcontainerreference or\n     * realmcontainerdn information\n     */\n    if (principal_dn == NULL && xargs.dn == NULL) { /* creation of standalone principal */\n        /* get the subtree information */\n        if (entry->princ->length == 2 && entry->princ->data[0].length == strlen(\"krbtgt\") &&\n            strncmp(entry->princ->data[0].data, \"krbtgt\", entry->princ->data[0].length) == 0) {\n            /* if the principal is a inter-realm principal, always created in the realm container */\n            subtree = strdup(ldap_context->lrparams->realmdn);\n        } else if (xargs.containerdn) {\n            if ((st=checkattributevalue(ld, xargs.containerdn, NULL, NULL, NULL)) != 0) {\n                if (st == KRB5_KDB_NOENTRY || st == KRB5_KDB_CONSTRAINT_VIOLATION) {\n                    int ost = st;\n                    st = EINVAL;\n                    k5_wrapmsg(context, ost, st, _(\"'%s' not found\"),\n                               xargs.containerdn);\n                }\n                goto cleanup;\n            }\n            subtree = strdup(xargs.containerdn);\n        } else if (ldap_context->lrparams->containerref && strlen(ldap_context->lrparams->containerref) != 0) {\n            /*\n             * Here the subtree should be changed with\n             * principalcontainerreference attribute value\n             */\n            subtree = strdup(ldap_context->lrparams->containerref);\n        } else {\n            subtree = strdup(ldap_context->lrparams->realmdn);\n        }\n        CHECK_NULL(subtree);\n\n        if (asprintf(&standalone_principal_dn, \"krbprincipalname=%s,%s\",\n                     filtuser, subtree) < 0)\n            standalone_principal_dn = NULL;\n        CHECK_NULL(standalone_principal_dn);\n        /*\n         * free subtree when you are done using the subtree\n         * set the boolean create_standalone to TRUE\n         */\n        create_standalone = TRUE;\n        free(subtree);\n        subtree = NULL;\n    }\n\n    /*\n     * If the DN information is presented by the user, time to\n     * validate the input to ensure that the DN falls under\n     * any of the subtrees\n     */\n    if (xargs.dn_from_kbd == TRUE) {\n        /* Get the current subtree list if we haven't already done so. */\n        if (subtreelist == NULL) {\n            st = krb5_get_subtree_info(ldap_context, &subtreelist, &ntrees);\n            if (st)\n                goto cleanup;\n        }\n\n        st = validate_xargs(context, ldap_server_handle, &xargs,\n                            standalone_principal_dn, subtreelist, ntrees);\n        if (st)\n            goto cleanup;\n    }\n\n    if (xargs.linkdn != NULL) {\n        /*\n         * link information can be changed using modprinc.\n         * However, link information can be changed only on the\n         * standalone kerberos principal objects. A standalone\n         * kerberos principal object is of type krbprincipal\n         * structural objectclass.\n         *\n         * NOTE: kerberos principals on an ldap object can't be\n         * linked to other ldap objects.\n         */\n        if (optype == MODIFY_PRINCIPAL &&\n            kerberos_principal_object_type != KDB_STANDALONE_PRINCIPAL_OBJECT) {\n            st = EINVAL;\n            snprintf(errbuf, sizeof(errbuf),\n                     _(\"link information can not be set/updated as the \"\n                       \"kerberos principal belongs to an ldap object\"));\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n        /*\n         * Check the link information. If there is already a link\n         * existing then this operation is not allowed.\n         */\n        {\n            char **linkdns=NULL;\n            int  j=0;\n\n            if ((st=krb5_get_linkdn(context, entry, &linkdns)) != 0) {\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"Failed getting object references\"));\n                k5_setmsg(context, st, \"%s\", errbuf);\n                goto cleanup;\n            }\n            if (linkdns != NULL) {\n                st = EINVAL;\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"kerberos principal is already linked to a ldap \"\n                           \"object\"));\n                k5_setmsg(context, st, \"%s\", errbuf);\n                for (j=0; linkdns[j] != NULL; ++j)\n                    free (linkdns[j]);\n                free (linkdns);\n                goto cleanup;\n            }\n        }\n\n        establish_links = TRUE;\n    }\n\n    if (entry->mask & KADM5_LAST_SUCCESS) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->last_success)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastSuccessfulAuth\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_LAST_FAILED) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->last_failed)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastFailedAuth\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free(strval[0]);\n    }\n\n    if (entry->mask & KADM5_FAIL_AUTH_COUNT) {\n        krb5_kvno fail_auth_count;\n\n        fail_auth_count = entry->fail_auth_count;\n        if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT)\n            fail_auth_count++;\n\n        st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                       LDAP_MOD_REPLACE,\n                                       fail_auth_count);\n        if (st != 0)\n            goto cleanup;\n    } else if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT) {\n        int attr_mask = 0;\n        krb5_boolean has_fail_count;\n\n        /* Check if the krbLoginFailedCount attribute exists.  (Through\n         * krb5 1.8.1, it wasn't set in new entries.) */\n        st = krb5_get_attributes_mask(context, entry, &attr_mask);\n        if (st != 0)\n            goto cleanup;\n        has_fail_count = ((attr_mask & KDB_FAIL_AUTH_COUNT_ATTR) != 0);\n\n        /*\n         * If the client library and server supports RFC 4525,\n         * then use it to increment by one the value of the\n         * krbLoginFailedCount attribute. Otherwise, assert the\n         * (provided) old value by deleting it before adding.\n         */\n#ifdef LDAP_MOD_INCREMENT\n        if (ldap_server_handle->server_info->modify_increment &&\n            has_fail_count) {\n            st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                           LDAP_MOD_INCREMENT, 1);\n            if (st != 0)\n                goto cleanup;\n        } else {\n#endif /* LDAP_MOD_INCREMENT */\n            if (has_fail_count) {\n                st = krb5_add_int_mem_ldap_mod(&mods,\n                                               \"krbLoginFailedCount\",\n                                               LDAP_MOD_DELETE,\n                                               entry->fail_auth_count);\n                if (st != 0)\n                    goto cleanup;\n            }\n            st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                           LDAP_MOD_ADD,\n                                           entry->fail_auth_count + 1);\n            if (st != 0)\n                goto cleanup;\n#ifdef LDAP_MOD_INCREMENT\n        }\n#endif\n    } else if (optype == ADD_PRINCIPAL) {\n        /* Initialize krbLoginFailedCount in new entries to help avoid a\n         * race during the first failed login. */\n        st = krb5_add_int_mem_ldap_mod(&mods, \"krbLoginFailedCount\",\n                                       LDAP_MOD_ADD, 0);\n    }\n\n    if (entry->mask & KADM5_MAX_LIFE) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbmaxticketlife\", LDAP_MOD_REPLACE, entry->max_life)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_MAX_RLIFE) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbmaxrenewableage\", LDAP_MOD_REPLACE,\n                                          entry->max_renewable_life)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_ATTRIBUTES) {\n        if ((st=krb5_add_int_mem_ldap_mod(&mods, \"krbticketflags\", LDAP_MOD_REPLACE,\n                                          entry->attributes)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_PRINCIPAL) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = user;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbprincipalname\", LDAP_MOD_REPLACE, strval)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_PRINC_EXPIRE_TIME) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->expiration)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbprincipalexpiration\", LDAP_MOD_REPLACE, strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_PW_EXPIRATION) {\n        memset(strval, 0, sizeof(strval));\n        if ((strval[0]=getstringtime(entry->pw_expiration)) == NULL)\n            goto cleanup;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpasswordexpiration\",\n                                          LDAP_MOD_REPLACE,\n                                          strval)) != 0) {\n            free (strval[0]);\n            goto cleanup;\n        }\n        free (strval[0]);\n    }\n\n    if (entry->mask & KADM5_POLICY || entry->mask & KADM5_KEY_HIST) {\n        memset(&princ_ent, 0, sizeof(princ_ent));\n        for (tl_data=entry->tl_data; tl_data; tl_data=tl_data->tl_data_next) {\n            if (tl_data->tl_data_type == KRB5_TL_KADM_DATA) {\n                if ((st = krb5_lookup_tl_kadm_data(tl_data, &princ_ent)) != 0) {\n                    goto cleanup;\n                }\n                break;\n            }\n        }\n    }\n\n    if (entry->mask & KADM5_POLICY) {\n        if (princ_ent.aux_attributes & KADM5_POLICY) {\n            memset(strval, 0, sizeof(strval));\n            if ((st = krb5_ldap_name_to_policydn (context, princ_ent.policy, &polname)) != 0)\n                goto cleanup;\n            strval[0] = polname;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_REPLACE, strval)) != 0)\n                goto cleanup;\n        } else {\n            st = EINVAL;\n            k5_setmsg(context, st, \"Password policy value null\");\n            goto cleanup;\n        }\n    } else if (entry->mask & KADM5_LOAD && found_entry == TRUE) {\n        /*\n         * a load is special in that existing entries must have attrs that\n         * removed.\n         */\n\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_REPLACE, NULL)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_POLICY_CLR) {\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbpwdpolicyreference\", LDAP_MOD_DELETE, NULL)) != 0)\n            goto cleanup;\n    }\n\n    if (entry->mask & KADM5_KEY_HIST) {\n        bersecretkey = krb5_encode_histkey(&princ_ent);\n        if (bersecretkey == NULL) {\n            st = ENOMEM;\n            goto cleanup;\n        }\n\n        st = krb5_add_ber_mem_ldap_mod(&mods, \"krbpwdhistory\",\n                                       LDAP_MOD_REPLACE | LDAP_MOD_BVALUES,\n                                       bersecretkey);\n        if (st != 0)\n            goto cleanup;\n        free_berdata(bersecretkey);\n        bersecretkey = NULL;\n    }\n\n    if (entry->mask & KADM5_KEY_DATA || entry->mask & KADM5_KVNO) {\n        krb5_kvno mkvno;\n\n        if ((st=krb5_dbe_lookup_mkvno(context, entry, &mkvno)) != 0)\n            goto cleanup;\n        bersecretkey = krb5_encode_krbsecretkey (entry->key_data,\n                                                 entry->n_key_data, mkvno);\n\n        if (bersecretkey == NULL) {\n            st = ENOMEM;\n            goto cleanup;\n        }\n        /* An empty list of bervals is only accepted for modify operations,\n         * not add operations. */\n        if (bersecretkey[0] != NULL || !create_standalone) {\n            st = krb5_add_ber_mem_ldap_mod(&mods, \"krbprincipalkey\",\n                                           LDAP_MOD_REPLACE | LDAP_MOD_BVALUES,\n                                           bersecretkey);\n            if (st != 0)\n                goto cleanup;\n        }\n\n        if (!(entry->mask & KADM5_PRINCIPAL)) {\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0]=getstringtime(entry->pw_expiration)) == NULL)\n                goto cleanup;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods,\n                                              \"krbpasswordexpiration\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n\n        /* Update last password change whenever a new key is set */\n        {\n            krb5_timestamp last_pw_changed;\n            if ((st=krb5_dbe_lookup_last_pwd_change(context, entry,\n                                                    &last_pw_changed)) != 0)\n                goto cleanup;\n\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0] = getstringtime(last_pw_changed)) == NULL)\n                goto cleanup;\n\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastPwdChange\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n\n    } /* Modify Key data ends here */\n\n    /* Auth indicators will also be stored in krbExtraData when processing\n     * tl_data. */\n    st = update_ldap_mod_auth_ind(context, entry, &mods);\n    if (st != 0)\n        goto cleanup;\n\n    /* Set tl_data */\n    if (entry->tl_data != NULL) {\n        int count = 0;\n        struct berval **ber_tl_data = NULL;\n        krb5_tl_data *ptr;\n        krb5_timestamp unlock_time;\n        for (ptr = entry->tl_data; ptr != NULL; ptr = ptr->tl_data_next) {\n            if (ptr->tl_data_type == KRB5_TL_LAST_PWD_CHANGE\n#ifdef SECURID\n                || ptr->tl_data_type == KRB5_TL_DB_ARGS\n#endif\n                || ptr->tl_data_type == KRB5_TL_KADM_DATA\n                || ptr->tl_data_type == KDB_TL_USER_INFO\n                || ptr->tl_data_type == KRB5_TL_CONSTRAINED_DELEGATION_ACL\n                || ptr->tl_data_type == KRB5_TL_LAST_ADMIN_UNLOCK)\n                continue;\n            count++;\n        }\n        if (count != 0) {\n            int j;\n            ber_tl_data = (struct berval **) calloc (count + 1,\n                                                     sizeof (struct berval*));\n            if (ber_tl_data == NULL) {\n                st = ENOMEM;\n                goto cleanup;\n            }\n            for (j = 0, ptr = entry->tl_data; ptr != NULL; ptr = ptr->tl_data_next) {\n                /* Ignore tl_data that are stored in separate directory\n                 * attributes */\n                if (ptr->tl_data_type == KRB5_TL_LAST_PWD_CHANGE\n#ifdef SECURID\n                    || ptr->tl_data_type == KRB5_TL_DB_ARGS\n#endif\n                    || ptr->tl_data_type == KRB5_TL_KADM_DATA\n                    || ptr->tl_data_type == KDB_TL_USER_INFO\n                    || ptr->tl_data_type == KRB5_TL_CONSTRAINED_DELEGATION_ACL\n                    || ptr->tl_data_type == KRB5_TL_LAST_ADMIN_UNLOCK)\n                    continue;\n                if ((st = tl_data2berval (ptr, &ber_tl_data[j])) != 0)\n                    break;\n                j++;\n            }\n            if (st == 0) {\n                ber_tl_data[count] = NULL;\n                st=krb5_add_ber_mem_ldap_mod(&mods, \"krbExtraData\",\n                                             LDAP_MOD_REPLACE |\n                                             LDAP_MOD_BVALUES, ber_tl_data);\n            }\n            free_berdata(ber_tl_data);\n            if (st != 0)\n                goto cleanup;\n        }\n        if ((st=krb5_dbe_lookup_last_admin_unlock(context, entry,\n                                                  &unlock_time)) != 0)\n            goto cleanup;\n        if (unlock_time != 0) {\n            /* Update last admin unlock */\n            memset(strval, 0, sizeof(strval));\n            if ((strval[0] = getstringtime(unlock_time)) == NULL)\n                goto cleanup;\n\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbLastAdminUnlock\",\n                                              LDAP_MOD_REPLACE, strval)) != 0) {\n                free (strval[0]);\n                goto cleanup;\n            }\n            free (strval[0]);\n        }\n    }\n\n    /* Directory specific attribute */\n    if (xargs.tktpolicydn != NULL) {\n        int tmask=0;\n\n        if (strlen(xargs.tktpolicydn) != 0) {\n            st = checkattributevalue(ld, xargs.tktpolicydn, \"objectclass\", policyclass, &tmask);\n            CHECK_CLASS_VALIDITY(st, tmask, _(\"ticket policy object value: \"));\n\n            strval[0] = xargs.tktpolicydn;\n            strval[1] = NULL;\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbticketpolicyreference\", LDAP_MOD_REPLACE, strval)) != 0)\n                goto cleanup;\n\n        } else {\n            /* if xargs.tktpolicydn is a empty string, then delete\n             * already existing krbticketpolicyreference attr */\n            if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbticketpolicyreference\", LDAP_MOD_DELETE, NULL)) != 0)\n                goto cleanup;\n        }\n\n    }\n\n    if (establish_links == TRUE) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = xargs.linkdn;\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"krbObjectReferences\", LDAP_MOD_REPLACE, strval)) != 0)\n            goto cleanup;\n    }\n\n    /*\n     * in case mods is NULL then return\n     * not sure but can happen in a modprinc\n     * so no need to return an error\n     * addprinc will at least have the principal name\n     * and the keys passed in\n     */\n    if (mods == NULL)\n        goto cleanup;\n\n    if (create_standalone == TRUE) {\n        memset(strval, 0, sizeof(strval));\n        strval[0] = \"krbprincipal\";\n        strval[1] = \"krbprincipalaux\";\n        strval[2] = \"krbTicketPolicyAux\";\n\n        if ((st=krb5_add_str_mem_ldap_mod(&mods, \"objectclass\", LDAP_MOD_ADD, strval)) != 0)\n            goto cleanup;\n\n        st = ldap_add_ext_s(ld, standalone_principal_dn, mods, NULL, NULL);\n        if (st == LDAP_ALREADY_EXISTS && entry->mask & KADM5_LOAD) {\n            /* a load operation must replace an existing entry */\n            st = ldap_delete_ext_s(ld, standalone_principal_dn, NULL, NULL);\n            if (st != LDAP_SUCCESS) {\n                snprintf(errbuf, sizeof(errbuf),\n                         _(\"Principal delete failed (trying to replace \"\n                           \"entry): %s\"), ldap_err2string(st));\n                st = translate_ldap_error (st, OP_ADD);\n                k5_setmsg(context, st, \"%s\", errbuf);\n                goto cleanup;\n            } else {\n                st = ldap_add_ext_s(ld, standalone_principal_dn, mods, NULL, NULL);\n            }\n        }\n        if (st != LDAP_SUCCESS) {\n            snprintf(errbuf, sizeof(errbuf), _(\"Principal add failed: %s\"),\n                     ldap_err2string(st));\n            st = translate_ldap_error (st, OP_ADD);\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n    } else {\n        /*\n         * Here existing ldap object is modified and can be related\n         * to any attribute, so always ensure that the ldap\n         * object is extended with all the kerberos related\n         * objectclasses so that there are no constraint\n         * violations.\n         */\n        {\n            char *attrvalues[] = {\"krbprincipalaux\", \"krbTicketPolicyAux\", NULL};\n            int p, q, r=0, amask=0;\n\n            if ((st=checkattributevalue(ld, (xargs.dn) ? xargs.dn : principal_dn,\n                                        \"objectclass\", attrvalues, &amask)) != 0)\n                goto cleanup;\n\n            memset(strval, 0, sizeof(strval));\n            for (p=1, q=0; p<=2; p<<=1, ++q) {\n                if ((p & amask) == 0)\n                    strval[r++] = attrvalues[q];\n            }\n            if (r != 0) {\n                if ((st=krb5_add_str_mem_ldap_mod(&mods, \"objectclass\", LDAP_MOD_ADD, strval)) != 0)\n                    goto cleanup;\n            }\n        }\n        if (xargs.dn != NULL)\n            st=ldap_modify_ext_s(ld, xargs.dn, mods, NULL, NULL);\n        else\n            st = ldap_modify_ext_s(ld, principal_dn, mods, NULL, NULL);\n\n        if (st != LDAP_SUCCESS) {\n            snprintf(errbuf, sizeof(errbuf), _(\"User modification failed: %s\"),\n                     ldap_err2string(st));\n            st = translate_ldap_error (st, OP_MOD);\n            k5_setmsg(context, st, \"%s\", errbuf);\n            goto cleanup;\n        }\n\n        if (entry->mask & KADM5_FAIL_AUTH_COUNT_INCREMENT)\n            entry->fail_auth_count++;\n    }\n\ncleanup:\n    if (user)\n        free(user);\n\n    if (filtuser)\n        free(filtuser);\n\n    free_xargs(xargs);\n\n    if (standalone_principal_dn)\n        free(standalone_principal_dn);\n\n    if (principal_dn)\n        free (principal_dn);\n\n    if (polname != NULL)\n        free(polname);\n\n    for (tre = 0; tre < ntrees; tre++)\n        free(subtreelist[tre]);\n    free(subtreelist);\n\n    if (subtree)\n        free (subtree);\n\n    if (bersecretkey) {\n        for (l=0; bersecretkey[l]; ++l) {\n            if (bersecretkey[l]->bv_val)\n                free (bersecretkey[l]->bv_val);\n            free (bersecretkey[l]);\n        }\n        free (bersecretkey);\n    }\n\n    if (keys)\n        free (keys);\n\n    ldap_mods_free(mods, 1);\n    ldap_osa_free_princ_ent(&princ_ent);\n    ldap_msgfree(result);\n    krb5_ldap_put_handle_to_pool(ldap_context, ldap_server_handle);\n    return(st);\n}",
        "description": "An authenticated kadmin user with permissions to add principals to an LDAP Kerberos database can trigger a denial of service through a NULL pointer dereference or bypass a DN container check by providing tagged data that is internal to the database module.",
        "commit": "It was discovered that the LDAP DN checking mechanism in the MIT krb5 library had several vulnerabilities. Specifically, the `KDB_TL_USER_INFO` tl-data, which is intended to be internal to the LDAP KDB module, could be inadvertently sent by kadmin clients due to insufficient type number validation. This allowed authenticated kadmin users with permission to add principals to an LDAP Kerberos database to either cause a null dereference in kadmind or bypass intended DN container checks by manipulating tagged data.\n\nTo address these issues, the type number for `KDB_TL_USER_INFO` was set to less than 256, and filtering was implemented in `kadm5_create_principal_3()` to exclude such low type numbers. Additionally, in the LDAP KDB module, checks for container membership were enhanced by factoring out the checks into helper functions and applying them to all relevant client-influenced DNs during `put_principal` operations. These changes aimed to prevent unauthorized access and ensure proper DN validation."
    },
    {
        "cwe": "CWE-436",
        "func_name": "xen-project/sh_page_fault",
        "score": 0.7628994584083557,
        "func_before": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && (mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying) )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "func_after": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn) && is_hvm_domain(d) &&\n         mfn_to_page(gmfn)->pagetable_dying )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "description": "An issue was discovered in Xen versions up to 4.11.x, where x86 PV guest OS users could potentially cause a denial of service (host OS crash) or escalate their privileges to those of the host OS due to an interpretation conflict involving a union data structure related to shadow paging. This problem arose from an incorrect fix applied to address CVE-2017-15595.",
        "commit": "The vulnerability involves modifying the `struct page_info` in the x86/shadow module by reducing the size of the `shadow_flags` field to 16 bits to prevent overlap with the `linear_pt_count` field used in PV domains. To address this change, a new, HVM-only field named `pagetable_dying` was introduced to handle the functionality previously managed by the upper 16 bits of `shadow_flags`. The accesses to `shadow_flags` in functions like `shadow_{pro,de}mote()` were changed to non-atomic, non-bitops operations because atomic operations such as `{test,set,clear}_bit()` are not supported on `uint16_t` fields. This modification is justified by the fact that all updates to `shadow_flags` should occur while holding the paging lock, and other updates use bitwise operations (`|=`, `&=`), which inherently provide some level of atomicity. This change is part of the XSA-280 patch set."
    },
    {
        "cwe": "CWE-252",
        "func_name": "libtiff/createCroppedImage",
        "score": 0.7595900297164917,
        "func_before": "static int\ncreateCroppedImage(struct image_data *image, struct crop_mask *crop, \n                   unsigned char **read_buff_ptr, unsigned char **crop_buff_ptr)\n  {\n  tsize_t   cropsize;\n  unsigned  char *read_buff = NULL;\n  unsigned  char *crop_buff = NULL;\n  unsigned  char *new_buff  = NULL;\n  static    tsize_t  prev_cropsize = 0;\n\n  read_buff = *read_buff_ptr;\n\n  /* process full image, no crop buffer needed */\n  crop_buff = read_buff;\n  *crop_buff_ptr = read_buff;\n  crop->combined_width = image->width;\n  crop->combined_length = image->length;\n\n  cropsize = crop->bufftotal;\n  crop_buff = *crop_buff_ptr;\n  if (!crop_buff)\n    {\n    crop_buff = (unsigned char *)limitMalloc(cropsize);\n    *crop_buff_ptr = crop_buff;\n    _TIFFmemset(crop_buff, 0, cropsize);\n    prev_cropsize = cropsize;\n    }\n  else\n    {\n    if (prev_cropsize < cropsize)\n      {\n      new_buff = _TIFFrealloc(crop_buff, cropsize);\n      if (!new_buff)\n        {\n\tfree (crop_buff);\n        crop_buff = (unsigned char *)limitMalloc(cropsize);\n        }\n      else\n        crop_buff = new_buff;\n      _TIFFmemset(crop_buff, 0, cropsize);\n      }\n    }\n\n  if (!crop_buff)\n    {\n    TIFFError(\"createCroppedImage\", \"Unable to allocate/reallocate crop buffer\");\n    return (-1);\n    }\n  *crop_buff_ptr = crop_buff;\n\n  if (crop->crop_mode & CROP_INVERT)\n    {\n    switch (crop->photometric)\n      {\n      /* Just change the interpretation */\n      case PHOTOMETRIC_MINISWHITE:\n      case PHOTOMETRIC_MINISBLACK:\n\t   image->photometric = crop->photometric;\n\t   break;\n      case INVERT_DATA_ONLY:\n      case INVERT_DATA_AND_TAG:\n           if (invertImage(image->photometric, image->spp, image->bps, \n                           crop->combined_width, crop->combined_length, crop_buff))\n             {\n             TIFFError(\"createCroppedImage\", \n                       \"Failed to invert colorspace for image or cropped selection\");\n             return (-1);\n             }\n           if (crop->photometric == INVERT_DATA_AND_TAG)\n             {\n             switch (image->photometric)\n               {\n               case PHOTOMETRIC_MINISWHITE:\n \t            image->photometric = PHOTOMETRIC_MINISBLACK;\n\t            break;\n               case PHOTOMETRIC_MINISBLACK:\n \t            image->photometric = PHOTOMETRIC_MINISWHITE;\n\t            break;\n               default:\n\t            break;\n\t       }\n\t     }\n           break;\n      default: break;\n      }\n    }\n\n  if (crop->crop_mode & CROP_MIRROR)\n    {\n    if (mirrorImage(image->spp, image->bps, crop->mirror, \n                    crop->combined_width, crop->combined_length, crop_buff))\n      {\n      TIFFError(\"createCroppedImage\", \"Failed to mirror image or cropped selection %s\", \n\t       (crop->rotation == MIRROR_HORIZ) ? \"horizontally\" : \"vertically\");\n      return (-1);\n      }\n    }\n\n  if (crop->crop_mode & CROP_ROTATE) /* rotate should be last as it can reallocate the buffer */\n    {\n    if (rotateImage(crop->rotation, image, &crop->combined_width, \n                    &crop->combined_length, crop_buff_ptr))\n      {\n      TIFFError(\"createCroppedImage\", \n                \"Failed to rotate image or cropped selection by %\"PRIu16\" degrees\", crop->rotation);\n      return (-1);\n      }\n    }\n\n  if (crop_buff == read_buff) /* we used the read buffer for the crop buffer */\n    *read_buff_ptr = NULL;    /* so we don't try to free it later */\n\n  return (0);\n  }",
        "func_after": "static int\ncreateCroppedImage(struct image_data *image, struct crop_mask *crop, \n                   unsigned char **read_buff_ptr, unsigned char **crop_buff_ptr)\n  {\n  tsize_t   cropsize;\n  unsigned  char *read_buff = NULL;\n  unsigned  char *crop_buff = NULL;\n  unsigned  char *new_buff  = NULL;\n  static    tsize_t  prev_cropsize = 0;\n\n  read_buff = *read_buff_ptr;\n\n  /* process full image, no crop buffer needed */\n  crop_buff = read_buff;\n  *crop_buff_ptr = read_buff;\n  crop->combined_width = image->width;\n  crop->combined_length = image->length;\n\n  cropsize = crop->bufftotal;\n  crop_buff = *crop_buff_ptr;\n  if (!crop_buff)\n    {\n    crop_buff = (unsigned char *)limitMalloc(cropsize);\n    if (!crop_buff)\n    {\n        TIFFError(\"createCroppedImage\", \"Unable to allocate/reallocate crop buffer\");\n        return (-1);\n    }\n    _TIFFmemset(crop_buff, 0, cropsize);\n    prev_cropsize = cropsize;\n    }\n  else\n    {\n    if (prev_cropsize < cropsize)\n      {\n      new_buff = _TIFFrealloc(crop_buff, cropsize);\n      if (!new_buff)\n        {\n\tfree (crop_buff);\n        crop_buff = (unsigned char *)limitMalloc(cropsize);\n        }\n      else\n        crop_buff = new_buff;\n      if (!crop_buff)\n      {\n          TIFFError(\"createCroppedImage\", \"Unable to allocate/reallocate crop buffer\");\n          return (-1);\n      }\n      _TIFFmemset(crop_buff, 0, cropsize);\n      }\n    }\n\n  *crop_buff_ptr = crop_buff;\n\n  if (crop->crop_mode & CROP_INVERT)\n    {\n    switch (crop->photometric)\n      {\n      /* Just change the interpretation */\n      case PHOTOMETRIC_MINISWHITE:\n      case PHOTOMETRIC_MINISBLACK:\n\t   image->photometric = crop->photometric;\n\t   break;\n      case INVERT_DATA_ONLY:\n      case INVERT_DATA_AND_TAG:\n           if (invertImage(image->photometric, image->spp, image->bps, \n                           crop->combined_width, crop->combined_length, crop_buff))\n             {\n             TIFFError(\"createCroppedImage\", \n                       \"Failed to invert colorspace for image or cropped selection\");\n             return (-1);\n             }\n           if (crop->photometric == INVERT_DATA_AND_TAG)\n             {\n             switch (image->photometric)\n               {\n               case PHOTOMETRIC_MINISWHITE:\n \t            image->photometric = PHOTOMETRIC_MINISBLACK;\n\t            break;\n               case PHOTOMETRIC_MINISBLACK:\n \t            image->photometric = PHOTOMETRIC_MINISWHITE;\n\t            break;\n               default:\n\t            break;\n\t       }\n\t     }\n           break;\n      default: break;\n      }\n    }\n\n  if (crop->crop_mode & CROP_MIRROR)\n    {\n    if (mirrorImage(image->spp, image->bps, crop->mirror, \n                    crop->combined_width, crop->combined_length, crop_buff))\n      {\n      TIFFError(\"createCroppedImage\", \"Failed to mirror image or cropped selection %s\", \n\t       (crop->rotation == MIRROR_HORIZ) ? \"horizontally\" : \"vertically\");\n      return (-1);\n      }\n    }\n\n  if (crop->crop_mode & CROP_ROTATE) /* rotate should be last as it can reallocate the buffer */\n    {\n    if (rotateImage(crop->rotation, image, &crop->combined_width, \n                    &crop->combined_length, crop_buff_ptr))\n      {\n      TIFFError(\"createCroppedImage\", \n                \"Failed to rotate image or cropped selection by %\"PRIu16\" degrees\", crop->rotation);\n      return (-1);\n      }\n    }\n\n  if (crop_buff == read_buff) /* we used the read buffer for the crop buffer */\n    *read_buff_ptr = NULL;    /* so we don't try to free it later */\n\n  return (0);\n  }",
        "description": "Unchecked return value leading to NULL pointer dereference in the tiffcrop utility of libtiff 4.3.0 enables attackers to trigger a denial-of-service condition through a specially crafted TIFF file. Users who compile libtiff from source can apply the fix available in commit f2b656e2.",
        "commit": "It was identified that checks for the return value of memory allocation functions like `malloc` were missing, which could lead to potential issues if the allocation fails."
    }
]