[
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7217648029327393,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-190",
        "func_name": "torvalds/ring_buffer_resize",
        "score": 0.7831047177314758,
        "func_before": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tsize = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tsize *= BUF_PAGE_SIZE;\n\n\t/* we need a minimum of two pages */\n\tif (size < BUF_PAGE_SIZE * 2)\n\t\tsize = BUF_PAGE_SIZE * 2;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}",
        "func_after": "int ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/* we need a minimum of two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tsize = nr_pages * BUF_PAGE_SIZE;\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}",
        "description": "The `ring_buffer_resize` function in the profiling subsystem of the Linux kernel, prior to version 4.6.1, mishandles specific integer calculations. This flaw enables local users to escalate their privileges by writing to the `/sys/kernel/debug/tracing/buffer_size_kb` file.",
        "commit": "A vulnerability was identified in the ring buffer resizing functionality within the Linux kernel. Specifically, if the size passed to the `ring_buffer_resize()` function exceeds `MAX_LONG - BUF_PAGE_SIZE`, the `DIV_ROUND_UP()` operation will incorrectly return zero. This occurs because the intermediate calculation during the resizing process leads to an overflow, resulting in an incorrect number of buffer pages being allocated. Consequently, the logic fails, potentially causing a kernel crash. To address this issue, the redundant use of `DIV_ROUND_UP()` should be cleaned up, ensuring proper handling of large sizes and preventing overflow."
    },
    {
        "cwe": "CWE-697",
        "func_name": "cvxopt/spsolve",
        "score": 0.7667700052261353,
        "func_before": "static PyObject* spsolve(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *B, *X=NULL;\n    cholmod_sparse *Bc=NULL, *Xc=NULL;\n    PyObject *F;\n    cholmod_factor *L;\n    int n, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", NULL};\n    int sysvalues[] = {CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|i\", kwlist, &F,\n        &B, &sys)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!SpMatrix_Check(B) ||\n        (SP_ID(B) == DOUBLE  && L->xtype == CHOLMOD_COMPLEX) ||\n        (SP_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a sparse matrix of the same \"\n                \"numerical type as F\");\n    if (SP_NROWS(B) != n)\n        PY_ERR(PyExc_ValueError, \"incompatible dimensions for B\");\n\n    if (!(Bc = create_matrix(B))) return PyErr_NoMemory();\n    Xc = CHOL(spsolve)(sysvalues[sys], L, Bc, &Common);\n    free_matrix(Bc);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n    if (Common.status != CHOLMOD_OK)\n        PY_ERR(PyExc_ValueError, \"solve step failed\");\n\n    if (!(X = SpMatrix_New(Xc->nrow, Xc->ncol,\n        ((int_t*)Xc->p)[Xc->ncol], (L->xtype == CHOLMOD_REAL ? DOUBLE :\n        COMPLEX)))) {\n        CHOL(free_sparse)(&Xc, &Common);\n        return NULL;\n    }\n    memcpy(SP_COL(X), Xc->p, (Xc->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(X), Xc->i, ((int_t *)Xc->p)[Xc->ncol]*sizeof(int_t));\n    memcpy(SP_VAL(X), Xc->x,\n        ((int_t *) Xc->p)[Xc->ncol]*E_SIZE[SP_ID(X)]);\n    CHOL(free_sparse)(&Xc, &Common);\n    return (PyObject *) X;\n}",
        "func_after": "static PyObject* spsolve(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *B, *X=NULL;\n    cholmod_sparse *Bc=NULL, *Xc=NULL;\n    PyObject *F;\n    cholmod_factor *L;\n    int n, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", NULL};\n    int sysvalues[] = {CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|i\", kwlist, &F,\n        &B, &sys)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!SpMatrix_Check(B) ||\n        (SP_ID(B) == DOUBLE  && L->xtype == CHOLMOD_COMPLEX) ||\n        (SP_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a sparse matrix of the same \"\n                \"numerical type as F\");\n    if (SP_NROWS(B) != n)\n        PY_ERR(PyExc_ValueError, \"incompatible dimensions for B\");\n\n    if (!(Bc = create_matrix(B))) return PyErr_NoMemory();\n    Xc = CHOL(spsolve)(sysvalues[sys], L, Bc, &Common);\n    free_matrix(Bc);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n    if (Common.status != CHOLMOD_OK)\n        PY_ERR(PyExc_ValueError, \"solve step failed\");\n\n    if (!(X = SpMatrix_New(Xc->nrow, Xc->ncol,\n        ((int_t*)Xc->p)[Xc->ncol], (L->xtype == CHOLMOD_REAL ? DOUBLE :\n        COMPLEX)))) {\n        CHOL(free_sparse)(&Xc, &Common);\n        return NULL;\n    }\n    memcpy(SP_COL(X), Xc->p, (Xc->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(X), Xc->i, ((int_t *)Xc->p)[Xc->ncol]*sizeof(int_t));\n    memcpy(SP_VAL(X), Xc->x,\n        ((int_t *) Xc->p)[Xc->ncol]*E_SIZE[SP_ID(X)]);\n    CHOL(free_sparse)(&Xc, &Common);\n    return (PyObject *) X;\n}",
        "description": "An incomplete string comparison vulnerability exists in the cvxopt library versions up to 1.2.6 within certain APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve). This flaw enables attackers to carry out Denial of Service (DoS) attacks by constructing fake Capsule objects.",
        "commit": "This update resolves an unspecified issue identified by #193, updates the associated documentation, and increments the version number to 1.2.7."
    },
    {
        "cwe": "CWE-131",
        "func_name": "the-tcpdump-group/daemon_msg_findallif_req",
        "score": 0.776790976524353,
        "func_before": "static int\ndaemon_msg_findallif_req(uint8 ver, struct daemon_slpars *pars, uint32 plen)\n{\n\tchar errbuf[PCAP_ERRBUF_SIZE];\t\t// buffer for network errors\n\tchar errmsgbuf[PCAP_ERRBUF_SIZE];\t// buffer for errors to send to the client\n\tchar sendbuf[RPCAP_NETBUF_SIZE];\t// temporary buffer in which data to be sent is buffered\n\tint sendbufidx = 0;\t\t\t// index which keeps the number of bytes currently buffered\n\tpcap_if_t *alldevs = NULL;\t\t// pointer to the header of the interface chain\n\tpcap_if_t *d;\t\t\t\t// temp pointer needed to scan the interface chain\n\tstruct pcap_addr *address;\t\t// pcap structure that keeps a network address of an interface\n\tstruct rpcap_findalldevs_if *findalldevs_if;// rpcap structure that packet all the data of an interface together\n\tuint16 nif = 0;\t\t\t\t// counts the number of interface listed\n\n\t// Discard the rest of the message; there shouldn't be any payload.\n\tif (rpcapd_discard(pars->sockctrl, plen) == -1)\n\t{\n\t\t// Network error.\n\t\treturn -1;\n\t}\n\n\t// Retrieve the device list\n\tif (pcap_findalldevs(&alldevs, errmsgbuf) == -1)\n\t\tgoto error;\n\n\tif (alldevs == NULL)\n\t{\n\t\tif (rpcap_senderror(pars->sockctrl, ver, PCAP_ERR_NOREMOTEIF,\n\t\t\t\"No interfaces found! Make sure libpcap/WinPcap is properly installed\"\n\t\t\t\" and you have the right to access to the remote device.\",\n\t\t\terrbuf) == -1)\n\t\t{\n\t\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\t\treturn -1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t// checks the number of interfaces and it computes the total length of the payload\n\tfor (d = alldevs; d != NULL; d = d->next)\n\t{\n\t\tnif++;\n\n\t\tif (d->description)\n\t\t\tplen+= strlen(d->description);\n\t\tif (d->name)\n\t\t\tplen+= strlen(d->name);\n\n\t\tplen+= sizeof(struct rpcap_findalldevs_if);\n\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\tplen+= (sizeof(struct rpcap_sockaddr) * 4);\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// RPCAP findalldevs command\n\tif (sock_bufferize(NULL, sizeof(struct rpcap_header), NULL,\n\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf,\n\t    PCAP_ERRBUF_SIZE) == -1)\n\t\tgoto error;\n\n\trpcap_createhdr((struct rpcap_header *) sendbuf, ver,\n\t    RPCAP_MSG_FINDALLIF_REPLY, nif, plen);\n\n\t// send the interface list\n\tfor (d = alldevs; d != NULL; d = d->next)\n\t{\n\t\tuint16 lname, ldescr;\n\n\t\tfindalldevs_if = (struct rpcap_findalldevs_if *) &sendbuf[sendbufidx];\n\n\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_findalldevs_if), NULL,\n\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\tmemset(findalldevs_if, 0, sizeof(struct rpcap_findalldevs_if));\n\n\t\tif (d->description) ldescr = (short) strlen(d->description);\n\t\telse ldescr = 0;\n\t\tif (d->name) lname = (short) strlen(d->name);\n\t\telse lname = 0;\n\n\t\tfindalldevs_if->desclen = htons(ldescr);\n\t\tfindalldevs_if->namelen = htons(lname);\n\t\tfindalldevs_if->flags = htonl(d->flags);\n\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\tfindalldevs_if->naddr++;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tfindalldevs_if->naddr = htons(findalldevs_if->naddr);\n\n\t\tif (sock_bufferize(d->name, lname, sendbuf, &sendbufidx,\n\t\t    RPCAP_NETBUF_SIZE, SOCKBUF_BUFFERIZE, errmsgbuf,\n\t\t    PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\tif (sock_bufferize(d->description, ldescr, sendbuf, &sendbufidx,\n\t\t    RPCAP_NETBUF_SIZE, SOCKBUF_BUFFERIZE, errmsgbuf,\n\t\t    PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\t// send all addresses\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\tstruct rpcap_sockaddr *sockaddr;\n\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->addr, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->netmask, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->broadaddr, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->dstaddr, sockaddr);\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// We no longer need the device list. Free it.\n\tpcap_freealldevs(alldevs);\n\n\t// Send a final command that says \"now send it!\"\n\tif (sock_send(pars->sockctrl, sendbuf, sendbufidx, errbuf, PCAP_ERRBUF_SIZE) == -1)\n\t{\n\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\nerror:\n\tif (alldevs)\n\t\tpcap_freealldevs(alldevs);\n\n\tif (rpcap_senderror(pars->sockctrl, ver, PCAP_ERR_FINDALLIF,\n\t    errmsgbuf, errbuf) == -1)\n\t{\n\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\treturn -1;\n\t}\n\treturn 0;\n}",
        "func_after": "static int\ndaemon_msg_findallif_req(uint8 ver, struct daemon_slpars *pars, uint32 plen)\n{\n\tchar errbuf[PCAP_ERRBUF_SIZE];\t\t// buffer for network errors\n\tchar errmsgbuf[PCAP_ERRBUF_SIZE];\t// buffer for errors to send to the client\n\tchar sendbuf[RPCAP_NETBUF_SIZE];\t// temporary buffer in which data to be sent is buffered\n\tint sendbufidx = 0;\t\t\t// index which keeps the number of bytes currently buffered\n\tpcap_if_t *alldevs = NULL;\t\t// pointer to the header of the interface chain\n\tpcap_if_t *d;\t\t\t\t// temp pointer needed to scan the interface chain\n\tstruct pcap_addr *address;\t\t// pcap structure that keeps a network address of an interface\n\tstruct rpcap_findalldevs_if *findalldevs_if;// rpcap structure that packet all the data of an interface together\n\tuint32 replylen;\t\t\t// length of reply payload\n\tuint16 nif = 0;\t\t\t\t// counts the number of interface listed\n\n\t// Discard the rest of the message; there shouldn't be any payload.\n\tif (rpcapd_discard(pars->sockctrl, plen) == -1)\n\t{\n\t\t// Network error.\n\t\treturn -1;\n\t}\n\n\t// Retrieve the device list\n\tif (pcap_findalldevs(&alldevs, errmsgbuf) == -1)\n\t\tgoto error;\n\n\tif (alldevs == NULL)\n\t{\n\t\tif (rpcap_senderror(pars->sockctrl, ver, PCAP_ERR_NOREMOTEIF,\n\t\t\t\"No interfaces found! Make sure libpcap/WinPcap is properly installed\"\n\t\t\t\" and you have the right to access to the remote device.\",\n\t\t\terrbuf) == -1)\n\t\t{\n\t\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\t\treturn -1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t// This checks the number of interfaces and computes the total\n\t// length of the payload.\n\treplylen = 0;\n\tfor (d = alldevs; d != NULL; d = d->next)\n\t{\n\t\tnif++;\n\n\t\tif (d->description)\n\t\t\treplylen += strlen(d->description);\n\t\tif (d->name)\n\t\t\treplylen += strlen(d->name);\n\n\t\treplylen += sizeof(struct rpcap_findalldevs_if);\n\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\treplylen += (sizeof(struct rpcap_sockaddr) * 4);\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// RPCAP findalldevs command\n\tif (sock_bufferize(NULL, sizeof(struct rpcap_header), NULL,\n\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf,\n\t    PCAP_ERRBUF_SIZE) == -1)\n\t\tgoto error;\n\n\trpcap_createhdr((struct rpcap_header *) sendbuf, ver,\n\t    RPCAP_MSG_FINDALLIF_REPLY, nif, replylen);\n\n\t// send the interface list\n\tfor (d = alldevs; d != NULL; d = d->next)\n\t{\n\t\tuint16 lname, ldescr;\n\n\t\tfindalldevs_if = (struct rpcap_findalldevs_if *) &sendbuf[sendbufidx];\n\n\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_findalldevs_if), NULL,\n\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\tmemset(findalldevs_if, 0, sizeof(struct rpcap_findalldevs_if));\n\n\t\tif (d->description) ldescr = (short) strlen(d->description);\n\t\telse ldescr = 0;\n\t\tif (d->name) lname = (short) strlen(d->name);\n\t\telse lname = 0;\n\n\t\tfindalldevs_if->desclen = htons(ldescr);\n\t\tfindalldevs_if->namelen = htons(lname);\n\t\tfindalldevs_if->flags = htonl(d->flags);\n\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\tfindalldevs_if->naddr++;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tfindalldevs_if->naddr = htons(findalldevs_if->naddr);\n\n\t\tif (sock_bufferize(d->name, lname, sendbuf, &sendbufidx,\n\t\t    RPCAP_NETBUF_SIZE, SOCKBUF_BUFFERIZE, errmsgbuf,\n\t\t    PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\tif (sock_bufferize(d->description, ldescr, sendbuf, &sendbufidx,\n\t\t    RPCAP_NETBUF_SIZE, SOCKBUF_BUFFERIZE, errmsgbuf,\n\t\t    PCAP_ERRBUF_SIZE) == -1)\n\t\t\tgoto error;\n\n\t\t// send all addresses\n\t\tfor (address = d->addresses; address != NULL; address = address->next)\n\t\t{\n\t\t\tstruct rpcap_sockaddr *sockaddr;\n\n\t\t\t/*\n\t\t\t * Send only IPv4 and IPv6 addresses over the wire.\n\t\t\t */\n\t\t\tswitch (address->addr->sa_family)\n\t\t\t{\n\t\t\tcase AF_INET:\n#ifdef AF_INET6\n\t\t\tcase AF_INET6:\n#endif\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->addr, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->netmask, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->broadaddr, sockaddr);\n\n\t\t\t\tsockaddr = (struct rpcap_sockaddr *) &sendbuf[sendbufidx];\n\t\t\t\tif (sock_bufferize(NULL, sizeof(struct rpcap_sockaddr), NULL,\n\t\t\t\t    &sendbufidx, RPCAP_NETBUF_SIZE, SOCKBUF_CHECKONLY, errmsgbuf, PCAP_ERRBUF_SIZE) == -1)\n\t\t\t\t\tgoto error;\n\t\t\t\tdaemon_seraddr((struct sockaddr_storage *) address->dstaddr, sockaddr);\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t// We no longer need the device list. Free it.\n\tpcap_freealldevs(alldevs);\n\n\t// Send a final command that says \"now send it!\"\n\tif (sock_send(pars->sockctrl, sendbuf, sendbufidx, errbuf, PCAP_ERRBUF_SIZE) == -1)\n\t{\n\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n\nerror:\n\tif (alldevs)\n\t\tpcap_freealldevs(alldevs);\n\n\tif (rpcap_senderror(pars->sockctrl, ver, PCAP_ERR_FINDALLIF,\n\t    errmsgbuf, errbuf) == -1)\n\t{\n\t\trpcapd_log(LOGPRIO_ERROR, \"Send to client failed: %s\", errbuf);\n\t\treturn -1;\n\t}\n\treturn 0;\n}",
        "description": "rpcapd/daemon.c in libpcap before 1.9.1 improperly handles certain length values due to the reuse of a variable, potentially exposing an attack vector that involves additional data appended to the end of a request.",
        "commit": "Using the same local variable to calculate both the remaining request length and the reply payload length can lead to confusion and potential errors, especially if the request contains additional data at the end. This practice can result in incorrect handling of packet capture operations, posing a security risk."
    },
    {
        "cwe": "CWE-667",
        "func_name": "torvalds/pipe_to_file",
        "score": 0.7786850929260254,
        "func_before": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tpage_cache_release(page);\n\tunlock_page(page);\nout_ret:\n\treturn ret;\n}",
        "func_after": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out_release;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tunlock_page(page);\nout_release:\n\tpage_cache_release(page);\nout_ret:\n\treturn ret;\n}",
        "description": "In the splice subsystem of the Linux kernel prior to version 2.6.22.2, there is an issue where the `add_to_page_cache_lru` function's failure is not adequately managed. As a result, the system attempts to unlock a page that was not previously locked, leading to a denial of service condition characterized by a kernel BUG and potential system crash. This vulnerability can be exploited by local users using tools such as fio for I/O operations.",
        "commit": "If the `add_to_page_cache_lru()` function fails, the page remains unlocked. However, the subsequent code in `splice` jumps to an error path that attempts to release and unlock the page, leading to a `BUG()` in `unlock_page()`. This issue can be resolved by introducing an additional label that solely handles the page release without attempting to unlock it. This bug was triggered on EL5 by Gurudas Pai using the fio tool."
    }
]