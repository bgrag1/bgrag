[
    {
        "cwe": "CWE-1284",
        "func_name": "Samsung/crypto_bignum_allocate",
        "score": 0.7929982542991638,
        "func_before": "struct bignum *crypto_bignum_allocate(size_t size_bits)\n{\n\tsize_t sz = mpa_StaticVarSizeInU32(size_bits) *\tsizeof(uint32_t);\n\tstruct mpa_numbase_struct *bn = calloc(1, sz);\n\n\tif (!bn)\n\t\treturn NULL;\n\tbn->alloc = sz - MPA_NUMBASE_METADATA_SIZE_IN_U32 * sizeof(uint32_t);\n\treturn (struct bignum *)bn;\n}",
        "func_after": "struct bignum *crypto_bignum_allocate(size_t size_bits)\n{\n\tsize_t sz = mpa_StaticVarSizeInU32(size_bits) *\tsizeof(uint32_t);\n\t// struct mpa_numbase_struct *bn = calloc(1, sz);\n\tstruct mpa_numbase_struct *bn= TEE_Malloc(sz, TEE_MALLOC_FILL_ZERO);\n\tif (!bn)\n\t\treturn NULL;\n\tbn->alloc = sz - MPA_NUMBASE_METADATA_SIZE_IN_U32 * sizeof(uint32_t);\n\treturn (struct bignum *)bn;\n}",
        "description": "The function `tee_obj_free` in Samsung mTower through version 0.3.0 enables a trusted application to cause a Denial of Service (DoS) by calling the function `TEE_AllocateOperation` with a disrupted heap layout, which is associated with `utee_cryp_obj_alloc`.",
        "commit": "A vulnerability has been addressed in a software system, specifically identified by CVE-2022-40761."
    },
    {
        "cwe": "CWE-476",
        "func_name": "ImageMagick/LoadOpenCLDevices",
        "score": 0.8170896172523499,
        "func_before": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireQuantumMemory(length,\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "func_after": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireCriticalMemory(length*\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "description": "An issue was discovered in ImageMagick where a NULL pointer dereference vulnerability exists in the function responsible for loading OpenCL devices. This flaw allows attackers to trigger a denial of service through the use of a specially crafted file.",
        "commit": "It was discovered that ImageMagick, a popular image processing library, contains a vulnerability related to improper handling of certain image formats, potentially leading to buffer overflows or other memory corruption issues."
    },
    {
        "cwe": "CWE-787",
        "func_name": "radareorg/grub_ext2_read_block",
        "score": 0.8223506808280945,
        "func_before": "static grub_disk_addr_t\ngrub_ext2_read_block (grub_fshelp_node_t node, grub_disk_addr_t fileblock)\n{\n  struct grub_ext2_data *data = node->data;\n  struct grub_ext2_inode *inode = &node->inode;\n  int blknr = -1;\n  unsigned int blksz = EXT2_BLOCK_SIZE (data);\n  int log2_blksz = LOG2_EXT2_BLOCK_SIZE (data);\n\n  if (grub_le_to_cpu32(inode->flags) & EXT4_EXTENTS_FLAG)\n    {\n#ifndef _MSC_VER\n\t  char buf[EXT2_BLOCK_SIZE (data)];\n#else\n\t  char * buf = grub_malloc (EXT2_BLOCK_SIZE(data));\n#endif\n      struct grub_ext4_extent_header *leaf;\n      struct grub_ext4_extent *ext;\n      int i;\n\n      leaf = grub_ext4_find_leaf (data, buf,\n\t\t  (struct grub_ext4_extent_header *) inode->blocks.dir_blocks,\n\t\t  fileblock);\n      if (! leaf)\n        {\n          grub_error (GRUB_ERR_BAD_FS, \"invalid extent\");\n          return -1;\n        }\n\n      ext = (struct grub_ext4_extent *) (leaf + 1);\n      for (i = 0; i < grub_le_to_cpu16 (leaf->entries); i++)\n        {\n          if (fileblock < grub_le_to_cpu32 (ext[i].block))\n            break;\n        }\n\n      if (--i >= 0)\n        {\n          fileblock -= grub_le_to_cpu32 (ext[i].block);\n          if (fileblock >= grub_le_to_cpu16 (ext[i].len))\n            return 0;\n          else\n            {\n              grub_disk_addr_t start;\n\n              start = grub_le_to_cpu16 (ext[i].start_hi);\n              start = (start << 32) + grub_le_to_cpu32 (ext[i].start);\n\n              return fileblock + start;\n            }\n        }\n      else\n        {\n          grub_error (GRUB_ERR_BAD_FS, \"something wrong with extent\");\n          return -1;\n        }\n    }\n  /* Direct blocks.  */\n  if (fileblock < INDIRECT_BLOCKS)\n    blknr = grub_le_to_cpu32 (inode->blocks.dir_blocks[fileblock]);\n  /* Indirect.  */\n  else if (fileblock < INDIRECT_BLOCKS + blksz / 4)\n    {\n      grub_uint32_t *indir;\n\n      indir = grub_malloc (blksz);\n      if (! indir)\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (inode->blocks.indir_block))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      blknr = grub_le_to_cpu32 (indir[fileblock - INDIRECT_BLOCKS]);\n      grub_free (indir);\n    }\n  /* Double indirect.  */\n  else if (fileblock < (grub_disk_addr_t)(INDIRECT_BLOCKS + blksz / 4) \\\n\t\t  * (grub_disk_addr_t)(blksz / 4 + 1))\n    {\n      unsigned int perblock = blksz / 4;\n      unsigned int rblock = fileblock - (INDIRECT_BLOCKS\n\t\t\t\t\t + blksz / 4);\n      grub_uint32_t *indir;\n\n      indir = grub_malloc (blksz);\n      if (! indir)\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (inode->blocks.double_indir_block))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (indir[rblock / perblock]))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      blknr = grub_le_to_cpu32 (indir[rblock % perblock]);\n            grub_free (indir);\n    }\n  /* triple indirect.  */\n  else\n    {\n      grub_error (GRUB_ERR_NOT_IMPLEMENTED_YET,\n\t\t  \"ext2fs doesn't support triple indirect blocks\");\n    }\n\n  return blknr;\n}",
        "func_after": "static grub_disk_addr_t\ngrub_ext2_read_block (grub_fshelp_node_t node, grub_disk_addr_t fileblock)\n{\n  struct grub_ext2_data *data = node->data;\n  struct grub_ext2_inode *inode = &node->inode;\n  int blknr = -1;\n  unsigned int blksz = EXT2_BLOCK_SIZE (data);\n  int log2_blksz = LOG2_EXT2_BLOCK_SIZE (data);\n\n  if (grub_le_to_cpu32(inode->flags) & EXT4_EXTENTS_FLAG)\n    {\n#ifndef _MSC_VER\n\t  char buf[EXT2_BLOCK_SIZE (data)];\n#else\n\t  char * buf = grub_malloc (EXT2_BLOCK_SIZE(data));\n#endif\n      struct grub_ext4_extent_header *leaf;\n      struct grub_ext4_extent *ext;\n      int i;\n\n      leaf = grub_ext4_find_leaf (data, buf,\n\t\t  (struct grub_ext4_extent_header *) inode->blocks.dir_blocks,\n\t\t  fileblock);\n      if (! leaf)\n        {\n          grub_error (GRUB_ERR_BAD_FS, \"invalid extent\");\n          return -1;\n        }\n\n      ext = (struct grub_ext4_extent *) (leaf + 1);\n      for (i = 0; i < grub_le_to_cpu16 (leaf->entries); i++)\n        {\n          if (fileblock < grub_le_to_cpu32 (ext[i].block))\n            break;\n        }\n\n      if (--i >= 0)\n        {\n          fileblock -= grub_le_to_cpu32 (ext[i].block);\n          if (fileblock >= grub_le_to_cpu16 (ext[i].len))\n            return 0;\n          else\n            {\n              grub_disk_addr_t start;\n\n              start = grub_le_to_cpu16 (ext[i].start_hi);\n              start = (start << 32) + grub_le_to_cpu32 (ext[i].start);\n\n              return fileblock + start;\n            }\n        }\n      else\n        {\n          grub_error (GRUB_ERR_BAD_FS, \"something wrong with extent\");\n          return -1;\n        }\n    }\n  /* Direct blocks.  */\n  if (fileblock < INDIRECT_BLOCKS) {\n    blknr = grub_le_to_cpu32 (inode->blocks.dir_blocks[fileblock]);\n  /* Indirect.  */\n  } else if (fileblock < INDIRECT_BLOCKS + blksz / 4)\n    {\n      grub_uint32_t *indir;\n\n      indir = grub_malloc (blksz);\n      if (! indir)\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (inode->blocks.indir_block))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      blknr = grub_le_to_cpu32 (indir[fileblock - INDIRECT_BLOCKS]);\n      grub_free (indir);\n    }\n  /* Double indirect.  */\n  else if (fileblock < (grub_disk_addr_t)(INDIRECT_BLOCKS + blksz / 4) \\\n\t\t  * (grub_disk_addr_t)(blksz / 4 + 1))\n    {\n      unsigned int perblock = blksz / 4;\n      unsigned int rblock = fileblock - (INDIRECT_BLOCKS\n\t\t\t\t\t + blksz / 4);\n      grub_uint32_t *indir;\n\n      indir = grub_malloc (blksz);\n      if (! indir)\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (inode->blocks.double_indir_block))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      if (grub_disk_read (data->disk,\n\t\t\t  ((grub_disk_addr_t)\n\t\t\t   grub_le_to_cpu32 (indir[rblock / perblock]))\n\t\t\t  << log2_blksz,\n\t\t\t  0, blksz, indir))\n\treturn grub_errno;\n\n      blknr = grub_le_to_cpu32 (indir[rblock % perblock]);\n            grub_free (indir);\n    }\n  /* triple indirect.  */\n  else\n    {\n      grub_error (GRUB_ERR_NOT_IMPLEMENTED_YET,\n\t\t  \"ext2fs doesn't support triple indirect blocks\");\n    }\n\n  return blknr;\n}",
        "description": "The `grub_memmove` function in the radare2 library, specifically within the GRUB module, is susceptible to a denial of service (DoS) condition resulting from a stack-based buffer underflow. This vulnerability can be exploited by remote attackers through the use of a specially crafted binary file. Additionally, there is a potential for unspecified other impacts due to a similar buffer underflow issue found in the `fs/ext2.c` file of GNU GRUB 2.02.",
        "commit": "A buffer overflow vulnerability was identified and fixed in the `r2_sbu_grub_memmove` function within the ext2 file system implementation."
    },
    {
        "cwe": "CWE-755",
        "func_name": "xen-project/map_grant_ref",
        "score": 0.8164376020431519,
        "func_before": "static void\nmap_grant_ref(\n    struct gnttab_map_grant_ref *op)\n{\n    struct domain *ld, *rd, *owner = NULL;\n    struct grant_table *lgt, *rgt;\n    grant_ref_t ref;\n    grant_handle_t handle;\n    mfn_t mfn;\n    struct page_info *pg = NULL;\n    int            rc = GNTST_okay;\n    unsigned int   cache_flags, clear_flags = 0, refcnt = 0, typecnt = 0;\n    bool           host_map_created = false;\n    struct active_grant_entry *act = NULL;\n    struct grant_mapping *mt;\n    grant_entry_header_t *shah;\n    uint16_t *status;\n    bool_t need_iommu;\n\n    ld = current->domain;\n\n    if ( unlikely((op->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad flags in grant map op: %x\\n\", op->flags);\n        op->status = GNTST_bad_gntref;\n        return;\n    }\n\n    if ( unlikely(paging_mode_external(ld) &&\n                  (op->flags & (GNTMAP_device_map|GNTMAP_application_map|\n                            GNTMAP_contains_pte))) )\n    {\n        gdprintk(XENLOG_INFO, \"No device mapping in HVM domain\\n\");\n        op->status = GNTST_general_error;\n        return;\n    }\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(op->dom)) == NULL) )\n    {\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", op->dom);\n        op->status = GNTST_bad_domain;\n        return;\n    }\n\n    rc = xsm_grant_mapref(XSM_HOOK, ld, rd, op->flags);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    lgt = ld->grant_table;\n    handle = get_maptrack_handle(lgt);\n    if ( unlikely(handle == INVALID_MAPTRACK_HANDLE) )\n    {\n        rcu_unlock_domain(rd);\n        gdprintk(XENLOG_INFO, \"Failed to obtain maptrack handle\\n\");\n        op->status = GNTST_no_device_space;\n        return;\n    }\n\n    rgt = rd->grant_table;\n    grant_read_lock(rgt);\n\n    /* Bounds check on the grant ref */\n    ref = op->ref;\n    if ( unlikely(ref >= nr_grant_entries(rgt)))\n        PIN_FAIL(unlock_out, GNTST_bad_gntref, \"Bad ref %#x for d%d\\n\",\n                 ref, rgt->domain->domain_id);\n\n    /* This call also ensures the above check cannot be passed speculatively */\n    shah = shared_entry_header(rgt, ref);\n    act = active_entry_acquire(rgt, ref);\n\n    /* Make sure we do not access memory speculatively */\n    status = evaluate_nospec(rgt->gt_version == 1) ? &shah->flags\n                                                 : &status_entry(rgt, ref);\n\n    /* If already pinned, check the active domid and avoid refcnt overflow. */\n    if ( act->pin &&\n         ((act->domid != ld->domain_id) ||\n          (act->pin & 0x80808080U) != 0 ||\n          (act->is_sub_page)) )\n        PIN_FAIL(act_release_out, GNTST_general_error,\n                 \"Bad domain (%d != %d), or risk of counter overflow %08x, or subpage %d\\n\",\n                 act->domid, ld->domain_id, act->pin, act->is_sub_page);\n\n    if ( !act->pin ||\n         (!(op->flags & GNTMAP_readonly) &&\n          !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask))) )\n    {\n        if ( (rc = _set_status(shah, status, rd, rgt->gt_version, act,\n                               op->flags & GNTMAP_readonly, 1,\n                               ld->domain_id) != GNTST_okay) )\n            goto act_release_out;\n\n        if ( !act->pin )\n        {\n            unsigned long gfn = evaluate_nospec(rgt->gt_version == 1) ?\n                                shared_entry_v1(rgt, ref).frame :\n                                shared_entry_v2(rgt, ref).full_page.frame;\n\n            rc = get_paged_frame(gfn, &mfn, &pg,\n                                 op->flags & GNTMAP_readonly, rd);\n            if ( rc != GNTST_okay )\n                goto unlock_out_clear;\n            act_set_gfn(act, _gfn(gfn));\n            act->domid = ld->domain_id;\n            act->mfn = mfn;\n            act->start = 0;\n            act->length = PAGE_SIZE;\n            act->is_sub_page = false;\n            act->trans_domain = rd;\n            act->trans_gref = ref;\n        }\n    }\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n    mfn = act->mfn;\n\n    cache_flags = (shah->flags & (GTF_PAT | GTF_PWT | GTF_PCD) );\n\n    active_entry_release(act);\n    grant_read_unlock(rgt);\n\n    /* pg may be set, with a refcount included, from get_paged_frame(). */\n    if ( !pg )\n    {\n        pg = mfn_valid(mfn) ? mfn_to_page(mfn) : NULL;\n        if ( pg )\n            owner = page_get_owner_and_reference(pg);\n    }\n    else\n        owner = page_get_owner(pg);\n\n    if ( owner )\n        refcnt++;\n\n    if ( !pg || (owner == dom_io) )\n    {\n        /* Only needed the reference to confirm dom_io ownership. */\n        if ( pg )\n        {\n            put_page(pg);\n            refcnt--;\n        }\n\n        if ( paging_mode_external(ld) )\n        {\n            gdprintk(XENLOG_WARNING, \"HVM guests can't grant map iomem\\n\");\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( !iomem_access_permitted(rd, mfn_x(mfn), mfn_x(mfn)) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Iomem mapping not permitted %#\"PRI_mfn\" (domain %d)\\n\",\n                     mfn_x(mfn), rd->domain_id);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags,\n                                           cache_flags);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else if ( owner == rd || (dom_cow && owner == dom_cow) )\n    {\n        if ( (op->flags & GNTMAP_device_map) && !(op->flags & GNTMAP_readonly) )\n        {\n            if ( (owner == dom_cow) ||\n                 !get_page_type(pg, PGT_writable_page) )\n                goto could_not_pin;\n            typecnt++;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            /*\n             * Only need to grab another reference if device_map claimed\n             * the other one.\n             */\n            if ( op->flags & GNTMAP_device_map )\n            {\n                if ( !get_page(pg, rd) )\n                    goto could_not_pin;\n                refcnt++;\n            }\n\n            if ( gnttab_host_mapping_get_page_type(op->flags & GNTMAP_readonly,\n                                                   ld, rd) )\n            {\n                if ( (owner == dom_cow) ||\n                     !get_page_type(pg, PGT_writable_page) )\n                    goto could_not_pin;\n                typecnt++;\n            }\n\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags, 0);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else\n    {\n    could_not_pin:\n        if ( !rd->is_dying )\n            gdprintk(XENLOG_WARNING, \"Could not pin grant frame %#\"PRI_mfn\"\\n\",\n                     mfn_x(mfn));\n        rc = GNTST_general_error;\n        goto undo_out;\n    }\n\n    need_iommu = gnttab_need_iommu_mapping(ld);\n    if ( need_iommu )\n    {\n        unsigned int kind;\n\n        double_gt_lock(lgt, rgt);\n\n        /*\n         * We're not translated, so we know that dfns and mfns are\n         * the same things, so the IOMMU entry is always 1-to-1.\n         */\n        kind = mapkind(lgt, rd, mfn);\n        if ( !(op->flags & GNTMAP_readonly) &&\n             !(kind & MAPKIND_WRITE) )\n            kind = IOMMUF_readable | IOMMUF_writable;\n        else if ( !kind )\n            kind = IOMMUF_readable;\n        else\n            kind = 0;\n        if ( kind && iommu_legacy_map(ld, _dfn(mfn_x(mfn)), mfn, 0, kind) )\n        {\n            double_gt_unlock(lgt, rgt);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_MAP, op->dom);\n\n    /*\n     * All maptrack entry users check mt->flags first before using the\n     * other fields so just ensure the flags field is stored last.\n     *\n     * However, if gnttab_need_iommu_mapping() then this would race\n     * with a concurrent mapkind() call (on an unmap, for example)\n     * and a lock is required.\n     */\n    mt = &maptrack_entry(lgt, handle);\n    mt->domid = op->dom;\n    mt->ref   = op->ref;\n    smp_wmb();\n    write_atomic(&mt->flags, op->flags);\n\n    if ( need_iommu )\n        double_gt_unlock(lgt, rgt);\n\n    op->dev_bus_addr = mfn_to_maddr(mfn);\n    op->handle       = handle;\n    op->status       = GNTST_okay;\n\n    rcu_unlock_domain(rd);\n    return;\n\n undo_out:\n    if ( host_map_created )\n    {\n        replace_grant_host_mapping(op->host_addr, mfn, 0, op->flags);\n        gnttab_flush_tlb(ld);\n    }\n\n    while ( typecnt-- )\n        put_page_type(pg);\n\n    while ( refcnt-- )\n        put_page(pg);\n\n    grant_read_lock(rgt);\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n unlock_out_clear:\n    if ( !(op->flags & GNTMAP_readonly) &&\n         !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask)) )\n        clear_flags |= GTF_writing;\n\n    if ( !act->pin )\n        clear_flags |= GTF_reading;\n\n    if ( clear_flags )\n        gnttab_clear_flags(rd, clear_flags, status);\n\n act_release_out:\n    active_entry_release(act);\n\n unlock_out:\n    grant_read_unlock(rgt);\n    op->status = rc;\n    put_maptrack_handle(lgt, handle);\n    rcu_unlock_domain(rd);\n}",
        "func_after": "static void\nmap_grant_ref(\n    struct gnttab_map_grant_ref *op)\n{\n    struct domain *ld, *rd, *owner = NULL;\n    struct grant_table *lgt, *rgt;\n    grant_ref_t ref;\n    grant_handle_t handle;\n    mfn_t mfn;\n    struct page_info *pg = NULL;\n    int            rc = GNTST_okay;\n    unsigned int   cache_flags, clear_flags = 0, refcnt = 0, typecnt = 0;\n    bool           host_map_created = false;\n    struct active_grant_entry *act = NULL;\n    struct grant_mapping *mt;\n    grant_entry_header_t *shah;\n    uint16_t *status;\n    bool_t need_iommu;\n\n    ld = current->domain;\n\n    if ( unlikely((op->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad flags in grant map op: %x\\n\", op->flags);\n        op->status = GNTST_bad_gntref;\n        return;\n    }\n\n    if ( unlikely(paging_mode_external(ld) &&\n                  (op->flags & (GNTMAP_device_map|GNTMAP_application_map|\n                            GNTMAP_contains_pte))) )\n    {\n        gdprintk(XENLOG_INFO, \"No device mapping in HVM domain\\n\");\n        op->status = GNTST_general_error;\n        return;\n    }\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(op->dom)) == NULL) )\n    {\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", op->dom);\n        op->status = GNTST_bad_domain;\n        return;\n    }\n\n    rc = xsm_grant_mapref(XSM_HOOK, ld, rd, op->flags);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    lgt = ld->grant_table;\n    handle = get_maptrack_handle(lgt);\n    if ( unlikely(handle == INVALID_MAPTRACK_HANDLE) )\n    {\n        rcu_unlock_domain(rd);\n        gdprintk(XENLOG_INFO, \"Failed to obtain maptrack handle\\n\");\n        op->status = GNTST_no_device_space;\n        return;\n    }\n\n    rgt = rd->grant_table;\n    grant_read_lock(rgt);\n\n    /* Bounds check on the grant ref */\n    ref = op->ref;\n    if ( unlikely(ref >= nr_grant_entries(rgt)))\n        PIN_FAIL(unlock_out, GNTST_bad_gntref, \"Bad ref %#x for d%d\\n\",\n                 ref, rgt->domain->domain_id);\n\n    /* This call also ensures the above check cannot be passed speculatively */\n    shah = shared_entry_header(rgt, ref);\n    act = active_entry_acquire(rgt, ref);\n\n    /* Make sure we do not access memory speculatively */\n    status = evaluate_nospec(rgt->gt_version == 1) ? &shah->flags\n                                                 : &status_entry(rgt, ref);\n\n    /* If already pinned, check the active domid and avoid refcnt overflow. */\n    if ( act->pin &&\n         ((act->domid != ld->domain_id) ||\n          (act->pin & 0x80808080U) != 0 ||\n          (act->is_sub_page)) )\n        PIN_FAIL(act_release_out, GNTST_general_error,\n                 \"Bad domain (%d != %d), or risk of counter overflow %08x, or subpage %d\\n\",\n                 act->domid, ld->domain_id, act->pin, act->is_sub_page);\n\n    if ( !act->pin ||\n         (!(op->flags & GNTMAP_readonly) &&\n          !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask))) )\n    {\n        if ( (rc = _set_status(shah, status, rd, rgt->gt_version, act,\n                               op->flags & GNTMAP_readonly, 1,\n                               ld->domain_id)) != GNTST_okay )\n            goto act_release_out;\n\n        if ( !act->pin )\n        {\n            unsigned long gfn = evaluate_nospec(rgt->gt_version == 1) ?\n                                shared_entry_v1(rgt, ref).frame :\n                                shared_entry_v2(rgt, ref).full_page.frame;\n\n            rc = get_paged_frame(gfn, &mfn, &pg,\n                                 op->flags & GNTMAP_readonly, rd);\n            if ( rc != GNTST_okay )\n                goto unlock_out_clear;\n            act_set_gfn(act, _gfn(gfn));\n            act->domid = ld->domain_id;\n            act->mfn = mfn;\n            act->start = 0;\n            act->length = PAGE_SIZE;\n            act->is_sub_page = false;\n            act->trans_domain = rd;\n            act->trans_gref = ref;\n        }\n    }\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n    mfn = act->mfn;\n\n    cache_flags = (shah->flags & (GTF_PAT | GTF_PWT | GTF_PCD) );\n\n    active_entry_release(act);\n    grant_read_unlock(rgt);\n\n    /* pg may be set, with a refcount included, from get_paged_frame(). */\n    if ( !pg )\n    {\n        pg = mfn_valid(mfn) ? mfn_to_page(mfn) : NULL;\n        if ( pg )\n            owner = page_get_owner_and_reference(pg);\n    }\n    else\n        owner = page_get_owner(pg);\n\n    if ( owner )\n        refcnt++;\n\n    if ( !pg || (owner == dom_io) )\n    {\n        /* Only needed the reference to confirm dom_io ownership. */\n        if ( pg )\n        {\n            put_page(pg);\n            refcnt--;\n        }\n\n        if ( paging_mode_external(ld) )\n        {\n            gdprintk(XENLOG_WARNING, \"HVM guests can't grant map iomem\\n\");\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( !iomem_access_permitted(rd, mfn_x(mfn), mfn_x(mfn)) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Iomem mapping not permitted %#\"PRI_mfn\" (domain %d)\\n\",\n                     mfn_x(mfn), rd->domain_id);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags,\n                                           cache_flags);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else if ( owner == rd || (dom_cow && owner == dom_cow) )\n    {\n        if ( (op->flags & GNTMAP_device_map) && !(op->flags & GNTMAP_readonly) )\n        {\n            if ( (owner == dom_cow) ||\n                 !get_page_type(pg, PGT_writable_page) )\n                goto could_not_pin;\n            typecnt++;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            /*\n             * Only need to grab another reference if device_map claimed\n             * the other one.\n             */\n            if ( op->flags & GNTMAP_device_map )\n            {\n                if ( !get_page(pg, rd) )\n                    goto could_not_pin;\n                refcnt++;\n            }\n\n            if ( gnttab_host_mapping_get_page_type(op->flags & GNTMAP_readonly,\n                                                   ld, rd) )\n            {\n                if ( (owner == dom_cow) ||\n                     !get_page_type(pg, PGT_writable_page) )\n                    goto could_not_pin;\n                typecnt++;\n            }\n\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags, 0);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else\n    {\n    could_not_pin:\n        if ( !rd->is_dying )\n            gdprintk(XENLOG_WARNING, \"Could not pin grant frame %#\"PRI_mfn\"\\n\",\n                     mfn_x(mfn));\n        rc = GNTST_general_error;\n        goto undo_out;\n    }\n\n    need_iommu = gnttab_need_iommu_mapping(ld);\n    if ( need_iommu )\n    {\n        unsigned int kind;\n\n        double_gt_lock(lgt, rgt);\n\n        /*\n         * We're not translated, so we know that dfns and mfns are\n         * the same things, so the IOMMU entry is always 1-to-1.\n         */\n        kind = mapkind(lgt, rd, mfn);\n        if ( !(op->flags & GNTMAP_readonly) &&\n             !(kind & MAPKIND_WRITE) )\n            kind = IOMMUF_readable | IOMMUF_writable;\n        else if ( !kind )\n            kind = IOMMUF_readable;\n        else\n            kind = 0;\n        if ( kind && iommu_legacy_map(ld, _dfn(mfn_x(mfn)), mfn, 0, kind) )\n        {\n            double_gt_unlock(lgt, rgt);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_MAP, op->dom);\n\n    /*\n     * All maptrack entry users check mt->flags first before using the\n     * other fields so just ensure the flags field is stored last.\n     *\n     * However, if gnttab_need_iommu_mapping() then this would race\n     * with a concurrent mapkind() call (on an unmap, for example)\n     * and a lock is required.\n     */\n    mt = &maptrack_entry(lgt, handle);\n    mt->domid = op->dom;\n    mt->ref   = op->ref;\n    smp_wmb();\n    write_atomic(&mt->flags, op->flags);\n\n    if ( need_iommu )\n        double_gt_unlock(lgt, rgt);\n\n    op->dev_bus_addr = mfn_to_maddr(mfn);\n    op->handle       = handle;\n    op->status       = GNTST_okay;\n\n    rcu_unlock_domain(rd);\n    return;\n\n undo_out:\n    if ( host_map_created )\n    {\n        replace_grant_host_mapping(op->host_addr, mfn, 0, op->flags);\n        gnttab_flush_tlb(ld);\n    }\n\n    while ( typecnt-- )\n        put_page_type(pg);\n\n    while ( refcnt-- )\n        put_page(pg);\n\n    grant_read_lock(rgt);\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n unlock_out_clear:\n    if ( !(op->flags & GNTMAP_readonly) &&\n         !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask)) )\n        clear_flags |= GTF_writing;\n\n    if ( !act->pin )\n        clear_flags |= GTF_reading;\n\n    if ( clear_flags )\n        gnttab_clear_flags(rd, clear_flags, status);\n\n act_release_out:\n    active_entry_release(act);\n\n unlock_out:\n    grant_read_unlock(rgt);\n    op->status = rc;\n    put_maptrack_handle(lgt, handle);\n    rcu_unlock_domain(rd);\n}",
        "description": "An issue was discovered in Xen versions up to 4.13.x, where guest operating system users could trigger a denial of service due to an improper error handling mechanism in the GNTTABOP_map_grant operation. Grant table operations are supposed to return 0 for success and a negative number for errors; however, a coding mistake resulted in one error path returning 1 instead of a negative value. The grant table code in Linux interprets this as a successful operation and continues with incorrectly initialized state. A malicious or buggy guest could manipulate its grant table to exploit this flaw, causing a crash in the Linux-based dom0 or backend domain when a backend domain attempts to map a grant.",
        "commit": "A function within the Xen hypervisor, specifically related to mapping grant references, had its error handling logic inadvertently altered. This change caused the function to return an incorrect status code when a critical operation failed, leading to unexpected behavior in the Linux kernel's network and block device backends. This issue could result in system crashes due to improper handling of guest states."
    },
    {
        "cwe": "CWE-346",
        "func_name": "hotplug/udev_monitor_receive_device",
        "score": 0.8086540102958679,
        "func_before": "struct udev_device *udev_monitor_receive_device(struct udev_monitor *udev_monitor)\n{\n\tstruct udev_device *udev_device;\n\tstruct msghdr smsg;\n\tstruct iovec iov;\n\tchar cred_msg[CMSG_SPACE(sizeof(struct ucred))];\n\tchar buf[4096];\n\tsize_t bufpos;\n\tint devpath_set = 0;\n\tint subsystem_set = 0;\n\tint action_set = 0;\n\tint maj = 0;\n\tint min = 0;\n\n\tif (udev_monitor == NULL)\n\t\treturn NULL;\n\tmemset(buf, 0x00, sizeof(buf));\n\tiov.iov_base = &buf;\n\tiov.iov_len = sizeof(buf);\n\tmemset (&smsg, 0x00, sizeof(struct msghdr));\n\tsmsg.msg_iov = &iov;\n\tsmsg.msg_iovlen = 1;\n\tsmsg.msg_control = cred_msg;\n\tsmsg.msg_controllen = sizeof(cred_msg);\n\n\tif (recvmsg(udev_monitor->sock, &smsg, 0) < 0) {\n\t\tif (errno != EINTR)\n\t\t\tinfo(udev_monitor->udev, \"unable to receive message\");\n\t\treturn NULL;\n\t}\n\n\tif (udev_monitor->sun.sun_family != 0) {\n\t\tstruct cmsghdr *cmsg = CMSG_FIRSTHDR(&smsg);\n\t\tstruct ucred *cred = (struct ucred *)CMSG_DATA (cmsg);\n\n\t\tif (cmsg == NULL || cmsg->cmsg_type != SCM_CREDENTIALS) {\n\t\t\tinfo(udev_monitor->udev, \"no sender credentials received, message ignored\");\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (cred->uid != 0) {\n\t\t\tinfo(udev_monitor->udev, \"sender uid=%d, message ignored\", cred->uid);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t/* skip header */\n\tbufpos = strlen(buf) + 1;\n\tif (bufpos < sizeof(\"a@/d\") || bufpos >= sizeof(buf)) {\n\t\tinfo(udev_monitor->udev, \"invalid message length\");\n\t\treturn NULL;\n\t}\n\n\t/* check message header */\n\tif (strstr(buf, \"@/\") == NULL) {\n\t\tinfo(udev_monitor->udev, \"unrecognized message header\");\n\t\treturn NULL;\n\t}\n\n\tudev_device = device_new(udev_monitor->udev);\n\tif (udev_device == NULL) {\n\t\treturn NULL;\n\t}\n\n\twhile (bufpos < sizeof(buf)) {\n\t\tchar *key;\n\t\tsize_t keylen;\n\n\t\tkey = &buf[bufpos];\n\t\tkeylen = strlen(key);\n\t\tif (keylen == 0)\n\t\t\tbreak;\n\t\tbufpos += keylen + 1;\n\n\t\tif (strncmp(key, \"DEVPATH=\", 8) == 0) {\n\t\t\tchar path[UTIL_PATH_SIZE];\n\n\t\t\tutil_strlcpy(path, udev_get_sys_path(udev_monitor->udev), sizeof(path));\n\t\t\tutil_strlcat(path, &key[8], sizeof(path));\n\t\t\tudev_device_set_syspath(udev_device, path);\n\t\t\tdevpath_set = 1;\n\t\t} else if (strncmp(key, \"SUBSYSTEM=\", 10) == 0) {\n\t\t\tudev_device_set_subsystem(udev_device, &key[10]);\n\t\t\tsubsystem_set = 1;\n\t\t} else if (strncmp(key, \"DEVTYPE=\", 8) == 0) {\n\t\t\tudev_device_set_devtype(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVNAME=\", 8) == 0) {\n\t\t\tudev_device_set_devnode(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVLINKS=\", 9) == 0) {\n\t\t\tchar devlinks[UTIL_PATH_SIZE];\n\t\t\tchar *slink;\n\t\t\tchar *next;\n\n\t\t\tutil_strlcpy(devlinks, &key[9], sizeof(devlinks));\n\t\t\tslink = devlinks;\n\t\t\tnext = strchr(slink, ' ');\n\t\t\twhile (next != NULL) {\n\t\t\t\tnext[0] = '\\0';\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t\t\tslink = &next[1];\n\t\t\t\tnext = strchr(slink, ' ');\n\t\t\t}\n\t\t\tif (slink[0] != '\\0')\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t} else if (strncmp(key, \"DRIVER=\", 7) == 0) {\n\t\t\tudev_device_set_driver(udev_device, &key[7]);\n\t\t} else if (strncmp(key, \"ACTION=\", 7) == 0) {\n\t\t\tudev_device_set_action(udev_device, &key[7]);\n\t\t\taction_set = 1;\n\t\t} else if (strncmp(key, \"MAJOR=\", 6) == 0) {\n\t\t\tmaj = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"MINOR=\", 6) == 0) {\n\t\t\tmin = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"DEVPATH_OLD=\", 12) == 0) {\n\t\t\tudev_device_set_devpath_old(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"PHYSDEVPATH=\", 12) == 0) {\n\t\t\tudev_device_set_physdevpath(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"SEQNUM=\", 7) == 0) {\n\t\t\tudev_device_set_seqnum(udev_device, strtoull(&key[7], NULL, 10));\n\t\t} else if (strncmp(key, \"TIMEOUT=\", 8) == 0) {\n\t\t\tudev_device_set_timeout(udev_device, strtoull(&key[8], NULL, 10));\n\t\t} else if (strncmp(key, \"PHYSDEV\", 7) == 0) {\n\t\t\t/* skip deprecated values */\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tudev_device_add_property_from_string(udev_device, key);\n\t\t}\n\t}\n\tif (!devpath_set || !subsystem_set || !action_set) {\n\t\tinfo(udev_monitor->udev, \"missing values, skip\\n\");\n\t\tudev_device_unref(udev_device);\n\t\treturn NULL;\n\t}\n\tif (maj > 0)\n\t\tudev_device_set_devnum(udev_device, makedev(maj, min));\n\tudev_device_set_info_loaded(udev_device);\n\treturn udev_device;\n}",
        "func_after": "struct udev_device *udev_monitor_receive_device(struct udev_monitor *udev_monitor)\n{\n\tstruct udev_device *udev_device;\n\tstruct msghdr smsg;\n\tstruct iovec iov;\n\tchar cred_msg[CMSG_SPACE(sizeof(struct ucred))];\n\tstruct cmsghdr *cmsg;\n\tstruct ucred *cred;\n\tchar buf[4096];\n\tsize_t bufpos;\n\tint devpath_set = 0;\n\tint subsystem_set = 0;\n\tint action_set = 0;\n\tint maj = 0;\n\tint min = 0;\n\n\tif (udev_monitor == NULL)\n\t\treturn NULL;\n\tmemset(buf, 0x00, sizeof(buf));\n\tiov.iov_base = &buf;\n\tiov.iov_len = sizeof(buf);\n\tmemset (&smsg, 0x00, sizeof(struct msghdr));\n\tsmsg.msg_iov = &iov;\n\tsmsg.msg_iovlen = 1;\n\tsmsg.msg_control = cred_msg;\n\tsmsg.msg_controllen = sizeof(cred_msg);\n\n\tif (recvmsg(udev_monitor->sock, &smsg, 0) < 0) {\n\t\tif (errno != EINTR)\n\t\t\tinfo(udev_monitor->udev, \"unable to receive message\");\n\t\treturn NULL;\n\t}\n\n\tcmsg = CMSG_FIRSTHDR(&smsg);\n\tif (cmsg == NULL || cmsg->cmsg_type != SCM_CREDENTIALS) {\n\t\tinfo(udev_monitor->udev, \"no sender credentials received, message ignored\");\n\t\treturn NULL;\n\t}\n\n\tcred = (struct ucred *)CMSG_DATA(cmsg);\n\tif (cred->uid != 0) {\n\t\tinfo(udev_monitor->udev, \"sender uid=%d, message ignored\", cred->uid);\n\t\treturn NULL;\n\t}\n\n\t/* skip header */\n\tbufpos = strlen(buf) + 1;\n\tif (bufpos < sizeof(\"a@/d\") || bufpos >= sizeof(buf)) {\n\t\tinfo(udev_monitor->udev, \"invalid message length\");\n\t\treturn NULL;\n\t}\n\n\t/* check message header */\n\tif (strstr(buf, \"@/\") == NULL) {\n\t\tinfo(udev_monitor->udev, \"unrecognized message header\");\n\t\treturn NULL;\n\t}\n\n\tudev_device = device_new(udev_monitor->udev);\n\tif (udev_device == NULL) {\n\t\treturn NULL;\n\t}\n\n\twhile (bufpos < sizeof(buf)) {\n\t\tchar *key;\n\t\tsize_t keylen;\n\n\t\tkey = &buf[bufpos];\n\t\tkeylen = strlen(key);\n\t\tif (keylen == 0)\n\t\t\tbreak;\n\t\tbufpos += keylen + 1;\n\n\t\tif (strncmp(key, \"DEVPATH=\", 8) == 0) {\n\t\t\tchar path[UTIL_PATH_SIZE];\n\n\t\t\tutil_strlcpy(path, udev_get_sys_path(udev_monitor->udev), sizeof(path));\n\t\t\tutil_strlcat(path, &key[8], sizeof(path));\n\t\t\tudev_device_set_syspath(udev_device, path);\n\t\t\tdevpath_set = 1;\n\t\t} else if (strncmp(key, \"SUBSYSTEM=\", 10) == 0) {\n\t\t\tudev_device_set_subsystem(udev_device, &key[10]);\n\t\t\tsubsystem_set = 1;\n\t\t} else if (strncmp(key, \"DEVTYPE=\", 8) == 0) {\n\t\t\tudev_device_set_devtype(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVNAME=\", 8) == 0) {\n\t\t\tudev_device_set_devnode(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVLINKS=\", 9) == 0) {\n\t\t\tchar devlinks[UTIL_PATH_SIZE];\n\t\t\tchar *slink;\n\t\t\tchar *next;\n\n\t\t\tutil_strlcpy(devlinks, &key[9], sizeof(devlinks));\n\t\t\tslink = devlinks;\n\t\t\tnext = strchr(slink, ' ');\n\t\t\twhile (next != NULL) {\n\t\t\t\tnext[0] = '\\0';\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t\t\tslink = &next[1];\n\t\t\t\tnext = strchr(slink, ' ');\n\t\t\t}\n\t\t\tif (slink[0] != '\\0')\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t} else if (strncmp(key, \"DRIVER=\", 7) == 0) {\n\t\t\tudev_device_set_driver(udev_device, &key[7]);\n\t\t} else if (strncmp(key, \"ACTION=\", 7) == 0) {\n\t\t\tudev_device_set_action(udev_device, &key[7]);\n\t\t\taction_set = 1;\n\t\t} else if (strncmp(key, \"MAJOR=\", 6) == 0) {\n\t\t\tmaj = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"MINOR=\", 6) == 0) {\n\t\t\tmin = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"DEVPATH_OLD=\", 12) == 0) {\n\t\t\tudev_device_set_devpath_old(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"PHYSDEVPATH=\", 12) == 0) {\n\t\t\tudev_device_set_physdevpath(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"SEQNUM=\", 7) == 0) {\n\t\t\tudev_device_set_seqnum(udev_device, strtoull(&key[7], NULL, 10));\n\t\t} else if (strncmp(key, \"TIMEOUT=\", 8) == 0) {\n\t\t\tudev_device_set_timeout(udev_device, strtoull(&key[8], NULL, 10));\n\t\t} else if (strncmp(key, \"PHYSDEV\", 7) == 0) {\n\t\t\t/* skip deprecated values */\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tudev_device_add_property_from_string(udev_device, key);\n\t\t}\n\t}\n\tif (!devpath_set || !subsystem_set || !action_set) {\n\t\tinfo(udev_monitor->udev, \"missing values, skip\\n\");\n\t\tudev_device_unref(udev_device);\n\t\treturn NULL;\n\t}\n\tif (maj > 0)\n\t\tudev_device_set_devnum(udev_device, makedev(maj, min));\n\tudev_device_set_info_loaded(udev_device);\n\treturn udev_device;\n}",
        "description": "udev versions prior to 1.4.1 do not validate whether a NETLINK message originates from kernel space, enabling local users to escalate privileges by transmitting a NETLINK message from user space.",
        "commit": "To enhance protection, the system should disregard any unicast messages received on the netlink socket and any multicast messages on the kernel group that do not originate from the kernel."
    }
]