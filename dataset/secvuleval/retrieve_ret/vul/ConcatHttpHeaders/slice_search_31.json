[
    {
        "cwe": "CWE-254",
        "func_name": "android/impeg2d_dec_d_slice",
        "score": 0.7678250670433044,
        "func_before": "IMPEG2D_ERROR_CODES_T impeg2d_dec_d_slice(dec_state_t *ps_dec)\n{\n    UWORD32 i;\n    yuv_buf_t *ps_cur_frm_buf  = &ps_dec->s_cur_frm_buf;\n\n    stream_t   *ps_stream       = &ps_dec->s_bit_stream;\n    UWORD8   *pu1_vld_buf;\n\n    WORD16 i2_dc_diff;\n    UWORD32 u4_frame_width = ps_dec->u2_frame_width;\n    UWORD32 u4_frm_offset = 0;\n    if(ps_dec->u2_picture_structure != FRAME_PICTURE)\n    {\n        u4_frame_width <<= 1;\n        if(ps_dec->u2_picture_structure == BOTTOM_FIELD)\n        {\n            u4_frm_offset = ps_dec->u2_frame_width;\n        }\n    }\n\n    do\n    {\n\n        UWORD32 u4_x_offset, u4_y_offset;\n        UWORD32 u4_blk_pos;\n        WORD16 i2_dc_val;\n\n        UWORD32 u4_dst_x_offset     = u4_frm_offset + (ps_dec->u2_mb_x << 4);\n        UWORD32 u4_dst_y_offset     = (ps_dec->u2_mb_y << 4) * u4_frame_width;\n        UWORD8 *pu1_vld_buf8        = ps_cur_frm_buf->pu1_y + u4_dst_x_offset + u4_dst_y_offset;\n        UWORD32 u4_dst_wd           = u4_frame_width;\n        /*------------------------------------------------------------------*/\n        /* Discard the Macroblock stuffing in case of MPEG-1 stream         */\n        /*------------------------------------------------------------------*/\n        while(impeg2d_bit_stream_nxt(ps_stream,MB_STUFFING_CODE_LEN) == MB_STUFFING_CODE)\n            impeg2d_bit_stream_flush(ps_stream,MB_STUFFING_CODE_LEN);\n\n        /*------------------------------------------------------------------*/\n        /* Flush 2 bits from bitstream [MB_Type and MacroBlockAddrIncrement]*/\n        /*------------------------------------------------------------------*/\n        impeg2d_bit_stream_flush(ps_stream,1);\n\n        if(impeg2d_bit_stream_get(ps_stream, 1) != 0x01)\n        {\n            /* Ignore and continue decoding. */\n        }\n\n        /* Process LUMA blocks of the MB */\n        for(i = 0; i < NUM_LUMA_BLKS; ++i)\n        {\n\n            u4_x_offset    = gai2_impeg2_blk_x_off[i];\n            u4_y_offset    = gai2_impeg2_blk_y_off_frm[i] ;\n            u4_blk_pos     = (u4_y_offset * u4_dst_wd) + u4_x_offset;\n            pu1_vld_buf     = pu1_vld_buf8 + u4_blk_pos;\n\n            i2_dc_diff = impeg2d_get_luma_dc_diff(ps_stream);\n            i2_dc_val = ps_dec->u2_def_dc_pred[Y_LUMA] + i2_dc_diff;\n            ps_dec->u2_def_dc_pred[Y_LUMA] = i2_dc_val;\n            i2_dc_val = CLIP_U8(i2_dc_val);\n\n            ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n        }\n\n\n\n        /* Process U block of the MB */\n\n        u4_dst_x_offset                >>= 1;\n        u4_dst_y_offset                >>= 2;\n        u4_dst_wd                      >>= 1;\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_u + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[U_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[U_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n\n        /* Process V block of the MB */\n\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_v + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[V_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[V_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n        /* Common MB processing Steps */\n\n\n        ps_dec->u2_num_mbs_left--;\n        ps_dec->u2_mb_x++;\n\n        if(ps_dec->s_bit_stream.u4_offset > ps_dec->s_bit_stream.u4_max_offset)\n        {\n            return IMPEG2D_BITSTREAM_BUFF_EXCEEDED_ERR;\n        }\n        else if (ps_dec->u2_mb_x == ps_dec->u2_num_horiz_mb)\n        {\n            ps_dec->u2_mb_x = 0;\n            ps_dec->u2_mb_y++;\n\n        }\n\n        /* Flush end of macro block */\n        impeg2d_bit_stream_flush(ps_stream,1);\n    }\n    while(ps_dec->u2_num_mbs_left != 0 && impeg2d_bit_stream_nxt(&ps_dec->s_bit_stream,23) != 0x0);\n    return (IMPEG2D_ERROR_CODES_T)IVD_ERROR_NONE;\n}",
        "func_after": "IMPEG2D_ERROR_CODES_T impeg2d_dec_d_slice(dec_state_t *ps_dec)\n{\n    UWORD32 i;\n    yuv_buf_t *ps_cur_frm_buf  = &ps_dec->s_cur_frm_buf;\n\n    stream_t   *ps_stream       = &ps_dec->s_bit_stream;\n    UWORD8   *pu1_vld_buf;\n\n    WORD16 i2_dc_diff;\n    UWORD32 u4_frame_width = ps_dec->u2_frame_width;\n    UWORD32 u4_frm_offset = 0;\n    if(ps_dec->u2_picture_structure != FRAME_PICTURE)\n    {\n        u4_frame_width <<= 1;\n        if(ps_dec->u2_picture_structure == BOTTOM_FIELD)\n        {\n            u4_frm_offset = ps_dec->u2_frame_width;\n        }\n    }\n\n    do\n    {\n\n        UWORD32 u4_x_offset, u4_y_offset;\n        UWORD32 u4_blk_pos;\n        WORD16 i2_dc_val;\n\n        UWORD32 u4_dst_x_offset     = u4_frm_offset + (ps_dec->u2_mb_x << 4);\n        UWORD32 u4_dst_y_offset     = (ps_dec->u2_mb_y << 4) * u4_frame_width;\n        UWORD8 *pu1_vld_buf8        = ps_cur_frm_buf->pu1_y + u4_dst_x_offset + u4_dst_y_offset;\n        UWORD32 u4_dst_wd           = u4_frame_width;\n        /*------------------------------------------------------------------*/\n        /* Discard the Macroblock stuffing in case of MPEG-1 stream         */\n        /*------------------------------------------------------------------*/\n        while(impeg2d_bit_stream_nxt(ps_stream,MB_STUFFING_CODE_LEN) == MB_STUFFING_CODE &&\n                ps_stream->u4_offset < ps_stream->u4_max_offset)\n            impeg2d_bit_stream_flush(ps_stream,MB_STUFFING_CODE_LEN);\n\n        /*------------------------------------------------------------------*/\n        /* Flush 2 bits from bitstream [MB_Type and MacroBlockAddrIncrement]*/\n        /*------------------------------------------------------------------*/\n        impeg2d_bit_stream_flush(ps_stream,1);\n\n        if(impeg2d_bit_stream_get(ps_stream, 1) != 0x01)\n        {\n            /* Ignore and continue decoding. */\n        }\n\n        /* Process LUMA blocks of the MB */\n        for(i = 0; i < NUM_LUMA_BLKS; ++i)\n        {\n\n            u4_x_offset    = gai2_impeg2_blk_x_off[i];\n            u4_y_offset    = gai2_impeg2_blk_y_off_frm[i] ;\n            u4_blk_pos     = (u4_y_offset * u4_dst_wd) + u4_x_offset;\n            pu1_vld_buf     = pu1_vld_buf8 + u4_blk_pos;\n\n            i2_dc_diff = impeg2d_get_luma_dc_diff(ps_stream);\n            i2_dc_val = ps_dec->u2_def_dc_pred[Y_LUMA] + i2_dc_diff;\n            ps_dec->u2_def_dc_pred[Y_LUMA] = i2_dc_val;\n            i2_dc_val = CLIP_U8(i2_dc_val);\n\n            ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n        }\n\n\n\n        /* Process U block of the MB */\n\n        u4_dst_x_offset                >>= 1;\n        u4_dst_y_offset                >>= 2;\n        u4_dst_wd                      >>= 1;\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_u + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[U_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[U_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n\n        /* Process V block of the MB */\n\n        pu1_vld_buf                     = ps_cur_frm_buf->pu1_v + u4_dst_x_offset + u4_dst_y_offset;\n        i2_dc_diff                     = impeg2d_get_chroma_dc_diff(ps_stream);\n        i2_dc_val                      = ps_dec->u2_def_dc_pred[V_CHROMA] + i2_dc_diff;\n        ps_dec->u2_def_dc_pred[V_CHROMA]    = i2_dc_val;\n        i2_dc_val = CLIP_U8(i2_dc_val);\n        ps_dec->pf_memset_8bit_8x8_block(pu1_vld_buf, i2_dc_val, u4_dst_wd);\n\n        /* Common MB processing Steps */\n\n\n        ps_dec->u2_num_mbs_left--;\n        ps_dec->u2_mb_x++;\n\n        if(ps_dec->s_bit_stream.u4_offset > ps_dec->s_bit_stream.u4_max_offset)\n        {\n            return IMPEG2D_BITSTREAM_BUFF_EXCEEDED_ERR;\n        }\n        else if (ps_dec->u2_mb_x == ps_dec->u2_num_horiz_mb)\n        {\n            ps_dec->u2_mb_x = 0;\n            ps_dec->u2_mb_y++;\n\n        }\n\n        /* Flush end of macro block */\n        impeg2d_bit_stream_flush(ps_stream,1);\n    }\n    while(ps_dec->u2_num_mbs_left != 0 && impeg2d_bit_stream_nxt(&ps_dec->s_bit_stream,23) != 0x0);\n    return (IMPEG2D_ERROR_CODES_T)IVD_ERROR_NONE;\n}",
        "description": "libstagefright in Android versions prior to March 1, 2016, contains a vulnerability that allows attackers to gain unauthorized access to sensitive information by manipulating Bitstream data. This flaw enables attackers to potentially bypass an unspecified security mechanism, such as Signature or SignatureOrSystem access, through crafted input.",
        "commit": "Fixed bit stream access to ensure that reads do not exceed the allocated size."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/hugetlb_mcopy_atomic_pte",
        "score": 0.7811921238899231,
        "func_before": "int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pte_t *dst_pte,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_huge_page(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(page))\n\t\t\tgoto out;\n\n\t\tret = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *) src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), false);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -EFAULT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\tset_page_huge_active(page);\n\n\t/*\n\t * If shared, add to page cache\n\t */\n\tif (vm_shared) {\n\t\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;\n\t\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\n\t\tret = huge_add_to_page_cache(page, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t}\n\n\tptl = huge_pte_lockptr(h, dst_mm, dst_pte);\n\tspin_lock(ptl);\n\n\tret = -EEXIST;\n\tif (!huge_pte_none(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (vm_shared) {\n\t\tpage_dup_rmap(page, true);\n\t} else {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t}\n\n\t_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,\n\t\t\t\t\tdst_vma->vm_flags & VM_WRITE);\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\nout_release_nounlock:\n\tput_page(page);\n\tgoto out;\n}",
        "func_after": "int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,\n\t\t\t    pte_t *dst_pte,\n\t\t\t    struct vm_area_struct *dst_vma,\n\t\t\t    unsigned long dst_addr,\n\t\t\t    unsigned long src_addr,\n\t\t\t    struct page **pagep)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tunsigned long size;\n\tint vm_shared = dst_vma->vm_flags & VM_SHARED;\n\tstruct hstate *h = hstate_vma(dst_vma);\n\tpte_t _dst_pte;\n\tspinlock_t *ptl;\n\tint ret;\n\tstruct page *page;\n\n\tif (!*pagep) {\n\t\tret = -ENOMEM;\n\t\tpage = alloc_huge_page(dst_vma, dst_addr, 0);\n\t\tif (IS_ERR(page))\n\t\t\tgoto out;\n\n\t\tret = copy_huge_page_from_user(page,\n\t\t\t\t\t\t(const void __user *) src_addr,\n\t\t\t\t\t\tpages_per_huge_page(h), false);\n\n\t\t/* fallback to copy_from_user outside mmap_sem */\n\t\tif (unlikely(ret)) {\n\t\t\tret = -EFAULT;\n\t\t\t*pagep = page;\n\t\t\t/* don't free the page */\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpage = *pagep;\n\t\t*pagep = NULL;\n\t}\n\n\t/*\n\t * The memory barrier inside __SetPageUptodate makes sure that\n\t * preceding stores to the page contents become visible before\n\t * the set_pte_at() write.\n\t */\n\t__SetPageUptodate(page);\n\tset_page_huge_active(page);\n\n\tmapping = dst_vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, dst_vma, dst_addr);\n\n\t/*\n\t * If shared, add to page cache\n\t */\n\tif (vm_shared) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tret = -EFAULT;\n\t\tif (idx >= size)\n\t\t\tgoto out_release_nounlock;\n\n\t\t/*\n\t\t * Serialization between remove_inode_hugepages() and\n\t\t * huge_add_to_page_cache() below happens through the\n\t\t * hugetlb_fault_mutex_table that here must be hold by\n\t\t * the caller.\n\t\t */\n\t\tret = huge_add_to_page_cache(page, mapping, idx);\n\t\tif (ret)\n\t\t\tgoto out_release_nounlock;\n\t}\n\n\tptl = huge_pte_lockptr(h, dst_mm, dst_pte);\n\tspin_lock(ptl);\n\n\t/*\n\t * Recheck the i_size after holding PT lock to make sure not\n\t * to leave any page mapped (as page_mapped()) beyond the end\n\t * of the i_size (remove_inode_hugepages() is strict about\n\t * enforcing that). If we bail out here, we'll also leave a\n\t * page in the radix tree in the vm_shared case beyond the end\n\t * of the i_size, but remove_inode_hugepages() will take care\n\t * of it as soon as we drop the hugetlb_fault_mutex_table.\n\t */\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tret = -EFAULT;\n\tif (idx >= size)\n\t\tgoto out_release_unlock;\n\n\tret = -EEXIST;\n\tif (!huge_pte_none(huge_ptep_get(dst_pte)))\n\t\tgoto out_release_unlock;\n\n\tif (vm_shared) {\n\t\tpage_dup_rmap(page, true);\n\t} else {\n\t\tClearPagePrivate(page);\n\t\thugepage_add_new_anon_rmap(page, dst_vma, dst_addr);\n\t}\n\n\t_dst_pte = make_huge_pte(dst_vma, page, dst_vma->vm_flags & VM_WRITE);\n\tif (dst_vma->vm_flags & VM_WRITE)\n\t\t_dst_pte = huge_pte_mkdirty(_dst_pte);\n\t_dst_pte = pte_mkyoung(_dst_pte);\n\n\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\n\n\t(void)huge_ptep_set_access_flags(dst_vma, dst_addr, dst_pte, _dst_pte,\n\t\t\t\t\tdst_vma->vm_flags & VM_WRITE);\n\thugetlb_count_add(pages_per_huge_page(h), dst_mm);\n\n\t/* No need to invalidate - it was non-present before */\n\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\n\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\n\tret = 0;\nout:\n\treturn ret;\nout_release_unlock:\n\tspin_unlock(ptl);\n\tif (vm_shared)\n\t\tunlock_page(page);\nout_release_nounlock:\n\tput_page(page);\n\tgoto out;\n}",
        "description": "A flaw was identified in the hugetlb_mcopy_atomic_pte function within the Linux kernel's memory management module, affecting versions prior to 4.13.12. The absence of a proper size check in this function could lead to a denial of service condition, indicated by a BUG.",
        "commit": "A vulnerability was identified in the userfaultfd functionality related to hugetlbfs, where the UFFDIO_COPY operation could inadvertently extend beyond the intended size of the file (i_size). This issue led to a kernel panic (oops) at fs/hugetlbfs/inode.c:484, triggered by the absence of an i_size check in the hugetlb_mcopy_atomic_pte function. Although mmap() operations could succeed beyond the end of the i_size after vmtruncate had removed virtual memory areas (vmas) in those ranges, subsequent faults, including UFFDIO_COPY, should not be allowed to succeed. The proposed solution involves modifying the return value to userland to indicate a SIGBUS-like condition, similar to what a page fault would produce, but this approach was deemed less useful due to the difficulty in distinguishing between SIGSEGV and SIGBUS through meaningful syscall return values."
    },
    {
        "cwe": "CWE-200",
        "func_name": "torvalds/xfs_iget_cache_miss",
        "score": 0.7826887369155884,
        "func_before": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\txfs_daddr_t\t\tbno,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\tunsigned long\t\tfirst_index, mask;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, bno, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\txfs_itrace_entry(ip);\n\n\tif ((ip->i_d.di_mode == 0) && !(flags & XFS_IGET_CREATE)) {\n\t\terror = ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region.\n\t */\n\tif (radix_tree_preload(GFP_KERNEL)) {\n\t\terror = EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\tmask = ~(((XFS_INODE_CLUSTER_SIZE(mp) >> mp->m_sb.sb_inodelog)) - 1);\n\tfirst_index = agino & mask;\n\twrite_lock(&pag->pag_ici_lock);\n\n\t/* insert the new inode */\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(xs_ig_dup);\n\t\terror = EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\n\t/* These values _must_ be set before releasing the radix tree lock! */\n\tip->i_udquot = ip->i_gdquot = NULL;\n\txfs_iflags_set(ip, XFS_INEW);\n\n\twrite_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\ttrace_xfs_iget_alloc(ip);\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\twrite_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
        "func_after": "static int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\tunsigned long\t\tfirst_index, mask;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\txfs_itrace_entry(ip);\n\n\tif ((ip->i_d.di_mode == 0) && !(flags & XFS_IGET_CREATE)) {\n\t\terror = ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region.\n\t */\n\tif (radix_tree_preload(GFP_KERNEL)) {\n\t\terror = EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\tmask = ~(((XFS_INODE_CLUSTER_SIZE(mp) >> mp->m_sb.sb_inodelog)) - 1);\n\tfirst_index = agino & mask;\n\twrite_lock(&pag->pag_ici_lock);\n\n\t/* insert the new inode */\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(xs_ig_dup);\n\t\terror = EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\n\t/* These values _must_ be set before releasing the radix tree lock! */\n\tip->i_udquot = ip->i_gdquot = NULL;\n\txfs_iflags_set(ip, XFS_INEW);\n\n\twrite_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\ttrace_xfs_iget_alloc(ip);\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\twrite_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}",
        "description": "The XFS implementation in the Linux kernel prior to version 2.6.35 fails to check inode allocation btrees before reading inode buffers. This oversight enables remote authenticated users to access unlinked files or modify disk blocks that were previously associated with unlinked files but are now linked to active files through stale NFS filehandles.",
        "commit": "The vulnerability involves using block numbers obtained from bulkstat-based inode lookups to optimize mapping calculations. However, since bulkstat data cannot be trusted, the block numbers should be discarded to ensure accurate and reliable lookups and mappings."
    },
    {
        "cwe": "CWE-346",
        "func_name": "hotplug/udev_monitor_receive_device",
        "score": 0.7728879451751709,
        "func_before": "struct udev_device *udev_monitor_receive_device(struct udev_monitor *udev_monitor)\n{\n\tstruct udev_device *udev_device;\n\tstruct msghdr smsg;\n\tstruct iovec iov;\n\tchar cred_msg[CMSG_SPACE(sizeof(struct ucred))];\n\tchar buf[4096];\n\tsize_t bufpos;\n\tint devpath_set = 0;\n\tint subsystem_set = 0;\n\tint action_set = 0;\n\tint maj = 0;\n\tint min = 0;\n\n\tif (udev_monitor == NULL)\n\t\treturn NULL;\n\tmemset(buf, 0x00, sizeof(buf));\n\tiov.iov_base = &buf;\n\tiov.iov_len = sizeof(buf);\n\tmemset (&smsg, 0x00, sizeof(struct msghdr));\n\tsmsg.msg_iov = &iov;\n\tsmsg.msg_iovlen = 1;\n\tsmsg.msg_control = cred_msg;\n\tsmsg.msg_controllen = sizeof(cred_msg);\n\n\tif (recvmsg(udev_monitor->sock, &smsg, 0) < 0) {\n\t\tif (errno != EINTR)\n\t\t\tinfo(udev_monitor->udev, \"unable to receive message\");\n\t\treturn NULL;\n\t}\n\n\tif (udev_monitor->sun.sun_family != 0) {\n\t\tstruct cmsghdr *cmsg = CMSG_FIRSTHDR(&smsg);\n\t\tstruct ucred *cred = (struct ucred *)CMSG_DATA (cmsg);\n\n\t\tif (cmsg == NULL || cmsg->cmsg_type != SCM_CREDENTIALS) {\n\t\t\tinfo(udev_monitor->udev, \"no sender credentials received, message ignored\");\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (cred->uid != 0) {\n\t\t\tinfo(udev_monitor->udev, \"sender uid=%d, message ignored\", cred->uid);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t/* skip header */\n\tbufpos = strlen(buf) + 1;\n\tif (bufpos < sizeof(\"a@/d\") || bufpos >= sizeof(buf)) {\n\t\tinfo(udev_monitor->udev, \"invalid message length\");\n\t\treturn NULL;\n\t}\n\n\t/* check message header */\n\tif (strstr(buf, \"@/\") == NULL) {\n\t\tinfo(udev_monitor->udev, \"unrecognized message header\");\n\t\treturn NULL;\n\t}\n\n\tudev_device = device_new(udev_monitor->udev);\n\tif (udev_device == NULL) {\n\t\treturn NULL;\n\t}\n\n\twhile (bufpos < sizeof(buf)) {\n\t\tchar *key;\n\t\tsize_t keylen;\n\n\t\tkey = &buf[bufpos];\n\t\tkeylen = strlen(key);\n\t\tif (keylen == 0)\n\t\t\tbreak;\n\t\tbufpos += keylen + 1;\n\n\t\tif (strncmp(key, \"DEVPATH=\", 8) == 0) {\n\t\t\tchar path[UTIL_PATH_SIZE];\n\n\t\t\tutil_strlcpy(path, udev_get_sys_path(udev_monitor->udev), sizeof(path));\n\t\t\tutil_strlcat(path, &key[8], sizeof(path));\n\t\t\tudev_device_set_syspath(udev_device, path);\n\t\t\tdevpath_set = 1;\n\t\t} else if (strncmp(key, \"SUBSYSTEM=\", 10) == 0) {\n\t\t\tudev_device_set_subsystem(udev_device, &key[10]);\n\t\t\tsubsystem_set = 1;\n\t\t} else if (strncmp(key, \"DEVTYPE=\", 8) == 0) {\n\t\t\tudev_device_set_devtype(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVNAME=\", 8) == 0) {\n\t\t\tudev_device_set_devnode(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVLINKS=\", 9) == 0) {\n\t\t\tchar devlinks[UTIL_PATH_SIZE];\n\t\t\tchar *slink;\n\t\t\tchar *next;\n\n\t\t\tutil_strlcpy(devlinks, &key[9], sizeof(devlinks));\n\t\t\tslink = devlinks;\n\t\t\tnext = strchr(slink, ' ');\n\t\t\twhile (next != NULL) {\n\t\t\t\tnext[0] = '\\0';\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t\t\tslink = &next[1];\n\t\t\t\tnext = strchr(slink, ' ');\n\t\t\t}\n\t\t\tif (slink[0] != '\\0')\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t} else if (strncmp(key, \"DRIVER=\", 7) == 0) {\n\t\t\tudev_device_set_driver(udev_device, &key[7]);\n\t\t} else if (strncmp(key, \"ACTION=\", 7) == 0) {\n\t\t\tudev_device_set_action(udev_device, &key[7]);\n\t\t\taction_set = 1;\n\t\t} else if (strncmp(key, \"MAJOR=\", 6) == 0) {\n\t\t\tmaj = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"MINOR=\", 6) == 0) {\n\t\t\tmin = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"DEVPATH_OLD=\", 12) == 0) {\n\t\t\tudev_device_set_devpath_old(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"PHYSDEVPATH=\", 12) == 0) {\n\t\t\tudev_device_set_physdevpath(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"SEQNUM=\", 7) == 0) {\n\t\t\tudev_device_set_seqnum(udev_device, strtoull(&key[7], NULL, 10));\n\t\t} else if (strncmp(key, \"TIMEOUT=\", 8) == 0) {\n\t\t\tudev_device_set_timeout(udev_device, strtoull(&key[8], NULL, 10));\n\t\t} else if (strncmp(key, \"PHYSDEV\", 7) == 0) {\n\t\t\t/* skip deprecated values */\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tudev_device_add_property_from_string(udev_device, key);\n\t\t}\n\t}\n\tif (!devpath_set || !subsystem_set || !action_set) {\n\t\tinfo(udev_monitor->udev, \"missing values, skip\\n\");\n\t\tudev_device_unref(udev_device);\n\t\treturn NULL;\n\t}\n\tif (maj > 0)\n\t\tudev_device_set_devnum(udev_device, makedev(maj, min));\n\tudev_device_set_info_loaded(udev_device);\n\treturn udev_device;\n}",
        "func_after": "struct udev_device *udev_monitor_receive_device(struct udev_monitor *udev_monitor)\n{\n\tstruct udev_device *udev_device;\n\tstruct msghdr smsg;\n\tstruct iovec iov;\n\tchar cred_msg[CMSG_SPACE(sizeof(struct ucred))];\n\tstruct cmsghdr *cmsg;\n\tstruct ucred *cred;\n\tchar buf[4096];\n\tsize_t bufpos;\n\tint devpath_set = 0;\n\tint subsystem_set = 0;\n\tint action_set = 0;\n\tint maj = 0;\n\tint min = 0;\n\n\tif (udev_monitor == NULL)\n\t\treturn NULL;\n\tmemset(buf, 0x00, sizeof(buf));\n\tiov.iov_base = &buf;\n\tiov.iov_len = sizeof(buf);\n\tmemset (&smsg, 0x00, sizeof(struct msghdr));\n\tsmsg.msg_iov = &iov;\n\tsmsg.msg_iovlen = 1;\n\tsmsg.msg_control = cred_msg;\n\tsmsg.msg_controllen = sizeof(cred_msg);\n\n\tif (recvmsg(udev_monitor->sock, &smsg, 0) < 0) {\n\t\tif (errno != EINTR)\n\t\t\tinfo(udev_monitor->udev, \"unable to receive message\");\n\t\treturn NULL;\n\t}\n\n\tcmsg = CMSG_FIRSTHDR(&smsg);\n\tif (cmsg == NULL || cmsg->cmsg_type != SCM_CREDENTIALS) {\n\t\tinfo(udev_monitor->udev, \"no sender credentials received, message ignored\");\n\t\treturn NULL;\n\t}\n\n\tcred = (struct ucred *)CMSG_DATA(cmsg);\n\tif (cred->uid != 0) {\n\t\tinfo(udev_monitor->udev, \"sender uid=%d, message ignored\", cred->uid);\n\t\treturn NULL;\n\t}\n\n\t/* skip header */\n\tbufpos = strlen(buf) + 1;\n\tif (bufpos < sizeof(\"a@/d\") || bufpos >= sizeof(buf)) {\n\t\tinfo(udev_monitor->udev, \"invalid message length\");\n\t\treturn NULL;\n\t}\n\n\t/* check message header */\n\tif (strstr(buf, \"@/\") == NULL) {\n\t\tinfo(udev_monitor->udev, \"unrecognized message header\");\n\t\treturn NULL;\n\t}\n\n\tudev_device = device_new(udev_monitor->udev);\n\tif (udev_device == NULL) {\n\t\treturn NULL;\n\t}\n\n\twhile (bufpos < sizeof(buf)) {\n\t\tchar *key;\n\t\tsize_t keylen;\n\n\t\tkey = &buf[bufpos];\n\t\tkeylen = strlen(key);\n\t\tif (keylen == 0)\n\t\t\tbreak;\n\t\tbufpos += keylen + 1;\n\n\t\tif (strncmp(key, \"DEVPATH=\", 8) == 0) {\n\t\t\tchar path[UTIL_PATH_SIZE];\n\n\t\t\tutil_strlcpy(path, udev_get_sys_path(udev_monitor->udev), sizeof(path));\n\t\t\tutil_strlcat(path, &key[8], sizeof(path));\n\t\t\tudev_device_set_syspath(udev_device, path);\n\t\t\tdevpath_set = 1;\n\t\t} else if (strncmp(key, \"SUBSYSTEM=\", 10) == 0) {\n\t\t\tudev_device_set_subsystem(udev_device, &key[10]);\n\t\t\tsubsystem_set = 1;\n\t\t} else if (strncmp(key, \"DEVTYPE=\", 8) == 0) {\n\t\t\tudev_device_set_devtype(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVNAME=\", 8) == 0) {\n\t\t\tudev_device_set_devnode(udev_device, &key[8]);\n\t\t} else if (strncmp(key, \"DEVLINKS=\", 9) == 0) {\n\t\t\tchar devlinks[UTIL_PATH_SIZE];\n\t\t\tchar *slink;\n\t\t\tchar *next;\n\n\t\t\tutil_strlcpy(devlinks, &key[9], sizeof(devlinks));\n\t\t\tslink = devlinks;\n\t\t\tnext = strchr(slink, ' ');\n\t\t\twhile (next != NULL) {\n\t\t\t\tnext[0] = '\\0';\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t\t\tslink = &next[1];\n\t\t\t\tnext = strchr(slink, ' ');\n\t\t\t}\n\t\t\tif (slink[0] != '\\0')\n\t\t\t\tudev_device_add_devlink(udev_device, slink);\n\t\t} else if (strncmp(key, \"DRIVER=\", 7) == 0) {\n\t\t\tudev_device_set_driver(udev_device, &key[7]);\n\t\t} else if (strncmp(key, \"ACTION=\", 7) == 0) {\n\t\t\tudev_device_set_action(udev_device, &key[7]);\n\t\t\taction_set = 1;\n\t\t} else if (strncmp(key, \"MAJOR=\", 6) == 0) {\n\t\t\tmaj = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"MINOR=\", 6) == 0) {\n\t\t\tmin = strtoull(&key[6], NULL, 10);\n\t\t} else if (strncmp(key, \"DEVPATH_OLD=\", 12) == 0) {\n\t\t\tudev_device_set_devpath_old(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"PHYSDEVPATH=\", 12) == 0) {\n\t\t\tudev_device_set_physdevpath(udev_device, &key[12]);\n\t\t} else if (strncmp(key, \"SEQNUM=\", 7) == 0) {\n\t\t\tudev_device_set_seqnum(udev_device, strtoull(&key[7], NULL, 10));\n\t\t} else if (strncmp(key, \"TIMEOUT=\", 8) == 0) {\n\t\t\tudev_device_set_timeout(udev_device, strtoull(&key[8], NULL, 10));\n\t\t} else if (strncmp(key, \"PHYSDEV\", 7) == 0) {\n\t\t\t/* skip deprecated values */\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tudev_device_add_property_from_string(udev_device, key);\n\t\t}\n\t}\n\tif (!devpath_set || !subsystem_set || !action_set) {\n\t\tinfo(udev_monitor->udev, \"missing values, skip\\n\");\n\t\tudev_device_unref(udev_device);\n\t\treturn NULL;\n\t}\n\tif (maj > 0)\n\t\tudev_device_set_devnum(udev_device, makedev(maj, min));\n\tudev_device_set_info_loaded(udev_device);\n\treturn udev_device;\n}",
        "description": "udev versions prior to 1.4.1 do not validate whether a NETLINK message originates from kernel space, enabling local users to escalate privileges by transmitting a NETLINK message from user space.",
        "commit": "To enhance protection, the system should disregard any unicast messages received on the netlink socket and any multicast messages on the kernel group that do not originate from the kernel."
    },
    {
        "cwe": "CWE-129",
        "func_name": "admesh/stl_fix_normal_directions",
        "score": 0.7600418329238892,
        "func_before": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "func_after": "void\nstl_fix_normal_directions(stl_file *stl) {\n  char *norm_sw;\n  /*  int edge_num;*/\n  /*  int vnot;*/\n  int checked = 0;\n  int facet_num;\n  /*  int next_facet;*/\n  int i;\n  int j;\n  struct stl_normal {\n    int               facet_num;\n    struct stl_normal *next;\n  };\n  struct stl_normal *head;\n  struct stl_normal *tail;\n  struct stl_normal *newn;\n  struct stl_normal *temp;\n\n  if (stl->error) return;\n\n  /* Initialize linked list. */\n  head = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(head == NULL) perror(\"stl_fix_normal_directions\");\n  tail = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n  if(tail == NULL) perror(\"stl_fix_normal_directions\");\n  head->next = tail;\n  tail->next = tail;\n\n  /* Initialize list that keeps track of already fixed facets. */\n  norm_sw = (char*)calloc(stl->stats.number_of_facets, sizeof(char));\n  if(norm_sw == NULL) perror(\"stl_fix_normal_directions\");\n\n\n  facet_num = 0;\n  /* If normal vector is not within tolerance and backwards:\n     Arbitrarily starts at face 0.  If this one is wrong, we're screwed.  Thankfully, the chances\n     of it being wrong randomly are low if most of the triangles are right: */\n  if(stl_check_normal_vector(stl, 0, 0) == 2)\n    stl_reverse_facet(stl, 0);\n\n  /* Say that we've fixed this facet: */\n  norm_sw[facet_num] = 1;\n  checked++;\n\n  for(;;) {\n    /* Add neighbors_to_list.\n       Add unconnected neighbors to the list:a  */\n    for(j = 0; j < 3; j++) {\n      /* Reverse the neighboring facets if necessary. */\n      if(stl->neighbors_start[facet_num].which_vertex_not[j] > 2) {\n        /* If the facet has a neighbor that is -1, it means that edge isn't shared by another facet */\n        if(stl->neighbors_start[facet_num].neighbor[j] != -1) {\n          stl_reverse_facet\n          (stl, stl->neighbors_start[facet_num].neighbor[j]);\n        }\n      }\n      /* If this edge of the facet is connected: */\n      if(stl->neighbors_start[facet_num].neighbor[j] != -1 &&\n         stl->neighbors_start[facet_num].neighbor[j] < stl->stats.number_of_facets*sizeof(char)) {\n        /* If we haven't fixed this facet yet, add it to the list: */\n        if(norm_sw[stl->neighbors_start[facet_num].neighbor[j]] != 1) {\n          /* Add node to beginning of list. */\n          newn = (struct stl_normal*)malloc(sizeof(struct stl_normal));\n          if(newn == NULL) perror(\"stl_fix_normal_directions\");\n          newn->facet_num = stl->neighbors_start[facet_num].neighbor[j];\n          newn->next = head->next;\n          head->next = newn;\n        }\n      }\n    }\n    /* Get next facet to fix from top of list. */\n    if(head->next != tail) {\n      facet_num = head->next->facet_num;\n      if(norm_sw[facet_num] != 1) { /* If facet is in list mutiple times */\n        norm_sw[facet_num] = 1; /* Record this one as being fixed. */\n        checked++;\n      }\n      temp = head->next;\t/* Delete this facet from the list. */\n      head->next = head->next->next;\n      free(temp);\n    } else { /* if we ran out of facets to fix: */\n      /* All of the facets in this part have been fixed. */\n      stl->stats.number_of_parts += 1;\n      if(checked >= stl->stats.number_of_facets) {\n        /* All of the facets have been checked.  Bail out. */\n        break;\n      } else {\n        /* There is another part here.  Find it and continue. */\n        for(i = 0; i < stl->stats.number_of_facets; i++) {\n          if(norm_sw[i] == 0) {\n            /* This is the first facet of the next part. */\n            facet_num = i;\n            if(stl_check_normal_vector(stl, i, 0) == 2) {\n              stl_reverse_facet(stl, i);\n            }\n\n            norm_sw[facet_num] = 1;\n            checked++;\n            break;\n          }\n        }\n      }\n    }\n  }\n  free(head);\n  free(tail);\n  free(norm_sw);\n}",
        "description": "An improper array index validation vulnerability exists in the stl_fix_normal_directions functionality of ADMesh. A specially-crafted STL file can lead to a heap buffer overflow. An attacker can exploit this by providing a malicious file.",
        "commit": "The vulnerability involves a check for the `neighbor_index` within the `stl_check_normal_vector` function. This fix addresses an issue identified in ticket #60."
    }
]