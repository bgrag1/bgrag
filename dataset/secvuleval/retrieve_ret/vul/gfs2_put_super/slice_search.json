[
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.745034396648407,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-755",
        "func_name": "xen-project/map_grant_ref",
        "score": 0.7887109518051147,
        "func_before": "static void\nmap_grant_ref(\n    struct gnttab_map_grant_ref *op)\n{\n    struct domain *ld, *rd, *owner = NULL;\n    struct grant_table *lgt, *rgt;\n    grant_ref_t ref;\n    grant_handle_t handle;\n    mfn_t mfn;\n    struct page_info *pg = NULL;\n    int            rc = GNTST_okay;\n    unsigned int   cache_flags, clear_flags = 0, refcnt = 0, typecnt = 0;\n    bool           host_map_created = false;\n    struct active_grant_entry *act = NULL;\n    struct grant_mapping *mt;\n    grant_entry_header_t *shah;\n    uint16_t *status;\n    bool_t need_iommu;\n\n    ld = current->domain;\n\n    if ( unlikely((op->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad flags in grant map op: %x\\n\", op->flags);\n        op->status = GNTST_bad_gntref;\n        return;\n    }\n\n    if ( unlikely(paging_mode_external(ld) &&\n                  (op->flags & (GNTMAP_device_map|GNTMAP_application_map|\n                            GNTMAP_contains_pte))) )\n    {\n        gdprintk(XENLOG_INFO, \"No device mapping in HVM domain\\n\");\n        op->status = GNTST_general_error;\n        return;\n    }\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(op->dom)) == NULL) )\n    {\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", op->dom);\n        op->status = GNTST_bad_domain;\n        return;\n    }\n\n    rc = xsm_grant_mapref(XSM_HOOK, ld, rd, op->flags);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    lgt = ld->grant_table;\n    handle = get_maptrack_handle(lgt);\n    if ( unlikely(handle == INVALID_MAPTRACK_HANDLE) )\n    {\n        rcu_unlock_domain(rd);\n        gdprintk(XENLOG_INFO, \"Failed to obtain maptrack handle\\n\");\n        op->status = GNTST_no_device_space;\n        return;\n    }\n\n    rgt = rd->grant_table;\n    grant_read_lock(rgt);\n\n    /* Bounds check on the grant ref */\n    ref = op->ref;\n    if ( unlikely(ref >= nr_grant_entries(rgt)))\n        PIN_FAIL(unlock_out, GNTST_bad_gntref, \"Bad ref %#x for d%d\\n\",\n                 ref, rgt->domain->domain_id);\n\n    /* This call also ensures the above check cannot be passed speculatively */\n    shah = shared_entry_header(rgt, ref);\n    act = active_entry_acquire(rgt, ref);\n\n    /* Make sure we do not access memory speculatively */\n    status = evaluate_nospec(rgt->gt_version == 1) ? &shah->flags\n                                                 : &status_entry(rgt, ref);\n\n    /* If already pinned, check the active domid and avoid refcnt overflow. */\n    if ( act->pin &&\n         ((act->domid != ld->domain_id) ||\n          (act->pin & 0x80808080U) != 0 ||\n          (act->is_sub_page)) )\n        PIN_FAIL(act_release_out, GNTST_general_error,\n                 \"Bad domain (%d != %d), or risk of counter overflow %08x, or subpage %d\\n\",\n                 act->domid, ld->domain_id, act->pin, act->is_sub_page);\n\n    if ( !act->pin ||\n         (!(op->flags & GNTMAP_readonly) &&\n          !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask))) )\n    {\n        if ( (rc = _set_status(shah, status, rd, rgt->gt_version, act,\n                               op->flags & GNTMAP_readonly, 1,\n                               ld->domain_id) != GNTST_okay) )\n            goto act_release_out;\n\n        if ( !act->pin )\n        {\n            unsigned long gfn = evaluate_nospec(rgt->gt_version == 1) ?\n                                shared_entry_v1(rgt, ref).frame :\n                                shared_entry_v2(rgt, ref).full_page.frame;\n\n            rc = get_paged_frame(gfn, &mfn, &pg,\n                                 op->flags & GNTMAP_readonly, rd);\n            if ( rc != GNTST_okay )\n                goto unlock_out_clear;\n            act_set_gfn(act, _gfn(gfn));\n            act->domid = ld->domain_id;\n            act->mfn = mfn;\n            act->start = 0;\n            act->length = PAGE_SIZE;\n            act->is_sub_page = false;\n            act->trans_domain = rd;\n            act->trans_gref = ref;\n        }\n    }\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n    mfn = act->mfn;\n\n    cache_flags = (shah->flags & (GTF_PAT | GTF_PWT | GTF_PCD) );\n\n    active_entry_release(act);\n    grant_read_unlock(rgt);\n\n    /* pg may be set, with a refcount included, from get_paged_frame(). */\n    if ( !pg )\n    {\n        pg = mfn_valid(mfn) ? mfn_to_page(mfn) : NULL;\n        if ( pg )\n            owner = page_get_owner_and_reference(pg);\n    }\n    else\n        owner = page_get_owner(pg);\n\n    if ( owner )\n        refcnt++;\n\n    if ( !pg || (owner == dom_io) )\n    {\n        /* Only needed the reference to confirm dom_io ownership. */\n        if ( pg )\n        {\n            put_page(pg);\n            refcnt--;\n        }\n\n        if ( paging_mode_external(ld) )\n        {\n            gdprintk(XENLOG_WARNING, \"HVM guests can't grant map iomem\\n\");\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( !iomem_access_permitted(rd, mfn_x(mfn), mfn_x(mfn)) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Iomem mapping not permitted %#\"PRI_mfn\" (domain %d)\\n\",\n                     mfn_x(mfn), rd->domain_id);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags,\n                                           cache_flags);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else if ( owner == rd || (dom_cow && owner == dom_cow) )\n    {\n        if ( (op->flags & GNTMAP_device_map) && !(op->flags & GNTMAP_readonly) )\n        {\n            if ( (owner == dom_cow) ||\n                 !get_page_type(pg, PGT_writable_page) )\n                goto could_not_pin;\n            typecnt++;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            /*\n             * Only need to grab another reference if device_map claimed\n             * the other one.\n             */\n            if ( op->flags & GNTMAP_device_map )\n            {\n                if ( !get_page(pg, rd) )\n                    goto could_not_pin;\n                refcnt++;\n            }\n\n            if ( gnttab_host_mapping_get_page_type(op->flags & GNTMAP_readonly,\n                                                   ld, rd) )\n            {\n                if ( (owner == dom_cow) ||\n                     !get_page_type(pg, PGT_writable_page) )\n                    goto could_not_pin;\n                typecnt++;\n            }\n\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags, 0);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else\n    {\n    could_not_pin:\n        if ( !rd->is_dying )\n            gdprintk(XENLOG_WARNING, \"Could not pin grant frame %#\"PRI_mfn\"\\n\",\n                     mfn_x(mfn));\n        rc = GNTST_general_error;\n        goto undo_out;\n    }\n\n    need_iommu = gnttab_need_iommu_mapping(ld);\n    if ( need_iommu )\n    {\n        unsigned int kind;\n\n        double_gt_lock(lgt, rgt);\n\n        /*\n         * We're not translated, so we know that dfns and mfns are\n         * the same things, so the IOMMU entry is always 1-to-1.\n         */\n        kind = mapkind(lgt, rd, mfn);\n        if ( !(op->flags & GNTMAP_readonly) &&\n             !(kind & MAPKIND_WRITE) )\n            kind = IOMMUF_readable | IOMMUF_writable;\n        else if ( !kind )\n            kind = IOMMUF_readable;\n        else\n            kind = 0;\n        if ( kind && iommu_legacy_map(ld, _dfn(mfn_x(mfn)), mfn, 0, kind) )\n        {\n            double_gt_unlock(lgt, rgt);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_MAP, op->dom);\n\n    /*\n     * All maptrack entry users check mt->flags first before using the\n     * other fields so just ensure the flags field is stored last.\n     *\n     * However, if gnttab_need_iommu_mapping() then this would race\n     * with a concurrent mapkind() call (on an unmap, for example)\n     * and a lock is required.\n     */\n    mt = &maptrack_entry(lgt, handle);\n    mt->domid = op->dom;\n    mt->ref   = op->ref;\n    smp_wmb();\n    write_atomic(&mt->flags, op->flags);\n\n    if ( need_iommu )\n        double_gt_unlock(lgt, rgt);\n\n    op->dev_bus_addr = mfn_to_maddr(mfn);\n    op->handle       = handle;\n    op->status       = GNTST_okay;\n\n    rcu_unlock_domain(rd);\n    return;\n\n undo_out:\n    if ( host_map_created )\n    {\n        replace_grant_host_mapping(op->host_addr, mfn, 0, op->flags);\n        gnttab_flush_tlb(ld);\n    }\n\n    while ( typecnt-- )\n        put_page_type(pg);\n\n    while ( refcnt-- )\n        put_page(pg);\n\n    grant_read_lock(rgt);\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n unlock_out_clear:\n    if ( !(op->flags & GNTMAP_readonly) &&\n         !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask)) )\n        clear_flags |= GTF_writing;\n\n    if ( !act->pin )\n        clear_flags |= GTF_reading;\n\n    if ( clear_flags )\n        gnttab_clear_flags(rd, clear_flags, status);\n\n act_release_out:\n    active_entry_release(act);\n\n unlock_out:\n    grant_read_unlock(rgt);\n    op->status = rc;\n    put_maptrack_handle(lgt, handle);\n    rcu_unlock_domain(rd);\n}",
        "func_after": "static void\nmap_grant_ref(\n    struct gnttab_map_grant_ref *op)\n{\n    struct domain *ld, *rd, *owner = NULL;\n    struct grant_table *lgt, *rgt;\n    grant_ref_t ref;\n    grant_handle_t handle;\n    mfn_t mfn;\n    struct page_info *pg = NULL;\n    int            rc = GNTST_okay;\n    unsigned int   cache_flags, clear_flags = 0, refcnt = 0, typecnt = 0;\n    bool           host_map_created = false;\n    struct active_grant_entry *act = NULL;\n    struct grant_mapping *mt;\n    grant_entry_header_t *shah;\n    uint16_t *status;\n    bool_t need_iommu;\n\n    ld = current->domain;\n\n    if ( unlikely((op->flags & (GNTMAP_device_map|GNTMAP_host_map)) == 0) )\n    {\n        gdprintk(XENLOG_INFO, \"Bad flags in grant map op: %x\\n\", op->flags);\n        op->status = GNTST_bad_gntref;\n        return;\n    }\n\n    if ( unlikely(paging_mode_external(ld) &&\n                  (op->flags & (GNTMAP_device_map|GNTMAP_application_map|\n                            GNTMAP_contains_pte))) )\n    {\n        gdprintk(XENLOG_INFO, \"No device mapping in HVM domain\\n\");\n        op->status = GNTST_general_error;\n        return;\n    }\n\n    if ( unlikely((rd = rcu_lock_domain_by_id(op->dom)) == NULL) )\n    {\n        gdprintk(XENLOG_INFO, \"Could not find domain %d\\n\", op->dom);\n        op->status = GNTST_bad_domain;\n        return;\n    }\n\n    rc = xsm_grant_mapref(XSM_HOOK, ld, rd, op->flags);\n    if ( rc )\n    {\n        rcu_unlock_domain(rd);\n        op->status = GNTST_permission_denied;\n        return;\n    }\n\n    lgt = ld->grant_table;\n    handle = get_maptrack_handle(lgt);\n    if ( unlikely(handle == INVALID_MAPTRACK_HANDLE) )\n    {\n        rcu_unlock_domain(rd);\n        gdprintk(XENLOG_INFO, \"Failed to obtain maptrack handle\\n\");\n        op->status = GNTST_no_device_space;\n        return;\n    }\n\n    rgt = rd->grant_table;\n    grant_read_lock(rgt);\n\n    /* Bounds check on the grant ref */\n    ref = op->ref;\n    if ( unlikely(ref >= nr_grant_entries(rgt)))\n        PIN_FAIL(unlock_out, GNTST_bad_gntref, \"Bad ref %#x for d%d\\n\",\n                 ref, rgt->domain->domain_id);\n\n    /* This call also ensures the above check cannot be passed speculatively */\n    shah = shared_entry_header(rgt, ref);\n    act = active_entry_acquire(rgt, ref);\n\n    /* Make sure we do not access memory speculatively */\n    status = evaluate_nospec(rgt->gt_version == 1) ? &shah->flags\n                                                 : &status_entry(rgt, ref);\n\n    /* If already pinned, check the active domid and avoid refcnt overflow. */\n    if ( act->pin &&\n         ((act->domid != ld->domain_id) ||\n          (act->pin & 0x80808080U) != 0 ||\n          (act->is_sub_page)) )\n        PIN_FAIL(act_release_out, GNTST_general_error,\n                 \"Bad domain (%d != %d), or risk of counter overflow %08x, or subpage %d\\n\",\n                 act->domid, ld->domain_id, act->pin, act->is_sub_page);\n\n    if ( !act->pin ||\n         (!(op->flags & GNTMAP_readonly) &&\n          !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask))) )\n    {\n        if ( (rc = _set_status(shah, status, rd, rgt->gt_version, act,\n                               op->flags & GNTMAP_readonly, 1,\n                               ld->domain_id)) != GNTST_okay )\n            goto act_release_out;\n\n        if ( !act->pin )\n        {\n            unsigned long gfn = evaluate_nospec(rgt->gt_version == 1) ?\n                                shared_entry_v1(rgt, ref).frame :\n                                shared_entry_v2(rgt, ref).full_page.frame;\n\n            rc = get_paged_frame(gfn, &mfn, &pg,\n                                 op->flags & GNTMAP_readonly, rd);\n            if ( rc != GNTST_okay )\n                goto unlock_out_clear;\n            act_set_gfn(act, _gfn(gfn));\n            act->domid = ld->domain_id;\n            act->mfn = mfn;\n            act->start = 0;\n            act->length = PAGE_SIZE;\n            act->is_sub_page = false;\n            act->trans_domain = rd;\n            act->trans_gref = ref;\n        }\n    }\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin += (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n    mfn = act->mfn;\n\n    cache_flags = (shah->flags & (GTF_PAT | GTF_PWT | GTF_PCD) );\n\n    active_entry_release(act);\n    grant_read_unlock(rgt);\n\n    /* pg may be set, with a refcount included, from get_paged_frame(). */\n    if ( !pg )\n    {\n        pg = mfn_valid(mfn) ? mfn_to_page(mfn) : NULL;\n        if ( pg )\n            owner = page_get_owner_and_reference(pg);\n    }\n    else\n        owner = page_get_owner(pg);\n\n    if ( owner )\n        refcnt++;\n\n    if ( !pg || (owner == dom_io) )\n    {\n        /* Only needed the reference to confirm dom_io ownership. */\n        if ( pg )\n        {\n            put_page(pg);\n            refcnt--;\n        }\n\n        if ( paging_mode_external(ld) )\n        {\n            gdprintk(XENLOG_WARNING, \"HVM guests can't grant map iomem\\n\");\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( !iomem_access_permitted(rd, mfn_x(mfn), mfn_x(mfn)) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Iomem mapping not permitted %#\"PRI_mfn\" (domain %d)\\n\",\n                     mfn_x(mfn), rd->domain_id);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags,\n                                           cache_flags);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else if ( owner == rd || (dom_cow && owner == dom_cow) )\n    {\n        if ( (op->flags & GNTMAP_device_map) && !(op->flags & GNTMAP_readonly) )\n        {\n            if ( (owner == dom_cow) ||\n                 !get_page_type(pg, PGT_writable_page) )\n                goto could_not_pin;\n            typecnt++;\n        }\n\n        if ( op->flags & GNTMAP_host_map )\n        {\n            /*\n             * Only need to grab another reference if device_map claimed\n             * the other one.\n             */\n            if ( op->flags & GNTMAP_device_map )\n            {\n                if ( !get_page(pg, rd) )\n                    goto could_not_pin;\n                refcnt++;\n            }\n\n            if ( gnttab_host_mapping_get_page_type(op->flags & GNTMAP_readonly,\n                                                   ld, rd) )\n            {\n                if ( (owner == dom_cow) ||\n                     !get_page_type(pg, PGT_writable_page) )\n                    goto could_not_pin;\n                typecnt++;\n            }\n\n            rc = create_grant_host_mapping(op->host_addr, mfn, op->flags, 0);\n            if ( rc != GNTST_okay )\n                goto undo_out;\n\n            host_map_created = true;\n        }\n    }\n    else\n    {\n    could_not_pin:\n        if ( !rd->is_dying )\n            gdprintk(XENLOG_WARNING, \"Could not pin grant frame %#\"PRI_mfn\"\\n\",\n                     mfn_x(mfn));\n        rc = GNTST_general_error;\n        goto undo_out;\n    }\n\n    need_iommu = gnttab_need_iommu_mapping(ld);\n    if ( need_iommu )\n    {\n        unsigned int kind;\n\n        double_gt_lock(lgt, rgt);\n\n        /*\n         * We're not translated, so we know that dfns and mfns are\n         * the same things, so the IOMMU entry is always 1-to-1.\n         */\n        kind = mapkind(lgt, rd, mfn);\n        if ( !(op->flags & GNTMAP_readonly) &&\n             !(kind & MAPKIND_WRITE) )\n            kind = IOMMUF_readable | IOMMUF_writable;\n        else if ( !kind )\n            kind = IOMMUF_readable;\n        else\n            kind = 0;\n        if ( kind && iommu_legacy_map(ld, _dfn(mfn_x(mfn)), mfn, 0, kind) )\n        {\n            double_gt_unlock(lgt, rgt);\n            rc = GNTST_general_error;\n            goto undo_out;\n        }\n    }\n\n    TRACE_1D(TRC_MEM_PAGE_GRANT_MAP, op->dom);\n\n    /*\n     * All maptrack entry users check mt->flags first before using the\n     * other fields so just ensure the flags field is stored last.\n     *\n     * However, if gnttab_need_iommu_mapping() then this would race\n     * with a concurrent mapkind() call (on an unmap, for example)\n     * and a lock is required.\n     */\n    mt = &maptrack_entry(lgt, handle);\n    mt->domid = op->dom;\n    mt->ref   = op->ref;\n    smp_wmb();\n    write_atomic(&mt->flags, op->flags);\n\n    if ( need_iommu )\n        double_gt_unlock(lgt, rgt);\n\n    op->dev_bus_addr = mfn_to_maddr(mfn);\n    op->handle       = handle;\n    op->status       = GNTST_okay;\n\n    rcu_unlock_domain(rd);\n    return;\n\n undo_out:\n    if ( host_map_created )\n    {\n        replace_grant_host_mapping(op->host_addr, mfn, 0, op->flags);\n        gnttab_flush_tlb(ld);\n    }\n\n    while ( typecnt-- )\n        put_page_type(pg);\n\n    while ( refcnt-- )\n        put_page(pg);\n\n    grant_read_lock(rgt);\n\n    act = active_entry_acquire(rgt, op->ref);\n\n    if ( op->flags & GNTMAP_device_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_devr_inc : GNTPIN_devw_inc;\n    if ( op->flags & GNTMAP_host_map )\n        act->pin -= (op->flags & GNTMAP_readonly) ?\n            GNTPIN_hstr_inc : GNTPIN_hstw_inc;\n\n unlock_out_clear:\n    if ( !(op->flags & GNTMAP_readonly) &&\n         !(act->pin & (GNTPIN_hstw_mask|GNTPIN_devw_mask)) )\n        clear_flags |= GTF_writing;\n\n    if ( !act->pin )\n        clear_flags |= GTF_reading;\n\n    if ( clear_flags )\n        gnttab_clear_flags(rd, clear_flags, status);\n\n act_release_out:\n    active_entry_release(act);\n\n unlock_out:\n    grant_read_unlock(rgt);\n    op->status = rc;\n    put_maptrack_handle(lgt, handle);\n    rcu_unlock_domain(rd);\n}",
        "description": "An issue was discovered in Xen versions up to 4.13.x, where guest operating system users could trigger a denial of service due to an improper error handling mechanism in the GNTTABOP_map_grant operation. Grant table operations are supposed to return 0 for success and a negative number for errors; however, a coding mistake resulted in one error path returning 1 instead of a negative value. The grant table code in Linux interprets this as a successful operation and continues with incorrectly initialized state. A malicious or buggy guest could manipulate its grant table to exploit this flaw, causing a crash in the Linux-based dom0 or backend domain when a backend domain attempts to map a grant.",
        "commit": "A function within the Xen hypervisor, specifically related to mapping grant references, had its error handling logic inadvertently altered. This change caused the function to return an incorrect status code when a critical operation failed, leading to unexpected behavior in the Linux kernel's network and block device backends. This issue could result in system crashes due to improper handling of guest states."
    },
    {
        "cwe": "CWE-400",
        "func_name": "kernel/tcp_prune_ofo_queue",
        "score": 0.7821576595306396,
        "func_before": "static bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\tnode = &tp->ooo_last_skb->rbnode;\n\tdo {\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\ttcp_drop(sk, rb_to_skb(node));\n\t\tsk_mem_reclaim(sk);\n\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t    !tcp_under_memory_pressure(sk))\n\t\t\tbreak;\n\t\tnode = prev;\n\t} while (node);\n\ttp->ooo_last_skb = rb_to_skb(prev);\n\n\t/* Reset SACK state.  A conforming SACK implementation will\n\t * do the same at a timeout based retransmit.  When a connection\n\t * is in a sad state like this, we care only about integrity\n\t * of the connection not performance.\n\t */\n\tif (tp->rx_opt.sack_ok)\n\t\ttcp_sack_reset(&tp->rx_opt);\n\treturn true;\n}",
        "func_after": "static bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\tint goal;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\tgoal = sk->sk_rcvbuf >> 3;\n\tnode = &tp->ooo_last_skb->rbnode;\n\tdo {\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\tgoal -= rb_to_skb(node)->truesize;\n\t\ttcp_drop(sk, rb_to_skb(node));\n\t\tif (!prev || goal <= 0) {\n\t\t\tsk_mem_reclaim(sk);\n\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t\t    !tcp_under_memory_pressure(sk))\n\t\t\t\tbreak;\n\t\t\tgoal = sk->sk_rcvbuf >> 3;\n\t\t}\n\t\tnode = prev;\n\t} while (node);\n\ttp->ooo_last_skb = rb_to_skb(prev);\n\n\t/* Reset SACK state.  A conforming SACK implementation will\n\t * do the same at a timeout based retransmit.  When a connection\n\t * is in a sad state like this, we care only about integrity\n\t * of the connection not performance.\n\t */\n\tif (tp->rx_opt.sack_ok)\n\t\ttcp_sack_reset(&tp->rx_opt);\n\treturn true;\n}",
        "description": "Linux kernel versions 4.9 and later can be coerced into making highly resource-intensive calls to functions responsible for collapsing and pruning out-of-order queues for every incoming packet, potentially leading to a denial of service.",
        "commit": "Malicious peers can exploit a vulnerability by injecting small packets into the out-of-order queue, leading to repeated and costly calls to functions such as `tcp_collapse_ofo_queue()` and `tcp_prune_ofo_queue()` for each incoming packet. When the out-of-order queue's default memory limit is set to 6MB, it can hold approximately 7000 nodes. This patch series aims to reduce CPU usage sufficiently to mitigate the severity of the attack. Future enhancements may include disconnecting or blackholing confirmed malicious flows."
    },
    {
        "cwe": "CWE-19",
        "func_name": "torvalds/xfs_attr_rmtval_set",
        "score": 0.7883840799331665,
        "func_before": "int\nxfs_attr_rmtval_set(\n\tstruct xfs_da_args\t*args)\n{\n\tstruct xfs_inode\t*dp = args->dp;\n\tstruct xfs_mount\t*mp = dp->i_mount;\n\tstruct xfs_bmbt_irec\tmap;\n\txfs_dablk_t\t\tlblkno;\n\txfs_fileoff_t\t\tlfileoff = 0;\n\t__uint8_t\t\t*src = args->value;\n\tint\t\t\tblkcnt;\n\tint\t\t\tvaluelen;\n\tint\t\t\tnmap;\n\tint\t\t\terror;\n\tint\t\t\toffset = 0;\n\n\ttrace_xfs_attr_rmtval_set(args);\n\n\t/*\n\t * Find a \"hole\" in the attribute address space large enough for\n\t * us to drop the new attribute's value into. Because CRC enable\n\t * attributes have headers, we can't just do a straight byte to FSB\n\t * conversion and have to take the header space into account.\n\t */\n\tblkcnt = xfs_attr3_rmt_blocks(mp, args->valuelen);\n\terror = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,\n\t\t\t\t\t\t   XFS_ATTR_FORK);\n\tif (error)\n\t\treturn error;\n\n\targs->rmtblkno = lblkno = (xfs_dablk_t)lfileoff;\n\targs->rmtblkcnt = blkcnt;\n\n\t/*\n\t * Roll through the \"value\", allocating blocks on disk as required.\n\t */\n\twhile (blkcnt > 0) {\n\t\tint\tcommitted;\n\n\t\t/*\n\t\t * Allocate a single extent, up to the size of the value.\n\t\t */\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_write(args->trans, dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t  blkcnt,\n\t\t\t\t  XFS_BMAPI_ATTRFORK | XFS_BMAPI_METADATA,\n\t\t\t\t  args->firstblock, args->total, &map, &nmap,\n\t\t\t\t  args->flist);\n\t\tif (!error) {\n\t\t\terror = xfs_bmap_finish(&args->trans, args->flist,\n\t\t\t\t\t\t&committed);\n\t\t}\n\t\tif (error) {\n\t\t\tASSERT(committed);\n\t\t\targs->trans = NULL;\n\t\t\txfs_bmap_cancel(args->flist);\n\t\t\treturn(error);\n\t\t}\n\n\t\t/*\n\t\t * bmap_finish() may have committed the last trans and started\n\t\t * a new one.  We need the inode to be in all transactions.\n\t\t */\n\t\tif (committed)\n\t\t\txfs_trans_ijoin(args->trans, dp, 0);\n\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\n\t\t/*\n\t\t * Start the next trans in the chain.\n\t\t */\n\t\terror = xfs_trans_roll(&args->trans, dp);\n\t\tif (error)\n\t\t\treturn (error);\n\t}\n\n\t/*\n\t * Roll through the \"value\", copying the attribute value to the\n\t * already-allocated blocks.  Blocks are written synchronously\n\t * so that we can know they are all on disk before we turn off\n\t * the INCOMPLETE flag.\n\t */\n\tlblkno = args->rmtblkno;\n\tblkcnt = args->rmtblkcnt;\n\tvaluelen = args->valuelen;\n\twhile (valuelen > 0) {\n\t\tstruct xfs_buf\t*bp;\n\t\txfs_daddr_t\tdblkno;\n\t\tint\t\tdblkcnt;\n\n\t\tASSERT(blkcnt > 0);\n\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_read(dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t       blkcnt, &map, &nmap,\n\t\t\t\t       XFS_BMAPI_ATTRFORK);\n\t\tif (error)\n\t\t\treturn(error);\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\n\t\tdblkno = XFS_FSB_TO_DADDR(mp, map.br_startblock),\n\t\tdblkcnt = XFS_FSB_TO_BB(mp, map.br_blockcount);\n\n\t\tbp = xfs_buf_get(mp->m_ddev_targp, dblkno, dblkcnt, 0);\n\t\tif (!bp)\n\t\t\treturn ENOMEM;\n\t\tbp->b_ops = &xfs_attr3_rmt_buf_ops;\n\n\t\txfs_attr_rmtval_copyin(mp, bp, args->dp->i_ino, &offset,\n\t\t\t\t       &valuelen, &src);\n\n\t\terror = xfs_bwrite(bp);\t/* GROT: NOTE: synchronous write */\n\t\txfs_buf_relse(bp);\n\t\tif (error)\n\t\t\treturn error;\n\n\n\t\t/* roll attribute extent map forwards */\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\t}\n\tASSERT(valuelen == 0);\n\treturn 0;\n}",
        "func_after": "int\nxfs_attr_rmtval_set(\n\tstruct xfs_da_args\t*args)\n{\n\tstruct xfs_inode\t*dp = args->dp;\n\tstruct xfs_mount\t*mp = dp->i_mount;\n\tstruct xfs_bmbt_irec\tmap;\n\txfs_dablk_t\t\tlblkno;\n\txfs_fileoff_t\t\tlfileoff = 0;\n\t__uint8_t\t\t*src = args->value;\n\tint\t\t\tblkcnt;\n\tint\t\t\tvaluelen;\n\tint\t\t\tnmap;\n\tint\t\t\terror;\n\tint\t\t\toffset = 0;\n\n\ttrace_xfs_attr_rmtval_set(args);\n\n\t/*\n\t * Find a \"hole\" in the attribute address space large enough for\n\t * us to drop the new attribute's value into. Because CRC enable\n\t * attributes have headers, we can't just do a straight byte to FSB\n\t * conversion and have to take the header space into account.\n\t */\n\tblkcnt = xfs_attr3_rmt_blocks(mp, args->rmtvaluelen);\n\terror = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,\n\t\t\t\t\t\t   XFS_ATTR_FORK);\n\tif (error)\n\t\treturn error;\n\n\targs->rmtblkno = lblkno = (xfs_dablk_t)lfileoff;\n\targs->rmtblkcnt = blkcnt;\n\n\t/*\n\t * Roll through the \"value\", allocating blocks on disk as required.\n\t */\n\twhile (blkcnt > 0) {\n\t\tint\tcommitted;\n\n\t\t/*\n\t\t * Allocate a single extent, up to the size of the value.\n\t\t */\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_write(args->trans, dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t  blkcnt,\n\t\t\t\t  XFS_BMAPI_ATTRFORK | XFS_BMAPI_METADATA,\n\t\t\t\t  args->firstblock, args->total, &map, &nmap,\n\t\t\t\t  args->flist);\n\t\tif (!error) {\n\t\t\terror = xfs_bmap_finish(&args->trans, args->flist,\n\t\t\t\t\t\t&committed);\n\t\t}\n\t\tif (error) {\n\t\t\tASSERT(committed);\n\t\t\targs->trans = NULL;\n\t\t\txfs_bmap_cancel(args->flist);\n\t\t\treturn(error);\n\t\t}\n\n\t\t/*\n\t\t * bmap_finish() may have committed the last trans and started\n\t\t * a new one.  We need the inode to be in all transactions.\n\t\t */\n\t\tif (committed)\n\t\t\txfs_trans_ijoin(args->trans, dp, 0);\n\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\n\t\t/*\n\t\t * Start the next trans in the chain.\n\t\t */\n\t\terror = xfs_trans_roll(&args->trans, dp);\n\t\tif (error)\n\t\t\treturn (error);\n\t}\n\n\t/*\n\t * Roll through the \"value\", copying the attribute value to the\n\t * already-allocated blocks.  Blocks are written synchronously\n\t * so that we can know they are all on disk before we turn off\n\t * the INCOMPLETE flag.\n\t */\n\tlblkno = args->rmtblkno;\n\tblkcnt = args->rmtblkcnt;\n\tvaluelen = args->rmtvaluelen;\n\twhile (valuelen > 0) {\n\t\tstruct xfs_buf\t*bp;\n\t\txfs_daddr_t\tdblkno;\n\t\tint\t\tdblkcnt;\n\n\t\tASSERT(blkcnt > 0);\n\n\t\txfs_bmap_init(args->flist, args->firstblock);\n\t\tnmap = 1;\n\t\terror = xfs_bmapi_read(dp, (xfs_fileoff_t)lblkno,\n\t\t\t\t       blkcnt, &map, &nmap,\n\t\t\t\t       XFS_BMAPI_ATTRFORK);\n\t\tif (error)\n\t\t\treturn(error);\n\t\tASSERT(nmap == 1);\n\t\tASSERT((map.br_startblock != DELAYSTARTBLOCK) &&\n\t\t       (map.br_startblock != HOLESTARTBLOCK));\n\n\t\tdblkno = XFS_FSB_TO_DADDR(mp, map.br_startblock),\n\t\tdblkcnt = XFS_FSB_TO_BB(mp, map.br_blockcount);\n\n\t\tbp = xfs_buf_get(mp->m_ddev_targp, dblkno, dblkcnt, 0);\n\t\tif (!bp)\n\t\t\treturn ENOMEM;\n\t\tbp->b_ops = &xfs_attr3_rmt_buf_ops;\n\n\t\txfs_attr_rmtval_copyin(mp, bp, args->dp->i_ino, &offset,\n\t\t\t\t       &valuelen, &src);\n\n\t\terror = xfs_bwrite(bp);\t/* GROT: NOTE: synchronous write */\n\t\txfs_buf_relse(bp);\n\t\tif (error)\n\t\t\treturn error;\n\n\n\t\t/* roll attribute extent map forwards */\n\t\tlblkno += map.br_blockcount;\n\t\tblkcnt -= map.br_blockcount;\n\t}\n\tASSERT(valuelen == 0);\n\treturn 0;\n}",
        "description": "The XFS implementation in the Linux kernel prior to version 3.15 improperly utilizes an outdated size value during remote attribute replacement, enabling local users to trigger a denial of service through transaction overruns and data corruption, or potentially escalate their privileges by exploiting XFS filesystem access.",
        "commit": "A vulnerability in the XFS filesystem allows for a remote attribute overwrite, leading to a transaction overrun. During remote attribute lookups, the length of the attribute is passed in the `xfs_da_args` structure to ensure correct CRC calculations and validity checks. However, this inadvertently modifies the `args->valuelen` parameter when replacing a remote attribute. Specifically, the lookup operation overwrites `args->valuelen` with the length of the existing remote attribute, causing the new attribute to be created with the incorrect size. If the new attribute is significantly smaller than the old one, this results in a transaction overrun and an assertion failure in debug kernels. The fix involves maintaining separate lengths for the remote attribute value and the attribute value within the `xfs_da_args` structure. Additionally, ensuring that remote block contexts saved for later renames are properly reset to avoid confusing the state of the attribute to be removed with the new attribute's state."
    },
    {
        "cwe": "CWE-285",
        "func_name": "ClusterLabs/crm_client_new",
        "score": 0.7592138051986694,
        "func_before": "crm_client_t *\ncrm_client_new(qb_ipcs_connection_t * c, uid_t uid_client, gid_t gid_client)\n{\n    static uid_t uid_server = 0;\n    static gid_t gid_cluster = 0;\n\n    crm_client_t *client = NULL;\n\n    CRM_LOG_ASSERT(c);\n    if (c == NULL) {\n        return NULL;\n    }\n\n    if (gid_cluster == 0) {\n        uid_server = getuid();\n        if(crm_user_lookup(CRM_DAEMON_USER, NULL, &gid_cluster) < 0) {\n            static bool have_error = FALSE;\n            if(have_error == FALSE) {\n                crm_warn(\"Could not find group for user %s\", CRM_DAEMON_USER);\n                have_error = TRUE;\n            }\n        }\n    }\n\n    if(gid_cluster != 0 && gid_client != 0) {\n        uid_t best_uid = -1; /* Passing -1 to chown(2) means don't change */\n\n        if(uid_client == 0 || uid_server == 0) { /* Someone is priveliged, but the other may not be */\n            best_uid = QB_MAX(uid_client, uid_server);\n            crm_trace(\"Allowing user %u to clean up after disconnect\", best_uid);\n        }\n\n        crm_trace(\"Giving access to group %u\", gid_cluster);\n        qb_ipcs_connection_auth_set(c, best_uid, gid_cluster, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP);\n    }\n\n    crm_client_init();\n\n    /* TODO: Do our own auth checking, return NULL if unauthorized */\n    client = calloc(1, sizeof(crm_client_t));\n\n    client->ipcs = c;\n    client->kind = CRM_CLIENT_IPC;\n    client->pid = crm_ipcs_client_pid(c);\n\n    client->id = crm_generate_uuid();\n\n    crm_debug(\"Connecting %p for uid=%d gid=%d pid=%u id=%s\", c, uid_client, gid_client, client->pid, client->id);\n\n#if ENABLE_ACL\n    client->user = uid2username(uid_client);\n#endif\n\n    g_hash_table_insert(client_connections, c, client);\n    return client;\n}",
        "func_after": "crm_client_t *\ncrm_client_new(qb_ipcs_connection_t * c, uid_t uid_client, gid_t gid_client)\n{\n    static gid_t gid_cluster = 0;\n\n    crm_client_t *client = NULL;\n\n    CRM_LOG_ASSERT(c);\n    if (c == NULL) {\n        return NULL;\n    }\n\n    if (gid_cluster == 0) {\n        if(crm_user_lookup(CRM_DAEMON_USER, NULL, &gid_cluster) < 0) {\n            static bool have_error = FALSE;\n            if(have_error == FALSE) {\n                crm_warn(\"Could not find group for user %s\", CRM_DAEMON_USER);\n                have_error = TRUE;\n            }\n        }\n    }\n\n    if (uid_client != 0) {\n        crm_trace(\"Giving access to group %u\", gid_cluster);\n        /* Passing -1 to chown(2) means don't change */\n        qb_ipcs_connection_auth_set(c, -1, gid_cluster, S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP);\n    }\n\n    crm_client_init();\n\n    /* TODO: Do our own auth checking, return NULL if unauthorized */\n    client = calloc(1, sizeof(crm_client_t));\n\n    client->ipcs = c;\n    client->kind = CRM_CLIENT_IPC;\n    client->pid = crm_ipcs_client_pid(c);\n\n    client->id = crm_generate_uuid();\n\n    crm_debug(\"Connecting %p for uid=%d gid=%d pid=%u id=%s\", c, uid_client, gid_client, client->pid, client->id);\n\n#if ENABLE_ACL\n    client->user = uid2username(uid_client);\n#endif\n\n    g_hash_table_insert(client_connections, c, client);\n    return client;\n}",
        "description": "An authorization flaw exists in Pacemaker versions prior to 1.1.16, where the IPC interface lacks adequate protection. An attacker with an unprivileged account on a Pacemaker node could exploit this flaw to, for instance, compel the Local Resource Manager daemon to execute a script with root privileges, thereby gaining unauthorized root access to the system.",
        "commit": "It was discovered that under certain conditions, unprivileged clients could communicate with some Pacemaker daemons via libqb-facilitated IPC due to flawed authorization decisions. Depending on the capabilities of the affected daemons, this could grant unauthorized users local privilege escalation or allow them to execute arbitrary commands remotely across the cluster. The original vulnerability aimed to permit unprivileged IPC clients to clean up leftover filesystem materials if the server crashes, but this fix inadvertently introduced the described security flaw. A best-effort patch has been proposed to address this issue systematically within libqb."
    }
]