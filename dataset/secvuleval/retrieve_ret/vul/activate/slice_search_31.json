[
    {
        "cwe": "CWE-476",
        "func_name": "ImageMagick/LoadOpenCLDevices",
        "score": 0.7497376203536987,
        "func_before": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireQuantumMemory(length,\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "func_after": "static void LoadOpenCLDevices(MagickCLEnv clEnv)\n{\n  cl_context_properties\n    properties[3];\n\n  cl_device_id\n    *devices;\n\n  cl_int\n    status;\n\n  cl_platform_id\n    *platforms;\n\n  cl_uint\n    i,\n    j,\n    next,\n    number_devices,\n    number_platforms;\n\n  size_t\n    length;\n\n  number_platforms=0;\n  if (openCL_library->clGetPlatformIDs(0,NULL,&number_platforms) != CL_SUCCESS)\n    return;\n  if (number_platforms == 0)\n    return;\n  platforms=(cl_platform_id *) AcquireMagickMemory(number_platforms*\n    sizeof(cl_platform_id));\n  if (platforms == (cl_platform_id *) NULL)\n    return;\n  if (openCL_library->clGetPlatformIDs(number_platforms,platforms,NULL) != CL_SUCCESS)\n    {\n       platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n       return;\n    }\n  for (i = 0; i < number_platforms; i++)\n  {\n    number_devices=GetOpenCLDeviceCount(clEnv,platforms[i]);\n    if (number_devices == 0)\n      platforms[i]=(cl_platform_id) NULL;\n    else\n      clEnv->number_devices+=number_devices;\n  }\n  if (clEnv->number_devices == 0)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  clEnv->devices=(MagickCLDevice *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(MagickCLDevice));\n  if (clEnv->devices == (MagickCLDevice *) NULL)\n    {\n      RelinquishMagickCLDevices(clEnv);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      return;\n    }\n  (void) ResetMagickMemory(clEnv->devices,0,clEnv->number_devices*\n    sizeof(MagickCLDevice));\n  devices=(cl_device_id *) AcquireQuantumMemory(clEnv->number_devices,\n    sizeof(cl_device_id));\n  if (devices == (cl_device_id *) NULL)\n    {\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  clEnv->number_contexts=(size_t) number_platforms;\n  clEnv->contexts=(cl_context *) AcquireQuantumMemory(clEnv->number_contexts,\n    sizeof(cl_context));\n  if (clEnv->contexts == (cl_context *) NULL)\n    {\n      devices=(cl_device_id *) RelinquishMagickMemory(devices);\n      platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n      RelinquishMagickCLDevices(clEnv);\n      return;\n    }\n  next=0;\n  for (i = 0; i < number_platforms; i++)\n  {\n    if (platforms[i] == (cl_platform_id) NULL)\n      continue;\n\n    status=clEnv->library->clGetDeviceIDs(platforms[i],CL_DEVICE_TYPE_CPU | \n      CL_DEVICE_TYPE_GPU,(cl_uint) clEnv->number_devices,devices,&number_devices);\n    if (status != CL_SUCCESS)\n      continue;\n\n    properties[0]=CL_CONTEXT_PLATFORM;\n    properties[1]=(cl_context_properties) platforms[i];\n    properties[2]=0;\n    clEnv->contexts[i]=openCL_library->clCreateContext(properties,number_devices,\n      devices,NULL,NULL,&status);\n    if (status != CL_SUCCESS)\n      continue;\n\n    for (j = 0; j < number_devices; j++,next++)\n    {\n      MagickCLDevice\n        device;\n\n      device=AcquireMagickCLDevice();\n      if (device == (MagickCLDevice) NULL)\n        break;\n\n      device->context=clEnv->contexts[i];\n      device->deviceID=devices[j];\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,0,NULL,\n        &length);\n      device->platform_name=AcquireCriticalMemory(length*\n        sizeof(*device->platform_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_NAME,length,\n        device->platform_name,NULL);\n\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,0,NULL,\n        &length);\n      device->vendor_name=AcquireQuantumMemory(length,\n        sizeof(*device->vendor_name));\n      openCL_library->clGetPlatformInfo(platforms[i],CL_PLATFORM_VENDOR,length,\n        device->vendor_name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,0,NULL,\n        &length);\n      device->name=AcquireQuantumMemory(length,sizeof(*device->name));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_NAME,length,\n        device->name,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,0,NULL,\n        &length);\n      device->version=AcquireQuantumMemory(length,sizeof(*device->version));\n      openCL_library->clGetDeviceInfo(devices[j],CL_DRIVER_VERSION,length,\n        device->version,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_CLOCK_FREQUENCY,\n        sizeof(cl_uint),&device->max_clock_frequency,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_MAX_COMPUTE_UNITS,\n        sizeof(cl_uint),&device->max_compute_units,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_TYPE,\n        sizeof(cl_device_type),&device->type,NULL);\n\n      openCL_library->clGetDeviceInfo(devices[j],CL_DEVICE_LOCAL_MEM_SIZE,\n        sizeof(cl_ulong),&device->local_memory_size,NULL);\n\n      clEnv->devices[next]=device;\n    }\n  }\n  if (next != clEnv->number_devices)\n    RelinquishMagickCLDevices(clEnv);\n  platforms=(cl_platform_id *) RelinquishMagickMemory(platforms);\n  devices=(cl_device_id *) RelinquishMagickMemory(devices);\n}",
        "description": "An issue was discovered in ImageMagick where a NULL pointer dereference vulnerability exists in the function responsible for loading OpenCL devices. This flaw allows attackers to trigger a denial of service through the use of a specially crafted file.",
        "commit": "It was discovered that ImageMagick, a popular image processing library, contains a vulnerability related to improper handling of certain image formats, potentially leading to buffer overflows or other memory corruption issues."
    },
    {
        "cwe": "CWE-369",
        "func_name": "upx/PackLinuxElf64::elf_lookup",
        "score": 0.7428545951843262,
        "func_before": "Elf64_Sym const *PackLinuxElf64__elf_lookup(char const *name) const\n{\n    if (hashtab && dynsym && dynstr) {\n        unsigned const nbucket = get_te32(&hashtab[0]);\n        unsigned const *const buckets = &hashtab[2];\n        unsigned const *const chains = &buckets[nbucket];\n        unsigned const m = elf_hash(name) % nbucket;\n        if (!nbucket\n        ||  (unsigned)(file_size - ((char const *)buckets - (char const *)(void const *)file_image))\n                <= sizeof(unsigned)*nbucket ) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad nbucket %#x\\n\", nbucket);\n            throwCantPack(msg);\n        }\n        unsigned si;\n        for (si= get_te32(&buckets[m]); 0!=si; si= get_te32(&chains[si])) {\n            char const *const p= get_dynsym_name(si, (unsigned)-1);\n            if (0==strcmp(name, p)) {\n                return &dynsym[si];\n            }\n        }\n    }\n    if (gashtab && dynsym && dynstr) {\n        unsigned const n_bucket = get_te32(&gashtab[0]);\n        unsigned const symbias  = get_te32(&gashtab[1]);\n        unsigned const n_bitmask = get_te32(&gashtab[2]);\n        unsigned const gnu_shift = get_te32(&gashtab[3]);\n        upx_uint64_t const *const bitmask = (upx_uint64_t const *)(void const *)&gashtab[4];\n        unsigned     const *const buckets = (unsigned const *)&bitmask[n_bitmask];\n        unsigned     const *const hasharr = &buckets[n_bucket];\n        if (!n_bucket\n        || (void const *)&file_image[file_size] <= (void const *)hasharr) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad n_bucket %#x\\n\", n_bucket);\n            throwCantPack(msg);\n        }\n        if (!n_bitmask\n        || (unsigned)(file_size - ((char const *)bitmask - (char const *)(void const *)file_image))\n                <= sizeof(unsigned)*n_bitmask ) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad n_bitmask %#x\\n\", n_bitmask);\n            throwCantPack(msg);\n        }\n\n        unsigned const h = gnu_hash(name);\n        unsigned const hbit1 = 077& h;\n        unsigned const hbit2 = 077& (h>>gnu_shift);\n        upx_uint64_t const w = get_te64(&bitmask[(n_bitmask -1) & (h>>6)]);\n\n        if (1& (w>>hbit1) & (w>>hbit2)) {\n            unsigned bucket = get_te32(&buckets[h % n_bucket]);\n            if (n_bucket <= bucket) {\n                char msg[80]; snprintf(msg, sizeof(msg),\n                        \"bad DT_GNU_HASH n_bucket{%#x} <= buckets[%d]{%#x}\\n\",\n                        n_bucket, h % n_bucket, bucket);\n                throwCantPack(msg);\n            }\n            if (0!=bucket) {\n                Elf64_Sym const *dsp = &dynsym[bucket];\n                unsigned const *hp = &hasharr[bucket - symbias];\n\n                do if (0==((h ^ get_te32(hp))>>1)) {\n                    unsigned st_name = get_te32(&dsp->st_name);\n                    char const *const p = get_str_name(st_name, (unsigned)-1);\n                    if (0==strcmp(name, p)) {\n                        return dsp;\n                    }\n                } while (++dsp,\n                        (char const *)hp < (char const *)&file_image[file_size]\n                    &&  0==(1u& get_te32(hp++)));\n            }\n        }\n    }\n    return 0;\n\n}",
        "func_after": "Elf64_Sym const *PackLinuxElf64__elf_lookup(char const *name) const\n{\n    if (hashtab && dynsym && dynstr) {\n        unsigned const nbucket = get_te32(&hashtab[0]);\n        unsigned const *const buckets = &hashtab[2];\n        unsigned const *const chains = &buckets[nbucket];\n        if (!nbucket\n        ||  (unsigned)(file_size - ((char const *)buckets - (char const *)(void const *)file_image))\n                <= sizeof(unsigned)*nbucket ) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad nbucket %#x\\n\", nbucket);\n            throwCantPack(msg);\n        }\n        unsigned const m = elf_hash(name) % nbucket;\n        unsigned si;\n        for (si= get_te32(&buckets[m]); 0!=si; si= get_te32(&chains[si])) {\n            char const *const p= get_dynsym_name(si, (unsigned)-1);\n            if (0==strcmp(name, p)) {\n                return &dynsym[si];\n            }\n        }\n    }\n    if (gashtab && dynsym && dynstr) {\n        unsigned const n_bucket = get_te32(&gashtab[0]);\n        unsigned const symbias  = get_te32(&gashtab[1]);\n        unsigned const n_bitmask = get_te32(&gashtab[2]);\n        unsigned const gnu_shift = get_te32(&gashtab[3]);\n        upx_uint64_t const *const bitmask = (upx_uint64_t const *)(void const *)&gashtab[4];\n        unsigned     const *const buckets = (unsigned const *)&bitmask[n_bitmask];\n        unsigned     const *const hasharr = &buckets[n_bucket];\n        if (!n_bucket\n        || (void const *)&file_image[file_size] <= (void const *)hasharr) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad n_bucket %#x\\n\", n_bucket);\n            throwCantPack(msg);\n        }\n        if (!n_bitmask\n        || (unsigned)(file_size - ((char const *)bitmask - (char const *)(void const *)file_image))\n                <= sizeof(unsigned)*n_bitmask ) {\n            char msg[80]; snprintf(msg, sizeof(msg),\n                \"bad n_bitmask %#x\\n\", n_bitmask);\n            throwCantPack(msg);\n        }\n\n        unsigned const h = gnu_hash(name);\n        unsigned const hbit1 = 077& h;\n        unsigned const hbit2 = 077& (h>>gnu_shift);\n        upx_uint64_t const w = get_te64(&bitmask[(n_bitmask -1) & (h>>6)]);\n\n        if (1& (w>>hbit1) & (w>>hbit2)) {\n            unsigned bucket = get_te32(&buckets[h % n_bucket]);\n            if (n_bucket <= bucket) {\n                char msg[80]; snprintf(msg, sizeof(msg),\n                        \"bad DT_GNU_HASH n_bucket{%#x} <= buckets[%d]{%#x}\\n\",\n                        n_bucket, h % n_bucket, bucket);\n                throwCantPack(msg);\n            }\n            if (0!=bucket) {\n                Elf64_Sym const *dsp = &dynsym[bucket];\n                unsigned const *hp = &hasharr[bucket - symbias];\n\n                do if (0==((h ^ get_te32(hp))>>1)) {\n                    unsigned st_name = get_te32(&dsp->st_name);\n                    char const *const p = get_str_name(st_name, (unsigned)-1);\n                    if (0==strcmp(name, p)) {\n                        return dsp;\n                    }\n                } while (++dsp,\n                        (char const *)hp < (char const *)&file_image[file_size]\n                    &&  0==(1u& get_te32(hp++)));\n            }\n        }\n    }\n    return 0;\n\n}",
        "description": "An floating point exception was discovered in the elf_lookup function within the UPX decompression utility, specifically in version 4.0.0, when processing a specially crafted Mach-O file.",
        "commit": "It was identified that there is a need to avoid setting `nbucket` to zero in the UPX compression tool, as indicated by an issue reported in the GitHub repository. This modification was made in the `p_lx_elf.cpp` file."
    },
    {
        "cwe": "CWE-911",
        "func_name": "torvalds/u32_destroy_key",
        "score": 0.7028190493583679,
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func_after": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "description": "An Improper Update of Reference Count vulnerability in the networking scheduler component of the Linux Kernel enables a local attacker to achieve privilege escalation to root. This issue impacts Linux Kernel versions prior to 5.18 and versions 4.14 and later.",
        "commit": "A vulnerability was identified in the Linux kernel where an extra `put_net()` operation is detected prematurely. Specifically, functions such as `u32_init_knode()` and `tcf_exts_init()` populate the `->exts.net` pointer without elevating the reference count on the network namespace (`netns`). The reference count is incremented only when `tcf_exts_get_net()` is called. Consequently, two calls to `u32_destroy_key()` from `u32_change()` attempt to release an invalid reference on the `netns`, leading to a refcount decrement hitting zero and potential memory leaks. This issue occurs in the Linux kernel prior to a specific version, affecting the handling of network traffic classification and filtering mechanisms."
    },
    {
        "cwe": "CWE-399",
        "func_name": "torvalds/__udf_read_inode",
        "score": 0.7556681632995605,
        "func_before": "static void __udf_read_inode(struct inode *inode)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct fileEntry *fe;\n\tstruct extendedFileEntry *efe;\n\tuint16_t ident;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tstruct udf_sb_info *sbi = UDF_SB(inode->i_sb);\n\tunsigned int link_count;\n\n\t/*\n\t * Set defaults, but the inode is still incomplete!\n\t * Note: get_new_inode() sets the following on a new inode:\n\t *      i_sb = sb\n\t *      i_no = ino\n\t *      i_flags = sb->s_flags\n\t *      i_state = 0\n\t * clean_inode(): zero fills and sets\n\t *      i_count = 1\n\t *      i_nlink = 1\n\t *      i_op = NULL;\n\t */\n\tbh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 0, &ident);\n\tif (!bh) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed !bh\\n\", inode->i_ino);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\n\tif (ident != TAG_IDENT_FE && ident != TAG_IDENT_EFE &&\n\t    ident != TAG_IDENT_USE) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed ident=%d\\n\",\n\t\t\tinode->i_ino, ident);\n\t\tbrelse(bh);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\n\tfe = (struct fileEntry *)bh->b_data;\n\tefe = (struct extendedFileEntry *)bh->b_data;\n\n\tif (fe->icbTag.strategyType == cpu_to_le16(4096)) {\n\t\tstruct buffer_head *ibh;\n\n\t\tibh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 1,\n\t\t\t\t\t&ident);\n\t\tif (ident == TAG_IDENT_IE && ibh) {\n\t\t\tstruct buffer_head *nbh = NULL;\n\t\t\tstruct kernel_lb_addr loc;\n\t\t\tstruct indirectEntry *ie;\n\n\t\t\tie = (struct indirectEntry *)ibh->b_data;\n\t\t\tloc = lelb_to_cpu(ie->indirectICB.extLocation);\n\n\t\t\tif (ie->indirectICB.extLength &&\n\t\t\t\t(nbh = udf_read_ptagged(inode->i_sb, &loc, 0,\n\t\t\t\t\t\t\t&ident))) {\n\t\t\t\tif (ident == TAG_IDENT_FE ||\n\t\t\t\t\tident == TAG_IDENT_EFE) {\n\t\t\t\t\tmemcpy(&iinfo->i_location,\n\t\t\t\t\t\t&loc,\n\t\t\t\t\t\tsizeof(struct kernel_lb_addr));\n\t\t\t\t\tbrelse(bh);\n\t\t\t\t\tbrelse(ibh);\n\t\t\t\t\tbrelse(nbh);\n\t\t\t\t\t__udf_read_inode(inode);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tbrelse(nbh);\n\t\t\t}\n\t\t}\n\t\tbrelse(ibh);\n\t} else if (fe->icbTag.strategyType != cpu_to_le16(4)) {\n\t\tudf_err(inode->i_sb, \"unsupported strategy type: %d\\n\",\n\t\t\tle16_to_cpu(fe->icbTag.strategyType));\n\t\tbrelse(bh);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\tif (fe->icbTag.strategyType == cpu_to_le16(4))\n\t\tiinfo->i_strat4096 = 0;\n\telse /* if (fe->icbTag.strategyType == cpu_to_le16(4096)) */\n\t\tiinfo->i_strat4096 = 1;\n\n\tiinfo->i_alloc_type = le16_to_cpu(fe->icbTag.flags) &\n\t\t\t\t\t\t\tICBTAG_FLAG_AD_MASK;\n\tiinfo->i_unique = 0;\n\tiinfo->i_lenEAttr = 0;\n\tiinfo->i_lenExtents = 0;\n\tiinfo->i_lenAlloc = 0;\n\tiinfo->i_next_alloc_block = 0;\n\tiinfo->i_next_alloc_goal = 0;\n\tif (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_EFE)) {\n\t\tiinfo->i_efe = 1;\n\t\tiinfo->i_use = 0;\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct extendedFileEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct extendedFileEntry),\n\t\t       inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct extendedFileEntry));\n\t} else if (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_FE)) {\n\t\tiinfo->i_efe = 0;\n\t\tiinfo->i_use = 0;\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\t\tsizeof(struct fileEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct fileEntry),\n\t\t       inode->i_sb->s_blocksize - sizeof(struct fileEntry));\n\t} else if (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_USE)) {\n\t\tiinfo->i_efe = 0;\n\t\tiinfo->i_use = 1;\n\t\tiinfo->i_lenAlloc = le32_to_cpu(\n\t\t\t\t((struct unallocSpaceEntry *)bh->b_data)->\n\t\t\t\t lengthAllocDescs);\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct unallocSpaceEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct unallocSpaceEntry),\n\t\t       inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct unallocSpaceEntry));\n\t\treturn;\n\t}\n\n\tread_lock(&sbi->s_cred_lock);\n\ti_uid_write(inode, le32_to_cpu(fe->uid));\n\tif (!uid_valid(inode->i_uid) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_UID_IGNORE) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_UID_SET))\n\t\tinode->i_uid = UDF_SB(inode->i_sb)->s_uid;\n\n\ti_gid_write(inode, le32_to_cpu(fe->gid));\n\tif (!gid_valid(inode->i_gid) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_GID_IGNORE) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_GID_SET))\n\t\tinode->i_gid = UDF_SB(inode->i_sb)->s_gid;\n\n\tif (fe->icbTag.fileType != ICBTAG_FILE_TYPE_DIRECTORY &&\n\t\t\tsbi->s_fmode != UDF_INVALID_MODE)\n\t\tinode->i_mode = sbi->s_fmode;\n\telse if (fe->icbTag.fileType == ICBTAG_FILE_TYPE_DIRECTORY &&\n\t\t\tsbi->s_dmode != UDF_INVALID_MODE)\n\t\tinode->i_mode = sbi->s_dmode;\n\telse\n\t\tinode->i_mode = udf_convert_permissions(fe);\n\tinode->i_mode &= ~sbi->s_umask;\n\tread_unlock(&sbi->s_cred_lock);\n\n\tlink_count = le16_to_cpu(fe->fileLinkCount);\n\tif (!link_count)\n\t\tlink_count = 1;\n\tset_nlink(inode, link_count);\n\n\tinode->i_size = le64_to_cpu(fe->informationLength);\n\tiinfo->i_lenExtents = inode->i_size;\n\n\tif (iinfo->i_efe == 0) {\n\t\tinode->i_blocks = le64_to_cpu(fe->logicalBlocksRecorded) <<\n\t\t\t(inode->i_sb->s_blocksize_bits - 9);\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_atime, fe->accessTime))\n\t\t\tinode->i_atime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_mtime,\n\t\t\t\t\t    fe->modificationTime))\n\t\t\tinode->i_mtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_ctime, fe->attrTime))\n\t\t\tinode->i_ctime = sbi->s_record_time;\n\n\t\tiinfo->i_unique = le64_to_cpu(fe->uniqueID);\n\t\tiinfo->i_lenEAttr = le32_to_cpu(fe->lengthExtendedAttr);\n\t\tiinfo->i_lenAlloc = le32_to_cpu(fe->lengthAllocDescs);\n\t\tiinfo->i_checkpoint = le32_to_cpu(fe->checkpoint);\n\t} else {\n\t\tinode->i_blocks = le64_to_cpu(efe->logicalBlocksRecorded) <<\n\t\t    (inode->i_sb->s_blocksize_bits - 9);\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_atime, efe->accessTime))\n\t\t\tinode->i_atime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_mtime,\n\t\t\t\t\t    efe->modificationTime))\n\t\t\tinode->i_mtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&iinfo->i_crtime, efe->createTime))\n\t\t\tiinfo->i_crtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_ctime, efe->attrTime))\n\t\t\tinode->i_ctime = sbi->s_record_time;\n\n\t\tiinfo->i_unique = le64_to_cpu(efe->uniqueID);\n\t\tiinfo->i_lenEAttr = le32_to_cpu(efe->lengthExtendedAttr);\n\t\tiinfo->i_lenAlloc = le32_to_cpu(efe->lengthAllocDescs);\n\t\tiinfo->i_checkpoint = le32_to_cpu(efe->checkpoint);\n\t}\n\n\tswitch (fe->icbTag.fileType) {\n\tcase ICBTAG_FILE_TYPE_DIRECTORY:\n\t\tinode->i_op = &udf_dir_inode_operations;\n\t\tinode->i_fop = &udf_dir_operations;\n\t\tinode->i_mode |= S_IFDIR;\n\t\tinc_nlink(inode);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_REALTIME:\n\tcase ICBTAG_FILE_TYPE_REGULAR:\n\tcase ICBTAG_FILE_TYPE_UNDEF:\n\tcase ICBTAG_FILE_TYPE_VAT20:\n\t\tif (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\n\t\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\telse\n\t\t\tinode->i_data.a_ops = &udf_aops;\n\t\tinode->i_op = &udf_file_inode_operations;\n\t\tinode->i_fop = &udf_file_operations;\n\t\tinode->i_mode |= S_IFREG;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_BLOCK:\n\t\tinode->i_mode |= S_IFBLK;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_CHAR:\n\t\tinode->i_mode |= S_IFCHR;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_FIFO:\n\t\tinit_special_inode(inode, inode->i_mode | S_IFIFO, 0);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_SOCKET:\n\t\tinit_special_inode(inode, inode->i_mode | S_IFSOCK, 0);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_SYMLINK:\n\t\tinode->i_data.a_ops = &udf_symlink_aops;\n\t\tinode->i_op = &udf_symlink_inode_operations;\n\t\tinode->i_mode = S_IFLNK | S_IRWXUGO;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_MAIN:\n\t\tudf_debug(\"METADATA FILE-----\\n\");\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_MIRROR:\n\t\tudf_debug(\"METADATA MIRROR FILE-----\\n\");\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_BITMAP:\n\t\tudf_debug(\"METADATA BITMAP FILE-----\\n\");\n\t\tbreak;\n\tdefault:\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed unknown file type=%d\\n\",\n\t\t\tinode->i_ino, fe->icbTag.fileType);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tstruct deviceSpec *dsea =\n\t\t\t(struct deviceSpec *)udf_get_extendedattr(inode, 12, 1);\n\t\tif (dsea) {\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t\tMKDEV(le32_to_cpu(dsea->majorDeviceIdent),\n\t\t\t\t      le32_to_cpu(dsea->minorDeviceIdent)));\n\t\t\t/* Developer ID ??? */\n\t\t} else\n\t\t\tmake_bad_inode(inode);\n\t}\n\tbrelse(bh);\n}",
        "func_after": "static void __udf_read_inode(struct inode *inode)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct fileEntry *fe;\n\tstruct extendedFileEntry *efe;\n\tuint16_t ident;\n\tstruct udf_inode_info *iinfo = UDF_I(inode);\n\tstruct udf_sb_info *sbi = UDF_SB(inode->i_sb);\n\tunsigned int link_count;\n\tunsigned int indirections = 0;\n\nreread:\n\t/*\n\t * Set defaults, but the inode is still incomplete!\n\t * Note: get_new_inode() sets the following on a new inode:\n\t *      i_sb = sb\n\t *      i_no = ino\n\t *      i_flags = sb->s_flags\n\t *      i_state = 0\n\t * clean_inode(): zero fills and sets\n\t *      i_count = 1\n\t *      i_nlink = 1\n\t *      i_op = NULL;\n\t */\n\tbh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 0, &ident);\n\tif (!bh) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed !bh\\n\", inode->i_ino);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\n\tif (ident != TAG_IDENT_FE && ident != TAG_IDENT_EFE &&\n\t    ident != TAG_IDENT_USE) {\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed ident=%d\\n\",\n\t\t\tinode->i_ino, ident);\n\t\tbrelse(bh);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\n\tfe = (struct fileEntry *)bh->b_data;\n\tefe = (struct extendedFileEntry *)bh->b_data;\n\n\tif (fe->icbTag.strategyType == cpu_to_le16(4096)) {\n\t\tstruct buffer_head *ibh;\n\n\t\tibh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 1,\n\t\t\t\t\t&ident);\n\t\tif (ident == TAG_IDENT_IE && ibh) {\n\t\t\tstruct kernel_lb_addr loc;\n\t\t\tstruct indirectEntry *ie;\n\n\t\t\tie = (struct indirectEntry *)ibh->b_data;\n\t\t\tloc = lelb_to_cpu(ie->indirectICB.extLocation);\n\n\t\t\tif (ie->indirectICB.extLength) {\n\t\t\t\tbrelse(bh);\n\t\t\t\tbrelse(ibh);\n\t\t\t\tmemcpy(&iinfo->i_location, &loc,\n\t\t\t\t       sizeof(struct kernel_lb_addr));\n\t\t\t\tif (++indirections > UDF_MAX_ICB_NESTING) {\n\t\t\t\t\tudf_err(inode->i_sb,\n\t\t\t\t\t\t\"too many ICBs in ICB hierarchy\"\n\t\t\t\t\t\t\" (max %d supported)\\n\",\n\t\t\t\t\t\tUDF_MAX_ICB_NESTING);\n\t\t\t\t\tmake_bad_inode(inode);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tgoto reread;\n\t\t\t}\n\t\t}\n\t\tbrelse(ibh);\n\t} else if (fe->icbTag.strategyType != cpu_to_le16(4)) {\n\t\tudf_err(inode->i_sb, \"unsupported strategy type: %d\\n\",\n\t\t\tle16_to_cpu(fe->icbTag.strategyType));\n\t\tbrelse(bh);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\tif (fe->icbTag.strategyType == cpu_to_le16(4))\n\t\tiinfo->i_strat4096 = 0;\n\telse /* if (fe->icbTag.strategyType == cpu_to_le16(4096)) */\n\t\tiinfo->i_strat4096 = 1;\n\n\tiinfo->i_alloc_type = le16_to_cpu(fe->icbTag.flags) &\n\t\t\t\t\t\t\tICBTAG_FLAG_AD_MASK;\n\tiinfo->i_unique = 0;\n\tiinfo->i_lenEAttr = 0;\n\tiinfo->i_lenExtents = 0;\n\tiinfo->i_lenAlloc = 0;\n\tiinfo->i_next_alloc_block = 0;\n\tiinfo->i_next_alloc_goal = 0;\n\tif (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_EFE)) {\n\t\tiinfo->i_efe = 1;\n\t\tiinfo->i_use = 0;\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct extendedFileEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct extendedFileEntry),\n\t\t       inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct extendedFileEntry));\n\t} else if (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_FE)) {\n\t\tiinfo->i_efe = 0;\n\t\tiinfo->i_use = 0;\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\t\tsizeof(struct fileEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct fileEntry),\n\t\t       inode->i_sb->s_blocksize - sizeof(struct fileEntry));\n\t} else if (fe->descTag.tagIdent == cpu_to_le16(TAG_IDENT_USE)) {\n\t\tiinfo->i_efe = 0;\n\t\tiinfo->i_use = 1;\n\t\tiinfo->i_lenAlloc = le32_to_cpu(\n\t\t\t\t((struct unallocSpaceEntry *)bh->b_data)->\n\t\t\t\t lengthAllocDescs);\n\t\tif (udf_alloc_i_data(inode, inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct unallocSpaceEntry))) {\n\t\t\tmake_bad_inode(inode);\n\t\t\treturn;\n\t\t}\n\t\tmemcpy(iinfo->i_ext.i_data,\n\t\t       bh->b_data + sizeof(struct unallocSpaceEntry),\n\t\t       inode->i_sb->s_blocksize -\n\t\t\t\t\tsizeof(struct unallocSpaceEntry));\n\t\treturn;\n\t}\n\n\tread_lock(&sbi->s_cred_lock);\n\ti_uid_write(inode, le32_to_cpu(fe->uid));\n\tif (!uid_valid(inode->i_uid) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_UID_IGNORE) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_UID_SET))\n\t\tinode->i_uid = UDF_SB(inode->i_sb)->s_uid;\n\n\ti_gid_write(inode, le32_to_cpu(fe->gid));\n\tif (!gid_valid(inode->i_gid) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_GID_IGNORE) ||\n\t    UDF_QUERY_FLAG(inode->i_sb, UDF_FLAG_GID_SET))\n\t\tinode->i_gid = UDF_SB(inode->i_sb)->s_gid;\n\n\tif (fe->icbTag.fileType != ICBTAG_FILE_TYPE_DIRECTORY &&\n\t\t\tsbi->s_fmode != UDF_INVALID_MODE)\n\t\tinode->i_mode = sbi->s_fmode;\n\telse if (fe->icbTag.fileType == ICBTAG_FILE_TYPE_DIRECTORY &&\n\t\t\tsbi->s_dmode != UDF_INVALID_MODE)\n\t\tinode->i_mode = sbi->s_dmode;\n\telse\n\t\tinode->i_mode = udf_convert_permissions(fe);\n\tinode->i_mode &= ~sbi->s_umask;\n\tread_unlock(&sbi->s_cred_lock);\n\n\tlink_count = le16_to_cpu(fe->fileLinkCount);\n\tif (!link_count)\n\t\tlink_count = 1;\n\tset_nlink(inode, link_count);\n\n\tinode->i_size = le64_to_cpu(fe->informationLength);\n\tiinfo->i_lenExtents = inode->i_size;\n\n\tif (iinfo->i_efe == 0) {\n\t\tinode->i_blocks = le64_to_cpu(fe->logicalBlocksRecorded) <<\n\t\t\t(inode->i_sb->s_blocksize_bits - 9);\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_atime, fe->accessTime))\n\t\t\tinode->i_atime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_mtime,\n\t\t\t\t\t    fe->modificationTime))\n\t\t\tinode->i_mtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_ctime, fe->attrTime))\n\t\t\tinode->i_ctime = sbi->s_record_time;\n\n\t\tiinfo->i_unique = le64_to_cpu(fe->uniqueID);\n\t\tiinfo->i_lenEAttr = le32_to_cpu(fe->lengthExtendedAttr);\n\t\tiinfo->i_lenAlloc = le32_to_cpu(fe->lengthAllocDescs);\n\t\tiinfo->i_checkpoint = le32_to_cpu(fe->checkpoint);\n\t} else {\n\t\tinode->i_blocks = le64_to_cpu(efe->logicalBlocksRecorded) <<\n\t\t    (inode->i_sb->s_blocksize_bits - 9);\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_atime, efe->accessTime))\n\t\t\tinode->i_atime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_mtime,\n\t\t\t\t\t    efe->modificationTime))\n\t\t\tinode->i_mtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&iinfo->i_crtime, efe->createTime))\n\t\t\tiinfo->i_crtime = sbi->s_record_time;\n\n\t\tif (!udf_disk_stamp_to_time(&inode->i_ctime, efe->attrTime))\n\t\t\tinode->i_ctime = sbi->s_record_time;\n\n\t\tiinfo->i_unique = le64_to_cpu(efe->uniqueID);\n\t\tiinfo->i_lenEAttr = le32_to_cpu(efe->lengthExtendedAttr);\n\t\tiinfo->i_lenAlloc = le32_to_cpu(efe->lengthAllocDescs);\n\t\tiinfo->i_checkpoint = le32_to_cpu(efe->checkpoint);\n\t}\n\n\tswitch (fe->icbTag.fileType) {\n\tcase ICBTAG_FILE_TYPE_DIRECTORY:\n\t\tinode->i_op = &udf_dir_inode_operations;\n\t\tinode->i_fop = &udf_dir_operations;\n\t\tinode->i_mode |= S_IFDIR;\n\t\tinc_nlink(inode);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_REALTIME:\n\tcase ICBTAG_FILE_TYPE_REGULAR:\n\tcase ICBTAG_FILE_TYPE_UNDEF:\n\tcase ICBTAG_FILE_TYPE_VAT20:\n\t\tif (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\n\t\t\tinode->i_data.a_ops = &udf_adinicb_aops;\n\t\telse\n\t\t\tinode->i_data.a_ops = &udf_aops;\n\t\tinode->i_op = &udf_file_inode_operations;\n\t\tinode->i_fop = &udf_file_operations;\n\t\tinode->i_mode |= S_IFREG;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_BLOCK:\n\t\tinode->i_mode |= S_IFBLK;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_CHAR:\n\t\tinode->i_mode |= S_IFCHR;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_FIFO:\n\t\tinit_special_inode(inode, inode->i_mode | S_IFIFO, 0);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_SOCKET:\n\t\tinit_special_inode(inode, inode->i_mode | S_IFSOCK, 0);\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_SYMLINK:\n\t\tinode->i_data.a_ops = &udf_symlink_aops;\n\t\tinode->i_op = &udf_symlink_inode_operations;\n\t\tinode->i_mode = S_IFLNK | S_IRWXUGO;\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_MAIN:\n\t\tudf_debug(\"METADATA FILE-----\\n\");\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_MIRROR:\n\t\tudf_debug(\"METADATA MIRROR FILE-----\\n\");\n\t\tbreak;\n\tcase ICBTAG_FILE_TYPE_BITMAP:\n\t\tudf_debug(\"METADATA BITMAP FILE-----\\n\");\n\t\tbreak;\n\tdefault:\n\t\tudf_err(inode->i_sb, \"(ino %ld) failed unknown file type=%d\\n\",\n\t\t\tinode->i_ino, fe->icbTag.fileType);\n\t\tmake_bad_inode(inode);\n\t\treturn;\n\t}\n\tif (S_ISCHR(inode->i_mode) || S_ISBLK(inode->i_mode)) {\n\t\tstruct deviceSpec *dsea =\n\t\t\t(struct deviceSpec *)udf_get_extendedattr(inode, 12, 1);\n\t\tif (dsea) {\n\t\t\tinit_special_inode(inode, inode->i_mode,\n\t\t\t\tMKDEV(le32_to_cpu(dsea->majorDeviceIdent),\n\t\t\t\t      le32_to_cpu(dsea->minorDeviceIdent)));\n\t\t\t/* Developer ID ??? */\n\t\t} else\n\t\t\tmake_bad_inode(inode);\n\t}\n\tbrelse(bh);\n}",
        "description": "The `__udf_read_inode` function in the UDF filesystem module of the Linux kernel up to version 3.16.3 lacks proper restrictions on the depth of ICB (Information Control Block) indirection. This deficiency enables physically proximate attackers to trigger a denial of service condition, such as an infinite loop or excessive stack consumption, by manipulating the UDF filesystem with a specially crafted inode.",
        "commit": "It was discovered that there was a risk of an infinite loop during the processing of indirect ICBs in the UDF file system. The issue arose because there was no limit set on the number of indirect ICBs that could be followed when loading an inode, potentially leading to a stack overflow if the medium was corrupted. To address this vulnerability, the recursion in the `__udf_read_inode()` function was removed, and a limit was imposed on the number of indirect ICBs that could be processed to prevent infinite loops."
    },
    {
        "cwe": "CWE-119",
        "func_name": "torvalds/do_page_fault",
        "score": 0.750317394733429,
        "func_before": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (notify_page_fault(regs))\n\t\treturn;\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "func_after": "asmlinkage\n#endif\nvoid __kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tint write, si_code;\n\tint fault;\n#ifdef CONFIG_X86_64\n\tunsigned long flags;\n\tint sig;\n#endif\n\n\ttsk = current;\n\tmm = tsk->mm;\n\tprefetchw(&mm->mmap_sem);\n\n\t/* get the address */\n\taddress = read_cr2();\n\n\tsi_code = SEGV_MAPERR;\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t/*\n\t * We fault-in kernel-space virtual memory on-demand. The\n\t * 'reference' page table is init_mm.pgd.\n\t *\n\t * NOTE! We MUST NOT take any locks for this case. We may\n\t * be in an interrupt or a critical region, and should\n\t * only copy the information from the master page table,\n\t * nothing more.\n\t *\n\t * This verifies that the fault happens in kernel space\n\t * (error_code & 4) == 0, and that the fault was not a\n\t * protection error (error_code & 9) == 0.\n\t */\n#ifdef CONFIG_X86_32\n\tif (unlikely(address >= TASK_SIZE)) {\n#else\n\tif (unlikely(address >= TASK_SIZE64)) {\n#endif\n\t\tif (!(error_code & (PF_RSVD|PF_USER|PF_PROT)) &&\n\t\t    vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\n\t\t/* Can handle a stale RO->RW TLB */\n\t\tif (spurious_fault(address, error_code))\n\t\t\treturn;\n\n\t\t/* kprobes don't want to hook the spurious faults. */\n\t\tif (notify_page_fault(regs))\n\t\t\treturn;\n\t\t/*\n\t\t * Don't take the mm semaphore here. If we fixup a prefetch\n\t\t * fault we could otherwise deadlock.\n\t\t */\n\t\tgoto bad_area_nosemaphore;\n\t}\n\n\t/* kprobes don't want to hook the spurious faults. */\n\tif (notify_page_fault(regs))\n\t\treturn;\n\n\t/*\n\t * It's safe to allow irq's after cr2 has been saved and the\n\t * vmalloc fault has been handled.\n\t *\n\t * User-mode registers count as a user access even for any\n\t * potential system fault or CPU buglet.\n\t */\n\tif (user_mode_vm(regs)) {\n\t\tlocal_irq_enable();\n\t\terror_code |= PF_USER;\n\t} else if (regs->flags & X86_EFLAGS_IF)\n\t\tlocal_irq_enable();\n\n#ifdef CONFIG_X86_64\n\tif (unlikely(error_code & PF_RSVD))\n\t\tpgtable_bad(address, regs, error_code);\n#endif\n\n\t/*\n\t * If we're in an interrupt, have no user context or are running in an\n\t * atomic region then we must not take the fault.\n\t */\n\tif (unlikely(in_atomic() || !mm))\n\t\tgoto bad_area_nosemaphore;\n\n\t/*\n\t * When running in the kernel we expect faults to occur only to\n\t * addresses in user space.  All other faults represent errors in the\n\t * kernel and should generate an OOPS.  Unfortunately, in the case of an\n\t * erroneous fault occurring in a code path which already holds mmap_sem\n\t * we will deadlock attempting to validate the fault against the\n\t * address space.  Luckily the kernel only validly references user\n\t * space from well defined areas of code, which are listed in the\n\t * exceptions table.\n\t *\n\t * As the vast majority of faults will be valid we will only perform\n\t * the source reference check when there is a possibility of a deadlock.\n\t * Attempt to lock the address space, if we cannot we then validate the\n\t * source.  If this is invalid we can skip the address space check,\n\t * thus avoiding the deadlock.\n\t */\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tif ((error_code & PF_USER) == 0 &&\n\t\t    !search_exception_tables(regs->ip))\n\t\t\tgoto bad_area_nosemaphore;\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\n\tvma = find_vma(mm, address);\n\tif (!vma)\n\t\tgoto bad_area;\n\tif (vma->vm_start <= address)\n\t\tgoto good_area;\n\tif (!(vma->vm_flags & VM_GROWSDOWN))\n\t\tgoto bad_area;\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * Accessing the stack below %sp is always a bug.\n\t\t * The large cushion allows instructions like enter\n\t\t * and pusha to work.  (\"enter $65535,$31\" pushes\n\t\t * 32 pointers and then decrements %sp by 65535.)\n\t\t */\n\t\tif (address + 65536 + 32 * sizeof(unsigned long) < regs->sp)\n\t\t\tgoto bad_area;\n\t}\n\tif (expand_stack(vma, address))\n\t\tgoto bad_area;\n/*\n * Ok, we have a good vm_area for this memory access, so\n * we can handle it..\n */\ngood_area:\n\tsi_code = SEGV_ACCERR;\n\twrite = 0;\n\tswitch (error_code & (PF_PROT|PF_WRITE)) {\n\tdefault:\t/* 3: write, present */\n\t\t/* fall through */\n\tcase PF_WRITE:\t\t/* write, not present */\n\t\tif (!(vma->vm_flags & VM_WRITE))\n\t\t\tgoto bad_area;\n\t\twrite++;\n\t\tbreak;\n\tcase PF_PROT:\t\t/* read, present */\n\t\tgoto bad_area;\n\tcase 0:\t\t\t/* read, not present */\n\t\tif (!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE)))\n\t\t\tgoto bad_area;\n\t}\n\n\t/*\n\t * If for any reason at all we couldn't handle the fault,\n\t * make sure we exit gracefully rather than endlessly redo\n\t * the fault.\n\t */\n\tfault = handle_mm_fault(mm, vma, address, write);\n\tif (unlikely(fault & VM_FAULT_ERROR)) {\n\t\tif (fault & VM_FAULT_OOM)\n\t\t\tgoto out_of_memory;\n\t\telse if (fault & VM_FAULT_SIGBUS)\n\t\t\tgoto do_sigbus;\n\t\tBUG();\n\t}\n\tif (fault & VM_FAULT_MAJOR)\n\t\ttsk->maj_flt++;\n\telse\n\t\ttsk->min_flt++;\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Did it hit the DOS screen memory VA from vm86 mode?\n\t */\n\tif (v8086_mode(regs)) {\n\t\tunsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;\n\t\tif (bit < 32)\n\t\t\ttsk->thread.screen_bitmap |= 1 << bit;\n\t}\n#endif\n\tup_read(&mm->mmap_sem);\n\treturn;\n\n/*\n * Something tried to access memory that isn't in our memory map..\n * Fix it, but check if it's kernel or user first..\n */\nbad_area:\n\tup_read(&mm->mmap_sem);\n\nbad_area_nosemaphore:\n\t/* User mode accesses just cause a SIGSEGV */\n\tif (error_code & PF_USER) {\n\t\t/*\n\t\t * It's possible to have interrupts off here.\n\t\t */\n\t\tlocal_irq_enable();\n\n\t\t/*\n\t\t * Valid to do another page fault here because this one came\n\t\t * from user space.\n\t\t */\n\t\tif (is_prefetch(regs, address, error_code))\n\t\t\treturn;\n\n\t\tif (is_errata100(regs, address))\n\t\t\treturn;\n\n\t\tif (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&\n\t\t    printk_ratelimit()) {\n\t\t\tprintk(\n\t\t\t\"%s%s[%d]: segfault at %lx ip %p sp %p error %lx\",\n\t\t\ttask_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,\n\t\t\ttsk->comm, task_pid_nr(tsk), address,\n\t\t\t(void *) regs->ip, (void *) regs->sp, error_code);\n\t\t\tprint_vma_addr(\" in \", regs->ip);\n\t\t\tprintk(\"\\n\");\n\t\t}\n\n\t\ttsk->thread.cr2 = address;\n\t\t/* Kernel addresses are always protection faults */\n\t\ttsk->thread.error_code = error_code | (address >= TASK_SIZE);\n\t\ttsk->thread.trap_no = 14;\n\t\tforce_sig_info_fault(SIGSEGV, si_code, address, tsk);\n\t\treturn;\n\t}\n\n\tif (is_f00f_bug(regs, address))\n\t\treturn;\n\nno_context:\n\t/* Are we prepared to handle this kernel fault?  */\n\tif (fixup_exception(regs))\n\t\treturn;\n\n\t/*\n\t * X86_32\n\t * Valid to do another page fault here, because if this fault\n\t * had been triggered by is_prefetch fixup_exception would have\n\t * handled it.\n\t *\n\t * X86_64\n\t * Hall of shame of CPU/BIOS bugs.\n\t */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n\n\tif (is_errata93(regs, address))\n\t\treturn;\n\n/*\n * Oops. The kernel tried to access some bad page. We'll have to\n * terminate things with extreme prejudice.\n */\n#ifdef CONFIG_X86_32\n\tbust_spinlocks(1);\n#else\n\tflags = oops_begin();\n#endif\n\n\tshow_fault_oops(regs, error_code, address);\n\n\ttsk->thread.cr2 = address;\n\ttsk->thread.trap_no = 14;\n\ttsk->thread.error_code = error_code;\n\n#ifdef CONFIG_X86_32\n\tdie(\"Oops\", regs, error_code);\n\tbust_spinlocks(0);\n\tdo_exit(SIGKILL);\n#else\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\t/* Executive summary in case the body of the oops scrolled away */\n\tprintk(KERN_EMERG \"CR2: %016lx\\n\", address);\n\toops_end(flags, regs, sig);\n#endif\n\nout_of_memory:\n\t/*\n\t * We ran out of memory, call the OOM killer, and return the userspace\n\t * (which will retry the fault, or kill us if we got oom-killed).\n\t */\n\tup_read(&mm->mmap_sem);\n\tpagefault_out_of_memory();\n\treturn;\n\ndo_sigbus:\n\tup_read(&mm->mmap_sem);\n\n\t/* Kernel mode? Handle exceptions or die */\n\tif (!(error_code & PF_USER))\n\t\tgoto no_context;\n#ifdef CONFIG_X86_32\n\t/* User space => ok to do another page fault */\n\tif (is_prefetch(regs, address, error_code))\n\t\treturn;\n#endif\n\ttsk->thread.cr2 = address;\n\ttsk->thread.error_code = error_code;\n\ttsk->thread.trap_no = 14;\n\tforce_sig_info_fault(SIGBUS, BUS_ADRERR, address, tsk);\n}",
        "description": "A stack consumption vulnerability exists in the `do_page_fault` function within the Linux kernel prior to version 2.6.28.5. This flaw allows local users to induce a denial of service through memory corruption or potentially escalate privileges by exploiting undefined vectors that trigger page faults on systems with registered Kprobes probes.",
        "commit": "To prevent kprobes from catching spurious faults that lead to infinite recursive page faults and memory corruption due to stack overflow."
    }
]