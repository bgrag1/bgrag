[
    {
        "cve_id": "CVE-2022-29581",
        "func_name": "torvalds/linux/u32_destroy_key",
        "description": "Improper Update of Reference Count vulnerability in net/sched of Linux Kernel allows local attacker to cause privilege escalation to root. This issue affects: Linux Kernel versions prior to 5.18; version 4.14 and later versions.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=3db09e762dc79584a69c10d74a6b98f89a9979f8",
        "commit_title": "We are now able to detect extra put_net() at the moment",
        "commit_text": "they happen, instead of much later in correct code paths.  u32_init_knode() / tcf_exts_init() populates the ->exts.net pointer, but as mentioned in tcf_exts_init(), the refcount on netns has not been elevated yet.  The refcount is taken only once tcf_exts_get_net() is called.  So the two u32_destroy_key() calls from u32_change() are attempting to release an invalid reference on the netns.  syzbot report:  refcount_t: decrement hit 0; leaking memory. WARNING: CPU: 0 PID: 21708 at lib/refcount.c:31 refcount_warn_saturate+0xbf/0x1e0 lib/refcount.c:31 Modules linked in: CPU: 0 PID: 21708 Comm: syz-executor.5 Not tainted 5.18.0-rc2-next-20220412-syzkaller #0 Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011 RIP: 0010:refcount_warn_saturate+0xbf/0x1e0 lib/refcount.c:31 Code: 1d 14 b6 b2 09 31 ff 89 de e8 6d e9 89 fd 84 db 75 e0 e8 84 e5 89 fd 48 c7 c7 40 aa 26 8a c6 05 f4 b5 b2 09 01 e8 e5 81 2e 05 <0f> 0b eb c4 e8 68 e5 89 fd 0f b6 1d e3 b5 b2 09 31 ff 89 de e8 38 RSP: 0018:ffffc900051af1b0 EFLAGS: 00010286 RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000 RDX: 0000000000040000 RSI: ffffffff8160a0c8 RDI: fffff52000a35e28 RBP: 0000000000000004 R08: 0000000000000000 R09: 0000000000000000 R10: ffffffff81604a9e R11: 0000000000000000 R12: 1ffff92000a35e3b R13: 00000000ffffffef R14: ffff8880211a0194 R15: ffff8880577d0a00 FS:  00007f25d183e700(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000 CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 CR2: 00007f19c859c028 CR3: 0000000051009000 CR4: 00000000003506f0 DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 Call Trace:  <TASK>  __refcount_dec include/linux/refcount.h:344 [inline]  refcount_dec include/linux/refcount.h:359 [inline]  ref_tracker_free+0x535/0x6b0 lib/ref_tracker.c:118  netns_tracker_free include/net/net_namespace.h:327 [inline]  put_net_track include/net/net_namespace.h:341 [inline]  tcf_exts_put_net include/net/pkt_cls.h:255 [inline]  u32_destroy_key.isra.0+0xa7/0x2b0 net/sched/cls_u32.c:394  u32_change+0xe01/0x3140 net/sched/cls_u32.c:909  tc_new_tfilter+0x98d/0x2200 net/sched/cls_api.c:2148  rtnetlink_rcv_msg+0x80d/0xb80 net/core/rtnetlink.c:6016  netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2495  netlink_unicast_kernel net/netlink/af_netlink.c:1319 [inline]  netlink_unicast+0x543/0x7f0 net/netlink/af_netlink.c:1345  netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1921  sock_sendmsg_nosec net/socket.c:705 [inline]  sock_sendmsg+0xcf/0x120 net/socket.c:725  ____sys_sendmsg+0x6e2/0x800 net/socket.c:2413  ___sys_sendmsg+0xf3/0x170 net/socket.c:2467  __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496  do_syscall_x64 arch/x86/entry/common.c:50 [inline]  do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80  entry_SYSCALL_64_after_hwframe+0x44/0xae RIP: 0033:0x7f25d0689049 Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b8 ff ff ff f7 d8 64 89 01 48 RSP: 002b:00007f25d183e168 EFLAGS: 00000246 ORIG_RAX: 000000000000002e RAX: ffffffffffffffda RBX: 00007f25d079c030 RCX: 00007f25d0689049 RDX: 0000000000000000 RSI: 0000000020000340 RDI: 0000000000000005 RBP: 00007f25d06e308d R08: 0000000000000000 R09: 0000000000000000 R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000 R13: 00007ffd0b752e3f R14: 00007f25d183e300 R15: 0000000000022000  </TASK>  Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Jiri Pirko <jiri@resnulli.us> ",
        "func_before": "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n\n\ttcf_exts_destroy(&n->exts);\n\ttcf_exts_put_net(&n->exts);\n\tif (ht && --ht->refcnt == 0)\n\t\tkfree(ht);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\tkfree(n);\n\treturn 0;\n}",
        "func": "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n{\n\ttcf_exts_put_net(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\n\tif (free_pf)\n\t\tfree_percpu(n->pf);\n#endif\n#ifdef CONFIG_CLS_U32_MARK\n\tif (free_pf)\n\t\tfree_percpu(n->pcpu_success);\n#endif\n\t__u32_destroy_key(n);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,6 @@\n-static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n+static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)\n {\n-\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);\n-\n-\ttcf_exts_destroy(&n->exts);\n \ttcf_exts_put_net(&n->exts);\n-\tif (ht && --ht->refcnt == 0)\n-\t\tkfree(ht);\n #ifdef CONFIG_CLS_U32_PERF\n \tif (free_pf)\n \t\tfree_percpu(n->pf);\n@@ -14,6 +9,5 @@\n \tif (free_pf)\n \t\tfree_percpu(n->pcpu_success);\n #endif\n-\tkfree(n);\n-\treturn 0;\n+\t__u32_destroy_key(n);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static int u32_destroy_key(struct tc_u_knode *n, bool free_pf)",
                "\tstruct tc_u_hnode *ht = rtnl_dereference(n->ht_down);",
                "",
                "\ttcf_exts_destroy(&n->exts);",
                "\tif (ht && --ht->refcnt == 0)",
                "\t\tkfree(ht);",
                "\tkfree(n);",
                "\treturn 0;"
            ],
            "added_lines": [
                "static void u32_destroy_key(struct tc_u_knode *n, bool free_pf)",
                "\t__u32_destroy_key(n);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29581",
        "func_name": "torvalds/linux/u32_change",
        "description": "Improper Update of Reference Count vulnerability in net/sched of Linux Kernel allows local attacker to cause privilege escalation to root. This issue affects: Linux Kernel versions prior to 5.18; version 4.14 and later versions.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=3db09e762dc79584a69c10d74a6b98f89a9979f8",
        "commit_title": "We are now able to detect extra put_net() at the moment",
        "commit_text": "they happen, instead of much later in correct code paths.  u32_init_knode() / tcf_exts_init() populates the ->exts.net pointer, but as mentioned in tcf_exts_init(), the refcount on netns has not been elevated yet.  The refcount is taken only once tcf_exts_get_net() is called.  So the two u32_destroy_key() calls from u32_change() are attempting to release an invalid reference on the netns.  syzbot report:  refcount_t: decrement hit 0; leaking memory. WARNING: CPU: 0 PID: 21708 at lib/refcount.c:31 refcount_warn_saturate+0xbf/0x1e0 lib/refcount.c:31 Modules linked in: CPU: 0 PID: 21708 Comm: syz-executor.5 Not tainted 5.18.0-rc2-next-20220412-syzkaller #0 Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011 RIP: 0010:refcount_warn_saturate+0xbf/0x1e0 lib/refcount.c:31 Code: 1d 14 b6 b2 09 31 ff 89 de e8 6d e9 89 fd 84 db 75 e0 e8 84 e5 89 fd 48 c7 c7 40 aa 26 8a c6 05 f4 b5 b2 09 01 e8 e5 81 2e 05 <0f> 0b eb c4 e8 68 e5 89 fd 0f b6 1d e3 b5 b2 09 31 ff 89 de e8 38 RSP: 0018:ffffc900051af1b0 EFLAGS: 00010286 RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000 RDX: 0000000000040000 RSI: ffffffff8160a0c8 RDI: fffff52000a35e28 RBP: 0000000000000004 R08: 0000000000000000 R09: 0000000000000000 R10: ffffffff81604a9e R11: 0000000000000000 R12: 1ffff92000a35e3b R13: 00000000ffffffef R14: ffff8880211a0194 R15: ffff8880577d0a00 FS:  00007f25d183e700(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000 CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033 CR2: 00007f19c859c028 CR3: 0000000051009000 CR4: 00000000003506f0 DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000 DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400 Call Trace:  <TASK>  __refcount_dec include/linux/refcount.h:344 [inline]  refcount_dec include/linux/refcount.h:359 [inline]  ref_tracker_free+0x535/0x6b0 lib/ref_tracker.c:118  netns_tracker_free include/net/net_namespace.h:327 [inline]  put_net_track include/net/net_namespace.h:341 [inline]  tcf_exts_put_net include/net/pkt_cls.h:255 [inline]  u32_destroy_key.isra.0+0xa7/0x2b0 net/sched/cls_u32.c:394  u32_change+0xe01/0x3140 net/sched/cls_u32.c:909  tc_new_tfilter+0x98d/0x2200 net/sched/cls_api.c:2148  rtnetlink_rcv_msg+0x80d/0xb80 net/core/rtnetlink.c:6016  netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2495  netlink_unicast_kernel net/netlink/af_netlink.c:1319 [inline]  netlink_unicast+0x543/0x7f0 net/netlink/af_netlink.c:1345  netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1921  sock_sendmsg_nosec net/socket.c:705 [inline]  sock_sendmsg+0xcf/0x120 net/socket.c:725  ____sys_sendmsg+0x6e2/0x800 net/socket.c:2413  ___sys_sendmsg+0xf3/0x170 net/socket.c:2467  __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496  do_syscall_x64 arch/x86/entry/common.c:50 [inline]  do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80  entry_SYSCALL_64_after_hwframe+0x44/0xae RIP: 0033:0x7f25d0689049 Code: ff ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 b8 ff ff ff f7 d8 64 89 01 48 RSP: 002b:00007f25d183e168 EFLAGS: 00000246 ORIG_RAX: 000000000000002e RAX: ffffffffffffffda RBX: 00007f25d079c030 RCX: 00007f25d0689049 RDX: 0000000000000000 RSI: 0000000020000340 RDI: 0000000000000005 RBP: 00007f25d06e308d R08: 0000000000000000 R09: 0000000000000000 R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000 R13: 00007ffd0b752e3f R14: 00007f25d183e300 R15: 0000000000022000  </TASK>  Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Jiri Pirko <jiri@resnulli.us> ",
        "func_before": "static int u32_change(struct net *net, struct sk_buff *in_skb,\n\t\t      struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t      struct nlattr **tca, void **arg, u32 flags,\n\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct tc_u_common *tp_c = tp->data;\n\tstruct tc_u_hnode *ht;\n\tstruct tc_u_knode *n;\n\tstruct tc_u32_sel *s;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_U32_MAX + 1];\n\tu32 htid, userflags = 0;\n\tsize_t sel_size;\n\tint err;\n\n\tif (!opt) {\n\t\tif (handle) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Filter handle requires options\");\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_U32_MAX, opt, u32_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_U32_FLAGS]) {\n\t\tuserflags = nla_get_u32(tb[TCA_U32_FLAGS]);\n\t\tif (!tc_flags_valid(userflags)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Invalid filter flags\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tn = *arg;\n\tif (n) {\n\t\tstruct tc_u_knode *new;\n\n\t\tif (TC_U32_KEY(n->handle) == 0) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Key node id cannot be zero\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((n->flags ^ userflags) &\n\t\t    ~(TCA_CLS_FLAGS_IN_HW | TCA_CLS_FLAGS_NOT_IN_HW)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Key node flags do not match passed flags\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnew = u32_init_knode(net, tp, n);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\n\t\terr = u32_set_parms(net, tp, base, new, tb,\n\t\t\t\t    tca[TCA_RATE], flags, new->flags,\n\t\t\t\t    extack);\n\n\t\tif (err) {\n\t\t\tu32_destroy_key(new, false);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = u32_replace_hw_knode(tp, new, flags, extack);\n\t\tif (err) {\n\t\t\tu32_destroy_key(new, false);\n\t\t\treturn err;\n\t\t}\n\n\t\tif (!tc_in_hw(new->flags))\n\t\t\tnew->flags |= TCA_CLS_FLAGS_NOT_IN_HW;\n\n\t\tu32_replace_knode(tp, tp_c, new);\n\t\ttcf_unbind_filter(tp, &n->res);\n\t\ttcf_exts_get_net(&n->exts);\n\t\ttcf_queue_work(&n->rwork, u32_delete_key_work);\n\t\treturn 0;\n\t}\n\n\tif (tb[TCA_U32_DIVISOR]) {\n\t\tunsigned int divisor = nla_get_u32(tb[TCA_U32_DIVISOR]);\n\n\t\tif (!is_power_of_2(divisor)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Divisor is not a power of 2\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (divisor-- > 0x100) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Exceeded maximum 256 hash buckets\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (TC_U32_KEY(handle)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Divisor can only be used on a hash table\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tht = kzalloc(struct_size(ht, ht, divisor + 1), GFP_KERNEL);\n\t\tif (ht == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tif (handle == 0) {\n\t\t\thandle = gen_new_htid(tp->data, ht);\n\t\t\tif (handle == 0) {\n\t\t\t\tkfree(ht);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\terr = idr_alloc_u32(&tp_c->handle_idr, ht, &handle,\n\t\t\t\t\t    handle, GFP_KERNEL);\n\t\t\tif (err) {\n\t\t\t\tkfree(ht);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tht->refcnt = 1;\n\t\tht->divisor = divisor;\n\t\tht->handle = handle;\n\t\tht->prio = tp->prio;\n\t\tidr_init(&ht->handle_idr);\n\t\tht->flags = userflags;\n\n\t\terr = u32_replace_hw_hnode(tp, ht, userflags, extack);\n\t\tif (err) {\n\t\t\tidr_remove(&tp_c->handle_idr, handle);\n\t\t\tkfree(ht);\n\t\t\treturn err;\n\t\t}\n\n\t\tRCU_INIT_POINTER(ht->next, tp_c->hlist);\n\t\trcu_assign_pointer(tp_c->hlist, ht);\n\t\t*arg = ht;\n\n\t\treturn 0;\n\t}\n\n\tif (tb[TCA_U32_HASH]) {\n\t\thtid = nla_get_u32(tb[TCA_U32_HASH]);\n\t\tif (TC_U32_HTID(htid) == TC_U32_ROOT) {\n\t\t\tht = rtnl_dereference(tp->root);\n\t\t\thtid = ht->handle;\n\t\t} else {\n\t\t\tht = u32_lookup_ht(tp->data, TC_U32_HTID(htid));\n\t\t\tif (!ht) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Specified hash table not found\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tht = rtnl_dereference(tp->root);\n\t\thtid = ht->handle;\n\t}\n\n\tif (ht->divisor < TC_U32_HASH(htid)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Specified hash table buckets exceed configured value\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (handle) {\n\t\tif (TC_U32_HTID(handle) && TC_U32_HTID(handle ^ htid)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Handle specified hash table address mismatch\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\thandle = htid | TC_U32_NODE(handle);\n\t\terr = idr_alloc_u32(&ht->handle_idr, NULL, &handle, handle,\n\t\t\t\t    GFP_KERNEL);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\thandle = gen_new_kid(ht, htid);\n\n\tif (tb[TCA_U32_SEL] == NULL) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Selector not specified\");\n\t\terr = -EINVAL;\n\t\tgoto erridr;\n\t}\n\n\ts = nla_data(tb[TCA_U32_SEL]);\n\tsel_size = struct_size(s, keys, s->nkeys);\n\tif (nla_len(tb[TCA_U32_SEL]) < sel_size) {\n\t\terr = -EINVAL;\n\t\tgoto erridr;\n\t}\n\n\tn = kzalloc(struct_size(n, sel.keys, s->nkeys), GFP_KERNEL);\n\tif (n == NULL) {\n\t\terr = -ENOBUFS;\n\t\tgoto erridr;\n\t}\n\n#ifdef CONFIG_CLS_U32_PERF\n\tn->pf = __alloc_percpu(struct_size(n->pf, kcnts, s->nkeys),\n\t\t\t       __alignof__(struct tc_u32_pcnt));\n\tif (!n->pf) {\n\t\terr = -ENOBUFS;\n\t\tgoto errfree;\n\t}\n#endif\n\n\tmemcpy(&n->sel, s, sel_size);\n\tRCU_INIT_POINTER(n->ht_up, ht);\n\tn->handle = handle;\n\tn->fshift = s->hmask ? ffs(ntohl(s->hmask)) - 1 : 0;\n\tn->flags = userflags;\n\n\terr = tcf_exts_init(&n->exts, net, TCA_U32_ACT, TCA_U32_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\n#ifdef CONFIG_CLS_U32_MARK\n\tn->pcpu_success = alloc_percpu(u32);\n\tif (!n->pcpu_success) {\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tif (tb[TCA_U32_MARK]) {\n\t\tstruct tc_u32_mark *mark;\n\n\t\tmark = nla_data(tb[TCA_U32_MARK]);\n\t\tn->val = mark->val;\n\t\tn->mask = mark->mask;\n\t}\n#endif\n\n\terr = u32_set_parms(net, tp, base, n, tb, tca[TCA_RATE],\n\t\t\t    flags, n->flags, extack);\n\tif (err == 0) {\n\t\tstruct tc_u_knode __rcu **ins;\n\t\tstruct tc_u_knode *pins;\n\n\t\terr = u32_replace_hw_knode(tp, n, flags, extack);\n\t\tif (err)\n\t\t\tgoto errhw;\n\n\t\tif (!tc_in_hw(n->flags))\n\t\t\tn->flags |= TCA_CLS_FLAGS_NOT_IN_HW;\n\n\t\tins = &ht->ht[TC_U32_HASH(handle)];\n\t\tfor (pins = rtnl_dereference(*ins); pins;\n\t\t     ins = &pins->next, pins = rtnl_dereference(*ins))\n\t\t\tif (TC_U32_NODE(handle) < TC_U32_NODE(pins->handle))\n\t\t\t\tbreak;\n\n\t\tRCU_INIT_POINTER(n->next, pins);\n\t\trcu_assign_pointer(*ins, n);\n\t\ttp_c->knodes++;\n\t\t*arg = n;\n\t\treturn 0;\n\t}\n\nerrhw:\n#ifdef CONFIG_CLS_U32_MARK\n\tfree_percpu(n->pcpu_success);\n#endif\n\nerrout:\n\ttcf_exts_destroy(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\nerrfree:\n\tfree_percpu(n->pf);\n#endif\n\tkfree(n);\nerridr:\n\tidr_remove(&ht->handle_idr, handle);\n\treturn err;\n}",
        "func": "static int u32_change(struct net *net, struct sk_buff *in_skb,\n\t\t      struct tcf_proto *tp, unsigned long base, u32 handle,\n\t\t      struct nlattr **tca, void **arg, u32 flags,\n\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct tc_u_common *tp_c = tp->data;\n\tstruct tc_u_hnode *ht;\n\tstruct tc_u_knode *n;\n\tstruct tc_u32_sel *s;\n\tstruct nlattr *opt = tca[TCA_OPTIONS];\n\tstruct nlattr *tb[TCA_U32_MAX + 1];\n\tu32 htid, userflags = 0;\n\tsize_t sel_size;\n\tint err;\n\n\tif (!opt) {\n\t\tif (handle) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Filter handle requires options\");\n\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_U32_MAX, opt, u32_policy,\n\t\t\t\t\t  extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_U32_FLAGS]) {\n\t\tuserflags = nla_get_u32(tb[TCA_U32_FLAGS]);\n\t\tif (!tc_flags_valid(userflags)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Invalid filter flags\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tn = *arg;\n\tif (n) {\n\t\tstruct tc_u_knode *new;\n\n\t\tif (TC_U32_KEY(n->handle) == 0) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Key node id cannot be zero\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((n->flags ^ userflags) &\n\t\t    ~(TCA_CLS_FLAGS_IN_HW | TCA_CLS_FLAGS_NOT_IN_HW)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Key node flags do not match passed flags\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnew = u32_init_knode(net, tp, n);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\n\t\terr = u32_set_parms(net, tp, base, new, tb,\n\t\t\t\t    tca[TCA_RATE], flags, new->flags,\n\t\t\t\t    extack);\n\n\t\tif (err) {\n\t\t\t__u32_destroy_key(new);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = u32_replace_hw_knode(tp, new, flags, extack);\n\t\tif (err) {\n\t\t\t__u32_destroy_key(new);\n\t\t\treturn err;\n\t\t}\n\n\t\tif (!tc_in_hw(new->flags))\n\t\t\tnew->flags |= TCA_CLS_FLAGS_NOT_IN_HW;\n\n\t\tu32_replace_knode(tp, tp_c, new);\n\t\ttcf_unbind_filter(tp, &n->res);\n\t\ttcf_exts_get_net(&n->exts);\n\t\ttcf_queue_work(&n->rwork, u32_delete_key_work);\n\t\treturn 0;\n\t}\n\n\tif (tb[TCA_U32_DIVISOR]) {\n\t\tunsigned int divisor = nla_get_u32(tb[TCA_U32_DIVISOR]);\n\n\t\tif (!is_power_of_2(divisor)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Divisor is not a power of 2\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (divisor-- > 0x100) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Exceeded maximum 256 hash buckets\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (TC_U32_KEY(handle)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Divisor can only be used on a hash table\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tht = kzalloc(struct_size(ht, ht, divisor + 1), GFP_KERNEL);\n\t\tif (ht == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tif (handle == 0) {\n\t\t\thandle = gen_new_htid(tp->data, ht);\n\t\t\tif (handle == 0) {\n\t\t\t\tkfree(ht);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\terr = idr_alloc_u32(&tp_c->handle_idr, ht, &handle,\n\t\t\t\t\t    handle, GFP_KERNEL);\n\t\t\tif (err) {\n\t\t\t\tkfree(ht);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tht->refcnt = 1;\n\t\tht->divisor = divisor;\n\t\tht->handle = handle;\n\t\tht->prio = tp->prio;\n\t\tidr_init(&ht->handle_idr);\n\t\tht->flags = userflags;\n\n\t\terr = u32_replace_hw_hnode(tp, ht, userflags, extack);\n\t\tif (err) {\n\t\t\tidr_remove(&tp_c->handle_idr, handle);\n\t\t\tkfree(ht);\n\t\t\treturn err;\n\t\t}\n\n\t\tRCU_INIT_POINTER(ht->next, tp_c->hlist);\n\t\trcu_assign_pointer(tp_c->hlist, ht);\n\t\t*arg = ht;\n\n\t\treturn 0;\n\t}\n\n\tif (tb[TCA_U32_HASH]) {\n\t\thtid = nla_get_u32(tb[TCA_U32_HASH]);\n\t\tif (TC_U32_HTID(htid) == TC_U32_ROOT) {\n\t\t\tht = rtnl_dereference(tp->root);\n\t\t\thtid = ht->handle;\n\t\t} else {\n\t\t\tht = u32_lookup_ht(tp->data, TC_U32_HTID(htid));\n\t\t\tif (!ht) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Specified hash table not found\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tht = rtnl_dereference(tp->root);\n\t\thtid = ht->handle;\n\t}\n\n\tif (ht->divisor < TC_U32_HASH(htid)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Specified hash table buckets exceed configured value\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (handle) {\n\t\tif (TC_U32_HTID(handle) && TC_U32_HTID(handle ^ htid)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Handle specified hash table address mismatch\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\thandle = htid | TC_U32_NODE(handle);\n\t\terr = idr_alloc_u32(&ht->handle_idr, NULL, &handle, handle,\n\t\t\t\t    GFP_KERNEL);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\thandle = gen_new_kid(ht, htid);\n\n\tif (tb[TCA_U32_SEL] == NULL) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Selector not specified\");\n\t\terr = -EINVAL;\n\t\tgoto erridr;\n\t}\n\n\ts = nla_data(tb[TCA_U32_SEL]);\n\tsel_size = struct_size(s, keys, s->nkeys);\n\tif (nla_len(tb[TCA_U32_SEL]) < sel_size) {\n\t\terr = -EINVAL;\n\t\tgoto erridr;\n\t}\n\n\tn = kzalloc(struct_size(n, sel.keys, s->nkeys), GFP_KERNEL);\n\tif (n == NULL) {\n\t\terr = -ENOBUFS;\n\t\tgoto erridr;\n\t}\n\n#ifdef CONFIG_CLS_U32_PERF\n\tn->pf = __alloc_percpu(struct_size(n->pf, kcnts, s->nkeys),\n\t\t\t       __alignof__(struct tc_u32_pcnt));\n\tif (!n->pf) {\n\t\terr = -ENOBUFS;\n\t\tgoto errfree;\n\t}\n#endif\n\n\tmemcpy(&n->sel, s, sel_size);\n\tRCU_INIT_POINTER(n->ht_up, ht);\n\tn->handle = handle;\n\tn->fshift = s->hmask ? ffs(ntohl(s->hmask)) - 1 : 0;\n\tn->flags = userflags;\n\n\terr = tcf_exts_init(&n->exts, net, TCA_U32_ACT, TCA_U32_POLICE);\n\tif (err < 0)\n\t\tgoto errout;\n\n#ifdef CONFIG_CLS_U32_MARK\n\tn->pcpu_success = alloc_percpu(u32);\n\tif (!n->pcpu_success) {\n\t\terr = -ENOMEM;\n\t\tgoto errout;\n\t}\n\n\tif (tb[TCA_U32_MARK]) {\n\t\tstruct tc_u32_mark *mark;\n\n\t\tmark = nla_data(tb[TCA_U32_MARK]);\n\t\tn->val = mark->val;\n\t\tn->mask = mark->mask;\n\t}\n#endif\n\n\terr = u32_set_parms(net, tp, base, n, tb, tca[TCA_RATE],\n\t\t\t    flags, n->flags, extack);\n\tif (err == 0) {\n\t\tstruct tc_u_knode __rcu **ins;\n\t\tstruct tc_u_knode *pins;\n\n\t\terr = u32_replace_hw_knode(tp, n, flags, extack);\n\t\tif (err)\n\t\t\tgoto errhw;\n\n\t\tif (!tc_in_hw(n->flags))\n\t\t\tn->flags |= TCA_CLS_FLAGS_NOT_IN_HW;\n\n\t\tins = &ht->ht[TC_U32_HASH(handle)];\n\t\tfor (pins = rtnl_dereference(*ins); pins;\n\t\t     ins = &pins->next, pins = rtnl_dereference(*ins))\n\t\t\tif (TC_U32_NODE(handle) < TC_U32_NODE(pins->handle))\n\t\t\t\tbreak;\n\n\t\tRCU_INIT_POINTER(n->next, pins);\n\t\trcu_assign_pointer(*ins, n);\n\t\ttp_c->knodes++;\n\t\t*arg = n;\n\t\treturn 0;\n\t}\n\nerrhw:\n#ifdef CONFIG_CLS_U32_MARK\n\tfree_percpu(n->pcpu_success);\n#endif\n\nerrout:\n\ttcf_exts_destroy(&n->exts);\n#ifdef CONFIG_CLS_U32_PERF\nerrfree:\n\tfree_percpu(n->pf);\n#endif\n\tkfree(n);\nerridr:\n\tidr_remove(&ht->handle_idr, handle);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -59,13 +59,13 @@\n \t\t\t\t    extack);\n \n \t\tif (err) {\n-\t\t\tu32_destroy_key(new, false);\n+\t\t\t__u32_destroy_key(new);\n \t\t\treturn err;\n \t\t}\n \n \t\terr = u32_replace_hw_knode(tp, new, flags, extack);\n \t\tif (err) {\n-\t\t\tu32_destroy_key(new, false);\n+\t\t\t__u32_destroy_key(new);\n \t\t\treturn err;\n \t\t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tu32_destroy_key(new, false);",
                "\t\t\tu32_destroy_key(new, false);"
            ],
            "added_lines": [
                "\t\t\t__u32_destroy_key(new);",
                "\t\t\t__u32_destroy_key(new);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1678",
        "func_name": "torvalds/linux/tcp_mtu_probe",
        "description": "An issue was discovered in the Linux Kernel from 4.18 to 4.19, an improper update of sock reference in TCP pacing can lead to memory/netns leak, which can be used by remote clients.",
        "git_url": "https://github.com/torvalds/linux/commit/0a70f118475e037732557796accd0878a00fc25a",
        "commit_title": "tcp: fix possible socket leaks in internal pacing mode",
        "commit_text": " This patch is addressing an issue in stable linux-4.19 only.  In linux-4.20, TCP stack adopted EDT (Earliest Departure Time) model and this issue was incidentally fixed.  Issue at hand was an extra sock_hold() from tcp_internal_pacing() in paths not using tcp_xmit_retransmit_queue()  Jason Xing reported this leak and provided a patch stopping the extra sock_hold() to happen.  This patch is more complete and makes sure to avoid unnecessary extra delays, by reprogramming the high resolution timer.  Reference: https://lore.kernel.org/all/CANn89i+7-wE4xr5D9DpH+N-xkL1SB8oVghCKgz+CT5eG1ODQhA@mail.gmail.com/ Cc: liweishi <liweishi@kuaishou.com> Cc: Shujin Li <lishujin@kuaishou.com> Cc: Neal Cardwell <ncardwell@google.com>",
        "func_before": "static int tcp_mtu_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *nskb, *next;\n\tstruct net *net = sock_net(sk);\n\tint probe_size;\n\tint size_needed;\n\tint copy, len;\n\tint mss_now;\n\tint interval;\n\n\t/* Not currently probing/verifying,\n\t * not in recovery,\n\t * have enough cwnd, and\n\t * not SACKing (the variable headers throw things off)\n\t */\n\tif (likely(!icsk->icsk_mtup.enabled ||\n\t\t   icsk->icsk_mtup.probe_size ||\n\t\t   inet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\n\t\t   tp->snd_cwnd < 11 ||\n\t\t   tp->rx_opt.num_sacks || tp->rx_opt.dsack))\n\t\treturn -1;\n\n\t/* Use binary search for probe_size between tcp_mss_base,\n\t * and current mss_clamp. if (search_high - search_low)\n\t * smaller than a threshold, backoff from probing.\n\t */\n\tmss_now = tcp_current_mss(sk);\n\tprobe_size = tcp_mtu_to_mss(sk, (icsk->icsk_mtup.search_high +\n\t\t\t\t    icsk->icsk_mtup.search_low) >> 1);\n\tsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\n\tinterval = icsk->icsk_mtup.search_high - icsk->icsk_mtup.search_low;\n\t/* When misfortune happens, we are reprobing actively,\n\t * and then reprobe timer has expired. We stick with current\n\t * probing process by not resetting search range to its orignal.\n\t */\n\tif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high) ||\n\t\tinterval < net->ipv4.sysctl_tcp_probe_threshold) {\n\t\t/* Check whether enough time has elaplased for\n\t\t * another round of probing.\n\t\t */\n\t\ttcp_mtu_check_reprobe(sk);\n\t\treturn -1;\n\t}\n\n\t/* Have enough data in the send queue to probe? */\n\tif (tp->write_seq - tp->snd_nxt < size_needed)\n\t\treturn -1;\n\n\tif (tp->snd_wnd < size_needed)\n\t\treturn -1;\n\tif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\n\t\treturn 0;\n\n\t/* Do we need to wait to drain cwnd? With none in flight, don't stall */\n\tif (tcp_packets_in_flight(tp) + 2 > tp->snd_cwnd) {\n\t\tif (!tcp_packets_in_flight(tp))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n\t\treturn -1;\n\n\t/* We're allowed to probe.  Build it now. */\n\tnskb = sk_stream_alloc_skb(sk, probe_size, GFP_ATOMIC, false);\n\tif (!nskb)\n\t\treturn -1;\n\tsk->sk_wmem_queued += nskb->truesize;\n\tsk_mem_charge(sk, nskb->truesize);\n\n\tskb = tcp_send_head(sk);\n\n\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\n\tTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\n\tTCP_SKB_CB(nskb)->tcp_flags = TCPHDR_ACK;\n\tTCP_SKB_CB(nskb)->sacked = 0;\n\tnskb->csum = 0;\n\tnskb->ip_summed = CHECKSUM_PARTIAL;\n\n\ttcp_insert_write_queue_before(nskb, skb, sk);\n\ttcp_highest_sack_replace(sk, skb, nskb);\n\n\tlen = 0;\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tcopy = min_t(int, skb->len, probe_size - len);\n\t\tskb_copy_bits(skb, 0, skb_put(nskb, copy), copy);\n\n\t\tif (skb->len <= copy) {\n\t\t\t/* We've eaten all the data from this skb.\n\t\t\t * Throw it away. */\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\t\t\t/* If this is the last SKB we copy and eor is set\n\t\t\t * we need to propagate it to the new skb.\n\t\t\t */\n\t\t\tTCP_SKB_CB(nskb)->eor = TCP_SKB_CB(skb)->eor;\n\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\n\t\t\ttcp_unlink_write_queue(skb, sk);\n\t\t\tsk_wmem_free_skb(sk, skb);\n\t\t} else {\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags &\n\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\n\t\t\tif (!skb_shinfo(skb)->nr_frags) {\n\t\t\t\tskb_pull(skb, copy);\n\t\t\t} else {\n\t\t\t\t__pskb_trim_head(skb, copy);\n\t\t\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\t\t}\n\t\t\tTCP_SKB_CB(skb)->seq += copy;\n\t\t}\n\n\t\tlen += copy;\n\n\t\tif (len >= probe_size)\n\t\t\tbreak;\n\t}\n\ttcp_init_tso_segs(nskb, nskb->len);\n\n\t/* We're ready to send.  If this fails, the probe will\n\t * be resegmented into mss-sized pieces by tcp_write_xmit().\n\t */\n\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\n\t\t/* Decrement cwnd here because we are sending\n\t\t * effectively two packets. */\n\t\ttp->snd_cwnd--;\n\t\ttcp_event_new_data_sent(sk, nskb);\n\n\t\ticsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\n\t\ttp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\n\t\ttp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\n\n\t\treturn 1;\n\t}\n\n\treturn -1;\n}",
        "func": "static int tcp_mtu_probe(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *nskb, *next;\n\tstruct net *net = sock_net(sk);\n\tint probe_size;\n\tint size_needed;\n\tint copy, len;\n\tint mss_now;\n\tint interval;\n\n\t/* Not currently probing/verifying,\n\t * not in recovery,\n\t * have enough cwnd, and\n\t * not SACKing (the variable headers throw things off)\n\t */\n\tif (likely(!icsk->icsk_mtup.enabled ||\n\t\t   icsk->icsk_mtup.probe_size ||\n\t\t   inet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\n\t\t   tp->snd_cwnd < 11 ||\n\t\t   tp->rx_opt.num_sacks || tp->rx_opt.dsack))\n\t\treturn -1;\n\n\t/* Use binary search for probe_size between tcp_mss_base,\n\t * and current mss_clamp. if (search_high - search_low)\n\t * smaller than a threshold, backoff from probing.\n\t */\n\tmss_now = tcp_current_mss(sk);\n\tprobe_size = tcp_mtu_to_mss(sk, (icsk->icsk_mtup.search_high +\n\t\t\t\t    icsk->icsk_mtup.search_low) >> 1);\n\tsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\n\tinterval = icsk->icsk_mtup.search_high - icsk->icsk_mtup.search_low;\n\t/* When misfortune happens, we are reprobing actively,\n\t * and then reprobe timer has expired. We stick with current\n\t * probing process by not resetting search range to its orignal.\n\t */\n\tif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high) ||\n\t\tinterval < net->ipv4.sysctl_tcp_probe_threshold) {\n\t\t/* Check whether enough time has elaplased for\n\t\t * another round of probing.\n\t\t */\n\t\ttcp_mtu_check_reprobe(sk);\n\t\treturn -1;\n\t}\n\n\t/* Have enough data in the send queue to probe? */\n\tif (tp->write_seq - tp->snd_nxt < size_needed)\n\t\treturn -1;\n\n\tif (tp->snd_wnd < size_needed)\n\t\treturn -1;\n\tif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\n\t\treturn 0;\n\n\t/* Do we need to wait to drain cwnd? With none in flight, don't stall */\n\tif (tcp_packets_in_flight(tp) + 2 > tp->snd_cwnd) {\n\t\tif (!tcp_packets_in_flight(tp))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n\t\treturn -1;\n\n\tif (tcp_pacing_check(sk))\n\t\treturn -1;\n\n\t/* We're allowed to probe.  Build it now. */\n\tnskb = sk_stream_alloc_skb(sk, probe_size, GFP_ATOMIC, false);\n\tif (!nskb)\n\t\treturn -1;\n\tsk->sk_wmem_queued += nskb->truesize;\n\tsk_mem_charge(sk, nskb->truesize);\n\n\tskb = tcp_send_head(sk);\n\n\tTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\n\tTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\n\tTCP_SKB_CB(nskb)->tcp_flags = TCPHDR_ACK;\n\tTCP_SKB_CB(nskb)->sacked = 0;\n\tnskb->csum = 0;\n\tnskb->ip_summed = CHECKSUM_PARTIAL;\n\n\ttcp_insert_write_queue_before(nskb, skb, sk);\n\ttcp_highest_sack_replace(sk, skb, nskb);\n\n\tlen = 0;\n\ttcp_for_write_queue_from_safe(skb, next, sk) {\n\t\tcopy = min_t(int, skb->len, probe_size - len);\n\t\tskb_copy_bits(skb, 0, skb_put(nskb, copy), copy);\n\n\t\tif (skb->len <= copy) {\n\t\t\t/* We've eaten all the data from this skb.\n\t\t\t * Throw it away. */\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags;\n\t\t\t/* If this is the last SKB we copy and eor is set\n\t\t\t * we need to propagate it to the new skb.\n\t\t\t */\n\t\t\tTCP_SKB_CB(nskb)->eor = TCP_SKB_CB(skb)->eor;\n\t\t\ttcp_skb_collapse_tstamp(nskb, skb);\n\t\t\ttcp_unlink_write_queue(skb, sk);\n\t\t\tsk_wmem_free_skb(sk, skb);\n\t\t} else {\n\t\t\tTCP_SKB_CB(nskb)->tcp_flags |= TCP_SKB_CB(skb)->tcp_flags &\n\t\t\t\t\t\t   ~(TCPHDR_FIN|TCPHDR_PSH);\n\t\t\tif (!skb_shinfo(skb)->nr_frags) {\n\t\t\t\tskb_pull(skb, copy);\n\t\t\t} else {\n\t\t\t\t__pskb_trim_head(skb, copy);\n\t\t\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\t\t}\n\t\t\tTCP_SKB_CB(skb)->seq += copy;\n\t\t}\n\n\t\tlen += copy;\n\n\t\tif (len >= probe_size)\n\t\t\tbreak;\n\t}\n\ttcp_init_tso_segs(nskb, nskb->len);\n\n\t/* We're ready to send.  If this fails, the probe will\n\t * be resegmented into mss-sized pieces by tcp_write_xmit().\n\t */\n\tif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\n\t\t/* Decrement cwnd here because we are sending\n\t\t * effectively two packets. */\n\t\ttp->snd_cwnd--;\n\t\ttcp_event_new_data_sent(sk, nskb);\n\n\t\ticsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\n\t\ttp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\n\t\ttp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\n\n\t\treturn 1;\n\t}\n\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -62,6 +62,9 @@\n \t}\n \n \tif (!tcp_can_coalesce_send_queue_head(sk, probe_size))\n+\t\treturn -1;\n+\n+\tif (tcp_pacing_check(sk))\n \t\treturn -1;\n \n \t/* We're allowed to probe.  Build it now. */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\treturn -1;",
                "",
                "\tif (tcp_pacing_check(sk))"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1678",
        "func_name": "torvalds/linux/tcp_internal_pacing",
        "description": "An issue was discovered in the Linux Kernel from 4.18 to 4.19, an improper update of sock reference in TCP pacing can lead to memory/netns leak, which can be used by remote clients.",
        "git_url": "https://github.com/torvalds/linux/commit/0a70f118475e037732557796accd0878a00fc25a",
        "commit_title": "tcp: fix possible socket leaks in internal pacing mode",
        "commit_text": " This patch is addressing an issue in stable linux-4.19 only.  In linux-4.20, TCP stack adopted EDT (Earliest Departure Time) model and this issue was incidentally fixed.  Issue at hand was an extra sock_hold() from tcp_internal_pacing() in paths not using tcp_xmit_retransmit_queue()  Jason Xing reported this leak and provided a patch stopping the extra sock_hold() to happen.  This patch is more complete and makes sure to avoid unnecessary extra delays, by reprogramming the high resolution timer.  Reference: https://lore.kernel.org/all/CANn89i+7-wE4xr5D9DpH+N-xkL1SB8oVghCKgz+CT5eG1ODQhA@mail.gmail.com/ Cc: liweishi <liweishi@kuaishou.com> Cc: Shujin Li <lishujin@kuaishou.com> Cc: Neal Cardwell <ncardwell@google.com>",
        "func_before": "static void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)\n{\n\tu64 len_ns;\n\tu32 rate;\n\n\tif (!tcp_needs_internal_pacing(sk))\n\t\treturn;\n\trate = sk->sk_pacing_rate;\n\tif (!rate || rate == ~0U)\n\t\treturn;\n\n\tlen_ns = (u64)skb->len * NSEC_PER_SEC;\n\tdo_div(len_ns, rate);\n\thrtimer_start(&tcp_sk(sk)->pacing_timer,\n\t\t      ktime_add_ns(ktime_get(), len_ns),\n\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n\tsock_hold(sk);\n}",
        "func": "static void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tktime_t expire, now;\n\tu64 len_ns;\n\tu32 rate;\n\n\tif (!tcp_needs_internal_pacing(sk))\n\t\treturn;\n\trate = sk->sk_pacing_rate;\n\tif (!rate || rate == ~0U)\n\t\treturn;\n\n\tlen_ns = (u64)skb->len * NSEC_PER_SEC;\n\tdo_div(len_ns, rate);\n\tnow = ktime_get();\n\t/* If hrtimer is already armed, then our caller has not\n\t * used tcp_pacing_check().\n\t */\n\tif (unlikely(hrtimer_is_queued(&tp->pacing_timer))) {\n\t\texpire = hrtimer_get_softexpires(&tp->pacing_timer);\n\t\tif (ktime_after(expire, now))\n\t\t\tnow = expire;\n\t\tif (hrtimer_try_to_cancel(&tp->pacing_timer) == 1)\n\t\t\t__sock_put(sk);\n\t}\n\thrtimer_start(&tp->pacing_timer, ktime_add_ns(now, len_ns),\n\t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n\tsock_hold(sk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n static void tcp_internal_pacing(struct sock *sk, const struct sk_buff *skb)\n {\n+\tstruct tcp_sock *tp = tcp_sk(sk);\n+\tktime_t expire, now;\n \tu64 len_ns;\n \tu32 rate;\n \n@@ -11,8 +13,18 @@\n \n \tlen_ns = (u64)skb->len * NSEC_PER_SEC;\n \tdo_div(len_ns, rate);\n-\thrtimer_start(&tcp_sk(sk)->pacing_timer,\n-\t\t      ktime_add_ns(ktime_get(), len_ns),\n+\tnow = ktime_get();\n+\t/* If hrtimer is already armed, then our caller has not\n+\t * used tcp_pacing_check().\n+\t */\n+\tif (unlikely(hrtimer_is_queued(&tp->pacing_timer))) {\n+\t\texpire = hrtimer_get_softexpires(&tp->pacing_timer);\n+\t\tif (ktime_after(expire, now))\n+\t\t\tnow = expire;\n+\t\tif (hrtimer_try_to_cancel(&tp->pacing_timer) == 1)\n+\t\t\t__sock_put(sk);\n+\t}\n+\thrtimer_start(&tp->pacing_timer, ktime_add_ns(now, len_ns),\n \t\t      HRTIMER_MODE_ABS_PINNED_SOFT);\n \tsock_hold(sk);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\thrtimer_start(&tcp_sk(sk)->pacing_timer,",
                "\t\t      ktime_add_ns(ktime_get(), len_ns),"
            ],
            "added_lines": [
                "\tstruct tcp_sock *tp = tcp_sk(sk);",
                "\tktime_t expire, now;",
                "\tnow = ktime_get();",
                "\t/* If hrtimer is already armed, then our caller has not",
                "\t * used tcp_pacing_check().",
                "\t */",
                "\tif (unlikely(hrtimer_is_queued(&tp->pacing_timer))) {",
                "\t\texpire = hrtimer_get_softexpires(&tp->pacing_timer);",
                "\t\tif (ktime_after(expire, now))",
                "\t\t\tnow = expire;",
                "\t\tif (hrtimer_try_to_cancel(&tp->pacing_timer) == 1)",
                "\t\t\t__sock_put(sk);",
                "\t}",
                "\thrtimer_start(&tp->pacing_timer, ktime_add_ns(now, len_ns),"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2019",
        "func_name": "torvalds/linux/nsim_fib_event_schedule_work",
        "description": "A flaw was found in the Linux kernel's netdevsim device driver, within the scheduling of events. This issue results from the improper management of a reference count. This may allow an attacker to create a denial of service condition on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/180a6a3ee60a7cb69ed1232388460644f6a21f00",
        "commit_title": "netdevsim: fib: Fix reference count leak on route deletion failure",
        "commit_text": " As part of FIB offload simulation, netdevsim stores IPv4 and IPv6 routes and holds a reference on FIB info structures that in turn hold a reference on the associated nexthop device(s).  In the unlikely case where we are unable to allocate memory to process a route deletion request, netdevsim will not release the reference from the associated FIB info structure, thereby preventing the associated nexthop device(s) from ever being removed [1].  Fix this by scheduling a work item that will flush netdevsim's FIB table upon route deletion failure. This will cause netdevsim to release its reference from all the FIB info structures in its table.  Reported by Lucas Leong of Trend Micro Zero Day Initiative. ",
        "func_before": "static int nsim_fib_event_schedule_work(struct nsim_fib_data *data,\n\t\t\t\t\tstruct fib_notifier_info *info,\n\t\t\t\t\tunsigned long event)\n{\n\tstruct nsim_fib_event *fib_event;\n\tint err;\n\n\tif (info->family != AF_INET && info->family != AF_INET6)\n\t\t/* netdevsim does not support 'RTNL_FAMILY_IP6MR' and\n\t\t * 'RTNL_FAMILY_IPMR' and should ignore them.\n\t\t */\n\t\treturn NOTIFY_DONE;\n\n\tfib_event = kzalloc(sizeof(*fib_event), GFP_ATOMIC);\n\tif (!fib_event)\n\t\treturn NOTIFY_BAD;\n\n\tfib_event->data = data;\n\tfib_event->event = event;\n\tfib_event->family = info->family;\n\n\tswitch (info->family) {\n\tcase AF_INET:\n\t\terr = nsim_fib4_prepare_event(info, fib_event, event);\n\t\tbreak;\n\tcase AF_INET6:\n\t\terr = nsim_fib6_prepare_event(info, fib_event, event);\n\t\tbreak;\n\t}\n\n\tif (err)\n\t\tgoto err_fib_prepare_event;\n\n\t/* Enqueue the event and trigger the work */\n\tspin_lock_bh(&data->fib_event_queue_lock);\n\tlist_add_tail(&fib_event->list, &data->fib_event_queue);\n\tspin_unlock_bh(&data->fib_event_queue_lock);\n\tschedule_work(&data->fib_event_work);\n\n\treturn NOTIFY_DONE;\n\nerr_fib_prepare_event:\n\tkfree(fib_event);\n\treturn NOTIFY_BAD;\n}",
        "func": "static int nsim_fib_event_schedule_work(struct nsim_fib_data *data,\n\t\t\t\t\tstruct fib_notifier_info *info,\n\t\t\t\t\tunsigned long event)\n{\n\tstruct nsim_fib_event *fib_event;\n\tint err;\n\n\tif (info->family != AF_INET && info->family != AF_INET6)\n\t\t/* netdevsim does not support 'RTNL_FAMILY_IP6MR' and\n\t\t * 'RTNL_FAMILY_IPMR' and should ignore them.\n\t\t */\n\t\treturn NOTIFY_DONE;\n\n\tfib_event = kzalloc(sizeof(*fib_event), GFP_ATOMIC);\n\tif (!fib_event)\n\t\tgoto err_fib_event_alloc;\n\n\tfib_event->data = data;\n\tfib_event->event = event;\n\tfib_event->family = info->family;\n\n\tswitch (info->family) {\n\tcase AF_INET:\n\t\terr = nsim_fib4_prepare_event(info, fib_event, event);\n\t\tbreak;\n\tcase AF_INET6:\n\t\terr = nsim_fib6_prepare_event(info, fib_event, event);\n\t\tbreak;\n\t}\n\n\tif (err)\n\t\tgoto err_fib_prepare_event;\n\n\t/* Enqueue the event and trigger the work */\n\tspin_lock_bh(&data->fib_event_queue_lock);\n\tlist_add_tail(&fib_event->list, &data->fib_event_queue);\n\tspin_unlock_bh(&data->fib_event_queue_lock);\n\tschedule_work(&data->fib_event_work);\n\n\treturn NOTIFY_DONE;\n\nerr_fib_prepare_event:\n\tkfree(fib_event);\nerr_fib_event_alloc:\n\tif (event == FIB_EVENT_ENTRY_DEL)\n\t\tschedule_work(&data->fib_flush_work);\n\treturn NOTIFY_BAD;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \n \tfib_event = kzalloc(sizeof(*fib_event), GFP_ATOMIC);\n \tif (!fib_event)\n-\t\treturn NOTIFY_BAD;\n+\t\tgoto err_fib_event_alloc;\n \n \tfib_event->data = data;\n \tfib_event->event = event;\n@@ -41,5 +41,8 @@\n \n err_fib_prepare_event:\n \tkfree(fib_event);\n+err_fib_event_alloc:\n+\tif (event == FIB_EVENT_ENTRY_DEL)\n+\t\tschedule_work(&data->fib_flush_work);\n \treturn NOTIFY_BAD;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn NOTIFY_BAD;"
            ],
            "added_lines": [
                "\t\tgoto err_fib_event_alloc;",
                "err_fib_event_alloc:",
                "\tif (event == FIB_EVENT_ENTRY_DEL)",
                "\t\tschedule_work(&data->fib_flush_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2019",
        "func_name": "torvalds/linux/nsim_fib_create",
        "description": "A flaw was found in the Linux kernel's netdevsim device driver, within the scheduling of events. This issue results from the improper management of a reference count. This may allow an attacker to create a denial of service condition on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/180a6a3ee60a7cb69ed1232388460644f6a21f00",
        "commit_title": "netdevsim: fib: Fix reference count leak on route deletion failure",
        "commit_text": " As part of FIB offload simulation, netdevsim stores IPv4 and IPv6 routes and holds a reference on FIB info structures that in turn hold a reference on the associated nexthop device(s).  In the unlikely case where we are unable to allocate memory to process a route deletion request, netdevsim will not release the reference from the associated FIB info structure, thereby preventing the associated nexthop device(s) from ever being removed [1].  Fix this by scheduling a work item that will flush netdevsim's FIB table upon route deletion failure. This will cause netdevsim to release its reference from all the FIB info structures in its table.  Reported by Lucas Leong of Trend Micro Zero Day Initiative. ",
        "func_before": "struct nsim_fib_data *nsim_fib_create(struct devlink *devlink,\n\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct nsim_fib_data *data;\n\tstruct nsim_dev *nsim_dev;\n\tint err;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn ERR_PTR(-ENOMEM);\n\tdata->devlink = devlink;\n\n\tnsim_dev = devlink_priv(devlink);\n\terr = nsim_fib_debugfs_init(data, nsim_dev);\n\tif (err)\n\t\tgoto err_data_free;\n\n\tmutex_init(&data->nh_lock);\n\terr = rhashtable_init(&data->nexthop_ht, &nsim_nexthop_ht_params);\n\tif (err)\n\t\tgoto err_debugfs_exit;\n\n\tmutex_init(&data->fib_lock);\n\tINIT_LIST_HEAD(&data->fib_rt_list);\n\terr = rhashtable_init(&data->fib_rt_ht, &nsim_fib_rt_ht_params);\n\tif (err)\n\t\tgoto err_rhashtable_nexthop_destroy;\n\n\tINIT_WORK(&data->fib_event_work, nsim_fib_event_work);\n\tINIT_LIST_HEAD(&data->fib_event_queue);\n\tspin_lock_init(&data->fib_event_queue_lock);\n\n\tnsim_fib_set_max_all(data, devlink);\n\n\tdata->nexthop_nb.notifier_call = nsim_nexthop_event_nb;\n\terr = register_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb,\n\t\t\t\t\textack);\n\tif (err) {\n\t\tpr_err(\"Failed to register nexthop notifier\\n\");\n\t\tgoto err_rhashtable_fib_destroy;\n\t}\n\n\tdata->fib_nb.notifier_call = nsim_fib_event_nb;\n\terr = register_fib_notifier(devlink_net(devlink), &data->fib_nb,\n\t\t\t\t    nsim_fib_dump_inconsistent, extack);\n\tif (err) {\n\t\tpr_err(\"Failed to register fib notifier\\n\");\n\t\tgoto err_nexthop_nb_unregister;\n\t}\n\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV4_FIB,\n\t\t\t\t\t  nsim_fib_ipv4_resource_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV4_FIB_RULES,\n\t\t\t\t\t  nsim_fib_ipv4_rules_res_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV6_FIB,\n\t\t\t\t\t  nsim_fib_ipv6_resource_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV6_FIB_RULES,\n\t\t\t\t\t  nsim_fib_ipv6_rules_res_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_NEXTHOPS,\n\t\t\t\t\t  nsim_fib_nexthops_res_occ_get,\n\t\t\t\t\t  data);\n\treturn data;\n\nerr_nexthop_nb_unregister:\n\tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\nerr_rhashtable_fib_destroy:\n\tflush_work(&data->fib_event_work);\n\trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n\t\t\t\t    data);\nerr_rhashtable_nexthop_destroy:\n\trhashtable_free_and_destroy(&data->nexthop_ht, nsim_nexthop_free,\n\t\t\t\t    data);\n\tmutex_destroy(&data->fib_lock);\nerr_debugfs_exit:\n\tmutex_destroy(&data->nh_lock);\n\tnsim_fib_debugfs_exit(data);\nerr_data_free:\n\tkfree(data);\n\treturn ERR_PTR(err);\n}",
        "func": "struct nsim_fib_data *nsim_fib_create(struct devlink *devlink,\n\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct nsim_fib_data *data;\n\tstruct nsim_dev *nsim_dev;\n\tint err;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn ERR_PTR(-ENOMEM);\n\tdata->devlink = devlink;\n\n\tnsim_dev = devlink_priv(devlink);\n\terr = nsim_fib_debugfs_init(data, nsim_dev);\n\tif (err)\n\t\tgoto err_data_free;\n\n\tmutex_init(&data->nh_lock);\n\terr = rhashtable_init(&data->nexthop_ht, &nsim_nexthop_ht_params);\n\tif (err)\n\t\tgoto err_debugfs_exit;\n\n\tmutex_init(&data->fib_lock);\n\tINIT_LIST_HEAD(&data->fib_rt_list);\n\terr = rhashtable_init(&data->fib_rt_ht, &nsim_fib_rt_ht_params);\n\tif (err)\n\t\tgoto err_rhashtable_nexthop_destroy;\n\n\tINIT_WORK(&data->fib_event_work, nsim_fib_event_work);\n\tINIT_WORK(&data->fib_flush_work, nsim_fib_flush_work);\n\tINIT_LIST_HEAD(&data->fib_event_queue);\n\tspin_lock_init(&data->fib_event_queue_lock);\n\n\tnsim_fib_set_max_all(data, devlink);\n\n\tdata->nexthop_nb.notifier_call = nsim_nexthop_event_nb;\n\terr = register_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb,\n\t\t\t\t\textack);\n\tif (err) {\n\t\tpr_err(\"Failed to register nexthop notifier\\n\");\n\t\tgoto err_rhashtable_fib_destroy;\n\t}\n\n\tdata->fib_nb.notifier_call = nsim_fib_event_nb;\n\terr = register_fib_notifier(devlink_net(devlink), &data->fib_nb,\n\t\t\t\t    nsim_fib_dump_inconsistent, extack);\n\tif (err) {\n\t\tpr_err(\"Failed to register fib notifier\\n\");\n\t\tgoto err_nexthop_nb_unregister;\n\t}\n\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV4_FIB,\n\t\t\t\t\t  nsim_fib_ipv4_resource_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV4_FIB_RULES,\n\t\t\t\t\t  nsim_fib_ipv4_rules_res_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV6_FIB,\n\t\t\t\t\t  nsim_fib_ipv6_resource_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_IPV6_FIB_RULES,\n\t\t\t\t\t  nsim_fib_ipv6_rules_res_occ_get,\n\t\t\t\t\t  data);\n\tdevlink_resource_occ_get_register(devlink,\n\t\t\t\t\t  NSIM_RESOURCE_NEXTHOPS,\n\t\t\t\t\t  nsim_fib_nexthops_res_occ_get,\n\t\t\t\t\t  data);\n\treturn data;\n\nerr_nexthop_nb_unregister:\n\tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\nerr_rhashtable_fib_destroy:\n\tcancel_work_sync(&data->fib_flush_work);\n\tflush_work(&data->fib_event_work);\n\trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n\t\t\t\t    data);\nerr_rhashtable_nexthop_destroy:\n\trhashtable_free_and_destroy(&data->nexthop_ht, nsim_nexthop_free,\n\t\t\t\t    data);\n\tmutex_destroy(&data->fib_lock);\nerr_debugfs_exit:\n\tmutex_destroy(&data->nh_lock);\n\tnsim_fib_debugfs_exit(data);\nerr_data_free:\n\tkfree(data);\n\treturn ERR_PTR(err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,7 @@\n \t\tgoto err_rhashtable_nexthop_destroy;\n \n \tINIT_WORK(&data->fib_event_work, nsim_fib_event_work);\n+\tINIT_WORK(&data->fib_flush_work, nsim_fib_flush_work);\n \tINIT_LIST_HEAD(&data->fib_event_queue);\n \tspin_lock_init(&data->fib_event_queue_lock);\n \n@@ -73,6 +74,7 @@\n err_nexthop_nb_unregister:\n \tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\n err_rhashtable_fib_destroy:\n+\tcancel_work_sync(&data->fib_flush_work);\n \tflush_work(&data->fib_event_work);\n \trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n \t\t\t\t    data);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tINIT_WORK(&data->fib_flush_work, nsim_fib_flush_work);",
                "\tcancel_work_sync(&data->fib_flush_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2019",
        "func_name": "torvalds/linux/nsim_fib_destroy",
        "description": "A flaw was found in the Linux kernel's netdevsim device driver, within the scheduling of events. This issue results from the improper management of a reference count. This may allow an attacker to create a denial of service condition on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/180a6a3ee60a7cb69ed1232388460644f6a21f00",
        "commit_title": "netdevsim: fib: Fix reference count leak on route deletion failure",
        "commit_text": " As part of FIB offload simulation, netdevsim stores IPv4 and IPv6 routes and holds a reference on FIB info structures that in turn hold a reference on the associated nexthop device(s).  In the unlikely case where we are unable to allocate memory to process a route deletion request, netdevsim will not release the reference from the associated FIB info structure, thereby preventing the associated nexthop device(s) from ever being removed [1].  Fix this by scheduling a work item that will flush netdevsim's FIB table upon route deletion failure. This will cause netdevsim to release its reference from all the FIB info structures in its table.  Reported by Lucas Leong of Trend Micro Zero Day Initiative. ",
        "func_before": "void nsim_fib_destroy(struct devlink *devlink, struct nsim_fib_data *data)\n{\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_NEXTHOPS);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV6_FIB_RULES);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV6_FIB);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV4_FIB_RULES);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV4_FIB);\n\tunregister_fib_notifier(devlink_net(devlink), &data->fib_nb);\n\tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\n\tflush_work(&data->fib_event_work);\n\trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n\t\t\t\t    data);\n\trhashtable_free_and_destroy(&data->nexthop_ht, nsim_nexthop_free,\n\t\t\t\t    data);\n\tWARN_ON_ONCE(!list_empty(&data->fib_event_queue));\n\tWARN_ON_ONCE(!list_empty(&data->fib_rt_list));\n\tmutex_destroy(&data->fib_lock);\n\tmutex_destroy(&data->nh_lock);\n\tnsim_fib_debugfs_exit(data);\n\tkfree(data);\n}",
        "func": "void nsim_fib_destroy(struct devlink *devlink, struct nsim_fib_data *data)\n{\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_NEXTHOPS);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV6_FIB_RULES);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV6_FIB);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV4_FIB_RULES);\n\tdevlink_resource_occ_get_unregister(devlink,\n\t\t\t\t\t    NSIM_RESOURCE_IPV4_FIB);\n\tunregister_fib_notifier(devlink_net(devlink), &data->fib_nb);\n\tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\n\tcancel_work_sync(&data->fib_flush_work);\n\tflush_work(&data->fib_event_work);\n\trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n\t\t\t\t    data);\n\trhashtable_free_and_destroy(&data->nexthop_ht, nsim_nexthop_free,\n\t\t\t\t    data);\n\tWARN_ON_ONCE(!list_empty(&data->fib_event_queue));\n\tWARN_ON_ONCE(!list_empty(&data->fib_rt_list));\n\tmutex_destroy(&data->fib_lock);\n\tmutex_destroy(&data->nh_lock);\n\tnsim_fib_debugfs_exit(data);\n\tkfree(data);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,7 @@\n \t\t\t\t\t    NSIM_RESOURCE_IPV4_FIB);\n \tunregister_fib_notifier(devlink_net(devlink), &data->fib_nb);\n \tunregister_nexthop_notifier(devlink_net(devlink), &data->nexthop_nb);\n+\tcancel_work_sync(&data->fib_flush_work);\n \tflush_work(&data->fib_event_work);\n \trhashtable_free_and_destroy(&data->fib_rt_ht, nsim_fib_rt_free,\n \t\t\t\t    data);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcancel_work_sync(&data->fib_flush_work);"
            ]
        }
    }
]