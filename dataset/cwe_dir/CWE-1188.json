[
    {
        "cve_id": "CVE-2022-40468",
        "func_name": "tinyproxy/process_request",
        "description": "Potential leak of left-over heap data if custom error page templates containing special non-standard variables are used. Tinyproxy commit 84f203f and earlier use uninitialized buffers in process_request() function.",
        "git_url": "https://github.com/tinyproxy/tinyproxy/commit/3764b8551463b900b5b4e3ec0cd9bb9182191cb7",
        "commit_title": "prevent junk from showing up in error page in invalid requests",
        "commit_text": " fixes #457",
        "func_before": "static struct request_s *process_request (struct conn_s *connptr,\n                                          orderedmap hashofheaders)\n{\n        char *url;\n        struct request_s *request;\n        int ret, skip_trans;\n        size_t request_len;\n\n        skip_trans = 0;\n\n        /* NULL out all the fields so frees don't cause segfaults. */\n        request =\n            (struct request_s *) safecalloc (1, sizeof (struct request_s));\n        if (!request)\n                return NULL;\n\n        request_len = strlen (connptr->request_line) + 1;\n\n        request->method = (char *) safemalloc (request_len);\n        url = (char *) safemalloc (request_len);\n        request->protocol = (char *) safemalloc (request_len);\n\n        if (!request->method || !url || !request->protocol) {\n                goto fail;\n        }\n\n        ret = sscanf (connptr->request_line, \"%[^ ] %[^ ] %[^ ]\",\n                      request->method, url, request->protocol);\n        if (ret == 2 && !strcasecmp (request->method, \"GET\")) {\n                request->protocol[0] = 0;\n\n                /* Indicate that this is a HTTP/0.9 GET request */\n                connptr->protocol.major = 0;\n                connptr->protocol.minor = 9;\n        } else if (ret == 3 && !strncasecmp (request->protocol, \"HTTP/\", 5)) {\n                /*\n                 * Break apart the protocol and update the connection\n                 * structure.\n                 */\n                ret = sscanf (request->protocol + 5, \"%u.%u\",\n                              &connptr->protocol.major,\n                              &connptr->protocol.minor);\n\n                /*\n                 * If the conversion doesn't succeed, drop down below and\n                 * send the error to the user.\n                 */\n                if (ret != 2)\n                        goto BAD_REQUEST_ERROR;\n        } else {\nBAD_REQUEST_ERROR:\n                log_message (LOG_ERR,\n                             \"process_request: Bad Request on file descriptor %d\",\n                             connptr->client_fd);\n                indicate_http_error (connptr, 400, \"Bad Request\",\n                                     \"detail\", \"Request has an invalid format\",\n                                     \"url\", url, NULL);\n                goto fail;\n        }\n\n#ifdef REVERSE_SUPPORT\n        if (config->reversepath_list != NULL) {\n                /*\n                 * Rewrite the URL based on the reverse path.  After calling\n                 * reverse_rewrite_url \"url\" can be freed since we either\n                 * have the newly rewritten URL, or something failed and\n                 * we'll be closing anyway.\n                 */\n                char *reverse_url;\n                int reverse_status;\n\n                reverse_url = reverse_rewrite_url (connptr, hashofheaders, url, &reverse_status);\n\n                if (reverse_url != NULL) {\n                        if (reverse_status == 301) {\n                                char buf[PATH_MAX];\n                                snprintf (buf, sizeof buf, \"Location: %s\\r\\n\", reverse_url);\n                                send_http_headers (connptr, 301, \"Moved Permanently\", buf);\n                                goto fail;\n                        }\n                        safefree (url);\n                        url = reverse_url;\n                        skip_trans = 1;\n                } else if (config->reverseonly) {\n                        log_message (LOG_ERR,\n                                     \"Bad request, no mapping for '%s' found\",\n                                     url);\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"No mapping found for \"\n                                             \"requested url\", \"url\", url, NULL);\n                        goto fail;\n                }\n        }\n#endif\n\n        if (strncasecmp (url, \"http://\", 7) == 0\n            || (UPSTREAM_CONFIGURED () && strncasecmp (url, \"ftp://\", 6) == 0))\n        {\n                char *skipped_type = strstr (url, \"//\") + 2;\n\n                if (extract_url (skipped_type, HTTP_PORT, request) < 0) {\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"Could not parse URL\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n        } else if (strcmp (request->method, \"CONNECT\") == 0) {\n                if (extract_url (url, HTTP_PORT_SSL, request) < 0) {\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"Could not parse URL\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n\n                /* Verify that the port in the CONNECT method is allowed */\n                if (!check_allowed_connect_ports (request->port,\n                                                  config->connect_ports))\n                {\n                        indicate_http_error (connptr, 403, \"Access violation\",\n                                             \"detail\",\n                                             \"The CONNECT method not allowed \"\n                                             \"with the port you tried to use.\",\n                                             \"url\", url, NULL);\n                        log_message (LOG_INFO,\n                                     \"Refused CONNECT method on port %d\",\n                                     request->port);\n                        goto fail;\n                }\n\n                connptr->connect_method = TRUE;\n        } else {\n#ifdef TRANSPARENT_PROXY\n                if (!skip_trans) {\n                        if (!do_transparent_proxy\n                            (connptr, hashofheaders, request, config, &url))\n                                goto fail;\n                } else\n#endif\n                {\n                indicate_http_error (connptr, 501, \"Not Implemented\",\n                                     \"detail\",\n                                     \"Unknown method or unsupported protocol.\",\n                                     \"url\", url, NULL);\n                log_message (LOG_INFO, \"Unknown method (%s) or protocol (%s)\",\n                             request->method, url);\n                goto fail;\n                }\n        }\n\n#ifdef FILTER_ENABLE\n        /*\n         * Filter restricted domains/urls\n         */\n        if (config->filter) {\n                int fu = config->filter_opts & FILTER_OPT_URL;\n                ret = filter_run (fu ? url : request->host);\n\n                if (ret) {\n                        update_stats (STAT_DENIED);\n\n                        log_message (LOG_NOTICE,\n                                     \"Proxying refused on filtered %s \\\"%s\\\"\",\n                                     fu ? \"url\" : \"domain\",\n                                     fu ? url : request->host);\n\n                        indicate_http_error (connptr, 403, \"Filtered\",\n                                             \"detail\",\n                                             \"The request you made has been filtered\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n        }\n#endif\n\n\n        /*\n         * Check to see if they're requesting the stat host\n         */\n        if (config->stathost && strcmp (config->stathost, request->host) == 0) {\n                log_message (LOG_NOTICE, \"Request for the stathost.\");\n                connptr->show_stats = TRUE;\n                goto fail;\n        }\n\n        safefree (url);\n\n        return request;\n\nfail:\n        safefree (url);\n        free_request_struct (request);\n        return NULL;\n}",
        "func": "static struct request_s *process_request (struct conn_s *connptr,\n                                          orderedmap hashofheaders)\n{\n        char *url;\n        struct request_s *request;\n        int ret, skip_trans;\n        size_t request_len;\n\n        skip_trans = 0;\n\n        /* NULL out all the fields so frees don't cause segfaults. */\n        request =\n            (struct request_s *) safecalloc (1, sizeof (struct request_s));\n        if (!request)\n                return NULL;\n\n        request_len = strlen (connptr->request_line) + 1;\n\n        request->method = (char *) safemalloc (request_len);\n        url = (char *) safemalloc (request_len);\n        request->protocol = (char *) safemalloc (request_len);\n\n        if (!request->method || !url || !request->protocol) {\n                goto fail;\n        }\n\n        /* zero-terminate the strings so they don't contain junk in error page */\n        request->method[0] = url[0] = request->protocol[0] = 0;\n\n        ret = sscanf (connptr->request_line, \"%[^ ] %[^ ] %[^ ]\",\n                      request->method, url, request->protocol);\n\n        if (ret == 2 && !strcasecmp (request->method, \"GET\")) {\n                request->protocol[0] = 0;\n\n                /* Indicate that this is a HTTP/0.9 GET request */\n                connptr->protocol.major = 0;\n                connptr->protocol.minor = 9;\n        } else if (ret == 3 && !strncasecmp (request->protocol, \"HTTP/\", 5)) {\n                /*\n                 * Break apart the protocol and update the connection\n                 * structure.\n                 */\n                ret = sscanf (request->protocol + 5, \"%u.%u\",\n                              &connptr->protocol.major,\n                              &connptr->protocol.minor);\n\n                /*\n                 * If the conversion doesn't succeed, drop down below and\n                 * send the error to the user.\n                 */\n                if (ret != 2)\n                        goto BAD_REQUEST_ERROR;\n        } else {\nBAD_REQUEST_ERROR:\n                log_message (LOG_ERR,\n                             \"process_request: Bad Request on file descriptor %d\",\n                             connptr->client_fd);\n                indicate_http_error (connptr, 400, \"Bad Request\",\n                                     \"detail\", \"Request has an invalid format\",\n                                     \"url\", url, NULL);\n                goto fail;\n        }\n\n#ifdef REVERSE_SUPPORT\n        if (config->reversepath_list != NULL) {\n                /*\n                 * Rewrite the URL based on the reverse path.  After calling\n                 * reverse_rewrite_url \"url\" can be freed since we either\n                 * have the newly rewritten URL, or something failed and\n                 * we'll be closing anyway.\n                 */\n                char *reverse_url;\n                int reverse_status;\n\n                reverse_url = reverse_rewrite_url (connptr, hashofheaders, url, &reverse_status);\n\n                if (reverse_url != NULL) {\n                        if (reverse_status == 301) {\n                                char buf[PATH_MAX];\n                                snprintf (buf, sizeof buf, \"Location: %s\\r\\n\", reverse_url);\n                                send_http_headers (connptr, 301, \"Moved Permanently\", buf);\n                                goto fail;\n                        }\n                        safefree (url);\n                        url = reverse_url;\n                        skip_trans = 1;\n                } else if (config->reverseonly) {\n                        log_message (LOG_ERR,\n                                     \"Bad request, no mapping for '%s' found\",\n                                     url);\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"No mapping found for \"\n                                             \"requested url\", \"url\", url, NULL);\n                        goto fail;\n                }\n        }\n#endif\n\n        if (strncasecmp (url, \"http://\", 7) == 0\n            || (UPSTREAM_CONFIGURED () && strncasecmp (url, \"ftp://\", 6) == 0))\n        {\n                char *skipped_type = strstr (url, \"//\") + 2;\n\n                if (extract_url (skipped_type, HTTP_PORT, request) < 0) {\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"Could not parse URL\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n        } else if (strcmp (request->method, \"CONNECT\") == 0) {\n                if (extract_url (url, HTTP_PORT_SSL, request) < 0) {\n                        indicate_http_error (connptr, 400, \"Bad Request\",\n                                             \"detail\", \"Could not parse URL\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n\n                /* Verify that the port in the CONNECT method is allowed */\n                if (!check_allowed_connect_ports (request->port,\n                                                  config->connect_ports))\n                {\n                        indicate_http_error (connptr, 403, \"Access violation\",\n                                             \"detail\",\n                                             \"The CONNECT method not allowed \"\n                                             \"with the port you tried to use.\",\n                                             \"url\", url, NULL);\n                        log_message (LOG_INFO,\n                                     \"Refused CONNECT method on port %d\",\n                                     request->port);\n                        goto fail;\n                }\n\n                connptr->connect_method = TRUE;\n        } else {\n#ifdef TRANSPARENT_PROXY\n                if (!skip_trans) {\n                        if (!do_transparent_proxy\n                            (connptr, hashofheaders, request, config, &url))\n                                goto fail;\n                } else\n#endif\n                {\n                indicate_http_error (connptr, 501, \"Not Implemented\",\n                                     \"detail\",\n                                     \"Unknown method or unsupported protocol.\",\n                                     \"url\", url, NULL);\n                log_message (LOG_INFO, \"Unknown method (%s) or protocol (%s)\",\n                             request->method, url);\n                goto fail;\n                }\n        }\n\n#ifdef FILTER_ENABLE\n        /*\n         * Filter restricted domains/urls\n         */\n        if (config->filter) {\n                int fu = config->filter_opts & FILTER_OPT_URL;\n                ret = filter_run (fu ? url : request->host);\n\n                if (ret) {\n                        update_stats (STAT_DENIED);\n\n                        log_message (LOG_NOTICE,\n                                     \"Proxying refused on filtered %s \\\"%s\\\"\",\n                                     fu ? \"url\" : \"domain\",\n                                     fu ? url : request->host);\n\n                        indicate_http_error (connptr, 403, \"Filtered\",\n                                             \"detail\",\n                                             \"The request you made has been filtered\",\n                                             \"url\", url, NULL);\n                        goto fail;\n                }\n        }\n#endif\n\n\n        /*\n         * Check to see if they're requesting the stat host\n         */\n        if (config->stathost && strcmp (config->stathost, request->host) == 0) {\n                log_message (LOG_NOTICE, \"Request for the stathost.\");\n                connptr->show_stats = TRUE;\n                goto fail;\n        }\n\n        safefree (url);\n\n        return request;\n\nfail:\n        safefree (url);\n        free_request_struct (request);\n        return NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,8 +24,12 @@\n                 goto fail;\n         }\n \n+        /* zero-terminate the strings so they don't contain junk in error page */\n+        request->method[0] = url[0] = request->protocol[0] = 0;\n+\n         ret = sscanf (connptr->request_line, \"%[^ ] %[^ ] %[^ ]\",\n                       request->method, url, request->protocol);\n+\n         if (ret == 2 && !strcasecmp (request->method, \"GET\")) {\n                 request->protocol[0] = 0;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /* zero-terminate the strings so they don't contain junk in error page */",
                "        request->method[0] = url[0] = request->protocol[0] = 0;",
                "",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-2196",
        "func_name": "torvalds/linux/nested_vmx_vmexit",
        "description": "A regression exists in the Linux Kernel within KVM: nVMX that allowed for speculative execution attacks. L2 can carry out Spectre v2 attacks on L1 due to L1 thinking it doesn't need retpolines or IBPB after running L2 due to KVM (L0) advertising eIBRS support to L1. An attacker at L2 with code execution can execute code on an indirect branch on the host machine. We recommend upgrading to Kernel 6.2 or past commit 2e7eab81425a\n",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=2e7eab81425ad6c875f2ed47c0ce01e78afc38a5",
        "commit_title": "According to Intel's document on Indirect Branch Restricted",
        "commit_text": "Speculation, \"Enabling IBRS does not prevent software from controlling the predicted targets of indirect branches of unrelated software executed later at the same predictor mode (for example, between two different user applications, or two different virtual machines). Such isolation can be ensured through use of the Indirect Branch Predictor Barrier (IBPB) command.\" This applies to both basic and enhanced IBRS.  Since L1 and L2 VMs share hardware predictor modes (guest-user and guest-kernel), hardware IBRS is not sufficient to virtualize IBRS. (The way that basic IBRS is implemented on pre-eIBRS parts, hardware IBRS is actually sufficient in practice, even though it isn't sufficient architecturally.)  For virtual CPUs that support IBRS, add an indirect branch prediction barrier on emulated VM-exit, to ensure that the predicted targets of indirect branches executed in L1 cannot be controlled by software that was executed in L2.  Since we typically don't intercept guest writes to IA32_SPEC_CTRL, perform the IBPB at emulated VM-exit regardless of the current IA32_SPEC_CTRL.IBRS value, even though the IBPB could technically be deferred until L1 sets IA32_SPEC_CTRL.IBRS, if IA32_SPEC_CTRL.IBRS is clear at emulated VM-exit.  This is CVE-2022-2196.  Cc: Sean Christopherson <seanjc@google.com> Link: https://lore.kernel.org/r/20221019213620.1953281-3-jmattson@google.com ",
        "func_before": "void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,\n\t\t       u32 exit_intr_info, unsigned long exit_qualification)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t/* Pending MTF traps are discarded on VM-Exit. */\n\tvmx->nested.mtf_pending = false;\n\n\t/* trying to cancel vmlaunch/vmresume is a bug */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t/*\n\t\t * KVM_REQ_GET_NESTED_STATE_PAGES is also used to map\n\t\t * Enlightened VMCS after migration and we still need to\n\t\t * do that when something is forcing L2->L1 exit prior to\n\t\t * the first L2 run.\n\t\t */\n\t\t(void)nested_get_evmcs_page(vcpu);\n\t}\n\n\t/* Service pending TLB flush requests for L2 before switching to L1. */\n\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\t/*\n\t * VCPU_EXREG_PDPTR will be clobbered in arch/x86/kvm/vmx/vmx.h between\n\t * now and the new vmentry.  Ensure that the VMCS02 PDPTR fields are\n\t * up-to-date before switching to L1.\n\t */\n\tif (enable_ept && is_pae_paging(vcpu))\n\t\tvmx_ept_load_pdptrs(vcpu);\n\n\tleave_guest_mode(vcpu);\n\n\tif (nested_cpu_has_preemption_timer(vmcs12))\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING)) {\n\t\tvcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;\n\t\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))\n\t\t\tvcpu->arch.tsc_scaling_ratio = vcpu->arch.l1_tsc_scaling_ratio;\n\t}\n\n\tif (likely(!vmx->fail)) {\n\t\tsync_vmcs02_to_vmcs12(vcpu, vmcs12);\n\n\t\tif (vm_exit_reason != -1)\n\t\t\tprepare_vmcs12(vcpu, vmcs12, vm_exit_reason,\n\t\t\t\t       exit_intr_info, exit_qualification);\n\n\t\t/*\n\t\t * Must happen outside of sync_vmcs02_to_vmcs12() as it will\n\t\t * also be used to capture vmcs12 cache as part of\n\t\t * capturing nVMX state for snapshot (migration).\n\t\t *\n\t\t * Otherwise, this flush will dirty guest memory at a\n\t\t * point it is already assumed by user-space to be\n\t\t * immutable.\n\t\t */\n\t\tnested_flush_cached_shadow_vmcs12(vcpu, vmcs12);\n\t} else {\n\t\t/*\n\t\t * The only expected VM-instruction error is \"VM entry with\n\t\t * invalid control field(s).\" Anything else indicates a\n\t\t * problem with L0.  And we should never get here with a\n\t\t * VMFail of any type if early consistency checks are enabled.\n\t\t */\n\t\tWARN_ON_ONCE(vmcs_read32(VM_INSTRUCTION_ERROR) !=\n\t\t\t     VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\t\tWARN_ON_ONCE(nested_early_check);\n\t}\n\n\t/*\n\t * Drop events/exceptions that were queued for re-injection to L2\n\t * (picked up via vmx_complete_interrupts()), as well as exceptions\n\t * that were pending for L2.  Note, this must NOT be hoisted above\n\t * prepare_vmcs12(), events/exceptions queued for re-injection need to\n\t * be captured in vmcs12 (see vmcs12_save_pending_event()).\n\t */\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\n\t/* Update any VMCS fields that might have changed while L2 ran */\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (kvm_caps.has_tsc_control)\n\t\tvmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);\n\n\tif (vmx->nested.l1_tpr_threshold != -1)\n\t\tvmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);\n\n\tif (vmx->nested.change_vmcs01_virtual_apic_mode) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = false;\n\t\tvmx_set_virtual_apic_mode(vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_cpu_dirty_logging) {\n\t\tvmx->nested.update_vmcs01_cpu_dirty_logging = false;\n\t\tvmx_update_cpu_dirty_logging(vcpu);\n\t}\n\n\t/* Unpin physical memory we referred to in vmcs02 */\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map, false);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map, true);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map, true);\n\tvmx->nested.pi_desc = NULL;\n\n\tif (vmx->nested.reload_vmcs01_apic_access_page) {\n\t\tvmx->nested.reload_vmcs01_apic_access_page = false;\n\t\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_apicv_status) {\n\t\tvmx->nested.update_vmcs01_apicv_status = false;\n\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t}\n\n\tif ((vm_exit_reason != -1) &&\n\t    (enable_shadow_vmcs || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)))\n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\n\t/* in case we halted in L2 */\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tif (likely(!vmx->fail)) {\n\t\tif ((u16)vm_exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&\n\t\t    nested_exit_intr_ack_set(vcpu)) {\n\t\t\tint irq = kvm_cpu_get_interrupt(vcpu);\n\t\t\tWARN_ON(irq < 0);\n\t\t\tvmcs12->vm_exit_intr_info = irq |\n\t\t\t\tINTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;\n\t\t}\n\n\t\tif (vm_exit_reason != -1)\n\t\t\ttrace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,\n\t\t\t\t\t\t       vmcs12->exit_qualification,\n\t\t\t\t\t\t       vmcs12->idt_vectoring_info_field,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_info,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_error_code,\n\t\t\t\t\t\t       KVM_ISA_VMX);\n\n\t\tload_vmcs12_host_state(vcpu, vmcs12);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * After an early L2 VM-entry failure, we're now back\n\t * in L1 which thinks it just finished a VMLAUNCH or\n\t * VMRESUME instruction, so we need to set the failure\n\t * flag and the VM-instruction error field of the VMCS\n\t * accordingly, and skip the emulated instruction.\n\t */\n\t(void)nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\t/*\n\t * Restore L1's host state to KVM's software model.  We're here\n\t * because a consistency check was caught by hardware, which\n\t * means some amount of guest state has been propagated to KVM's\n\t * model and needs to be unwound to the host's state.\n\t */\n\tnested_vmx_restore_host_state(vcpu);\n\n\tvmx->fail = 0;\n}",
        "func": "void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,\n\t\t       u32 exit_intr_info, unsigned long exit_qualification)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t/* Pending MTF traps are discarded on VM-Exit. */\n\tvmx->nested.mtf_pending = false;\n\n\t/* trying to cancel vmlaunch/vmresume is a bug */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t/*\n\t\t * KVM_REQ_GET_NESTED_STATE_PAGES is also used to map\n\t\t * Enlightened VMCS after migration and we still need to\n\t\t * do that when something is forcing L2->L1 exit prior to\n\t\t * the first L2 run.\n\t\t */\n\t\t(void)nested_get_evmcs_page(vcpu);\n\t}\n\n\t/* Service pending TLB flush requests for L2 before switching to L1. */\n\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\t/*\n\t * VCPU_EXREG_PDPTR will be clobbered in arch/x86/kvm/vmx/vmx.h between\n\t * now and the new vmentry.  Ensure that the VMCS02 PDPTR fields are\n\t * up-to-date before switching to L1.\n\t */\n\tif (enable_ept && is_pae_paging(vcpu))\n\t\tvmx_ept_load_pdptrs(vcpu);\n\n\tleave_guest_mode(vcpu);\n\n\tif (nested_cpu_has_preemption_timer(vmcs12))\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING)) {\n\t\tvcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;\n\t\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))\n\t\t\tvcpu->arch.tsc_scaling_ratio = vcpu->arch.l1_tsc_scaling_ratio;\n\t}\n\n\tif (likely(!vmx->fail)) {\n\t\tsync_vmcs02_to_vmcs12(vcpu, vmcs12);\n\n\t\tif (vm_exit_reason != -1)\n\t\t\tprepare_vmcs12(vcpu, vmcs12, vm_exit_reason,\n\t\t\t\t       exit_intr_info, exit_qualification);\n\n\t\t/*\n\t\t * Must happen outside of sync_vmcs02_to_vmcs12() as it will\n\t\t * also be used to capture vmcs12 cache as part of\n\t\t * capturing nVMX state for snapshot (migration).\n\t\t *\n\t\t * Otherwise, this flush will dirty guest memory at a\n\t\t * point it is already assumed by user-space to be\n\t\t * immutable.\n\t\t */\n\t\tnested_flush_cached_shadow_vmcs12(vcpu, vmcs12);\n\t} else {\n\t\t/*\n\t\t * The only expected VM-instruction error is \"VM entry with\n\t\t * invalid control field(s).\" Anything else indicates a\n\t\t * problem with L0.  And we should never get here with a\n\t\t * VMFail of any type if early consistency checks are enabled.\n\t\t */\n\t\tWARN_ON_ONCE(vmcs_read32(VM_INSTRUCTION_ERROR) !=\n\t\t\t     VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\t\tWARN_ON_ONCE(nested_early_check);\n\t}\n\n\t/*\n\t * Drop events/exceptions that were queued for re-injection to L2\n\t * (picked up via vmx_complete_interrupts()), as well as exceptions\n\t * that were pending for L2.  Note, this must NOT be hoisted above\n\t * prepare_vmcs12(), events/exceptions queued for re-injection need to\n\t * be captured in vmcs12 (see vmcs12_save_pending_event()).\n\t */\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\n\t/*\n\t * If IBRS is advertised to the vCPU, KVM must flush the indirect\n\t * branch predictors when transitioning from L2 to L1, as L1 expects\n\t * hardware (KVM in this case) to provide separate predictor modes.\n\t * Bare metal isolates VMX root (host) from VMX non-root (guest), but\n\t * doesn't isolate different VMCSs, i.e. in this case, doesn't provide\n\t * separate modes for L2 vs L1.\n\t */\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\tindirect_branch_prediction_barrier();\n\n\t/* Update any VMCS fields that might have changed while L2 ran */\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (kvm_caps.has_tsc_control)\n\t\tvmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);\n\n\tif (vmx->nested.l1_tpr_threshold != -1)\n\t\tvmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);\n\n\tif (vmx->nested.change_vmcs01_virtual_apic_mode) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = false;\n\t\tvmx_set_virtual_apic_mode(vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_cpu_dirty_logging) {\n\t\tvmx->nested.update_vmcs01_cpu_dirty_logging = false;\n\t\tvmx_update_cpu_dirty_logging(vcpu);\n\t}\n\n\t/* Unpin physical memory we referred to in vmcs02 */\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map, false);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map, true);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map, true);\n\tvmx->nested.pi_desc = NULL;\n\n\tif (vmx->nested.reload_vmcs01_apic_access_page) {\n\t\tvmx->nested.reload_vmcs01_apic_access_page = false;\n\t\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_apicv_status) {\n\t\tvmx->nested.update_vmcs01_apicv_status = false;\n\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t}\n\n\tif ((vm_exit_reason != -1) &&\n\t    (enable_shadow_vmcs || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)))\n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\n\t/* in case we halted in L2 */\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tif (likely(!vmx->fail)) {\n\t\tif ((u16)vm_exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&\n\t\t    nested_exit_intr_ack_set(vcpu)) {\n\t\t\tint irq = kvm_cpu_get_interrupt(vcpu);\n\t\t\tWARN_ON(irq < 0);\n\t\t\tvmcs12->vm_exit_intr_info = irq |\n\t\t\t\tINTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;\n\t\t}\n\n\t\tif (vm_exit_reason != -1)\n\t\t\ttrace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,\n\t\t\t\t\t\t       vmcs12->exit_qualification,\n\t\t\t\t\t\t       vmcs12->idt_vectoring_info_field,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_info,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_error_code,\n\t\t\t\t\t\t       KVM_ISA_VMX);\n\n\t\tload_vmcs12_host_state(vcpu, vmcs12);\n\n\t\treturn;\n\t}\n\n\t/*\n\t * After an early L2 VM-entry failure, we're now back\n\t * in L1 which thinks it just finished a VMLAUNCH or\n\t * VMRESUME instruction, so we need to set the failure\n\t * flag and the VM-instruction error field of the VMCS\n\t * accordingly, and skip the emulated instruction.\n\t */\n\t(void)nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\t/*\n\t * Restore L1's host state to KVM's software model.  We're here\n\t * because a consistency check was caught by hardware, which\n\t * means some amount of guest state has been propagated to KVM's\n\t * model and needs to be unwound to the host's state.\n\t */\n\tnested_vmx_restore_host_state(vcpu);\n\n\tvmx->fail = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -84,6 +84,17 @@\n \n \tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n \n+\t/*\n+\t * If IBRS is advertised to the vCPU, KVM must flush the indirect\n+\t * branch predictors when transitioning from L2 to L1, as L1 expects\n+\t * hardware (KVM in this case) to provide separate predictor modes.\n+\t * Bare metal isolates VMX root (host) from VMX non-root (guest), but\n+\t * doesn't isolate different VMCSs, i.e. in this case, doesn't provide\n+\t * separate modes for L2 vs L1.\n+\t */\n+\tif (guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n+\t\tindirect_branch_prediction_barrier();\n+\n \t/* Update any VMCS fields that might have changed while L2 ran */\n \tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n \tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/*",
                "\t * If IBRS is advertised to the vCPU, KVM must flush the indirect",
                "\t * branch predictors when transitioning from L2 to L1, as L1 expects",
                "\t * hardware (KVM in this case) to provide separate predictor modes.",
                "\t * Bare metal isolates VMX root (host) from VMX non-root (guest), but",
                "\t * doesn't isolate different VMCSs, i.e. in this case, doesn't provide",
                "\t * separate modes for L2 vs L1.",
                "\t */",
                "\tif (guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))",
                "\t\tindirect_branch_prediction_barrier();",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-2196",
        "func_name": "torvalds/linux/vmx_vcpu_load_vmcs",
        "description": "A regression exists in the Linux Kernel within KVM: nVMX that allowed for speculative execution attacks. L2 can carry out Spectre v2 attacks on L1 due to L1 thinking it doesn't need retpolines or IBPB after running L2 due to KVM (L0) advertising eIBRS support to L1. An attacker at L2 with code execution can execute code on an indirect branch on the host machine. We recommend upgrading to Kernel 6.2 or past commit 2e7eab81425a\n",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=2e7eab81425ad6c875f2ed47c0ce01e78afc38a5",
        "commit_title": "According to Intel's document on Indirect Branch Restricted",
        "commit_text": "Speculation, \"Enabling IBRS does not prevent software from controlling the predicted targets of indirect branches of unrelated software executed later at the same predictor mode (for example, between two different user applications, or two different virtual machines). Such isolation can be ensured through use of the Indirect Branch Predictor Barrier (IBPB) command.\" This applies to both basic and enhanced IBRS.  Since L1 and L2 VMs share hardware predictor modes (guest-user and guest-kernel), hardware IBRS is not sufficient to virtualize IBRS. (The way that basic IBRS is implemented on pre-eIBRS parts, hardware IBRS is actually sufficient in practice, even though it isn't sufficient architecturally.)  For virtual CPUs that support IBRS, add an indirect branch prediction barrier on emulated VM-exit, to ensure that the predicted targets of indirect branches executed in L1 cannot be controlled by software that was executed in L2.  Since we typically don't intercept guest writes to IA32_SPEC_CTRL, perform the IBPB at emulated VM-exit regardless of the current IA32_SPEC_CTRL.IBRS value, even though the IBPB could technically be deferred until L1 sets IA32_SPEC_CTRL.IBRS, if IA32_SPEC_CTRL.IBRS is clear at emulated VM-exit.  This is CVE-2022-2196.  Cc: Sean Christopherson <seanjc@google.com> Link: https://lore.kernel.org/r/20221019213620.1953281-3-jmattson@google.com ",
        "func_before": "void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,\n\t\t\tstruct loaded_vmcs *buddy)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool already_loaded = vmx->loaded_vmcs->cpu == cpu;\n\tstruct vmcs *prev;\n\n\tif (!already_loaded) {\n\t\tloaded_vmcs_clear(vmx->loaded_vmcs);\n\t\tlocal_irq_disable();\n\n\t\t/*\n\t\t * Ensure loaded_vmcs->cpu is read before adding loaded_vmcs to\n\t\t * this cpu's percpu list, otherwise it may not yet be deleted\n\t\t * from its previous cpu's percpu list.  Pairs with the\n\t\t * smb_wmb() in __loaded_vmcs_clear().\n\t\t */\n\t\tsmp_rmb();\n\n\t\tlist_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,\n\t\t\t &per_cpu(loaded_vmcss_on_cpu, cpu));\n\t\tlocal_irq_enable();\n\t}\n\n\tprev = per_cpu(current_vmcs, cpu);\n\tif (prev != vmx->loaded_vmcs->vmcs) {\n\t\tper_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;\n\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\t\t/*\n\t\t * No indirect branch prediction barrier needed when switching\n\t\t * the active VMCS within a guest, e.g. on nested VM-Enter.\n\t\t * The L1 VMM can protect itself with retpolines, IBPB or IBRS.\n\t\t */\n\t\tif (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (!already_loaded) {\n\t\tvoid *gdt = get_current_gdt_ro();\n\n\t\t/*\n\t\t * Flush all EPTP/VPID contexts, the new pCPU may have stale\n\t\t * TLB entries from its previous association with the vCPU.\n\t\t */\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\n\t\t/*\n\t\t * Linux uses per-cpu TSS and GDT, so set these when switching\n\t\t * processors.  See 22.2.4.\n\t\t */\n\t\tvmcs_writel(HOST_TR_BASE,\n\t\t\t    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);\n\t\tvmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */\n\n\t\tif (IS_ENABLED(CONFIG_IA32_EMULATION) || IS_ENABLED(CONFIG_X86_32)) {\n\t\t\t/* 22.2.3 */\n\t\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP,\n\t\t\t\t    (unsigned long)(cpu_entry_stack(cpu) + 1));\n\t\t}\n\n\t\tvmx->loaded_vmcs->cpu = cpu;\n\t}\n}",
        "func": "void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,\n\t\t\tstruct loaded_vmcs *buddy)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool already_loaded = vmx->loaded_vmcs->cpu == cpu;\n\tstruct vmcs *prev;\n\n\tif (!already_loaded) {\n\t\tloaded_vmcs_clear(vmx->loaded_vmcs);\n\t\tlocal_irq_disable();\n\n\t\t/*\n\t\t * Ensure loaded_vmcs->cpu is read before adding loaded_vmcs to\n\t\t * this cpu's percpu list, otherwise it may not yet be deleted\n\t\t * from its previous cpu's percpu list.  Pairs with the\n\t\t * smb_wmb() in __loaded_vmcs_clear().\n\t\t */\n\t\tsmp_rmb();\n\n\t\tlist_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,\n\t\t\t &per_cpu(loaded_vmcss_on_cpu, cpu));\n\t\tlocal_irq_enable();\n\t}\n\n\tprev = per_cpu(current_vmcs, cpu);\n\tif (prev != vmx->loaded_vmcs->vmcs) {\n\t\tper_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;\n\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\t\t/*\n\t\t * No indirect branch prediction barrier needed when switching\n\t\t * the active VMCS within a vCPU, unless IBRS is advertised to\n\t\t * the vCPU.  To minimize the number of IBPBs executed, KVM\n\t\t * performs IBPB on nested VM-Exit (a single nested transition\n\t\t * may switch the active VMCS multiple times).\n\t\t */\n\t\tif (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (!already_loaded) {\n\t\tvoid *gdt = get_current_gdt_ro();\n\n\t\t/*\n\t\t * Flush all EPTP/VPID contexts, the new pCPU may have stale\n\t\t * TLB entries from its previous association with the vCPU.\n\t\t */\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\n\t\t/*\n\t\t * Linux uses per-cpu TSS and GDT, so set these when switching\n\t\t * processors.  See 22.2.4.\n\t\t */\n\t\tvmcs_writel(HOST_TR_BASE,\n\t\t\t    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);\n\t\tvmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */\n\n\t\tif (IS_ENABLED(CONFIG_IA32_EMULATION) || IS_ENABLED(CONFIG_X86_32)) {\n\t\t\t/* 22.2.3 */\n\t\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP,\n\t\t\t\t    (unsigned long)(cpu_entry_stack(cpu) + 1));\n\t\t}\n\n\t\tvmx->loaded_vmcs->cpu = cpu;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,8 +29,10 @@\n \n \t\t/*\n \t\t * No indirect branch prediction barrier needed when switching\n-\t\t * the active VMCS within a guest, e.g. on nested VM-Enter.\n-\t\t * The L1 VMM can protect itself with retpolines, IBPB or IBRS.\n+\t\t * the active VMCS within a vCPU, unless IBRS is advertised to\n+\t\t * the vCPU.  To minimize the number of IBPBs executed, KVM\n+\t\t * performs IBPB on nested VM-Exit (a single nested transition\n+\t\t * may switch the active VMCS multiple times).\n \t\t */\n \t\tif (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))\n \t\t\tindirect_branch_prediction_barrier();",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t * the active VMCS within a guest, e.g. on nested VM-Enter.",
                "\t\t * The L1 VMM can protect itself with retpolines, IBPB or IBRS."
            ],
            "added_lines": [
                "\t\t * the active VMCS within a vCPU, unless IBRS is advertised to",
                "\t\t * the vCPU.  To minimize the number of IBPBs executed, KVM",
                "\t\t * performs IBPB on nested VM-Exit (a single nested transition",
                "\t\t * may switch the active VMCS multiple times)."
            ]
        }
    }
]