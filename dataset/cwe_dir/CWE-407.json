[
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/hash_link_ref",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "static unsigned int\nhash_link_ref(const uint8_t *link_ref, size_t length)\n{\n\tsize_t i;\n\tunsigned int hash = 0;\n\n\tfor (i = 0; i < length; ++i)\n\t\thash = tolower(link_ref[i]) + (hash << 6) + (hash << 16) - hash;\n\n\treturn hash;\n}",
        "func": "static unsigned int\nhash_link_ref(const uint8_t *link_ref, size_t length)\n{\n\treturn siphash_nocase(link_ref, length, sip_hash_key);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,5 @@\n static unsigned int\n hash_link_ref(const uint8_t *link_ref, size_t length)\n {\n-\tsize_t i;\n-\tunsigned int hash = 0;\n-\n-\tfor (i = 0; i < length; ++i)\n-\t\thash = tolower(link_ref[i]) + (hash << 6) + (hash << 16) - hash;\n-\n-\treturn hash;\n+\treturn siphash_nocase(link_ref, length, sip_hash_key);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tsize_t i;",
                "\tunsigned int hash = 0;",
                "",
                "\tfor (i = 0; i < length; ++i)",
                "\t\thash = tolower(link_ref[i]) + (hash << 6) + (hash << 16) - hash;",
                "",
                "\treturn hash;"
            ],
            "added_lines": [
                "\treturn siphash_nocase(link_ref, length, sip_hash_key);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/sd_markdown_new",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "struct sd_markdown *\nsd_markdown_new(\n\tunsigned int extensions,\n\tsize_t max_nesting,\n\tsize_t max_table_cols,\n\tconst struct sd_callbacks *callbacks,\n\tvoid *opaque)\n{\n\tstruct sd_markdown *md = NULL;\n\n\tassert(max_nesting > 0 && max_table_cols > 0 && callbacks);\n\n\tmd = malloc(sizeof(struct sd_markdown));\n\tif (!md)\n\t\treturn NULL;\n\n\tmemcpy(&md->cb, callbacks, sizeof(struct sd_callbacks));\n\n\tstack_init(&md->work_bufs[BUFFER_BLOCK], 4);\n\tstack_init(&md->work_bufs[BUFFER_SPAN], 8);\n\n\tmemset(md->active_char, 0x0, 256);\n\n\tif (md->cb.emphasis || md->cb.double_emphasis || md->cb.triple_emphasis) {\n\t\tmd->active_char['*'] = MD_CHAR_EMPHASIS;\n\t\tmd->active_char['_'] = MD_CHAR_EMPHASIS;\n\t\tmd->active_char['>'] = MD_CHAR_EMPHASIS;\n\t\tif (extensions & MKDEXT_STRIKETHROUGH)\n\t\t\tmd->active_char['~'] = MD_CHAR_EMPHASIS;\n\t}\n\n\tif (md->cb.codespan)\n\t\tmd->active_char['`'] = MD_CHAR_CODESPAN;\n\n\tif (md->cb.linebreak)\n\t\tmd->active_char['\\n'] = MD_CHAR_LINEBREAK;\n\n\tif (md->cb.image || md->cb.link)\n\t\tmd->active_char['['] = MD_CHAR_LINK;\n\n\tmd->active_char['<'] = MD_CHAR_LANGLE;\n\tmd->active_char['\\\\'] = MD_CHAR_ESCAPE;\n\tmd->active_char['&'] = MD_CHAR_ENTITITY;\n\n\tif (extensions & MKDEXT_AUTOLINK) {\n\t\tif (!(extensions & MKDEXT_NO_EMAIL_AUTOLINK))\n\t\t\tmd->active_char['@'] = MD_CHAR_AUTOLINK_EMAIL;\n\t\tmd->active_char[':'] = MD_CHAR_AUTOLINK_URL;\n\t\tmd->active_char['w'] = MD_CHAR_AUTOLINK_WWW;\n\t\tmd->active_char['/'] = MD_CHAR_AUTOLINK_SUBREDDIT_OR_USERNAME;\n\t}\n\n\tif (extensions & MKDEXT_SUPERSCRIPT)\n\t\tmd->active_char['^'] = MD_CHAR_SUPERSCRIPT;\n\n\t/* Extension data */\n\tmd->ext_flags = extensions;\n\tmd->opaque = opaque;\n\tmd->max_nesting = max_nesting;\n\tmd->max_table_cols = max_table_cols;\n\tmd->in_link_body = 0;\n\n\treturn md;\n}",
        "func": "struct sd_markdown *\nsd_markdown_new(\n\tunsigned int extensions,\n\tsize_t max_nesting,\n\tsize_t max_table_cols,\n\tconst struct sd_callbacks *callbacks,\n\tvoid *opaque)\n{\n\tstruct sd_markdown *md = NULL;\n\n\tassert(max_nesting > 0 && max_table_cols > 0 && callbacks);\n\n\tmd = malloc(sizeof(struct sd_markdown));\n\tif (!md)\n\t\treturn NULL;\n\n\tif (!sip_hash_key_init) {\n\t\tif (getrandom(sip_hash_key, SIP_HASH_KEY_LEN, 0) < SIP_HASH_KEY_LEN)\n\t\t\treturn NULL;\n\t\tsip_hash_key_init = 1;\n\t}\n\n\tmemcpy(&md->cb, callbacks, sizeof(struct sd_callbacks));\n\n\tstack_init(&md->work_bufs[BUFFER_BLOCK], 4);\n\tstack_init(&md->work_bufs[BUFFER_SPAN], 8);\n\n\tmemset(md->active_char, 0x0, 256);\n\n\tif (md->cb.emphasis || md->cb.double_emphasis || md->cb.triple_emphasis) {\n\t\tmd->active_char['*'] = MD_CHAR_EMPHASIS;\n\t\tmd->active_char['_'] = MD_CHAR_EMPHASIS;\n\t\tmd->active_char['>'] = MD_CHAR_EMPHASIS;\n\t\tif (extensions & MKDEXT_STRIKETHROUGH)\n\t\t\tmd->active_char['~'] = MD_CHAR_EMPHASIS;\n\t}\n\n\tif (md->cb.codespan)\n\t\tmd->active_char['`'] = MD_CHAR_CODESPAN;\n\n\tif (md->cb.linebreak)\n\t\tmd->active_char['\\n'] = MD_CHAR_LINEBREAK;\n\n\tif (md->cb.image || md->cb.link)\n\t\tmd->active_char['['] = MD_CHAR_LINK;\n\n\tmd->active_char['<'] = MD_CHAR_LANGLE;\n\tmd->active_char['\\\\'] = MD_CHAR_ESCAPE;\n\tmd->active_char['&'] = MD_CHAR_ENTITITY;\n\n\tif (extensions & MKDEXT_AUTOLINK) {\n\t\tif (!(extensions & MKDEXT_NO_EMAIL_AUTOLINK))\n\t\t\tmd->active_char['@'] = MD_CHAR_AUTOLINK_EMAIL;\n\t\tmd->active_char[':'] = MD_CHAR_AUTOLINK_URL;\n\t\tmd->active_char['w'] = MD_CHAR_AUTOLINK_WWW;\n\t\tmd->active_char['/'] = MD_CHAR_AUTOLINK_SUBREDDIT_OR_USERNAME;\n\t}\n\n\tif (extensions & MKDEXT_SUPERSCRIPT)\n\t\tmd->active_char['^'] = MD_CHAR_SUPERSCRIPT;\n\n\t/* Extension data */\n\tmd->ext_flags = extensions;\n\tmd->opaque = opaque;\n\tmd->max_nesting = max_nesting;\n\tmd->max_table_cols = max_table_cols;\n\tmd->in_link_body = 0;\n\n\treturn md;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,12 @@\n \tmd = malloc(sizeof(struct sd_markdown));\n \tif (!md)\n \t\treturn NULL;\n+\n+\tif (!sip_hash_key_init) {\n+\t\tif (getrandom(sip_hash_key, SIP_HASH_KEY_LEN, 0) < SIP_HASH_KEY_LEN)\n+\t\t\treturn NULL;\n+\t\tsip_hash_key_init = 1;\n+\t}\n \n \tmemcpy(&md->cb, callbacks, sizeof(struct sd_callbacks));\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (!sip_hash_key_init) {",
                "\t\tif (getrandom(sip_hash_key, SIP_HASH_KEY_LEN, 0) < SIP_HASH_KEY_LEN)",
                "\t\t\treturn NULL;",
                "\t\tsip_hash_key_init = 1;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/find_link_ref",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "static struct link_ref *\nfind_link_ref(struct link_ref **references, uint8_t *name, size_t length)\n{\n\tunsigned int hash = hash_link_ref(name, length);\n\tstruct link_ref *ref = NULL;\n\n\tref = references[hash % REF_TABLE_SIZE];\n\n\twhile (ref != NULL) {\n\t\tif (ref->id == hash)\n\t\t\treturn ref;\n\n\t\tref = ref->next;\n\t}\n\n\treturn NULL;\n}",
        "func": "static struct link_ref *\nfind_link_ref(struct link_ref **references, uint8_t *name, size_t length)\n{\n\tunsigned int hash = hash_link_ref(name, length);\n\tstruct link_ref *ref = NULL;\n\n\tref = references[hash % REF_TABLE_SIZE];\n\n\twhile (ref != NULL) {\n\t\tif (ref->id == hash && ref->label->size == length) {\n\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, length) == 0) {\n\t\t\t\treturn ref;\n\t\t\t}\n\t\t}\n\n\t\tref = ref->next;\n\t}\n\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,8 +7,11 @@\n \tref = references[hash % REF_TABLE_SIZE];\n \n \twhile (ref != NULL) {\n-\t\tif (ref->id == hash)\n-\t\t\treturn ref;\n+\t\tif (ref->id == hash && ref->label->size == length) {\n+\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, length) == 0) {\n+\t\t\t\treturn ref;\n+\t\t\t}\n+\t\t}\n \n \t\tref = ref->next;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (ref->id == hash)",
                "\t\t\treturn ref;"
            ],
            "added_lines": [
                "\t\tif (ref->id == hash && ref->label->size == length) {",
                "\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, length) == 0) {",
                "\t\t\t\treturn ref;",
                "\t\t\t}",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/is_ref",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "static int\nis_ref(const uint8_t *data, size_t beg, size_t end, size_t *last, struct link_ref **refs)\n{\n/*\tint n; */\n\tsize_t i = 0;\n\tsize_t id_offset, id_end;\n\tsize_t link_offset, link_end;\n\tsize_t title_offset, title_end;\n\tsize_t line_end;\n\n\t/* up to 3 optional leading spaces */\n\tif (beg + 3 >= end) return 0;\n\tif (data[beg] == ' ') { i = 1;\n\tif (data[beg + 1] == ' ') { i = 2;\n\tif (data[beg + 2] == ' ') { i = 3;\n\tif (data[beg + 3] == ' ') return 0; } } }\n\ti += beg;\n\n\t/* id part: anything but a newline between brackets */\n\tif (data[i] != '[') return 0;\n\ti++;\n\tid_offset = i;\n\twhile (i < end && data[i] != '\\n' && data[i] != '\\r' && data[i] != ']')\n\t\ti++;\n\tif (i >= end || data[i] != ']') return 0;\n\tid_end = i;\n\n\t/* spacer: colon (space | tab)* newline? (space | tab)* */\n\ti++;\n\tif (i >= end || data[i] != ':') return 0;\n\ti++;\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i < end && (data[i] == '\\n' || data[i] == '\\r')) {\n\t\ti++;\n\t\tif (i < end && data[i] == '\\r' && data[i - 1] == '\\n') i++; }\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i >= end) return 0;\n\n\t/* link: whitespace-free sequence, optionally between angle brackets */\n\tif (data[i] == '<')\n\t\ti++;\n\n\tlink_offset = i;\n\n\twhile (i < end && data[i] != ' ' && data[i] != '\\n' && data[i] != '\\r')\n\t\ti++;\n\n\tif (data[i - 1] == '>') link_end = i - 1;\n\telse link_end = i;\n\n\t/* optional spacer: (space | tab)* (newline | '\\'' | '\"' | '(' ) */\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i < end && data[i] != '\\n' && data[i] != '\\r'\n\t\t\t&& data[i] != '\\'' && data[i] != '\"' && data[i] != '(')\n\t\treturn 0;\n\tline_end = 0;\n\t/* computing end-of-line */\n\tif (i >= end || data[i] == '\\r' || data[i] == '\\n') line_end = i;\n\tif (i + 1 < end && data[i] == '\\n' && data[i + 1] == '\\r')\n\t\tline_end = i + 1;\n\n\t/* optional (space|tab)* spacer after a newline */\n\tif (line_end) {\n\t\ti = line_end + 1;\n\t\twhile (i < end && data[i] == ' ') i++; }\n\n\t/* optional title: any non-newline sequence enclosed in '\"()\n\t\t\t\t\talone on its line */\n\ttitle_offset = title_end = 0;\n\tif (i + 1 < end\n\t&& (data[i] == '\\'' || data[i] == '\"' || data[i] == '(')) {\n\t\ti++;\n\t\ttitle_offset = i;\n\t\t/* looking for EOL */\n\t\twhile (i < end && data[i] != '\\n' && data[i] != '\\r') i++;\n\t\tif (i + 1 < end && data[i] == '\\n' && data[i + 1] == '\\r')\n\t\t\ttitle_end = i + 1;\n\t\telse\ttitle_end = i;\n\t\t/* stepping back */\n\t\ti -= 1;\n\t\twhile (i > title_offset && data[i] == ' ')\n\t\t\ti -= 1;\n\t\tif (i > title_offset\n\t\t&& (data[i] == '\\'' || data[i] == '\"' || data[i] == ')')) {\n\t\t\tline_end = title_end;\n\t\t\ttitle_end = i; } }\n\n\tif (!line_end || link_end == link_offset)\n\t\treturn 0; /* garbage after the link empty link */\n\n\t/* a valid ref has been found, filling-in return structures */\n\tif (last)\n\t\t*last = line_end;\n\n\tif (refs) {\n\t\tstruct link_ref *ref;\n\n\t\tref = add_link_ref(refs, data + id_offset, id_end - id_offset);\n\t\tif (!ref)\n\t\t\treturn 0;\n\n\t\tref->link = bufnew(link_end - link_offset);\n\t\tbufput(ref->link, data + link_offset, link_end - link_offset);\n\n\t\tif (title_end > title_offset) {\n\t\t\tref->title = bufnew(title_end - title_offset);\n\t\t\tbufput(ref->title, data + title_offset, title_end - title_offset);\n\t\t}\n\t}\n\n\treturn 1;\n}",
        "func": "static int\nis_ref(const uint8_t *data, size_t beg, size_t end, size_t *last, struct link_ref **refs)\n{\n/*\tint n; */\n\tsize_t i = 0;\n\tsize_t id_offset, id_end;\n\tsize_t link_offset, link_end;\n\tsize_t title_offset, title_end;\n\tsize_t line_end;\n\n\t/* up to 3 optional leading spaces */\n\tif (beg + 3 >= end) return 0;\n\tif (data[beg] == ' ') { i = 1;\n\tif (data[beg + 1] == ' ') { i = 2;\n\tif (data[beg + 2] == ' ') { i = 3;\n\tif (data[beg + 3] == ' ') return 0; } } }\n\ti += beg;\n\n\t/* id part: anything but a newline between brackets */\n\tif (data[i] != '[') return 0;\n\ti++;\n\tid_offset = i;\n\twhile (i < end && data[i] != '\\n' && data[i] != '\\r' && data[i] != ']')\n\t\ti++;\n\tif (i >= end || data[i] != ']') return 0;\n\tid_end = i;\n\n\t/* spacer: colon (space | tab)* newline? (space | tab)* */\n\ti++;\n\tif (i >= end || data[i] != ':') return 0;\n\ti++;\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i < end && (data[i] == '\\n' || data[i] == '\\r')) {\n\t\ti++;\n\t\tif (i < end && data[i] == '\\r' && data[i - 1] == '\\n') i++; }\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i >= end) return 0;\n\n\t/* link: whitespace-free sequence, optionally between angle brackets */\n\tif (data[i] == '<')\n\t\ti++;\n\n\tlink_offset = i;\n\n\twhile (i < end && data[i] != ' ' && data[i] != '\\n' && data[i] != '\\r')\n\t\ti++;\n\n\tif (data[i - 1] == '>') link_end = i - 1;\n\telse link_end = i;\n\n\t/* optional spacer: (space | tab)* (newline | '\\'' | '\"' | '(' ) */\n\twhile (i < end && data[i] == ' ') i++;\n\tif (i < end && data[i] != '\\n' && data[i] != '\\r'\n\t\t\t&& data[i] != '\\'' && data[i] != '\"' && data[i] != '(')\n\t\treturn 0;\n\tline_end = 0;\n\t/* computing end-of-line */\n\tif (i >= end || data[i] == '\\r' || data[i] == '\\n') line_end = i;\n\tif (i + 1 < end && data[i] == '\\n' && data[i + 1] == '\\r')\n\t\tline_end = i + 1;\n\n\t/* optional (space|tab)* spacer after a newline */\n\tif (line_end) {\n\t\ti = line_end + 1;\n\t\twhile (i < end && data[i] == ' ') i++; }\n\n\t/* optional title: any non-newline sequence enclosed in '\"()\n\t\t\t\t\talone on its line */\n\ttitle_offset = title_end = 0;\n\tif (i + 1 < end\n\t&& (data[i] == '\\'' || data[i] == '\"' || data[i] == '(')) {\n\t\ti++;\n\t\ttitle_offset = i;\n\t\t/* looking for EOL */\n\t\twhile (i < end && data[i] != '\\n' && data[i] != '\\r') i++;\n\t\tif (i + 1 < end && data[i] == '\\n' && data[i + 1] == '\\r')\n\t\t\ttitle_end = i + 1;\n\t\telse\ttitle_end = i;\n\t\t/* stepping back */\n\t\ti -= 1;\n\t\twhile (i > title_offset && data[i] == ' ')\n\t\t\ti -= 1;\n\t\tif (i > title_offset\n\t\t&& (data[i] == '\\'' || data[i] == '\"' || data[i] == ')')) {\n\t\t\tline_end = title_end;\n\t\t\ttitle_end = i; } }\n\n\tif (!line_end || link_end == link_offset)\n\t\treturn 0; /* garbage after the link empty link */\n\n\t/* a valid ref has been found, filling-in return structures */\n\tif (last)\n\t\t*last = line_end;\n\n\tif (refs) {\n\t\tstruct link_ref *ref;\n\n\t\tref = add_link_ref(refs, data + id_offset, id_end - id_offset);\n\t\tif (!ref)\n\t\t\treturn 0;\n\n\t\tref->label = bufnew(id_end - id_offset);\n\t\tbufput(ref->label, data + id_offset, id_end - id_offset);\n\t\tref->link = bufnew(link_end - link_offset);\n\t\tbufput(ref->link, data + link_offset, link_end - link_offset);\n\n\t\tif (title_end > title_offset) {\n\t\t\tref->title = bufnew(title_end - title_offset);\n\t\t\tbufput(ref->title, data + title_offset, title_end - title_offset);\n\t\t}\n\t}\n\n\treturn 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -99,6 +99,8 @@\n \t\tif (!ref)\n \t\t\treturn 0;\n \n+\t\tref->label = bufnew(id_end - id_offset);\n+\t\tbufput(ref->label, data + id_offset, id_end - id_offset);\n \t\tref->link = bufnew(link_end - link_offset);\n \t\tbufput(ref->link, data + link_offset, link_end - link_offset);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tref->label = bufnew(id_end - id_offset);",
                "\t\tbufput(ref->label, data + id_offset, id_end - id_offset);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/free_link_refs",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "static void\nfree_link_refs(struct link_ref **references)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < REF_TABLE_SIZE; ++i) {\n\t\tstruct link_ref *r = references[i];\n\t\tstruct link_ref *next;\n\n\t\twhile (r) {\n\t\t\tnext = r->next;\n\t\t\tbufrelease(r->link);\n\t\t\tbufrelease(r->title);\n\t\t\tfree(r);\n\t\t\tr = next;\n\t\t}\n\t}\n}",
        "func": "static void\nfree_link_refs(struct link_ref **references)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < REF_TABLE_SIZE; ++i) {\n\t\tstruct link_ref *r = references[i];\n\t\tstruct link_ref *next;\n\n\t\twhile (r) {\n\t\t\tnext = r->next;\n\t\t\tbufrelease(r->label);\n\t\t\tbufrelease(r->link);\n\t\t\tbufrelease(r->title);\n\t\t\tfree(r);\n\t\t\tr = next;\n\t\t}\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,7 @@\n \n \t\twhile (r) {\n \t\t\tnext = r->next;\n+\t\t\tbufrelease(r->label);\n \t\t\tbufrelease(r->link);\n \t\t\tbufrelease(r->title);\n \t\t\tfree(r);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tbufrelease(r->label);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41168",
        "func_name": "reddit/snudown/add_link_ref",
        "description": "Snudown is a reddit-specific fork of the Sundown Markdown parser used by GitHub, with Python integration added. In affected versions snudown was found to be vulnerable to denial of service attacks to its reference table implementation. References written in markdown ` [reference_name]: https://www.example.com` are inserted into a hash table which was found to have a weak hash function, meaning that an attacker can reliably generate a large number of collisions for it. This makes the hash table vulnerable to a hash-collision DoS attack, a type of algorithmic complexity attack. Further the hash table allowed for duplicate entries resulting in long retrieval times. Proofs of concept and further discussion of the hash collision issue are discussed on the snudown GHSA(https://github.com/reddit/snudown/security/advisories/GHSA-6gvv-9q92-w5f6). Users are advised to update to version 1.7.0.",
        "git_url": "https://github.com/reddit/snudown/commit/1ac2c130b210539ee1e5d67a7bac93f9d8007c0e",
        "commit_title": "Hash-collision denial-of-service vulnerabilities (#87)",
        "commit_text": " * Add case insensitive SipHash implementation * Replace ref hash function with SipHash * Add label to link_ref struct. * Update find_link_ref to compare link labels as well as hashes * Update add_link_ref to disallow duplicate entries. * cast to char from uint8_t for strncasecmp * update README markdown, remove TODO * add py2 wheel generation * fix: add logic for older glibc not having getrandom, impacting staging  Co-authored-by: Nicolaas <nweidema@usc.edu>",
        "func_before": "static struct link_ref *\nadd_link_ref(\n\tstruct link_ref **references,\n\tconst uint8_t *name, size_t name_size)\n{\n\tstruct link_ref *ref = calloc(1, sizeof(struct link_ref));\n\n\tif (!ref)\n\t\treturn NULL;\n\n\tref->id = hash_link_ref(name, name_size);\n\tref->next = references[ref->id % REF_TABLE_SIZE];\n\n\treferences[ref->id % REF_TABLE_SIZE] = ref;\n\treturn ref;\n}",
        "func": "static struct link_ref *\nadd_link_ref(\n\tstruct link_ref **references,\n\tconst uint8_t *name, size_t name_size)\n{\n\tunsigned int hash;\n\tstruct link_ref *ref;\n\thash = hash_link_ref(name, name_size);\n\tref = references[hash % REF_TABLE_SIZE];\n\twhile (ref != NULL) {\n\t\t/* If a reference with the same label exists already, replace it with the new reference */\n\t\tif (ref->id == hash && ref->label->size == name_size) {\n\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, name_size) == 0) {\n\t\t\t\tbufrelease(ref->label);\n\t\t\t\tbufrelease(ref->link);\n\t\t\t\tbufrelease(ref->title);\n\t\t\t\treturn ref;\n\t\t\t}\n\t\t}\n\n\t\tref = ref->next;\n\t}\n\tref = calloc(1, sizeof(struct link_ref));\n\tif (!ref)\n\t\treturn NULL;\n\tref->id = hash;\n\tref->next = references[ref->id % REF_TABLE_SIZE];\n\n\treferences[ref->id % REF_TABLE_SIZE] = ref;\n\treturn ref;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,12 +3,27 @@\n \tstruct link_ref **references,\n \tconst uint8_t *name, size_t name_size)\n {\n-\tstruct link_ref *ref = calloc(1, sizeof(struct link_ref));\n+\tunsigned int hash;\n+\tstruct link_ref *ref;\n+\thash = hash_link_ref(name, name_size);\n+\tref = references[hash % REF_TABLE_SIZE];\n+\twhile (ref != NULL) {\n+\t\t/* If a reference with the same label exists already, replace it with the new reference */\n+\t\tif (ref->id == hash && ref->label->size == name_size) {\n+\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, name_size) == 0) {\n+\t\t\t\tbufrelease(ref->label);\n+\t\t\t\tbufrelease(ref->link);\n+\t\t\t\tbufrelease(ref->title);\n+\t\t\t\treturn ref;\n+\t\t\t}\n+\t\t}\n \n+\t\tref = ref->next;\n+\t}\n+\tref = calloc(1, sizeof(struct link_ref));\n \tif (!ref)\n \t\treturn NULL;\n-\n-\tref->id = hash_link_ref(name, name_size);\n+\tref->id = hash;\n \tref->next = references[ref->id % REF_TABLE_SIZE];\n \n \treferences[ref->id % REF_TABLE_SIZE] = ref;",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct link_ref *ref = calloc(1, sizeof(struct link_ref));",
                "",
                "\tref->id = hash_link_ref(name, name_size);"
            ],
            "added_lines": [
                "\tunsigned int hash;",
                "\tstruct link_ref *ref;",
                "\thash = hash_link_ref(name, name_size);",
                "\tref = references[hash % REF_TABLE_SIZE];",
                "\twhile (ref != NULL) {",
                "\t\t/* If a reference with the same label exists already, replace it with the new reference */",
                "\t\tif (ref->id == hash && ref->label->size == name_size) {",
                "\t\t\tif (strncasecmp((char *)ref->label->data, (char *) name, name_size) == 0) {",
                "\t\t\t\tbufrelease(ref->label);",
                "\t\t\t\tbufrelease(ref->link);",
                "\t\t\t\tbufrelease(ref->title);",
                "\t\t\t\treturn ref;",
                "\t\t\t}",
                "\t\t}",
                "\t\tref = ref->next;",
                "\t}",
                "\tref = calloc(1, sizeof(struct link_ref));",
                "\tref->id = hash;"
            ]
        }
    }
]