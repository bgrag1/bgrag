[
    {
        "cve_id": "CVE-2022-29207",
        "func_name": "tensorflow/GetDeviceForInput",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, multiple TensorFlow operations misbehave in eager mode when the resource handle provided to them is invalid. In graph mode, it would have been impossible to perform these API calls, but migration to TF 2.x eager mode opened up this vulnerability. If the resource handle is empty, then a reference is bound to a null pointer inside TensorFlow codebase (various codepaths). This is undefined behavior. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/a5b89cd68c02329d793356bda85d079e9e69b4e7",
        "commit_title": "Fix empty resource handle vulnerability.",
        "commit_text": " Some ops that attempt to extract a resource handle from user input can lead to nullptr dereferences.  This returns an error in such a case.  PiperOrigin-RevId: 445571938",
        "func_before": "Status GetDeviceForInput(const EagerOperation& op, const EagerContext& ctx,\n                         TensorHandle* tensor_handle, Device** result) {\n  Device* cpu_device = ctx.HostCPU();\n  string device_name;\n  if (tensor_handle->Type() != TensorHandle::LOCAL) {\n    Device* device = tensor_handle->device();\n    device_name = device != nullptr ? device->name() : cpu_device->name();\n    *result = (device == nullptr ? cpu_device : device);\n  } else if (tensor_handle->dtype == DT_RESOURCE) {\n    // Use the resource's actual device because it is the device that will\n    // influence partitioning the multi-device function.\n    const Tensor* tensor;\n    // TODO(fishx): Avoid blocking here.\n    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));\n    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);\n    device_name = handle.device();\n\n    Device* input_device;\n    TF_RETURN_IF_ERROR(\n        ctx.FindDeviceFromName(device_name.c_str(), &input_device));\n    *result = input_device;\n  } else {\n    Device* device = tensor_handle->device();\n    const bool is_tpu = device != nullptr && device->device_type() == \"TPU\";\n    // int32 return values can be placed on TPUs.\n    const bool use_host_memory =\n        is_tpu ? MTypeFromDTypeIntsOnDevice(tensor_handle->dtype)\n               : MTypeFromDType(tensor_handle->dtype);\n    if (use_host_memory) {\n      *result = cpu_device;\n    } else {\n      // Eager ops executing as functions should have their preferred inputs set\n      // to the op's device. This allows us to avoid expensive D2H copies if a\n      // mirror of the tensor already exists on the op's device.\n      if (!op.is_function() && device != nullptr && device != cpu_device) {\n        device = absl::get<Device*>(op.Device());\n      }\n      *result = (device == nullptr ? cpu_device : device);\n    }\n  }\n  return Status::OK();\n}",
        "func": "Status GetDeviceForInput(const EagerOperation& op, const EagerContext& ctx,\n                         TensorHandle* tensor_handle, Device** result) {\n  Device* cpu_device = ctx.HostCPU();\n  string device_name;\n  if (tensor_handle->Type() != TensorHandle::LOCAL) {\n    Device* device = tensor_handle->device();\n    device_name = device != nullptr ? device->name() : cpu_device->name();\n    *result = (device == nullptr ? cpu_device : device);\n  } else if (tensor_handle->dtype == DT_RESOURCE) {\n    // Use the resource's actual device because it is the device that will\n    // influence partitioning the multi-device function.\n    const Tensor* tensor;\n    // TODO(fishx): Avoid blocking here.\n    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));\n    if (tensor->NumElements() == 0) {\n      return errors::InvalidArgument(\"Empty resource handle\");\n    }\n    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);\n    device_name = handle.device();\n\n    Device* input_device;\n    TF_RETURN_IF_ERROR(\n        ctx.FindDeviceFromName(device_name.c_str(), &input_device));\n    *result = input_device;\n  } else {\n    Device* device = tensor_handle->device();\n    const bool is_tpu = device != nullptr && device->device_type() == \"TPU\";\n    // int32 return values can be placed on TPUs.\n    const bool use_host_memory =\n        is_tpu ? MTypeFromDTypeIntsOnDevice(tensor_handle->dtype)\n               : MTypeFromDType(tensor_handle->dtype);\n    if (use_host_memory) {\n      *result = cpu_device;\n    } else {\n      // Eager ops executing as functions should have their preferred inputs set\n      // to the op's device. This allows us to avoid expensive D2H copies if a\n      // mirror of the tensor already exists on the op's device.\n      if (!op.is_function() && device != nullptr && device != cpu_device) {\n        device = absl::get<Device*>(op.Device());\n      }\n      *result = (device == nullptr ? cpu_device : device);\n    }\n  }\n  return Status::OK();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,9 @@\n     const Tensor* tensor;\n     // TODO(fishx): Avoid blocking here.\n     TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));\n+    if (tensor->NumElements() == 0) {\n+      return errors::InvalidArgument(\"Empty resource handle\");\n+    }\n     const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);\n     device_name = handle.device();\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (tensor->NumElements() == 0) {",
                "      return errors::InvalidArgument(\"Empty resource handle\");",
                "    }"
            ]
        }
    }
]