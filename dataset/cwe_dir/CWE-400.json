[
    {
        "cve_id": "CVE-2017-9732",
        "func_name": "elric1/knc/gstd_accept",
        "description": "The read_packet function in knc (Kerberised NetCat) before 1.11-1 is vulnerable to denial of service (memory exhaustion) that can be exploited remotely without authentication, possibly affecting another services running on the targeted host.",
        "git_url": "https://github.com/elric1/knc/commit/f237f3e09ecbaf59c897f5046538a7b1a3fa40c1",
        "commit_title": "knc: fix a couple of memory leaks.",
        "commit_text": " One of these can be remotely triggered during the authentication phase which leads to a remote DoS possibility. ",
        "func_before": "void *\ngstd_accept(int fd, char **display_creds, char **export_name, char **mech)\n{\n\tgss_name_t\t client;\n\tgss_OID\t\t mech_oid;\n\tstruct gstd_tok *tok;\n\tgss_ctx_id_t\t ctx = GSS_C_NO_CONTEXT;\n\tgss_buffer_desc\t in, out;\n\tOM_uint32\t maj, min;\n\tint\t\t ret;\n\n\t*display_creds = NULL;\n\t*export_name = NULL;\n\tout.length = 0;\n\tin.length = 0;\n\tread_packet(fd, &in, 60000, 1);\nagain:\n\twhile ((ret = read_packet(fd, &in, 60000, 0)) == -2)\n\t\t;\n\n\tif (ret < 1)\n\t\treturn NULL;\n\n\tmaj = gss_accept_sec_context(&min, &ctx, GSS_C_NO_CREDENTIAL,\n\t    &in, GSS_C_NO_CHANNEL_BINDINGS, &client, &mech_oid, &out, NULL,\n\t    NULL, NULL);\n\n\tif (out.length && write_packet(fd, &out)) {\n\t\tgss_release_buffer(&min, &out);\n\t\treturn NULL;\n\t}\n\n\tGSTD_GSS_ERROR(maj, min, NULL, \"gss_accept_sec_context\");\n\n\tif (maj & GSS_S_CONTINUE_NEEDED)\n\t\tgoto again;\n\n\t*display_creds = gstd_get_display_name(client);\n\t*export_name = gstd_get_export_name(client);\n\t*mech = gstd_get_mech(mech_oid);\n\n\tgss_release_name(&min, &client);\n\tSETUP_GSTD_TOK(tok, ctx, fd, \"gstd_accept\");\n\treturn tok;\n}",
        "func": "void *\ngstd_accept(int fd, char **display_creds, char **export_name, char **mech)\n{\n\tgss_name_t\t client;\n\tgss_OID\t\t mech_oid;\n\tstruct gstd_tok *tok;\n\tgss_ctx_id_t\t ctx = GSS_C_NO_CONTEXT;\n\tgss_buffer_desc\t in, out;\n\tOM_uint32\t maj, min;\n\tint\t\t ret;\n\n\t*display_creds = NULL;\n\t*export_name = NULL;\n\tout.length = 0;\n\tin.length = 0;\n\tread_packet(fd, &in, 60000, 1);\nagain:\n\twhile ((ret = read_packet(fd, &in, 60000, 0)) == -2)\n\t\t;\n\n\tif (ret < 1)\n\t\treturn NULL;\n\n\tmaj = gss_accept_sec_context(&min, &ctx, GSS_C_NO_CREDENTIAL,\n\t    &in, GSS_C_NO_CHANNEL_BINDINGS, &client, &mech_oid, &out, NULL,\n\t    NULL, NULL);\n\n\tgss_release_buffer(&min, &in);\n\n\tif (out.length && write_packet(fd, &out)) {\n\t\tgss_release_buffer(&min, &out);\n\t\treturn NULL;\n\t}\n\tgss_release_buffer(&min, &out);\n\n\tGSTD_GSS_ERROR(maj, min, NULL, \"gss_accept_sec_context\");\n\n\tif (maj & GSS_S_CONTINUE_NEEDED)\n\t\tgoto again;\n\n\t*display_creds = gstd_get_display_name(client);\n\t*export_name = gstd_get_export_name(client);\n\t*mech = gstd_get_mech(mech_oid);\n\n\tgss_release_name(&min, &client);\n\tSETUP_GSTD_TOK(tok, ctx, fd, \"gstd_accept\");\n\treturn tok;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,10 +25,13 @@\n \t    &in, GSS_C_NO_CHANNEL_BINDINGS, &client, &mech_oid, &out, NULL,\n \t    NULL, NULL);\n \n+\tgss_release_buffer(&min, &in);\n+\n \tif (out.length && write_packet(fd, &out)) {\n \t\tgss_release_buffer(&min, &out);\n \t\treturn NULL;\n \t}\n+\tgss_release_buffer(&min, &out);\n \n \tGSTD_GSS_ERROR(maj, min, NULL, \"gss_accept_sec_context\");\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tgss_release_buffer(&min, &in);",
                "",
                "\tgss_release_buffer(&min, &out);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-9732",
        "func_name": "elric1/knc/read_packet",
        "description": "The read_packet function in knc (Kerberised NetCat) before 1.11-1 is vulnerable to denial of service (memory exhaustion) that can be exploited remotely without authentication, possibly affecting another services running on the targeted host.",
        "git_url": "https://github.com/elric1/knc/commit/f237f3e09ecbaf59c897f5046538a7b1a3fa40c1",
        "commit_title": "knc: fix a couple of memory leaks.",
        "commit_text": " One of these can be remotely triggered during the authentication phase which leads to a remote DoS possibility. ",
        "func_before": "static int\nread_packet(int fd, gss_buffer_t buf, int timeout, int first)\n{\n\tint\t  ret;\n\n\tstatic uint32_t\t\tlen = 0;\n\tstatic char\t\tlen_buf[4];\n\tstatic int\t\tlen_buf_pos = 0;\n\tstatic char *\t\ttmpbuf = 0;\n\tstatic int\t\ttmpbuf_pos = 0;\n\n\tif (first) {\n\t\tlen_buf_pos = 0;\n\t\treturn -2;\n\t}\n\n\tif (len_buf_pos < 4) {\n\t\tret = timed_read(fd, &len_buf[len_buf_pos], 4 - len_buf_pos,\n\t\t    timeout);\n\n\t\tif (ret == -1) {\n\t\t\tif (errno == EINTR || errno == EAGAIN)\n\t\t\t\treturn -2;\n\n\t\t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (ret == 0) {\t\t/* EOF */\n\t\t\t/* Failure to read ANY length just means we're done */\n\t\t\tif (len_buf_pos == 0)\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * Otherwise, we got EOF mid-length, and that's\n\t\t\t * a protocol error.\n\t\t\t */\n\t\t\tLOG(LOG_INFO, (\"EOF reading packet len\"));\n\t\t\treturn -1;\n\t\t}\n\n\t\tlen_buf_pos += ret;\n\t}\n\n\t/* Not done reading the length? */\n\tif (len_buf_pos != 4)\n\t\treturn -2;\n\n\t/* We have the complete length */\n\tlen = ntohl(*(uint32_t *)len_buf);\n\n\t/*\n\t * We make sure recvd length is reasonable, allowing for some\n\t * slop in enc overhead, beyond the actual maximum number of\n\t * bytes of decrypted payload.\n\t */\n\tif (len > GSTD_MAXPACKETCONTENTS + 512) {\n\t\tLOG(LOG_ERR, (\"ridiculous length, %ld\", len));\n\t\treturn -1;\n\t}\n\n\tif (!tmpbuf) {\n\t\tif ((tmpbuf = malloc(len)) == NULL) {\n\t\t\tLOG(LOG_CRIT, (\"malloc failure, %ld bytes\", len));\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tret = timed_read(fd, tmpbuf + tmpbuf_pos, len - tmpbuf_pos, timeout);\n\tif (ret == -1) {\n\t\tif (errno == EINTR || errno == EAGAIN)\n\t\t\treturn -2;\n\n\t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n\t\treturn -1;\n\t}\n\n\tif (ret == 0) {\n\t\tLOG(LOG_ERR, (\"EOF while reading packet (len=%d)\", len));\n\t\treturn -1;\n\t}\n\n\ttmpbuf_pos += ret;\n\n\tif (tmpbuf_pos == len) {\n\t\tbuf->length = len;\n\t\tbuf->value = tmpbuf;\n\t\tlen = len_buf_pos = tmpbuf_pos = 0;\n\t\ttmpbuf = NULL;\n\n\t\tLOG(LOG_DEBUG, (\"read packet of length %d\", buf->length));\n\t\treturn 1;\n\t}\n\n\treturn -2;\n}",
        "func": "static int\nread_packet(int fd, gss_buffer_t buf, int timeout, int first)\n{\n\tint\t  ret;\n\n\tstatic uint32_t\t\tlen = 0;\n\tstatic char\t\tlen_buf[4];\n\tstatic int\t\tlen_buf_pos = 0;\n\tstatic char *\t\ttmpbuf = 0;\n\tstatic int\t\ttmpbuf_pos = 0;\n\n\tif (first) {\n\t\tlen_buf_pos = 0;\n\t\treturn -2;\n\t}\n\n\tif (len_buf_pos < 4) {\n\t\tret = timed_read(fd, &len_buf[len_buf_pos], 4 - len_buf_pos,\n\t\t    timeout);\n\n\t\tif (ret == -1) {\n\t\t\tif (errno == EINTR || errno == EAGAIN)\n\t\t\t\treturn -2;\n\n\t\t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n\t\t\tgoto bail;\n\t\t}\n\n\t\tif (ret == 0) {\t\t/* EOF */\n\t\t\t/* Failure to read ANY length just means we're done */\n\t\t\tif (len_buf_pos == 0)\n\t\t\t\treturn 0;\n\n\t\t\t/*\n\t\t\t * Otherwise, we got EOF mid-length, and that's\n\t\t\t * a protocol error.\n\t\t\t */\n\t\t\tLOG(LOG_INFO, (\"EOF reading packet len\"));\n\t\t\tgoto bail;\n\t\t}\n\n\t\tlen_buf_pos += ret;\n\t}\n\n\t/* Not done reading the length? */\n\tif (len_buf_pos != 4)\n\t\treturn -2;\n\n\t/* We have the complete length */\n\tlen = ntohl(*(uint32_t *)len_buf);\n\n\t/*\n\t * We make sure recvd length is reasonable, allowing for some\n\t * slop in enc overhead, beyond the actual maximum number of\n\t * bytes of decrypted payload.\n\t */\n\tif (len > GSTD_MAXPACKETCONTENTS + 512) {\n\t\tLOG(LOG_ERR, (\"ridiculous length, %ld\", len));\n\t\tgoto bail;\n\t}\n\n\tif (!tmpbuf) {\n\t\tif ((tmpbuf = malloc(len)) == NULL) {\n\t\t\tLOG(LOG_CRIT, (\"malloc failure, %ld bytes\", len));\n\t\t\tgoto bail;\n\t\t}\n\t}\n\n\tret = timed_read(fd, tmpbuf + tmpbuf_pos, len - tmpbuf_pos, timeout);\n\tif (ret == -1) {\n\n\t\tif (errno == EINTR || errno == EAGAIN)\n\t\t\treturn -2;\n\n\t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n\t\tgoto bail;\n\t}\n\n\tif (ret == 0) {\n\t\tLOG(LOG_ERR, (\"EOF while reading packet (len=%d)\", len));\n\t\tgoto bail;\n\t}\n\n\ttmpbuf_pos += ret;\n\n\tif (tmpbuf_pos == len) {\n\t\tbuf->length = len;\n\t\tbuf->value = tmpbuf;\n\t\tlen = len_buf_pos = tmpbuf_pos = 0;\n\t\ttmpbuf = NULL;\n\n\t\tLOG(LOG_DEBUG, (\"read packet of length %d\", buf->length));\n\t\treturn 1;\n\t}\n\n\treturn -2;\n\nbail:\n\tfree(tmpbuf);\n\ttmpbuf = NULL;\n\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,7 +23,7 @@\n \t\t\t\treturn -2;\n \n \t\t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n-\t\t\treturn -1;\n+\t\t\tgoto bail;\n \t\t}\n \n \t\tif (ret == 0) {\t\t/* EOF */\n@@ -36,7 +36,7 @@\n \t\t\t * a protocol error.\n \t\t\t */\n \t\t\tLOG(LOG_INFO, (\"EOF reading packet len\"));\n-\t\t\treturn -1;\n+\t\t\tgoto bail;\n \t\t}\n \n \t\tlen_buf_pos += ret;\n@@ -56,28 +56,29 @@\n \t */\n \tif (len > GSTD_MAXPACKETCONTENTS + 512) {\n \t\tLOG(LOG_ERR, (\"ridiculous length, %ld\", len));\n-\t\treturn -1;\n+\t\tgoto bail;\n \t}\n \n \tif (!tmpbuf) {\n \t\tif ((tmpbuf = malloc(len)) == NULL) {\n \t\t\tLOG(LOG_CRIT, (\"malloc failure, %ld bytes\", len));\n-\t\t\treturn -1;\n+\t\t\tgoto bail;\n \t\t}\n \t}\n \n \tret = timed_read(fd, tmpbuf + tmpbuf_pos, len - tmpbuf_pos, timeout);\n \tif (ret == -1) {\n+\n \t\tif (errno == EINTR || errno == EAGAIN)\n \t\t\treturn -2;\n \n \t\tLOG(LOG_ERR, (\"%s\", strerror(errno)));\n-\t\treturn -1;\n+\t\tgoto bail;\n \t}\n \n \tif (ret == 0) {\n \t\tLOG(LOG_ERR, (\"EOF while reading packet (len=%d)\", len));\n-\t\treturn -1;\n+\t\tgoto bail;\n \t}\n \n \ttmpbuf_pos += ret;\n@@ -93,4 +94,10 @@\n \t}\n \n \treturn -2;\n+\n+bail:\n+\tfree(tmpbuf);\n+\ttmpbuf = NULL;\n+\n+\treturn -1;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\treturn -1;",
                "\t\t\treturn -1;",
                "\t\treturn -1;",
                "\t\t\treturn -1;",
                "\t\treturn -1;",
                "\t\treturn -1;"
            ],
            "added_lines": [
                "\t\t\tgoto bail;",
                "\t\t\tgoto bail;",
                "\t\tgoto bail;",
                "\t\t\tgoto bail;",
                "",
                "\t\tgoto bail;",
                "\t\tgoto bail;",
                "",
                "bail:",
                "\tfree(tmpbuf);",
                "\ttmpbuf = NULL;",
                "",
                "\treturn -1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12806",
        "func_name": "ImageMagick/format8BIM",
        "description": "In ImageMagick 7.0.6-6, a memory exhaustion vulnerability was found in the function format8BIM, which allows attackers to cause a denial of service.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/4a8a6274f5e690f9106a998de9b8a8f3929402bc",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/660",
        "commit_text": "",
        "func_before": "static int format8BIM(Image *ifile, Image *ofile)\n{\n  char\n    temp[MaxTextExtent];\n\n  unsigned int\n    foundOSType;\n\n  int\n    ID,\n    resCount,\n    i,\n    c;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *PString,\n    *str;\n\n  resCount=0;\n  foundOSType=0; /* found the OSType */\n  (void) foundOSType;\n  c=ReadBlobByte(ifile);\n  while (c != EOF)\n  {\n    if (c == '8')\n      {\n        unsigned char\n          buffer[5];\n\n        buffer[0]=(unsigned char) c;\n        for (i=1; i<4; i++)\n        {\n          c=ReadBlobByte(ifile);\n          if (c == EOF)\n            return(-1);\n          buffer[i] = (unsigned char) c;\n        }\n        buffer[4]=0;\n        if (strcmp((const char *)buffer, \"8BIM\") == 0)\n          foundOSType=1;\n        else\n          continue;\n      }\n    else\n      {\n        c=ReadBlobByte(ifile);\n        continue;\n      }\n    /*\n      We found the OSType (8BIM) and now grab the ID, PString, and Size fields.\n    */\n    ID=ReadBlobMSBSignedShort(ifile);\n    if (ID < 0)\n      return(-1);\n    {\n      unsigned char\n        plen;\n\n      c=ReadBlobByte(ifile);\n      if (c == EOF)\n        return(-1);\n      plen = (unsigned char) c;\n      PString=(unsigned char *) AcquireQuantumMemory((size_t) (plen+\n        MaxTextExtent),sizeof(*PString));\n      if (PString == (unsigned char *) NULL)\n        {\n          printf(\"MemoryAllocationFailed\");\n          return 0;\n        }\n      for (i=0; i<plen; i++)\n      {\n        c=ReadBlobByte(ifile);\n        if (c == EOF)\n          {\n            PString=(unsigned char *) RelinquishMagickMemory(PString);\n            return(-1);\n          }\n        PString[i] = (unsigned char) c;\n      }\n      PString[ plen ] = 0;\n      if ((plen & 0x01) == 0)\n      {\n        c=ReadBlobByte(ifile);\n        if (c == EOF)\n          {\n            PString=(unsigned char *) RelinquishMagickMemory(PString);\n            return(-1);\n          }\n      }\n    }\n    count=ReadBlobMSBSignedLong(ifile);\n    if (count < 0)\n      {\n        PString=(unsigned char *) RelinquishMagickMemory(PString);\n        return(-1);\n      }\n    /* make a buffer to hold the datand snag it from the input stream */\n    str=(unsigned char *) AcquireQuantumMemory((size_t) count,sizeof(*str));\n    if (str == (unsigned char *) NULL)\n      {\n        PString=(unsigned char *) RelinquishMagickMemory(PString);\n        printf(\"MemoryAllocationFailed\");\n        return 0;\n      }\n    for (i=0; i < (ssize_t) count; i++)\n    {\n      c=ReadBlobByte(ifile);\n      if (c == EOF)\n        {\n          PString=(unsigned char *) RelinquishMagickMemory(PString);\n          return(-1);\n        }\n      str[i]=(unsigned char) c;\n    }\n\n    /* we currently skip thumbnails, since it does not make\n     * any sense preserving them in a real world application\n     */\n    if (ID != THUMBNAIL_ID)\n      {\n        /* now finish up by formatting this binary data into\n         * ASCII equivalent\n         */\n        if (strlen((const char *)PString) > 0)\n          (void) FormatLocaleString(temp,MaxTextExtent,\"8BIM#%d#%s=\",ID,\n            PString);\n        else\n          (void) FormatLocaleString(temp,MaxTextExtent,\"8BIM#%d=\",ID);\n        (void) WriteBlobString(ofile,temp);\n        if (ID == IPTC_ID)\n          {\n            formatString(ofile, \"IPTC\", 4);\n            formatIPTCfromBuffer(ofile, (char *)str, (ssize_t) count);\n          }\n        else\n          formatString(ofile, (char *)str, (ssize_t) count);\n      }\n    str=(unsigned char *) RelinquishMagickMemory(str);\n    PString=(unsigned char *) RelinquishMagickMemory(PString);\n    resCount++;\n    c=ReadBlobByte(ifile);\n  }\n  return resCount;\n}",
        "func": "static int format8BIM(Image *ifile, Image *ofile)\n{\n  char\n    temp[MaxTextExtent];\n\n  unsigned int\n    foundOSType;\n\n  int\n    ID,\n    resCount,\n    i,\n    c;\n\n  ssize_t\n    count;\n\n  unsigned char\n    *PString,\n    *str;\n\n  resCount=0;\n  foundOSType=0; /* found the OSType */\n  (void) foundOSType;\n  c=ReadBlobByte(ifile);\n  while (c != EOF)\n  {\n    if (c == '8')\n      {\n        unsigned char\n          buffer[5];\n\n        buffer[0]=(unsigned char) c;\n        for (i=1; i<4; i++)\n        {\n          c=ReadBlobByte(ifile);\n          if (c == EOF)\n            return(-1);\n          buffer[i] = (unsigned char) c;\n        }\n        buffer[4]=0;\n        if (strcmp((const char *)buffer, \"8BIM\") == 0)\n          foundOSType=1;\n        else\n          continue;\n      }\n    else\n      {\n        c=ReadBlobByte(ifile);\n        continue;\n      }\n    /*\n      We found the OSType (8BIM) and now grab the ID, PString, and Size fields.\n    */\n    ID=ReadBlobMSBSignedShort(ifile);\n    if (ID < 0)\n      return(-1);\n    {\n      unsigned char\n        plen;\n\n      c=ReadBlobByte(ifile);\n      if (c == EOF)\n        return(-1);\n      plen = (unsigned char) c;\n      PString=(unsigned char *) AcquireQuantumMemory((size_t) (plen+\n        MaxTextExtent),sizeof(*PString));\n      if (PString == (unsigned char *) NULL)\n        {\n          printf(\"MemoryAllocationFailed\");\n          return 0;\n        }\n      for (i=0; i<plen; i++)\n      {\n        c=ReadBlobByte(ifile);\n        if (c == EOF)\n          {\n            PString=(unsigned char *) RelinquishMagickMemory(PString);\n            return(-1);\n          }\n        PString[i] = (unsigned char) c;\n      }\n      PString[ plen ] = 0;\n      if ((plen & 0x01) == 0)\n      {\n        c=ReadBlobByte(ifile);\n        if (c == EOF)\n          {\n            PString=(unsigned char *) RelinquishMagickMemory(PString);\n            return(-1);\n          }\n      }\n    }\n    count=(ssize_t) ReadBlobMSBSignedLong(ifile);\n    if (count < 0)\n      {\n        PString=(unsigned char *) RelinquishMagickMemory(PString);\n        return(-1);\n      }\n    /* make a buffer to hold the datand snag it from the input stream */\n    str=(unsigned char *) AcquireQuantumMemory((size_t) count,sizeof(*str));\n    if (str == (unsigned char *) NULL)\n      {\n        PString=(unsigned char *) RelinquishMagickMemory(PString);\n        printf(\"MemoryAllocationFailed\");\n        return 0;\n      }\n    for (i=0; i < (ssize_t) count; i++)\n    {\n      c=ReadBlobByte(ifile);\n      if (c == EOF)\n        {\n          PString=(unsigned char *) RelinquishMagickMemory(PString);\n          return(-1);\n        }\n      str[i]=(unsigned char) c;\n    }\n\n    /* we currently skip thumbnails, since it does not make\n     * any sense preserving them in a real world application\n     */\n    if (ID != THUMBNAIL_ID)\n      {\n        /* now finish up by formatting this binary data into\n         * ASCII equivalent\n         */\n        if (strlen((const char *)PString) > 0)\n          (void) FormatLocaleString(temp,MaxTextExtent,\"8BIM#%d#%s=\",ID,\n            PString);\n        else\n          (void) FormatLocaleString(temp,MaxTextExtent,\"8BIM#%d=\",ID);\n        (void) WriteBlobString(ofile,temp);\n        if (ID == IPTC_ID)\n          {\n            formatString(ofile, \"IPTC\", 4);\n            formatIPTCfromBuffer(ofile, (char *)str, (ssize_t) count);\n          }\n        else\n          formatString(ofile, (char *)str, (ssize_t) count);\n      }\n    str=(unsigned char *) RelinquishMagickMemory(str);\n    PString=(unsigned char *) RelinquishMagickMemory(PString);\n    resCount++;\n    c=ReadBlobByte(ifile);\n  }\n  return resCount;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -91,7 +91,7 @@\n           }\n       }\n     }\n-    count=ReadBlobMSBSignedLong(ifile);\n+    count=(ssize_t) ReadBlobMSBSignedLong(ifile);\n     if (count < 0)\n       {\n         PString=(unsigned char *) RelinquishMagickMemory(PString);",
        "diff_line_info": {
            "deleted_lines": [
                "    count=ReadBlobMSBSignedLong(ifile);"
            ],
            "added_lines": [
                "    count=(ssize_t) ReadBlobMSBSignedLong(ifile);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11478",
        "func_name": "kernel/git/netdev/net/tcp_fragment",
        "description": "Jonathan Looney discovered that the TCP retransmission queue implementation in tcp_fragment in the Linux kernel could be fragmented when handling certain TCP Selective Acknowledgment (SACK) sequences. A remote attacker could use this to cause a denial of service. This has been fixed in stable kernel releases 4.4.182, 4.9.182, 4.14.127, 4.19.52, 5.1.11, and is fixed in commit f070ef2ac66716357066b683fb0baf55f8191a2e.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?h=f070ef2ac66716357066b683fb0baf55f8191a2e",
        "commit_title": "Jonathan Looney reported that a malicious peer can force a sender",
        "commit_text": "to fragment its retransmit queue into tiny skbs, inflating memory usage and/or overflow 32bit counters.  TCP allows an application to queue up to sk_sndbuf bytes, so we need to give some allowance for non malicious splitting of retransmit queue.  A new SNMP counter is added to monitor how many times TCP did not allow to split an skb if the allowance was exceeded.  Note that this counter might increase in the case applications use SO_SNDBUF socket option to lower sk_sndbuf.  CVE-2019-11478 : tcp_fragment, prevent fragmenting a packet when the \tsocket is already using more than half the allowed space  Cc: Bruce Curtis <brucec@netflix.com> Cc: Jonathan Lemon <jonathan.lemon@gmail.com> ",
        "func_before": "int tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint nsize, old_factor;\n\tint nlen;\n\tu8 flags;\n\n\tif (WARN_ON(len > skb->len))\n\t\treturn -EINVAL;\n\n\tnsize = skb_headlen(skb) - len;\n\tif (nsize < 0)\n\t\tnsize = 0;\n\n\tif (skb_unclone(skb, gfp))\n\t\treturn -ENOMEM;\n\n\t/* Get a new skb... force flag on. */\n\tbuff = sk_stream_alloc_skb(sk, nsize, gfp, true);\n\tif (!buff)\n\t\treturn -ENOMEM; /* We'll just try again later. */\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tnlen = skb->len - len - nsize;\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\tTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\n\tbuff->tstamp = skb->tstamp;\n\ttcp_fragment_tstamp(skb, buff);\n\n\told_factor = tcp_skb_pcount(skb);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Update delivered info for the new segment */\n\tTCP_SKB_CB(buff)->tx = TCP_SKB_CB(skb)->tx;\n\n\t/* If this packet has been sent out already, we must\n\t * adjust the various packet counters.\n\t */\n\tif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\n\t\tint diff = old_factor - tcp_skb_pcount(skb) -\n\t\t\ttcp_skb_pcount(buff);\n\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t}\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\n\t\tlist_add(&buff->tcp_tsorted_anchor, &skb->tcp_tsorted_anchor);\n\n\treturn 0;\n}",
        "func": "int tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint nsize, old_factor;\n\tint nlen;\n\tu8 flags;\n\n\tif (WARN_ON(len > skb->len))\n\t\treturn -EINVAL;\n\n\tnsize = skb_headlen(skb) - len;\n\tif (nsize < 0)\n\t\tnsize = 0;\n\n\tif (unlikely((sk->sk_wmem_queued >> 1) > sk->sk_sndbuf)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (skb_unclone(skb, gfp))\n\t\treturn -ENOMEM;\n\n\t/* Get a new skb... force flag on. */\n\tbuff = sk_stream_alloc_skb(sk, nsize, gfp, true);\n\tif (!buff)\n\t\treturn -ENOMEM; /* We'll just try again later. */\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tnlen = skb->len - len - nsize;\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\tTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tskb_split(skb, buff, len);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\n\tbuff->tstamp = skb->tstamp;\n\ttcp_fragment_tstamp(skb, buff);\n\n\told_factor = tcp_skb_pcount(skb);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Update delivered info for the new segment */\n\tTCP_SKB_CB(buff)->tx = TCP_SKB_CB(skb)->tx;\n\n\t/* If this packet has been sent out already, we must\n\t * adjust the various packet counters.\n\t */\n\tif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\n\t\tint diff = old_factor - tcp_skb_pcount(skb) -\n\t\t\ttcp_skb_pcount(buff);\n\n\t\tif (diff)\n\t\t\ttcp_adjust_pcount(sk, skb, diff);\n\t}\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, tcp_queue);\n\tif (tcp_queue == TCP_FRAG_IN_RTX_QUEUE)\n\t\tlist_add(&buff->tcp_tsorted_anchor, &skb->tcp_tsorted_anchor);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,11 @@\n \tnsize = skb_headlen(skb) - len;\n \tif (nsize < 0)\n \t\tnsize = 0;\n+\n+\tif (unlikely((sk->sk_wmem_queued >> 1) > sk->sk_sndbuf)) {\n+\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);\n+\t\treturn -ENOMEM;\n+\t}\n \n \tif (skb_unclone(skb, gfp))\n \t\treturn -ENOMEM;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (unlikely((sk->sk_wmem_queued >> 1) > sk->sk_sndbuf)) {",
                "\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPWQUEUETOOBIG);",
                "\t\treturn -ENOMEM;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15538",
        "func_name": "torvalds/linux/xfs_setattr_nonsize",
        "description": "An issue was discovered in xfs_setattr_nonsize in fs/xfs/xfs_iops.c in the Linux kernel through 5.2.9. XFS partially wedges when a chgrp fails on account of being out of disk quota. xfs_setattr_nonsize is failing to unlock the ILOCK after the xfs_qm_vop_chown_reserve call fails. This is primarily a local DoS attack vector, but it might result as well in remote DoS if the XFS filesystem is exported for instance via NFS.",
        "git_url": "https://github.com/torvalds/linux/commit/1fb254aa983bf190cfd685d40c64a480a9bafaee",
        "commit_title": "xfs: fix missing ILOCK unlock when xfs_setattr_nonsize fails due to EDQUOT",
        "commit_text": " Benjamin Moody reported to Debian that XFS partially wedges when a chgrp fails on account of being out of disk quota.  I ran his reproducer script:  # adduser dummy # adduser dummy plugdev  # dd if=/dev/zero bs=1M count=100 of=test.img # mkfs.xfs test.img # mount -t xfs -o gquota test.img /mnt # mkdir -p /mnt/dummy # chown -c dummy /mnt/dummy # xfs_quota -xc 'limit -g bsoft=100k bhard=100k plugdev' /mnt  (and then as user dummy)  $ dd if=/dev/urandom bs=1M count=50 of=/mnt/dummy/foo $ chgrp plugdev /mnt/dummy/foo  and saw:  ================================================ WARNING: lock held when returning to user space! 5.3.0-rc5 #rc5 Tainted: G        W ------------------------------------------------ chgrp/47006 is leaving the kernel with locks still held! 1 lock held by chgrp/47006:  #0: 000000006664ea2d (&xfs_nondir_ilock_class){++++}, at: xfs_ilock+0xd2/0x290 [xfs]  ...which is clearly caused by xfs_setattr_nonsize failing to unlock the ILOCK after the xfs_qm_vop_chown_reserve call fails.  Add the missing unlock. ",
        "func_before": "int\nxfs_setattr_nonsize(\n\tstruct xfs_inode\t*ip,\n\tstruct iattr\t\t*iattr,\n\tint\t\t\tflags)\n{\n\txfs_mount_t\t\t*mp = ip->i_mount;\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tint\t\t\tmask = iattr->ia_valid;\n\txfs_trans_t\t\t*tp;\n\tint\t\t\terror;\n\tkuid_t\t\t\tuid = GLOBAL_ROOT_UID, iuid = GLOBAL_ROOT_UID;\n\tkgid_t\t\t\tgid = GLOBAL_ROOT_GID, igid = GLOBAL_ROOT_GID;\n\tstruct xfs_dquot\t*udqp = NULL, *gdqp = NULL;\n\tstruct xfs_dquot\t*olddquot1 = NULL, *olddquot2 = NULL;\n\n\tASSERT((mask & ATTR_SIZE) == 0);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & (ATTR_UID|ATTR_GID))) {\n\t\tuint\tqflags = 0;\n\n\t\tif ((mask & ATTR_UID) && XFS_IS_UQUOTA_ON(mp)) {\n\t\t\tuid = iattr->ia_uid;\n\t\t\tqflags |= XFS_QMOPT_UQUOTA;\n\t\t} else {\n\t\t\tuid = inode->i_uid;\n\t\t}\n\t\tif ((mask & ATTR_GID) && XFS_IS_GQUOTA_ON(mp)) {\n\t\t\tgid = iattr->ia_gid;\n\t\t\tqflags |= XFS_QMOPT_GQUOTA;\n\t\t}  else {\n\t\t\tgid = inode->i_gid;\n\t\t}\n\n\t\t/*\n\t\t * We take a reference when we initialize udqp and gdqp,\n\t\t * so it is important that we never blindly double trip on\n\t\t * the same variable. See xfs_create() for an example.\n\t\t */\n\t\tASSERT(udqp == NULL);\n\t\tASSERT(gdqp == NULL);\n\t\terror = xfs_qm_vop_dqalloc(ip, xfs_kuid_to_uid(uid),\n\t\t\t\t\t   xfs_kgid_to_gid(gid),\n\t\t\t\t\t   xfs_get_projid(ip),\n\t\t\t\t\t   qflags, &udqp, &gdqp, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_ichange, 0, 0, 0, &tp);\n\tif (error)\n\t\tgoto out_dqrele;\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & (ATTR_UID|ATTR_GID)) {\n\t\t/*\n\t\t * These IDs could have changed since we last looked at them.\n\t\t * But, we're assured that if the ownership did change\n\t\t * while we didn't have the inode locked, inode's dquot(s)\n\t\t * would have changed also.\n\t\t */\n\t\tiuid = inode->i_uid;\n\t\tigid = inode->i_gid;\n\t\tgid = (mask & ATTR_GID) ? iattr->ia_gid : igid;\n\t\tuid = (mask & ATTR_UID) ? iattr->ia_uid : iuid;\n\n\t\t/*\n\t\t * Do a quota reservation only if uid/gid is actually\n\t\t * going to change.\n\t\t */\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    ((XFS_IS_UQUOTA_ON(mp) && !uid_eq(iuid, uid)) ||\n\t\t     (XFS_IS_GQUOTA_ON(mp) && !gid_eq(igid, gid)))) {\n\t\t\tASSERT(tp);\n\t\t\terror = xfs_qm_vop_chown_reserve(tp, ip, udqp, gdqp,\n\t\t\t\t\t\tNULL, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (error)\t/* out of quota */\n\t\t\t\tgoto out_cancel;\n\t\t}\n\t}\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & (ATTR_UID|ATTR_GID)) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((inode->i_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable(CAP_FSETID))\n\t\t\tinode->i_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (!uid_eq(iuid, uid)) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_UQUOTA_ON(mp)) {\n\t\t\t\tASSERT(mask & ATTR_UID);\n\t\t\t\tASSERT(udqp);\n\t\t\t\tolddquot1 = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_udquot, udqp);\n\t\t\t}\n\t\t\tip->i_d.di_uid = xfs_kuid_to_uid(uid);\n\t\t\tinode->i_uid = uid;\n\t\t}\n\t\tif (!gid_eq(igid, gid)) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_GQUOTA_ON(mp)) {\n\t\t\t\tASSERT(xfs_sb_version_has_pquotino(&mp->m_sb) ||\n\t\t\t\t       !XFS_IS_PQUOTA_ON(mp));\n\t\t\t\tASSERT(mask & ATTR_GID);\n\t\t\t\tASSERT(gdqp);\n\t\t\t\tolddquot2 = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_gdquot, gdqp);\n\t\t\t}\n\t\t\tip->i_d.di_gid = xfs_kgid_to_gid(gid);\n\t\t\tinode->i_gid = gid;\n\t\t}\n\t}\n\n\tif (mask & ATTR_MODE)\n\t\txfs_setattr_mode(ip, iattr);\n\tif (mask & (ATTR_ATIME|ATTR_CTIME|ATTR_MTIME))\n\t\txfs_setattr_time(ip, iattr);\n\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(mp, xs_ig_attrchg);\n\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\terror = xfs_trans_commit(tp);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot1);\n\txfs_qm_dqrele(olddquot2);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(gdqp);\n\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * XXX(hch): Updating the ACL entries is not atomic vs the i_mode\n\t * \t     update.  We could avoid this with linked transactions\n\t * \t     and passing down the transaction pointer all the way\n\t *\t     to attr_set.  No previous user of the generic\n\t * \t     Posix ACL code seems to care about this issue either.\n\t */\n\tif ((mask & ATTR_MODE) && !(flags & XFS_ATTR_NOACL)) {\n\t\terror = posix_acl_chmod(inode, inode->i_mode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n\nout_cancel:\n\txfs_trans_cancel(tp);\nout_dqrele:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(gdqp);\n\treturn error;\n}",
        "func": "int\nxfs_setattr_nonsize(\n\tstruct xfs_inode\t*ip,\n\tstruct iattr\t\t*iattr,\n\tint\t\t\tflags)\n{\n\txfs_mount_t\t\t*mp = ip->i_mount;\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tint\t\t\tmask = iattr->ia_valid;\n\txfs_trans_t\t\t*tp;\n\tint\t\t\terror;\n\tkuid_t\t\t\tuid = GLOBAL_ROOT_UID, iuid = GLOBAL_ROOT_UID;\n\tkgid_t\t\t\tgid = GLOBAL_ROOT_GID, igid = GLOBAL_ROOT_GID;\n\tstruct xfs_dquot\t*udqp = NULL, *gdqp = NULL;\n\tstruct xfs_dquot\t*olddquot1 = NULL, *olddquot2 = NULL;\n\n\tASSERT((mask & ATTR_SIZE) == 0);\n\n\t/*\n\t * If disk quotas is on, we make sure that the dquots do exist on disk,\n\t * before we start any other transactions. Trying to do this later\n\t * is messy. We don't care to take a readlock to look at the ids\n\t * in inode here, because we can't hold it across the trans_reserve.\n\t * If the IDs do change before we take the ilock, we're covered\n\t * because the i_*dquot fields will get updated anyway.\n\t */\n\tif (XFS_IS_QUOTA_ON(mp) && (mask & (ATTR_UID|ATTR_GID))) {\n\t\tuint\tqflags = 0;\n\n\t\tif ((mask & ATTR_UID) && XFS_IS_UQUOTA_ON(mp)) {\n\t\t\tuid = iattr->ia_uid;\n\t\t\tqflags |= XFS_QMOPT_UQUOTA;\n\t\t} else {\n\t\t\tuid = inode->i_uid;\n\t\t}\n\t\tif ((mask & ATTR_GID) && XFS_IS_GQUOTA_ON(mp)) {\n\t\t\tgid = iattr->ia_gid;\n\t\t\tqflags |= XFS_QMOPT_GQUOTA;\n\t\t}  else {\n\t\t\tgid = inode->i_gid;\n\t\t}\n\n\t\t/*\n\t\t * We take a reference when we initialize udqp and gdqp,\n\t\t * so it is important that we never blindly double trip on\n\t\t * the same variable. See xfs_create() for an example.\n\t\t */\n\t\tASSERT(udqp == NULL);\n\t\tASSERT(gdqp == NULL);\n\t\terror = xfs_qm_vop_dqalloc(ip, xfs_kuid_to_uid(uid),\n\t\t\t\t\t   xfs_kgid_to_gid(gid),\n\t\t\t\t\t   xfs_get_projid(ip),\n\t\t\t\t\t   qflags, &udqp, &gdqp, NULL);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_ichange, 0, 0, 0, &tp);\n\tif (error)\n\t\tgoto out_dqrele;\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\txfs_trans_ijoin(tp, ip, 0);\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & (ATTR_UID|ATTR_GID)) {\n\t\t/*\n\t\t * These IDs could have changed since we last looked at them.\n\t\t * But, we're assured that if the ownership did change\n\t\t * while we didn't have the inode locked, inode's dquot(s)\n\t\t * would have changed also.\n\t\t */\n\t\tiuid = inode->i_uid;\n\t\tigid = inode->i_gid;\n\t\tgid = (mask & ATTR_GID) ? iattr->ia_gid : igid;\n\t\tuid = (mask & ATTR_UID) ? iattr->ia_uid : iuid;\n\n\t\t/*\n\t\t * Do a quota reservation only if uid/gid is actually\n\t\t * going to change.\n\t\t */\n\t\tif (XFS_IS_QUOTA_RUNNING(mp) &&\n\t\t    ((XFS_IS_UQUOTA_ON(mp) && !uid_eq(iuid, uid)) ||\n\t\t     (XFS_IS_GQUOTA_ON(mp) && !gid_eq(igid, gid)))) {\n\t\t\tASSERT(tp);\n\t\t\terror = xfs_qm_vop_chown_reserve(tp, ip, udqp, gdqp,\n\t\t\t\t\t\tNULL, capable(CAP_FOWNER) ?\n\t\t\t\t\t\tXFS_QMOPT_FORCE_RES : 0);\n\t\t\tif (error)\t/* out of quota */\n\t\t\t\tgoto out_cancel;\n\t\t}\n\t}\n\n\t/*\n\t * Change file ownership.  Must be the owner or privileged.\n\t */\n\tif (mask & (ATTR_UID|ATTR_GID)) {\n\t\t/*\n\t\t * CAP_FSETID overrides the following restrictions:\n\t\t *\n\t\t * The set-user-ID and set-group-ID bits of a file will be\n\t\t * cleared upon successful return from chown()\n\t\t */\n\t\tif ((inode->i_mode & (S_ISUID|S_ISGID)) &&\n\t\t    !capable(CAP_FSETID))\n\t\t\tinode->i_mode &= ~(S_ISUID|S_ISGID);\n\n\t\t/*\n\t\t * Change the ownerships and register quota modifications\n\t\t * in the transaction.\n\t\t */\n\t\tif (!uid_eq(iuid, uid)) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_UQUOTA_ON(mp)) {\n\t\t\t\tASSERT(mask & ATTR_UID);\n\t\t\t\tASSERT(udqp);\n\t\t\t\tolddquot1 = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_udquot, udqp);\n\t\t\t}\n\t\t\tip->i_d.di_uid = xfs_kuid_to_uid(uid);\n\t\t\tinode->i_uid = uid;\n\t\t}\n\t\tif (!gid_eq(igid, gid)) {\n\t\t\tif (XFS_IS_QUOTA_RUNNING(mp) && XFS_IS_GQUOTA_ON(mp)) {\n\t\t\t\tASSERT(xfs_sb_version_has_pquotino(&mp->m_sb) ||\n\t\t\t\t       !XFS_IS_PQUOTA_ON(mp));\n\t\t\t\tASSERT(mask & ATTR_GID);\n\t\t\t\tASSERT(gdqp);\n\t\t\t\tolddquot2 = xfs_qm_vop_chown(tp, ip,\n\t\t\t\t\t\t\t&ip->i_gdquot, gdqp);\n\t\t\t}\n\t\t\tip->i_d.di_gid = xfs_kgid_to_gid(gid);\n\t\t\tinode->i_gid = gid;\n\t\t}\n\t}\n\n\tif (mask & ATTR_MODE)\n\t\txfs_setattr_mode(ip, iattr);\n\tif (mask & (ATTR_ATIME|ATTR_CTIME|ATTR_MTIME))\n\t\txfs_setattr_time(ip, iattr);\n\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\tXFS_STATS_INC(mp, xs_ig_attrchg);\n\n\tif (mp->m_flags & XFS_MOUNT_WSYNC)\n\t\txfs_trans_set_sync(tp);\n\terror = xfs_trans_commit(tp);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\t/*\n\t * Release any dquot(s) the inode had kept before chown.\n\t */\n\txfs_qm_dqrele(olddquot1);\n\txfs_qm_dqrele(olddquot2);\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(gdqp);\n\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * XXX(hch): Updating the ACL entries is not atomic vs the i_mode\n\t * \t     update.  We could avoid this with linked transactions\n\t * \t     and passing down the transaction pointer all the way\n\t *\t     to attr_set.  No previous user of the generic\n\t * \t     Posix ACL code seems to care about this issue either.\n\t */\n\tif ((mask & ATTR_MODE) && !(flags & XFS_ATTR_NOACL)) {\n\t\terror = posix_acl_chmod(inode, inode->i_mode);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n\nout_cancel:\n\txfs_trans_cancel(tp);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\nout_dqrele:\n\txfs_qm_dqrele(udqp);\n\txfs_qm_dqrele(gdqp);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -178,6 +178,7 @@\n \n out_cancel:\n \txfs_trans_cancel(tp);\n+\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n out_dqrele:\n \txfs_qm_dqrele(udqp);\n \txfs_qm_dqrele(gdqp);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\txfs_iunlock(ip, XFS_ILOCK_EXCL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionImpl::onHeadersCompleteBase",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "int ConnectionImpl::onHeadersCompleteBase() {\n  ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n  completeLastHeader();\n  if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n    // This is not necessarily true, but it's good enough since higher layers only care if this is\n    // HTTP/1.1 or not.\n    protocol_ = Protocol::Http10;\n  }\n  if (Utility::isUpgrade(*current_header_map_)) {\n    // Ignore h2c upgrade requests until we support them.\n    // See https://github.com/envoyproxy/envoy/issues/7161 for details.\n    if (current_header_map_->Upgrade() &&\n        absl::EqualsIgnoreCase(current_header_map_->Upgrade()->value().getStringView(),\n                               Http::Headers::get().UpgradeValues.H2c)) {\n      ENVOY_CONN_LOG(trace, \"removing unsupported h2c upgrade headers.\", connection_);\n      current_header_map_->removeUpgrade();\n      if (current_header_map_->Connection()) {\n        const auto& tokens_to_remove = caseUnorderdSetContainingUpgradeAndHttp2Settings();\n        std::string new_value = StringUtil::removeTokens(\n            current_header_map_->Connection()->value().getStringView(), \",\", tokens_to_remove, \",\");\n        if (new_value.empty()) {\n          current_header_map_->removeConnection();\n        } else {\n          current_header_map_->Connection()->value(new_value);\n        }\n      }\n      current_header_map_->remove(Headers::get().Http2Settings);\n    } else {\n      ENVOY_CONN_LOG(trace, \"codec entering upgrade mode.\", connection_);\n      handling_upgrade_ = true;\n    }\n  }\n\n  int rc = onHeadersComplete(std::move(current_header_map_));\n  current_header_map_.reset();\n  header_parsing_state_ = HeaderParsingState::Done;\n\n  // Returning 2 informs http_parser to not expect a body or further data on this connection.\n  return handling_upgrade_ ? 2 : rc;\n}",
        "func": "int ConnectionImpl::onHeadersCompleteBase() {\n  ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n  completeLastHeader();\n  // Validate that the completed HeaderMap's cached byte size exists and is correct.\n  // This assert iterates over the HeaderMap.\n  ASSERT(current_header_map_->byteSize().has_value() &&\n         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());\n  if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n    // This is not necessarily true, but it's good enough since higher layers only care if this is\n    // HTTP/1.1 or not.\n    protocol_ = Protocol::Http10;\n  }\n  if (Utility::isUpgrade(*current_header_map_)) {\n    // Ignore h2c upgrade requests until we support them.\n    // See https://github.com/envoyproxy/envoy/issues/7161 for details.\n    if (current_header_map_->Upgrade() &&\n        absl::EqualsIgnoreCase(current_header_map_->Upgrade()->value().getStringView(),\n                               Http::Headers::get().UpgradeValues.H2c)) {\n      ENVOY_CONN_LOG(trace, \"removing unsupported h2c upgrade headers.\", connection_);\n      current_header_map_->removeUpgrade();\n      if (current_header_map_->Connection()) {\n        const auto& tokens_to_remove = caseUnorderdSetContainingUpgradeAndHttp2Settings();\n        std::string new_value = StringUtil::removeTokens(\n            current_header_map_->Connection()->value().getStringView(), \",\", tokens_to_remove, \",\");\n        if (new_value.empty()) {\n          current_header_map_->removeConnection();\n        } else {\n          current_header_map_->Connection()->value(new_value);\n        }\n      }\n      current_header_map_->remove(Headers::get().Http2Settings);\n    } else {\n      ENVOY_CONN_LOG(trace, \"codec entering upgrade mode.\", connection_);\n      handling_upgrade_ = true;\n    }\n  }\n\n  int rc = onHeadersComplete(std::move(current_header_map_));\n  current_header_map_.reset();\n  header_parsing_state_ = HeaderParsingState::Done;\n\n  // Returning 2 informs http_parser to not expect a body or further data on this connection.\n  return handling_upgrade_ ? 2 : rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n int ConnectionImpl::onHeadersCompleteBase() {\n   ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n   completeLastHeader();\n+  // Validate that the completed HeaderMap's cached byte size exists and is correct.\n+  // This assert iterates over the HeaderMap.\n+  ASSERT(current_header_map_->byteSize().has_value() &&\n+         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());\n   if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n     // This is not necessarily true, but it's good enough since higher layers only care if this is\n     // HTTP/1.1 or not.",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Validate that the completed HeaderMap's cached byte size exists and is correct.",
                "  // This assert iterates over the HeaderMap.",
                "  ASSERT(current_header_map_->byteSize().has_value() &&",
                "         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionImpl::onHeaderValue",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void ConnectionImpl::onHeaderValue(const char* data, size_t length) {\n  if (header_parsing_state_ == HeaderParsingState::Done) {\n    // Ignore trailers.\n    return;\n  }\n\n  const absl::string_view header_value = absl::string_view(data, length);\n\n  if (strict_header_validation_) {\n    if (!Http::HeaderUtility::headerIsValid(header_value)) {\n      ENVOY_CONN_LOG(debug, \"invalid header value: {}\", connection_, header_value);\n      error_code_ = Http::Code::BadRequest;\n      sendProtocolError();\n      throw CodecProtocolException(\"http/1.1 protocol error: header value contains invalid chars\");\n    }\n  } else if (header_value.find('\\0') != absl::string_view::npos) {\n    // http-parser should filter for this\n    // (https://tools.ietf.org/html/rfc7230#section-3.2.6), but it doesn't today. HeaderStrings\n    // have an invariant that they must not contain embedded zero characters\n    // (NUL, ASCII 0x0).\n    throw CodecProtocolException(\"http/1.1 protocol error: header value contains NUL\");\n  }\n\n  header_parsing_state_ = HeaderParsingState::Value;\n  current_header_value_.append(data, length);\n\n  const uint32_t total =\n      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();\n  if (total > (max_request_headers_kb_ * 1024)) {\n    error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n    sendProtocolError();\n    throw CodecProtocolException(\"headers size exceeds limit\");\n  }\n}",
        "func": "void ConnectionImpl::onHeaderValue(const char* data, size_t length) {\n  if (header_parsing_state_ == HeaderParsingState::Done) {\n    // Ignore trailers.\n    return;\n  }\n\n  const absl::string_view header_value = absl::string_view(data, length);\n\n  if (strict_header_validation_) {\n    if (!Http::HeaderUtility::headerIsValid(header_value)) {\n      ENVOY_CONN_LOG(debug, \"invalid header value: {}\", connection_, header_value);\n      error_code_ = Http::Code::BadRequest;\n      sendProtocolError();\n      throw CodecProtocolException(\"http/1.1 protocol error: header value contains invalid chars\");\n    }\n  } else if (header_value.find('\\0') != absl::string_view::npos) {\n    // http-parser should filter for this\n    // (https://tools.ietf.org/html/rfc7230#section-3.2.6), but it doesn't today. HeaderStrings\n    // have an invariant that they must not contain embedded zero characters\n    // (NUL, ASCII 0x0).\n    throw CodecProtocolException(\"http/1.1 protocol error: header value contains NUL\");\n  }\n\n  header_parsing_state_ = HeaderParsingState::Value;\n  current_header_value_.append(data, length);\n\n  // Verify that the cached value in byte size exists.\n  ASSERT(current_header_map_->byteSize().has_value());\n  const uint32_t total = current_header_field_.size() + current_header_value_.size() +\n                         current_header_map_->byteSize().value();\n  if (total > (max_request_headers_kb_ * 1024)) {\n    error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n    sendProtocolError();\n    throw CodecProtocolException(\"headers size exceeds limit\");\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,8 +24,10 @@\n   header_parsing_state_ = HeaderParsingState::Value;\n   current_header_value_.append(data, length);\n \n-  const uint32_t total =\n-      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();\n+  // Verify that the cached value in byte size exists.\n+  ASSERT(current_header_map_->byteSize().has_value());\n+  const uint32_t total = current_header_field_.size() + current_header_value_.size() +\n+                         current_header_map_->byteSize().value();\n   if (total > (max_request_headers_kb_ * 1024)) {\n     error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n     sendProtocolError();",
        "diff_line_info": {
            "deleted_lines": [
                "  const uint32_t total =",
                "      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();"
            ],
            "added_lines": [
                "  // Verify that the cached value in byte size exists.",
                "  ASSERT(current_header_map_->byteSize().has_value());",
                "  const uint32_t total = current_header_field_.size() + current_header_value_.size() +",
                "                         current_header_map_->byteSize().value();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/Filter::UpstreamRequest::~UpstreamRequest",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "Filter::UpstreamRequest::~UpstreamRequest() {\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::finalizeUpstreamSpan(*span_, upstream_headers_.get(),\n                                                     upstream_trailers_.get(), stream_info_,\n                                                     Tracing::EgressConfig::get());\n  }\n\n  if (per_try_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_timeout_->disableTimer();\n  }\n  clearRequestEncoder();\n\n  stream_info_.setUpstreamTiming(upstream_timing_);\n  stream_info_.onRequestComplete();\n  for (const auto& upstream_log : parent_.config_.upstream_logs_) {\n    upstream_log->log(parent_.downstream_headers_, upstream_headers_.get(),\n                      upstream_trailers_.get(), stream_info_);\n  }\n}",
        "func": "Filter::UpstreamRequest::~UpstreamRequest() {\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::finalizeUpstreamSpan(*span_, upstream_headers_.get(),\n                                                     upstream_trailers_.get(), stream_info_,\n                                                     Tracing::EgressConfig::get());\n  }\n\n  if (per_try_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_timeout_->disableTimer();\n  }\n  clearRequestEncoder();\n\n  stream_info_.setUpstreamTiming(upstream_timing_);\n  stream_info_.onRequestComplete();\n  // Prior to logging, refresh the byte size of the HeaderMaps.\n  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n  // HeaderMap holds an accurate internal byte size count.\n  if (upstream_headers_ != nullptr) {\n    upstream_headers_->refreshByteSize();\n  }\n  if (upstream_trailers_ != nullptr) {\n    upstream_trailers_->refreshByteSize();\n  }\n  for (const auto& upstream_log : parent_.config_.upstream_logs_) {\n    upstream_log->log(parent_.downstream_headers_, upstream_headers_.get(),\n                      upstream_trailers_.get(), stream_info_);\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,15 @@\n \n   stream_info_.setUpstreamTiming(upstream_timing_);\n   stream_info_.onRequestComplete();\n+  // Prior to logging, refresh the byte size of the HeaderMaps.\n+  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n+  // HeaderMap holds an accurate internal byte size count.\n+  if (upstream_headers_ != nullptr) {\n+    upstream_headers_->refreshByteSize();\n+  }\n+  if (upstream_trailers_ != nullptr) {\n+    upstream_trailers_->refreshByteSize();\n+  }\n   for (const auto& upstream_log : parent_.config_.upstream_logs_) {\n     upstream_log->log(parent_.downstream_headers_, upstream_headers_.get(),\n                       upstream_trailers_.get(), stream_info_);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Prior to logging, refresh the byte size of the HeaderMaps.",
                "  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and",
                "  // HeaderMap holds an accurate internal byte size count.",
                "  if (upstream_headers_ != nullptr) {",
                "    upstream_headers_->refreshByteSize();",
                "  }",
                "  if (upstream_trailers_ != nullptr) {",
                "    upstream_trailers_->refreshByteSize();",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/RequestWrapper::operator[]",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "absl::optional<CelValue> RequestWrapper::operator[](CelValue key) const {\n  if (!key.IsString()) {\n    return {};\n  }\n  auto value = key.StringOrDie().value();\n\n  if (value == Headers) {\n    return CelValue::CreateMap(&headers_);\n  } else if (value == Time) {\n    return CelValue::CreateTimestamp(absl::FromChrono(info_.startTime()));\n  } else if (value == Size) {\n    // it is important to make a choice whether to rely on content-length vs stream info\n    // (which is not available at the time of the request headers)\n    if (headers_.value_ != nullptr && headers_.value_->ContentLength() != nullptr) {\n      int64_t length;\n      if (absl::SimpleAtoi(headers_.value_->ContentLength()->value().getStringView(), &length)) {\n        return CelValue::CreateInt64(length);\n      }\n    } else {\n      return CelValue::CreateInt64(info_.bytesReceived());\n    }\n  } else if (value == Duration) {\n    auto duration = info_.requestComplete();\n    if (duration.has_value()) {\n      return CelValue::CreateDuration(absl::FromChrono(duration.value()));\n    }\n  }\n\n  if (headers_.value_ != nullptr) {\n    if (value == Path) {\n      return convertHeaderEntry(headers_.value_->Path());\n    } else if (value == UrlPath) {\n      absl::string_view path = headers_.value_->Path()->value().getStringView();\n      size_t query_offset = path.find('?');\n      if (query_offset == absl::string_view::npos) {\n        return CelValue::CreateString(path);\n      }\n      return CelValue::CreateString(path.substr(0, query_offset));\n    } else if (value == Host) {\n      return convertHeaderEntry(headers_.value_->Host());\n    } else if (value == Scheme) {\n      return convertHeaderEntry(headers_.value_->Scheme());\n    } else if (value == Method) {\n      return convertHeaderEntry(headers_.value_->Method());\n    } else if (value == Referer) {\n      return convertHeaderEntry(headers_.value_->Referer());\n    } else if (value == ID) {\n      return convertHeaderEntry(headers_.value_->RequestId());\n    } else if (value == UserAgent) {\n      return convertHeaderEntry(headers_.value_->UserAgent());\n    } else if (value == TotalSize) {\n      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize());\n    }\n  }\n  return {};\n}",
        "func": "absl::optional<CelValue> RequestWrapper::operator[](CelValue key) const {\n  if (!key.IsString()) {\n    return {};\n  }\n  auto value = key.StringOrDie().value();\n\n  if (value == Headers) {\n    return CelValue::CreateMap(&headers_);\n  } else if (value == Time) {\n    return CelValue::CreateTimestamp(absl::FromChrono(info_.startTime()));\n  } else if (value == Size) {\n    // it is important to make a choice whether to rely on content-length vs stream info\n    // (which is not available at the time of the request headers)\n    if (headers_.value_ != nullptr && headers_.value_->ContentLength() != nullptr) {\n      int64_t length;\n      if (absl::SimpleAtoi(headers_.value_->ContentLength()->value().getStringView(), &length)) {\n        return CelValue::CreateInt64(length);\n      }\n    } else {\n      return CelValue::CreateInt64(info_.bytesReceived());\n    }\n  } else if (value == Duration) {\n    auto duration = info_.requestComplete();\n    if (duration.has_value()) {\n      return CelValue::CreateDuration(absl::FromChrono(duration.value()));\n    }\n  }\n\n  if (headers_.value_ != nullptr) {\n    if (value == Path) {\n      return convertHeaderEntry(headers_.value_->Path());\n    } else if (value == UrlPath) {\n      absl::string_view path = headers_.value_->Path()->value().getStringView();\n      size_t query_offset = path.find('?');\n      if (query_offset == absl::string_view::npos) {\n        return CelValue::CreateString(path);\n      }\n      return CelValue::CreateString(path.substr(0, query_offset));\n    } else if (value == Host) {\n      return convertHeaderEntry(headers_.value_->Host());\n    } else if (value == Scheme) {\n      return convertHeaderEntry(headers_.value_->Scheme());\n    } else if (value == Method) {\n      return convertHeaderEntry(headers_.value_->Method());\n    } else if (value == Referer) {\n      return convertHeaderEntry(headers_.value_->Referer());\n    } else if (value == ID) {\n      return convertHeaderEntry(headers_.value_->RequestId());\n    } else if (value == UserAgent) {\n      return convertHeaderEntry(headers_.value_->UserAgent());\n    } else if (value == TotalSize) {\n      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize().value());\n    }\n  }\n  return {};\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,7 +49,7 @@\n     } else if (value == UserAgent) {\n       return convertHeaderEntry(headers_.value_->UserAgent());\n     } else if (value == TotalSize) {\n-      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize());\n+      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize().value());\n     }\n   }\n   return {};",
        "diff_line_info": {
            "deleted_lines": [
                "      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize());"
            ],
            "added_lines": [
                "      return CelValue::CreateInt64(info_.bytesReceived() + headers_.value_->byteSize().value());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::removePrefix",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::removePrefix(const LowerCaseString& prefix) {\n  headers_.remove_if([&](const HeaderEntryImpl& entry) {\n    bool to_remove = absl::StartsWith(entry.key().getStringView(), prefix.get());\n    if (to_remove) {\n      // If this header should be removed, make sure any references in the\n      // static lookup table are cleared as well.\n      EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(entry.key().getStringView());\n      if (cb) {\n        StaticLookupResponse ref_lookup_response = cb(*this);\n        if (ref_lookup_response.entry_) {\n          *ref_lookup_response.entry_ = nullptr;\n        }\n      }\n    }\n    return to_remove;\n  });\n}",
        "func": "void HeaderMapImpl::removePrefix(const LowerCaseString& prefix) {\n  headers_.remove_if([&prefix, this](const HeaderEntryImpl& entry) {\n    bool to_remove = absl::StartsWith(entry.key().getStringView(), prefix.get());\n    if (to_remove) {\n      // If this header should be removed, make sure any references in the\n      // static lookup table are cleared as well.\n      EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(entry.key().getStringView());\n      if (cb) {\n        StaticLookupResponse ref_lookup_response = cb(*this);\n        if (ref_lookup_response.entry_) {\n          const uint32_t key_value_size = (*ref_lookup_response.entry_)->key().size() +\n                                          (*ref_lookup_response.entry_)->value().size();\n          subtractSize(key_value_size);\n          *ref_lookup_response.entry_ = nullptr;\n        }\n      } else {\n        subtractSize(entry.key().size() + entry.value().size());\n      }\n    }\n    return to_remove;\n  });\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n void HeaderMapImpl::removePrefix(const LowerCaseString& prefix) {\n-  headers_.remove_if([&](const HeaderEntryImpl& entry) {\n+  headers_.remove_if([&prefix, this](const HeaderEntryImpl& entry) {\n     bool to_remove = absl::StartsWith(entry.key().getStringView(), prefix.get());\n     if (to_remove) {\n       // If this header should be removed, make sure any references in the\n@@ -8,8 +8,13 @@\n       if (cb) {\n         StaticLookupResponse ref_lookup_response = cb(*this);\n         if (ref_lookup_response.entry_) {\n+          const uint32_t key_value_size = (*ref_lookup_response.entry_)->key().size() +\n+                                          (*ref_lookup_response.entry_)->value().size();\n+          subtractSize(key_value_size);\n           *ref_lookup_response.entry_ = nullptr;\n         }\n+      } else {\n+        subtractSize(entry.key().size() + entry.value().size());\n       }\n     }\n     return to_remove;",
        "diff_line_info": {
            "deleted_lines": [
                "  headers_.remove_if([&](const HeaderEntryImpl& entry) {"
            ],
            "added_lines": [
                "  headers_.remove_if([&prefix, this](const HeaderEntryImpl& entry) {",
                "          const uint32_t key_value_size = (*ref_lookup_response.entry_)->key().size() +",
                "                                          (*ref_lookup_response.entry_)->value().size();",
                "          subtractSize(key_value_size);",
                "      } else {",
                "        subtractSize(entry.key().size() + entry.value().size());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::remove",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::remove(const LowerCaseString& key) {\n  EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(key.get());\n  if (cb) {\n    StaticLookupResponse ref_lookup_response = cb(*this);\n    removeInline(ref_lookup_response.entry_);\n  } else {\n    for (auto i = headers_.begin(); i != headers_.end();) {\n      if (i->key() == key.get().c_str()) {\n        i = headers_.erase(i);\n      } else {\n        ++i;\n      }\n    }\n  }\n}",
        "func": "void HeaderMapImpl::remove(const LowerCaseString& key) {\n  EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(key.get());\n  if (cb) {\n    StaticLookupResponse ref_lookup_response = cb(*this);\n    removeInline(ref_lookup_response.entry_);\n  } else {\n    for (auto i = headers_.begin(); i != headers_.end();) {\n      if (i->key() == key.get().c_str()) {\n        subtractSize(i->key().size() + i->value().size());\n        i = headers_.erase(i);\n      } else {\n        ++i;\n      }\n    }\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n   } else {\n     for (auto i = headers_.begin(); i != headers_.end();) {\n       if (i->key() == key.get().c_str()) {\n+        subtractSize(i->key().size() + i->value().size());\n         i = headers_.erase(i);\n       } else {\n         ++i;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        subtractSize(i->key().size() + i->value().size());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::removeInline",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::removeInline(HeaderEntryImpl** ptr_to_entry) {\n  if (!*ptr_to_entry) {\n    return;\n  }\n\n  HeaderEntryImpl* entry = *ptr_to_entry;\n  *ptr_to_entry = nullptr;\n  headers_.erase(entry->entry_);\n}",
        "func": "void HeaderMapImpl::removeInline(HeaderEntryImpl** ptr_to_entry) {\n  if (!*ptr_to_entry) {\n    return;\n  }\n\n  HeaderEntryImpl* entry = *ptr_to_entry;\n  const uint64_t size_to_subtract = entry->entry_->key().size() + entry->entry_->value().size();\n  subtractSize(size_to_subtract);\n  *ptr_to_entry = nullptr;\n  headers_.erase(entry->entry_);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,8 @@\n   }\n \n   HeaderEntryImpl* entry = *ptr_to_entry;\n+  const uint64_t size_to_subtract = entry->entry_->key().size() + entry->entry_->value().size();\n+  subtractSize(size_to_subtract);\n   *ptr_to_entry = nullptr;\n   headers_.erase(entry->entry_);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  const uint64_t size_to_subtract = entry->entry_->key().size() + entry->entry_->value().size();",
                "  subtractSize(size_to_subtract);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::byteSize",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "uint64_t HeaderMapImpl::byteSize() const {\n  uint64_t byte_size = 0;\n  for (const HeaderEntryImpl& header : headers_) {\n    byte_size += header.key().size();\n    byte_size += header.value().size();\n  }\n\n  return byte_size;\n}",
        "func": "absl::optional<uint64_t> HeaderMapImpl::byteSize() const { return cached_byte_size_; }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1 @@\n-uint64_t HeaderMapImpl::byteSize() const {\n-  uint64_t byte_size = 0;\n-  for (const HeaderEntryImpl& header : headers_) {\n-    byte_size += header.key().size();\n-    byte_size += header.value().size();\n-  }\n-\n-  return byte_size;\n-}\n+absl::optional<uint64_t> HeaderMapImpl::byteSize() const { return cached_byte_size_; }",
        "diff_line_info": {
            "deleted_lines": [
                "uint64_t HeaderMapImpl::byteSize() const {",
                "  uint64_t byte_size = 0;",
                "  for (const HeaderEntryImpl& header : headers_) {",
                "    byte_size += header.key().size();",
                "    byte_size += header.value().size();",
                "  }",
                "",
                "  return byte_size;",
                "}"
            ],
            "added_lines": [
                "absl::optional<uint64_t> HeaderMapImpl::byteSize() const { return cached_byte_size_; }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::appendToHeader",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {\n  if (data.empty()) {\n    return;\n  }\n  if (!header.empty()) {\n    header.append(\",\", 1);\n  }\n  header.append(data.data(), data.size());\n}",
        "func": "uint64_t HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {\n  if (data.empty()) {\n    return 0;\n  }\n  uint64_t byte_size = 0;\n  if (!header.empty()) {\n    header.append(\",\", 1);\n    byte_size += 1;\n  }\n  header.append(data.data(), data.size());\n  return data.size() + byte_size;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,12 @@\n-void HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {\n+uint64_t HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {\n   if (data.empty()) {\n-    return;\n+    return 0;\n   }\n+  uint64_t byte_size = 0;\n   if (!header.empty()) {\n     header.append(\",\", 1);\n+    byte_size += 1;\n   }\n   header.append(data.data(), data.size());\n+  return data.size() + byte_size;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "void HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {",
                "    return;"
            ],
            "added_lines": [
                "uint64_t HeaderMapImpl::appendToHeader(HeaderString& header, absl::string_view data) {",
                "    return 0;",
                "  uint64_t byte_size = 0;",
                "    byte_size += 1;",
                "  return data.size() + byte_size;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::addCopy",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::addCopy(const LowerCaseString& key, uint64_t value) {\n  auto* entry = getExistingInline(key.get());\n  if (entry != nullptr) {\n    char buf[32];\n    StringUtil::itoa(buf, sizeof(buf), value);\n    appendToHeader(entry->value(), buf);\n    return;\n  }\n  HeaderString new_key;\n  new_key.setCopy(key.get().c_str(), key.get().size());\n  HeaderString new_value;\n  new_value.setInteger(value);\n  insertByKey(std::move(new_key), std::move(new_value));\n  ASSERT(new_key.empty());   // NOLINT(bugprone-use-after-move)\n  ASSERT(new_value.empty()); // NOLINT(bugprone-use-after-move)\n}",
        "func": "void HeaderMapImpl::addCopy(const LowerCaseString& key, uint64_t value) {\n  auto* entry = getExistingInline(key.get());\n  if (entry != nullptr) {\n    char buf[32];\n    StringUtil::itoa(buf, sizeof(buf), value);\n    const uint64_t added_size = appendToHeader(entry->value(), buf);\n    addSize(added_size);\n    return;\n  }\n  HeaderString new_key;\n  new_key.setCopy(key.get().c_str(), key.get().size());\n  HeaderString new_value;\n  new_value.setInteger(value);\n  insertByKey(std::move(new_key), std::move(new_value));\n  ASSERT(new_key.empty());   // NOLINT(bugprone-use-after-move)\n  ASSERT(new_value.empty()); // NOLINT(bugprone-use-after-move)\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,8 @@\n   if (entry != nullptr) {\n     char buf[32];\n     StringUtil::itoa(buf, sizeof(buf), value);\n-    appendToHeader(entry->value(), buf);\n+    const uint64_t added_size = appendToHeader(entry->value(), buf);\n+    addSize(added_size);\n     return;\n   }\n   HeaderString new_key;",
        "diff_line_info": {
            "deleted_lines": [
                "    appendToHeader(entry->value(), buf);"
            ],
            "added_lines": [
                "    const uint64_t added_size = appendToHeader(entry->value(), buf);",
                "    addSize(added_size);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::insertByKey",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::insertByKey(HeaderString&& key, HeaderString&& value) {\n  EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(key.getStringView());\n  if (cb) {\n    key.clear();\n    StaticLookupResponse ref_lookup_response = cb(*this);\n    if (*ref_lookup_response.entry_ == nullptr) {\n      maybeCreateInline(ref_lookup_response.entry_, *ref_lookup_response.key_, std::move(value));\n    } else {\n      appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());\n      value.clear();\n    }\n  } else {\n    std::list<HeaderEntryImpl>::iterator i = headers_.insert(std::move(key), std::move(value));\n    i->entry_ = i;\n  }\n}",
        "func": "void HeaderMapImpl::insertByKey(HeaderString&& key, HeaderString&& value) {\n  EntryCb cb = ConstSingleton<StaticLookupTable>::get().find(key.getStringView());\n  if (cb) {\n    key.clear();\n    StaticLookupResponse ref_lookup_response = cb(*this);\n    if (*ref_lookup_response.entry_ == nullptr) {\n      maybeCreateInline(ref_lookup_response.entry_, *ref_lookup_response.key_, std::move(value));\n    } else {\n      const uint64_t added_size =\n          appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());\n      addSize(added_size);\n      value.clear();\n    }\n  } else {\n    addSize(key.size() + value.size());\n    std::list<HeaderEntryImpl>::iterator i = headers_.insert(std::move(key), std::move(value));\n    i->entry_ = i;\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,10 +6,13 @@\n     if (*ref_lookup_response.entry_ == nullptr) {\n       maybeCreateInline(ref_lookup_response.entry_, *ref_lookup_response.key_, std::move(value));\n     } else {\n-      appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());\n+      const uint64_t added_size =\n+          appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());\n+      addSize(added_size);\n       value.clear();\n     }\n   } else {\n+    addSize(key.size() + value.size());\n     std::list<HeaderEntryImpl>::iterator i = headers_.insert(std::move(key), std::move(value));\n     i->entry_ = i;\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "      appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());"
            ],
            "added_lines": [
                "      const uint64_t added_size =",
                "          appendToHeader((*ref_lookup_response.entry_)->value(), value.getStringView());",
                "      addSize(added_size);",
                "    addSize(key.size() + value.size());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::addCopy",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::addCopy(const LowerCaseString& key, const std::string& value) {\n  auto* entry = getExistingInline(key.get());\n  if (entry != nullptr) {\n    appendToHeader(entry->value(), value);\n    return;\n  }\n  HeaderString new_key;\n  new_key.setCopy(key.get().c_str(), key.get().size());\n  HeaderString new_value;\n  new_value.setCopy(value.c_str(), value.size());\n  insertByKey(std::move(new_key), std::move(new_value));\n  ASSERT(new_key.empty());   // NOLINT(bugprone-use-after-move)\n  ASSERT(new_value.empty()); // NOLINT(bugprone-use-after-move)\n}",
        "func": "void HeaderMapImpl::addCopy(const LowerCaseString& key, const std::string& value) {\n  auto* entry = getExistingInline(key.get());\n  if (entry != nullptr) {\n    const uint64_t added_size = appendToHeader(entry->value(), value);\n    addSize(added_size);\n    return;\n  }\n  HeaderString new_key;\n  new_key.setCopy(key.get().c_str(), key.get().size());\n  HeaderString new_value;\n  new_value.setCopy(value.c_str(), value.size());\n  insertByKey(std::move(new_key), std::move(new_value));\n  ASSERT(new_key.empty());   // NOLINT(bugprone-use-after-move)\n  ASSERT(new_value.empty()); // NOLINT(bugprone-use-after-move)\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,8 @@\n void HeaderMapImpl::addCopy(const LowerCaseString& key, const std::string& value) {\n   auto* entry = getExistingInline(key.get());\n   if (entry != nullptr) {\n-    appendToHeader(entry->value(), value);\n+    const uint64_t added_size = appendToHeader(entry->value(), value);\n+    addSize(added_size);\n     return;\n   }\n   HeaderString new_key;",
        "diff_line_info": {
            "deleted_lines": [
                "    appendToHeader(entry->value(), value);"
            ],
            "added_lines": [
                "    const uint64_t added_size = appendToHeader(entry->value(), value);",
                "    addSize(added_size);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::get",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "HeaderEntry* HeaderMapImpl::get(const LowerCaseString& key) {\n  for (HeaderEntryImpl& header : headers_) {\n    if (header.key() == key.get().c_str()) {\n      return &header;\n    }\n  }\n\n  return nullptr;\n}",
        "func": "HeaderEntry* HeaderMapImpl::get(const LowerCaseString& key) {\n  for (HeaderEntryImpl& header : headers_) {\n    if (header.key() == key.get().c_str()) {\n      cached_byte_size_.reset();\n      return &header;\n    }\n  }\n\n  return nullptr;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n HeaderEntry* HeaderMapImpl::get(const LowerCaseString& key) {\n   for (HeaderEntryImpl& header : headers_) {\n     if (header.key() == key.get().c_str()) {\n+      cached_byte_size_.reset();\n       return &header;\n     }\n   }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      cached_byte_size_.reset();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HeaderMapImpl::addViaMove",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HeaderMapImpl::addViaMove(HeaderString&& key, HeaderString&& value) {\n  // If this is an inline header, we can't addViaMove, because we'll overwrite\n  // the existing value.\n  auto* entry = getExistingInline(key.getStringView());\n  if (entry != nullptr) {\n    appendToHeader(entry->value(), value.getStringView());\n    key.clear();\n    value.clear();\n  } else {\n    insertByKey(std::move(key), std::move(value));\n  }\n}",
        "func": "void HeaderMapImpl::addViaMove(HeaderString&& key, HeaderString&& value) {\n  // If this is an inline header, we can't addViaMove, because we'll overwrite\n  // the existing value.\n  auto* entry = getExistingInline(key.getStringView());\n  if (entry != nullptr) {\n    const uint64_t added_size = appendToHeader(entry->value(), value.getStringView());\n    addSize(added_size);\n    key.clear();\n    value.clear();\n  } else {\n    insertByKey(std::move(key), std::move(value));\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,8 @@\n   // the existing value.\n   auto* entry = getExistingInline(key.getStringView());\n   if (entry != nullptr) {\n-    appendToHeader(entry->value(), value.getStringView());\n+    const uint64_t added_size = appendToHeader(entry->value(), value.getStringView());\n+    addSize(added_size);\n     key.clear();\n     value.clear();\n   } else {",
        "diff_line_info": {
            "deleted_lines": [
                "    appendToHeader(entry->value(), value.getStringView());"
            ],
            "added_lines": [
                "    const uint64_t added_size = appendToHeader(entry->value(), value.getStringView());",
                "    addSize(added_size);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/RoleBasedAccessControlFilter::decodeHeaders",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "Http::FilterHeadersStatus RoleBasedAccessControlFilter::decodeHeaders(Http::HeaderMap& headers,\n                                                                      bool) {\n  ENVOY_LOG(debug,\n            \"checking request: remoteAddress: {}, localAddress: {}, ssl: {}, headers: {}, \"\n            \"dynamicMetadata: {}\",\n            callbacks_->connection()->remoteAddress()->asString(),\n            callbacks_->connection()->localAddress()->asString(),\n            callbacks_->connection()->ssl()\n                ? \"uriSanPeerCertificate: \" +\n                      absl::StrJoin(callbacks_->connection()->ssl()->uriSanPeerCertificate(), \",\") +\n                      \", subjectPeerCertificate: \" +\n                      callbacks_->connection()->ssl()->subjectPeerCertificate()\n                : \"none\",\n            headers, callbacks_->streamInfo().dynamicMetadata().DebugString());\n\n  std::string effective_policy_id;\n  const auto shadow_engine =\n      config_->engine(callbacks_->route(), Filters::Common::RBAC::EnforcementMode::Shadow);\n\n  if (shadow_engine != nullptr) {\n    std::string shadow_resp_code =\n        Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().EngineResultAllowed;\n    if (shadow_engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(),\n                               &effective_policy_id)) {\n      ENVOY_LOG(debug, \"shadow allowed\");\n      config_->stats().shadow_allowed_.inc();\n    } else {\n      ENVOY_LOG(debug, \"shadow denied\");\n      config_->stats().shadow_denied_.inc();\n      shadow_resp_code =\n          Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().EngineResultDenied;\n    }\n\n    ProtobufWkt::Struct metrics;\n\n    auto& fields = *metrics.mutable_fields();\n    if (!effective_policy_id.empty()) {\n      *fields[Filters::Common::RBAC::DynamicMetadataKeysSingleton::get()\n                  .ShadowEffectivePolicyIdField]\n           .mutable_string_value() = effective_policy_id;\n    }\n\n    *fields[Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().ShadowEngineResultField]\n         .mutable_string_value() = shadow_resp_code;\n\n    callbacks_->streamInfo().setDynamicMetadata(HttpFilterNames::get().Rbac, metrics);\n  }\n\n  const auto engine =\n      config_->engine(callbacks_->route(), Filters::Common::RBAC::EnforcementMode::Enforced);\n  if (engine != nullptr) {\n    if (engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(), nullptr)) {\n      ENVOY_LOG(debug, \"enforced allowed\");\n      config_->stats().allowed_.inc();\n      return Http::FilterHeadersStatus::Continue;\n    } else {\n      ENVOY_LOG(debug, \"enforced denied\");\n      callbacks_->sendLocalReply(Http::Code::Forbidden, \"RBAC: access denied\", nullptr,\n                                 absl::nullopt, RcDetails::get().RbacAccessDenied);\n      config_->stats().denied_.inc();\n      return Http::FilterHeadersStatus::StopIteration;\n    }\n  }\n\n  ENVOY_LOG(debug, \"no engine, allowed by default\");\n  return Http::FilterHeadersStatus::Continue;\n}",
        "func": "Http::FilterHeadersStatus RoleBasedAccessControlFilter::decodeHeaders(Http::HeaderMap& headers,\n                                                                      bool) {\n  ENVOY_LOG(debug,\n            \"checking request: remoteAddress: {}, localAddress: {}, ssl: {}, headers: {}, \"\n            \"dynamicMetadata: {}\",\n            callbacks_->connection()->remoteAddress()->asString(),\n            callbacks_->connection()->localAddress()->asString(),\n            callbacks_->connection()->ssl()\n                ? \"uriSanPeerCertificate: \" +\n                      absl::StrJoin(callbacks_->connection()->ssl()->uriSanPeerCertificate(), \",\") +\n                      \", subjectPeerCertificate: \" +\n                      callbacks_->connection()->ssl()->subjectPeerCertificate()\n                : \"none\",\n            headers, callbacks_->streamInfo().dynamicMetadata().DebugString());\n\n  std::string effective_policy_id;\n  const auto shadow_engine =\n      config_->engine(callbacks_->route(), Filters::Common::RBAC::EnforcementMode::Shadow);\n\n  if (shadow_engine != nullptr) {\n    std::string shadow_resp_code =\n        Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().EngineResultAllowed;\n    // Refresh headers byte size before checking if allowed.\n    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n    // HeaderMap holds an accurate internal byte size count.\n    headers.refreshByteSize();\n    if (shadow_engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(),\n                               &effective_policy_id)) {\n      ENVOY_LOG(debug, \"shadow allowed\");\n      config_->stats().shadow_allowed_.inc();\n    } else {\n      ENVOY_LOG(debug, \"shadow denied\");\n      config_->stats().shadow_denied_.inc();\n      shadow_resp_code =\n          Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().EngineResultDenied;\n    }\n\n    ProtobufWkt::Struct metrics;\n\n    auto& fields = *metrics.mutable_fields();\n    if (!effective_policy_id.empty()) {\n      *fields[Filters::Common::RBAC::DynamicMetadataKeysSingleton::get()\n                  .ShadowEffectivePolicyIdField]\n           .mutable_string_value() = effective_policy_id;\n    }\n\n    *fields[Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().ShadowEngineResultField]\n         .mutable_string_value() = shadow_resp_code;\n\n    callbacks_->streamInfo().setDynamicMetadata(HttpFilterNames::get().Rbac, metrics);\n  }\n\n  const auto engine =\n      config_->engine(callbacks_->route(), Filters::Common::RBAC::EnforcementMode::Enforced);\n  if (engine != nullptr) {\n    // Refresh headers byte size before checking if allowed.\n    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n    // HeaderMap holds an accurate internal byte size count.\n    headers.refreshByteSize();\n    if (engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(), nullptr)) {\n      ENVOY_LOG(debug, \"enforced allowed\");\n      config_->stats().allowed_.inc();\n      return Http::FilterHeadersStatus::Continue;\n    } else {\n      ENVOY_LOG(debug, \"enforced denied\");\n      callbacks_->sendLocalReply(Http::Code::Forbidden, \"RBAC: access denied\", nullptr,\n                                 absl::nullopt, RcDetails::get().RbacAccessDenied);\n      config_->stats().denied_.inc();\n      return Http::FilterHeadersStatus::StopIteration;\n    }\n  }\n\n  ENVOY_LOG(debug, \"no engine, allowed by default\");\n  return Http::FilterHeadersStatus::Continue;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,10 @@\n   if (shadow_engine != nullptr) {\n     std::string shadow_resp_code =\n         Filters::Common::RBAC::DynamicMetadataKeysSingleton::get().EngineResultAllowed;\n+    // Refresh headers byte size before checking if allowed.\n+    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n+    // HeaderMap holds an accurate internal byte size count.\n+    headers.refreshByteSize();\n     if (shadow_engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(),\n                                &effective_policy_id)) {\n       ENVOY_LOG(debug, \"shadow allowed\");\n@@ -49,6 +53,10 @@\n   const auto engine =\n       config_->engine(callbacks_->route(), Filters::Common::RBAC::EnforcementMode::Enforced);\n   if (engine != nullptr) {\n+    // Refresh headers byte size before checking if allowed.\n+    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n+    // HeaderMap holds an accurate internal byte size count.\n+    headers.refreshByteSize();\n     if (engine->allowed(*callbacks_->connection(), headers, callbacks_->streamInfo(), nullptr)) {\n       ENVOY_LOG(debug, \"enforced allowed\");\n       config_->stats().allowed_.inc();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    // Refresh headers byte size before checking if allowed.",
                "    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and",
                "    // HeaderMap holds an accurate internal byte size count.",
                "    headers.refreshByteSize();",
                "    // Refresh headers byte size before checking if allowed.",
                "    // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and",
                "    // HeaderMap holds an accurate internal byte size count.",
                "    headers.refreshByteSize();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionImpl::onHeadersCompleteBase",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "int ConnectionImpl::onHeadersCompleteBase() {\n  ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n  completeLastHeader();\n  if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n    // This is not necessarily true, but it's good enough since higher layers only care if this is\n    // HTTP/1.1 or not.\n    protocol_ = Protocol::Http10;\n  }\n  if (Utility::isUpgrade(*current_header_map_)) {\n    // Ignore h2c upgrade requests until we support them.\n    // See https://github.com/envoyproxy/envoy/issues/7161 for details.\n    if (current_header_map_->Upgrade() &&\n        absl::EqualsIgnoreCase(current_header_map_->Upgrade()->value().getStringView(),\n                               Http::Headers::get().UpgradeValues.H2c)) {\n      ENVOY_CONN_LOG(trace, \"removing unsupported h2c upgrade headers.\", connection_);\n      current_header_map_->removeUpgrade();\n      if (current_header_map_->Connection()) {\n        const auto& tokens_to_remove = caseUnorderdSetContainingUpgradeAndHttp2Settings();\n        std::string new_value = StringUtil::removeTokens(\n            current_header_map_->Connection()->value().getStringView(), \",\", tokens_to_remove, \",\");\n        if (new_value.empty()) {\n          current_header_map_->removeConnection();\n        } else {\n          current_header_map_->Connection()->value(new_value);\n        }\n      }\n      current_header_map_->remove(Headers::get().Http2Settings);\n    } else {\n      ENVOY_CONN_LOG(trace, \"codec entering upgrade mode.\", connection_);\n      handling_upgrade_ = true;\n    }\n  }\n\n  int rc = onHeadersComplete(std::move(current_header_map_));\n  current_header_map_.reset();\n  header_parsing_state_ = HeaderParsingState::Done;\n\n  // Returning 2 informs http_parser to not expect a body or further data on this connection.\n  return handling_upgrade_ ? 2 : rc;\n}",
        "func": "int ConnectionImpl::onHeadersCompleteBase() {\n  ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n  completeLastHeader();\n  // Validate that the completed HeaderMap's cached byte size exists and is correct.\n  // This assert iterates over the HeaderMap.\n  ASSERT(current_header_map_->byteSize().has_value() &&\n         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());\n  if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n    // This is not necessarily true, but it's good enough since higher layers only care if this is\n    // HTTP/1.1 or not.\n    protocol_ = Protocol::Http10;\n  }\n  if (Utility::isUpgrade(*current_header_map_)) {\n    // Ignore h2c upgrade requests until we support them.\n    // See https://github.com/envoyproxy/envoy/issues/7161 for details.\n    if (current_header_map_->Upgrade() &&\n        absl::EqualsIgnoreCase(current_header_map_->Upgrade()->value().getStringView(),\n                               Http::Headers::get().UpgradeValues.H2c)) {\n      ENVOY_CONN_LOG(trace, \"removing unsupported h2c upgrade headers.\", connection_);\n      current_header_map_->removeUpgrade();\n      if (current_header_map_->Connection()) {\n        const auto& tokens_to_remove = caseUnorderdSetContainingUpgradeAndHttp2Settings();\n        std::string new_value = StringUtil::removeTokens(\n            current_header_map_->Connection()->value().getStringView(), \",\", tokens_to_remove, \",\");\n        if (new_value.empty()) {\n          current_header_map_->removeConnection();\n        } else {\n          current_header_map_->Connection()->value(new_value);\n        }\n      }\n      current_header_map_->remove(Headers::get().Http2Settings);\n    } else {\n      ENVOY_CONN_LOG(trace, \"codec entering upgrade mode.\", connection_);\n      handling_upgrade_ = true;\n    }\n  }\n\n  int rc = onHeadersComplete(std::move(current_header_map_));\n  current_header_map_.reset();\n  header_parsing_state_ = HeaderParsingState::Done;\n\n  // Returning 2 informs http_parser to not expect a body or further data on this connection.\n  return handling_upgrade_ ? 2 : rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n int ConnectionImpl::onHeadersCompleteBase() {\n   ENVOY_CONN_LOG(trace, \"headers complete\", connection_);\n   completeLastHeader();\n+  // Validate that the completed HeaderMap's cached byte size exists and is correct.\n+  // This assert iterates over the HeaderMap.\n+  ASSERT(current_header_map_->byteSize().has_value() &&\n+         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());\n   if (!(parser_.http_major == 1 && parser_.http_minor == 1)) {\n     // This is not necessarily true, but it's good enough since higher layers only care if this is\n     // HTTP/1.1 or not.",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Validate that the completed HeaderMap's cached byte size exists and is correct.",
                "  // This assert iterates over the HeaderMap.",
                "  ASSERT(current_header_map_->byteSize().has_value() &&",
                "         current_header_map_->byteSize() == current_header_map_->byteSizeInternal());"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionImpl::onHeaderValue",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void ConnectionImpl::onHeaderValue(const char* data, size_t length) {\n  if (header_parsing_state_ == HeaderParsingState::Done) {\n    // Ignore trailers.\n    return;\n  }\n\n  const absl::string_view header_value = absl::string_view(data, length);\n\n  if (strict_header_validation_) {\n    if (!Http::HeaderUtility::headerIsValid(header_value)) {\n      ENVOY_CONN_LOG(debug, \"invalid header value: {}\", connection_, header_value);\n      error_code_ = Http::Code::BadRequest;\n      sendProtocolError();\n      throw CodecProtocolException(\"http/1.1 protocol error: header value contains invalid chars\");\n    }\n  } else if (header_value.find('\\0') != absl::string_view::npos) {\n    // http-parser should filter for this\n    // (https://tools.ietf.org/html/rfc7230#section-3.2.6), but it doesn't today. HeaderStrings\n    // have an invariant that they must not contain embedded zero characters\n    // (NUL, ASCII 0x0).\n    throw CodecProtocolException(\"http/1.1 protocol error: header value contains NUL\");\n  }\n\n  header_parsing_state_ = HeaderParsingState::Value;\n  current_header_value_.append(data, length);\n\n  const uint32_t total =\n      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();\n  if (total > (max_request_headers_kb_ * 1024)) {\n    error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n    sendProtocolError();\n    throw CodecProtocolException(\"headers size exceeds limit\");\n  }\n}",
        "func": "void ConnectionImpl::onHeaderValue(const char* data, size_t length) {\n  if (header_parsing_state_ == HeaderParsingState::Done) {\n    // Ignore trailers.\n    return;\n  }\n\n  const absl::string_view header_value = absl::string_view(data, length);\n\n  if (strict_header_validation_) {\n    if (!Http::HeaderUtility::headerIsValid(header_value)) {\n      ENVOY_CONN_LOG(debug, \"invalid header value: {}\", connection_, header_value);\n      error_code_ = Http::Code::BadRequest;\n      sendProtocolError();\n      throw CodecProtocolException(\"http/1.1 protocol error: header value contains invalid chars\");\n    }\n  } else if (header_value.find('\\0') != absl::string_view::npos) {\n    // http-parser should filter for this\n    // (https://tools.ietf.org/html/rfc7230#section-3.2.6), but it doesn't today. HeaderStrings\n    // have an invariant that they must not contain embedded zero characters\n    // (NUL, ASCII 0x0).\n    throw CodecProtocolException(\"http/1.1 protocol error: header value contains NUL\");\n  }\n\n  header_parsing_state_ = HeaderParsingState::Value;\n  current_header_value_.append(data, length);\n\n  // Verify that the cached value in byte size exists.\n  ASSERT(current_header_map_->byteSize().has_value());\n  const uint32_t total = current_header_field_.size() + current_header_value_.size() +\n                         current_header_map_->byteSize().value();\n  if (total > (max_request_headers_kb_ * 1024)) {\n    error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n    sendProtocolError();\n    throw CodecProtocolException(\"headers size exceeds limit\");\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,8 +24,10 @@\n   header_parsing_state_ = HeaderParsingState::Value;\n   current_header_value_.append(data, length);\n \n-  const uint32_t total =\n-      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();\n+  // Verify that the cached value in byte size exists.\n+  ASSERT(current_header_map_->byteSize().has_value());\n+  const uint32_t total = current_header_field_.size() + current_header_value_.size() +\n+                         current_header_map_->byteSize().value();\n   if (total > (max_request_headers_kb_ * 1024)) {\n     error_code_ = Http::Code::RequestHeaderFieldsTooLarge;\n     sendProtocolError();",
        "diff_line_info": {
            "deleted_lines": [
                "  const uint32_t total =",
                "      current_header_field_.size() + current_header_value_.size() + current_header_map_->byteSize();"
            ],
            "added_lines": [
                "  // Verify that the cached value in byte size exists.",
                "  ASSERT(current_header_map_->byteSize().has_value());",
                "  const uint32_t total = current_header_field_.size() + current_header_value_.size() +",
                "                         current_header_map_->byteSize().value();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ActiveStream::decodeHeaders",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void ConnectionManagerImpl::ActiveStream::decodeHeaders(HeaderMapPtr&& headers, bool end_stream) {\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  request_headers_ = std::move(headers);\n  // For Admin thread, we don't use routeConfigProvider or SRDS route provider.\n  if (dynamic_cast<Server::Admin*>(&connection_manager_.config_) == nullptr &&\n      connection_manager_.config_.scopedRouteConfigProvider() != nullptr) {\n    ASSERT(snapped_route_config_ == nullptr,\n           \"Route config already latched to the active stream when scoped RDS is enabled.\");\n    // We need to snap snapped_route_config_ here as it's used in mutateRequestHeaders later.\n    snapScopedRouteConfig();\n  }\n\n  if (Http::Headers::get().MethodValues.Head ==\n      request_headers_->Method()->value().getStringView()) {\n    is_head_request_ = true;\n  }\n  ENVOY_STREAM_LOG(debug, \"request headers complete (end_stream={}):\\n{}\", *this, end_stream,\n                   *request_headers_);\n\n  // We end the decode here only if the request is header only. If we convert the request to a\n  // header only, the stream will be marked as done once a subsequent decodeData/decodeTrailers is\n  // called with end_stream=true.\n  maybeEndDecode(end_stream);\n\n  // Drop new requests when overloaded as soon as we have decoded the headers.\n  if (connection_manager_.overload_stop_accepting_requests_ref_ ==\n      Server::OverloadActionState::Active) {\n    // In this one special case, do not create the filter chain. If there is a risk of memory\n    // overload it is more important to avoid unnecessary allocation than to create the filters.\n    state_.created_filter_chain_ = true;\n    connection_manager_.stats_.named_.downstream_rq_overload_close_.inc();\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_),\n                   Http::Code::ServiceUnavailable, \"envoy overloaded\", nullptr, is_head_request_,\n                   absl::nullopt, StreamInfo::ResponseCodeDetails::get().Overload);\n    return;\n  }\n\n  if (!connection_manager_.config_.proxy100Continue() && request_headers_->Expect() &&\n      request_headers_->Expect()->value() == Headers::get().ExpectValues._100Continue.c_str()) {\n    // Note in the case Envoy is handling 100-Continue complexity, it skips the filter chain\n    // and sends the 100-Continue directly to the encoder.\n    chargeStats(continueHeader());\n    response_encoder_->encode100ContinueHeaders(continueHeader());\n    // Remove the Expect header so it won't be handled again upstream.\n    request_headers_->removeExpect();\n  }\n\n  connection_manager_.user_agent_.initializeFromHeaders(\n      *request_headers_, connection_manager_.stats_.prefix_, connection_manager_.stats_.scope_);\n\n  // Make sure we are getting a codec version we support.\n  Protocol protocol = connection_manager_.codec_->protocol();\n  if (protocol == Protocol::Http10) {\n    // Assume this is HTTP/1.0. This is fine for HTTP/0.9 but this code will also affect any\n    // requests with non-standard version numbers (0.9, 1.3), basically anything which is not\n    // HTTP/1.1.\n    //\n    // The protocol may have shifted in the HTTP/1.0 case so reset it.\n    stream_info_.protocol(protocol);\n    if (!connection_manager_.config_.http1Settings().accept_http_10_) {\n      // Send \"Upgrade Required\" if HTTP/1.0 support is not explicitly configured on.\n      sendLocalReply(false, Code::UpgradeRequired, \"\", nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().LowVersion);\n      return;\n    } else {\n      // HTTP/1.0 defaults to single-use connections. Make sure the connection\n      // will be closed unless Keep-Alive is present.\n      state_.saw_connection_close_ = true;\n      if (request_headers_->Connection() &&\n          absl::EqualsIgnoreCase(request_headers_->Connection()->value().getStringView(),\n                                 Http::Headers::get().ConnectionValues.KeepAlive)) {\n        state_.saw_connection_close_ = false;\n      }\n    }\n  }\n\n  if (!request_headers_->Host()) {\n    if ((protocol == Protocol::Http10) &&\n        !connection_manager_.config_.http1Settings().default_host_for_http_10_.empty()) {\n      // Add a default host if configured to do so.\n      request_headers_->insertHost().value(\n          connection_manager_.config_.http1Settings().default_host_for_http_10_);\n    } else {\n      // Require host header. For HTTP/1.1 Host has already been translated to :authority.\n      sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, \"\",\n                     nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().MissingHost);\n      return;\n    }\n  }\n\n  ASSERT(connection_manager_.config_.maxRequestHeadersKb() > 0);\n  if (request_headers_->byteSize() > (connection_manager_.config_.maxRequestHeadersKb() * 1024)) {\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_),\n                   Code::RequestHeaderFieldsTooLarge, \"\", nullptr, is_head_request_, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().RequestHeadersTooLarge);\n    return;\n  }\n\n  // Currently we only support relative paths at the application layer. We expect the codec to have\n  // broken the path into pieces if applicable. NOTE: Currently the HTTP/1.1 codec only does this\n  // when the allow_absolute_url flag is enabled on the HCM.\n  // https://tools.ietf.org/html/rfc7230#section-5.3 We also need to check for the existence of\n  // :path because CONNECT does not have a path, and we don't support that currently.\n  if (!request_headers_->Path() || request_headers_->Path()->value().getStringView().empty() ||\n      request_headers_->Path()->value().getStringView()[0] != '/') {\n    const bool has_path =\n        request_headers_->Path() && !request_headers_->Path()->value().getStringView().empty();\n    connection_manager_.stats_.named_.downstream_rq_non_relative_path_.inc();\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::NotFound, \"\", nullptr,\n                   is_head_request_, absl::nullopt,\n                   has_path ? StreamInfo::ResponseCodeDetails::get().AbsolutePath\n                            : StreamInfo::ResponseCodeDetails::get().MissingPath);\n    return;\n  }\n\n  // Path sanitization should happen before any path access other than the above sanity check.\n  if (!ConnectionManagerUtility::maybeNormalizePath(*request_headers_,\n                                                    connection_manager_.config_)) {\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, \"\",\n                   nullptr, is_head_request_, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  }\n\n  if (protocol == Protocol::Http11 && request_headers_->Connection() &&\n      absl::EqualsIgnoreCase(request_headers_->Connection()->value().getStringView(),\n                             Http::Headers::get().ConnectionValues.Close)) {\n    state_.saw_connection_close_ = true;\n  }\n  // Note: Proxy-Connection is not a standard header, but is supported here\n  // since it is supported by http-parser the underlying parser for http\n  // requests.\n  if (protocol != Protocol::Http2 && !state_.saw_connection_close_ &&\n      request_headers_->ProxyConnection() &&\n      absl::EqualsIgnoreCase(request_headers_->ProxyConnection()->value().getStringView(),\n                             Http::Headers::get().ConnectionValues.Close)) {\n    state_.saw_connection_close_ = true;\n  }\n\n  if (!state_.is_internally_created_) { // Only sanitize headers on first pass.\n    // Modify the downstream remote address depending on configuration and headers.\n    stream_info_.setDownstreamRemoteAddress(ConnectionManagerUtility::mutateRequestHeaders(\n        *request_headers_, connection_manager_.read_callbacks_->connection(),\n        connection_manager_.config_, *snapped_route_config_, connection_manager_.random_generator_,\n        connection_manager_.local_info_));\n  }\n  ASSERT(stream_info_.downstreamRemoteAddress() != nullptr);\n\n  ASSERT(!cached_route_);\n  refreshCachedRoute();\n\n  if (!state_.is_internally_created_) { // Only mutate tracing headers on first pass.\n    ConnectionManagerUtility::mutateTracingRequestHeader(\n        *request_headers_, connection_manager_.runtime_, connection_manager_.config_,\n        cached_route_.value().get());\n  }\n\n  const bool upgrade_rejected = createFilterChain() == false;\n\n  // TODO if there are no filters when starting a filter iteration, the connection manager\n  // should return 404. The current returns no response if there is no router filter.\n  if (protocol == Protocol::Http11 && hasCachedRoute()) {\n    if (upgrade_rejected) {\n      // Do not allow upgrades if the route does not support it.\n      connection_manager_.stats_.named_.downstream_rq_ws_on_non_ws_route_.inc();\n      sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::Forbidden, \"\",\n                     nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().UpgradeFailed);\n      return;\n    }\n    // Allow non websocket requests to go through websocket enabled routes.\n  }\n\n  if (hasCachedRoute()) {\n    const Router::RouteEntry* route_entry = cached_route_.value()->routeEntry();\n    if (route_entry != nullptr && route_entry->idleTimeout()) {\n      idle_timeout_ms_ = route_entry->idleTimeout().value();\n      if (idle_timeout_ms_.count()) {\n        // If we have a route-level idle timeout but no global stream idle timeout, create a timer.\n        if (stream_idle_timer_ == nullptr) {\n          stream_idle_timer_ =\n              connection_manager_.read_callbacks_->connection().dispatcher().createTimer(\n                  [this]() -> void { onIdleTimeout(); });\n        }\n      } else if (stream_idle_timer_ != nullptr) {\n        // If we had a global stream idle timeout but the route-level idle timeout is set to zero\n        // (to override), we disable the idle timer.\n        stream_idle_timer_->disableTimer();\n        stream_idle_timer_ = nullptr;\n      }\n    }\n  }\n\n  // Check if tracing is enabled at all.\n  if (connection_manager_.config_.tracingConfig()) {\n    traceRequest();\n  }\n\n  decodeHeaders(nullptr, *request_headers_, end_stream);\n\n  // Reset it here for both global and overridden cases.\n  resetIdleTimer();\n}",
        "func": "void ConnectionManagerImpl::ActiveStream::decodeHeaders(HeaderMapPtr&& headers, bool end_stream) {\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  request_headers_ = std::move(headers);\n  // For Admin thread, we don't use routeConfigProvider or SRDS route provider.\n  if (dynamic_cast<Server::Admin*>(&connection_manager_.config_) == nullptr &&\n      connection_manager_.config_.scopedRouteConfigProvider() != nullptr) {\n    ASSERT(snapped_route_config_ == nullptr,\n           \"Route config already latched to the active stream when scoped RDS is enabled.\");\n    // We need to snap snapped_route_config_ here as it's used in mutateRequestHeaders later.\n    snapScopedRouteConfig();\n  }\n\n  if (Http::Headers::get().MethodValues.Head ==\n      request_headers_->Method()->value().getStringView()) {\n    is_head_request_ = true;\n  }\n  ENVOY_STREAM_LOG(debug, \"request headers complete (end_stream={}):\\n{}\", *this, end_stream,\n                   *request_headers_);\n\n  // We end the decode here only if the request is header only. If we convert the request to a\n  // header only, the stream will be marked as done once a subsequent decodeData/decodeTrailers is\n  // called with end_stream=true.\n  maybeEndDecode(end_stream);\n\n  // Drop new requests when overloaded as soon as we have decoded the headers.\n  if (connection_manager_.overload_stop_accepting_requests_ref_ ==\n      Server::OverloadActionState::Active) {\n    // In this one special case, do not create the filter chain. If there is a risk of memory\n    // overload it is more important to avoid unnecessary allocation than to create the filters.\n    state_.created_filter_chain_ = true;\n    connection_manager_.stats_.named_.downstream_rq_overload_close_.inc();\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_),\n                   Http::Code::ServiceUnavailable, \"envoy overloaded\", nullptr, is_head_request_,\n                   absl::nullopt, StreamInfo::ResponseCodeDetails::get().Overload);\n    return;\n  }\n\n  if (!connection_manager_.config_.proxy100Continue() && request_headers_->Expect() &&\n      request_headers_->Expect()->value() == Headers::get().ExpectValues._100Continue.c_str()) {\n    // Note in the case Envoy is handling 100-Continue complexity, it skips the filter chain\n    // and sends the 100-Continue directly to the encoder.\n    chargeStats(continueHeader());\n    response_encoder_->encode100ContinueHeaders(continueHeader());\n    // Remove the Expect header so it won't be handled again upstream.\n    request_headers_->removeExpect();\n  }\n\n  connection_manager_.user_agent_.initializeFromHeaders(\n      *request_headers_, connection_manager_.stats_.prefix_, connection_manager_.stats_.scope_);\n\n  // Make sure we are getting a codec version we support.\n  Protocol protocol = connection_manager_.codec_->protocol();\n  if (protocol == Protocol::Http10) {\n    // Assume this is HTTP/1.0. This is fine for HTTP/0.9 but this code will also affect any\n    // requests with non-standard version numbers (0.9, 1.3), basically anything which is not\n    // HTTP/1.1.\n    //\n    // The protocol may have shifted in the HTTP/1.0 case so reset it.\n    stream_info_.protocol(protocol);\n    if (!connection_manager_.config_.http1Settings().accept_http_10_) {\n      // Send \"Upgrade Required\" if HTTP/1.0 support is not explicitly configured on.\n      sendLocalReply(false, Code::UpgradeRequired, \"\", nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().LowVersion);\n      return;\n    } else {\n      // HTTP/1.0 defaults to single-use connections. Make sure the connection\n      // will be closed unless Keep-Alive is present.\n      state_.saw_connection_close_ = true;\n      if (request_headers_->Connection() &&\n          absl::EqualsIgnoreCase(request_headers_->Connection()->value().getStringView(),\n                                 Http::Headers::get().ConnectionValues.KeepAlive)) {\n        state_.saw_connection_close_ = false;\n      }\n    }\n  }\n\n  if (!request_headers_->Host()) {\n    if ((protocol == Protocol::Http10) &&\n        !connection_manager_.config_.http1Settings().default_host_for_http_10_.empty()) {\n      // Add a default host if configured to do so.\n      request_headers_->insertHost().value(\n          connection_manager_.config_.http1Settings().default_host_for_http_10_);\n    } else {\n      // Require host header. For HTTP/1.1 Host has already been translated to :authority.\n      sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, \"\",\n                     nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().MissingHost);\n      return;\n    }\n  }\n\n  // Currently we only support relative paths at the application layer. We expect the codec to have\n  // broken the path into pieces if applicable. NOTE: Currently the HTTP/1.1 codec only does this\n  // when the allow_absolute_url flag is enabled on the HCM.\n  // https://tools.ietf.org/html/rfc7230#section-5.3 We also need to check for the existence of\n  // :path because CONNECT does not have a path, and we don't support that currently.\n  if (!request_headers_->Path() || request_headers_->Path()->value().getStringView().empty() ||\n      request_headers_->Path()->value().getStringView()[0] != '/') {\n    const bool has_path =\n        request_headers_->Path() && !request_headers_->Path()->value().getStringView().empty();\n    connection_manager_.stats_.named_.downstream_rq_non_relative_path_.inc();\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::NotFound, \"\", nullptr,\n                   is_head_request_, absl::nullopt,\n                   has_path ? StreamInfo::ResponseCodeDetails::get().AbsolutePath\n                            : StreamInfo::ResponseCodeDetails::get().MissingPath);\n    return;\n  }\n\n  // Path sanitization should happen before any path access other than the above sanity check.\n  if (!ConnectionManagerUtility::maybeNormalizePath(*request_headers_,\n                                                    connection_manager_.config_)) {\n    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::BadRequest, \"\",\n                   nullptr, is_head_request_, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  }\n\n  if (protocol == Protocol::Http11 && request_headers_->Connection() &&\n      absl::EqualsIgnoreCase(request_headers_->Connection()->value().getStringView(),\n                             Http::Headers::get().ConnectionValues.Close)) {\n    state_.saw_connection_close_ = true;\n  }\n  // Note: Proxy-Connection is not a standard header, but is supported here\n  // since it is supported by http-parser the underlying parser for http\n  // requests.\n  if (protocol != Protocol::Http2 && !state_.saw_connection_close_ &&\n      request_headers_->ProxyConnection() &&\n      absl::EqualsIgnoreCase(request_headers_->ProxyConnection()->value().getStringView(),\n                             Http::Headers::get().ConnectionValues.Close)) {\n    state_.saw_connection_close_ = true;\n  }\n\n  if (!state_.is_internally_created_) { // Only sanitize headers on first pass.\n    // Modify the downstream remote address depending on configuration and headers.\n    stream_info_.setDownstreamRemoteAddress(ConnectionManagerUtility::mutateRequestHeaders(\n        *request_headers_, connection_manager_.read_callbacks_->connection(),\n        connection_manager_.config_, *snapped_route_config_, connection_manager_.random_generator_,\n        connection_manager_.local_info_));\n  }\n  ASSERT(stream_info_.downstreamRemoteAddress() != nullptr);\n\n  ASSERT(!cached_route_);\n  refreshCachedRoute();\n\n  if (!state_.is_internally_created_) { // Only mutate tracing headers on first pass.\n    ConnectionManagerUtility::mutateTracingRequestHeader(\n        *request_headers_, connection_manager_.runtime_, connection_manager_.config_,\n        cached_route_.value().get());\n  }\n\n  const bool upgrade_rejected = createFilterChain() == false;\n\n  // TODO if there are no filters when starting a filter iteration, the connection manager\n  // should return 404. The current returns no response if there is no router filter.\n  if (protocol == Protocol::Http11 && hasCachedRoute()) {\n    if (upgrade_rejected) {\n      // Do not allow upgrades if the route does not support it.\n      connection_manager_.stats_.named_.downstream_rq_ws_on_non_ws_route_.inc();\n      sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_), Code::Forbidden, \"\",\n                     nullptr, is_head_request_, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().UpgradeFailed);\n      return;\n    }\n    // Allow non websocket requests to go through websocket enabled routes.\n  }\n\n  if (hasCachedRoute()) {\n    const Router::RouteEntry* route_entry = cached_route_.value()->routeEntry();\n    if (route_entry != nullptr && route_entry->idleTimeout()) {\n      idle_timeout_ms_ = route_entry->idleTimeout().value();\n      if (idle_timeout_ms_.count()) {\n        // If we have a route-level idle timeout but no global stream idle timeout, create a timer.\n        if (stream_idle_timer_ == nullptr) {\n          stream_idle_timer_ =\n              connection_manager_.read_callbacks_->connection().dispatcher().createTimer(\n                  [this]() -> void { onIdleTimeout(); });\n        }\n      } else if (stream_idle_timer_ != nullptr) {\n        // If we had a global stream idle timeout but the route-level idle timeout is set to zero\n        // (to override), we disable the idle timer.\n        stream_idle_timer_->disableTimer();\n        stream_idle_timer_ = nullptr;\n      }\n    }\n  }\n\n  // Check if tracing is enabled at all.\n  if (connection_manager_.config_.tracingConfig()) {\n    traceRequest();\n  }\n\n  decodeHeaders(nullptr, *request_headers_, end_stream);\n\n  // Reset it here for both global and overridden cases.\n  resetIdleTimer();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -88,14 +88,6 @@\n                      StreamInfo::ResponseCodeDetails::get().MissingHost);\n       return;\n     }\n-  }\n-\n-  ASSERT(connection_manager_.config_.maxRequestHeadersKb() > 0);\n-  if (request_headers_->byteSize() > (connection_manager_.config_.maxRequestHeadersKb() * 1024)) {\n-    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_),\n-                   Code::RequestHeaderFieldsTooLarge, \"\", nullptr, is_head_request_, absl::nullopt,\n-                   StreamInfo::ResponseCodeDetails::get().RequestHeadersTooLarge);\n-    return;\n   }\n \n   // Currently we only support relative paths at the application layer. We expect the codec to have",
        "diff_line_info": {
            "deleted_lines": [
                "  }",
                "",
                "  ASSERT(connection_manager_.config_.maxRequestHeadersKb() > 0);",
                "  if (request_headers_->byteSize() > (connection_manager_.config_.maxRequestHeadersKb() * 1024)) {",
                "    sendLocalReply(Grpc::Common::hasGrpcContentType(*request_headers_),",
                "                   Code::RequestHeaderFieldsTooLarge, \"\", nullptr, is_head_request_, absl::nullopt,",
                "                   StreamInfo::ResponseCodeDetails::get().RequestHeadersTooLarge);",
                "    return;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ActiveStream::~ActiveStream",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "ConnectionManagerImpl::ActiveStream::~ActiveStream() {\n  stream_info_.onRequestComplete();\n\n  // A downstream disconnect can be identified for HTTP requests when the upstream returns with a 0\n  // response code and when no other response flags are set.\n  if (!stream_info_.hasAnyResponseFlag() && !stream_info_.responseCode()) {\n    stream_info_.setResponseFlag(StreamInfo::ResponseFlag::DownstreamConnectionTermination);\n  }\n\n  connection_manager_.stats_.named_.downstream_rq_active_.dec();\n  for (const AccessLog::InstanceSharedPtr& access_log : connection_manager_.config_.accessLogs()) {\n    access_log->log(request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n                    stream_info_);\n  }\n  for (const auto& log_handler : access_log_handlers_) {\n    log_handler->log(request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n                     stream_info_);\n  }\n\n  if (stream_info_.healthCheck()) {\n    connection_manager_.config_.tracingStats().health_check_.inc();\n  }\n\n  if (active_span_) {\n    Tracing::HttpTracerUtility::finalizeDownstreamSpan(\n        *active_span_, request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n        stream_info_, *this);\n  }\n  if (state_.successful_upgrade_) {\n    connection_manager_.stats_.named_.downstream_cx_upgrades_active_.dec();\n  }\n\n  ASSERT(state_.filter_call_state_ == 0);\n}",
        "func": "ConnectionManagerImpl::ActiveStream::~ActiveStream() {\n  stream_info_.onRequestComplete();\n\n  // A downstream disconnect can be identified for HTTP requests when the upstream returns with a 0\n  // response code and when no other response flags are set.\n  if (!stream_info_.hasAnyResponseFlag() && !stream_info_.responseCode()) {\n    stream_info_.setResponseFlag(StreamInfo::ResponseFlag::DownstreamConnectionTermination);\n  }\n\n  connection_manager_.stats_.named_.downstream_rq_active_.dec();\n  // Refresh byte sizes of the HeaderMaps before logging.\n  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n  // HeaderMap holds an accurate internal byte size count.\n  if (request_headers_ != nullptr) {\n    request_headers_->refreshByteSize();\n  }\n  if (response_headers_ != nullptr) {\n    response_headers_->refreshByteSize();\n  }\n  if (response_trailers_ != nullptr) {\n    response_trailers_->refreshByteSize();\n  }\n  for (const AccessLog::InstanceSharedPtr& access_log : connection_manager_.config_.accessLogs()) {\n    access_log->log(request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n                    stream_info_);\n  }\n  for (const auto& log_handler : access_log_handlers_) {\n    log_handler->log(request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n                     stream_info_);\n  }\n\n  if (stream_info_.healthCheck()) {\n    connection_manager_.config_.tracingStats().health_check_.inc();\n  }\n\n  if (active_span_) {\n    Tracing::HttpTracerUtility::finalizeDownstreamSpan(\n        *active_span_, request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n        stream_info_, *this);\n  }\n  if (state_.successful_upgrade_) {\n    connection_manager_.stats_.named_.downstream_cx_upgrades_active_.dec();\n  }\n\n  ASSERT(state_.filter_call_state_ == 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,18 @@\n   }\n \n   connection_manager_.stats_.named_.downstream_rq_active_.dec();\n+  // Refresh byte sizes of the HeaderMaps before logging.\n+  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and\n+  // HeaderMap holds an accurate internal byte size count.\n+  if (request_headers_ != nullptr) {\n+    request_headers_->refreshByteSize();\n+  }\n+  if (response_headers_ != nullptr) {\n+    response_headers_->refreshByteSize();\n+  }\n+  if (response_trailers_ != nullptr) {\n+    response_trailers_->refreshByteSize();\n+  }\n   for (const AccessLog::InstanceSharedPtr& access_log : connection_manager_.config_.accessLogs()) {\n     access_log->log(request_headers_.get(), response_headers_.get(), response_trailers_.get(),\n                     stream_info_);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Refresh byte sizes of the HeaderMaps before logging.",
                "  // TODO(asraa): Remove this when entries in HeaderMap can no longer be modified by reference and",
                "  // HeaderMap holds an accurate internal byte size count.",
                "  if (request_headers_ != nullptr) {",
                "    request_headers_->refreshByteSize();",
                "  }",
                "  if (response_headers_ != nullptr) {",
                "    response_headers_->refreshByteSize();",
                "  }",
                "  if (response_trailers_ != nullptr) {",
                "    response_trailers_->refreshByteSize();",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-15226",
        "func_name": "envoyproxy/envoy/HttpGrpcAccessLog::emitLog",
        "description": "Upon receiving each incoming request header data, Envoy will iterate over existing request headers to verify that the total size of the headers stays below a maximum limit. The implementation in versions 1.10.0 through 1.11.1 for HTTP/1.x traffic and all versions of Envoy for HTTP/2 traffic had O(n^2) performance characteristics. A remote attacker may craft a request that stays below the maximum request header size but consists of many thousands of small headers to consume CPU and result in a denial-of-service attack.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/afc39bea36fd436e54262f150c009e8d72db5014",
        "commit_title": "Track byteSize of HeaderMap internally.",
        "commit_text": " Introduces a cached byte size updated internally in HeaderMap. The value is stored as an optional, and is cleared whenever a non-const pointer or reference to a HeaderEntry is accessed. The cached value can be set with refreshByteSize() which performs an iteration over the HeaderMap to sum the size of each key and value in the HeaderMap. ",
        "func_before": "void HttpGrpcAccessLog::emitLog(const Http::HeaderMap& request_headers,\n                                const Http::HeaderMap& response_headers,\n                                const Http::HeaderMap& response_trailers,\n                                const StreamInfo::StreamInfo& stream_info) {\n  // Common log properties.\n  // TODO(mattklein123): Populate sample_rate field.\n  envoy::data::accesslog::v2::HTTPAccessLogEntry log_entry;\n  GrpcCommon::Utility::extractCommonAccessLogProperties(*log_entry.mutable_common_properties(),\n                                                        stream_info);\n\n  if (stream_info.protocol()) {\n    switch (stream_info.protocol().value()) {\n    case Http::Protocol::Http10:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP10);\n      break;\n    case Http::Protocol::Http11:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP11);\n      break;\n    case Http::Protocol::Http2:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP2);\n      break;\n    }\n  }\n\n  // HTTP request properties.\n  // TODO(mattklein123): Populate port field.\n  auto* request_properties = log_entry.mutable_request();\n  if (request_headers.Scheme() != nullptr) {\n    request_properties->set_scheme(std::string(request_headers.Scheme()->value().getStringView()));\n  }\n  if (request_headers.Host() != nullptr) {\n    request_properties->set_authority(std::string(request_headers.Host()->value().getStringView()));\n  }\n  if (request_headers.Path() != nullptr) {\n    request_properties->set_path(std::string(request_headers.Path()->value().getStringView()));\n  }\n  if (request_headers.UserAgent() != nullptr) {\n    request_properties->set_user_agent(\n        std::string(request_headers.UserAgent()->value().getStringView()));\n  }\n  if (request_headers.Referer() != nullptr) {\n    request_properties->set_referer(\n        std::string(request_headers.Referer()->value().getStringView()));\n  }\n  if (request_headers.ForwardedFor() != nullptr) {\n    request_properties->set_forwarded_for(\n        std::string(request_headers.ForwardedFor()->value().getStringView()));\n  }\n  if (request_headers.RequestId() != nullptr) {\n    request_properties->set_request_id(\n        std::string(request_headers.RequestId()->value().getStringView()));\n  }\n  if (request_headers.EnvoyOriginalPath() != nullptr) {\n    request_properties->set_original_path(\n        std::string(request_headers.EnvoyOriginalPath()->value().getStringView()));\n  }\n  request_properties->set_request_headers_bytes(request_headers.byteSize());\n  request_properties->set_request_body_bytes(stream_info.bytesReceived());\n  if (request_headers.Method() != nullptr) {\n    envoy::api::v2::core::RequestMethod method =\n        envoy::api::v2::core::RequestMethod::METHOD_UNSPECIFIED;\n    envoy::api::v2::core::RequestMethod_Parse(\n        std::string(request_headers.Method()->value().getStringView()), &method);\n    request_properties->set_request_method(method);\n  }\n  if (!request_headers_to_log_.empty()) {\n    auto* logged_headers = request_properties->mutable_request_headers();\n\n    for (const auto& header : request_headers_to_log_) {\n      const Http::HeaderEntry* entry = request_headers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  // HTTP response properties.\n  auto* response_properties = log_entry.mutable_response();\n  if (stream_info.responseCode()) {\n    response_properties->mutable_response_code()->set_value(stream_info.responseCode().value());\n  }\n  if (stream_info.responseCodeDetails()) {\n    response_properties->set_response_code_details(stream_info.responseCodeDetails().value());\n  }\n  response_properties->set_response_headers_bytes(response_headers.byteSize());\n  response_properties->set_response_body_bytes(stream_info.bytesSent());\n  if (!response_headers_to_log_.empty()) {\n    auto* logged_headers = response_properties->mutable_response_headers();\n\n    for (const auto& header : response_headers_to_log_) {\n      const Http::HeaderEntry* entry = response_headers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  if (!response_trailers_to_log_.empty()) {\n    auto* logged_headers = response_properties->mutable_response_trailers();\n\n    for (const auto& header : response_trailers_to_log_) {\n      const Http::HeaderEntry* entry = response_trailers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  tls_slot_->getTyped<ThreadLocalLogger>().logger_->log(std::move(log_entry));\n}",
        "func": "void HttpGrpcAccessLog::emitLog(const Http::HeaderMap& request_headers,\n                                const Http::HeaderMap& response_headers,\n                                const Http::HeaderMap& response_trailers,\n                                const StreamInfo::StreamInfo& stream_info) {\n  // Common log properties.\n  // TODO(mattklein123): Populate sample_rate field.\n  envoy::data::accesslog::v2::HTTPAccessLogEntry log_entry;\n  GrpcCommon::Utility::extractCommonAccessLogProperties(*log_entry.mutable_common_properties(),\n                                                        stream_info);\n\n  if (stream_info.protocol()) {\n    switch (stream_info.protocol().value()) {\n    case Http::Protocol::Http10:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP10);\n      break;\n    case Http::Protocol::Http11:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP11);\n      break;\n    case Http::Protocol::Http2:\n      log_entry.set_protocol_version(envoy::data::accesslog::v2::HTTPAccessLogEntry::HTTP2);\n      break;\n    }\n  }\n\n  // HTTP request properties.\n  // TODO(mattklein123): Populate port field.\n  auto* request_properties = log_entry.mutable_request();\n  if (request_headers.Scheme() != nullptr) {\n    request_properties->set_scheme(std::string(request_headers.Scheme()->value().getStringView()));\n  }\n  if (request_headers.Host() != nullptr) {\n    request_properties->set_authority(std::string(request_headers.Host()->value().getStringView()));\n  }\n  if (request_headers.Path() != nullptr) {\n    request_properties->set_path(std::string(request_headers.Path()->value().getStringView()));\n  }\n  if (request_headers.UserAgent() != nullptr) {\n    request_properties->set_user_agent(\n        std::string(request_headers.UserAgent()->value().getStringView()));\n  }\n  if (request_headers.Referer() != nullptr) {\n    request_properties->set_referer(\n        std::string(request_headers.Referer()->value().getStringView()));\n  }\n  if (request_headers.ForwardedFor() != nullptr) {\n    request_properties->set_forwarded_for(\n        std::string(request_headers.ForwardedFor()->value().getStringView()));\n  }\n  if (request_headers.RequestId() != nullptr) {\n    request_properties->set_request_id(\n        std::string(request_headers.RequestId()->value().getStringView()));\n  }\n  if (request_headers.EnvoyOriginalPath() != nullptr) {\n    request_properties->set_original_path(\n        std::string(request_headers.EnvoyOriginalPath()->value().getStringView()));\n  }\n  request_properties->set_request_headers_bytes(request_headers.byteSize().value());\n  request_properties->set_request_body_bytes(stream_info.bytesReceived());\n  if (request_headers.Method() != nullptr) {\n    envoy::api::v2::core::RequestMethod method =\n        envoy::api::v2::core::RequestMethod::METHOD_UNSPECIFIED;\n    envoy::api::v2::core::RequestMethod_Parse(\n        std::string(request_headers.Method()->value().getStringView()), &method);\n    request_properties->set_request_method(method);\n  }\n  if (!request_headers_to_log_.empty()) {\n    auto* logged_headers = request_properties->mutable_request_headers();\n\n    for (const auto& header : request_headers_to_log_) {\n      const Http::HeaderEntry* entry = request_headers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  // HTTP response properties.\n  auto* response_properties = log_entry.mutable_response();\n  if (stream_info.responseCode()) {\n    response_properties->mutable_response_code()->set_value(stream_info.responseCode().value());\n  }\n  if (stream_info.responseCodeDetails()) {\n    response_properties->set_response_code_details(stream_info.responseCodeDetails().value());\n  }\n  response_properties->set_response_headers_bytes(response_headers.byteSize().value());\n  response_properties->set_response_body_bytes(stream_info.bytesSent());\n  if (!response_headers_to_log_.empty()) {\n    auto* logged_headers = response_properties->mutable_response_headers();\n\n    for (const auto& header : response_headers_to_log_) {\n      const Http::HeaderEntry* entry = response_headers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  if (!response_trailers_to_log_.empty()) {\n    auto* logged_headers = response_properties->mutable_response_trailers();\n\n    for (const auto& header : response_trailers_to_log_) {\n      const Http::HeaderEntry* entry = response_trailers.get(header);\n      if (entry != nullptr) {\n        logged_headers->insert({header.get(), std::string(entry->value().getStringView())});\n      }\n    }\n  }\n\n  tls_slot_->getTyped<ThreadLocalLogger>().logger_->log(std::move(log_entry));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -54,7 +54,7 @@\n     request_properties->set_original_path(\n         std::string(request_headers.EnvoyOriginalPath()->value().getStringView()));\n   }\n-  request_properties->set_request_headers_bytes(request_headers.byteSize());\n+  request_properties->set_request_headers_bytes(request_headers.byteSize().value());\n   request_properties->set_request_body_bytes(stream_info.bytesReceived());\n   if (request_headers.Method() != nullptr) {\n     envoy::api::v2::core::RequestMethod method =\n@@ -82,7 +82,7 @@\n   if (stream_info.responseCodeDetails()) {\n     response_properties->set_response_code_details(stream_info.responseCodeDetails().value());\n   }\n-  response_properties->set_response_headers_bytes(response_headers.byteSize());\n+  response_properties->set_response_headers_bytes(response_headers.byteSize().value());\n   response_properties->set_response_body_bytes(stream_info.bytesSent());\n   if (!response_headers_to_log_.empty()) {\n     auto* logged_headers = response_properties->mutable_response_headers();",
        "diff_line_info": {
            "deleted_lines": [
                "  request_properties->set_request_headers_bytes(request_headers.byteSize());",
                "  response_properties->set_response_headers_bytes(response_headers.byteSize());"
            ],
            "added_lines": [
                "  request_properties->set_request_headers_bytes(request_headers.byteSize().value());",
                "  response_properties->set_response_headers_bytes(response_headers.byteSize().value());"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0669",
        "func_name": "DPDK/dpdk/vhost_backend_cleanup",
        "description": "A flaw was found in dpdk. This flaw allows a malicious vhost-user master to attach an unexpected number of fds as ancillary data to VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD messages that are not closed by the vhost-user slave. By sending such messages continuously, the vhost-user master exhausts available fd in the vhost-user slave process, leading to a denial of service.",
        "git_url": "https://github.com/DPDK/dpdk/commit/d87f1a1cb7b666550bb53e39c1d85d9f7b861e6f",
        "commit_title": "vhost: support inflight info sharing",
        "commit_text": " This patch introduces two new messages VHOST_USER_GET_INFLIGHT_FD and VHOST_USER_SET_INFLIGHT_FD to support transferring a shared buffer between qemu and backend. ",
        "func_before": "void\nvhost_backend_cleanup(struct virtio_net *dev)\n{\n\tif (dev->mem) {\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\tfree(dev->guest_pages);\n\tdev->guest_pages = NULL;\n\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t\tdev->log_addr = 0;\n\t}\n\n\tif (dev->slave_req_fd >= 0) {\n\t\tclose(dev->slave_req_fd);\n\t\tdev->slave_req_fd = -1;\n\t}\n\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tdev->postcopy_listening = 0;\n}",
        "func": "void\nvhost_backend_cleanup(struct virtio_net *dev)\n{\n\tif (dev->mem) {\n\t\tfree_mem_region(dev);\n\t\trte_free(dev->mem);\n\t\tdev->mem = NULL;\n\t}\n\n\tfree(dev->guest_pages);\n\tdev->guest_pages = NULL;\n\n\tif (dev->log_addr) {\n\t\tmunmap((void *)(uintptr_t)dev->log_addr, dev->log_size);\n\t\tdev->log_addr = 0;\n\t}\n\n\tif (dev->inflight_info) {\n\t\tif (dev->inflight_info->addr) {\n\t\t\tmunmap(dev->inflight_info->addr,\n\t\t\t       dev->inflight_info->size);\n\t\t\tdev->inflight_info->addr = NULL;\n\t\t}\n\n\t\tif (dev->inflight_info->fd > 0) {\n\t\t\tclose(dev->inflight_info->fd);\n\t\t\tdev->inflight_info->fd = -1;\n\t\t}\n\n\t\tfree(dev->inflight_info);\n\t\tdev->inflight_info = NULL;\n\t}\n\n\tif (dev->slave_req_fd >= 0) {\n\t\tclose(dev->slave_req_fd);\n\t\tdev->slave_req_fd = -1;\n\t}\n\n\tif (dev->postcopy_ufd >= 0) {\n\t\tclose(dev->postcopy_ufd);\n\t\tdev->postcopy_ufd = -1;\n\t}\n\n\tdev->postcopy_listening = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,6 +15,22 @@\n \t\tdev->log_addr = 0;\n \t}\n \n+\tif (dev->inflight_info) {\n+\t\tif (dev->inflight_info->addr) {\n+\t\t\tmunmap(dev->inflight_info->addr,\n+\t\t\t       dev->inflight_info->size);\n+\t\t\tdev->inflight_info->addr = NULL;\n+\t\t}\n+\n+\t\tif (dev->inflight_info->fd > 0) {\n+\t\t\tclose(dev->inflight_info->fd);\n+\t\t\tdev->inflight_info->fd = -1;\n+\t\t}\n+\n+\t\tfree(dev->inflight_info);\n+\t\tdev->inflight_info = NULL;\n+\t}\n+\n \tif (dev->slave_req_fd >= 0) {\n \t\tclose(dev->slave_req_fd);\n \t\tdev->slave_req_fd = -1;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (dev->inflight_info) {",
                "\t\tif (dev->inflight_info->addr) {",
                "\t\t\tmunmap(dev->inflight_info->addr,",
                "\t\t\t       dev->inflight_info->size);",
                "\t\t\tdev->inflight_info->addr = NULL;",
                "\t\t}",
                "",
                "\t\tif (dev->inflight_info->fd > 0) {",
                "\t\t\tclose(dev->inflight_info->fd);",
                "\t\t\tdev->inflight_info->fd = -1;",
                "\t\t}",
                "",
                "\t\tfree(dev->inflight_info);",
                "\t\tdev->inflight_info = NULL;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0669",
        "func_name": "DPDK/dpdk/vhost_user_set_inflight_fd",
        "description": "A flaw was found in dpdk. This flaw allows a malicious vhost-user master to attach an unexpected number of fds as ancillary data to VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD messages that are not closed by the vhost-user slave. By sending such messages continuously, the vhost-user master exhausts available fd in the vhost-user slave process, leading to a denial of service.",
        "git_url": "https://github.com/DPDK/dpdk/commit/af74f7db384ed149fe42b21dbd7975f8a54ef227",
        "commit_title": "vhost: fix FD leak with inflight messages",
        "commit_text": " Even if unlikely, a buggy vhost-user master might attach fds to inflight messages. Add checks like for other types of vhost-user messages.  Cc: stable@dpdk.org ",
        "func_before": "static int\nvhost_user_set_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tuint64_t mmap_size, mmap_offset;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t pervq_inflight_size;\n\tstruct vhost_virtqueue *vq;\n\tvoid *addr;\n\tint fd, i;\n\tint numa_node = SOCKET_ID_ANY;\n\n\tfd = ctx->fds[0];\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight) || fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid set_inflight_fd message size is %d,fd is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tmmap_size = ctx->msg.payload.inflight.mmap_size;\n\tmmap_offset = ctx->msg.payload.inflight.mmap_offset;\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd num_queues: %u\\n\", dev->ifname, num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd queue_size: %u\\n\", dev->ifname, queue_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd fd: %d\\n\", dev->ifname, fd);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd pervq_inflight_size: %d\\n\",\n\t\t\tdev->ifname, pervq_inflight_size);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (!dev->inflight_info) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (dev->inflight_info == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\taddr = mmap(0, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,\n\t\t    fd, mmap_offset);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap share memory.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->fd = fd;\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = mmap_size;\n\n\tfor (i = 0; i < num_queues; i++) {\n\t\tvq = dev->virtqueue[i];\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq_is_packed(dev)) {\n\t\t\tvq->inflight_packed = addr;\n\t\t\tvq->inflight_packed->desc_num = queue_size;\n\t\t} else {\n\t\t\tvq->inflight_split = addr;\n\t\t\tvq->inflight_split->desc_num = queue_size;\n\t\t}\n\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}",
        "func": "static int\nvhost_user_set_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tuint64_t mmap_size, mmap_offset;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tuint32_t pervq_inflight_size;\n\tstruct vhost_virtqueue *vq;\n\tvoid *addr;\n\tint fd, i;\n\tint numa_node = SOCKET_ID_ANY;\n\n\tif (validate_msg_fds(dev, ctx, 1) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tfd = ctx->fds[0];\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight) || fd < 0) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid set_inflight_fd message size is %d,fd is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size, fd);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tmmap_size = ctx->msg.payload.inflight.mmap_size;\n\tmmap_offset = ctx->msg.payload.inflight.mmap_offset;\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd num_queues: %u\\n\", dev->ifname, num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd queue_size: %u\\n\", dev->ifname, queue_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd fd: %d\\n\", dev->ifname, fd);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) set_inflight_fd pervq_inflight_size: %d\\n\",\n\t\t\tdev->ifname, pervq_inflight_size);\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (!dev->inflight_info) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (dev->inflight_info == NULL) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\taddr = mmap(0, mmap_size, PROT_READ | PROT_WRITE, MAP_SHARED,\n\t\t    fd, mmap_offset);\n\tif (addr == MAP_FAILED) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to mmap share memory.\\n\", dev->ifname);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->fd = fd;\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = mmap_size;\n\n\tfor (i = 0; i < num_queues; i++) {\n\t\tvq = dev->virtqueue[i];\n\t\tif (!vq)\n\t\t\tcontinue;\n\n\t\tif (vq_is_packed(dev)) {\n\t\t\tvq->inflight_packed = addr;\n\t\t\tvq->inflight_packed->desc_num = queue_size;\n\t\t} else {\n\t\t\tvq->inflight_split = addr;\n\t\t\tvq->inflight_split->desc_num = queue_size;\n\t\t}\n\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t}\n\n\treturn RTE_VHOST_MSG_RESULT_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,9 @@\n \tvoid *addr;\n \tint fd, i;\n \tint numa_node = SOCKET_ID_ANY;\n+\n+\tif (validate_msg_fds(dev, ctx, 1) != 0)\n+\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n \n \tfd = ctx->fds[0];\n \tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight) || fd < 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (validate_msg_fds(dev, ctx, 1) != 0)",
                "\t\treturn RTE_VHOST_MSG_RESULT_ERR;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0669",
        "func_name": "DPDK/dpdk/vhost_user_get_inflight_fd",
        "description": "A flaw was found in dpdk. This flaw allows a malicious vhost-user master to attach an unexpected number of fds as ancillary data to VHOST_USER_GET_INFLIGHT_FD / VHOST_USER_SET_INFLIGHT_FD messages that are not closed by the vhost-user slave. By sending such messages continuously, the vhost-user master exhausts available fd in the vhost-user slave process, leading to a denial of service.",
        "git_url": "https://github.com/DPDK/dpdk/commit/af74f7db384ed149fe42b21dbd7975f8a54ef227",
        "commit_title": "vhost: fix FD leak with inflight messages",
        "commit_text": " Even if unlikely, a buggy vhost-user master might attach fds to inflight messages. Add checks like for other types of vhost-user messages.  Cc: stable@dpdk.org ",
        "func_before": "static int\nvhost_user_get_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\tuint64_t pervq_inflight_size, mmap_size;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tint fd, i, j;\n\tint numa_node = SOCKET_ID_ANY;\n\tvoid *addr;\n\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid get_inflight_fd message size is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (dev->inflight_info == NULL) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (!dev->inflight_info) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd num_queues: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd queue_size: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.queue_size);\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tmmap_size = num_queues * pervq_inflight_size;\n\taddr = inflight_mem_alloc(dev, \"vhost-inflight\", mmap_size, &fd);\n\tif (!addr) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc vhost inflight area\\n\", dev->ifname);\n\t\t\tctx->msg.payload.inflight.mmap_size = 0;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tmemset(addr, 0, mmap_size);\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = ctx->msg.payload.inflight.mmap_size = mmap_size;\n\tdev->inflight_info->fd = ctx->fds[0] = fd;\n\tctx->msg.payload.inflight.mmap_offset = 0;\n\tctx->fd_num = 1;\n\n\tif (vq_is_packed(dev)) {\n\t\tfor (i = 0; i < num_queues; i++) {\n\t\t\tinflight_packed =\n\t\t\t\t(struct rte_vhost_inflight_info_packed *)addr;\n\t\t\tinflight_packed->used_wrap_counter = 1;\n\t\t\tinflight_packed->old_used_wrap_counter = 1;\n\t\t\tfor (j = 0; j < queue_size; j++)\n\t\t\t\tinflight_packed->desc[j].next = j + 1;\n\t\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight fd: %d\\n\", dev->ifname, ctx->fds[0]);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}",
        "func": "static int\nvhost_user_get_inflight_fd(struct virtio_net **pdev,\n\t\t\t   struct vhu_msg_context *ctx,\n\t\t\t   int main_fd __rte_unused)\n{\n\tstruct rte_vhost_inflight_info_packed *inflight_packed;\n\tuint64_t pervq_inflight_size, mmap_size;\n\tuint16_t num_queues, queue_size;\n\tstruct virtio_net *dev = *pdev;\n\tint fd, i, j;\n\tint numa_node = SOCKET_ID_ANY;\n\tvoid *addr;\n\n\tif (validate_msg_fds(dev, ctx, 0) != 0)\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\n\tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight)) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid get_inflight_fd message size is %d\\n\",\n\t\t\tdev->ifname, ctx->msg.size);\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\n\t/*\n\t * If VQ 0 has already been allocated, try to allocate on the same\n\t * NUMA node. It can be reallocated later in numa_realloc().\n\t */\n\tif (dev->nr_vring > 0)\n\t\tnuma_node = dev->virtqueue[0]->numa_node;\n\n\tif (dev->inflight_info == NULL) {\n\t\tdev->inflight_info = rte_zmalloc_socket(\"inflight_info\",\n\t\t\t\tsizeof(struct inflight_mem_info), 0, numa_node);\n\t\tif (!dev->inflight_info) {\n\t\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc dev inflight area\\n\",\n\t\t\t\t\tdev->ifname);\n\t\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t\t}\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tnum_queues = ctx->msg.payload.inflight.num_queues;\n\tqueue_size = ctx->msg.payload.inflight.queue_size;\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd num_queues: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.num_queues);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) get_inflight_fd queue_size: %u\\n\",\n\t\tdev->ifname, ctx->msg.payload.inflight.queue_size);\n\n\tif (vq_is_packed(dev))\n\t\tpervq_inflight_size = get_pervq_shm_size_packed(queue_size);\n\telse\n\t\tpervq_inflight_size = get_pervq_shm_size_split(queue_size);\n\n\tmmap_size = num_queues * pervq_inflight_size;\n\taddr = inflight_mem_alloc(dev, \"vhost-inflight\", mmap_size, &fd);\n\tif (!addr) {\n\t\tVHOST_LOG_CONFIG(ERR, \"(%s) failed to alloc vhost inflight area\\n\", dev->ifname);\n\t\t\tctx->msg.payload.inflight.mmap_size = 0;\n\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n\t}\n\tmemset(addr, 0, mmap_size);\n\n\tif (dev->inflight_info->addr) {\n\t\tmunmap(dev->inflight_info->addr, dev->inflight_info->size);\n\t\tdev->inflight_info->addr = NULL;\n\t}\n\n\tif (dev->inflight_info->fd >= 0) {\n\t\tclose(dev->inflight_info->fd);\n\t\tdev->inflight_info->fd = -1;\n\t}\n\n\tdev->inflight_info->addr = addr;\n\tdev->inflight_info->size = ctx->msg.payload.inflight.mmap_size = mmap_size;\n\tdev->inflight_info->fd = ctx->fds[0] = fd;\n\tctx->msg.payload.inflight.mmap_offset = 0;\n\tctx->fd_num = 1;\n\n\tif (vq_is_packed(dev)) {\n\t\tfor (i = 0; i < num_queues; i++) {\n\t\t\tinflight_packed =\n\t\t\t\t(struct rte_vhost_inflight_info_packed *)addr;\n\t\t\tinflight_packed->used_wrap_counter = 1;\n\t\t\tinflight_packed->old_used_wrap_counter = 1;\n\t\t\tfor (j = 0; j < queue_size; j++)\n\t\t\t\tinflight_packed->desc[j].next = j + 1;\n\t\t\taddr = (void *)((char *)addr + pervq_inflight_size);\n\t\t}\n\t}\n\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_size: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_size);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight mmap_offset: %\"PRIu64\"\\n\",\n\t\t\tdev->ifname, ctx->msg.payload.inflight.mmap_offset);\n\tVHOST_LOG_CONFIG(INFO, \"(%s) send inflight fd: %d\\n\", dev->ifname, ctx->fds[0]);\n\n\treturn RTE_VHOST_MSG_RESULT_REPLY;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,9 @@\n \tint fd, i, j;\n \tint numa_node = SOCKET_ID_ANY;\n \tvoid *addr;\n+\n+\tif (validate_msg_fds(dev, ctx, 0) != 0)\n+\t\treturn RTE_VHOST_MSG_RESULT_ERR;\n \n \tif (ctx->msg.size != sizeof(ctx->msg.payload.inflight)) {\n \t\tVHOST_LOG_CONFIG(ERR, \"(%s) invalid get_inflight_fd message size is %d\\n\",",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (validate_msg_fds(dev, ctx, 0) != 0)",
                "\t\treturn RTE_VHOST_MSG_RESULT_ERR;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29260",
        "func_name": "LibVNC/libvncserver/rfbClientCleanup",
        "description": "libvncclient v0.9.13 was discovered to contain a memory leak via the function rfbClientCleanup().",
        "git_url": "https://github.com/LibVNC/libvncserver/commit/bef41f6ec4097a8ee094f90a1b34a708fbd757ec",
        "commit_title": "libvncclient: free vncRec memory in rfbClientCleanup()",
        "commit_text": " Otherwise we leak memory. Spotted by Ramin Farajpour Cami <ramin.blackhat@gmail.com>, thanks!",
        "func_before": "void rfbClientCleanup(rfbClient* client) {\n#ifdef LIBVNCSERVER_HAVE_LIBZ\n  int i;\n\n  for ( i = 0; i < 4; i++ ) {\n    if (client->zlibStreamActive[i] == TRUE ) {\n      if (inflateEnd (&client->zlibStream[i]) != Z_OK &&\n\t  client->zlibStream[i].msg != NULL)\n\trfbClientLog(\"inflateEnd: %s\\n\", client->zlibStream[i].msg);\n    }\n  }\n\n  if ( client->decompStreamInited == TRUE ) {\n    if (inflateEnd (&client->decompStream) != Z_OK &&\n\tclient->decompStream.msg != NULL)\n      rfbClientLog(\"inflateEnd: %s\\n\", client->decompStream.msg );\n  }\n#endif\n\n  if (client->ultra_buffer)\n    free(client->ultra_buffer);\n\n  if (client->raw_buffer)\n    free(client->raw_buffer);\n\n  FreeTLS(client);\n\n  while (client->clientData) {\n    rfbClientData* next = client->clientData->next;\n    free(client->clientData);\n    client->clientData = next;\n  }\n\n  if (client->sock != RFB_INVALID_SOCKET)\n    rfbCloseSocket(client->sock);\n  if (client->listenSock != RFB_INVALID_SOCKET)\n    rfbCloseSocket(client->listenSock);\n  free(client->desktopName);\n  free(client->serverHost);\n  if (client->destHost)\n    free(client->destHost);\n  if (client->clientAuthSchemes)\n    free(client->clientAuthSchemes);\n\n#ifdef LIBVNCSERVER_HAVE_SASL\n  if (client->saslSecret)\n    free(client->saslSecret);\n#endif /* LIBVNCSERVER_HAVE_SASL */\n\n#ifdef WIN32\n  if(WSACleanup() != 0) {\n      errno=WSAGetLastError();\n      rfbClientErr(\"Could not terminate Windows Sockets: %s\\n\", strerror(errno));\n  }\n#endif\n\n  free(client);\n}",
        "func": "void rfbClientCleanup(rfbClient* client) {\n#ifdef LIBVNCSERVER_HAVE_LIBZ\n  int i;\n\n  for ( i = 0; i < 4; i++ ) {\n    if (client->zlibStreamActive[i] == TRUE ) {\n      if (inflateEnd (&client->zlibStream[i]) != Z_OK &&\n\t  client->zlibStream[i].msg != NULL)\n\trfbClientLog(\"inflateEnd: %s\\n\", client->zlibStream[i].msg);\n    }\n  }\n\n  if ( client->decompStreamInited == TRUE ) {\n    if (inflateEnd (&client->decompStream) != Z_OK &&\n\tclient->decompStream.msg != NULL)\n      rfbClientLog(\"inflateEnd: %s\\n\", client->decompStream.msg );\n  }\n#endif\n\n  if (client->ultra_buffer)\n    free(client->ultra_buffer);\n\n  if (client->raw_buffer)\n    free(client->raw_buffer);\n\n  FreeTLS(client);\n\n  while (client->clientData) {\n    rfbClientData* next = client->clientData->next;\n    free(client->clientData);\n    client->clientData = next;\n  }\n\n  free(client->vncRec);\n\n  if (client->sock != RFB_INVALID_SOCKET)\n    rfbCloseSocket(client->sock);\n  if (client->listenSock != RFB_INVALID_SOCKET)\n    rfbCloseSocket(client->listenSock);\n  free(client->desktopName);\n  free(client->serverHost);\n  if (client->destHost)\n    free(client->destHost);\n  if (client->clientAuthSchemes)\n    free(client->clientAuthSchemes);\n\n#ifdef LIBVNCSERVER_HAVE_SASL\n  if (client->saslSecret)\n    free(client->saslSecret);\n#endif /* LIBVNCSERVER_HAVE_SASL */\n\n#ifdef WIN32\n  if(WSACleanup() != 0) {\n      errno=WSAGetLastError();\n      rfbClientErr(\"Could not terminate Windows Sockets: %s\\n\", strerror(errno));\n  }\n#endif\n\n  free(client);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,6 +31,8 @@\n     client->clientData = next;\n   }\n \n+  free(client->vncRec);\n+\n   if (client->sock != RFB_INVALID_SOCKET)\n     rfbCloseSocket(client->sock);\n   if (client->listenSock != RFB_INVALID_SOCKET)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  free(client->vncRec);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2906",
        "func_name": "torvalds/linux/pmcraid_ioctl_passthrough",
        "description": "Integer signedness error in the pmcraid_ioctl_passthrough function in drivers/scsi/pmcraid.c in the Linux kernel before 3.1 might allow local users to cause a denial of service (memory consumption or memory corruption) via a negative size value in an ioctl call. NOTE: this may be a vulnerability only in unusual environments that provide a privileged program for obtaining the required file descriptor.",
        "git_url": "https://github.com/torvalds/linux/commit/b5b515445f4f5a905c5dd27e6e682868ccd6c09d",
        "commit_title": "[SCSI] pmcraid: reject negative request size",
        "commit_text": " There's a code path in pmcraid that can be reached via device ioctl that causes all sorts of ugliness, including heap corruption or triggering the OOM killer due to consecutive allocation of large numbers of pages.  First, the user can call pmcraid_chr_ioctl(), with a type PMCRAID_PASSTHROUGH_IOCTL.  This calls through to pmcraid_ioctl_passthrough().  Next, a pmcraid_passthrough_ioctl_buffer is copied in, and the request_size variable is set to buffer->ioarcb.data_transfer_length, which is an arbitrary 32-bit signed value provided by the user.  If a negative value is provided here, bad things can happen.  For example, pmcraid_build_passthrough_ioadls() is called with this request_size, which immediately calls pmcraid_alloc_sglist() with a negative size. The resulting math on allocating a scatter list can result in an overflow in the kzalloc() call (if num_elem is 0, the sglist will be smaller than expected), or if num_elem is unexpectedly large the subsequent loop will call alloc_pages() repeatedly, a high number of pages will be allocated and the OOM killer might be invoked.  It looks like preventing this value from being negative in pmcraid_ioctl_passthrough() would be sufficient.  Cc: <stable@kernel.org>",
        "func_before": "static long pmcraid_ioctl_passthrough(\n\tstruct pmcraid_instance *pinstance,\n\tunsigned int ioctl_cmd,\n\tunsigned int buflen,\n\tunsigned long arg\n)\n{\n\tstruct pmcraid_passthrough_ioctl_buffer *buffer;\n\tstruct pmcraid_ioarcb *ioarcb;\n\tstruct pmcraid_cmd *cmd;\n\tstruct pmcraid_cmd *cancel_cmd;\n\tunsigned long request_buffer;\n\tunsigned long request_offset;\n\tunsigned long lock_flags;\n\tvoid *ioasa;\n\tu32 ioasc;\n\tint request_size;\n\tint buffer_size;\n\tu8 access, direction;\n\tint rc = 0;\n\n\t/* If IOA reset is in progress, wait 10 secs for reset to complete */\n\tif (pinstance->ioa_reset_in_progress) {\n\t\trc = wait_event_interruptible_timeout(\n\t\t\t\tpinstance->reset_wait_q,\n\t\t\t\t!pinstance->ioa_reset_in_progress,\n\t\t\t\tmsecs_to_jiffies(10000));\n\n\t\tif (!rc)\n\t\t\treturn -ETIMEDOUT;\n\t\telse if (rc < 0)\n\t\t\treturn -ERESTARTSYS;\n\t}\n\n\t/* If adapter is not in operational state, return error */\n\tif (pinstance->ioa_state != IOA_STATE_OPERATIONAL) {\n\t\tpmcraid_err(\"IOA is not operational\\n\");\n\t\treturn -ENOTTY;\n\t}\n\n\tbuffer_size = sizeof(struct pmcraid_passthrough_ioctl_buffer);\n\tbuffer = kmalloc(buffer_size, GFP_KERNEL);\n\n\tif (!buffer) {\n\t\tpmcraid_err(\"no memory for passthrough buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trequest_offset =\n\t    offsetof(struct pmcraid_passthrough_ioctl_buffer, request_buffer);\n\n\trequest_buffer = arg + request_offset;\n\n\trc = __copy_from_user(buffer,\n\t\t\t     (struct pmcraid_passthrough_ioctl_buffer *) arg,\n\t\t\t     sizeof(struct pmcraid_passthrough_ioctl_buffer));\n\n\tioasa =\n\t(void *)(arg +\n\t\toffsetof(struct pmcraid_passthrough_ioctl_buffer, ioasa));\n\n\tif (rc) {\n\t\tpmcraid_err(\"ioctl: can't copy passthrough buffer\\n\");\n\t\trc = -EFAULT;\n\t\tgoto out_free_buffer;\n\t}\n\n\trequest_size = buffer->ioarcb.data_transfer_length;\n\n\tif (buffer->ioarcb.request_flags0 & TRANSFER_DIR_WRITE) {\n\t\taccess = VERIFY_READ;\n\t\tdirection = DMA_TO_DEVICE;\n\t} else {\n\t\taccess = VERIFY_WRITE;\n\t\tdirection = DMA_FROM_DEVICE;\n\t}\n\n\tif (request_size > 0) {\n\t\trc = access_ok(access, arg, request_offset + request_size);\n\n\t\tif (!rc) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out_free_buffer;\n\t\t}\n\t} else if (request_size < 0) {\n\t\trc = -EINVAL;\n\t\tgoto out_free_buffer;\n\t}\n\n\t/* check if we have any additional command parameters */\n\tif (buffer->ioarcb.add_cmd_param_length > PMCRAID_ADD_CMD_PARAM_LEN) {\n\t\trc = -EINVAL;\n\t\tgoto out_free_buffer;\n\t}\n\n\tcmd = pmcraid_get_free_cmd(pinstance);\n\n\tif (!cmd) {\n\t\tpmcraid_err(\"free command block is not available\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto out_free_buffer;\n\t}\n\n\tcmd->scsi_cmd = NULL;\n\tioarcb = &(cmd->ioa_cb->ioarcb);\n\n\t/* Copy the user-provided IOARCB stuff field by field */\n\tioarcb->resource_handle = buffer->ioarcb.resource_handle;\n\tioarcb->data_transfer_length = buffer->ioarcb.data_transfer_length;\n\tioarcb->cmd_timeout = buffer->ioarcb.cmd_timeout;\n\tioarcb->request_type = buffer->ioarcb.request_type;\n\tioarcb->request_flags0 = buffer->ioarcb.request_flags0;\n\tioarcb->request_flags1 = buffer->ioarcb.request_flags1;\n\tmemcpy(ioarcb->cdb, buffer->ioarcb.cdb, PMCRAID_MAX_CDB_LEN);\n\n\tif (buffer->ioarcb.add_cmd_param_length) {\n\t\tioarcb->add_cmd_param_length =\n\t\t\tbuffer->ioarcb.add_cmd_param_length;\n\t\tioarcb->add_cmd_param_offset =\n\t\t\tbuffer->ioarcb.add_cmd_param_offset;\n\t\tmemcpy(ioarcb->add_data.u.add_cmd_params,\n\t\t\tbuffer->ioarcb.add_data.u.add_cmd_params,\n\t\t\tbuffer->ioarcb.add_cmd_param_length);\n\t}\n\n\t/* set hrrq number where the IOA should respond to. Note that all cmds\n\t * generated internally uses hrrq_id 0, exception to this is the cmd\n\t * block of scsi_cmd which is re-used (e.g. cancel/abort), which uses\n\t * hrrq_id assigned here in queuecommand\n\t */\n\tioarcb->hrrq_id = atomic_add_return(1, &(pinstance->last_message_id)) %\n\t\t\t  pinstance->num_hrrq;\n\n\tif (request_size) {\n\t\trc = pmcraid_build_passthrough_ioadls(cmd,\n\t\t\t\t\t\t      request_size,\n\t\t\t\t\t\t      direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"couldn't build passthrough ioadls\\n\");\n\t\t\tgoto out_free_buffer;\n\t\t}\n\t}\n\n\t/* If data is being written into the device, copy the data from user\n\t * buffers\n\t */\n\tif (direction == DMA_TO_DEVICE && request_size > 0) {\n\t\trc = pmcraid_copy_sglist(cmd->sglist,\n\t\t\t\t\t request_buffer,\n\t\t\t\t\t request_size,\n\t\t\t\t\t direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"failed to copy user buffer\\n\");\n\t\t\tgoto out_free_sglist;\n\t\t}\n\t}\n\n\t/* passthrough ioctl is a blocking command so, put the user to sleep\n\t * until timeout. Note that a timeout value of 0 means, do timeout.\n\t */\n\tcmd->cmd_done = pmcraid_internal_done;\n\tinit_completion(&cmd->wait_for_completion);\n\tcmd->completion_req = 1;\n\n\tpmcraid_info(\"command(%d) (CDB[0] = %x) for %x\\n\",\n\t\t     le32_to_cpu(cmd->ioa_cb->ioarcb.response_handle) >> 2,\n\t\t     cmd->ioa_cb->ioarcb.cdb[0],\n\t\t     le32_to_cpu(cmd->ioa_cb->ioarcb.resource_handle));\n\n\tspin_lock_irqsave(pinstance->host->host_lock, lock_flags);\n\t_pmcraid_fire_command(cmd);\n\tspin_unlock_irqrestore(pinstance->host->host_lock, lock_flags);\n\n\t/* NOTE ! Remove the below line once abort_task is implemented\n\t * in firmware. This line disables ioctl command timeout handling logic\n\t * similar to IO command timeout handling, making ioctl commands to wait\n\t * until the command completion regardless of timeout value specified in\n\t * ioarcb\n\t */\n\tbuffer->ioarcb.cmd_timeout = 0;\n\n\t/* If command timeout is specified put caller to wait till that time,\n\t * otherwise it would be blocking wait. If command gets timed out, it\n\t * will be aborted.\n\t */\n\tif (buffer->ioarcb.cmd_timeout == 0) {\n\t\twait_for_completion(&cmd->wait_for_completion);\n\t} else if (!wait_for_completion_timeout(\n\t\t\t&cmd->wait_for_completion,\n\t\t\tmsecs_to_jiffies(buffer->ioarcb.cmd_timeout * 1000))) {\n\n\t\tpmcraid_info(\"aborting cmd %d (CDB[0] = %x) due to timeout\\n\",\n\t\t\tle32_to_cpu(cmd->ioa_cb->ioarcb.response_handle >> 2),\n\t\t\tcmd->ioa_cb->ioarcb.cdb[0]);\n\n\t\tspin_lock_irqsave(pinstance->host->host_lock, lock_flags);\n\t\tcancel_cmd = pmcraid_abort_cmd(cmd);\n\t\tspin_unlock_irqrestore(pinstance->host->host_lock, lock_flags);\n\n\t\tif (cancel_cmd) {\n\t\t\twait_for_completion(&cancel_cmd->wait_for_completion);\n\t\t\tioasc = cancel_cmd->ioa_cb->ioasa.ioasc;\n\t\t\tpmcraid_return_cmd(cancel_cmd);\n\n\t\t\t/* if abort task couldn't find the command i.e it got\n\t\t\t * completed prior to aborting, return good completion.\n\t\t\t * if command got aborted successfully or there was IOA\n\t\t\t * reset due to abort task itself getting timedout then\n\t\t\t * return -ETIMEDOUT\n\t\t\t */\n\t\t\tif (ioasc == PMCRAID_IOASC_IOA_WAS_RESET ||\n\t\t\t    PMCRAID_IOASC_SENSE_KEY(ioasc) == 0x00) {\n\t\t\t\tif (ioasc != PMCRAID_IOASC_GC_IOARCB_NOTFOUND)\n\t\t\t\t\trc = -ETIMEDOUT;\n\t\t\t\tgoto out_handle_response;\n\t\t\t}\n\t\t}\n\n\t\t/* no command block for abort task or abort task failed to abort\n\t\t * the IOARCB, then wait for 150 more seconds and initiate reset\n\t\t * sequence after timeout\n\t\t */\n\t\tif (!wait_for_completion_timeout(\n\t\t\t&cmd->wait_for_completion,\n\t\t\tmsecs_to_jiffies(150 * 1000))) {\n\t\t\tpmcraid_reset_bringup(cmd->drv_inst);\n\t\t\trc = -ETIMEDOUT;\n\t\t}\n\t}\n\nout_handle_response:\n\t/* copy entire IOASA buffer and return IOCTL success.\n\t * If copying IOASA to user-buffer fails, return\n\t * EFAULT\n\t */\n\tif (copy_to_user(ioasa, &cmd->ioa_cb->ioasa,\n\t\tsizeof(struct pmcraid_ioasa))) {\n\t\tpmcraid_err(\"failed to copy ioasa buffer to user\\n\");\n\t\trc = -EFAULT;\n\t}\n\n\t/* If the data transfer was from device, copy the data onto user\n\t * buffers\n\t */\n\telse if (direction == DMA_FROM_DEVICE && request_size > 0) {\n\t\trc = pmcraid_copy_sglist(cmd->sglist,\n\t\t\t\t\t request_buffer,\n\t\t\t\t\t request_size,\n\t\t\t\t\t direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"failed to copy user buffer\\n\");\n\t\t\trc = -EFAULT;\n\t\t}\n\t}\n\nout_free_sglist:\n\tpmcraid_release_passthrough_ioadls(cmd, request_size, direction);\n\tpmcraid_return_cmd(cmd);\n\nout_free_buffer:\n\tkfree(buffer);\n\n\treturn rc;\n}",
        "func": "static long pmcraid_ioctl_passthrough(\n\tstruct pmcraid_instance *pinstance,\n\tunsigned int ioctl_cmd,\n\tunsigned int buflen,\n\tunsigned long arg\n)\n{\n\tstruct pmcraid_passthrough_ioctl_buffer *buffer;\n\tstruct pmcraid_ioarcb *ioarcb;\n\tstruct pmcraid_cmd *cmd;\n\tstruct pmcraid_cmd *cancel_cmd;\n\tunsigned long request_buffer;\n\tunsigned long request_offset;\n\tunsigned long lock_flags;\n\tvoid *ioasa;\n\tu32 ioasc;\n\tint request_size;\n\tint buffer_size;\n\tu8 access, direction;\n\tint rc = 0;\n\n\t/* If IOA reset is in progress, wait 10 secs for reset to complete */\n\tif (pinstance->ioa_reset_in_progress) {\n\t\trc = wait_event_interruptible_timeout(\n\t\t\t\tpinstance->reset_wait_q,\n\t\t\t\t!pinstance->ioa_reset_in_progress,\n\t\t\t\tmsecs_to_jiffies(10000));\n\n\t\tif (!rc)\n\t\t\treturn -ETIMEDOUT;\n\t\telse if (rc < 0)\n\t\t\treturn -ERESTARTSYS;\n\t}\n\n\t/* If adapter is not in operational state, return error */\n\tif (pinstance->ioa_state != IOA_STATE_OPERATIONAL) {\n\t\tpmcraid_err(\"IOA is not operational\\n\");\n\t\treturn -ENOTTY;\n\t}\n\n\tbuffer_size = sizeof(struct pmcraid_passthrough_ioctl_buffer);\n\tbuffer = kmalloc(buffer_size, GFP_KERNEL);\n\n\tif (!buffer) {\n\t\tpmcraid_err(\"no memory for passthrough buffer\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trequest_offset =\n\t    offsetof(struct pmcraid_passthrough_ioctl_buffer, request_buffer);\n\n\trequest_buffer = arg + request_offset;\n\n\trc = __copy_from_user(buffer,\n\t\t\t     (struct pmcraid_passthrough_ioctl_buffer *) arg,\n\t\t\t     sizeof(struct pmcraid_passthrough_ioctl_buffer));\n\n\tioasa =\n\t(void *)(arg +\n\t\toffsetof(struct pmcraid_passthrough_ioctl_buffer, ioasa));\n\n\tif (rc) {\n\t\tpmcraid_err(\"ioctl: can't copy passthrough buffer\\n\");\n\t\trc = -EFAULT;\n\t\tgoto out_free_buffer;\n\t}\n\n\trequest_size = buffer->ioarcb.data_transfer_length;\n\n\tif (buffer->ioarcb.request_flags0 & TRANSFER_DIR_WRITE) {\n\t\taccess = VERIFY_READ;\n\t\tdirection = DMA_TO_DEVICE;\n\t} else {\n\t\taccess = VERIFY_WRITE;\n\t\tdirection = DMA_FROM_DEVICE;\n\t}\n\n\tif (request_size > 0) {\n\t\trc = access_ok(access, arg, request_offset + request_size);\n\n\t\tif (!rc) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out_free_buffer;\n\t\t}\n\t} else if (request_size < 0) {\n\t\trc = -EINVAL;\n\t\tgoto out_free_buffer;\n\t}\n\n\t/* check if we have any additional command parameters */\n\tif (buffer->ioarcb.add_cmd_param_length > PMCRAID_ADD_CMD_PARAM_LEN) {\n\t\trc = -EINVAL;\n\t\tgoto out_free_buffer;\n\t}\n\n\tcmd = pmcraid_get_free_cmd(pinstance);\n\n\tif (!cmd) {\n\t\tpmcraid_err(\"free command block is not available\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto out_free_buffer;\n\t}\n\n\tcmd->scsi_cmd = NULL;\n\tioarcb = &(cmd->ioa_cb->ioarcb);\n\n\t/* Copy the user-provided IOARCB stuff field by field */\n\tioarcb->resource_handle = buffer->ioarcb.resource_handle;\n\tioarcb->data_transfer_length = buffer->ioarcb.data_transfer_length;\n\tioarcb->cmd_timeout = buffer->ioarcb.cmd_timeout;\n\tioarcb->request_type = buffer->ioarcb.request_type;\n\tioarcb->request_flags0 = buffer->ioarcb.request_flags0;\n\tioarcb->request_flags1 = buffer->ioarcb.request_flags1;\n\tmemcpy(ioarcb->cdb, buffer->ioarcb.cdb, PMCRAID_MAX_CDB_LEN);\n\n\tif (buffer->ioarcb.add_cmd_param_length) {\n\t\tioarcb->add_cmd_param_length =\n\t\t\tbuffer->ioarcb.add_cmd_param_length;\n\t\tioarcb->add_cmd_param_offset =\n\t\t\tbuffer->ioarcb.add_cmd_param_offset;\n\t\tmemcpy(ioarcb->add_data.u.add_cmd_params,\n\t\t\tbuffer->ioarcb.add_data.u.add_cmd_params,\n\t\t\tbuffer->ioarcb.add_cmd_param_length);\n\t}\n\n\t/* set hrrq number where the IOA should respond to. Note that all cmds\n\t * generated internally uses hrrq_id 0, exception to this is the cmd\n\t * block of scsi_cmd which is re-used (e.g. cancel/abort), which uses\n\t * hrrq_id assigned here in queuecommand\n\t */\n\tioarcb->hrrq_id = atomic_add_return(1, &(pinstance->last_message_id)) %\n\t\t\t  pinstance->num_hrrq;\n\n\tif (request_size) {\n\t\trc = pmcraid_build_passthrough_ioadls(cmd,\n\t\t\t\t\t\t      request_size,\n\t\t\t\t\t\t      direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"couldn't build passthrough ioadls\\n\");\n\t\t\tgoto out_free_buffer;\n\t\t}\n\t} else if (request_size < 0) {\n\t\trc = -EINVAL;\n\t\tgoto out_free_buffer;\n\t}\n\n\t/* If data is being written into the device, copy the data from user\n\t * buffers\n\t */\n\tif (direction == DMA_TO_DEVICE && request_size > 0) {\n\t\trc = pmcraid_copy_sglist(cmd->sglist,\n\t\t\t\t\t request_buffer,\n\t\t\t\t\t request_size,\n\t\t\t\t\t direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"failed to copy user buffer\\n\");\n\t\t\tgoto out_free_sglist;\n\t\t}\n\t}\n\n\t/* passthrough ioctl is a blocking command so, put the user to sleep\n\t * until timeout. Note that a timeout value of 0 means, do timeout.\n\t */\n\tcmd->cmd_done = pmcraid_internal_done;\n\tinit_completion(&cmd->wait_for_completion);\n\tcmd->completion_req = 1;\n\n\tpmcraid_info(\"command(%d) (CDB[0] = %x) for %x\\n\",\n\t\t     le32_to_cpu(cmd->ioa_cb->ioarcb.response_handle) >> 2,\n\t\t     cmd->ioa_cb->ioarcb.cdb[0],\n\t\t     le32_to_cpu(cmd->ioa_cb->ioarcb.resource_handle));\n\n\tspin_lock_irqsave(pinstance->host->host_lock, lock_flags);\n\t_pmcraid_fire_command(cmd);\n\tspin_unlock_irqrestore(pinstance->host->host_lock, lock_flags);\n\n\t/* NOTE ! Remove the below line once abort_task is implemented\n\t * in firmware. This line disables ioctl command timeout handling logic\n\t * similar to IO command timeout handling, making ioctl commands to wait\n\t * until the command completion regardless of timeout value specified in\n\t * ioarcb\n\t */\n\tbuffer->ioarcb.cmd_timeout = 0;\n\n\t/* If command timeout is specified put caller to wait till that time,\n\t * otherwise it would be blocking wait. If command gets timed out, it\n\t * will be aborted.\n\t */\n\tif (buffer->ioarcb.cmd_timeout == 0) {\n\t\twait_for_completion(&cmd->wait_for_completion);\n\t} else if (!wait_for_completion_timeout(\n\t\t\t&cmd->wait_for_completion,\n\t\t\tmsecs_to_jiffies(buffer->ioarcb.cmd_timeout * 1000))) {\n\n\t\tpmcraid_info(\"aborting cmd %d (CDB[0] = %x) due to timeout\\n\",\n\t\t\tle32_to_cpu(cmd->ioa_cb->ioarcb.response_handle >> 2),\n\t\t\tcmd->ioa_cb->ioarcb.cdb[0]);\n\n\t\tspin_lock_irqsave(pinstance->host->host_lock, lock_flags);\n\t\tcancel_cmd = pmcraid_abort_cmd(cmd);\n\t\tspin_unlock_irqrestore(pinstance->host->host_lock, lock_flags);\n\n\t\tif (cancel_cmd) {\n\t\t\twait_for_completion(&cancel_cmd->wait_for_completion);\n\t\t\tioasc = cancel_cmd->ioa_cb->ioasa.ioasc;\n\t\t\tpmcraid_return_cmd(cancel_cmd);\n\n\t\t\t/* if abort task couldn't find the command i.e it got\n\t\t\t * completed prior to aborting, return good completion.\n\t\t\t * if command got aborted successfully or there was IOA\n\t\t\t * reset due to abort task itself getting timedout then\n\t\t\t * return -ETIMEDOUT\n\t\t\t */\n\t\t\tif (ioasc == PMCRAID_IOASC_IOA_WAS_RESET ||\n\t\t\t    PMCRAID_IOASC_SENSE_KEY(ioasc) == 0x00) {\n\t\t\t\tif (ioasc != PMCRAID_IOASC_GC_IOARCB_NOTFOUND)\n\t\t\t\t\trc = -ETIMEDOUT;\n\t\t\t\tgoto out_handle_response;\n\t\t\t}\n\t\t}\n\n\t\t/* no command block for abort task or abort task failed to abort\n\t\t * the IOARCB, then wait for 150 more seconds and initiate reset\n\t\t * sequence after timeout\n\t\t */\n\t\tif (!wait_for_completion_timeout(\n\t\t\t&cmd->wait_for_completion,\n\t\t\tmsecs_to_jiffies(150 * 1000))) {\n\t\t\tpmcraid_reset_bringup(cmd->drv_inst);\n\t\t\trc = -ETIMEDOUT;\n\t\t}\n\t}\n\nout_handle_response:\n\t/* copy entire IOASA buffer and return IOCTL success.\n\t * If copying IOASA to user-buffer fails, return\n\t * EFAULT\n\t */\n\tif (copy_to_user(ioasa, &cmd->ioa_cb->ioasa,\n\t\tsizeof(struct pmcraid_ioasa))) {\n\t\tpmcraid_err(\"failed to copy ioasa buffer to user\\n\");\n\t\trc = -EFAULT;\n\t}\n\n\t/* If the data transfer was from device, copy the data onto user\n\t * buffers\n\t */\n\telse if (direction == DMA_FROM_DEVICE && request_size > 0) {\n\t\trc = pmcraid_copy_sglist(cmd->sglist,\n\t\t\t\t\t request_buffer,\n\t\t\t\t\t request_size,\n\t\t\t\t\t direction);\n\t\tif (rc) {\n\t\t\tpmcraid_err(\"failed to copy user buffer\\n\");\n\t\t\trc = -EFAULT;\n\t\t}\n\t}\n\nout_free_sglist:\n\tpmcraid_release_passthrough_ioadls(cmd, request_size, direction);\n\tpmcraid_return_cmd(cmd);\n\nout_free_buffer:\n\tkfree(buffer);\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -139,6 +139,9 @@\n \t\t\tpmcraid_err(\"couldn't build passthrough ioadls\\n\");\n \t\t\tgoto out_free_buffer;\n \t\t}\n+\t} else if (request_size < 0) {\n+\t\trc = -EINVAL;\n+\t\tgoto out_free_buffer;\n \t}\n \n \t/* If data is being written into the device, copy the data from user",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t} else if (request_size < 0) {",
                "\t\trc = -EINVAL;",
                "\t\tgoto out_free_buffer;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-14988",
        "func_name": "AcademySoftwareFoundation/openexr/Header::readFrom",
        "description": "Header::readfrom in IlmImf/ImfHeader.cpp in OpenEXR 2.2.0 allows remote attackers to cause a denial of service (excessive memory allocation) via a crafted file that is accessed with the ImfOpenInputFile function in IlmImf/ImfCRgbaFile.cpp. NOTE: The maintainer and multiple third parties believe that this vulnerability isn't valid",
        "git_url": "https://github.com/AcademySoftwareFoundation/openexr/commit/4c146c50e952655bc193567224c2a081c7da5e98",
        "commit_title": "specific check for bad size field in header attributes (related to #248)",
        "commit_text": "",
        "func_before": "void\nHeader::readFrom (OPENEXR_IMF_INTERNAL_NAMESPACE::IStream &is, int &version)\n{\n    //\n    // Read all attributes.\n    //\n\n    int attrCount = 0;\n\n    while (true)\n    {\n\t//\n\t// Read the name of the attribute.\n\t// A zero-length attribute name indicates the end of the header.\n\t//\n\n\tchar name[Name::SIZE];\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, Name::MAX_LENGTH, name);\n\n\tif (name[0] == 0)\n\t{\n\t    if (attrCount == 0) _readsNothing = true;\n\t    else                _readsNothing = false;\n\t    break;\n\t}\n\n\tattrCount++;\n\n\tcheckIsNullTerminated (name, \"attribute name\");\n\n\t//\n\t// Read the attribute type and the size of the attribute value.\n\t//\n\n\tchar typeName[Name::SIZE];\n\tint size;\n\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, Name::MAX_LENGTH, typeName);\n\tcheckIsNullTerminated (typeName, \"attribute type name\");\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, size);\n\n\tAttributeMap::iterator i = _map.find (name);\n\n\tif (i != _map.end())\n\t{\n\t    //\n\t    // The attribute already exists (for example,\n\t    // because it is a predefined attribute).\n\t    // Read the attribute's new value from the file.\n\t    //\n\n\t    if (strncmp (i->second->typeName(), typeName, sizeof (typeName)))\n\t\tTHROW (IEX_NAMESPACE::InputExc, \"Unexpected type for image attribute \"\n\t\t\t\t      \"\\\"\" << name << \"\\\".\");\n\n\t    i->second->readValueFrom (is, size, version);\n\t}\n\telse\n\t{\n\t    //\n\t    // The new attribute does not exist yet.\n\t    // If the attribute type is of a known type,\n\t    // read the attribute value.  If the attribute\n\t    // is of an unknown type, read its value and\n\t    // store it as an OpaqueAttribute.\n\t    //\n\n\t    Attribute *attr;\n\n\t    if (Attribute::knownType (typeName))\n\t\tattr = Attribute::newAttribute (typeName);\n\t    else\n\t\tattr = new OpaqueAttribute (typeName);\n\n\t    try\n\t    {\n\t\tattr->readValueFrom (is, size, version);\n\t\t_map[name] = attr;\n\t    }\n\t    catch (...)\n\t    {\n\t\tdelete attr;\n\t\tthrow;\n\t    }\n\t}\n    }\n}",
        "func": "void\nHeader::readFrom (OPENEXR_IMF_INTERNAL_NAMESPACE::IStream &is, int &version)\n{\n    //\n    // Read all attributes.\n    //\n\n    int attrCount = 0;\n\n    while (true)\n    {\n\t//\n\t// Read the name of the attribute.\n\t// A zero-length attribute name indicates the end of the header.\n\t//\n\n\tchar name[Name::SIZE];\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, Name::MAX_LENGTH, name);\n\n\tif (name[0] == 0)\n\t{\n\t    if (attrCount == 0) _readsNothing = true;\n\t    else                _readsNothing = false;\n\t    break;\n\t}\n\n\tattrCount++;\n\n\tcheckIsNullTerminated (name, \"attribute name\");\n\n\t//\n\t// Read the attribute type and the size of the attribute value.\n\t//\n\n\tchar typeName[Name::SIZE];\n\tint size;\n\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, Name::MAX_LENGTH, typeName);\n\tcheckIsNullTerminated (typeName, \"attribute type name\");\n\tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, size);\n\n    if( size < 0 )\n    {\n        throw IEX_NAMESPACE::InputExc(\"Invalid size field in header attribute\");\n    }\n\n\tAttributeMap::iterator i = _map.find (name);\n\n\tif (i != _map.end())\n\t{\n\t    //\n\t    // The attribute already exists (for example,\n\t    // because it is a predefined attribute).\n\t    // Read the attribute's new value from the file.\n\t    //\n\n\t    if (strncmp (i->second->typeName(), typeName, sizeof (typeName)))\n\t\tTHROW (IEX_NAMESPACE::InputExc, \"Unexpected type for image attribute \"\n\t\t\t\t      \"\\\"\" << name << \"\\\".\");\n\n\t    i->second->readValueFrom (is, size, version);\n\t}\n\telse\n\t{\n\t    //\n\t    // The new attribute does not exist yet.\n\t    // If the attribute type is of a known type,\n\t    // read the attribute value.  If the attribute\n\t    // is of an unknown type, read its value and\n\t    // store it as an OpaqueAttribute.\n\t    //\n\n\t    Attribute *attr;\n\n\t    if (Attribute::knownType (typeName))\n\t\tattr = Attribute::newAttribute (typeName);\n\t    else\n\t\tattr = new OpaqueAttribute (typeName);\n\n\t    try\n\t    {\n\t\tattr->readValueFrom (is, size, version);\n\t\t_map[name] = attr;\n\t    }\n\t    catch (...)\n\t    {\n\t\tdelete attr;\n\t\tthrow;\n\t    }\n\t}\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,6 +38,11 @@\n \tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, Name::MAX_LENGTH, typeName);\n \tcheckIsNullTerminated (typeName, \"attribute type name\");\n \tOPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (is, size);\n+\n+    if( size < 0 )\n+    {\n+        throw IEX_NAMESPACE::InputExc(\"Invalid size field in header attribute\");\n+    }\n \n \tAttributeMap::iterator i = _map.find (name);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if( size < 0 )",
                "    {",
                "        throw IEX_NAMESPACE::InputExc(\"Invalid size field in header attribute\");",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_packet_filters",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_packet_filters(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, packet_filters_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_packet_filters_session_id, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_packet_filters_packet_filters_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &packet_filters_count);\n    offset += 4;\n    if (packet_filters_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), packet_filters_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*packet_filters_count, ett_mbim_pair_list, NULL, \"Packet Filter Ref List\");\n        for (i = 0; i < packet_filters_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_packet_filters_packet_filters_packet_filter_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_packet_filters_packet_filters_packet_filter_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < packet_filters_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Packet Filter Element #%u\", i+1);\n                mbim_dissect_single_packet_filter(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_packet_filters(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, packet_filters_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_packet_filters_session_id, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_packet_filters_packet_filters_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &packet_filters_count);\n    offset += 4;\n    if (packet_filters_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*packet_filters_count, ett_mbim_pair_list, NULL, \"Packet Filter Ref List\");\n        for (i = 0; i < packet_filters_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_packet_filters_packet_filters_packet_filter_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_packet_filters_packet_filters_packet_filter_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < packet_filters_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Packet Filter Element #%u\", i+1);\n                mbim_dissect_single_packet_filter(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_packet_filters_packet_filters_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &packet_filters_count);\n     offset += 4;\n     if (packet_filters_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), packet_filters_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*packet_filters_count, ett_mbim_pair_list, NULL, \"Packet Filter Ref List\");\n         for (i = 0; i < packet_filters_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_packet_filters_packet_filters_packet_filter_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), packet_filters_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_atds_operators",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_atds_operators(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_atds_operators_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Operators List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_operators_operator_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_operators_operator_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Operator #%u\", i+1);\n                mbim_dissect_atds_operator(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_atds_operators(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_atds_operators_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Operators List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_operators_operator_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_operators_operator_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Operator #%u\", i+1);\n                mbim_dissect_atds_operator(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_atds_operators_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Operators List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_operators_operator_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_adpclk_freq_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_adpclk_freq_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_adpclk_freq_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Element Offset Length Pair\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_adpclk_freq_info_adpclk_freq_value_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_adpclk_freq_info_adpclk_freq_value_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"ADPCLK Freq Value #%u\", i+1);\n                mbim_dissect_adpclk_freq_value(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, p_pair_list_item->size);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_adpclk_freq_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_adpclk_freq_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Element Offset Length Pair\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_adpclk_freq_info_adpclk_freq_value_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_adpclk_freq_info_adpclk_freq_value_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"ADPCLK Freq Value #%u\", i+1);\n                mbim_dissect_adpclk_freq_value(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, p_pair_list_item->size);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_adpclk_freq_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Element Offset Length Pair\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_adpclk_freq_info_adpclk_freq_value_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_device_service_subscribe_list",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_device_service_subscribe_list(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_device_service_subscribe_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Device Service Subscribe Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_service_subscribe_device_service_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_service_subscribe_device_service_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Device Service Element #%u\", i+1);\n                mbim_dissect_event_entry(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_device_service_subscribe_list(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_device_service_subscribe_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Device Service Subscribe Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_service_subscribe_device_service_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_service_subscribe_device_service_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Device Service Element #%u\", i+1);\n                mbim_dissect_event_entry(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_device_service_subscribe_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n     offset += 4;\n     if (element_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Device Service Subscribe Ref List\");\n         for (i = 0; i < element_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_device_service_subscribe_device_service_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_provisioned_contexts_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_provisioned_contexts_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_provisioned_contexts_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Provisioned Context Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_provisioned_contexts_info_provisioned_context_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_provisioned_contexts_info_provisioned_context_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Provisioned Context #%u\", i+1);\n                mbim_dissect_context(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, FALSE);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_provisioned_contexts_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_provisioned_contexts_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Provisioned Context Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_provisioned_contexts_info_provisioned_context_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_provisioned_contexts_info_provisioned_context_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Provisioned Context #%u\", i+1);\n                mbim_dissect_context(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, FALSE);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_provisioned_contexts_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Provisioned Context Ref List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_provisioned_contexts_info_provisioned_context_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_atds_projection_tables",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_atds_projection_tables(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_atds_projection_tables_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Projection Tables List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_projection_tables_projection_table_offset,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_projection_tables_projection_table_size,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Projection Table #%u\", i+1);\n                mbim_dissect_atds_projection_table(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_atds_projection_tables(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_atds_projection_tables_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Projection Tables List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_projection_tables_projection_table_offset,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_projection_tables_projection_table_size,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Projection Table #%u\", i+1);\n                mbim_dissect_atds_projection_table(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_atds_projection_tables_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Projection Tables List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_atds_projection_tables_projection_table_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_providers",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_providers(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_providers_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Providers Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_providers_provider_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_providers_provider_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Provider #%u\", i+1);\n                mbim_dissect_provider(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_providers(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_providers_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Providers Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_providers_provider_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_providers_provider_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"Provider #%u\", i+1);\n                mbim_dissect_provider(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_providers_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Providers Ref List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_providers_provider_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_subscriber_ready_status",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_subscriber_ready_status(tvbuff_t *tvb, packet_info *pinfo _U_, proto_tree *tree, gint offset,\n                                     struct mbim_conv_info *mbim_conv)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, subscriber_id_offset, subscriber_id_size, sim_icc_id_offset, sim_icc_id_size, elem_count;\n    proto_item *it;\n    wmem_array_t *pair_list = NULL;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_subscr_ready_status_ready_state, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_susbcr_id_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &subscriber_id_offset);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_susbcr_id_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &subscriber_id_size);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_sim_icc_id_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &sim_icc_id_offset);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_sim_icc_id_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &sim_icc_id_size);\n    offset += 4;\n    proto_tree_add_item(tree, hf_mbim_subscr_ready_status_ready_info, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Telephone Numbers Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_subscr_ready_status_tel_nb_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_subscr_ready_status_tel_nb_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n    }\n    if (subscriber_id_offset && subscriber_id_size) {\n        it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_susbcr_id, tvb, base_offset + subscriber_id_offset,\n                                 subscriber_id_size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n        if ((mbim_conv->cellular_class == MBIM_CELLULAR_CLASS_CDMA) && (subscriber_id_size > 20)) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        } else if (subscriber_id_size > 30) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        }\n    }\n    if (sim_icc_id_offset && sim_icc_id_size) {\n        it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_sim_icc_id, tvb, base_offset + sim_icc_id_offset,\n                                 sim_icc_id_size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n        if (sim_icc_id_size > 40) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        }\n    }\n    for (i = 0; i < elem_count; i++) {\n        p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n        if (p_pair_list_item->offset && p_pair_list_item->size) {\n            it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_tel_nb, tvb, base_offset + p_pair_list_item->offset,\n                                     p_pair_list_item->size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n            if (p_pair_list_item->size > 44) {\n                expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_subscriber_ready_status(tvbuff_t *tvb, packet_info *pinfo _U_, proto_tree *tree, gint offset,\n                                     struct mbim_conv_info *mbim_conv)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, subscriber_id_offset, subscriber_id_size, sim_icc_id_offset, sim_icc_id_size, elem_count;\n    proto_item *it;\n    wmem_array_t *pair_list = NULL;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_subscr_ready_status_ready_state, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_susbcr_id_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &subscriber_id_offset);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_susbcr_id_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &subscriber_id_size);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_sim_icc_id_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &sim_icc_id_offset);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_sim_icc_id_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &sim_icc_id_size);\n    offset += 4;\n    proto_tree_add_item(tree, hf_mbim_subscr_ready_status_ready_info, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Telephone Numbers Ref List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_subscr_ready_status_tel_nb_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_subscr_ready_status_tel_nb_size, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n    }\n    if (subscriber_id_offset && subscriber_id_size) {\n        it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_susbcr_id, tvb, base_offset + subscriber_id_offset,\n                                 subscriber_id_size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n        if ((mbim_conv->cellular_class == MBIM_CELLULAR_CLASS_CDMA) && (subscriber_id_size > 20)) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        } else if (subscriber_id_size > 30) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        }\n    }\n    if (sim_icc_id_offset && sim_icc_id_size) {\n        it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_sim_icc_id, tvb, base_offset + sim_icc_id_offset,\n                                 sim_icc_id_size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n        if (sim_icc_id_size > 40) {\n            expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n        }\n    }\n    for (i = 0; i < elem_count; i++) {\n        p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n        if (p_pair_list_item->offset && p_pair_list_item->size) {\n            it = proto_tree_add_item(tree, hf_mbim_subscr_ready_status_tel_nb, tvb, base_offset + p_pair_list_item->offset,\n                                     p_pair_list_item->size, ENC_LITTLE_ENDIAN|ENC_UTF_16);\n            if (p_pair_list_item->size > 44) {\n                expert_add_info(pinfo, it, &ei_mbim_oversized_string);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,7 +25,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_subscr_ready_status_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"Telephone Numbers Ref List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_subscr_ready_status_tel_nb_offset, tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_sms_read_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_sms_read_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset,\n                           struct mbim_conv_info *mbim_conv)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, format, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_sms_read_info_format, tvb, offset, 4, ENC_LITTLE_ENDIAN, &format);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_sms_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"SMS Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_sms_read_info_sms_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_sms_read_info_sms_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"SMS Element #%u\", i+1);\n                if (format == MBIM_SMS_FORMAT_PDU) {\n                    mbim_dissect_sms_pdu_record(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, mbim_conv);\n                } else if (format == MBIM_SMS_FORMAT_CDMA) {\n                    mbim_dissect_sms_cdma_record(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n                } else {\n                    proto_tree_add_expert(subtree, pinfo, &ei_mbim_unknown_sms_format, tvb,\n                                          base_offset + p_pair_list_item->offset, p_pair_list_item->size);\n                }\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_sms_read_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset,\n                           struct mbim_conv_info *mbim_conv)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, format, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_sms_read_info_format, tvb, offset, 4, ENC_LITTLE_ENDIAN, &format);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_sms_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"SMS Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_sms_read_info_sms_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_sms_read_info_sms_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"SMS Element #%u\", i+1);\n                if (format == MBIM_SMS_FORMAT_PDU) {\n                    mbim_dissect_sms_pdu_record(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset, mbim_conv);\n                } else if (format == MBIM_SMS_FORMAT_CDMA) {\n                    mbim_dissect_sms_cdma_record(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n                } else {\n                    proto_tree_add_expert(subtree, pinfo, &ei_mbim_unknown_sms_format, tvb,\n                                          base_offset + p_pair_list_item->offset, p_pair_list_item->size);\n                }\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_sms_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n     offset += 4;\n     if (element_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"SMS Ref List\");\n         for (i = 0; i < element_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_sms_read_info_sms_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_phonebook_read_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_phonebook_read_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_phonebook_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Phonebook Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_phonebook_read_info_phonebook_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_phonebook_read_info_phonebook_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Phonebook Element #%u\", i+1);\n                mbim_dissect_phonebook_entry(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_phonebook_read_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, element_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_phonebook_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n    offset += 4;\n    if (element_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Phonebook Ref List\");\n        for (i = 0; i < element_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_phonebook_read_info_phonebook_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_phonebook_read_info_phonebook_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < element_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Phonebook Element #%u\", i+1);\n                mbim_dissect_phonebook_entry(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_phonebook_read_info_element_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &element_count);\n     offset += 4;\n     if (element_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*element_count, ett_mbim_pair_list, NULL, \"Phonebook Ref List\");\n         for (i = 0; i < element_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_phonebook_read_info_phonebook_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), element_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_device_services_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_device_services_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, device_services_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_device_services_info_device_services_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &device_services_count);\n    offset += 4;\n    proto_tree_add_item(tree, hf_mbim_device_services_info_max_dss_sessions, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    if (device_services_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), device_services_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*device_services_count, ett_mbim_pair_list, NULL, \"Device Services Ref List\");\n        for (i = 0; i < device_services_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_services_info_device_services_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_services_info_device_services_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < device_services_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Device Service Element #%u\", i+1);\n                mbim_dissect_device_service_element(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_device_services_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, device_services_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_device_services_info_device_services_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &device_services_count);\n    offset += 4;\n    proto_tree_add_item(tree, hf_mbim_device_services_info_max_dss_sessions, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    if (device_services_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*device_services_count, ett_mbim_pair_list, NULL, \"Device Services Ref List\");\n        for (i = 0; i < device_services_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_services_info_device_services_offset,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_device_services_info_device_services_size,\n                                tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < device_services_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset,\n                                         p_pair_list_item->size, ett_mbim_pair_list, NULL, \"Device Service Element #%u\", i+1);\n                mbim_dissect_device_service_element(tvb, pinfo, subtree, base_offset + p_pair_list_item->offset);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n     proto_tree_add_item(tree, hf_mbim_device_services_info_max_dss_sessions, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n     offset += 4;\n     if (device_services_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), device_services_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*device_services_count, ett_mbim_pair_list, NULL, \"Device Services Ref List\");\n         for (i = 0; i < device_services_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_device_services_info_device_services_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), device_services_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15193",
        "func_name": "wireshark/mbim_dissect_multiflow_tft_info",
        "description": "In Wireshark 2.4.0 to 2.4.1 and 2.2.0 to 2.2.9, the MBIM dissector could crash or exhaust system memory. This was addressed in epan/dissectors/packet-mbim.c by changing the memory-allocation approach.",
        "git_url": "https://github.com/wireshark/wireshark/commit/afb9ff7982971aba6e42472de0db4c1bedfc641b",
        "commit_title": "MBIM: stop pre sizing wmem arrays",
        "commit_text": " In case of malformed packet, this can lead to an insane amount of memory. Instead let's use the automatic growth mecanism. This way the malformed packet is caught by the dissection engine.  Bug: 14056",
        "func_before": "static void\nmbim_dissect_multiflow_tft_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_multiflow_tft_info_session_id, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_multiflow_tft_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"TFT List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_multiflow_tft_info_tft_list_offset,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_multiflow_tft_info_tft_list_size,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"TFT #%u\", i+1);\n                de_sm_tflow_temp(tvb, subtree, pinfo, base_offset + p_pair_list_item->offset, p_pair_list_item->size, NULL, 0);\n            }\n        }\n    }\n}",
        "func": "static void\nmbim_dissect_multiflow_tft_info(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, gint offset)\n{\n    proto_tree *subtree;\n    gint base_offset;\n    guint32 i, elem_count;\n    wmem_array_t *pair_list;\n    struct mbim_pair_list pair_list_item, *p_pair_list_item;\n\n    base_offset = offset;\n    proto_tree_add_item(tree, hf_mbim_multiflow_tft_info_session_id, tvb, offset, 4, ENC_LITTLE_ENDIAN);\n    offset += 4;\n    proto_tree_add_item_ret_uint(tree, hf_mbim_multiflow_tft_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n    offset += 4;\n    if (elem_count) {\n        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n        subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"TFT List\");\n        for (i = 0; i < elem_count; i++) {\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_multiflow_tft_info_tft_list_offset,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.offset);\n            offset += 4;\n            proto_tree_add_item_ret_uint(subtree, hf_mbim_multiflow_tft_info_tft_list_size,\n                                         tvb, offset, 4, ENC_LITTLE_ENDIAN, &pair_list_item.size);\n            offset += 4;\n            wmem_array_append_one(pair_list, pair_list_item);\n        }\n        for (i = 0; i < elem_count; i++) {\n            p_pair_list_item = (struct mbim_pair_list*)wmem_array_index(pair_list, i);\n            if (p_pair_list_item->offset && p_pair_list_item->size) {\n                subtree = proto_tree_add_subtree_format(tree, tvb, base_offset + p_pair_list_item->offset, p_pair_list_item->size,\n                            ett_mbim_pair_list, NULL, \"TFT #%u\", i+1);\n                de_sm_tflow_temp(tvb, subtree, pinfo, base_offset + p_pair_list_item->offset, p_pair_list_item->size, NULL, 0);\n            }\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n     proto_tree_add_item_ret_uint(tree, hf_mbim_multiflow_tft_info_elem_count, tvb, offset, 4, ENC_LITTLE_ENDIAN, &elem_count);\n     offset += 4;\n     if (elem_count) {\n-        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);\n+        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));\n         subtree = proto_tree_add_subtree(tree, tvb, offset, 8*elem_count, ett_mbim_pair_list, NULL, \"TFT List\");\n         for (i = 0; i < elem_count; i++) {\n             proto_tree_add_item_ret_uint(subtree, hf_mbim_multiflow_tft_info_tft_list_offset,",
        "diff_line_info": {
            "deleted_lines": [
                "        pair_list = wmem_array_sized_new(wmem_packet_scope(), sizeof(struct mbim_pair_list), elem_count);"
            ],
            "added_lines": [
                "        pair_list = wmem_array_new(wmem_packet_scope(), sizeof(struct mbim_pair_list));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/_put_final_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/e40b0219a8c77741ae48989efb520f4a762a5be3",
        "commit_title": "x86: don't wrongly trigger linear page table assertion (2)",
        "commit_text": " _put_final_page_type(), when free_page_type() has exited early to allow for preemption, should not update the time stamp, as the page continues to retain the typ which is in the process of being unvalidated. I can't see why the time stamp update was put on that path in the first place (albeit it may well have been me who had put it there years ago).  This is part of XSA-240. ",
        "func_before": "static int _put_final_page_type(struct page_info *page, unsigned long type,\n                                bool preemptible, struct page_info *ptpg)\n{\n    int rc = free_page_type(page, type, preemptible);\n\n    /* No need for atomic update of type_info here: noone else updates it. */\n    if ( rc == 0 )\n    {\n        if ( ptpg && PGT_type_equal(type, ptpg->u.inuse.type_info) )\n        {\n            dec_linear_uses(page);\n            dec_linear_entries(ptpg);\n        }\n        ASSERT(!page->linear_pt_count || page_get_owner(page)->is_dying);\n        set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info--;\n    }\n    else if ( rc == -EINTR )\n    {\n        ASSERT((page->u.inuse.type_info &\n                (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n        set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info |= PGT_validated;\n    }\n    else\n    {\n        BUG_ON(rc != -ERESTART);\n        smp_wmb();\n        get_page_light(page);\n        page->u.inuse.type_info |= PGT_partial;\n    }\n\n    return rc;\n}",
        "func": "static int _put_final_page_type(struct page_info *page, unsigned long type,\n                                bool preemptible, struct page_info *ptpg)\n{\n    int rc = free_page_type(page, type, preemptible);\n\n    /* No need for atomic update of type_info here: noone else updates it. */\n    if ( rc == 0 )\n    {\n        if ( ptpg && PGT_type_equal(type, ptpg->u.inuse.type_info) )\n        {\n            dec_linear_uses(page);\n            dec_linear_entries(ptpg);\n        }\n        ASSERT(!page->linear_pt_count || page_get_owner(page)->is_dying);\n        set_tlbflush_timestamp(page);\n        smp_wmb();\n        page->u.inuse.type_info--;\n    }\n    else if ( rc == -EINTR )\n    {\n        ASSERT((page->u.inuse.type_info &\n                (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n        smp_wmb();\n        page->u.inuse.type_info |= PGT_validated;\n    }\n    else\n    {\n        BUG_ON(rc != -ERESTART);\n        smp_wmb();\n        get_page_light(page);\n        page->u.inuse.type_info |= PGT_partial;\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,7 +20,6 @@\n     {\n         ASSERT((page->u.inuse.type_info &\n                 (PGT_count_mask|PGT_validated|PGT_partial)) == 1);\n-        set_tlbflush_timestamp(page);\n         smp_wmb();\n         page->u.inuse.type_info |= PGT_validated;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "        set_tlbflush_timestamp(page);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/arch_set_info_guest",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "int arch_set_info_guest(\n    struct vcpu *v, vcpu_guest_context_u c)\n{\n    struct domain *d = v->domain;\n    unsigned long cr3_gfn;\n    struct page_info *cr3_page;\n    unsigned long flags, cr4;\n    unsigned int i;\n    int rc = 0, compat;\n\n    /* The context is a compat-mode one if the target domain is compat-mode;\n     * we expect the tools to DTRT even in compat-mode callers. */\n    compat = is_pv_32bit_domain(d);\n\n#define c(fld) (compat ? (c.cmp->fld) : (c.nat->fld))\n    flags = c(flags);\n\n    if ( is_pv_domain(d) )\n    {\n        if ( !compat )\n        {\n            if ( !is_canonical_address(c.nat->user_regs.rip) ||\n                 !is_canonical_address(c.nat->user_regs.rsp) ||\n                 !is_canonical_address(c.nat->kernel_sp) ||\n                 (c.nat->ldt_ents && !is_canonical_address(c.nat->ldt_base)) ||\n                 !is_canonical_address(c.nat->fs_base) ||\n                 !is_canonical_address(c.nat->gs_base_kernel) ||\n                 !is_canonical_address(c.nat->gs_base_user) ||\n                 !is_canonical_address(c.nat->event_callback_eip) ||\n                 !is_canonical_address(c.nat->syscall_callback_eip) ||\n                 !is_canonical_address(c.nat->failsafe_callback_eip) )\n                return -EINVAL;\n\n            fixup_guest_stack_selector(d, c.nat->user_regs.ss);\n            fixup_guest_stack_selector(d, c.nat->kernel_ss);\n            fixup_guest_code_selector(d, c.nat->user_regs.cs);\n\n            for ( i = 0; i < ARRAY_SIZE(c.nat->trap_ctxt); i++ )\n            {\n                if ( !is_canonical_address(c.nat->trap_ctxt[i].address) )\n                    return -EINVAL;\n                fixup_guest_code_selector(d, c.nat->trap_ctxt[i].cs);\n            }\n\n            if ( !__addr_ok(c.nat->ldt_base) )\n                return -EINVAL;\n        }\n        else\n        {\n            fixup_guest_stack_selector(d, c.cmp->user_regs.ss);\n            fixup_guest_stack_selector(d, c.cmp->kernel_ss);\n            fixup_guest_code_selector(d, c.cmp->user_regs.cs);\n            fixup_guest_code_selector(d, c.cmp->event_callback_cs);\n            fixup_guest_code_selector(d, c.cmp->failsafe_callback_cs);\n\n            for ( i = 0; i < ARRAY_SIZE(c.cmp->trap_ctxt); i++ )\n                fixup_guest_code_selector(d, c.cmp->trap_ctxt[i].cs);\n        }\n\n        /* LDT safety checks. */\n        if ( ((c(ldt_base) & (PAGE_SIZE - 1)) != 0) ||\n             (c(ldt_ents) > 8192) )\n            return -EINVAL;\n    }\n\n    v->fpu_initialised = !!(flags & VGCF_I387_VALID);\n\n    v->arch.flags &= ~TF_kernel_mode;\n    if ( (flags & VGCF_in_kernel) || is_hvm_domain(d)/*???*/ )\n        v->arch.flags |= TF_kernel_mode;\n\n    v->arch.vgc_flags = flags;\n\n    if ( flags & VGCF_I387_VALID )\n    {\n        memcpy(v->arch.fpu_ctxt, &c.nat->fpu_ctxt, sizeof(c.nat->fpu_ctxt));\n        if ( v->arch.xsave_area )\n            v->arch.xsave_area->xsave_hdr.xstate_bv = XSTATE_FP_SSE;\n    }\n    else if ( v->arch.xsave_area )\n    {\n        v->arch.xsave_area->xsave_hdr.xstate_bv = 0;\n        v->arch.xsave_area->fpu_sse.mxcsr = MXCSR_DEFAULT;\n    }\n    else\n    {\n        typeof(v->arch.xsave_area->fpu_sse) *fpu_sse = v->arch.fpu_ctxt;\n\n        memset(fpu_sse, 0, sizeof(*fpu_sse));\n        fpu_sse->fcw = FCW_DEFAULT;\n        fpu_sse->mxcsr = MXCSR_DEFAULT;\n    }\n    if ( v->arch.xsave_area )\n        v->arch.xsave_area->xsave_hdr.xcomp_bv = 0;\n\n    if ( !compat )\n    {\n        memcpy(&v->arch.user_regs, &c.nat->user_regs, sizeof(c.nat->user_regs));\n        if ( is_pv_domain(d) )\n            memcpy(v->arch.pv_vcpu.trap_ctxt, c.nat->trap_ctxt,\n                   sizeof(c.nat->trap_ctxt));\n    }\n    else\n    {\n        XLAT_cpu_user_regs(&v->arch.user_regs, &c.cmp->user_regs);\n        if ( is_pv_domain(d) )\n        {\n            for ( i = 0; i < ARRAY_SIZE(c.cmp->trap_ctxt); ++i )\n                XLAT_trap_info(v->arch.pv_vcpu.trap_ctxt + i,\n                               c.cmp->trap_ctxt + i);\n        }\n    }\n\n    if ( is_hvm_domain(d) )\n    {\n        for ( i = 0; i < ARRAY_SIZE(v->arch.debugreg); ++i )\n            v->arch.debugreg[i] = c(debugreg[i]);\n\n        hvm_set_info_guest(v);\n        goto out;\n    }\n\n    init_int80_direct_trap(v);\n\n    /* IOPL privileges are virtualised. */\n    v->arch.pv_vcpu.iopl = v->arch.user_regs.eflags & X86_EFLAGS_IOPL;\n    v->arch.user_regs.eflags &= ~X86_EFLAGS_IOPL;\n\n    /* Ensure real hardware interrupts are enabled. */\n    v->arch.user_regs.eflags |= X86_EFLAGS_IF;\n\n    if ( !v->is_initialised )\n    {\n        if ( !compat && !(flags & VGCF_in_kernel) && !c.nat->ctrlreg[1] )\n            return -EINVAL;\n\n        v->arch.pv_vcpu.ldt_base = c(ldt_base);\n        v->arch.pv_vcpu.ldt_ents = c(ldt_ents);\n    }\n    else\n    {\n        unsigned long pfn = pagetable_get_pfn(v->arch.guest_table);\n        bool fail;\n\n        if ( !compat )\n        {\n            fail = xen_pfn_to_cr3(pfn) != c.nat->ctrlreg[3];\n            if ( pagetable_is_null(v->arch.guest_table_user) )\n                fail |= c.nat->ctrlreg[1] || !(flags & VGCF_in_kernel);\n            else\n            {\n                pfn = pagetable_get_pfn(v->arch.guest_table_user);\n                fail |= xen_pfn_to_cr3(pfn) != c.nat->ctrlreg[1];\n            }\n        } else {\n            l4_pgentry_t *l4tab = map_domain_page(_mfn(pfn));\n\n            pfn = l4e_get_pfn(*l4tab);\n            unmap_domain_page(l4tab);\n            fail = compat_pfn_to_cr3(pfn) != c.cmp->ctrlreg[3];\n        }\n\n        for ( i = 0; i < ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames); ++i )\n            fail |= v->arch.pv_vcpu.gdt_frames[i] != c(gdt_frames[i]);\n        fail |= v->arch.pv_vcpu.gdt_ents != c(gdt_ents);\n\n        fail |= v->arch.pv_vcpu.ldt_base != c(ldt_base);\n        fail |= v->arch.pv_vcpu.ldt_ents != c(ldt_ents);\n\n        if ( fail )\n           return -EOPNOTSUPP;\n    }\n\n    v->arch.pv_vcpu.kernel_ss = c(kernel_ss);\n    v->arch.pv_vcpu.kernel_sp = c(kernel_sp);\n    for ( i = 0; i < ARRAY_SIZE(v->arch.pv_vcpu.ctrlreg); ++i )\n        v->arch.pv_vcpu.ctrlreg[i] = c(ctrlreg[i]);\n\n    v->arch.pv_vcpu.event_callback_eip = c(event_callback_eip);\n    v->arch.pv_vcpu.failsafe_callback_eip = c(failsafe_callback_eip);\n    if ( !compat )\n    {\n        v->arch.pv_vcpu.syscall_callback_eip = c.nat->syscall_callback_eip;\n        v->arch.pv_vcpu.fs_base = c.nat->fs_base;\n        v->arch.pv_vcpu.gs_base_kernel = c.nat->gs_base_kernel;\n        v->arch.pv_vcpu.gs_base_user = c.nat->gs_base_user;\n    }\n    else\n    {\n        v->arch.pv_vcpu.event_callback_cs = c(event_callback_cs);\n        v->arch.pv_vcpu.failsafe_callback_cs = c(failsafe_callback_cs);\n    }\n\n    /* Only CR0.TS is modifiable by guest or admin. */\n    v->arch.pv_vcpu.ctrlreg[0] &= X86_CR0_TS;\n    v->arch.pv_vcpu.ctrlreg[0] |= read_cr0() & ~X86_CR0_TS;\n\n    cr4 = v->arch.pv_vcpu.ctrlreg[4];\n    v->arch.pv_vcpu.ctrlreg[4] = cr4 ? pv_guest_cr4_fixup(v, cr4) :\n        real_cr4_to_pv_guest_cr4(mmu_cr4_features);\n\n    memset(v->arch.debugreg, 0, sizeof(v->arch.debugreg));\n    for ( i = 0; i < 8; i++ )\n        (void)set_debugreg(v, i, c(debugreg[i]));\n\n    if ( v->is_initialised )\n        goto out;\n\n    if ( v->vcpu_id == 0 )\n    {\n        /*\n         * In the restore case we need to deal with L4 pages which got\n         * initialized with m2p_strict still clear (and which hence lack the\n         * correct initial RO_MPT_VIRT_{START,END} L4 entry).\n         */\n        if ( d != current->domain && !VM_ASSIST(d, m2p_strict) &&\n             is_pv_domain(d) && !is_pv_32bit_domain(d) &&\n             test_bit(VMASST_TYPE_m2p_strict, &c.nat->vm_assist) &&\n             atomic_read(&d->arch.pv_domain.nr_l4_pages) )\n        {\n            bool done = false;\n\n            spin_lock_recursive(&d->page_alloc_lock);\n\n            for ( i = 0; ; )\n            {\n                struct page_info *page = page_list_remove_head(&d->page_list);\n\n                if ( page_lock(page) )\n                {\n                    if ( (page->u.inuse.type_info & PGT_type_mask) ==\n                         PGT_l4_page_table )\n                        done = !fill_ro_mpt(_mfn(page_to_mfn(page)));\n\n                    page_unlock(page);\n                }\n\n                page_list_add_tail(page, &d->page_list);\n\n                if ( done || (!(++i & 0xff) && hypercall_preempt_check()) )\n                    break;\n            }\n\n            spin_unlock_recursive(&d->page_alloc_lock);\n\n            if ( !done )\n                return -ERESTART;\n        }\n\n        d->vm_assist = c(vm_assist);\n    }\n\n    rc = put_old_guest_table(current);\n    if ( rc )\n        return rc;\n\n    if ( !compat )\n        rc = (int)pv_set_gdt(v, c.nat->gdt_frames, c.nat->gdt_ents);\n    else\n    {\n        unsigned long gdt_frames[ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames)];\n        unsigned int n = (c.cmp->gdt_ents + 511) / 512;\n\n        if ( n > ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames) )\n            return -EINVAL;\n        for ( i = 0; i < n; ++i )\n            gdt_frames[i] = c.cmp->gdt_frames[i];\n        rc = (int)pv_set_gdt(v, gdt_frames, c.cmp->gdt_ents);\n    }\n    if ( rc != 0 )\n        return rc;\n\n    set_bit(_VPF_in_reset, &v->pause_flags);\n\n    if ( !compat )\n        cr3_gfn = xen_cr3_to_pfn(c.nat->ctrlreg[3]);\n    else\n        cr3_gfn = compat_cr3_to_pfn(c.cmp->ctrlreg[3]);\n    cr3_page = get_page_from_gfn(d, cr3_gfn, NULL, P2M_ALLOC);\n\n    if ( !cr3_page )\n        rc = -EINVAL;\n    else if ( paging_mode_refcounts(d) )\n        /* nothing */;\n    else if ( cr3_page == v->arch.old_guest_table )\n    {\n        v->arch.old_guest_table = NULL;\n        put_page(cr3_page);\n    }\n    else\n    {\n        if ( !compat )\n            rc = put_old_guest_table(v);\n        if ( !rc )\n            rc = get_page_type_preemptible(cr3_page,\n                                           !compat ? PGT_root_page_table\n                                                   : PGT_l3_page_table);\n        switch ( rc )\n        {\n        case -EINTR:\n            rc = -ERESTART;\n        case -ERESTART:\n            break;\n        case 0:\n            if ( !compat && !VM_ASSIST(d, m2p_strict) &&\n                 !paging_mode_refcounts(d) )\n                fill_ro_mpt(_mfn(cr3_gfn));\n            break;\n        default:\n            if ( cr3_page == current->arch.old_guest_table )\n                cr3_page = NULL;\n            break;\n        }\n    }\n    if ( rc )\n        /* handled below */;\n    else if ( !compat )\n    {\n        v->arch.guest_table = pagetable_from_page(cr3_page);\n        if ( c.nat->ctrlreg[1] )\n        {\n            cr3_gfn = xen_cr3_to_pfn(c.nat->ctrlreg[1]);\n            cr3_page = get_page_from_gfn(d, cr3_gfn, NULL, P2M_ALLOC);\n\n            if ( !cr3_page )\n                rc = -EINVAL;\n            else if ( !paging_mode_refcounts(d) )\n            {\n                rc = get_page_type_preemptible(cr3_page, PGT_root_page_table);\n                switch ( rc )\n                {\n                case -EINTR:\n                    rc = -ERESTART;\n                    /* Fallthrough */\n                case -ERESTART:\n                    v->arch.old_guest_table =\n                        pagetable_get_page(v->arch.guest_table);\n                    v->arch.guest_table = pagetable_null();\n                    break;\n                default:\n                    if ( cr3_page == current->arch.old_guest_table )\n                        cr3_page = NULL;\n                    break;\n                case 0:\n                    if ( VM_ASSIST(d, m2p_strict) )\n                        zap_ro_mpt(_mfn(cr3_gfn));\n                    break;\n                }\n            }\n            if ( !rc )\n               v->arch.guest_table_user = pagetable_from_page(cr3_page);\n        }\n    }\n    else\n    {\n        l4_pgentry_t *l4tab;\n\n        l4tab = map_domain_page(pagetable_get_mfn(v->arch.guest_table));\n        *l4tab = l4e_from_pfn(page_to_mfn(cr3_page),\n            _PAGE_PRESENT|_PAGE_RW|_PAGE_USER|_PAGE_ACCESSED);\n        unmap_domain_page(l4tab);\n    }\n    if ( rc )\n    {\n        if ( cr3_page )\n            put_page(cr3_page);\n        pv_destroy_gdt(v);\n        return rc;\n    }\n\n    clear_bit(_VPF_in_reset, &v->pause_flags);\n\n    if ( v->vcpu_id == 0 )\n        update_domain_wallclock_time(d);\n\n    /* Don't redo final setup */\n    v->is_initialised = 1;\n\n    if ( paging_mode_enabled(d) )\n        paging_update_paging_modes(v);\n\n    update_cr3(v);\n\n out:\n    if ( flags & VGCF_online )\n        clear_bit(_VPF_down, &v->pause_flags);\n    else\n        set_bit(_VPF_down, &v->pause_flags);\n    return 0;\n#undef c\n}",
        "func": "int arch_set_info_guest(\n    struct vcpu *v, vcpu_guest_context_u c)\n{\n    struct domain *d = v->domain;\n    unsigned long cr3_gfn;\n    struct page_info *cr3_page;\n    unsigned long flags, cr4;\n    unsigned int i;\n    int rc = 0, compat;\n\n    /* The context is a compat-mode one if the target domain is compat-mode;\n     * we expect the tools to DTRT even in compat-mode callers. */\n    compat = is_pv_32bit_domain(d);\n\n#define c(fld) (compat ? (c.cmp->fld) : (c.nat->fld))\n    flags = c(flags);\n\n    if ( is_pv_domain(d) )\n    {\n        if ( !compat )\n        {\n            if ( !is_canonical_address(c.nat->user_regs.rip) ||\n                 !is_canonical_address(c.nat->user_regs.rsp) ||\n                 !is_canonical_address(c.nat->kernel_sp) ||\n                 (c.nat->ldt_ents && !is_canonical_address(c.nat->ldt_base)) ||\n                 !is_canonical_address(c.nat->fs_base) ||\n                 !is_canonical_address(c.nat->gs_base_kernel) ||\n                 !is_canonical_address(c.nat->gs_base_user) ||\n                 !is_canonical_address(c.nat->event_callback_eip) ||\n                 !is_canonical_address(c.nat->syscall_callback_eip) ||\n                 !is_canonical_address(c.nat->failsafe_callback_eip) )\n                return -EINVAL;\n\n            fixup_guest_stack_selector(d, c.nat->user_regs.ss);\n            fixup_guest_stack_selector(d, c.nat->kernel_ss);\n            fixup_guest_code_selector(d, c.nat->user_regs.cs);\n\n            for ( i = 0; i < ARRAY_SIZE(c.nat->trap_ctxt); i++ )\n            {\n                if ( !is_canonical_address(c.nat->trap_ctxt[i].address) )\n                    return -EINVAL;\n                fixup_guest_code_selector(d, c.nat->trap_ctxt[i].cs);\n            }\n\n            if ( !__addr_ok(c.nat->ldt_base) )\n                return -EINVAL;\n        }\n        else\n        {\n            fixup_guest_stack_selector(d, c.cmp->user_regs.ss);\n            fixup_guest_stack_selector(d, c.cmp->kernel_ss);\n            fixup_guest_code_selector(d, c.cmp->user_regs.cs);\n            fixup_guest_code_selector(d, c.cmp->event_callback_cs);\n            fixup_guest_code_selector(d, c.cmp->failsafe_callback_cs);\n\n            for ( i = 0; i < ARRAY_SIZE(c.cmp->trap_ctxt); i++ )\n                fixup_guest_code_selector(d, c.cmp->trap_ctxt[i].cs);\n        }\n\n        /* LDT safety checks. */\n        if ( ((c(ldt_base) & (PAGE_SIZE - 1)) != 0) ||\n             (c(ldt_ents) > 8192) )\n            return -EINVAL;\n    }\n\n    v->fpu_initialised = !!(flags & VGCF_I387_VALID);\n\n    v->arch.flags &= ~TF_kernel_mode;\n    if ( (flags & VGCF_in_kernel) || is_hvm_domain(d)/*???*/ )\n        v->arch.flags |= TF_kernel_mode;\n\n    v->arch.vgc_flags = flags;\n\n    if ( flags & VGCF_I387_VALID )\n    {\n        memcpy(v->arch.fpu_ctxt, &c.nat->fpu_ctxt, sizeof(c.nat->fpu_ctxt));\n        if ( v->arch.xsave_area )\n            v->arch.xsave_area->xsave_hdr.xstate_bv = XSTATE_FP_SSE;\n    }\n    else if ( v->arch.xsave_area )\n    {\n        v->arch.xsave_area->xsave_hdr.xstate_bv = 0;\n        v->arch.xsave_area->fpu_sse.mxcsr = MXCSR_DEFAULT;\n    }\n    else\n    {\n        typeof(v->arch.xsave_area->fpu_sse) *fpu_sse = v->arch.fpu_ctxt;\n\n        memset(fpu_sse, 0, sizeof(*fpu_sse));\n        fpu_sse->fcw = FCW_DEFAULT;\n        fpu_sse->mxcsr = MXCSR_DEFAULT;\n    }\n    if ( v->arch.xsave_area )\n        v->arch.xsave_area->xsave_hdr.xcomp_bv = 0;\n\n    if ( !compat )\n    {\n        memcpy(&v->arch.user_regs, &c.nat->user_regs, sizeof(c.nat->user_regs));\n        if ( is_pv_domain(d) )\n            memcpy(v->arch.pv_vcpu.trap_ctxt, c.nat->trap_ctxt,\n                   sizeof(c.nat->trap_ctxt));\n    }\n    else\n    {\n        XLAT_cpu_user_regs(&v->arch.user_regs, &c.cmp->user_regs);\n        if ( is_pv_domain(d) )\n        {\n            for ( i = 0; i < ARRAY_SIZE(c.cmp->trap_ctxt); ++i )\n                XLAT_trap_info(v->arch.pv_vcpu.trap_ctxt + i,\n                               c.cmp->trap_ctxt + i);\n        }\n    }\n\n    if ( is_hvm_domain(d) )\n    {\n        for ( i = 0; i < ARRAY_SIZE(v->arch.debugreg); ++i )\n            v->arch.debugreg[i] = c(debugreg[i]);\n\n        hvm_set_info_guest(v);\n        goto out;\n    }\n\n    init_int80_direct_trap(v);\n\n    /* IOPL privileges are virtualised. */\n    v->arch.pv_vcpu.iopl = v->arch.user_regs.eflags & X86_EFLAGS_IOPL;\n    v->arch.user_regs.eflags &= ~X86_EFLAGS_IOPL;\n\n    /* Ensure real hardware interrupts are enabled. */\n    v->arch.user_regs.eflags |= X86_EFLAGS_IF;\n\n    if ( !v->is_initialised )\n    {\n        if ( !compat && !(flags & VGCF_in_kernel) && !c.nat->ctrlreg[1] )\n            return -EINVAL;\n\n        v->arch.pv_vcpu.ldt_base = c(ldt_base);\n        v->arch.pv_vcpu.ldt_ents = c(ldt_ents);\n    }\n    else\n    {\n        unsigned long pfn = pagetable_get_pfn(v->arch.guest_table);\n        bool fail;\n\n        if ( !compat )\n        {\n            fail = xen_pfn_to_cr3(pfn) != c.nat->ctrlreg[3];\n            if ( pagetable_is_null(v->arch.guest_table_user) )\n                fail |= c.nat->ctrlreg[1] || !(flags & VGCF_in_kernel);\n            else\n            {\n                pfn = pagetable_get_pfn(v->arch.guest_table_user);\n                fail |= xen_pfn_to_cr3(pfn) != c.nat->ctrlreg[1];\n            }\n        } else {\n            l4_pgentry_t *l4tab = map_domain_page(_mfn(pfn));\n\n            pfn = l4e_get_pfn(*l4tab);\n            unmap_domain_page(l4tab);\n            fail = compat_pfn_to_cr3(pfn) != c.cmp->ctrlreg[3];\n        }\n\n        for ( i = 0; i < ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames); ++i )\n            fail |= v->arch.pv_vcpu.gdt_frames[i] != c(gdt_frames[i]);\n        fail |= v->arch.pv_vcpu.gdt_ents != c(gdt_ents);\n\n        fail |= v->arch.pv_vcpu.ldt_base != c(ldt_base);\n        fail |= v->arch.pv_vcpu.ldt_ents != c(ldt_ents);\n\n        if ( fail )\n           return -EOPNOTSUPP;\n    }\n\n    v->arch.pv_vcpu.kernel_ss = c(kernel_ss);\n    v->arch.pv_vcpu.kernel_sp = c(kernel_sp);\n    for ( i = 0; i < ARRAY_SIZE(v->arch.pv_vcpu.ctrlreg); ++i )\n        v->arch.pv_vcpu.ctrlreg[i] = c(ctrlreg[i]);\n\n    v->arch.pv_vcpu.event_callback_eip = c(event_callback_eip);\n    v->arch.pv_vcpu.failsafe_callback_eip = c(failsafe_callback_eip);\n    if ( !compat )\n    {\n        v->arch.pv_vcpu.syscall_callback_eip = c.nat->syscall_callback_eip;\n        v->arch.pv_vcpu.fs_base = c.nat->fs_base;\n        v->arch.pv_vcpu.gs_base_kernel = c.nat->gs_base_kernel;\n        v->arch.pv_vcpu.gs_base_user = c.nat->gs_base_user;\n    }\n    else\n    {\n        v->arch.pv_vcpu.event_callback_cs = c(event_callback_cs);\n        v->arch.pv_vcpu.failsafe_callback_cs = c(failsafe_callback_cs);\n    }\n\n    /* Only CR0.TS is modifiable by guest or admin. */\n    v->arch.pv_vcpu.ctrlreg[0] &= X86_CR0_TS;\n    v->arch.pv_vcpu.ctrlreg[0] |= read_cr0() & ~X86_CR0_TS;\n\n    cr4 = v->arch.pv_vcpu.ctrlreg[4];\n    v->arch.pv_vcpu.ctrlreg[4] = cr4 ? pv_guest_cr4_fixup(v, cr4) :\n        real_cr4_to_pv_guest_cr4(mmu_cr4_features);\n\n    memset(v->arch.debugreg, 0, sizeof(v->arch.debugreg));\n    for ( i = 0; i < 8; i++ )\n        (void)set_debugreg(v, i, c(debugreg[i]));\n\n    if ( v->is_initialised )\n        goto out;\n\n    if ( v->vcpu_id == 0 )\n    {\n        /*\n         * In the restore case we need to deal with L4 pages which got\n         * initialized with m2p_strict still clear (and which hence lack the\n         * correct initial RO_MPT_VIRT_{START,END} L4 entry).\n         */\n        if ( d != current->domain && !VM_ASSIST(d, m2p_strict) &&\n             is_pv_domain(d) && !is_pv_32bit_domain(d) &&\n             test_bit(VMASST_TYPE_m2p_strict, &c.nat->vm_assist) &&\n             atomic_read(&d->arch.pv_domain.nr_l4_pages) )\n        {\n            bool done = false;\n\n            spin_lock_recursive(&d->page_alloc_lock);\n\n            for ( i = 0; ; )\n            {\n                struct page_info *page = page_list_remove_head(&d->page_list);\n\n                if ( page_lock(page) )\n                {\n                    if ( (page->u.inuse.type_info & PGT_type_mask) ==\n                         PGT_l4_page_table )\n                        done = !fill_ro_mpt(_mfn(page_to_mfn(page)));\n\n                    page_unlock(page);\n                }\n\n                page_list_add_tail(page, &d->page_list);\n\n                if ( done || (!(++i & 0xff) && hypercall_preempt_check()) )\n                    break;\n            }\n\n            spin_unlock_recursive(&d->page_alloc_lock);\n\n            if ( !done )\n                return -ERESTART;\n        }\n\n        d->vm_assist = c(vm_assist);\n    }\n\n    rc = put_old_guest_table(current);\n    if ( rc )\n        return rc;\n\n    if ( !compat )\n        rc = (int)pv_set_gdt(v, c.nat->gdt_frames, c.nat->gdt_ents);\n    else\n    {\n        unsigned long gdt_frames[ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames)];\n        unsigned int n = (c.cmp->gdt_ents + 511) / 512;\n\n        if ( n > ARRAY_SIZE(v->arch.pv_vcpu.gdt_frames) )\n            return -EINVAL;\n        for ( i = 0; i < n; ++i )\n            gdt_frames[i] = c.cmp->gdt_frames[i];\n        rc = (int)pv_set_gdt(v, gdt_frames, c.cmp->gdt_ents);\n    }\n    if ( rc != 0 )\n        return rc;\n\n    set_bit(_VPF_in_reset, &v->pause_flags);\n\n    if ( !compat )\n        cr3_gfn = xen_cr3_to_pfn(c.nat->ctrlreg[3]);\n    else\n        cr3_gfn = compat_cr3_to_pfn(c.cmp->ctrlreg[3]);\n    cr3_page = get_page_from_gfn(d, cr3_gfn, NULL, P2M_ALLOC);\n\n    if ( !cr3_page )\n        rc = -EINVAL;\n    else if ( paging_mode_refcounts(d) )\n        /* nothing */;\n    else if ( cr3_page == v->arch.old_guest_table )\n    {\n        v->arch.old_guest_table = NULL;\n        put_page(cr3_page);\n    }\n    else\n    {\n        if ( !compat )\n            rc = put_old_guest_table(v);\n        if ( !rc )\n            rc = get_page_type_preemptible(cr3_page,\n                                           !compat ? PGT_root_page_table\n                                                   : PGT_l3_page_table);\n        switch ( rc )\n        {\n        case -EINTR:\n            rc = -ERESTART;\n        case -ERESTART:\n            break;\n        case 0:\n            if ( !compat && !VM_ASSIST(d, m2p_strict) &&\n                 !paging_mode_refcounts(d) )\n                fill_ro_mpt(_mfn(cr3_gfn));\n            break;\n        default:\n            if ( cr3_page == current->arch.old_guest_table )\n                cr3_page = NULL;\n            break;\n        }\n    }\n    if ( rc )\n        /* handled below */;\n    else if ( !compat )\n    {\n        v->arch.guest_table = pagetable_from_page(cr3_page);\n        if ( c.nat->ctrlreg[1] )\n        {\n            cr3_gfn = xen_cr3_to_pfn(c.nat->ctrlreg[1]);\n            cr3_page = get_page_from_gfn(d, cr3_gfn, NULL, P2M_ALLOC);\n\n            if ( !cr3_page )\n                rc = -EINVAL;\n            else if ( !paging_mode_refcounts(d) )\n            {\n                rc = get_page_type_preemptible(cr3_page, PGT_root_page_table);\n                switch ( rc )\n                {\n                case -EINTR:\n                    rc = -ERESTART;\n                    /* Fallthrough */\n                case -ERESTART:\n                    v->arch.old_guest_ptpg = NULL;\n                    v->arch.old_guest_table =\n                        pagetable_get_page(v->arch.guest_table);\n                    v->arch.guest_table = pagetable_null();\n                    break;\n                default:\n                    if ( cr3_page == current->arch.old_guest_table )\n                        cr3_page = NULL;\n                    break;\n                case 0:\n                    if ( VM_ASSIST(d, m2p_strict) )\n                        zap_ro_mpt(_mfn(cr3_gfn));\n                    break;\n                }\n            }\n            if ( !rc )\n               v->arch.guest_table_user = pagetable_from_page(cr3_page);\n        }\n    }\n    else\n    {\n        l4_pgentry_t *l4tab;\n\n        l4tab = map_domain_page(pagetable_get_mfn(v->arch.guest_table));\n        *l4tab = l4e_from_pfn(page_to_mfn(cr3_page),\n            _PAGE_PRESENT|_PAGE_RW|_PAGE_USER|_PAGE_ACCESSED);\n        unmap_domain_page(l4tab);\n    }\n    if ( rc )\n    {\n        if ( cr3_page )\n            put_page(cr3_page);\n        pv_destroy_gdt(v);\n        return rc;\n    }\n\n    clear_bit(_VPF_in_reset, &v->pause_flags);\n\n    if ( v->vcpu_id == 0 )\n        update_domain_wallclock_time(d);\n\n    /* Don't redo final setup */\n    v->is_initialised = 1;\n\n    if ( paging_mode_enabled(d) )\n        paging_update_paging_modes(v);\n\n    update_cr3(v);\n\n out:\n    if ( flags & VGCF_online )\n        clear_bit(_VPF_down, &v->pause_flags);\n    else\n        set_bit(_VPF_down, &v->pause_flags);\n    return 0;\n#undef c\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -333,6 +333,7 @@\n                     rc = -ERESTART;\n                     /* Fallthrough */\n                 case -ERESTART:\n+                    v->arch.old_guest_ptpg = NULL;\n                     v->arch.old_guest_table =\n                         pagetable_get_page(v->arch.guest_table);\n                     v->arch.guest_table = pagetable_null();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                    v->arch.old_guest_ptpg = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/alloc_l4_table",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int alloc_l4_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l4_pgentry_t  *pl4e = map_domain_page(_mfn(pfn));\n    unsigned int   i;\n    int            rc = 0, partial = page->partial_pte;\n\n    for ( i = page->nr_validated_ptes; i < L4_PAGETABLE_ENTRIES;\n          i++, partial = 0 )\n    {\n        if ( !is_guest_l4_slot(d, i) ||\n             (rc = get_page_from_l4e(pl4e[i], pfn, d, partial)) > 0 )\n            continue;\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = partial ?: 1;\n        }\n        else if ( rc < 0 )\n        {\n            if ( rc != -EINTR )\n                gdprintk(XENLOG_WARNING,\n                         \"Failure in alloc_l4_table: slot %#x\\n\", i);\n            if ( i )\n            {\n                page->nr_validated_ptes = i;\n                page->partial_pte = 0;\n                if ( rc == -EINTR )\n                    rc = -ERESTART;\n                else\n                {\n                    if ( current->arch.old_guest_table )\n                        page->nr_validated_ptes++;\n                    current->arch.old_guest_table = page;\n                }\n            }\n        }\n        if ( rc < 0 )\n        {\n            unmap_domain_page(pl4e);\n            return rc;\n        }\n\n        pl4e[i] = adjust_guest_l4e(pl4e[i], d);\n    }\n\n    if ( rc >= 0 )\n    {\n        init_guest_l4_table(pl4e, d, !VM_ASSIST(d, m2p_strict));\n        atomic_inc(&d->arch.pv_domain.nr_l4_pages);\n        rc = 0;\n    }\n    unmap_domain_page(pl4e);\n\n    return rc;\n}",
        "func": "static int alloc_l4_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l4_pgentry_t  *pl4e = map_domain_page(_mfn(pfn));\n    unsigned int   i;\n    int            rc = 0, partial = page->partial_pte;\n\n    for ( i = page->nr_validated_ptes; i < L4_PAGETABLE_ENTRIES;\n          i++, partial = 0 )\n    {\n        if ( !is_guest_l4_slot(d, i) ||\n             (rc = get_page_from_l4e(pl4e[i], pfn, d, partial)) > 0 )\n            continue;\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = partial ?: 1;\n        }\n        else if ( rc < 0 )\n        {\n            if ( rc != -EINTR )\n                gdprintk(XENLOG_WARNING,\n                         \"Failure in alloc_l4_table: slot %#x\\n\", i);\n            if ( i )\n            {\n                page->nr_validated_ptes = i;\n                page->partial_pte = 0;\n                if ( rc == -EINTR )\n                    rc = -ERESTART;\n                else\n                {\n                    if ( current->arch.old_guest_table )\n                        page->nr_validated_ptes++;\n                    current->arch.old_guest_ptpg = NULL;\n                    current->arch.old_guest_table = page;\n                }\n            }\n        }\n        if ( rc < 0 )\n        {\n            unmap_domain_page(pl4e);\n            return rc;\n        }\n\n        pl4e[i] = adjust_guest_l4e(pl4e[i], d);\n    }\n\n    if ( rc >= 0 )\n    {\n        init_guest_l4_table(pl4e, d, !VM_ASSIST(d, m2p_strict));\n        atomic_inc(&d->arch.pv_domain.nr_l4_pages);\n        rc = 0;\n    }\n    unmap_domain_page(pl4e);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,6 +33,7 @@\n                 {\n                     if ( current->arch.old_guest_table )\n                         page->nr_validated_ptes++;\n+                    current->arch.old_guest_ptpg = NULL;\n                     current->arch.old_guest_table = page;\n                 }\n             }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                    current->arch.old_guest_ptpg = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/alloc_l3_table",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int alloc_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t  *pl3e;\n    unsigned int   i;\n    int            rc = 0, partial = page->partial_pte;\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    /*\n     * PAE guests allocate full pages, but aren't required to initialize\n     * more than the first four entries; when running in compatibility\n     * mode, however, the full page is visible to the MMU, and hence all\n     * 512 entries must be valid/verified, which is most easily achieved\n     * by clearing them out.\n     */\n    if ( is_pv_32bit_domain(d) )\n        memset(pl3e + 4, 0, (L3_PAGETABLE_ENTRIES - 4) * sizeof(*pl3e));\n\n    for ( i = page->nr_validated_ptes; i < L3_PAGETABLE_ENTRIES;\n          i++, partial = 0 )\n    {\n        if ( is_pv_32bit_domain(d) && (i == 3) )\n        {\n            if ( !(l3e_get_flags(pl3e[i]) & _PAGE_PRESENT) ||\n                 (l3e_get_flags(pl3e[i]) & l3_disallow_mask(d)) )\n                rc = -EINVAL;\n            else\n                rc = get_page_and_type_from_mfn(\n                    l3e_get_mfn(pl3e[i]),\n                    PGT_l2_page_table | PGT_pae_xen_l2, d, partial, 1);\n        }\n        else if ( (rc = get_page_from_l3e(pl3e[i], pfn, d, partial)) > 0 )\n            continue;\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = partial ?: 1;\n        }\n        else if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = 0;\n            rc = -ERESTART;\n        }\n        if ( rc < 0 )\n            break;\n\n        pl3e[i] = adjust_guest_l3e(pl3e[i], d);\n    }\n\n    if ( rc >= 0 && !create_pae_xen_mappings(d, pl3e) )\n        rc = -EINVAL;\n    if ( rc < 0 && rc != -ERESTART && rc != -EINTR )\n    {\n        gdprintk(XENLOG_WARNING, \"Failure in alloc_l3_table: slot %#x\\n\", i);\n        if ( i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = 0;\n            current->arch.old_guest_table = page;\n        }\n        while ( i-- > 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n    }\n\n    unmap_domain_page(pl3e);\n    return rc > 0 ? 0 : rc;\n}",
        "func": "static int alloc_l3_table(struct page_info *page)\n{\n    struct domain *d = page_get_owner(page);\n    unsigned long  pfn = mfn_x(page_to_mfn(page));\n    l3_pgentry_t  *pl3e;\n    unsigned int   i;\n    int            rc = 0, partial = page->partial_pte;\n\n    pl3e = map_domain_page(_mfn(pfn));\n\n    /*\n     * PAE guests allocate full pages, but aren't required to initialize\n     * more than the first four entries; when running in compatibility\n     * mode, however, the full page is visible to the MMU, and hence all\n     * 512 entries must be valid/verified, which is most easily achieved\n     * by clearing them out.\n     */\n    if ( is_pv_32bit_domain(d) )\n        memset(pl3e + 4, 0, (L3_PAGETABLE_ENTRIES - 4) * sizeof(*pl3e));\n\n    for ( i = page->nr_validated_ptes; i < L3_PAGETABLE_ENTRIES;\n          i++, partial = 0 )\n    {\n        if ( is_pv_32bit_domain(d) && (i == 3) )\n        {\n            if ( !(l3e_get_flags(pl3e[i]) & _PAGE_PRESENT) ||\n                 (l3e_get_flags(pl3e[i]) & l3_disallow_mask(d)) )\n                rc = -EINVAL;\n            else\n                rc = get_page_and_type_from_mfn(\n                    l3e_get_mfn(pl3e[i]),\n                    PGT_l2_page_table | PGT_pae_xen_l2, d, partial, 1);\n        }\n        else if ( (rc = get_page_from_l3e(pl3e[i], pfn, d, partial)) > 0 )\n            continue;\n\n        if ( rc == -ERESTART )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = partial ?: 1;\n        }\n        else if ( rc == -EINTR && i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = 0;\n            rc = -ERESTART;\n        }\n        if ( rc < 0 )\n            break;\n\n        pl3e[i] = adjust_guest_l3e(pl3e[i], d);\n    }\n\n    if ( rc >= 0 && !create_pae_xen_mappings(d, pl3e) )\n        rc = -EINVAL;\n    if ( rc < 0 && rc != -ERESTART && rc != -EINTR )\n    {\n        gdprintk(XENLOG_WARNING, \"Failure in alloc_l3_table: slot %#x\\n\", i);\n        if ( i )\n        {\n            page->nr_validated_ptes = i;\n            page->partial_pte = 0;\n            current->arch.old_guest_ptpg = NULL;\n            current->arch.old_guest_table = page;\n        }\n        while ( i-- > 0 )\n            pl3e[i] = unadjust_guest_l3e(pl3e[i], d);\n    }\n\n    unmap_domain_page(pl3e);\n    return rc > 0 ? 0 : rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -60,6 +60,7 @@\n         {\n             page->nr_validated_ptes = i;\n             page->partial_pte = 0;\n+            current->arch.old_guest_ptpg = NULL;\n             current->arch.old_guest_table = page;\n         }\n         while ( i-- > 0 )",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            current->arch.old_guest_ptpg = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_page_from_l4e",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int put_page_from_l4e(l4_pgentry_t l4e, unsigned long pfn,\n                             int partial, bool defer)\n{\n    if ( (l4e_get_flags(l4e) & _PAGE_PRESENT) &&\n         (l4e_get_pfn(l4e) != pfn) )\n    {\n        struct page_info *pg = l4e_get_page(l4e);\n\n        if ( unlikely(partial > 0) )\n        {\n            ASSERT(!defer);\n            return put_page_type_preemptible(pg);\n        }\n\n        if ( defer )\n        {\n            current->arch.old_guest_table = pg;\n            return 0;\n        }\n\n        return put_page_and_type_preemptible(pg);\n    }\n    return 1;\n}",
        "func": "static int put_page_from_l4e(l4_pgentry_t l4e, unsigned long pfn,\n                             int partial, bool defer)\n{\n    int rc = 1;\n\n    if ( (l4e_get_flags(l4e) & _PAGE_PRESENT) &&\n         (l4e_get_pfn(l4e) != pfn) )\n    {\n        struct page_info *pg = l4e_get_page(l4e);\n\n        if ( unlikely(partial > 0) )\n        {\n            ASSERT(!defer);\n            return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n        }\n\n        if ( defer )\n        {\n            current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));\n            current->arch.old_guest_table = pg;\n            return 0;\n        }\n\n        rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n        if ( likely(!rc) )\n            put_page(pg);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,8 @@\n static int put_page_from_l4e(l4_pgentry_t l4e, unsigned long pfn,\n                              int partial, bool defer)\n {\n+    int rc = 1;\n+\n     if ( (l4e_get_flags(l4e) & _PAGE_PRESENT) &&\n          (l4e_get_pfn(l4e) != pfn) )\n     {\n@@ -9,16 +11,20 @@\n         if ( unlikely(partial > 0) )\n         {\n             ASSERT(!defer);\n-            return put_page_type_preemptible(pg);\n+            return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n         }\n \n         if ( defer )\n         {\n+            current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));\n             current->arch.old_guest_table = pg;\n             return 0;\n         }\n \n-        return put_page_and_type_preemptible(pg);\n+        rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n+        if ( likely(!rc) )\n+            put_page(pg);\n     }\n-    return 1;\n+\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "            return put_page_type_preemptible(pg);",
                "        return put_page_and_type_preemptible(pg);",
                "    return 1;"
            ],
            "added_lines": [
                "    int rc = 1;",
                "",
                "            return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));",
                "            current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));",
                "        rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));",
                "        if ( likely(!rc) )",
                "            put_page(pg);",
                "",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_page_type_preemptible",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "int put_page_type_preemptible(struct page_info *page)\n{\n    return __put_page_type(page, 1);\n}",
        "func": "int put_page_type_preemptible(struct page_info *page)\n{\n    return _put_page_type(page, true, NULL);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n int put_page_type_preemptible(struct page_info *page)\n {\n-    return __put_page_type(page, 1);\n+    return _put_page_type(page, true, NULL);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    return __put_page_type(page, 1);"
            ],
            "added_lines": [
                "    return _put_page_type(page, true, NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/__get_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int __get_page_type(struct page_info *page, unsigned long type,\n                           int preemptible)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0, iommu_ret = 0;\n\n    ASSERT(!(type & ~(PGT_type_mask | PGT_pae_xen_l2)));\n    ASSERT(!in_irq());\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x + 1;\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Type count overflow on mfn %\"PRI_mfn\"\\n\",\n                     mfn_x(page_to_mfn(page)));\n            return -EINVAL;\n        }\n        else if ( unlikely((x & PGT_count_mask) == 0) )\n        {\n            struct domain *d = page_get_owner(page);\n\n            /*\n             * Normally we should never let a page go from type count 0\n             * to type count 1 when it is shadowed. One exception:\n             * out-of-sync shadowed pages are allowed to become\n             * writeable.\n             */\n            if ( d && shadow_mode_enabled(d)\n                 && (page->count_info & PGC_page_table)\n                 && !((page->shadow_flags & (1u<<29))\n                      && type == PGT_writable_page) )\n               shadow_remove_all_shadows(d, page_to_mfn(page));\n\n            ASSERT(!(x & PGT_pae_xen_l2));\n            if ( (x & PGT_type_mask) != type )\n            {\n                /*\n                 * On type change we check to flush stale TLB entries. This\n                 * may be unnecessary (e.g., page was GDT/LDT) but those\n                 * circumstances should be very rare.\n                 */\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                BUG_ON(in_irq());\n                cpumask_copy(mask, d->domain_dirty_cpumask);\n\n                /* Don't flush if the timestamp is old enough */\n                tlbflush_filter(mask, page->tlbflush_timestamp);\n\n                if ( unlikely(!cpumask_empty(mask)) &&\n                     /* Shadow mode: track only writable pages. */\n                     (!shadow_mode_enabled(page_get_owner(page)) ||\n                      ((nx & PGT_type_mask) == PGT_writable_page)) )\n                {\n                    perfc_incr(need_flush_tlb_flush);\n                    flush_tlb_mask(mask);\n                }\n\n                /* We lose existing type and validity. */\n                nx &= ~(PGT_type_mask | PGT_validated);\n                nx |= type;\n\n                /*\n                 * No special validation needed for writable pages.\n                 * Page tables and GDT/LDT need to be scanned for validity.\n                 */\n                if ( type == PGT_writable_page || type == PGT_shared_page )\n                    nx |= PGT_validated;\n            }\n        }\n        else if ( unlikely((x & (PGT_type_mask|PGT_pae_xen_l2)) != type) )\n        {\n            /* Don't log failure if it could be a recursive-mapping attempt. */\n            if ( ((x & PGT_type_mask) == PGT_l2_page_table) &&\n                 (type == PGT_l1_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l3_page_table) &&\n                 (type == PGT_l2_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l4_page_table) &&\n                 (type == PGT_l3_page_table) )\n                return -EINVAL;\n            gdprintk(XENLOG_WARNING,\n                     \"Bad type (saw %\" PRtype_info \" != exp %\" PRtype_info \") \"\n                     \"for mfn %\" PRI_mfn \" (pfn %\" PRI_pfn \")\\n\",\n                     x, type, mfn_x(page_to_mfn(page)),\n                     get_gpfn_from_mfn(mfn_x(page_to_mfn(page))));\n            return -EINVAL;\n        }\n        else if ( unlikely(!(x & PGT_validated)) )\n        {\n            if ( !(x & PGT_partial) )\n            {\n                /* Someone else is updating validation of this page. Wait... */\n                while ( (y = page->u.inuse.type_info) == x )\n                {\n                    if ( preemptible && hypercall_preempt_check() )\n                        return -EINTR;\n                    cpu_relax();\n                }\n                continue;\n            }\n            /* Type ref count was left at 1 when PGT_partial got set. */\n            ASSERT((x & PGT_count_mask) == 1);\n            nx = x & ~PGT_partial;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( unlikely((x & PGT_type_mask) != type) )\n    {\n        /* Special pages should not be accessible from devices. */\n        struct domain *d = page_get_owner(page);\n        if ( d && is_pv_domain(d) && unlikely(need_iommu(d)) )\n        {\n            gfn_t gfn = _gfn(mfn_to_gmfn(d, mfn_x(page_to_mfn(page))));\n\n            if ( (x & PGT_type_mask) == PGT_writable_page )\n                iommu_ret = iommu_unmap_page(d, gfn_x(gfn));\n            else if ( type == PGT_writable_page )\n                iommu_ret = iommu_map_page(d, gfn_x(gfn),\n                                           mfn_x(page_to_mfn(page)),\n                                           IOMMUF_readable|IOMMUF_writable);\n        }\n    }\n\n    if ( unlikely(!(nx & PGT_validated)) )\n    {\n        if ( !(x & PGT_partial) )\n        {\n            page->nr_validated_ptes = 0;\n            page->partial_pte = 0;\n        }\n        rc = alloc_page_type(page, type, preemptible);\n    }\n\n    if ( (x & PGT_partial) && !(nx & PGT_partial) )\n        put_page(page);\n\n    if ( !rc )\n        rc = iommu_ret;\n\n    return rc;\n}",
        "func": "static int __get_page_type(struct page_info *page, unsigned long type,\n                           int preemptible)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0, iommu_ret = 0;\n\n    ASSERT(!(type & ~(PGT_type_mask | PGT_pae_xen_l2)));\n    ASSERT(!in_irq());\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x + 1;\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Type count overflow on mfn %\"PRI_mfn\"\\n\",\n                     mfn_x(page_to_mfn(page)));\n            return -EINVAL;\n        }\n        else if ( unlikely((x & PGT_count_mask) == 0) )\n        {\n            struct domain *d = page_get_owner(page);\n\n            /*\n             * Normally we should never let a page go from type count 0\n             * to type count 1 when it is shadowed. One exception:\n             * out-of-sync shadowed pages are allowed to become\n             * writeable.\n             */\n            if ( d && shadow_mode_enabled(d)\n                 && (page->count_info & PGC_page_table)\n                 && !((page->shadow_flags & (1u<<29))\n                      && type == PGT_writable_page) )\n               shadow_remove_all_shadows(d, page_to_mfn(page));\n\n            ASSERT(!(x & PGT_pae_xen_l2));\n            if ( (x & PGT_type_mask) != type )\n            {\n                /*\n                 * On type change we check to flush stale TLB entries. This\n                 * may be unnecessary (e.g., page was GDT/LDT) but those\n                 * circumstances should be very rare.\n                 */\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                BUG_ON(in_irq());\n                cpumask_copy(mask, d->domain_dirty_cpumask);\n\n                /* Don't flush if the timestamp is old enough */\n                tlbflush_filter(mask, page->tlbflush_timestamp);\n\n                if ( unlikely(!cpumask_empty(mask)) &&\n                     /* Shadow mode: track only writable pages. */\n                     (!shadow_mode_enabled(page_get_owner(page)) ||\n                      ((nx & PGT_type_mask) == PGT_writable_page)) )\n                {\n                    perfc_incr(need_flush_tlb_flush);\n                    flush_tlb_mask(mask);\n                }\n\n                /* We lose existing type and validity. */\n                nx &= ~(PGT_type_mask | PGT_validated);\n                nx |= type;\n\n                /*\n                 * No special validation needed for writable pages.\n                 * Page tables and GDT/LDT need to be scanned for validity.\n                 */\n                if ( type == PGT_writable_page || type == PGT_shared_page )\n                    nx |= PGT_validated;\n            }\n        }\n        else if ( unlikely((x & (PGT_type_mask|PGT_pae_xen_l2)) != type) )\n        {\n            /* Don't log failure if it could be a recursive-mapping attempt. */\n            if ( ((x & PGT_type_mask) == PGT_l2_page_table) &&\n                 (type == PGT_l1_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l3_page_table) &&\n                 (type == PGT_l2_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l4_page_table) &&\n                 (type == PGT_l3_page_table) )\n                return -EINVAL;\n            gdprintk(XENLOG_WARNING,\n                     \"Bad type (saw %\" PRtype_info \" != exp %\" PRtype_info \") \"\n                     \"for mfn %\" PRI_mfn \" (pfn %\" PRI_pfn \")\\n\",\n                     x, type, mfn_x(page_to_mfn(page)),\n                     get_gpfn_from_mfn(mfn_x(page_to_mfn(page))));\n            return -EINVAL;\n        }\n        else if ( unlikely(!(x & PGT_validated)) )\n        {\n            if ( !(x & PGT_partial) )\n            {\n                /* Someone else is updating validation of this page. Wait... */\n                while ( (y = page->u.inuse.type_info) == x )\n                {\n                    if ( preemptible && hypercall_preempt_check() )\n                        return -EINTR;\n                    cpu_relax();\n                }\n                continue;\n            }\n            /* Type ref count was left at 1 when PGT_partial got set. */\n            ASSERT((x & PGT_count_mask) == 1);\n            nx = x & ~PGT_partial;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( unlikely((x & PGT_type_mask) != type) )\n    {\n        /* Special pages should not be accessible from devices. */\n        struct domain *d = page_get_owner(page);\n        if ( d && is_pv_domain(d) && unlikely(need_iommu(d)) )\n        {\n            gfn_t gfn = _gfn(mfn_to_gmfn(d, mfn_x(page_to_mfn(page))));\n\n            if ( (x & PGT_type_mask) == PGT_writable_page )\n                iommu_ret = iommu_unmap_page(d, gfn_x(gfn));\n            else if ( type == PGT_writable_page )\n                iommu_ret = iommu_map_page(d, gfn_x(gfn),\n                                           mfn_x(page_to_mfn(page)),\n                                           IOMMUF_readable|IOMMUF_writable);\n        }\n    }\n\n    if ( unlikely(!(nx & PGT_validated)) )\n    {\n        if ( !(x & PGT_partial) )\n        {\n            page->nr_validated_ptes = 0;\n            page->partial_pte = 0;\n        }\n        page->linear_pt_count = 0;\n        rc = alloc_page_type(page, type, preemptible);\n    }\n\n    if ( (x & PGT_partial) && !(nx & PGT_partial) )\n        put_page(page);\n\n    if ( !rc )\n        rc = iommu_ret;\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -139,6 +139,7 @@\n             page->nr_validated_ptes = 0;\n             page->partial_pte = 0;\n         }\n+        page->linear_pt_count = 0;\n         rc = alloc_page_type(page, type, preemptible);\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        page->linear_pt_count = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_page_from_l3e",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int put_page_from_l3e(l3_pgentry_t l3e, unsigned long pfn,\n                             int partial, bool defer)\n{\n    struct page_info *pg;\n\n    if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) || (l3e_get_pfn(l3e) == pfn) )\n        return 1;\n\n    if ( unlikely(l3e_get_flags(l3e) & _PAGE_PSE) )\n    {\n        unsigned long mfn = l3e_get_pfn(l3e);\n        int writeable = l3e_get_flags(l3e) & _PAGE_RW;\n\n        ASSERT(!(mfn & ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)));\n        do {\n            put_data_page(mfn_to_page(_mfn(mfn)), writeable);\n        } while ( ++mfn & ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1) );\n\n        return 0;\n    }\n\n    pg = l3e_get_page(l3e);\n\n    if ( unlikely(partial > 0) )\n    {\n        ASSERT(!defer);\n        return put_page_type_preemptible(pg);\n    }\n\n    if ( defer )\n    {\n        current->arch.old_guest_table = pg;\n        return 0;\n    }\n\n    return put_page_and_type_preemptible(pg);\n}",
        "func": "static int put_page_from_l3e(l3_pgentry_t l3e, unsigned long pfn,\n                             int partial, bool defer)\n{\n    struct page_info *pg;\n    int rc;\n\n    if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) || (l3e_get_pfn(l3e) == pfn) )\n        return 1;\n\n    if ( unlikely(l3e_get_flags(l3e) & _PAGE_PSE) )\n    {\n        unsigned long mfn = l3e_get_pfn(l3e);\n        int writeable = l3e_get_flags(l3e) & _PAGE_RW;\n\n        ASSERT(!(mfn & ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1)));\n        do {\n            put_data_page(mfn_to_page(_mfn(mfn)), writeable);\n        } while ( ++mfn & ((1UL << (L3_PAGETABLE_SHIFT - PAGE_SHIFT)) - 1) );\n\n        return 0;\n    }\n\n    pg = l3e_get_page(l3e);\n\n    if ( unlikely(partial > 0) )\n    {\n        ASSERT(!defer);\n        return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n    }\n\n    if ( defer )\n    {\n        current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));\n        current->arch.old_guest_table = pg;\n        return 0;\n    }\n\n    rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n    if ( likely(!rc) )\n        put_page(pg);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n                              int partial, bool defer)\n {\n     struct page_info *pg;\n+    int rc;\n \n     if ( !(l3e_get_flags(l3e) & _PAGE_PRESENT) || (l3e_get_pfn(l3e) == pfn) )\n         return 1;\n@@ -24,14 +25,19 @@\n     if ( unlikely(partial > 0) )\n     {\n         ASSERT(!defer);\n-        return put_page_type_preemptible(pg);\n+        return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n     }\n \n     if ( defer )\n     {\n+        current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));\n         current->arch.old_guest_table = pg;\n         return 0;\n     }\n \n-    return put_page_and_type_preemptible(pg);\n+    rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));\n+    if ( likely(!rc) )\n+        put_page(pg);\n+\n+    return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        return put_page_type_preemptible(pg);",
                "    return put_page_and_type_preemptible(pg);"
            ],
            "added_lines": [
                "    int rc;",
                "        return _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));",
                "        current->arch.old_guest_ptpg = mfn_to_page(_mfn(pfn));",
                "    rc = _put_page_type(pg, true, mfn_to_page(_mfn(pfn)));",
                "    if ( likely(!rc) )",
                "        put_page(pg);",
                "",
                "    return rc;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_page_from_l2e",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "static int put_page_from_l2e(l2_pgentry_t l2e, unsigned long pfn)\n{\n    if ( !(l2e_get_flags(l2e) & _PAGE_PRESENT) || (l2e_get_pfn(l2e) == pfn) )\n        return 1;\n\n    if ( l2e_get_flags(l2e) & _PAGE_PSE )\n    {\n        struct page_info *page = l2e_get_page(l2e);\n        unsigned int i;\n\n        for ( i = 0; i < (1u << PAGETABLE_ORDER); i++, page++ )\n            put_page_and_type(page);\n    } else\n        put_page_and_type(l2e_get_page(l2e));\n\n    return 0;\n}",
        "func": "static int put_page_from_l2e(l2_pgentry_t l2e, unsigned long pfn)\n{\n    if ( !(l2e_get_flags(l2e) & _PAGE_PRESENT) || (l2e_get_pfn(l2e) == pfn) )\n        return 1;\n\n    if ( l2e_get_flags(l2e) & _PAGE_PSE )\n    {\n        struct page_info *page = l2e_get_page(l2e);\n        unsigned int i;\n\n        for ( i = 0; i < (1u << PAGETABLE_ORDER); i++, page++ )\n            put_page_and_type(page);\n    }\n    else\n    {\n        struct page_info *pg = l2e_get_page(l2e);\n        int rc = _put_page_type(pg, false, mfn_to_page(_mfn(pfn)));\n\n        ASSERT(!rc);\n        put_page(pg);\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,15 @@\n \n         for ( i = 0; i < (1u << PAGETABLE_ORDER); i++, page++ )\n             put_page_and_type(page);\n-    } else\n-        put_page_and_type(l2e_get_page(l2e));\n+    }\n+    else\n+    {\n+        struct page_info *pg = l2e_get_page(l2e);\n+        int rc = _put_page_type(pg, false, mfn_to_page(_mfn(pfn)));\n+\n+        ASSERT(!rc);\n+        put_page(pg);\n+    }\n \n     return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    } else",
                "        put_page_and_type(l2e_get_page(l2e));"
            ],
            "added_lines": [
                "    }",
                "    else",
                "    {",
                "        struct page_info *pg = l2e_get_page(l2e);",
                "        int rc = _put_page_type(pg, false, mfn_to_page(_mfn(pfn)));",
                "",
                "        ASSERT(!rc);",
                "        put_page(pg);",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_old_guest_table",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "int put_old_guest_table(struct vcpu *v)\n{\n    int rc;\n\n    if ( !v->arch.old_guest_table )\n        return 0;\n\n    switch ( rc = put_page_and_type_preemptible(v->arch.old_guest_table) )\n    {\n    case -EINTR:\n    case -ERESTART:\n        return -ERESTART;\n    }\n\n    v->arch.old_guest_table = NULL;\n\n    return rc;\n}",
        "func": "int put_old_guest_table(struct vcpu *v)\n{\n    int rc;\n\n    if ( !v->arch.old_guest_table )\n        return 0;\n\n    switch ( rc = _put_page_type(v->arch.old_guest_table, true,\n                                 v->arch.old_guest_ptpg) )\n    {\n    case -EINTR:\n    case -ERESTART:\n        return -ERESTART;\n    case 0:\n        put_page(v->arch.old_guest_table);\n    }\n\n    v->arch.old_guest_table = NULL;\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,11 +5,14 @@\n     if ( !v->arch.old_guest_table )\n         return 0;\n \n-    switch ( rc = put_page_and_type_preemptible(v->arch.old_guest_table) )\n+    switch ( rc = _put_page_type(v->arch.old_guest_table, true,\n+                                 v->arch.old_guest_ptpg) )\n     {\n     case -EINTR:\n     case -ERESTART:\n         return -ERESTART;\n+    case 0:\n+        put_page(v->arch.old_guest_table);\n     }\n \n     v->arch.old_guest_table = NULL;",
        "diff_line_info": {
            "deleted_lines": [
                "    switch ( rc = put_page_and_type_preemptible(v->arch.old_guest_table) )"
            ],
            "added_lines": [
                "    switch ( rc = _put_page_type(v->arch.old_guest_table, true,",
                "                                 v->arch.old_guest_ptpg) )",
                "    case 0:",
                "        put_page(v->arch.old_guest_table);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/put_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "void put_page_type(struct page_info *page)\n{\n    int rc = __put_page_type(page, 0);\n    ASSERT(rc == 0);\n    (void)rc;\n}",
        "func": "void put_page_type(struct page_info *page)\n{\n    int rc = _put_page_type(page, false, NULL);\n    ASSERT(rc == 0);\n    (void)rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n void put_page_type(struct page_info *page)\n {\n-    int rc = __put_page_type(page, 0);\n+    int rc = _put_page_type(page, false, NULL);\n     ASSERT(rc == 0);\n     (void)rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    int rc = __put_page_type(page, 0);"
            ],
            "added_lines": [
                "    int rc = _put_page_type(page, false, NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/do_mmuext_op",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "long do_mmuext_op(\n    XEN_GUEST_HANDLE_PARAM(mmuext_op_t) uops,\n    unsigned int count,\n    XEN_GUEST_HANDLE_PARAM(uint) pdone,\n    unsigned int foreigndom)\n{\n    struct mmuext_op op;\n    unsigned long type;\n    unsigned int i, done = 0;\n    struct vcpu *curr = current;\n    struct domain *currd = curr->domain;\n    struct domain *pg_owner;\n    int rc = put_old_guest_table(curr);\n\n    if ( unlikely(rc) )\n    {\n        if ( likely(rc == -ERESTART) )\n            rc = hypercall_create_continuation(\n                     __HYPERVISOR_mmuext_op, \"hihi\", uops, count, pdone,\n                     foreigndom);\n        return rc;\n    }\n\n    if ( unlikely(count == MMU_UPDATE_PREEMPTED) &&\n         likely(guest_handle_is_null(uops)) )\n    {\n        /*\n         * See the curr->arch.old_guest_table related\n         * hypercall_create_continuation() below.\n         */\n        return (int)foreigndom;\n    }\n\n    if ( unlikely(count & MMU_UPDATE_PREEMPTED) )\n    {\n        count &= ~MMU_UPDATE_PREEMPTED;\n        if ( unlikely(!guest_handle_is_null(pdone)) )\n            (void)copy_from_guest(&done, pdone, 1);\n    }\n    else\n        perfc_incr(calls_to_mmuext_op);\n\n    if ( unlikely(!guest_handle_okay(uops, count)) )\n        return -EFAULT;\n\n    if ( (pg_owner = get_pg_owner(foreigndom)) == NULL )\n        return -ESRCH;\n\n    if ( !is_pv_domain(pg_owner) )\n    {\n        put_pg_owner(pg_owner);\n        return -EINVAL;\n    }\n\n    rc = xsm_mmuext_op(XSM_TARGET, currd, pg_owner);\n    if ( rc )\n    {\n        put_pg_owner(pg_owner);\n        return rc;\n    }\n\n    for ( i = 0; i < count; i++ )\n    {\n        if ( curr->arch.old_guest_table || (i && hypercall_preempt_check()) )\n        {\n            rc = -ERESTART;\n            break;\n        }\n\n        if ( unlikely(__copy_from_guest(&op, uops, 1) != 0) )\n        {\n            rc = -EFAULT;\n            break;\n        }\n\n        if ( is_hvm_domain(currd) )\n        {\n            switch ( op.cmd )\n            {\n            case MMUEXT_PIN_L1_TABLE:\n            case MMUEXT_PIN_L2_TABLE:\n            case MMUEXT_PIN_L3_TABLE:\n            case MMUEXT_PIN_L4_TABLE:\n            case MMUEXT_UNPIN_TABLE:\n                break;\n            default:\n                rc = -EOPNOTSUPP;\n                goto done;\n            }\n        }\n\n        rc = 0;\n\n        switch ( op.cmd )\n        {\n            struct page_info *page;\n            p2m_type_t p2mt;\n\n        case MMUEXT_PIN_L1_TABLE:\n            type = PGT_l1_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L2_TABLE:\n            type = PGT_l2_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L3_TABLE:\n            type = PGT_l3_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L4_TABLE:\n            if ( is_pv_32bit_domain(pg_owner) )\n                break;\n            type = PGT_l4_page_table;\n\n        pin_page:\n            /* Ignore pinning of invalid paging levels. */\n            if ( (op.cmd - MMUEXT_PIN_L1_TABLE) > (CONFIG_PAGING_LEVELS - 1) )\n                break;\n\n            if ( paging_mode_refcounts(pg_owner) )\n                break;\n\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, NULL, P2M_ALLOC);\n            if ( unlikely(!page) )\n            {\n                rc = -EINVAL;\n                break;\n            }\n\n            rc = get_page_type_preemptible(page, type);\n            if ( unlikely(rc) )\n            {\n                if ( rc == -EINTR )\n                    rc = -ERESTART;\n                else if ( rc != -ERESTART )\n                    gdprintk(XENLOG_WARNING,\n                             \"Error %d while pinning mfn %\" PRI_mfn \"\\n\",\n                             rc, mfn_x(page_to_mfn(page)));\n                if ( page != curr->arch.old_guest_table )\n                    put_page(page);\n                break;\n            }\n\n            rc = xsm_memory_pin_page(XSM_HOOK, currd, pg_owner, page);\n            if ( !rc && unlikely(test_and_set_bit(_PGT_pinned,\n                                                  &page->u.inuse.type_info)) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" already pinned\\n\",\n                         mfn_x(page_to_mfn(page)));\n                rc = -EINVAL;\n            }\n\n            if ( unlikely(rc) )\n                goto pin_drop;\n\n            /* A page is dirtied when its pin status is set. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n\n            /* We can race domain destruction (domain_relinquish_resources). */\n            if ( unlikely(pg_owner != currd) )\n            {\n                bool drop_ref;\n\n                spin_lock(&pg_owner->page_alloc_lock);\n                drop_ref = (pg_owner->is_dying &&\n                            test_and_clear_bit(_PGT_pinned,\n                                               &page->u.inuse.type_info));\n                spin_unlock(&pg_owner->page_alloc_lock);\n                if ( drop_ref )\n                {\n        pin_drop:\n                    if ( type == PGT_l1_page_table )\n                        put_page_and_type(page);\n                    else\n                        curr->arch.old_guest_table = page;\n                }\n            }\n            break;\n\n        case MMUEXT_UNPIN_TABLE:\n            if ( paging_mode_refcounts(pg_owner) )\n                break;\n\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, NULL, P2M_ALLOC);\n            if ( unlikely(!page) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" bad, or bad owner d%d\\n\",\n                         op.arg1.mfn, pg_owner->domain_id);\n                rc = -EINVAL;\n                break;\n            }\n\n            if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n            {\n                put_page(page);\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" not pinned\\n\", op.arg1.mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            switch ( rc = put_page_and_type_preemptible(page) )\n            {\n            case -EINTR:\n            case -ERESTART:\n                curr->arch.old_guest_table = page;\n                rc = 0;\n                break;\n            default:\n                BUG_ON(rc);\n                break;\n            }\n            put_page(page);\n\n            /* A page is dirtied when its pin status is cleared. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n            break;\n\n        case MMUEXT_NEW_BASEPTR:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(paging_mode_translate(currd)) )\n                rc = -EINVAL;\n            else\n                rc = new_guest_cr3(_mfn(op.arg1.mfn));\n            break;\n\n        case MMUEXT_NEW_USER_BASEPTR: {\n            unsigned long old_mfn;\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(paging_mode_translate(currd)) )\n                rc = -EINVAL;\n            if ( unlikely(rc) )\n                break;\n\n            old_mfn = pagetable_get_pfn(curr->arch.guest_table_user);\n            /*\n             * This is particularly important when getting restarted after the\n             * previous attempt got preempted in the put-old-MFN phase.\n             */\n            if ( old_mfn == op.arg1.mfn )\n                break;\n\n            if ( op.arg1.mfn != 0 )\n            {\n                rc = get_page_and_type_from_mfn(\n                    _mfn(op.arg1.mfn), PGT_root_page_table, currd, 0, 1);\n\n                if ( unlikely(rc) )\n                {\n                    if ( rc == -EINTR )\n                        rc = -ERESTART;\n                    else if ( rc != -ERESTART )\n                        gdprintk(XENLOG_WARNING,\n                                 \"Error %d installing new mfn %\" PRI_mfn \"\\n\",\n                                 rc, op.arg1.mfn);\n                    break;\n                }\n\n                if ( VM_ASSIST(currd, m2p_strict) )\n                    zap_ro_mpt(_mfn(op.arg1.mfn));\n            }\n\n            curr->arch.guest_table_user = pagetable_from_pfn(op.arg1.mfn);\n\n            if ( old_mfn != 0 )\n            {\n                page = mfn_to_page(_mfn(old_mfn));\n\n                switch ( rc = put_page_and_type_preemptible(page) )\n                {\n                case -EINTR:\n                    rc = -ERESTART;\n                    /* fallthrough */\n                case -ERESTART:\n                    curr->arch.old_guest_table = page;\n                    break;\n                default:\n                    BUG_ON(rc);\n                    break;\n                }\n            }\n\n            break;\n        }\n\n        case MMUEXT_TLB_FLUSH_LOCAL:\n            if ( likely(currd == pg_owner) )\n                flush_tlb_local();\n            else\n                rc = -EPERM;\n            break;\n\n        case MMUEXT_INVLPG_LOCAL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else\n                paging_invlpg(curr, op.arg1.linear_addr);\n            break;\n\n        case MMUEXT_TLB_FLUSH_MULTI:\n        case MMUEXT_INVLPG_MULTI:\n        {\n            cpumask_t *mask = this_cpu(scratch_cpumask);\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(vcpumask_to_pcpumask(currd,\n                                   guest_handle_to_param(op.arg2.vcpumask,\n                                                         const_void),\n                                   mask)) )\n                rc = -EINVAL;\n            if ( unlikely(rc) )\n                break;\n\n            if ( op.cmd == MMUEXT_TLB_FLUSH_MULTI )\n                flush_tlb_mask(mask);\n            else if ( __addr_ok(op.arg1.linear_addr) )\n                flush_tlb_one_mask(mask, op.arg1.linear_addr);\n            break;\n        }\n\n        case MMUEXT_TLB_FLUSH_ALL:\n            if ( likely(currd == pg_owner) )\n                flush_tlb_mask(currd->domain_dirty_cpumask);\n            else\n                rc = -EPERM;\n            break;\n\n        case MMUEXT_INVLPG_ALL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( __addr_ok(op.arg1.linear_addr) )\n                flush_tlb_one_mask(currd->domain_dirty_cpumask,\n                                   op.arg1.linear_addr);\n            break;\n\n        case MMUEXT_FLUSH_CACHE:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(!cache_flush_permitted(currd)) )\n                rc = -EACCES;\n            else\n                wbinvd();\n            break;\n\n        case MMUEXT_FLUSH_CACHE_GLOBAL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( likely(cache_flush_permitted(currd)) )\n            {\n                unsigned int cpu;\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                cpumask_clear(mask);\n                for_each_online_cpu(cpu)\n                    if ( !cpumask_intersects(mask,\n                                             per_cpu(cpu_sibling_mask, cpu)) )\n                        __cpumask_set_cpu(cpu, mask);\n                flush_mask(mask, FLUSH_CACHE);\n            }\n            else\n                rc = -EINVAL;\n            break;\n\n        case MMUEXT_SET_LDT:\n        {\n            unsigned int ents = op.arg2.nr_ents;\n            unsigned long ptr = ents ? op.arg1.linear_addr : 0;\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( paging_mode_external(currd) )\n                rc = -EINVAL;\n            else if ( ((ptr & (PAGE_SIZE - 1)) != 0) || !__addr_ok(ptr) ||\n                      (ents > 8192) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Bad args to SET_LDT: ptr=%lx, ents=%x\\n\", ptr, ents);\n                rc = -EINVAL;\n            }\n            else if ( (curr->arch.pv_vcpu.ldt_ents != ents) ||\n                      (curr->arch.pv_vcpu.ldt_base != ptr) )\n            {\n                invalidate_shadow_ldt(curr, 0);\n                flush_tlb_local();\n                curr->arch.pv_vcpu.ldt_base = ptr;\n                curr->arch.pv_vcpu.ldt_ents = ents;\n                load_LDT(curr);\n            }\n            break;\n        }\n\n        case MMUEXT_CLEAR_PAGE:\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, &p2mt, P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && page )\n            {\n                put_page(page);\n                page = NULL;\n            }\n            if ( !page || !get_page_type(page, PGT_writable_page) )\n            {\n                if ( page )\n                    put_page(page);\n                gdprintk(XENLOG_WARNING,\n                         \"Error clearing mfn %\" PRI_mfn \"\\n\", op.arg1.mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            /* A page is dirtied when it's being cleared. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n\n            clear_domain_page(page_to_mfn(page));\n\n            put_page_and_type(page);\n            break;\n\n        case MMUEXT_COPY_PAGE:\n        {\n            struct page_info *src_page, *dst_page;\n\n            src_page = get_page_from_gfn(pg_owner, op.arg2.src_mfn, &p2mt,\n                                         P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && src_page )\n            {\n                put_page(src_page);\n                src_page = NULL;\n            }\n            if ( unlikely(!src_page) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Error copying from mfn %\" PRI_mfn \"\\n\",\n                         op.arg2.src_mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            dst_page = get_page_from_gfn(pg_owner, op.arg1.mfn, &p2mt,\n                                         P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && dst_page )\n            {\n                put_page(dst_page);\n                dst_page = NULL;\n            }\n            rc = (dst_page &&\n                  get_page_type(dst_page, PGT_writable_page)) ? 0 : -EINVAL;\n            if ( unlikely(rc) )\n            {\n                put_page(src_page);\n                if ( dst_page )\n                    put_page(dst_page);\n                gdprintk(XENLOG_WARNING,\n                         \"Error copying to mfn %\" PRI_mfn \"\\n\", op.arg1.mfn);\n                break;\n            }\n\n            /* A page is dirtied when it's being copied to. */\n            paging_mark_dirty(pg_owner, page_to_mfn(dst_page));\n\n            copy_domain_page(page_to_mfn(dst_page), page_to_mfn(src_page));\n\n            put_page_and_type(dst_page);\n            put_page(src_page);\n            break;\n        }\n\n        case MMUEXT_MARK_SUPER:\n        case MMUEXT_UNMARK_SUPER:\n            rc = -EOPNOTSUPP;\n            break;\n\n        default:\n            rc = -ENOSYS;\n            break;\n        }\n\n done:\n        if ( unlikely(rc) )\n            break;\n\n        guest_handle_add_offset(uops, 1);\n    }\n\n    if ( rc == -ERESTART )\n    {\n        ASSERT(i < count);\n        rc = hypercall_create_continuation(\n            __HYPERVISOR_mmuext_op, \"hihi\",\n            uops, (count - i) | MMU_UPDATE_PREEMPTED, pdone, foreigndom);\n    }\n    else if ( curr->arch.old_guest_table )\n    {\n        XEN_GUEST_HANDLE_PARAM(void) null;\n\n        ASSERT(rc || i == count);\n        set_xen_guest_handle(null, NULL);\n        /*\n         * In order to have a way to communicate the final return value to\n         * our continuation, we pass this in place of \"foreigndom\", building\n         * on the fact that this argument isn't needed anymore.\n         */\n        rc = hypercall_create_continuation(\n                __HYPERVISOR_mmuext_op, \"hihi\", null,\n                MMU_UPDATE_PREEMPTED, null, rc);\n    }\n\n    put_pg_owner(pg_owner);\n\n    perfc_add(num_mmuext_ops, i);\n\n    /* Add incremental work we have done to the @done output parameter. */\n    if ( unlikely(!guest_handle_is_null(pdone)) )\n    {\n        done += i;\n        copy_to_guest(pdone, &done, 1);\n    }\n\n    return rc;\n}",
        "func": "long do_mmuext_op(\n    XEN_GUEST_HANDLE_PARAM(mmuext_op_t) uops,\n    unsigned int count,\n    XEN_GUEST_HANDLE_PARAM(uint) pdone,\n    unsigned int foreigndom)\n{\n    struct mmuext_op op;\n    unsigned long type;\n    unsigned int i, done = 0;\n    struct vcpu *curr = current;\n    struct domain *currd = curr->domain;\n    struct domain *pg_owner;\n    int rc = put_old_guest_table(curr);\n\n    if ( unlikely(rc) )\n    {\n        if ( likely(rc == -ERESTART) )\n            rc = hypercall_create_continuation(\n                     __HYPERVISOR_mmuext_op, \"hihi\", uops, count, pdone,\n                     foreigndom);\n        return rc;\n    }\n\n    if ( unlikely(count == MMU_UPDATE_PREEMPTED) &&\n         likely(guest_handle_is_null(uops)) )\n    {\n        /*\n         * See the curr->arch.old_guest_table related\n         * hypercall_create_continuation() below.\n         */\n        return (int)foreigndom;\n    }\n\n    if ( unlikely(count & MMU_UPDATE_PREEMPTED) )\n    {\n        count &= ~MMU_UPDATE_PREEMPTED;\n        if ( unlikely(!guest_handle_is_null(pdone)) )\n            (void)copy_from_guest(&done, pdone, 1);\n    }\n    else\n        perfc_incr(calls_to_mmuext_op);\n\n    if ( unlikely(!guest_handle_okay(uops, count)) )\n        return -EFAULT;\n\n    if ( (pg_owner = get_pg_owner(foreigndom)) == NULL )\n        return -ESRCH;\n\n    if ( !is_pv_domain(pg_owner) )\n    {\n        put_pg_owner(pg_owner);\n        return -EINVAL;\n    }\n\n    rc = xsm_mmuext_op(XSM_TARGET, currd, pg_owner);\n    if ( rc )\n    {\n        put_pg_owner(pg_owner);\n        return rc;\n    }\n\n    for ( i = 0; i < count; i++ )\n    {\n        if ( curr->arch.old_guest_table || (i && hypercall_preempt_check()) )\n        {\n            rc = -ERESTART;\n            break;\n        }\n\n        if ( unlikely(__copy_from_guest(&op, uops, 1) != 0) )\n        {\n            rc = -EFAULT;\n            break;\n        }\n\n        if ( is_hvm_domain(currd) )\n        {\n            switch ( op.cmd )\n            {\n            case MMUEXT_PIN_L1_TABLE:\n            case MMUEXT_PIN_L2_TABLE:\n            case MMUEXT_PIN_L3_TABLE:\n            case MMUEXT_PIN_L4_TABLE:\n            case MMUEXT_UNPIN_TABLE:\n                break;\n            default:\n                rc = -EOPNOTSUPP;\n                goto done;\n            }\n        }\n\n        rc = 0;\n\n        switch ( op.cmd )\n        {\n            struct page_info *page;\n            p2m_type_t p2mt;\n\n        case MMUEXT_PIN_L1_TABLE:\n            type = PGT_l1_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L2_TABLE:\n            type = PGT_l2_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L3_TABLE:\n            type = PGT_l3_page_table;\n            goto pin_page;\n\n        case MMUEXT_PIN_L4_TABLE:\n            if ( is_pv_32bit_domain(pg_owner) )\n                break;\n            type = PGT_l4_page_table;\n\n        pin_page:\n            /* Ignore pinning of invalid paging levels. */\n            if ( (op.cmd - MMUEXT_PIN_L1_TABLE) > (CONFIG_PAGING_LEVELS - 1) )\n                break;\n\n            if ( paging_mode_refcounts(pg_owner) )\n                break;\n\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, NULL, P2M_ALLOC);\n            if ( unlikely(!page) )\n            {\n                rc = -EINVAL;\n                break;\n            }\n\n            rc = get_page_type_preemptible(page, type);\n            if ( unlikely(rc) )\n            {\n                if ( rc == -EINTR )\n                    rc = -ERESTART;\n                else if ( rc != -ERESTART )\n                    gdprintk(XENLOG_WARNING,\n                             \"Error %d while pinning mfn %\" PRI_mfn \"\\n\",\n                             rc, mfn_x(page_to_mfn(page)));\n                if ( page != curr->arch.old_guest_table )\n                    put_page(page);\n                break;\n            }\n\n            rc = xsm_memory_pin_page(XSM_HOOK, currd, pg_owner, page);\n            if ( !rc && unlikely(test_and_set_bit(_PGT_pinned,\n                                                  &page->u.inuse.type_info)) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" already pinned\\n\",\n                         mfn_x(page_to_mfn(page)));\n                rc = -EINVAL;\n            }\n\n            if ( unlikely(rc) )\n                goto pin_drop;\n\n            /* A page is dirtied when its pin status is set. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n\n            /* We can race domain destruction (domain_relinquish_resources). */\n            if ( unlikely(pg_owner != currd) )\n            {\n                bool drop_ref;\n\n                spin_lock(&pg_owner->page_alloc_lock);\n                drop_ref = (pg_owner->is_dying &&\n                            test_and_clear_bit(_PGT_pinned,\n                                               &page->u.inuse.type_info));\n                spin_unlock(&pg_owner->page_alloc_lock);\n                if ( drop_ref )\n                {\n        pin_drop:\n                    if ( type == PGT_l1_page_table )\n                        put_page_and_type(page);\n                    else\n                    {\n                        curr->arch.old_guest_ptpg = NULL;\n                        curr->arch.old_guest_table = page;\n                    }\n                }\n            }\n            break;\n\n        case MMUEXT_UNPIN_TABLE:\n            if ( paging_mode_refcounts(pg_owner) )\n                break;\n\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, NULL, P2M_ALLOC);\n            if ( unlikely(!page) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" bad, or bad owner d%d\\n\",\n                         op.arg1.mfn, pg_owner->domain_id);\n                rc = -EINVAL;\n                break;\n            }\n\n            if ( !test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n            {\n                put_page(page);\n                gdprintk(XENLOG_WARNING,\n                         \"mfn %\" PRI_mfn \" not pinned\\n\", op.arg1.mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            switch ( rc = put_page_and_type_preemptible(page) )\n            {\n            case -EINTR:\n            case -ERESTART:\n                curr->arch.old_guest_ptpg = NULL;\n                curr->arch.old_guest_table = page;\n                rc = 0;\n                break;\n            default:\n                BUG_ON(rc);\n                break;\n            }\n            put_page(page);\n\n            /* A page is dirtied when its pin status is cleared. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n            break;\n\n        case MMUEXT_NEW_BASEPTR:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(paging_mode_translate(currd)) )\n                rc = -EINVAL;\n            else\n                rc = new_guest_cr3(_mfn(op.arg1.mfn));\n            break;\n\n        case MMUEXT_NEW_USER_BASEPTR: {\n            unsigned long old_mfn;\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(paging_mode_translate(currd)) )\n                rc = -EINVAL;\n            if ( unlikely(rc) )\n                break;\n\n            old_mfn = pagetable_get_pfn(curr->arch.guest_table_user);\n            /*\n             * This is particularly important when getting restarted after the\n             * previous attempt got preempted in the put-old-MFN phase.\n             */\n            if ( old_mfn == op.arg1.mfn )\n                break;\n\n            if ( op.arg1.mfn != 0 )\n            {\n                rc = get_page_and_type_from_mfn(\n                    _mfn(op.arg1.mfn), PGT_root_page_table, currd, 0, 1);\n\n                if ( unlikely(rc) )\n                {\n                    if ( rc == -EINTR )\n                        rc = -ERESTART;\n                    else if ( rc != -ERESTART )\n                        gdprintk(XENLOG_WARNING,\n                                 \"Error %d installing new mfn %\" PRI_mfn \"\\n\",\n                                 rc, op.arg1.mfn);\n                    break;\n                }\n\n                if ( VM_ASSIST(currd, m2p_strict) )\n                    zap_ro_mpt(_mfn(op.arg1.mfn));\n            }\n\n            curr->arch.guest_table_user = pagetable_from_pfn(op.arg1.mfn);\n\n            if ( old_mfn != 0 )\n            {\n                page = mfn_to_page(_mfn(old_mfn));\n\n                switch ( rc = put_page_and_type_preemptible(page) )\n                {\n                case -EINTR:\n                    rc = -ERESTART;\n                    /* fallthrough */\n                case -ERESTART:\n                    curr->arch.old_guest_ptpg = NULL;\n                    curr->arch.old_guest_table = page;\n                    break;\n                default:\n                    BUG_ON(rc);\n                    break;\n                }\n            }\n\n            break;\n        }\n\n        case MMUEXT_TLB_FLUSH_LOCAL:\n            if ( likely(currd == pg_owner) )\n                flush_tlb_local();\n            else\n                rc = -EPERM;\n            break;\n\n        case MMUEXT_INVLPG_LOCAL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else\n                paging_invlpg(curr, op.arg1.linear_addr);\n            break;\n\n        case MMUEXT_TLB_FLUSH_MULTI:\n        case MMUEXT_INVLPG_MULTI:\n        {\n            cpumask_t *mask = this_cpu(scratch_cpumask);\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(vcpumask_to_pcpumask(currd,\n                                   guest_handle_to_param(op.arg2.vcpumask,\n                                                         const_void),\n                                   mask)) )\n                rc = -EINVAL;\n            if ( unlikely(rc) )\n                break;\n\n            if ( op.cmd == MMUEXT_TLB_FLUSH_MULTI )\n                flush_tlb_mask(mask);\n            else if ( __addr_ok(op.arg1.linear_addr) )\n                flush_tlb_one_mask(mask, op.arg1.linear_addr);\n            break;\n        }\n\n        case MMUEXT_TLB_FLUSH_ALL:\n            if ( likely(currd == pg_owner) )\n                flush_tlb_mask(currd->domain_dirty_cpumask);\n            else\n                rc = -EPERM;\n            break;\n\n        case MMUEXT_INVLPG_ALL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( __addr_ok(op.arg1.linear_addr) )\n                flush_tlb_one_mask(currd->domain_dirty_cpumask,\n                                   op.arg1.linear_addr);\n            break;\n\n        case MMUEXT_FLUSH_CACHE:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( unlikely(!cache_flush_permitted(currd)) )\n                rc = -EACCES;\n            else\n                wbinvd();\n            break;\n\n        case MMUEXT_FLUSH_CACHE_GLOBAL:\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( likely(cache_flush_permitted(currd)) )\n            {\n                unsigned int cpu;\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                cpumask_clear(mask);\n                for_each_online_cpu(cpu)\n                    if ( !cpumask_intersects(mask,\n                                             per_cpu(cpu_sibling_mask, cpu)) )\n                        __cpumask_set_cpu(cpu, mask);\n                flush_mask(mask, FLUSH_CACHE);\n            }\n            else\n                rc = -EINVAL;\n            break;\n\n        case MMUEXT_SET_LDT:\n        {\n            unsigned int ents = op.arg2.nr_ents;\n            unsigned long ptr = ents ? op.arg1.linear_addr : 0;\n\n            if ( unlikely(currd != pg_owner) )\n                rc = -EPERM;\n            else if ( paging_mode_external(currd) )\n                rc = -EINVAL;\n            else if ( ((ptr & (PAGE_SIZE - 1)) != 0) || !__addr_ok(ptr) ||\n                      (ents > 8192) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Bad args to SET_LDT: ptr=%lx, ents=%x\\n\", ptr, ents);\n                rc = -EINVAL;\n            }\n            else if ( (curr->arch.pv_vcpu.ldt_ents != ents) ||\n                      (curr->arch.pv_vcpu.ldt_base != ptr) )\n            {\n                invalidate_shadow_ldt(curr, 0);\n                flush_tlb_local();\n                curr->arch.pv_vcpu.ldt_base = ptr;\n                curr->arch.pv_vcpu.ldt_ents = ents;\n                load_LDT(curr);\n            }\n            break;\n        }\n\n        case MMUEXT_CLEAR_PAGE:\n            page = get_page_from_gfn(pg_owner, op.arg1.mfn, &p2mt, P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && page )\n            {\n                put_page(page);\n                page = NULL;\n            }\n            if ( !page || !get_page_type(page, PGT_writable_page) )\n            {\n                if ( page )\n                    put_page(page);\n                gdprintk(XENLOG_WARNING,\n                         \"Error clearing mfn %\" PRI_mfn \"\\n\", op.arg1.mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            /* A page is dirtied when it's being cleared. */\n            paging_mark_dirty(pg_owner, page_to_mfn(page));\n\n            clear_domain_page(page_to_mfn(page));\n\n            put_page_and_type(page);\n            break;\n\n        case MMUEXT_COPY_PAGE:\n        {\n            struct page_info *src_page, *dst_page;\n\n            src_page = get_page_from_gfn(pg_owner, op.arg2.src_mfn, &p2mt,\n                                         P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && src_page )\n            {\n                put_page(src_page);\n                src_page = NULL;\n            }\n            if ( unlikely(!src_page) )\n            {\n                gdprintk(XENLOG_WARNING,\n                         \"Error copying from mfn %\" PRI_mfn \"\\n\",\n                         op.arg2.src_mfn);\n                rc = -EINVAL;\n                break;\n            }\n\n            dst_page = get_page_from_gfn(pg_owner, op.arg1.mfn, &p2mt,\n                                         P2M_ALLOC);\n            if ( unlikely(p2mt != p2m_ram_rw) && dst_page )\n            {\n                put_page(dst_page);\n                dst_page = NULL;\n            }\n            rc = (dst_page &&\n                  get_page_type(dst_page, PGT_writable_page)) ? 0 : -EINVAL;\n            if ( unlikely(rc) )\n            {\n                put_page(src_page);\n                if ( dst_page )\n                    put_page(dst_page);\n                gdprintk(XENLOG_WARNING,\n                         \"Error copying to mfn %\" PRI_mfn \"\\n\", op.arg1.mfn);\n                break;\n            }\n\n            /* A page is dirtied when it's being copied to. */\n            paging_mark_dirty(pg_owner, page_to_mfn(dst_page));\n\n            copy_domain_page(page_to_mfn(dst_page), page_to_mfn(src_page));\n\n            put_page_and_type(dst_page);\n            put_page(src_page);\n            break;\n        }\n\n        case MMUEXT_MARK_SUPER:\n        case MMUEXT_UNMARK_SUPER:\n            rc = -EOPNOTSUPP;\n            break;\n\n        default:\n            rc = -ENOSYS;\n            break;\n        }\n\n done:\n        if ( unlikely(rc) )\n            break;\n\n        guest_handle_add_offset(uops, 1);\n    }\n\n    if ( rc == -ERESTART )\n    {\n        ASSERT(i < count);\n        rc = hypercall_create_continuation(\n            __HYPERVISOR_mmuext_op, \"hihi\",\n            uops, (count - i) | MMU_UPDATE_PREEMPTED, pdone, foreigndom);\n    }\n    else if ( curr->arch.old_guest_table )\n    {\n        XEN_GUEST_HANDLE_PARAM(void) null;\n\n        ASSERT(rc || i == count);\n        set_xen_guest_handle(null, NULL);\n        /*\n         * In order to have a way to communicate the final return value to\n         * our continuation, we pass this in place of \"foreigndom\", building\n         * on the fact that this argument isn't needed anymore.\n         */\n        rc = hypercall_create_continuation(\n                __HYPERVISOR_mmuext_op, \"hihi\", null,\n                MMU_UPDATE_PREEMPTED, null, rc);\n    }\n\n    put_pg_owner(pg_owner);\n\n    perfc_add(num_mmuext_ops, i);\n\n    /* Add incremental work we have done to the @done output parameter. */\n    if ( unlikely(!guest_handle_is_null(pdone)) )\n    {\n        done += i;\n        copy_to_guest(pdone, &done, 1);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -174,7 +174,10 @@\n                     if ( type == PGT_l1_page_table )\n                         put_page_and_type(page);\n                     else\n+                    {\n+                        curr->arch.old_guest_ptpg = NULL;\n                         curr->arch.old_guest_table = page;\n+                    }\n                 }\n             }\n             break;\n@@ -206,6 +209,7 @@\n             {\n             case -EINTR:\n             case -ERESTART:\n+                curr->arch.old_guest_ptpg = NULL;\n                 curr->arch.old_guest_table = page;\n                 rc = 0;\n                 break;\n@@ -278,6 +282,7 @@\n                     rc = -ERESTART;\n                     /* fallthrough */\n                 case -ERESTART:\n+                    curr->arch.old_guest_ptpg = NULL;\n                     curr->arch.old_guest_table = page;\n                     break;\n                 default:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                    {",
                "                        curr->arch.old_guest_ptpg = NULL;",
                "                    }",
                "                curr->arch.old_guest_ptpg = NULL;",
                "                    curr->arch.old_guest_ptpg = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/new_guest_cr3",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/6987fc7558bdbab8119eabf026e3cdad1053f0e5",
        "commit_title": "x86: limit linear page table use to a single level",
        "commit_text": " That's the only way that they're meant to be used. Without such a restriction arbitrarily long chains of same-level page tables can be built, tearing down of which may then cause arbitrarily deep recursion, causing a stack overflow. To facilitate this restriction, a counter is being introduced to track both the number of same-level entries in a page table as well as the number of uses of a page table in another same-level one (counting into positive and negative direction respectively, utilizing the fact that both counts can't be non-zero at the same time).  Note that the added accounting introduces a restriction on the number of times a page can be used in other same-level page tables - more than 32k of such uses are no longer possible.  Note also that some put_page_and_type[_preemptible]() calls are replaced with open-coded equivalents.  This seemed preferrable to adding \"parent_table\" to the matrix of functions.  Note further that cross-domain same-level page table references are no longer permitted (they probably never should have been).  This is XSA-240. ",
        "func_before": "int new_guest_cr3(mfn_t mfn)\n{\n    struct vcpu *curr = current;\n    struct domain *d = curr->domain;\n    int rc;\n    mfn_t old_base_mfn;\n\n    if ( is_pv_32bit_domain(d) )\n    {\n        mfn_t gt_mfn = pagetable_get_mfn(curr->arch.guest_table);\n        l4_pgentry_t *pl4e = map_domain_page(gt_mfn);\n\n        rc = mod_l4_entry(pl4e,\n                          l4e_from_mfn(mfn,\n                                       (_PAGE_PRESENT | _PAGE_RW |\n                                        _PAGE_USER | _PAGE_ACCESSED)),\n                          mfn_x(gt_mfn), 0, curr);\n        unmap_domain_page(pl4e);\n        switch ( rc )\n        {\n        case 0:\n            break;\n        case -EINTR:\n        case -ERESTART:\n            return -ERESTART;\n        default:\n            gdprintk(XENLOG_WARNING,\n                     \"Error while installing new compat baseptr %\" PRI_mfn \"\\n\",\n                     mfn_x(mfn));\n            return rc;\n        }\n\n        invalidate_shadow_ldt(curr, 0);\n        write_ptbase(curr);\n\n        return 0;\n    }\n\n    rc = put_old_guest_table(curr);\n    if ( unlikely(rc) )\n        return rc;\n\n    old_base_mfn = pagetable_get_mfn(curr->arch.guest_table);\n    /*\n     * This is particularly important when getting restarted after the\n     * previous attempt got preempted in the put-old-MFN phase.\n     */\n    if ( mfn_eq(old_base_mfn, mfn) )\n    {\n        write_ptbase(curr);\n        return 0;\n    }\n\n    rc = paging_mode_refcounts(d)\n         ? (get_page_from_mfn(mfn, d) ? 0 : -EINVAL)\n         : get_page_and_type_from_mfn(mfn, PGT_root_page_table, d, 0, 1);\n    switch ( rc )\n    {\n    case 0:\n        break;\n    case -EINTR:\n    case -ERESTART:\n        return -ERESTART;\n    default:\n        gdprintk(XENLOG_WARNING,\n                 \"Error while installing new baseptr %\" PRI_mfn \"\\n\",\n                 mfn_x(mfn));\n        return rc;\n    }\n\n    invalidate_shadow_ldt(curr, 0);\n\n    if ( !VM_ASSIST(d, m2p_strict) && !paging_mode_refcounts(d) )\n        fill_ro_mpt(mfn);\n    curr->arch.guest_table = pagetable_from_mfn(mfn);\n    update_cr3(curr);\n\n    write_ptbase(curr);\n\n    if ( likely(mfn_x(old_base_mfn) != 0) )\n    {\n        struct page_info *page = mfn_to_page(old_base_mfn);\n\n        if ( paging_mode_refcounts(d) )\n            put_page(page);\n        else\n            switch ( rc = put_page_and_type_preemptible(page) )\n            {\n            case -EINTR:\n                rc = -ERESTART;\n                /* fallthrough */\n            case -ERESTART:\n                curr->arch.old_guest_table = page;\n                break;\n            default:\n                BUG_ON(rc);\n                break;\n            }\n    }\n\n    return rc;\n}",
        "func": "int new_guest_cr3(mfn_t mfn)\n{\n    struct vcpu *curr = current;\n    struct domain *d = curr->domain;\n    int rc;\n    mfn_t old_base_mfn;\n\n    if ( is_pv_32bit_domain(d) )\n    {\n        mfn_t gt_mfn = pagetable_get_mfn(curr->arch.guest_table);\n        l4_pgentry_t *pl4e = map_domain_page(gt_mfn);\n\n        rc = mod_l4_entry(pl4e,\n                          l4e_from_mfn(mfn,\n                                       (_PAGE_PRESENT | _PAGE_RW |\n                                        _PAGE_USER | _PAGE_ACCESSED)),\n                          mfn_x(gt_mfn), 0, curr);\n        unmap_domain_page(pl4e);\n        switch ( rc )\n        {\n        case 0:\n            break;\n        case -EINTR:\n        case -ERESTART:\n            return -ERESTART;\n        default:\n            gdprintk(XENLOG_WARNING,\n                     \"Error while installing new compat baseptr %\" PRI_mfn \"\\n\",\n                     mfn_x(mfn));\n            return rc;\n        }\n\n        invalidate_shadow_ldt(curr, 0);\n        write_ptbase(curr);\n\n        return 0;\n    }\n\n    rc = put_old_guest_table(curr);\n    if ( unlikely(rc) )\n        return rc;\n\n    old_base_mfn = pagetable_get_mfn(curr->arch.guest_table);\n    /*\n     * This is particularly important when getting restarted after the\n     * previous attempt got preempted in the put-old-MFN phase.\n     */\n    if ( mfn_eq(old_base_mfn, mfn) )\n    {\n        write_ptbase(curr);\n        return 0;\n    }\n\n    rc = paging_mode_refcounts(d)\n         ? (get_page_from_mfn(mfn, d) ? 0 : -EINVAL)\n         : get_page_and_type_from_mfn(mfn, PGT_root_page_table, d, 0, 1);\n    switch ( rc )\n    {\n    case 0:\n        break;\n    case -EINTR:\n    case -ERESTART:\n        return -ERESTART;\n    default:\n        gdprintk(XENLOG_WARNING,\n                 \"Error while installing new baseptr %\" PRI_mfn \"\\n\",\n                 mfn_x(mfn));\n        return rc;\n    }\n\n    invalidate_shadow_ldt(curr, 0);\n\n    if ( !VM_ASSIST(d, m2p_strict) && !paging_mode_refcounts(d) )\n        fill_ro_mpt(mfn);\n    curr->arch.guest_table = pagetable_from_mfn(mfn);\n    update_cr3(curr);\n\n    write_ptbase(curr);\n\n    if ( likely(mfn_x(old_base_mfn) != 0) )\n    {\n        struct page_info *page = mfn_to_page(old_base_mfn);\n\n        if ( paging_mode_refcounts(d) )\n            put_page(page);\n        else\n            switch ( rc = put_page_and_type_preemptible(page) )\n            {\n            case -EINTR:\n                rc = -ERESTART;\n                /* fallthrough */\n            case -ERESTART:\n                curr->arch.old_guest_ptpg = NULL;\n                curr->arch.old_guest_table = page;\n                break;\n            default:\n                BUG_ON(rc);\n                break;\n            }\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -90,6 +90,7 @@\n                 rc = -ERESTART;\n                 /* fallthrough */\n             case -ERESTART:\n+                curr->arch.old_guest_ptpg = NULL;\n                 curr->arch.old_guest_table = page;\n                 break;\n             default:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                curr->arch.old_guest_ptpg = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15595",
        "func_name": "xen-project/xen/_put_page_type",
        "description": "An issue was discovered in Xen through 4.9.x allowing x86 PV guest OS users to cause a denial of service (unbounded recursion, stack consumption, and hypervisor crash) or possibly gain privileges via crafted page-table stacking.",
        "git_url": "https://github.com/xen-project/xen/commit/2c458dfcb59f3d9d8a35fc5ffbf780b6ed7a26a6",
        "commit_title": "x86: don't wrongly trigger linear page table assertion",
        "commit_text": " _put_page_type() may do multiple iterations until its cmpxchg() succeeds. It invokes set_tlbflush_timestamp() on the first iteration, however. Code inside the function takes care of this, but - the assertion in _put_final_page_type() would trigger on the second   iteration if time stamps in a debug build are permitted to be   sufficiently much wider than the default 6 bits (see WRAP_MASK in   flushtlb.c), - it returning -EINTR (for a continuation to be scheduled) would leave   the page inconsistent state (until the re-invocation completes). Make the set_tlbflush_timestamp() invocation conditional, bypassing it (for now) only in the case we really can't tolerate the stamp to be stored.  This is part of XSA-240. ",
        "func_before": "static int _put_page_type(struct page_info *page, bool preemptible,\n                          struct page_info *ptpg)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0;\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x - 1;\n\n        ASSERT((x & PGT_count_mask) != 0);\n\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            if ( unlikely((nx & PGT_type_mask) <= PGT_l4_page_table) &&\n                 likely(nx & (PGT_validated|PGT_partial)) )\n            {\n                /*\n                 * Page-table pages must be unvalidated when count is zero. The\n                 * 'free' is safe because the refcnt is non-zero and validated\n                 * bit is clear => other ops will spin or fail.\n                 */\n                nx = x & ~(PGT_validated|PGT_partial);\n                if ( unlikely((y = cmpxchg(&page->u.inuse.type_info,\n                                           x, nx)) != x) )\n                    continue;\n                /* We cleared the 'valid bit' so we do the clean up. */\n                rc = _put_final_page_type(page, x, preemptible, ptpg);\n                ptpg = NULL;\n                if ( x & PGT_partial )\n                    put_page(page);\n                break;\n            }\n\n#ifdef CONFIG_PV_LINEAR_PT\n            if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n            {\n                /*\n                 * set_tlbflush_timestamp() accesses the same union\n                 * linear_pt_count lives in. Unvalidated page table pages,\n                 * however, should occur during domain destruction only\n                 * anyway.  Updating of linear_pt_count luckily is not\n                 * necessary anymore for a dying domain.\n                 */\n                ASSERT(page_get_owner(page)->is_dying);\n                ASSERT(page->linear_pt_count < 0);\n                ASSERT(ptpg->linear_pt_count > 0);\n                ptpg = NULL;\n            }\n#else /* CONFIG_PV_LINEAR_PT */\n            BUG_ON(ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info));\n#endif\n\n            set_tlbflush_timestamp(page);\n        }\n        else if ( unlikely((nx & (PGT_locked | PGT_count_mask)) ==\n                           (PGT_locked | 1)) )\n        {\n            /*\n             * We must not drop the second to last reference when the page is\n             * locked, as page_unlock() doesn't do any cleanup of the type.\n             */\n            cpu_relax();\n            y = page->u.inuse.type_info;\n            continue;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n    {\n        ASSERT(!rc);\n        dec_linear_uses(page);\n        dec_linear_entries(ptpg);\n    }\n\n    return rc;\n}",
        "func": "static int _put_page_type(struct page_info *page, bool preemptible,\n                          struct page_info *ptpg)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0;\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x - 1;\n\n        ASSERT((x & PGT_count_mask) != 0);\n\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            if ( unlikely((nx & PGT_type_mask) <= PGT_l4_page_table) &&\n                 likely(nx & (PGT_validated|PGT_partial)) )\n            {\n                /*\n                 * Page-table pages must be unvalidated when count is zero. The\n                 * 'free' is safe because the refcnt is non-zero and validated\n                 * bit is clear => other ops will spin or fail.\n                 */\n                nx = x & ~(PGT_validated|PGT_partial);\n                if ( unlikely((y = cmpxchg(&page->u.inuse.type_info,\n                                           x, nx)) != x) )\n                    continue;\n                /* We cleared the 'valid bit' so we do the clean up. */\n                rc = _put_final_page_type(page, x, preemptible, ptpg);\n                ptpg = NULL;\n                if ( x & PGT_partial )\n                    put_page(page);\n                break;\n            }\n\n            if ( !ptpg || !PGT_type_equal(x, ptpg->u.inuse.type_info) )\n            {\n                /*\n                 * set_tlbflush_timestamp() accesses the same union\n                 * linear_pt_count lives in. Pages (including page table ones),\n                 * however, don't need their flush time stamp set except when\n                 * the last reference is being dropped. For page table pages\n                 * this happens in _put_final_page_type().\n                 */\n                set_tlbflush_timestamp(page);\n            }\n            else\n                BUG_ON(!IS_ENABLED(CONFIG_PV_LINEAR_PT));\n        }\n        else if ( unlikely((nx & (PGT_locked | PGT_count_mask)) ==\n                           (PGT_locked | 1)) )\n        {\n            /*\n             * We must not drop the second to last reference when the page is\n             * locked, as page_unlock() doesn't do any cleanup of the type.\n             */\n            cpu_relax();\n            y = page->u.inuse.type_info;\n            continue;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n    {\n        ASSERT(!rc);\n        dec_linear_uses(page);\n        dec_linear_entries(ptpg);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,26 +33,19 @@\n                 break;\n             }\n \n-#ifdef CONFIG_PV_LINEAR_PT\n-            if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )\n+            if ( !ptpg || !PGT_type_equal(x, ptpg->u.inuse.type_info) )\n             {\n                 /*\n                  * set_tlbflush_timestamp() accesses the same union\n-                 * linear_pt_count lives in. Unvalidated page table pages,\n-                 * however, should occur during domain destruction only\n-                 * anyway.  Updating of linear_pt_count luckily is not\n-                 * necessary anymore for a dying domain.\n+                 * linear_pt_count lives in. Pages (including page table ones),\n+                 * however, don't need their flush time stamp set except when\n+                 * the last reference is being dropped. For page table pages\n+                 * this happens in _put_final_page_type().\n                  */\n-                ASSERT(page_get_owner(page)->is_dying);\n-                ASSERT(page->linear_pt_count < 0);\n-                ASSERT(ptpg->linear_pt_count > 0);\n-                ptpg = NULL;\n+                set_tlbflush_timestamp(page);\n             }\n-#else /* CONFIG_PV_LINEAR_PT */\n-            BUG_ON(ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info));\n-#endif\n-\n-            set_tlbflush_timestamp(page);\n+            else\n+                BUG_ON(!IS_ENABLED(CONFIG_PV_LINEAR_PT));\n         }\n         else if ( unlikely((nx & (PGT_locked | PGT_count_mask)) ==\n                            (PGT_locked | 1)) )",
        "diff_line_info": {
            "deleted_lines": [
                "#ifdef CONFIG_PV_LINEAR_PT",
                "            if ( ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info) )",
                "                 * linear_pt_count lives in. Unvalidated page table pages,",
                "                 * however, should occur during domain destruction only",
                "                 * anyway.  Updating of linear_pt_count luckily is not",
                "                 * necessary anymore for a dying domain.",
                "                ASSERT(page_get_owner(page)->is_dying);",
                "                ASSERT(page->linear_pt_count < 0);",
                "                ASSERT(ptpg->linear_pt_count > 0);",
                "                ptpg = NULL;",
                "#else /* CONFIG_PV_LINEAR_PT */",
                "            BUG_ON(ptpg && PGT_type_equal(x, ptpg->u.inuse.type_info));",
                "#endif",
                "",
                "            set_tlbflush_timestamp(page);"
            ],
            "added_lines": [
                "            if ( !ptpg || !PGT_type_equal(x, ptpg->u.inuse.type_info) )",
                "                 * linear_pt_count lives in. Pages (including page table ones),",
                "                 * however, don't need their flush time stamp set except when",
                "                 * the last reference is being dropped. For page table pages",
                "                 * this happens in _put_final_page_type().",
                "                set_tlbflush_timestamp(page);",
                "            else",
                "                BUG_ON(!IS_ENABLED(CONFIG_PV_LINEAR_PT));"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-15596",
        "func_name": "xen-project/xen/xenmem_add_to_physmap_one",
        "description": "An issue was discovered in Xen 4.4.x through 4.9.x allowing ARM guest OS users to cause a denial of service (prevent physical CPU usage) because of lock mishandling upon detection of an add-to-physmap error.",
        "git_url": "https://github.com/xen-project/xen/commit/59546c1897a90fe9af5ebbbb05ead8d98b4d17b9",
        "commit_title": "arm/mm: release grant lock on xenmem_add_to_physmap_one() error paths",
        "commit_text": " Commit 55021ff9ab (\"xen/arm: add_to_physmap_one: Avoid to map mfn 0 if an error occurs\") introduced error paths not releasing the grant table lock. Replace them by a suitable check after the lock was dropped.  This is XSA-235. ",
        "func_before": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union xen_add_to_physmap_batch_extra extra,\n    unsigned long idx,\n    gfn_t gfn)\n{\n    mfn_t mfn = INVALID_MFN;\n    int rc;\n    p2m_type_t t;\n    struct page_info *page = NULL;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_grant_table:\n        grant_write_lock(d->grant_table);\n\n        if ( d->grant_table->gt_version == 0 )\n            d->grant_table->gt_version = 1;\n\n        if ( d->grant_table->gt_version == 2 &&\n                (idx & XENMAPIDX_grant_table_status) )\n        {\n            idx &= ~XENMAPIDX_grant_table_status;\n            if ( idx < nr_status_frames(d->grant_table) )\n                mfn = virt_to_mfn(d->grant_table->status[idx]);\n            else\n                return -EINVAL;\n        }\n        else\n        {\n            if ( (idx >= nr_grant_frames(d->grant_table)) &&\n                 (idx < max_grant_frames) )\n                gnttab_grow_table(d, idx + 1);\n\n            if ( idx < nr_grant_frames(d->grant_table) )\n                mfn = virt_to_mfn(d->grant_table->shared_raw[idx]);\n            else\n                return -EINVAL;\n        }\n\n        d->arch.grant_table_gfn[idx] = gfn;\n\n        t = p2m_ram_rw;\n\n        grant_write_unlock(d->grant_table);\n        break;\n    case XENMAPSPACE_shared_info:\n        if ( idx != 0 )\n            return -EINVAL;\n\n        mfn = virt_to_mfn(d->shared_info);\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_gmfn_foreign:\n    {\n        struct domain *od;\n        p2m_type_t p2mt;\n\n        od = rcu_lock_domain_by_any_id(extra.foreign_domid);\n        if ( od == NULL )\n            return -ESRCH;\n\n        if ( od == d )\n        {\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        rc = xsm_map_gmfn_foreign(XSM_TARGET, d, od);\n        if ( rc )\n        {\n            rcu_unlock_domain(od);\n            return rc;\n        }\n\n        /* Take reference to the foreign domain page.\n         * Reference will be released in XENMEM_remove_from_physmap */\n        page = get_page_from_gfn(od, idx, &p2mt, P2M_ALLOC);\n        if ( !page )\n        {\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        if ( !p2m_is_ram(p2mt) )\n        {\n            put_page(page);\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        mfn = _mfn(page_to_mfn(page));\n        t = p2m_map_foreign;\n\n        rcu_unlock_domain(od);\n        break;\n    }\n    case XENMAPSPACE_dev_mmio:\n        /* extra should be 0. Reserved for future use. */\n        if ( extra.res0 )\n            return -EOPNOTSUPP;\n\n        rc = map_dev_mmio_region(d, gfn, 1, _mfn(idx));\n        return rc;\n\n    default:\n        return -ENOSYS;\n    }\n\n    /* Map at new location. */\n    rc = guest_physmap_add_entry(d, gfn, mfn, 0, t);\n\n    /* If we fail to add the mapping, we need to drop the reference we\n     * took earlier on foreign pages */\n    if ( rc && space == XENMAPSPACE_gmfn_foreign )\n    {\n        ASSERT(page != NULL);\n        put_page(page);\n    }\n\n    return rc;\n}",
        "func": "int xenmem_add_to_physmap_one(\n    struct domain *d,\n    unsigned int space,\n    union xen_add_to_physmap_batch_extra extra,\n    unsigned long idx,\n    gfn_t gfn)\n{\n    mfn_t mfn = INVALID_MFN;\n    int rc;\n    p2m_type_t t;\n    struct page_info *page = NULL;\n\n    switch ( space )\n    {\n    case XENMAPSPACE_grant_table:\n        grant_write_lock(d->grant_table);\n\n        if ( d->grant_table->gt_version == 0 )\n            d->grant_table->gt_version = 1;\n\n        if ( d->grant_table->gt_version == 2 &&\n                (idx & XENMAPIDX_grant_table_status) )\n        {\n            idx &= ~XENMAPIDX_grant_table_status;\n            if ( idx < nr_status_frames(d->grant_table) )\n                mfn = virt_to_mfn(d->grant_table->status[idx]);\n        }\n        else\n        {\n            if ( (idx >= nr_grant_frames(d->grant_table)) &&\n                 (idx < max_grant_frames) )\n                gnttab_grow_table(d, idx + 1);\n\n            if ( idx < nr_grant_frames(d->grant_table) )\n                mfn = virt_to_mfn(d->grant_table->shared_raw[idx]);\n        }\n\n        if ( !mfn_eq(mfn, INVALID_MFN) )\n        {\n            d->arch.grant_table_gfn[idx] = gfn;\n\n            t = p2m_ram_rw;\n        }\n\n        grant_write_unlock(d->grant_table);\n\n        if ( mfn_eq(mfn, INVALID_MFN) )\n            return -EINVAL;\n\n        break;\n    case XENMAPSPACE_shared_info:\n        if ( idx != 0 )\n            return -EINVAL;\n\n        mfn = virt_to_mfn(d->shared_info);\n        t = p2m_ram_rw;\n\n        break;\n    case XENMAPSPACE_gmfn_foreign:\n    {\n        struct domain *od;\n        p2m_type_t p2mt;\n\n        od = rcu_lock_domain_by_any_id(extra.foreign_domid);\n        if ( od == NULL )\n            return -ESRCH;\n\n        if ( od == d )\n        {\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        rc = xsm_map_gmfn_foreign(XSM_TARGET, d, od);\n        if ( rc )\n        {\n            rcu_unlock_domain(od);\n            return rc;\n        }\n\n        /* Take reference to the foreign domain page.\n         * Reference will be released in XENMEM_remove_from_physmap */\n        page = get_page_from_gfn(od, idx, &p2mt, P2M_ALLOC);\n        if ( !page )\n        {\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        if ( !p2m_is_ram(p2mt) )\n        {\n            put_page(page);\n            rcu_unlock_domain(od);\n            return -EINVAL;\n        }\n\n        mfn = _mfn(page_to_mfn(page));\n        t = p2m_map_foreign;\n\n        rcu_unlock_domain(od);\n        break;\n    }\n    case XENMAPSPACE_dev_mmio:\n        /* extra should be 0. Reserved for future use. */\n        if ( extra.res0 )\n            return -EOPNOTSUPP;\n\n        rc = map_dev_mmio_region(d, gfn, 1, _mfn(idx));\n        return rc;\n\n    default:\n        return -ENOSYS;\n    }\n\n    /* Map at new location. */\n    rc = guest_physmap_add_entry(d, gfn, mfn, 0, t);\n\n    /* If we fail to add the mapping, we need to drop the reference we\n     * took earlier on foreign pages */\n    if ( rc && space == XENMAPSPACE_gmfn_foreign )\n    {\n        ASSERT(page != NULL);\n        put_page(page);\n    }\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,8 +24,6 @@\n             idx &= ~XENMAPIDX_grant_table_status;\n             if ( idx < nr_status_frames(d->grant_table) )\n                 mfn = virt_to_mfn(d->grant_table->status[idx]);\n-            else\n-                return -EINVAL;\n         }\n         else\n         {\n@@ -35,15 +33,20 @@\n \n             if ( idx < nr_grant_frames(d->grant_table) )\n                 mfn = virt_to_mfn(d->grant_table->shared_raw[idx]);\n-            else\n-                return -EINVAL;\n         }\n \n-        d->arch.grant_table_gfn[idx] = gfn;\n+        if ( !mfn_eq(mfn, INVALID_MFN) )\n+        {\n+            d->arch.grant_table_gfn[idx] = gfn;\n \n-        t = p2m_ram_rw;\n+            t = p2m_ram_rw;\n+        }\n \n         grant_write_unlock(d->grant_table);\n+\n+        if ( mfn_eq(mfn, INVALID_MFN) )\n+            return -EINVAL;\n+\n         break;\n     case XENMAPSPACE_shared_info:\n         if ( idx != 0 )",
        "diff_line_info": {
            "deleted_lines": [
                "            else",
                "                return -EINVAL;",
                "            else",
                "                return -EINVAL;",
                "        d->arch.grant_table_gfn[idx] = gfn;",
                "        t = p2m_ram_rw;"
            ],
            "added_lines": [
                "        if ( !mfn_eq(mfn, INVALID_MFN) )",
                "        {",
                "            d->arch.grant_table_gfn[idx] = gfn;",
                "            t = p2m_ram_rw;",
                "        }",
                "",
                "        if ( mfn_eq(mfn, INVALID_MFN) )",
                "            return -EINVAL;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-14223",
        "func_name": "ffmpeg/asf_build_simple_index",
        "description": "In libavformat/asfdec_f.c in FFmpeg 3.3.3, a DoS in asf_build_simple_index() due to lack of an EOF (End of File) check might cause huge CPU consumption. When a crafted ASF file, which claims a large \"ict\" field in the header but does not contain sufficient backing data, is provided, the for loop would consume huge CPU and memory resources, since there is no EOF check inside the loop.",
        "git_url": "https://github.com/FFmpeg/FFmpeg/commit/afc9c683ed9db01edb357bc8c19edad4282b3a97",
        "commit_title": "avformat/asfdec: Fix DoS in asf_build_simple_index()",
        "commit_text": " No testcase ",
        "func_before": "static int asf_build_simple_index(AVFormatContext *s, int stream_index)\n{\n    ff_asf_guid g;\n    ASFContext *asf     = s->priv_data;\n    int64_t current_pos = avio_tell(s->pb);\n    int64_t ret;\n\n    if((ret = avio_seek(s->pb, asf->data_object_offset + asf->data_object_size, SEEK_SET)) < 0) {\n        return ret;\n    }\n\n    if ((ret = ff_get_guid(s->pb, &g)) < 0)\n        goto end;\n\n    /* the data object can be followed by other top-level objects,\n     * skip them until the simple index object is reached */\n    while (ff_guidcmp(&g, &ff_asf_simple_index_header)) {\n        int64_t gsize = avio_rl64(s->pb);\n        if (gsize < 24 || avio_feof(s->pb)) {\n            goto end;\n        }\n        avio_skip(s->pb, gsize - 24);\n        if ((ret = ff_get_guid(s->pb, &g)) < 0)\n            goto end;\n    }\n\n    {\n        int64_t itime, last_pos = -1;\n        int pct, ict;\n        int i;\n        int64_t av_unused gsize = avio_rl64(s->pb);\n        if ((ret = ff_get_guid(s->pb, &g)) < 0)\n            goto end;\n        itime = avio_rl64(s->pb);\n        pct   = avio_rl32(s->pb);\n        ict   = avio_rl32(s->pb);\n        av_log(s, AV_LOG_DEBUG,\n               \"itime:0x%\"PRIx64\", pct:%d, ict:%d\\n\", itime, pct, ict);\n\n        for (i = 0; i < ict; i++) {\n            int pktnum        = avio_rl32(s->pb);\n            int pktct         = avio_rl16(s->pb);\n            int64_t pos       = s->internal->data_offset + s->packet_size * (int64_t)pktnum;\n            int64_t index_pts = FFMAX(av_rescale(itime, i, 10000) - asf->hdr.preroll, 0);\n\n            if (pos != last_pos) {\n                av_log(s, AV_LOG_DEBUG, \"pktnum:%d, pktct:%d  pts: %\"PRId64\"\\n\",\n                       pktnum, pktct, index_pts);\n                av_add_index_entry(s->streams[stream_index], pos, index_pts,\n                                   s->packet_size, 0, AVINDEX_KEYFRAME);\n                last_pos = pos;\n            }\n        }\n        asf->index_read = ict > 1;\n    }\nend:\n//     if (avio_feof(s->pb)) {\n//         ret = 0;\n//     }\n    avio_seek(s->pb, current_pos, SEEK_SET);\n    return ret;\n}",
        "func": "static int asf_build_simple_index(AVFormatContext *s, int stream_index)\n{\n    ff_asf_guid g;\n    ASFContext *asf     = s->priv_data;\n    int64_t current_pos = avio_tell(s->pb);\n    int64_t ret;\n\n    if((ret = avio_seek(s->pb, asf->data_object_offset + asf->data_object_size, SEEK_SET)) < 0) {\n        return ret;\n    }\n\n    if ((ret = ff_get_guid(s->pb, &g)) < 0)\n        goto end;\n\n    /* the data object can be followed by other top-level objects,\n     * skip them until the simple index object is reached */\n    while (ff_guidcmp(&g, &ff_asf_simple_index_header)) {\n        int64_t gsize = avio_rl64(s->pb);\n        if (gsize < 24 || avio_feof(s->pb)) {\n            goto end;\n        }\n        avio_skip(s->pb, gsize - 24);\n        if ((ret = ff_get_guid(s->pb, &g)) < 0)\n            goto end;\n    }\n\n    {\n        int64_t itime, last_pos = -1;\n        int pct, ict;\n        int i;\n        int64_t av_unused gsize = avio_rl64(s->pb);\n        if ((ret = ff_get_guid(s->pb, &g)) < 0)\n            goto end;\n        itime = avio_rl64(s->pb);\n        pct   = avio_rl32(s->pb);\n        ict   = avio_rl32(s->pb);\n        av_log(s, AV_LOG_DEBUG,\n               \"itime:0x%\"PRIx64\", pct:%d, ict:%d\\n\", itime, pct, ict);\n\n        for (i = 0; i < ict; i++) {\n            int pktnum        = avio_rl32(s->pb);\n            int pktct         = avio_rl16(s->pb);\n            int64_t pos       = s->internal->data_offset + s->packet_size * (int64_t)pktnum;\n            int64_t index_pts = FFMAX(av_rescale(itime, i, 10000) - asf->hdr.preroll, 0);\n\n            if (avio_feof(s->pb)) {\n                ret = AVERROR_INVALIDDATA;\n                goto end;\n            }\n\n            if (pos != last_pos) {\n                av_log(s, AV_LOG_DEBUG, \"pktnum:%d, pktct:%d  pts: %\"PRId64\"\\n\",\n                       pktnum, pktct, index_pts);\n                av_add_index_entry(s->streams[stream_index], pos, index_pts,\n                                   s->packet_size, 0, AVINDEX_KEYFRAME);\n                last_pos = pos;\n            }\n        }\n        asf->index_read = ict > 1;\n    }\nend:\n//     if (avio_feof(s->pb)) {\n//         ret = 0;\n//     }\n    avio_seek(s->pb, current_pos, SEEK_SET);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,6 +43,11 @@\n             int64_t pos       = s->internal->data_offset + s->packet_size * (int64_t)pktnum;\n             int64_t index_pts = FFMAX(av_rescale(itime, i, 10000) - asf->hdr.preroll, 0);\n \n+            if (avio_feof(s->pb)) {\n+                ret = AVERROR_INVALIDDATA;\n+                goto end;\n+            }\n+\n             if (pos != last_pos) {\n                 av_log(s, AV_LOG_DEBUG, \"pktnum:%d, pktct:%d  pts: %\"PRId64\"\\n\",\n                        pktnum, pktct, index_pts);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            if (avio_feof(s->pb)) {",
                "                ret = AVERROR_INVALIDDATA;",
                "                goto end;",
                "            }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-14341",
        "func_name": "ImageMagick/ReadWPGImage",
        "description": "ImageMagick 7.0.6-6 has a large loop vulnerability in ReadWPGImage in coders/wpg.c, causing CPU exhaustion via a crafted wpg image file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/7d63315a64267c565d1f34b9cb523a14616fed24",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/654",
        "commit_text": "",
        "func_before": "static Image *ReadWPGImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n  typedef struct\n  {\n    size_t FileId;\n    MagickOffsetType DataOffset;\n    unsigned int ProductType;\n    unsigned int FileType;\n    unsigned char MajorVersion;\n    unsigned char MinorVersion;\n    unsigned int EncryptKey;\n    unsigned int Reserved;\n  } WPGHeader;\n\n  typedef struct\n  {\n    unsigned char RecType;\n    size_t RecordLength;\n  } WPGRecord;\n\n  typedef struct\n  {\n    unsigned char Class;\n    unsigned char RecType;\n    size_t Extension;\n    size_t RecordLength;\n  } WPG2Record;\n\n  typedef struct\n  {\n    unsigned  HorizontalUnits;\n    unsigned  VerticalUnits;\n    unsigned char PosSizePrecision;\n  } WPG2Start;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType1;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned char Depth;\n    unsigned char Compression;\n  } WPG2BitmapType1;\n\n  typedef struct\n  {\n    unsigned int RotAngle;\n    unsigned int LowLeftX;\n    unsigned int LowLeftY;\n    unsigned int UpRightX;\n    unsigned int UpRightY;\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType2;\n\n  typedef struct\n  {\n    unsigned int StartIndex;\n    unsigned int NumOfEntries;\n  } WPGColorMapRec;\n\n  /*\n  typedef struct {\n    size_t PS_unknown1;\n    unsigned int PS_unknown2;\n    unsigned int PS_unknown3;\n  } WPGPSl1Record;  \n  */\n\n  Image\n    *image;\n\n  unsigned int\n    status;\n\n  WPGHeader\n    Header;\n\n  WPGRecord\n    Rec;\n\n  WPG2Record\n    Rec2;\n\n  WPG2Start StartWPG;\n\n  WPGBitmapType1\n    BitmapHeader1;\n\n  WPG2BitmapType1\n    Bitmap2Header1;\n\n  WPGBitmapType2\n    BitmapHeader2;\n\n  WPGColorMapRec\n    WPG_Palette;\n\n  int\n    i,\n    bpp,\n    WPG2Flags;\n\n  ssize_t\n    ldblk;\n\n  size_t\n    one;\n\n  unsigned char\n    *BImgBuff;\n\n  tCTM CTM;         /*current transform matrix*/\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  one=1;\n  image=AcquireImage(image_info,exception);\n  image->depth=8;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read WPG image.\n  */\n  Header.FileId=ReadBlobLSBLong(image);\n  Header.DataOffset=(MagickOffsetType) ReadBlobLSBLong(image);\n  Header.ProductType=ReadBlobLSBShort(image);\n  Header.FileType=ReadBlobLSBShort(image);\n  Header.MajorVersion=ReadBlobByte(image);\n  Header.MinorVersion=ReadBlobByte(image);\n  Header.EncryptKey=ReadBlobLSBShort(image);\n  Header.Reserved=ReadBlobLSBShort(image);\n\n  if (Header.FileId!=0x435057FF || (Header.ProductType>>8)!=0x16)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  if (Header.EncryptKey!=0)\n    ThrowReaderException(CoderError,\"EncryptedWPGImageFileNotSupported\");\n\n  image->columns = 1;\n  image->rows = 1;\n  image->colors = 0;\n  bpp=0;\n  BitmapHeader2.RotAngle=0;\n  Rec2.RecordLength=0;\n\n  switch(Header.FileType)\n    {\n    case 1:     /* WPG level 1 */\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec.RecordLength);\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec.RecordLength;\n\n          switch(Rec.RecType)\n            {\n            case 0x0B: /* bitmap type 1 */\n              BitmapHeader1.Width=ReadBlobLSBShort(image);\n              BitmapHeader1.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader1.Width == 0) || (BitmapHeader1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader1.Depth=ReadBlobLSBShort(image);\n              BitmapHeader1.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader1.VertRes=ReadBlobLSBShort(image);\n\n              if(BitmapHeader1.HorzRes && BitmapHeader1.VertRes)\n                {\n                  image->units=PixelsPerCentimeterResolution;\n                  image->resolution.x=BitmapHeader1.HorzRes/470.0;\n                  image->resolution.y=BitmapHeader1.VertRes/470.0;\n                }\n              image->columns=BitmapHeader1.Width;\n              image->rows=BitmapHeader1.Height;\n              bpp=BitmapHeader1.Depth;\n\n              goto UnpackRaster;\n\n            case 0x0E:  /*Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (!AcquireImageColormap(image,image->colors,exception))\n                goto NoMemory;\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                }\n              break;\n     \n            case 0x11:  /* Start PS l1 */\n              if(Rec.RecordLength > 8)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+8,   /* skip PS header in the wpg */\n                  (ssize_t) Rec.RecordLength-8,exception);\n              break;     \n\n            case 0x14:  /* bitmap type 2 */\n              BitmapHeader2.RotAngle=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftX=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftY=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightX=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightY=ReadBlobLSBShort(image);\n              BitmapHeader2.Width=ReadBlobLSBShort(image);\n              BitmapHeader2.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader2.Width == 0) || (BitmapHeader2.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader2.Depth=ReadBlobLSBShort(image);\n              BitmapHeader2.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader2.VertRes=ReadBlobLSBShort(image);\n\n              image->units=PixelsPerCentimeterResolution;\n              image->page.width=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightX)/470.0);\n              image->page.height=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightY)/470.0);\n              image->page.x=(int) (BitmapHeader2.LowLeftX/470.0);\n              image->page.y=(int) (BitmapHeader2.LowLeftX/470.0);\n              if(BitmapHeader2.HorzRes && BitmapHeader2.VertRes)\n                {\n                  image->resolution.x=BitmapHeader2.HorzRes/470.0;\n                  image->resolution.y=BitmapHeader2.VertRes/470.0;\n                }\n              image->columns=BitmapHeader2.Width;\n              image->rows=BitmapHeader2.Height;\n              bpp=BitmapHeader2.Depth;\n\n            UnpackRaster:      \n              status=SetImageExtent(image,image->columns,image->rows,exception);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors,exception))\n                    {\n                    NoMemory:\n                      ThrowReaderException(ResourceLimitError,\n                        \"MemoryAllocationFailed\");\n                    }\n                  /* printf(\"Load default colormap \\n\"); */\n                  for (i=0; (i < (int) image->colors) && (i < 256); i++)\n                    {               \n                      image->colormap[i].red=ScaleCharToQuantum(WPG1_Palette[i].Red);\n                      image->colormap[i].green=ScaleCharToQuantum(WPG1_Palette[i].Green);\n                      image->colormap[i].blue=ScaleCharToQuantum(WPG1_Palette[i].Blue);\n                    }\n                }\n              else\n                {\n                  if (bpp < 24)\n                    if ( (image->colors < (one << bpp)) && (bpp != 24) )\n                      image->colormap=(PixelInfo *) ResizeQuantumMemory(\n                        image->colormap,(size_t) (one << bpp),\n                        sizeof(*image->colormap));\n                }\n          \n              if (bpp == 1)\n                {\n                  if(image->colormap[0].red==0 &&\n                     image->colormap[0].green==0 &&\n                     image->colormap[0].blue==0 &&\n                     image->colormap[1].red==0 &&\n                     image->colormap[1].green==0 &&\n                     image->colormap[1].blue==0)\n                    {  /* fix crippled monochrome palette */\n                      image->colormap[1].red =\n                        image->colormap[1].green =\n                        image->colormap[1].blue = QuantumRange;\n                    }\n                }      \n\n              if(UnpackWPGRaster(image,bpp,exception) < 0)\n                /* The raster cannot be unpacked */\n                {\n                DecompressionFailed:\n                  ThrowReaderException(CoderError,\"UnableToDecompressImage\");\n                    }\n\n              if(Rec.RecType==0x14 && BitmapHeader2.RotAngle!=0 && !image_info->ping)\n                {  \n                  /* flop command */\n                  if(BitmapHeader2.RotAngle & 0x8000)\n                    {\n                      Image\n                        *flop_image;\n\n                      flop_image = FlopImage(image, exception);\n                      if (flop_image != (Image *) NULL) {\n                        DuplicateBlob(flop_image,image);\n                        ReplaceImageInList(&image,flop_image);\n                      }\n                    }\n                  /* flip command */\n                  if(BitmapHeader2.RotAngle & 0x2000)\n                    {\n                      Image\n                        *flip_image;\n\n                      flip_image = FlipImage(image, exception);\n                      if (flip_image != (Image *) NULL) {\n                        DuplicateBlob(flip_image,image);\n                        ReplaceImageInList(&image,flip_image);\n                      }\n                    }\n                  /* rotate command */\n                  if(BitmapHeader2.RotAngle & 0x0FFF)\n                    {\n                      Image\n                        *rotate_image;\n\n                      rotate_image=RotateImage(image,(BitmapHeader2.RotAngle &\n                        0x0FFF), exception);\n                      if (rotate_image != (Image *) NULL) {\n                        DuplicateBlob(rotate_image,image);\n                        ReplaceImageInList(&image,rotate_image);\n                      }\n                    }\n                }\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image,exception);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x1B:  /* Postscript l2 */\n              if(Rec.RecordLength>0x3C)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+0x3C,   /* skip PS l2 header in the wpg */\n                  (ssize_t) Rec.RecordLength-0x3C,exception);\n              break;\n            }\n        }\n      break;\n\n    case 2:  /* WPG level 2 */\n      (void) memset(CTM,0,sizeof(CTM));\n      StartWPG.PosSizePrecision = 0;\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec2.Class=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rec2.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec2.Extension);\n          Rd_WP_DWORD(image,&Rec2.RecordLength);\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec2.RecordLength;\n\n          switch(Rec2.RecType)\n            {\n      case 1:\n              StartWPG.HorizontalUnits=ReadBlobLSBShort(image);\n              StartWPG.VerticalUnits=ReadBlobLSBShort(image);\n              StartWPG.PosSizePrecision=ReadBlobByte(image);\n              break;\n            case 0x0C:    /* Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (AcquireImageColormap(image,image->colors,exception) == MagickFalse)\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  (void) ReadBlobByte(image);   /*Opacity??*/\n                }\n              break;\n            case 0x0E:\n              Bitmap2Header1.Width=ReadBlobLSBShort(image);\n              Bitmap2Header1.Height=ReadBlobLSBShort(image);\n              if ((Bitmap2Header1.Width == 0) || (Bitmap2Header1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              Bitmap2Header1.Depth=ReadBlobByte(image);\n              Bitmap2Header1.Compression=ReadBlobByte(image);\n\n              if(Bitmap2Header1.Compression > 1)\n                continue; /*Unknown compression method */\n              switch(Bitmap2Header1.Depth)\n                {\n                case 1:\n                  bpp=1;\n                  break;\n                case 2:\n                  bpp=2;\n                  break;\n                case 3:\n                  bpp=4;\n                  break;\n                case 4:\n                  bpp=8;\n                  break;\n                case 8:\n                  bpp=24;\n                  break;\n                default:\n                  continue;  /*Ignore raster with unknown depth*/\n                }\n              image->columns=Bitmap2Header1.Width;\n              image->rows=Bitmap2Header1.Height;\n              status=SetImageExtent(image,image->columns,image->rows,exception);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors,exception))\n                    goto NoMemory;\n                }\n              else\n                {\n                  if(bpp < 24)\n                    if( image->colors<(one << bpp) && bpp!=24 )\n                      image->colormap=(PixelInfo *) ResizeQuantumMemory(\n                       image->colormap,(size_t) (one << bpp),\n                       sizeof(*image->colormap));\n                }\n\n\n              switch(Bitmap2Header1.Compression)\n                {\n                case 0:    /*Uncompressed raster*/\n                  {\n                    ldblk=(ssize_t) ((bpp*image->columns+7)/8);\n                    BImgBuff=(unsigned char *) AcquireQuantumMemory((size_t)\n                      ldblk+1,sizeof(*BImgBuff));\n                    if (BImgBuff == (unsigned char *) NULL)\n                      goto NoMemory;\n\n                    for(i=0; i< (ssize_t) image->rows; i++)\n                      {\n                        (void) ReadBlob(image,ldblk,BImgBuff);\n                        InsertRow(image,BImgBuff,i,bpp,exception);\n                      }\n\n                    if(BImgBuff)\n                      BImgBuff=(unsigned char *) RelinquishMagickMemory(BImgBuff);\n                    break;\n                  }\n                case 1:    /*RLE for WPG2 */\n                  {\n                    if( UnpackWPG2Raster(image,bpp,exception) < 0)\n                      goto DecompressionFailed;\n                    break;\n                  }   \n                }\n\n              if(CTM[0][0]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flop_image;\n\n                  flop_image = FlopImage(image, exception);\n                  if (flop_image != (Image *) NULL) {\n                    DuplicateBlob(flop_image,image);\n                    ReplaceImageInList(&image,flop_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     Tx(0,0)=-1;      Tx(1,0)=0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;      Tx(1,1)=1;   Tx(2,1)=0;\n                     Tx(0,2)=(WPG._2Rect.X_ur+WPG._2Rect.X_ll);\n                     Tx(1,2)=0;   Tx(2,2)=1; */\n                }\n              if(CTM[1][1]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flip_image;\n\n                   flip_image = FlipImage(image, exception);\n                   if (flip_image != (Image *) NULL) {\n                     DuplicateBlob(flip_image,image);\n                     ReplaceImageInList(&image,flip_image);\n                    }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     float_matrix Tx(3,3);\n                     Tx(0,0)= 1;   Tx(1,0)= 0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;   Tx(1,1)=-1;   Tx(2,1)=0;\n                     Tx(0,2)= 0;   Tx(1,2)=(WPG._2Rect.Y_ur+WPG._2Rect.Y_ll);\n                     Tx(2,2)=1; */\n              }\n\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image,exception);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x12:  /* Postscript WPG2*/\n        i=ReadBlobLSBShort(image);\n              if(Rec2.RecordLength > (unsigned int) i)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+i,    /*skip PS header in the wpg2*/\n                  (ssize_t) (Rec2.RecordLength-i-2),exception);\n              break;\n\n      case 0x1B:          /*bitmap rectangle*/\n              WPG2Flags = LoadWPG2Flags(image,StartWPG.PosSizePrecision,NULL,&CTM);\n              (void) WPG2Flags;\n              break;\n            }\n        }\n\n      break;\n\n    default:\n      {\n         ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n      }\n   }\n\n Finish:\n  (void) CloseBlob(image);\n\n  {\n    Image\n      *p;\n\n    ssize_t\n      scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n    /*\n      Fix scene numbers.\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=(size_t) scene++;\n  }\n  if (image == (Image *) NULL)\n    ThrowReaderException(CorruptImageError,\n      \"ImageFileDoesNotContainAnyImageData\");\n  return(image);\n}",
        "func": "static Image *ReadWPGImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n  typedef struct\n  {\n    size_t FileId;\n    MagickOffsetType DataOffset;\n    unsigned int ProductType;\n    unsigned int FileType;\n    unsigned char MajorVersion;\n    unsigned char MinorVersion;\n    unsigned int EncryptKey;\n    unsigned int Reserved;\n  } WPGHeader;\n\n  typedef struct\n  {\n    unsigned char RecType;\n    size_t RecordLength;\n  } WPGRecord;\n\n  typedef struct\n  {\n    unsigned char Class;\n    unsigned char RecType;\n    size_t Extension;\n    size_t RecordLength;\n  } WPG2Record;\n\n  typedef struct\n  {\n    unsigned  HorizontalUnits;\n    unsigned  VerticalUnits;\n    unsigned char PosSizePrecision;\n  } WPG2Start;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType1;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned char Depth;\n    unsigned char Compression;\n  } WPG2BitmapType1;\n\n  typedef struct\n  {\n    unsigned int RotAngle;\n    unsigned int LowLeftX;\n    unsigned int LowLeftY;\n    unsigned int UpRightX;\n    unsigned int UpRightY;\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType2;\n\n  typedef struct\n  {\n    unsigned int StartIndex;\n    unsigned int NumOfEntries;\n  } WPGColorMapRec;\n\n  /*\n  typedef struct {\n    size_t PS_unknown1;\n    unsigned int PS_unknown2;\n    unsigned int PS_unknown3;\n  } WPGPSl1Record;  \n  */\n\n  Image\n    *image;\n\n  unsigned int\n    status;\n\n  WPGHeader\n    Header;\n\n  WPGRecord\n    Rec;\n\n  WPG2Record\n    Rec2;\n\n  WPG2Start StartWPG;\n\n  WPGBitmapType1\n    BitmapHeader1;\n\n  WPG2BitmapType1\n    Bitmap2Header1;\n\n  WPGBitmapType2\n    BitmapHeader2;\n\n  WPGColorMapRec\n    WPG_Palette;\n\n  int\n    i,\n    bpp,\n    WPG2Flags;\n\n  ssize_t\n    ldblk;\n\n  size_t\n    one;\n\n  unsigned char\n    *BImgBuff;\n\n  tCTM CTM;         /*current transform matrix*/\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  one=1;\n  image=AcquireImage(image_info,exception);\n  image->depth=8;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read WPG image.\n  */\n  Header.FileId=ReadBlobLSBLong(image);\n  Header.DataOffset=(MagickOffsetType) ReadBlobLSBLong(image);\n  Header.ProductType=ReadBlobLSBShort(image);\n  Header.FileType=ReadBlobLSBShort(image);\n  Header.MajorVersion=ReadBlobByte(image);\n  Header.MinorVersion=ReadBlobByte(image);\n  Header.EncryptKey=ReadBlobLSBShort(image);\n  Header.Reserved=ReadBlobLSBShort(image);\n\n  if (Header.FileId!=0x435057FF || (Header.ProductType>>8)!=0x16)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  if (Header.EncryptKey!=0)\n    ThrowReaderException(CoderError,\"EncryptedWPGImageFileNotSupported\");\n\n  image->columns = 1;\n  image->rows = 1;\n  image->colors = 0;\n  bpp=0;\n  BitmapHeader2.RotAngle=0;\n  Rec2.RecordLength=0;\n\n  switch(Header.FileType)\n    {\n    case 1:     /* WPG level 1 */\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec.RecordLength);\n          if (Rec.RecordLength > GetBlobSize(image))\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec.RecordLength;\n\n          switch(Rec.RecType)\n            {\n            case 0x0B: /* bitmap type 1 */\n              BitmapHeader1.Width=ReadBlobLSBShort(image);\n              BitmapHeader1.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader1.Width == 0) || (BitmapHeader1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader1.Depth=ReadBlobLSBShort(image);\n              BitmapHeader1.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader1.VertRes=ReadBlobLSBShort(image);\n\n              if(BitmapHeader1.HorzRes && BitmapHeader1.VertRes)\n                {\n                  image->units=PixelsPerCentimeterResolution;\n                  image->resolution.x=BitmapHeader1.HorzRes/470.0;\n                  image->resolution.y=BitmapHeader1.VertRes/470.0;\n                }\n              image->columns=BitmapHeader1.Width;\n              image->rows=BitmapHeader1.Height;\n              bpp=BitmapHeader1.Depth;\n\n              goto UnpackRaster;\n\n            case 0x0E:  /*Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (!AcquireImageColormap(image,image->colors,exception))\n                goto NoMemory;\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                }\n              break;\n     \n            case 0x11:  /* Start PS l1 */\n              if(Rec.RecordLength > 8)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+8,   /* skip PS header in the wpg */\n                  (ssize_t) Rec.RecordLength-8,exception);\n              break;     \n\n            case 0x14:  /* bitmap type 2 */\n              BitmapHeader2.RotAngle=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftX=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftY=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightX=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightY=ReadBlobLSBShort(image);\n              BitmapHeader2.Width=ReadBlobLSBShort(image);\n              BitmapHeader2.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader2.Width == 0) || (BitmapHeader2.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader2.Depth=ReadBlobLSBShort(image);\n              BitmapHeader2.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader2.VertRes=ReadBlobLSBShort(image);\n\n              image->units=PixelsPerCentimeterResolution;\n              image->page.width=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightX)/470.0);\n              image->page.height=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightY)/470.0);\n              image->page.x=(int) (BitmapHeader2.LowLeftX/470.0);\n              image->page.y=(int) (BitmapHeader2.LowLeftX/470.0);\n              if(BitmapHeader2.HorzRes && BitmapHeader2.VertRes)\n                {\n                  image->resolution.x=BitmapHeader2.HorzRes/470.0;\n                  image->resolution.y=BitmapHeader2.VertRes/470.0;\n                }\n              image->columns=BitmapHeader2.Width;\n              image->rows=BitmapHeader2.Height;\n              bpp=BitmapHeader2.Depth;\n\n            UnpackRaster:      \n              status=SetImageExtent(image,image->columns,image->rows,exception);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors,exception))\n                    {\n                    NoMemory:\n                      ThrowReaderException(ResourceLimitError,\n                        \"MemoryAllocationFailed\");\n                    }\n                  /* printf(\"Load default colormap \\n\"); */\n                  for (i=0; (i < (int) image->colors) && (i < 256); i++)\n                    {               \n                      image->colormap[i].red=ScaleCharToQuantum(WPG1_Palette[i].Red);\n                      image->colormap[i].green=ScaleCharToQuantum(WPG1_Palette[i].Green);\n                      image->colormap[i].blue=ScaleCharToQuantum(WPG1_Palette[i].Blue);\n                    }\n                }\n              else\n                {\n                  if (bpp < 24)\n                    if ( (image->colors < (one << bpp)) && (bpp != 24) )\n                      image->colormap=(PixelInfo *) ResizeQuantumMemory(\n                        image->colormap,(size_t) (one << bpp),\n                        sizeof(*image->colormap));\n                }\n          \n              if (bpp == 1)\n                {\n                  if(image->colormap[0].red==0 &&\n                     image->colormap[0].green==0 &&\n                     image->colormap[0].blue==0 &&\n                     image->colormap[1].red==0 &&\n                     image->colormap[1].green==0 &&\n                     image->colormap[1].blue==0)\n                    {  /* fix crippled monochrome palette */\n                      image->colormap[1].red =\n                        image->colormap[1].green =\n                        image->colormap[1].blue = QuantumRange;\n                    }\n                }      \n\n              if(UnpackWPGRaster(image,bpp,exception) < 0)\n                /* The raster cannot be unpacked */\n                {\n                DecompressionFailed:\n                  ThrowReaderException(CoderError,\"UnableToDecompressImage\");\n                    }\n\n              if(Rec.RecType==0x14 && BitmapHeader2.RotAngle!=0 && !image_info->ping)\n                {  \n                  /* flop command */\n                  if(BitmapHeader2.RotAngle & 0x8000)\n                    {\n                      Image\n                        *flop_image;\n\n                      flop_image = FlopImage(image, exception);\n                      if (flop_image != (Image *) NULL) {\n                        DuplicateBlob(flop_image,image);\n                        ReplaceImageInList(&image,flop_image);\n                      }\n                    }\n                  /* flip command */\n                  if(BitmapHeader2.RotAngle & 0x2000)\n                    {\n                      Image\n                        *flip_image;\n\n                      flip_image = FlipImage(image, exception);\n                      if (flip_image != (Image *) NULL) {\n                        DuplicateBlob(flip_image,image);\n                        ReplaceImageInList(&image,flip_image);\n                      }\n                    }\n                  /* rotate command */\n                  if(BitmapHeader2.RotAngle & 0x0FFF)\n                    {\n                      Image\n                        *rotate_image;\n\n                      rotate_image=RotateImage(image,(BitmapHeader2.RotAngle &\n                        0x0FFF), exception);\n                      if (rotate_image != (Image *) NULL) {\n                        DuplicateBlob(rotate_image,image);\n                        ReplaceImageInList(&image,rotate_image);\n                      }\n                    }\n                }\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image,exception);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x1B:  /* Postscript l2 */\n              if(Rec.RecordLength>0x3C)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+0x3C,   /* skip PS l2 header in the wpg */\n                  (ssize_t) Rec.RecordLength-0x3C,exception);\n              break;\n            }\n        }\n      break;\n\n    case 2:  /* WPG level 2 */\n      (void) memset(CTM,0,sizeof(CTM));\n      StartWPG.PosSizePrecision = 0;\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec2.Class=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rec2.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec2.Extension);\n          Rd_WP_DWORD(image,&Rec2.RecordLength);\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec2.RecordLength;\n\n          switch(Rec2.RecType)\n            {\n      case 1:\n              StartWPG.HorizontalUnits=ReadBlobLSBShort(image);\n              StartWPG.VerticalUnits=ReadBlobLSBShort(image);\n              StartWPG.PosSizePrecision=ReadBlobByte(image);\n              break;\n            case 0x0C:    /* Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (AcquireImageColormap(image,image->colors,exception) == MagickFalse)\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  (void) ReadBlobByte(image);   /*Opacity??*/\n                }\n              break;\n            case 0x0E:\n              Bitmap2Header1.Width=ReadBlobLSBShort(image);\n              Bitmap2Header1.Height=ReadBlobLSBShort(image);\n              if ((Bitmap2Header1.Width == 0) || (Bitmap2Header1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              Bitmap2Header1.Depth=ReadBlobByte(image);\n              Bitmap2Header1.Compression=ReadBlobByte(image);\n\n              if(Bitmap2Header1.Compression > 1)\n                continue; /*Unknown compression method */\n              switch(Bitmap2Header1.Depth)\n                {\n                case 1:\n                  bpp=1;\n                  break;\n                case 2:\n                  bpp=2;\n                  break;\n                case 3:\n                  bpp=4;\n                  break;\n                case 4:\n                  bpp=8;\n                  break;\n                case 8:\n                  bpp=24;\n                  break;\n                default:\n                  continue;  /*Ignore raster with unknown depth*/\n                }\n              image->columns=Bitmap2Header1.Width;\n              image->rows=Bitmap2Header1.Height;\n              status=SetImageExtent(image,image->columns,image->rows,exception);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors,exception))\n                    goto NoMemory;\n                }\n              else\n                {\n                  if(bpp < 24)\n                    if( image->colors<(one << bpp) && bpp!=24 )\n                      image->colormap=(PixelInfo *) ResizeQuantumMemory(\n                       image->colormap,(size_t) (one << bpp),\n                       sizeof(*image->colormap));\n                }\n\n\n              switch(Bitmap2Header1.Compression)\n                {\n                case 0:    /*Uncompressed raster*/\n                  {\n                    ldblk=(ssize_t) ((bpp*image->columns+7)/8);\n                    BImgBuff=(unsigned char *) AcquireQuantumMemory((size_t)\n                      ldblk+1,sizeof(*BImgBuff));\n                    if (BImgBuff == (unsigned char *) NULL)\n                      goto NoMemory;\n\n                    for(i=0; i< (ssize_t) image->rows; i++)\n                      {\n                        (void) ReadBlob(image,ldblk,BImgBuff);\n                        InsertRow(image,BImgBuff,i,bpp,exception);\n                      }\n\n                    if(BImgBuff)\n                      BImgBuff=(unsigned char *) RelinquishMagickMemory(BImgBuff);\n                    break;\n                  }\n                case 1:    /*RLE for WPG2 */\n                  {\n                    if( UnpackWPG2Raster(image,bpp,exception) < 0)\n                      goto DecompressionFailed;\n                    break;\n                  }   \n                }\n\n              if(CTM[0][0]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flop_image;\n\n                  flop_image = FlopImage(image, exception);\n                  if (flop_image != (Image *) NULL) {\n                    DuplicateBlob(flop_image,image);\n                    ReplaceImageInList(&image,flop_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     Tx(0,0)=-1;      Tx(1,0)=0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;      Tx(1,1)=1;   Tx(2,1)=0;\n                     Tx(0,2)=(WPG._2Rect.X_ur+WPG._2Rect.X_ll);\n                     Tx(1,2)=0;   Tx(2,2)=1; */\n                }\n              if(CTM[1][1]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flip_image;\n\n                   flip_image = FlipImage(image, exception);\n                   if (flip_image != (Image *) NULL) {\n                     DuplicateBlob(flip_image,image);\n                     ReplaceImageInList(&image,flip_image);\n                    }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     float_matrix Tx(3,3);\n                     Tx(0,0)= 1;   Tx(1,0)= 0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;   Tx(1,1)=-1;   Tx(2,1)=0;\n                     Tx(0,2)= 0;   Tx(1,2)=(WPG._2Rect.Y_ur+WPG._2Rect.Y_ll);\n                     Tx(2,2)=1; */\n              }\n\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image,exception);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x12:  /* Postscript WPG2*/\n        i=ReadBlobLSBShort(image);\n              if(Rec2.RecordLength > (unsigned int) i)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+i,    /*skip PS header in the wpg2*/\n                  (ssize_t) (Rec2.RecordLength-i-2),exception);\n              break;\n\n      case 0x1B:          /*bitmap rectangle*/\n              WPG2Flags = LoadWPG2Flags(image,StartWPG.PosSizePrecision,NULL,&CTM);\n              (void) WPG2Flags;\n              break;\n            }\n        }\n\n      break;\n\n    default:\n      {\n         ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n      }\n   }\n\n Finish:\n  (void) CloseBlob(image);\n\n  {\n    Image\n      *p;\n\n    ssize_t\n      scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n    /*\n      Fix scene numbers.\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=(size_t) scene++;\n  }\n  if (image == (Image *) NULL)\n    ThrowReaderException(CorruptImageError,\n      \"ImageFileDoesNotContainAnyImageData\");\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -177,6 +177,8 @@\n           if(i==EOF)\n             break;\n           Rd_WP_DWORD(image,&Rec.RecordLength);\n+          if (Rec.RecordLength > GetBlobSize(image))\n+            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n           if(EOFBlob(image))\n             break;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "          if (Rec.RecordLength > GetBlobSize(image))",
                "            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-14342",
        "func_name": "ImageMagick/ReadWPGImage",
        "description": "ImageMagick 7.0.6-6 has a memory exhaustion vulnerability in ReadWPGImage in coders/wpg.c via a crafted wpg image file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/6d5b22baedd49ef8a35011789bd600762ce1ef21",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/650",
        "commit_text": "",
        "func_before": "static Image *ReadWPGImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n  typedef struct\n  {\n    size_t FileId;\n    MagickOffsetType DataOffset;\n    unsigned int ProductType;\n    unsigned int FileType;\n    unsigned char MajorVersion;\n    unsigned char MinorVersion;\n    unsigned int EncryptKey;\n    unsigned int Reserved;\n  } WPGHeader;\n\n  typedef struct\n  {\n    unsigned char RecType;\n    size_t RecordLength;\n  } WPGRecord;\n\n  typedef struct\n  {\n    unsigned char Class;\n    unsigned char RecType;\n    size_t Extension;\n    size_t RecordLength;\n  } WPG2Record;\n\n  typedef struct\n  {\n    unsigned  HorizontalUnits;\n    unsigned  VerticalUnits;\n    unsigned char PosSizePrecision;\n  } WPG2Start;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType1;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned char Depth;\n    unsigned char Compression;\n  } WPG2BitmapType1;\n\n  typedef struct\n  {\n    unsigned int RotAngle;\n    unsigned int LowLeftX;\n    unsigned int LowLeftY;\n    unsigned int UpRightX;\n    unsigned int UpRightY;\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType2;\n\n  typedef struct\n  {\n    unsigned int StartIndex;\n    unsigned int NumOfEntries;\n  } WPGColorMapRec;\n\n  /*\n  typedef struct {\n    size_t PS_unknown1;\n    unsigned int PS_unknown2;\n    unsigned int PS_unknown3;\n  } WPGPSl1Record;\n  */\n\n  Image\n    *image;\n\n  unsigned int\n    status;\n\n  WPGHeader\n    Header;\n\n  WPGRecord\n    Rec;\n\n  WPG2Record\n    Rec2;\n\n  WPG2Start StartWPG;\n\n  WPGBitmapType1\n    BitmapHeader1;\n\n  WPG2BitmapType1\n    Bitmap2Header1;\n\n  WPGBitmapType2\n    BitmapHeader2;\n\n  WPGColorMapRec\n    WPG_Palette;\n\n  int\n    i,\n    bpp,\n    WPG2Flags;\n\n  ssize_t\n    ldblk;\n\n  size_t\n    one;\n\n  unsigned char\n    *BImgBuff;\n\n  tCTM CTM;         /*current transform matrix*/\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  one=1;\n  image=AcquireImage(image_info);\n  image->depth=8;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read WPG image.\n  */\n  Header.FileId=ReadBlobLSBLong(image);\n  Header.DataOffset=(MagickOffsetType) ReadBlobLSBLong(image);\n  Header.ProductType=ReadBlobLSBShort(image);\n  Header.FileType=ReadBlobLSBShort(image);\n  Header.MajorVersion=ReadBlobByte(image);\n  Header.MinorVersion=ReadBlobByte(image);\n  Header.EncryptKey=ReadBlobLSBShort(image);\n  Header.Reserved=ReadBlobLSBShort(image);\n\n  if (Header.FileId!=0x435057FF || (Header.ProductType>>8)!=0x16)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  if (Header.EncryptKey!=0)\n    ThrowReaderException(CoderError,\"EncryptedWPGImageFileNotSupported\");\n\n  image->columns = 1;\n  image->rows = 1;\n  image->colors = 0;\n  bpp=0;\n  BitmapHeader2.RotAngle=0;\n  Rec2.RecordLength = 0;\n\n  switch(Header.FileType)\n    {\n    case 1:     /* WPG level 1 */\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec.RecordLength);\n          if (Rec.RecordLength > GetBlobSize(image))\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec.RecordLength;\n\n          switch(Rec.RecType)\n            {\n            case 0x0B: /* bitmap type 1 */\n              BitmapHeader1.Width=ReadBlobLSBShort(image);\n              BitmapHeader1.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader1.Width == 0) || (BitmapHeader1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader1.Depth=ReadBlobLSBShort(image);\n              BitmapHeader1.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader1.VertRes=ReadBlobLSBShort(image);\n\n              if(BitmapHeader1.HorzRes && BitmapHeader1.VertRes)\n                {\n                  image->units=PixelsPerCentimeterResolution;\n                  image->x_resolution=BitmapHeader1.HorzRes/470.0;\n                  image->y_resolution=BitmapHeader1.VertRes/470.0;\n                }\n              image->columns=BitmapHeader1.Width;\n              image->rows=BitmapHeader1.Height;\n              bpp=BitmapHeader1.Depth;\n\n              goto UnpackRaster;\n\n            case 0x0E:  /*Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (!AcquireImageColormap(image,image->colors))\n                goto NoMemory;\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                }\n              break;\n\n            case 0x11:  /* Start PS l1 */\n              if(Rec.RecordLength > 8)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+8,   /* skip PS header in the wpg */\n                  (ssize_t) Rec.RecordLength-8,exception);\n              break;\n\n            case 0x14:  /* bitmap type 2 */\n              BitmapHeader2.RotAngle=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftX=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftY=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightX=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightY=ReadBlobLSBShort(image);\n              BitmapHeader2.Width=ReadBlobLSBShort(image);\n              BitmapHeader2.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader2.Width == 0) || (BitmapHeader2.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader2.Depth=ReadBlobLSBShort(image);\n              BitmapHeader2.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader2.VertRes=ReadBlobLSBShort(image);\n\n              image->units=PixelsPerCentimeterResolution;\n              image->page.width=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightX)/470.0);\n              image->page.height=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightY)/470.0);\n              image->page.x=(int) (BitmapHeader2.LowLeftX/470.0);\n              image->page.y=(int) (BitmapHeader2.LowLeftX/470.0);\n              if(BitmapHeader2.HorzRes && BitmapHeader2.VertRes)\n                {\n                  image->x_resolution=BitmapHeader2.HorzRes/470.0;\n                  image->y_resolution=BitmapHeader2.VertRes/470.0;\n                }\n              image->columns=BitmapHeader2.Width;\n              image->rows=BitmapHeader2.Height;\n              bpp=BitmapHeader2.Depth;\n\n            UnpackRaster:\n              status=SetImageExtent(image,image->columns,image->rows);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors))\n                    {\n                    NoMemory:\n                      ThrowReaderException(ResourceLimitError,\n                        \"MemoryAllocationFailed\");\n                    }\n                  /* printf(\"Load default colormap \\n\"); */\n                  for (i=0; (i < (int) image->colors) && (i < 256); i++)\n                    {\n                      image->colormap[i].red=ScaleCharToQuantum(WPG1_Palette[i].Red);\n                      image->colormap[i].green=ScaleCharToQuantum(WPG1_Palette[i].Green);\n                      image->colormap[i].blue=ScaleCharToQuantum(WPG1_Palette[i].Blue);\n                    }\n                }\n              else\n                {\n                  if (bpp < 24)\n                    if ( (image->colors < (one << bpp)) && (bpp != 24) )\n                      image->colormap=(PixelPacket *) ResizeQuantumMemory(\n                        image->colormap,(size_t) (one << bpp),\n                        sizeof(*image->colormap));\n                }\n\n              if (bpp == 1)\n                {\n                  if(image->colormap[0].red==0 &&\n                     image->colormap[0].green==0 &&\n                     image->colormap[0].blue==0 &&\n                     image->colormap[1].red==0 &&\n                     image->colormap[1].green==0 &&\n                     image->colormap[1].blue==0)\n                    {  /* fix crippled monochrome palette */\n                      image->colormap[1].red =\n                        image->colormap[1].green =\n                        image->colormap[1].blue = QuantumRange;\n                    }\n                }\n\n              if(UnpackWPGRaster(image,bpp) < 0)\n                /* The raster cannot be unpacked */\n                {\n                DecompressionFailed:\n                  ThrowReaderException(CoderError,\"UnableToDecompressImage\");\n                    }\n\n              if(Rec.RecType==0x14 && BitmapHeader2.RotAngle!=0 && !image_info->ping)\n                {\n                  /* flop command */\n                  if(BitmapHeader2.RotAngle & 0x8000)\n                    {\n                      Image\n                        *flop_image;\n\n                      flop_image = FlopImage(image, exception);\n                      if (flop_image != (Image *) NULL) {\n                        DuplicateBlob(flop_image,image);\n                        ReplaceImageInList(&image,flop_image);\n                      }\n                    }\n                  /* flip command */\n                  if(BitmapHeader2.RotAngle & 0x2000)\n                    {\n                      Image\n                        *flip_image;\n\n                      flip_image = FlipImage(image, exception);\n                      if (flip_image != (Image *) NULL) {\n                        DuplicateBlob(flip_image,image);\n                        ReplaceImageInList(&image,flip_image);\n                      }\n                    }\n                  /* rotate command */\n                  if(BitmapHeader2.RotAngle & 0x0FFF)\n                    {\n                      Image\n                        *rotate_image;\n\n                      rotate_image=RotateImage(image,(BitmapHeader2.RotAngle &\n                        0x0FFF), exception);\n                      if (rotate_image != (Image *) NULL) {\n                        DuplicateBlob(rotate_image,image);\n                        ReplaceImageInList(&image,rotate_image);\n                      }\n                    }\n                }\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x1B:  /* Postscript l2 */\n              if(Rec.RecordLength>0x3C)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+0x3C,   /* skip PS l2 header in the wpg */\n                  (ssize_t) Rec.RecordLength-0x3C,exception);\n              break;\n            }\n        }\n      break;\n\n    case 2:  /* WPG level 2 */\n      (void) memset(CTM,0,sizeof(CTM));\n      StartWPG.PosSizePrecision = 0;\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec2.Class=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rec2.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec2.Extension);\n          Rd_WP_DWORD(image,&Rec2.RecordLength);\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec2.RecordLength;\n\n          switch(Rec2.RecType)\n            {\n      case 1:\n              StartWPG.HorizontalUnits=ReadBlobLSBShort(image);\n              StartWPG.VerticalUnits=ReadBlobLSBShort(image);\n              StartWPG.PosSizePrecision=ReadBlobByte(image);\n              break;\n            case 0x0C:    /* Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (AcquireImageColormap(image,image->colors) == MagickFalse)\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  (void) ReadBlobByte(image);   /*Opacity??*/\n                }\n              break;\n            case 0x0E:\n              Bitmap2Header1.Width=ReadBlobLSBShort(image);\n              Bitmap2Header1.Height=ReadBlobLSBShort(image);\n              if ((Bitmap2Header1.Width == 0) || (Bitmap2Header1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              Bitmap2Header1.Depth=ReadBlobByte(image);\n              Bitmap2Header1.Compression=ReadBlobByte(image);\n\n              if(Bitmap2Header1.Compression > 1)\n                continue; /*Unknown compression method */\n              switch(Bitmap2Header1.Depth)\n                {\n                case 1:\n                  bpp=1;\n                  break;\n                case 2:\n                  bpp=2;\n                  break;\n                case 3:\n                  bpp=4;\n                  break;\n                case 4:\n                  bpp=8;\n                  break;\n                case 8:\n                  bpp=24;\n                  break;\n                default:\n                  continue;  /*Ignore raster with unknown depth*/\n                }\n              image->columns=Bitmap2Header1.Width;\n              image->rows=Bitmap2Header1.Height;\n              status=SetImageExtent(image,image->columns,image->rows);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  size_t\n                    one;\n\n                  one=1;\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors))\n                    goto NoMemory;\n                }\n              else\n                {\n                  if(bpp < 24)\n                    if( image->colors<(one << bpp) && bpp!=24 )\n                      image->colormap=(PixelPacket *) ResizeQuantumMemory(\n                       image->colormap,(size_t) (one << bpp),\n                       sizeof(*image->colormap));\n                }\n\n\n              switch(Bitmap2Header1.Compression)\n                {\n                case 0:    /*Uncompressed raster*/\n                  {\n                    ldblk=(ssize_t) ((bpp*image->columns+7)/8);\n                    BImgBuff=(unsigned char *) AcquireQuantumMemory((size_t)\n                      ldblk+1,sizeof(*BImgBuff));\n                    if (BImgBuff == (unsigned char *) NULL)\n                      goto NoMemory;\n\n                    for(i=0; i< (ssize_t) image->rows; i++)\n                      {\n                        (void) ReadBlob(image,ldblk,BImgBuff);\n                        InsertRow(BImgBuff,i,image,bpp);\n                      }\n\n                    if(BImgBuff)\n                      BImgBuff=(unsigned char *) RelinquishMagickMemory(BImgBuff);\n                    break;\n                  }\n                case 1:    /*RLE for WPG2 */\n                  {\n                    if( UnpackWPG2Raster(image,bpp) < 0)\n                      goto DecompressionFailed;\n                    break;\n                  }\n                }\n\n              if(CTM[0][0]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flop_image;\n\n                  flop_image = FlopImage(image, exception);\n                  if (flop_image != (Image *) NULL) {\n                    DuplicateBlob(flop_image,image);\n                    ReplaceImageInList(&image,flop_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     Tx(0,0)=-1;      Tx(1,0)=0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;      Tx(1,1)=1;   Tx(2,1)=0;\n                     Tx(0,2)=(WPG._2Rect.X_ur+WPG._2Rect.X_ll);\n                     Tx(1,2)=0;   Tx(2,2)=1; */\n                }\n              if(CTM[1][1]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flip_image;\n\n                  flip_image = FlipImage(image, exception);\n                  if (flip_image != (Image *) NULL) {\n                    DuplicateBlob(flip_image,image);\n                    ReplaceImageInList(&image,flip_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     float_matrix Tx(3,3);\n                     Tx(0,0)= 1;   Tx(1,0)= 0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;   Tx(1,1)=-1;   Tx(2,1)=0;\n                     Tx(0,2)= 0;   Tx(1,2)=(WPG._2Rect.Y_ur+WPG._2Rect.Y_ll);\n                     Tx(2,2)=1; */\n                }\n\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x12:  /* Postscript WPG2*/\n        i=ReadBlobLSBShort(image);\n              if(Rec2.RecordLength > (unsigned int) i)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+i,    /*skip PS header in the wpg2*/\n                  (ssize_t) (Rec2.RecordLength-i-2),exception);\n              break;\n\n      case 0x1B:          /*bitmap rectangle*/\n              WPG2Flags = LoadWPG2Flags(image,StartWPG.PosSizePrecision,NULL,&CTM);\n              (void) WPG2Flags;\n              break;\n            }\n        }\n\n      break;\n\n    default:\n      {\n         ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n      }\n   }\n\n Finish:\n  (void) CloseBlob(image);\n\n  {\n    Image\n      *p;\n\n    ssize_t\n      scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n    /*\n      Fix scene numbers.\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=(size_t) scene++;\n  }\n  if (image == (Image *) NULL)\n    ThrowReaderException(CorruptImageError,\n      \"ImageFileDoesNotContainAnyImageData\");\n  return(image);\n}",
        "func": "static Image *ReadWPGImage(const ImageInfo *image_info,\n  ExceptionInfo *exception)\n{\n  typedef struct\n  {\n    size_t FileId;\n    MagickOffsetType DataOffset;\n    unsigned int ProductType;\n    unsigned int FileType;\n    unsigned char MajorVersion;\n    unsigned char MinorVersion;\n    unsigned int EncryptKey;\n    unsigned int Reserved;\n  } WPGHeader;\n\n  typedef struct\n  {\n    unsigned char RecType;\n    size_t RecordLength;\n  } WPGRecord;\n\n  typedef struct\n  {\n    unsigned char Class;\n    unsigned char RecType;\n    size_t Extension;\n    size_t RecordLength;\n  } WPG2Record;\n\n  typedef struct\n  {\n    unsigned  HorizontalUnits;\n    unsigned  VerticalUnits;\n    unsigned char PosSizePrecision;\n  } WPG2Start;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType1;\n\n  typedef struct\n  {\n    unsigned int Width;\n    unsigned int Height;\n    unsigned char Depth;\n    unsigned char Compression;\n  } WPG2BitmapType1;\n\n  typedef struct\n  {\n    unsigned int RotAngle;\n    unsigned int LowLeftX;\n    unsigned int LowLeftY;\n    unsigned int UpRightX;\n    unsigned int UpRightY;\n    unsigned int Width;\n    unsigned int Height;\n    unsigned int Depth;\n    unsigned int HorzRes;\n    unsigned int VertRes;\n  } WPGBitmapType2;\n\n  typedef struct\n  {\n    unsigned int StartIndex;\n    unsigned int NumOfEntries;\n  } WPGColorMapRec;\n\n  /*\n  typedef struct {\n    size_t PS_unknown1;\n    unsigned int PS_unknown2;\n    unsigned int PS_unknown3;\n  } WPGPSl1Record;\n  */\n\n  Image\n    *image;\n\n  unsigned int\n    status;\n\n  WPGHeader\n    Header;\n\n  WPGRecord\n    Rec;\n\n  WPG2Record\n    Rec2;\n\n  WPG2Start StartWPG;\n\n  WPGBitmapType1\n    BitmapHeader1;\n\n  WPG2BitmapType1\n    Bitmap2Header1;\n\n  WPGBitmapType2\n    BitmapHeader2;\n\n  WPGColorMapRec\n    WPG_Palette;\n\n  int\n    i,\n    bpp,\n    WPG2Flags;\n\n  ssize_t\n    ldblk;\n\n  size_t\n    one;\n\n  unsigned char\n    *BImgBuff;\n\n  tCTM CTM;         /*current transform matrix*/\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  one=1;\n  image=AcquireImage(image_info);\n  image->depth=8;\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read WPG image.\n  */\n  Header.FileId=ReadBlobLSBLong(image);\n  Header.DataOffset=(MagickOffsetType) ReadBlobLSBLong(image);\n  Header.ProductType=ReadBlobLSBShort(image);\n  Header.FileType=ReadBlobLSBShort(image);\n  Header.MajorVersion=ReadBlobByte(image);\n  Header.MinorVersion=ReadBlobByte(image);\n  Header.EncryptKey=ReadBlobLSBShort(image);\n  Header.Reserved=ReadBlobLSBShort(image);\n\n  if (Header.FileId!=0x435057FF || (Header.ProductType>>8)!=0x16)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  if (Header.EncryptKey!=0)\n    ThrowReaderException(CoderError,\"EncryptedWPGImageFileNotSupported\");\n\n  image->columns = 1;\n  image->rows = 1;\n  image->colors = 0;\n  bpp=0;\n  BitmapHeader2.RotAngle=0;\n  Rec2.RecordLength = 0;\n\n  switch(Header.FileType)\n    {\n    case 1:     /* WPG level 1 */\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec.RecordLength);\n          if (Rec.RecordLength > GetBlobSize(image))\n            ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec.RecordLength;\n\n          switch(Rec.RecType)\n            {\n            case 0x0B: /* bitmap type 1 */\n              BitmapHeader1.Width=ReadBlobLSBShort(image);\n              BitmapHeader1.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader1.Width == 0) || (BitmapHeader1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader1.Depth=ReadBlobLSBShort(image);\n              BitmapHeader1.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader1.VertRes=ReadBlobLSBShort(image);\n\n              if(BitmapHeader1.HorzRes && BitmapHeader1.VertRes)\n                {\n                  image->units=PixelsPerCentimeterResolution;\n                  image->x_resolution=BitmapHeader1.HorzRes/470.0;\n                  image->y_resolution=BitmapHeader1.VertRes/470.0;\n                }\n              image->columns=BitmapHeader1.Width;\n              image->rows=BitmapHeader1.Height;\n              bpp=BitmapHeader1.Depth;\n\n              goto UnpackRaster;\n\n            case 0x0E:  /*Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (!AcquireImageColormap(image,image->colors))\n                goto NoMemory;\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((unsigned char)\n                    ReadBlobByte(image));\n                }\n              break;\n\n            case 0x11:  /* Start PS l1 */\n              if(Rec.RecordLength > 8)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+8,   /* skip PS header in the wpg */\n                  (ssize_t) Rec.RecordLength-8,exception);\n              break;\n\n            case 0x14:  /* bitmap type 2 */\n              BitmapHeader2.RotAngle=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftX=ReadBlobLSBShort(image);\n              BitmapHeader2.LowLeftY=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightX=ReadBlobLSBShort(image);\n              BitmapHeader2.UpRightY=ReadBlobLSBShort(image);\n              BitmapHeader2.Width=ReadBlobLSBShort(image);\n              BitmapHeader2.Height=ReadBlobLSBShort(image);\n              if ((BitmapHeader2.Width == 0) || (BitmapHeader2.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              BitmapHeader2.Depth=ReadBlobLSBShort(image);\n              BitmapHeader2.HorzRes=ReadBlobLSBShort(image);\n              BitmapHeader2.VertRes=ReadBlobLSBShort(image);\n\n              image->units=PixelsPerCentimeterResolution;\n              image->page.width=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightX)/470.0);\n              image->page.height=(unsigned int)\n                ((BitmapHeader2.LowLeftX-BitmapHeader2.UpRightY)/470.0);\n              image->page.x=(int) (BitmapHeader2.LowLeftX/470.0);\n              image->page.y=(int) (BitmapHeader2.LowLeftX/470.0);\n              if(BitmapHeader2.HorzRes && BitmapHeader2.VertRes)\n                {\n                  image->x_resolution=BitmapHeader2.HorzRes/470.0;\n                  image->y_resolution=BitmapHeader2.VertRes/470.0;\n                }\n              image->columns=BitmapHeader2.Width;\n              image->rows=BitmapHeader2.Height;\n              bpp=BitmapHeader2.Depth;\n\n            UnpackRaster:\n              status=SetImageExtent(image,image->columns,image->rows);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp <= 16))\n                {\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors))\n                    {\n                    NoMemory:\n                      ThrowReaderException(ResourceLimitError,\n                        \"MemoryAllocationFailed\");\n                    }\n                  /* printf(\"Load default colormap \\n\"); */\n                  for (i=0; (i < (int) image->colors) && (i < 256); i++)\n                    {\n                      image->colormap[i].red=ScaleCharToQuantum(WPG1_Palette[i].Red);\n                      image->colormap[i].green=ScaleCharToQuantum(WPG1_Palette[i].Green);\n                      image->colormap[i].blue=ScaleCharToQuantum(WPG1_Palette[i].Blue);\n                    }\n                }\n              else\n                {\n                  if (bpp < 24)\n                    if ( (image->colors < (one << bpp)) && (bpp != 24) )\n                      image->colormap=(PixelPacket *) ResizeQuantumMemory(\n                        image->colormap,(size_t) (one << bpp),\n                        sizeof(*image->colormap));\n                }\n\n              if (bpp == 1)\n                {\n                  if(image->colormap[0].red==0 &&\n                     image->colormap[0].green==0 &&\n                     image->colormap[0].blue==0 &&\n                     image->colormap[1].red==0 &&\n                     image->colormap[1].green==0 &&\n                     image->colormap[1].blue==0)\n                    {  /* fix crippled monochrome palette */\n                      image->colormap[1].red =\n                        image->colormap[1].green =\n                        image->colormap[1].blue = QuantumRange;\n                    }\n                }\n\n              if(UnpackWPGRaster(image,bpp) < 0)\n                /* The raster cannot be unpacked */\n                {\n                DecompressionFailed:\n                  ThrowReaderException(CoderError,\"UnableToDecompressImage\");\n                    }\n\n              if(Rec.RecType==0x14 && BitmapHeader2.RotAngle!=0 && !image_info->ping)\n                {\n                  /* flop command */\n                  if(BitmapHeader2.RotAngle & 0x8000)\n                    {\n                      Image\n                        *flop_image;\n\n                      flop_image = FlopImage(image, exception);\n                      if (flop_image != (Image *) NULL) {\n                        DuplicateBlob(flop_image,image);\n                        ReplaceImageInList(&image,flop_image);\n                      }\n                    }\n                  /* flip command */\n                  if(BitmapHeader2.RotAngle & 0x2000)\n                    {\n                      Image\n                        *flip_image;\n\n                      flip_image = FlipImage(image, exception);\n                      if (flip_image != (Image *) NULL) {\n                        DuplicateBlob(flip_image,image);\n                        ReplaceImageInList(&image,flip_image);\n                      }\n                    }\n                  /* rotate command */\n                  if(BitmapHeader2.RotAngle & 0x0FFF)\n                    {\n                      Image\n                        *rotate_image;\n\n                      rotate_image=RotateImage(image,(BitmapHeader2.RotAngle &\n                        0x0FFF), exception);\n                      if (rotate_image != (Image *) NULL) {\n                        DuplicateBlob(rotate_image,image);\n                        ReplaceImageInList(&image,rotate_image);\n                      }\n                    }\n                }\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x1B:  /* Postscript l2 */\n              if(Rec.RecordLength>0x3C)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+0x3C,   /* skip PS l2 header in the wpg */\n                  (ssize_t) Rec.RecordLength-0x3C,exception);\n              break;\n            }\n        }\n      break;\n\n    case 2:  /* WPG level 2 */\n      (void) memset(CTM,0,sizeof(CTM));\n      StartWPG.PosSizePrecision = 0;\n      while(!EOFBlob(image)) /* object parser loop */\n        {\n          (void) SeekBlob(image,Header.DataOffset,SEEK_SET);\n          if(EOFBlob(image))\n            break;\n\n          Rec2.Class=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rec2.RecType=(i=ReadBlobByte(image));\n          if(i==EOF)\n            break;\n          Rd_WP_DWORD(image,&Rec2.Extension);\n          Rd_WP_DWORD(image,&Rec2.RecordLength);\n          if(EOFBlob(image))\n            break;\n\n          Header.DataOffset=TellBlob(image)+Rec2.RecordLength;\n\n          switch(Rec2.RecType)\n            {\n      case 1:\n              StartWPG.HorizontalUnits=ReadBlobLSBShort(image);\n              StartWPG.VerticalUnits=ReadBlobLSBShort(image);\n              StartWPG.PosSizePrecision=ReadBlobByte(image);\n              break;\n            case 0x0C:    /* Color palette */\n              WPG_Palette.StartIndex=ReadBlobLSBShort(image);\n              WPG_Palette.NumOfEntries=ReadBlobLSBShort(image);\n              if ((WPG_Palette.NumOfEntries-WPG_Palette.StartIndex) >\n                  (Rec2.RecordLength-2-2) / 3)\n                ThrowReaderException(CorruptImageError,\"InvalidColormapIndex\");\n              image->colors=WPG_Palette.NumOfEntries;\n              if (AcquireImageColormap(image,image->colors) == MagickFalse)\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              for (i=WPG_Palette.StartIndex;\n                   i < (int)WPG_Palette.NumOfEntries; i++)\n                {\n                  image->colormap[i].red=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].green=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  image->colormap[i].blue=ScaleCharToQuantum((char)\n                    ReadBlobByte(image));\n                  (void) ReadBlobByte(image);   /*Opacity??*/\n                }\n              break;\n            case 0x0E:\n              Bitmap2Header1.Width=ReadBlobLSBShort(image);\n              Bitmap2Header1.Height=ReadBlobLSBShort(image);\n              if ((Bitmap2Header1.Width == 0) || (Bitmap2Header1.Height == 0))\n                ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n              Bitmap2Header1.Depth=ReadBlobByte(image);\n              Bitmap2Header1.Compression=ReadBlobByte(image);\n\n              if(Bitmap2Header1.Compression > 1)\n                continue; /*Unknown compression method */\n              switch(Bitmap2Header1.Depth)\n                {\n                case 1:\n                  bpp=1;\n                  break;\n                case 2:\n                  bpp=2;\n                  break;\n                case 3:\n                  bpp=4;\n                  break;\n                case 4:\n                  bpp=8;\n                  break;\n                case 8:\n                  bpp=24;\n                  break;\n                default:\n                  continue;  /*Ignore raster with unknown depth*/\n                }\n              image->columns=Bitmap2Header1.Width;\n              image->rows=Bitmap2Header1.Height;\n              status=SetImageExtent(image,image->columns,image->rows);\n              if (status == MagickFalse)\n                break;\n              if ((image->colors == 0) && (bpp != 24))\n                {\n                  size_t\n                    one;\n\n                  one=1;\n                  image->colors=one << bpp;\n                  if (!AcquireImageColormap(image,image->colors))\n                    goto NoMemory;\n                }\n              else\n                {\n                  if(bpp < 24)\n                    if( image->colors<(one << bpp) && bpp!=24 )\n                      image->colormap=(PixelPacket *) ResizeQuantumMemory(\n                       image->colormap,(size_t) (one << bpp),\n                       sizeof(*image->colormap));\n                }\n\n\n              switch(Bitmap2Header1.Compression)\n                {\n                case 0:    /*Uncompressed raster*/\n                  {\n                    ldblk=(ssize_t) ((bpp*image->columns+7)/8);\n                    BImgBuff=(unsigned char *) AcquireQuantumMemory((size_t)\n                      ldblk+1,sizeof(*BImgBuff));\n                    if (BImgBuff == (unsigned char *) NULL)\n                      goto NoMemory;\n\n                    for(i=0; i< (ssize_t) image->rows; i++)\n                      {\n                        (void) ReadBlob(image,ldblk,BImgBuff);\n                        InsertRow(BImgBuff,i,image,bpp);\n                      }\n\n                    if(BImgBuff)\n                      BImgBuff=(unsigned char *) RelinquishMagickMemory(BImgBuff);\n                    break;\n                  }\n                case 1:    /*RLE for WPG2 */\n                  {\n                    if( UnpackWPG2Raster(image,bpp) < 0)\n                      goto DecompressionFailed;\n                    break;\n                  }\n                }\n\n              if(CTM[0][0]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flop_image;\n\n                  flop_image = FlopImage(image, exception);\n                  if (flop_image != (Image *) NULL) {\n                    DuplicateBlob(flop_image,image);\n                    ReplaceImageInList(&image,flop_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     Tx(0,0)=-1;      Tx(1,0)=0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;      Tx(1,1)=1;   Tx(2,1)=0;\n                     Tx(0,2)=(WPG._2Rect.X_ur+WPG._2Rect.X_ll);\n                     Tx(1,2)=0;   Tx(2,2)=1; */\n                }\n              if(CTM[1][1]<0 && !image_info->ping)\n                {    /*?? RotAngle=360-RotAngle;*/\n                  Image\n                    *flip_image;\n\n                  flip_image = FlipImage(image, exception);\n                  if (flip_image != (Image *) NULL) {\n                    DuplicateBlob(flip_image,image);\n                    ReplaceImageInList(&image,flip_image);\n                  }\n                  /* Try to change CTM according to Flip - I am not sure, must be checked.\n                     float_matrix Tx(3,3);\n                     Tx(0,0)= 1;   Tx(1,0)= 0;   Tx(2,0)=0;\n                     Tx(0,1)= 0;   Tx(1,1)=-1;   Tx(2,1)=0;\n                     Tx(0,2)= 0;   Tx(1,2)=(WPG._2Rect.Y_ur+WPG._2Rect.Y_ll);\n                     Tx(2,2)=1; */\n                }\n\n\n              /* Allocate next image structure. */\n              AcquireNextImage(image_info,image);\n              image->depth=8;\n              if (image->next == (Image *) NULL)\n                goto Finish;\n              image=SyncNextImageInList(image);\n              image->columns=image->rows=1;\n              image->colors=0;\n              break;\n\n            case 0x12:  /* Postscript WPG2*/\n        i=ReadBlobLSBShort(image);\n              if(Rec2.RecordLength > (unsigned int) i)\n                image=ExtractPostscript(image,image_info,\n                  TellBlob(image)+i,    /*skip PS header in the wpg2*/\n                  (ssize_t) (Rec2.RecordLength-i-2),exception);\n              break;\n\n      case 0x1B:          /*bitmap rectangle*/\n              WPG2Flags = LoadWPG2Flags(image,StartWPG.PosSizePrecision,NULL,&CTM);\n              (void) WPG2Flags;\n              break;\n            }\n        }\n\n      break;\n\n    default:\n      {\n         ThrowReaderException(CoderError,\"DataEncodingSchemeIsNotSupported\");\n      }\n   }\n\n Finish:\n  (void) CloseBlob(image);\n\n  {\n    Image\n      *p;\n\n    ssize_t\n      scene=0;\n\n    /*\n      Rewind list, removing any empty images while rewinding.\n    */\n    p=image;\n    image=NULL;\n    while (p != (Image *) NULL)\n      {\n        Image *tmp=p;\n        if ((p->rows == 0) || (p->columns == 0)) {\n          p=p->previous;\n          DeleteImageFromList(&tmp);\n        } else {\n          image=p;\n          p=p->previous;\n        }\n      }\n    /*\n      Fix scene numbers.\n    */\n    for (p=image; p != (Image *) NULL; p=p->next)\n      p->scene=(size_t) scene++;\n  }\n  if (image == (Image *) NULL)\n    ThrowReaderException(CorruptImageError,\n      \"ImageFileDoesNotContainAnyImageData\");\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -269,7 +269,7 @@\n               status=SetImageExtent(image,image->columns,image->rows);\n               if (status == MagickFalse)\n                 break;\n-              if ((image->colors == 0) && (bpp != 24))\n+              if ((image->colors == 0) && (bpp <= 16))\n                 {\n                   image->colors=one << bpp;\n                   if (!AcquireImageColormap(image,image->colors))",
        "diff_line_info": {
            "deleted_lines": [
                "              if ((image->colors == 0) && (bpp != 24))"
            ],
            "added_lines": [
                "              if ((image->colors == 0) && (bpp <= 16))"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2491",
        "func_name": "torvalds/linux/nlmclnt_unlock_callback",
        "description": "The Network Lock Manager (NLM) protocol implementation in the NFS client functionality in the Linux kernel before 3.0 allows local users to cause a denial of service (system hang) via a LOCK_UN flock system call.",
        "git_url": "https://github.com/torvalds/linux/commit/0b760113a3a155269a3fba93a409c640031dd68f",
        "commit_title": "NLM: Don't hang forever on NLM unlock requests",
        "commit_text": " If the NLM daemon is killed on the NFS server, we can currently end up hanging forever on an 'unlock' request, instead of aborting. Basically, if the rpcbind request fails, or the server keeps returning garbage, we really want to quit instead of retrying.  Cc: stable@kernel.org",
        "func_before": "static void nlmclnt_unlock_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: unlock failed (err = %d)\\n\", -task->tk_status);\n\t\tgoto retry_rebind;\n\t}\n\tif (status == NLM_LCK_DENIED_GRACE_PERIOD) {\n\t\trpc_delay(task, NLMCLNT_GRACE_WAIT);\n\t\tgoto retry_unlock;\n\t}\n\tif (status != NLM_LCK_GRANTED)\n\t\tprintk(KERN_WARNING \"lockd: unexpected unlock status: %d\\n\", status);\ndie:\n\treturn;\n retry_rebind:\n\tnlm_rebind_host(req->a_host);\n retry_unlock:\n\trpc_restart_call(task);\n}",
        "func": "static void nlmclnt_unlock_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t*req = data;\n\tu32 status = ntohl(req->a_res.status);\n\n\tif (RPC_ASSASSINATED(task))\n\t\tgoto die;\n\n\tif (task->tk_status < 0) {\n\t\tdprintk(\"lockd: unlock failed (err = %d)\\n\", -task->tk_status);\n\t\tswitch (task->tk_status) {\n\t\tcase -EACCES:\n\t\tcase -EIO:\n\t\t\tgoto die;\n\t\tdefault:\n\t\t\tgoto retry_rebind;\n\t\t}\n\t}\n\tif (status == NLM_LCK_DENIED_GRACE_PERIOD) {\n\t\trpc_delay(task, NLMCLNT_GRACE_WAIT);\n\t\tgoto retry_unlock;\n\t}\n\tif (status != NLM_LCK_GRANTED)\n\t\tprintk(KERN_WARNING \"lockd: unexpected unlock status: %d\\n\", status);\ndie:\n\treturn;\n retry_rebind:\n\tnlm_rebind_host(req->a_host);\n retry_unlock:\n\trpc_restart_call(task);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,13 @@\n \n \tif (task->tk_status < 0) {\n \t\tdprintk(\"lockd: unlock failed (err = %d)\\n\", -task->tk_status);\n-\t\tgoto retry_rebind;\n+\t\tswitch (task->tk_status) {\n+\t\tcase -EACCES:\n+\t\tcase -EIO:\n+\t\t\tgoto die;\n+\t\tdefault:\n+\t\t\tgoto retry_rebind;\n+\t\t}\n \t}\n \tif (status == NLM_LCK_DENIED_GRACE_PERIOD) {\n \t\trpc_delay(task, NLMCLNT_GRACE_WAIT);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgoto retry_rebind;"
            ],
            "added_lines": [
                "\t\tswitch (task->tk_status) {",
                "\t\tcase -EACCES:",
                "\t\tcase -EIO:",
                "\t\t\tgoto die;",
                "\t\tdefault:",
                "\t\t\tgoto retry_rebind;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2491",
        "func_name": "torvalds/linux/rpc_init_task",
        "description": "The Network Lock Manager (NLM) protocol implementation in the NFS client functionality in the Linux kernel before 3.0 allows local users to cause a denial of service (system hang) via a LOCK_UN flock system call.",
        "git_url": "https://github.com/torvalds/linux/commit/0b760113a3a155269a3fba93a409c640031dd68f",
        "commit_title": "NLM: Don't hang forever on NLM unlock requests",
        "commit_text": " If the NLM daemon is killed on the NFS server, we can currently end up hanging forever on an 'unlock' request, instead of aborting. Basically, if the rpcbind request fails, or the server keeps returning garbage, we really want to quit instead of retrying.  Cc: stable@kernel.org",
        "func_before": "static void rpc_init_task(struct rpc_task *task, const struct rpc_task_setup *task_setup_data)\n{\n\tmemset(task, 0, sizeof(*task));\n\tatomic_set(&task->tk_count, 1);\n\ttask->tk_flags  = task_setup_data->flags;\n\ttask->tk_ops = task_setup_data->callback_ops;\n\ttask->tk_calldata = task_setup_data->callback_data;\n\tINIT_LIST_HEAD(&task->tk_task);\n\n\t/* Initialize retry counters */\n\ttask->tk_garb_retry = 2;\n\ttask->tk_cred_retry = 2;\n\n\ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n\ttask->tk_owner = current->tgid;\n\n\t/* Initialize workqueue for async tasks */\n\ttask->tk_workqueue = task_setup_data->workqueue;\n\n\tif (task->tk_ops->rpc_call_prepare != NULL)\n\t\ttask->tk_action = rpc_prepare_task;\n\n\t/* starting timestamp */\n\ttask->tk_start = ktime_get();\n\n\tdprintk(\"RPC:       new task initialized, procpid %u\\n\",\n\t\t\t\ttask_pid_nr(current));\n}",
        "func": "static void rpc_init_task(struct rpc_task *task, const struct rpc_task_setup *task_setup_data)\n{\n\tmemset(task, 0, sizeof(*task));\n\tatomic_set(&task->tk_count, 1);\n\ttask->tk_flags  = task_setup_data->flags;\n\ttask->tk_ops = task_setup_data->callback_ops;\n\ttask->tk_calldata = task_setup_data->callback_data;\n\tINIT_LIST_HEAD(&task->tk_task);\n\n\t/* Initialize retry counters */\n\ttask->tk_garb_retry = 2;\n\ttask->tk_cred_retry = 2;\n\ttask->tk_rebind_retry = 2;\n\n\ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n\ttask->tk_owner = current->tgid;\n\n\t/* Initialize workqueue for async tasks */\n\ttask->tk_workqueue = task_setup_data->workqueue;\n\n\tif (task->tk_ops->rpc_call_prepare != NULL)\n\t\ttask->tk_action = rpc_prepare_task;\n\n\t/* starting timestamp */\n\ttask->tk_start = ktime_get();\n\n\tdprintk(\"RPC:       new task initialized, procpid %u\\n\",\n\t\t\t\ttask_pid_nr(current));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,7 @@\n \t/* Initialize retry counters */\n \ttask->tk_garb_retry = 2;\n \ttask->tk_cred_retry = 2;\n+\ttask->tk_rebind_retry = 2;\n \n \ttask->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;\n \ttask->tk_owner = current->tgid;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\ttask->tk_rebind_retry = 2;"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2491",
        "func_name": "torvalds/linux/call_bind_status",
        "description": "The Network Lock Manager (NLM) protocol implementation in the NFS client functionality in the Linux kernel before 3.0 allows local users to cause a denial of service (system hang) via a LOCK_UN flock system call.",
        "git_url": "https://github.com/torvalds/linux/commit/0b760113a3a155269a3fba93a409c640031dd68f",
        "commit_title": "NLM: Don't hang forever on NLM unlock requests",
        "commit_text": " If the NLM daemon is killed on the NFS server, we can currently end up hanging forever on an 'unlock' request, instead of aborting. Basically, if the rpcbind request fails, or the server keeps returning garbage, we really want to quit instead of retrying.  Cc: stable@kernel.org",
        "func_before": "static void\ncall_bind_status(struct rpc_task *task)\n{\n\tint status = -EIO;\n\n\tif (task->tk_status >= 0) {\n\t\tdprint_status(task);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_connect;\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC: %5u rpcbind out of memory\\n\", task->tk_pid);\n\t\trpc_delay(task, HZ >> 2);\n\t\tgoto retry_timeout;\n\tcase -EACCES:\n\t\tdprintk(\"RPC: %5u remote rpcbind: RPC program/version \"\n\t\t\t\t\"unavailable\\n\", task->tk_pid);\n\t\t/* fail immediately if this is an RPC ping */\n\t\tif (task->tk_msg.rpc_proc->p_proc == 0) {\n\t\t\tstatus = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\trpc_delay(task, 3*HZ);\n\t\tgoto retry_timeout;\n\tcase -ETIMEDOUT:\n\t\tdprintk(\"RPC: %5u rpcbind request timed out\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tgoto retry_timeout;\n\tcase -EPFNOSUPPORT:\n\t\t/* server doesn't support any rpcbind version we know of */\n\t\tdprintk(\"RPC: %5u unrecognized remote rpcbind service\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tbreak;\n\tcase -EPROTONOSUPPORT:\n\t\tdprintk(\"RPC: %5u remote rpcbind version unavailable, retrying\\n\",\n\t\t\t\ttask->tk_pid);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bind;\n\t\treturn;\n\tcase -ECONNREFUSED:\t\t/* connection problems */\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -EPIPE:\n\t\tdprintk(\"RPC: %5u remote rpcbind unreachable: %d\\n\",\n\t\t\t\ttask->tk_pid, task->tk_status);\n\t\tif (!RPC_IS_SOFTCONN(task)) {\n\t\t\trpc_delay(task, 5*HZ);\n\t\t\tgoto retry_timeout;\n\t\t}\n\t\tstatus = task->tk_status;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC: %5u unrecognized rpcbind error (%d)\\n\",\n\t\t\t\ttask->tk_pid, -task->tk_status);\n\t}\n\n\trpc_exit(task, status);\n\treturn;\n\nretry_timeout:\n\ttask->tk_action = call_timeout;\n}",
        "func": "static void\ncall_bind_status(struct rpc_task *task)\n{\n\tint status = -EIO;\n\n\tif (task->tk_status >= 0) {\n\t\tdprint_status(task);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_connect;\n\t\treturn;\n\t}\n\n\tswitch (task->tk_status) {\n\tcase -ENOMEM:\n\t\tdprintk(\"RPC: %5u rpcbind out of memory\\n\", task->tk_pid);\n\t\trpc_delay(task, HZ >> 2);\n\t\tgoto retry_timeout;\n\tcase -EACCES:\n\t\tdprintk(\"RPC: %5u remote rpcbind: RPC program/version \"\n\t\t\t\t\"unavailable\\n\", task->tk_pid);\n\t\t/* fail immediately if this is an RPC ping */\n\t\tif (task->tk_msg.rpc_proc->p_proc == 0) {\n\t\t\tstatus = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (task->tk_rebind_retry == 0)\n\t\t\tbreak;\n\t\ttask->tk_rebind_retry--;\n\t\trpc_delay(task, 3*HZ);\n\t\tgoto retry_timeout;\n\tcase -ETIMEDOUT:\n\t\tdprintk(\"RPC: %5u rpcbind request timed out\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tgoto retry_timeout;\n\tcase -EPFNOSUPPORT:\n\t\t/* server doesn't support any rpcbind version we know of */\n\t\tdprintk(\"RPC: %5u unrecognized remote rpcbind service\\n\",\n\t\t\t\ttask->tk_pid);\n\t\tbreak;\n\tcase -EPROTONOSUPPORT:\n\t\tdprintk(\"RPC: %5u remote rpcbind version unavailable, retrying\\n\",\n\t\t\t\ttask->tk_pid);\n\t\ttask->tk_status = 0;\n\t\ttask->tk_action = call_bind;\n\t\treturn;\n\tcase -ECONNREFUSED:\t\t/* connection problems */\n\tcase -ECONNRESET:\n\tcase -ENOTCONN:\n\tcase -EHOSTDOWN:\n\tcase -EHOSTUNREACH:\n\tcase -ENETUNREACH:\n\tcase -EPIPE:\n\t\tdprintk(\"RPC: %5u remote rpcbind unreachable: %d\\n\",\n\t\t\t\ttask->tk_pid, task->tk_status);\n\t\tif (!RPC_IS_SOFTCONN(task)) {\n\t\t\trpc_delay(task, 5*HZ);\n\t\t\tgoto retry_timeout;\n\t\t}\n\t\tstatus = task->tk_status;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"RPC: %5u unrecognized rpcbind error (%d)\\n\",\n\t\t\t\ttask->tk_pid, -task->tk_status);\n\t}\n\n\trpc_exit(task, status);\n\treturn;\n\nretry_timeout:\n\ttask->tk_action = call_timeout;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,6 +23,9 @@\n \t\t\tstatus = -EOPNOTSUPP;\n \t\t\tbreak;\n \t\t}\n+\t\tif (task->tk_rebind_retry == 0)\n+\t\t\tbreak;\n+\t\ttask->tk_rebind_retry--;\n \t\trpc_delay(task, 3*HZ);\n \t\tgoto retry_timeout;\n \tcase -ETIMEDOUT:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tif (task->tk_rebind_retry == 0)",
                "\t\t\tbreak;",
                "\t\ttask->tk_rebind_retry--;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20502",
        "func_name": "axiomatic-systems/Bento4/AP4_AtomFactory::CreateAtomFromStream",
        "description": "An issue was discovered in Bento4 1.5.1-627. There is an attempt at excessive memory allocation in the AP4_DataBuffer class when called from AP4_HvccAtom::Create in Core/Ap4HvccAtom.cpp.",
        "git_url": "https://github.com/axiomatic-systems/Bento4/commit/b8830036c76f3dedbbaafeaba1a9f23b6fd83485",
        "commit_title": "fix #349",
        "commit_text": "",
        "func_before": "AP4_Result\nAP4_AtomFactory::CreateAtomFromStream(AP4_ByteStream& stream, \n                                      AP4_LargeSize&  bytes_available,\n                                      AP4_Atom*&      atom)\n{\n    AP4_Result result;\n\n    // NULL by default\n    atom = NULL;\n\n    // check that there are enough bytes for at least a header\n    if (bytes_available < 8) return AP4_ERROR_EOS;\n\n    // remember current stream offset\n    AP4_Position start;\n    stream.Tell(start);\n\n    // read atom size\n    AP4_UI32 size_32;\n    result = stream.ReadUI32(size_32);\n    if (AP4_FAILED(result)) {\n        stream.Seek(start);\n        return result;\n    }\n    AP4_UI64 size = size_32;\n\n    // read atom type\n    AP4_Atom::Type type;\n    result = stream.ReadUI32(type);\n    if (AP4_FAILED(result)) {\n        stream.Seek(start);\n        return result;\n    }\n\n    // handle special size values\n    bool atom_is_large = false;\n    bool force_64      = false;\n    if (size == 0) {\n        // atom extends to end of file\n        AP4_LargeSize stream_size = 0;\n        stream.GetSize(stream_size);\n        if (stream_size >= start) {\n            size = stream_size - start;\n        }\n    } else if (size == 1) {\n        // 64-bit size\n        atom_is_large = true;\n        if (bytes_available < 16) {\n            stream.Seek(start);\n            return AP4_ERROR_INVALID_FORMAT;\n        }\n        stream.ReadUI64(size);\n        if (size <= 0xFFFFFFFF) {\n            force_64 = true;\n        }\n    }\n\n    // check the size\n    if ((size > 0 && size < 8) || size > bytes_available) {\n        stream.Seek(start);\n        return AP4_ERROR_INVALID_FORMAT;\n    }\n\n    // create the atom\n    result = CreateAtomFromStream(stream, type, size_32, size, atom);\n    if (AP4_FAILED(result)) return result;\n    \n    // if we failed to create an atom, use a generic version\n    if (atom == NULL) {\n        unsigned int payload_offset = 8;\n        if (atom_is_large) payload_offset += 8;\n        stream.Seek(start+payload_offset);\n        atom = new AP4_UnknownAtom(type, size, stream);\n    }\n\n    // special case: if the atom is poorly encoded and has a 64-bit\n    // size header but an actual size that fits on 32-bit, adjust the\n    // object to reflect that.\n    if (force_64) {\n        atom->SetSize32(1);\n        atom->SetSize64(size);\n    }\n\n    // adjust the available size\n    bytes_available -= size;\n\n    // skip to the end of the atom\n    result = stream.Seek(start+size);\n    if (AP4_FAILED(result)) {\n        delete atom;\n        atom = NULL;\n        return result;\n    }\n    \n    return AP4_SUCCESS;\n}",
        "func": "AP4_Result\nAP4_AtomFactory::CreateAtomFromStream(AP4_ByteStream& stream, \n                                      AP4_LargeSize&  bytes_available,\n                                      AP4_Atom*&      atom)\n{\n    AP4_Result result;\n\n    // NULL by default\n    atom = NULL;\n\n    // check that there are enough bytes for at least a header\n    if (bytes_available < 8) return AP4_ERROR_EOS;\n\n    // remember current stream offset\n    AP4_Position start;\n    stream.Tell(start);\n\n    // read atom size\n    AP4_UI32 size_32;\n    result = stream.ReadUI32(size_32);\n    if (AP4_FAILED(result)) {\n        stream.Seek(start);\n        return result;\n    }\n    AP4_UI64 size = size_32;\n\n    // read atom type\n    AP4_Atom::Type type;\n    result = stream.ReadUI32(type);\n    if (AP4_FAILED(result)) {\n        stream.Seek(start);\n        return result;\n    }\n\n    // handle special size values\n    bool atom_is_large = false;\n    bool force_64      = false;\n    if (size == 0) {\n        // atom extends to end of file\n        AP4_LargeSize stream_size = 0;\n        stream.GetSize(stream_size);\n        if (stream_size >= start) {\n            size = stream_size - start;\n\n            if (size <= 0xFFFFFFFF) {\n                size_32 = (AP4_UI32)size;\n            } else {\n                size_32 = 1; // signal a large atom\n            }\n        }\n    } else if (size == 1) {\n        // 64-bit size\n        atom_is_large = true;\n        if (bytes_available < 16) {\n            stream.Seek(start);\n            return AP4_ERROR_INVALID_FORMAT;\n        }\n        stream.ReadUI64(size);\n        if (size <= 0xFFFFFFFF) {\n            force_64 = true;\n        }\n    }\n\n    // check the size\n    if ((size > 0 && size < 8) || size > bytes_available) {\n        stream.Seek(start);\n        return AP4_ERROR_INVALID_FORMAT;\n    }\n\n    // create the atom\n    result = CreateAtomFromStream(stream, type, size_32, size, atom);\n    if (AP4_FAILED(result)) return result;\n    \n    // if we failed to create an atom, use a generic version\n    if (atom == NULL) {\n        unsigned int payload_offset = 8;\n        if (atom_is_large) payload_offset += 8;\n        stream.Seek(start+payload_offset);\n        atom = new AP4_UnknownAtom(type, size, stream);\n    }\n\n    // special case: if the atom is poorly encoded and has a 64-bit\n    // size header but an actual size that fits on 32-bit, adjust the\n    // object to reflect that.\n    if (force_64) {\n        atom->SetSize32(1);\n        atom->SetSize64(size);\n    }\n\n    // adjust the available size\n    bytes_available -= size;\n\n    // skip to the end of the atom\n    result = stream.Seek(start+size);\n    if (AP4_FAILED(result)) {\n        delete atom;\n        atom = NULL;\n        return result;\n    }\n    \n    return AP4_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,6 +41,12 @@\n         stream.GetSize(stream_size);\n         if (stream_size >= start) {\n             size = stream_size - start;\n+\n+            if (size <= 0xFFFFFFFF) {\n+                size_32 = (AP4_UI32)size;\n+            } else {\n+                size_32 = 1; // signal a large atom\n+            }\n         }\n     } else if (size == 1) {\n         // 64-bit size",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "            if (size <= 0xFFFFFFFF) {",
                "                size_32 = (AP4_UI32)size;",
                "            } else {",
                "                size_32 = 1; // signal a large atom",
                "            }"
            ]
        }
    },
    {
        "cve_id": "CVE-2013-2128",
        "func_name": "torvalds/linux/tcp_read_sock",
        "description": "The tcp_read_sock function in net/ipv4/tcp.c in the Linux kernel before 2.6.34 does not properly manage skb consumption, which allows local users to cause a denial of service (system crash) via a crafted splice system call for a TCP socket.",
        "git_url": "https://github.com/torvalds/linux/commit/baff42ab1494528907bf4d5870359e31711746ae",
        "commit_title": "net: Fix oops from tcp_collapse() when using splice()",
        "commit_text": " tcp_read_sock() can have a eat skbs without immediately advancing copied_seq. This can cause a panic in tcp_collapse() if it is called as a result of the recv_actor dropping the socket lock.  A userspace program that splices data from a socket to either another socket or to a file can trigger this bug. ",
        "func_before": "int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used < 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq-1, &offset);\n\t\t\tif (!skb || (offset+1 != skb->len))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (tcp_hdr(skb)->fin) {\n\t\t\tsk_eat_skb(sk, skb, 0);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb, 0);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0)\n\t\ttcp_cleanup_rbuf(sk, copied);\n\treturn copied;\n}",
        "func": "int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used < 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/*\n\t\t\t * If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq-1, &offset);\n\t\t\tif (!skb || (offset+1 != skb->len))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (tcp_hdr(skb)->fin) {\n\t\t\tsk_eat_skb(sk, skb, 0);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb, 0);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t\ttp->copied_seq = seq;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0)\n\t\ttcp_cleanup_rbuf(sk, copied);\n\treturn copied;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,6 +51,7 @@\n \t\tsk_eat_skb(sk, skb, 0);\n \t\tif (!desc->count)\n \t\t\tbreak;\n+\t\ttp->copied_seq = seq;\n \t}\n \ttp->copied_seq = seq;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\ttp->copied_seq = seq;"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-6638",
        "func_name": "torvalds/linux/tcp_rcv_state_process",
        "description": "The tcp_rcv_state_process function in net/ipv4/tcp_input.c in the Linux kernel before 3.2.24 allows remote attackers to cause a denial of service (kernel resource consumption) via a flood of SYN+FIN TCP packets, a different vulnerability than CVE-2012-2663.",
        "git_url": "https://github.com/torvalds/linux/commit/fdf5af0daf8019cec2396cdef8fb042d80fe71fa",
        "commit_title": "tcp: drop SYN+FIN messages",
        "commit_text": " Denys Fedoryshchenko reported that SYN+FIN attacks were bringing his linux machines to their limits.  Dont call conn_request() if the TCP flags includes SYN flag ",
        "func_before": "int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t  const struct tcphdr *th, unsigned int len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint queued = 0;\n\tint res;\n\n\ttp->rx_opt.saw_tstamp = 0;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn 1;\n\n\t\tif (th->rst)\n\t\t\tgoto discard;\n\n\t\tif (th->syn) {\n\t\t\tif (icsk->icsk_af_ops->conn_request(sk, skb) < 0)\n\t\t\t\treturn 1;\n\n\t\t\t/* Now we have several options: In theory there is\n\t\t\t * nothing else in the frame. KA9Q has an option to\n\t\t\t * send data with the syn, BSD accepts data with the\n\t\t\t * syn up to the [to be] advertised window and\n\t\t\t * Solaris 2.1 gives you a protocol error. For now\n\t\t\t * we just ignore it, that fits the spec precisely\n\t\t\t * and avoids incompatibilities. It would be nice in\n\t\t\t * future to drop through and process the data.\n\t\t\t *\n\t\t\t * Now that TTCP is starting to be used we ought to\n\t\t\t * queue this data.\n\t\t\t * But, this leaves one open to an easy denial of\n\t\t\t * service attack, and SYN cookies can't defend\n\t\t\t * against this problem. So, we drop the data\n\t\t\t * in the interest of security over speed unless\n\t\t\t * it's still in use.\n\t\t\t */\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th, len);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t/* Do step6 onward by hand. */\n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\tres = tcp_validate_incoming(sk, skb, th, 0);\n\tif (res <= 0)\n\t\treturn -res;\n\n\t/* step 5: check the ACK field */\n\tif (th->ack) {\n\t\tint acceptable = tcp_ack(sk, skb, FLAG_SLOWPATH) > 0;\n\n\t\tswitch (sk->sk_state) {\n\t\tcase TCP_SYN_RECV:\n\t\t\tif (acceptable) {\n\t\t\t\ttp->copied_seq = tp->rcv_nxt;\n\t\t\t\tsmp_mb();\n\t\t\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\t/* Note, that this wakeup is only for marginal\n\t\t\t\t * crossed SYN case. Passively open sockets\n\t\t\t\t * are not waked up, because sk->sk_sleep ==\n\t\t\t\t * NULL and sk->sk_socket == NULL.\n\t\t\t\t */\n\t\t\t\tif (sk->sk_socket)\n\t\t\t\t\tsk_wake_async(sk,\n\t\t\t\t\t\t      SOCK_WAKE_IO, POLL_OUT);\n\n\t\t\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\t\t\ttp->snd_wnd = ntohs(th->window) <<\n\t\t\t\t\t      tp->rx_opt.snd_wscale;\n\t\t\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\t\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\t\t\t/* Make sure socket is routed, for\n\t\t\t\t * correct metrics.\n\t\t\t\t */\n\t\t\t\ticsk->icsk_af_ops->rebuild_header(sk);\n\n\t\t\t\ttcp_init_metrics(sk);\n\n\t\t\t\ttcp_init_congestion_control(sk);\n\n\t\t\t\t/* Prevent spurious tcp_cwnd_restart() on\n\t\t\t\t * first data packet.\n\t\t\t\t */\n\t\t\t\ttp->lsndtime = tcp_time_stamp;\n\n\t\t\t\ttcp_mtup_init(sk);\n\t\t\t\ttcp_initialize_rcv_mss(sk);\n\t\t\t\ttcp_init_buffer_space(sk);\n\t\t\t\ttcp_fast_path_on(tp);\n\t\t\t} else {\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_FIN_WAIT1:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\t\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\t\t\t\tdst_confirm(__sk_dst_get(sk));\n\n\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t/* Wake up lingering close() */\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\telse {\n\t\t\t\t\tint tmo;\n\n\t\t\t\t\tif (tp->linger2 < 0 ||\n\t\t\t\t\t    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t\t\t     after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt))) {\n\t\t\t\t\t\ttcp_done(sk);\n\t\t\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\t\t\treturn 1;\n\t\t\t\t\t}\n\n\t\t\t\t\ttmo = tcp_fin_time(sk);\n\t\t\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t\t\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t\t\t\t/* Bad case. We could lose such FIN otherwise.\n\t\t\t\t\t\t * It is not a big problem, but it looks confusing\n\t\t\t\t\t\t * and not so rare event. We still can lose it now,\n\t\t\t\t\t\t * if it spins in bh_lock_sock(), but it is really\n\t\t\t\t\t\t * marginal case.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\t\t\tgoto discard;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CLOSING:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_LAST_ACK:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_update_metrics(sk);\n\t\t\t\ttcp_done(sk);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tgoto discard;\n\n\t/* step 6: check the URG bit */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t/* RFC 793 says to queue data in these states,\n\t\t * RFC 1122 says we MUST send a reset.\n\t\t * BSD 4.4 also does reset.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t\t/* Fall through */\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t/* tcp_data could move socket to TIME-WAIT */\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\t__kfree_skb(skb);\n\t}\n\treturn 0;\n}",
        "func": "int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,\n\t\t\t  const struct tcphdr *th, unsigned int len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint queued = 0;\n\tint res;\n\n\ttp->rx_opt.saw_tstamp = 0;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\t\tgoto discard;\n\n\tcase TCP_LISTEN:\n\t\tif (th->ack)\n\t\t\treturn 1;\n\n\t\tif (th->rst)\n\t\t\tgoto discard;\n\n\t\tif (th->syn) {\n\t\t\tif (th->fin)\n\t\t\t\tgoto discard;\n\t\t\tif (icsk->icsk_af_ops->conn_request(sk, skb) < 0)\n\t\t\t\treturn 1;\n\n\t\t\t/* Now we have several options: In theory there is\n\t\t\t * nothing else in the frame. KA9Q has an option to\n\t\t\t * send data with the syn, BSD accepts data with the\n\t\t\t * syn up to the [to be] advertised window and\n\t\t\t * Solaris 2.1 gives you a protocol error. For now\n\t\t\t * we just ignore it, that fits the spec precisely\n\t\t\t * and avoids incompatibilities. It would be nice in\n\t\t\t * future to drop through and process the data.\n\t\t\t *\n\t\t\t * Now that TTCP is starting to be used we ought to\n\t\t\t * queue this data.\n\t\t\t * But, this leaves one open to an easy denial of\n\t\t\t * service attack, and SYN cookies can't defend\n\t\t\t * against this problem. So, we drop the data\n\t\t\t * in the interest of security over speed unless\n\t\t\t * it's still in use.\n\t\t\t */\n\t\t\tkfree_skb(skb);\n\t\t\treturn 0;\n\t\t}\n\t\tgoto discard;\n\n\tcase TCP_SYN_SENT:\n\t\tqueued = tcp_rcv_synsent_state_process(sk, skb, th, len);\n\t\tif (queued >= 0)\n\t\t\treturn queued;\n\n\t\t/* Do step6 onward by hand. */\n\t\ttcp_urg(sk, skb, th);\n\t\t__kfree_skb(skb);\n\t\ttcp_data_snd_check(sk);\n\t\treturn 0;\n\t}\n\n\tres = tcp_validate_incoming(sk, skb, th, 0);\n\tif (res <= 0)\n\t\treturn -res;\n\n\t/* step 5: check the ACK field */\n\tif (th->ack) {\n\t\tint acceptable = tcp_ack(sk, skb, FLAG_SLOWPATH) > 0;\n\n\t\tswitch (sk->sk_state) {\n\t\tcase TCP_SYN_RECV:\n\t\t\tif (acceptable) {\n\t\t\t\ttp->copied_seq = tp->rcv_nxt;\n\t\t\t\tsmp_mb();\n\t\t\t\ttcp_set_state(sk, TCP_ESTABLISHED);\n\t\t\t\tsk->sk_state_change(sk);\n\n\t\t\t\t/* Note, that this wakeup is only for marginal\n\t\t\t\t * crossed SYN case. Passively open sockets\n\t\t\t\t * are not waked up, because sk->sk_sleep ==\n\t\t\t\t * NULL and sk->sk_socket == NULL.\n\t\t\t\t */\n\t\t\t\tif (sk->sk_socket)\n\t\t\t\t\tsk_wake_async(sk,\n\t\t\t\t\t\t      SOCK_WAKE_IO, POLL_OUT);\n\n\t\t\t\ttp->snd_una = TCP_SKB_CB(skb)->ack_seq;\n\t\t\t\ttp->snd_wnd = ntohs(th->window) <<\n\t\t\t\t\t      tp->rx_opt.snd_wscale;\n\t\t\t\ttcp_init_wl(tp, TCP_SKB_CB(skb)->seq);\n\n\t\t\t\tif (tp->rx_opt.tstamp_ok)\n\t\t\t\t\ttp->advmss -= TCPOLEN_TSTAMP_ALIGNED;\n\n\t\t\t\t/* Make sure socket is routed, for\n\t\t\t\t * correct metrics.\n\t\t\t\t */\n\t\t\t\ticsk->icsk_af_ops->rebuild_header(sk);\n\n\t\t\t\ttcp_init_metrics(sk);\n\n\t\t\t\ttcp_init_congestion_control(sk);\n\n\t\t\t\t/* Prevent spurious tcp_cwnd_restart() on\n\t\t\t\t * first data packet.\n\t\t\t\t */\n\t\t\t\ttp->lsndtime = tcp_time_stamp;\n\n\t\t\t\ttcp_mtup_init(sk);\n\t\t\t\ttcp_initialize_rcv_mss(sk);\n\t\t\t\ttcp_init_buffer_space(sk);\n\t\t\t\ttcp_fast_path_on(tp);\n\t\t\t} else {\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_FIN_WAIT1:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\t\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\t\t\t\tdst_confirm(__sk_dst_get(sk));\n\n\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t/* Wake up lingering close() */\n\t\t\t\t\tsk->sk_state_change(sk);\n\t\t\t\telse {\n\t\t\t\t\tint tmo;\n\n\t\t\t\t\tif (tp->linger2 < 0 ||\n\t\t\t\t\t    (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t\t\t     after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt))) {\n\t\t\t\t\t\ttcp_done(sk);\n\t\t\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\t\t\treturn 1;\n\t\t\t\t\t}\n\n\t\t\t\t\ttmo = tcp_fin_time(sk);\n\t\t\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\t\t\tinet_csk_reset_keepalive_timer(sk, tmo - TCP_TIMEWAIT_LEN);\n\t\t\t\t\t} else if (th->fin || sock_owned_by_user(sk)) {\n\t\t\t\t\t\t/* Bad case. We could lose such FIN otherwise.\n\t\t\t\t\t\t * It is not a big problem, but it looks confusing\n\t\t\t\t\t\t * and not so rare event. We still can lose it now,\n\t\t\t\t\t\t * if it spins in bh_lock_sock(), but it is really\n\t\t\t\t\t\t * marginal case.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tinet_csk_reset_keepalive_timer(sk, tmo);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\t\t\tgoto discard;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_CLOSING:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TCP_LAST_ACK:\n\t\t\tif (tp->snd_una == tp->write_seq) {\n\t\t\t\ttcp_update_metrics(sk);\n\t\t\t\ttcp_done(sk);\n\t\t\t\tgoto discard;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tgoto discard;\n\n\t/* step 6: check the URG bit */\n\ttcp_urg(sk, skb, th);\n\n\t/* step 7: process the segment text */\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSING:\n\tcase TCP_LAST_ACK:\n\t\tif (!before(TCP_SKB_CB(skb)->seq, tp->rcv_nxt))\n\t\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\t/* RFC 793 says to queue data in these states,\n\t\t * RFC 1122 says we MUST send a reset.\n\t\t * BSD 4.4 also does reset.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\tif (TCP_SKB_CB(skb)->end_seq != TCP_SKB_CB(skb)->seq &&\n\t\t\t    after(TCP_SKB_CB(skb)->end_seq - th->fin, tp->rcv_nxt)) {\n\t\t\t\tNET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t\t\t\ttcp_reset(sk);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t\t/* Fall through */\n\tcase TCP_ESTABLISHED:\n\t\ttcp_data_queue(sk, skb);\n\t\tqueued = 1;\n\t\tbreak;\n\t}\n\n\t/* tcp_data could move socket to TIME-WAIT */\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\ttcp_data_snd_check(sk);\n\t\ttcp_ack_snd_check(sk);\n\t}\n\n\tif (!queued) {\ndiscard:\n\t\t__kfree_skb(skb);\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,8 @@\n \t\t\tgoto discard;\n \n \t\tif (th->syn) {\n+\t\t\tif (th->fin)\n+\t\t\t\tgoto discard;\n \t\t\tif (icsk->icsk_af_ops->conn_request(sk, skb) < 0)\n \t\t\t\treturn 1;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tif (th->fin)",
                "\t\t\t\tgoto discard;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3122",
        "func_name": "torvalds/linux/mlock_vma_page",
        "description": "The try_to_unmap_cluster function in mm/rmap.c in the Linux kernel before 3.14.3 does not properly consider which pages must be locked, which allows local users to cause a denial of service (system crash) by triggering a memory-usage pattern that requires removal of page-table mappings.",
        "git_url": "https://github.com/torvalds/linux/commit/57e68e9cd65b4b8eb4045a1e0d0746458502554c",
        "commit_title": "mm: try_to_unmap_cluster() should lock_page() before mlocking",
        "commit_text": " A BUG_ON(!PageLocked) was triggered in mlock_vma_page() by Sasha Levin fuzzing with trinity.  The call site try_to_unmap_cluster() does not lock the pages other than its check_page parameter (which is already locked).  The BUG_ON in mlock_vma_page() is not documented and its purpose is somewhat unclear, but apparently it serializes against page migration, which could otherwise fail to transfer the PG_mlocked flag.  This would not be fatal, as the page would be eventually encountered again, but NR_MLOCK accounting would become distorted nevertheless.  This patch adds a comment to the BUG_ON in mlock_vma_page() and munlock_vma_page() to that effect.  The call site try_to_unmap_cluster() is fixed so that for page != check_page, trylock_page() is attempted (to avoid possible deadlocks as we already have check_page locked) and mlock_vma_page() is performed only upon success.  If the page lock cannot be obtained, the page is left without PG_mlocked, which is again not a problem in the whole unevictable memory design.  Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com> Cc: Michel Lespinasse <walken@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: David Rientjes <rientjes@google.com> Cc: Mel Gorman <mgorman@suse.de> Cc: Hugh Dickins <hughd@google.com> Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com> Cc: <stable@vger.kernel.org>",
        "func_before": "void mlock_vma_page(struct page *page)\n{\n\tBUG_ON(!PageLocked(page));\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}",
        "func": "void mlock_vma_page(struct page *page)\n{\n\t/* Serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n void mlock_vma_page(struct page *page)\n {\n+\t/* Serialize with page migration */\n \tBUG_ON(!PageLocked(page));\n \n \tif (!TestSetPageMlocked(page)) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/* Serialize with page migration */"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3122",
        "func_name": "torvalds/linux/munlock_vma_page",
        "description": "The try_to_unmap_cluster function in mm/rmap.c in the Linux kernel before 3.14.3 does not properly consider which pages must be locked, which allows local users to cause a denial of service (system crash) by triggering a memory-usage pattern that requires removal of page-table mappings.",
        "git_url": "https://github.com/torvalds/linux/commit/57e68e9cd65b4b8eb4045a1e0d0746458502554c",
        "commit_title": "mm: try_to_unmap_cluster() should lock_page() before mlocking",
        "commit_text": " A BUG_ON(!PageLocked) was triggered in mlock_vma_page() by Sasha Levin fuzzing with trinity.  The call site try_to_unmap_cluster() does not lock the pages other than its check_page parameter (which is already locked).  The BUG_ON in mlock_vma_page() is not documented and its purpose is somewhat unclear, but apparently it serializes against page migration, which could otherwise fail to transfer the PG_mlocked flag.  This would not be fatal, as the page would be eventually encountered again, but NR_MLOCK accounting would become distorted nevertheless.  This patch adds a comment to the BUG_ON in mlock_vma_page() and munlock_vma_page() to that effect.  The call site try_to_unmap_cluster() is fixed so that for page != check_page, trylock_page() is attempted (to avoid possible deadlocks as we already have check_page locked) and mlock_vma_page() is performed only upon success.  If the page lock cannot be obtained, the page is left without PG_mlocked, which is again not a problem in the whole unevictable memory design.  Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com> Cc: Michel Lespinasse <walken@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: David Rientjes <rientjes@google.com> Cc: Mel Gorman <mgorman@suse.de> Cc: Hugh Dickins <hughd@google.com> Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com> Cc: <stable@vger.kernel.org>",
        "func_before": "unsigned int munlock_vma_page(struct page *page)\n{\n\tunsigned int nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\tBUG_ON(!PageLocked(page));\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(&zone->lru_lock);\n\n\tnr_pages = hpage_nr_pages(page);\n\tif (!TestClearPageMlocked(page))\n\t\tgoto unlock_out;\n\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(&zone->lru_lock);\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(&zone->lru_lock);\n\nout:\n\treturn nr_pages - 1;\n}",
        "func": "unsigned int munlock_vma_page(struct page *page)\n{\n\tunsigned int nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\t/* For try_to_munlock() and to serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(&zone->lru_lock);\n\n\tnr_pages = hpage_nr_pages(page);\n\tif (!TestClearPageMlocked(page))\n\t\tgoto unlock_out;\n\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(&zone->lru_lock);\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(&zone->lru_lock);\n\nout:\n\treturn nr_pages - 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n \tunsigned int nr_pages;\n \tstruct zone *zone = page_zone(page);\n \n+\t/* For try_to_munlock() and to serialize with page migration */\n \tBUG_ON(!PageLocked(page));\n \n \t/*",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/* For try_to_munlock() and to serialize with page migration */"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3122",
        "func_name": "torvalds/linux/try_to_unmap_cluster",
        "description": "The try_to_unmap_cluster function in mm/rmap.c in the Linux kernel before 3.14.3 does not properly consider which pages must be locked, which allows local users to cause a denial of service (system crash) by triggering a memory-usage pattern that requires removal of page-table mappings.",
        "git_url": "https://github.com/torvalds/linux/commit/57e68e9cd65b4b8eb4045a1e0d0746458502554c",
        "commit_title": "mm: try_to_unmap_cluster() should lock_page() before mlocking",
        "commit_text": " A BUG_ON(!PageLocked) was triggered in mlock_vma_page() by Sasha Levin fuzzing with trinity.  The call site try_to_unmap_cluster() does not lock the pages other than its check_page parameter (which is already locked).  The BUG_ON in mlock_vma_page() is not documented and its purpose is somewhat unclear, but apparently it serializes against page migration, which could otherwise fail to transfer the PG_mlocked flag.  This would not be fatal, as the page would be eventually encountered again, but NR_MLOCK accounting would become distorted nevertheless.  This patch adds a comment to the BUG_ON in mlock_vma_page() and munlock_vma_page() to that effect.  The call site try_to_unmap_cluster() is fixed so that for page != check_page, trylock_page() is attempted (to avoid possible deadlocks as we already have check_page locked) and mlock_vma_page() is performed only upon success.  If the page lock cannot be obtained, the page is left without PG_mlocked, which is again not a problem in the whole unevictable memory design.  Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com> Cc: Michel Lespinasse <walken@google.com> Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: David Rientjes <rientjes@google.com> Cc: Mel Gorman <mgorman@suse.de> Cc: Hugh Dickins <hughd@google.com> Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,\n\t\tstruct vm_area_struct *vma, struct page *check_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tunsigned long address;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tunsigned long end;\n\tint ret = SWAP_AGAIN;\n\tint locked_vma = 0;\n\n\taddress = (vma->vm_start + cursor) & CLUSTER_MASK;\n\tend = address + CLUSTER_SIZE;\n\tif (address < vma->vm_start)\n\t\taddress = vma->vm_start;\n\tif (end > vma->vm_end)\n\t\tend = vma->vm_end;\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn ret;\n\n\tmmun_start = address;\n\tmmun_end   = end;\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,\n\t * keep the sem while scanning the cluster for mlocking pages.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tlocked_vma = (vma->vm_flags & VM_LOCKED);\n\t\tif (!locked_vma)\n\t\t\tup_read(&vma->vm_mm->mmap_sem); /* don't need it */\n\t}\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tfor (; address < end; pte++, address += PAGE_SIZE) {\n\t\tif (!pte_present(*pte))\n\t\t\tcontinue;\n\t\tpage = vm_normal_page(vma, address, *pte);\n\t\tBUG_ON(!page || PageAnon(page));\n\n\t\tif (locked_vma) {\n\t\t\tmlock_vma_page(page);   /* no-op if already mlocked */\n\t\t\tif (page == check_page)\n\t\t\t\tret = SWAP_MLOCK;\n\t\t\tcontinue;\t/* don't unmap */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte))\n\t\t\tcontinue;\n\n\t\t/* Nuke the page table entry. */\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t\t/* If nonlinear, store the file page offset in the pte. */\n\t\tif (page->index != linear_page_index(vma, address)) {\n\t\t\tpte_t ptfile = pgoff_to_pte(page->index);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tpte_file_mksoft_dirty(ptfile);\n\t\t\tset_pte_at(mm, address, pte, ptfile);\n\t\t}\n\n\t\t/* Move the dirty bit to the physical page now the pte is gone. */\n\t\tif (pte_dirty(pteval))\n\t\t\tset_page_dirty(page);\n\n\t\tpage_remove_rmap(page);\n\t\tpage_cache_release(page);\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t(*mapcount)--;\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\tif (locked_vma)\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\treturn ret;\n}",
        "func": "static int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,\n\t\tstruct vm_area_struct *vma, struct page *check_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tunsigned long address;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tunsigned long end;\n\tint ret = SWAP_AGAIN;\n\tint locked_vma = 0;\n\n\taddress = (vma->vm_start + cursor) & CLUSTER_MASK;\n\tend = address + CLUSTER_SIZE;\n\tif (address < vma->vm_start)\n\t\taddress = vma->vm_start;\n\tif (end > vma->vm_end)\n\t\tend = vma->vm_end;\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn ret;\n\n\tmmun_start = address;\n\tmmun_end   = end;\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,\n\t * keep the sem while scanning the cluster for mlocking pages.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tlocked_vma = (vma->vm_flags & VM_LOCKED);\n\t\tif (!locked_vma)\n\t\t\tup_read(&vma->vm_mm->mmap_sem); /* don't need it */\n\t}\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tfor (; address < end; pte++, address += PAGE_SIZE) {\n\t\tif (!pte_present(*pte))\n\t\t\tcontinue;\n\t\tpage = vm_normal_page(vma, address, *pte);\n\t\tBUG_ON(!page || PageAnon(page));\n\n\t\tif (locked_vma) {\n\t\t\tif (page == check_page) {\n\t\t\t\t/* we know we have check_page locked */\n\t\t\t\tmlock_vma_page(page);\n\t\t\t\tret = SWAP_MLOCK;\n\t\t\t} else if (trylock_page(page)) {\n\t\t\t\t/*\n\t\t\t\t * If we can lock the page, perform mlock.\n\t\t\t\t * Otherwise leave the page alone, it will be\n\t\t\t\t * eventually encountered again later.\n\t\t\t\t */\n\t\t\t\tmlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t}\n\t\t\tcontinue;\t/* don't unmap */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte))\n\t\t\tcontinue;\n\n\t\t/* Nuke the page table entry. */\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t\t/* If nonlinear, store the file page offset in the pte. */\n\t\tif (page->index != linear_page_index(vma, address)) {\n\t\t\tpte_t ptfile = pgoff_to_pte(page->index);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tpte_file_mksoft_dirty(ptfile);\n\t\t\tset_pte_at(mm, address, pte, ptfile);\n\t\t}\n\n\t\t/* Move the dirty bit to the physical page now the pte is gone. */\n\t\tif (pte_dirty(pteval))\n\t\t\tset_page_dirty(page);\n\n\t\tpage_remove_rmap(page);\n\t\tpage_cache_release(page);\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t(*mapcount)--;\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\tif (locked_vma)\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,9 +51,19 @@\n \t\tBUG_ON(!page || PageAnon(page));\n \n \t\tif (locked_vma) {\n-\t\t\tmlock_vma_page(page);   /* no-op if already mlocked */\n-\t\t\tif (page == check_page)\n+\t\t\tif (page == check_page) {\n+\t\t\t\t/* we know we have check_page locked */\n+\t\t\t\tmlock_vma_page(page);\n \t\t\t\tret = SWAP_MLOCK;\n+\t\t\t} else if (trylock_page(page)) {\n+\t\t\t\t/*\n+\t\t\t\t * If we can lock the page, perform mlock.\n+\t\t\t\t * Otherwise leave the page alone, it will be\n+\t\t\t\t * eventually encountered again later.\n+\t\t\t\t */\n+\t\t\t\tmlock_vma_page(page);\n+\t\t\t\tunlock_page(page);\n+\t\t\t}\n \t\t\tcontinue;\t/* don't unmap */\n \t\t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tmlock_vma_page(page);   /* no-op if already mlocked */",
                "\t\t\tif (page == check_page)"
            ],
            "added_lines": [
                "\t\t\tif (page == check_page) {",
                "\t\t\t\t/* we know we have check_page locked */",
                "\t\t\t\tmlock_vma_page(page);",
                "\t\t\t} else if (trylock_page(page)) {",
                "\t\t\t\t/*",
                "\t\t\t\t * If we can lock the page, perform mlock.",
                "\t\t\t\t * Otherwise leave the page alone, it will be",
                "\t\t\t\t * eventually encountered again later.",
                "\t\t\t\t */",
                "\t\t\t\tmlock_vma_page(page);",
                "\t\t\t\tunlock_page(page);",
                "\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::zmtp_engine_t::zmtp_engine_t",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "zmq::zmtp_engine_t::zmtp_engine_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),\n    _greeting_size (v2_greeting_size),\n    _greeting_bytes_read (0),\n    _subscription_required (false),\n    _heartbeat_timeout (0)\n{\n    _next_msg = static_cast<int (stream_engine_base_t::*) (msg_t *)> (\n      &zmtp_engine_t::routing_id_msg);\n    _process_msg = static_cast<int (stream_engine_base_t::*) (msg_t *)> (\n      &zmtp_engine_t::process_routing_id_msg);\n\n    int rc = _pong_msg.init ();\n    errno_assert (rc == 0);\n\n    rc = _routing_id_msg.init ();\n    errno_assert (rc == 0);\n\n    if (_options.heartbeat_interval > 0) {\n        _heartbeat_timeout = _options.heartbeat_timeout;\n        if (_heartbeat_timeout == -1)\n            _heartbeat_timeout = _options.heartbeat_interval;\n    }\n}\n\nzmq::zmtp_engine_t::~zmtp_engine_t ()\n{\n    const int rc = _routing_id_msg.close ();\n    errno_assert (rc == 0);\n}\n\nvoid zmq::zmtp_engine_t::plug_internal ()\n{\n    // start optional timer, to prevent handshake hanging on no input\n    set_handshake_timer ();\n\n    //  Send the 'length' and 'flags' fields of the routing id message.\n    //  The 'length' field is encoded in the long format.\n    _outpos = _greeting_send;\n    _outpos[_outsize++] = UCHAR_MAX;\n    put_uint64 (&_outpos[_outsize], _options.routing_id_size + 1);\n    _outsize += 8;\n    _outpos[_outsize++] = 0x7f;\n\n    set_pollin ();\n    set_pollout ();\n    //  Flush all the data that may have been already received downstream.\n    in_event ();\n}\n\n//  Position of the revision and minor fields in the greeting.\nconst size_t revision_pos = 10;\nconst size_t minor_pos = 11;\n\nbool zmq::zmtp_engine_t::handshake ()\n{\n    zmq_assert (_greeting_bytes_read < _greeting_size);\n    //  Receive the greeting.\n    const int rc = receive_greeting ();\n    if (rc == -1)\n        return false;\n    const bool unversioned = rc != 0;\n\n    if (!(this\n            ->*select_handshake_fun (unversioned, _greeting_recv[revision_pos],\n                                     _greeting_recv[minor_pos])) ())\n        return false;\n\n    // Start polling for output if necessary.\n    if (_outsize == 0)\n        set_pollout ();\n\n    if (_has_handshake_timer) {\n        cancel_timer (handshake_timer_id);\n        _has_handshake_timer = false;\n    }\n\n    return true;\n}\n\nint zmq::zmtp_engine_t::receive_greeting ()\n{\n    bool unversioned = false;\n    while (_greeting_bytes_read < _greeting_size) {\n        const int n = read (_greeting_recv + _greeting_bytes_read,\n                            _greeting_size - _greeting_bytes_read);\n        if (n == -1) {\n            if (errno != EAGAIN)\n                error (connection_error);\n            return -1;\n        }\n\n        _greeting_bytes_read += n;\n\n        //  We have received at least one byte from the peer.\n        //  If the first byte is not 0xff, we know that the\n        //  peer is using unversioned protocol.\n        if (_greeting_recv[0] != 0xff) {\n            unversioned = true;\n            break;\n        }\n\n        if (_greeting_bytes_read < signature_size)\n            continue;\n\n        //  Inspect the right-most bit of the 10th byte (which coincides\n        //  with the 'flags' field if a regular message was sent).\n        //  Zero indicates this is a header of a routing id message\n        //  (i.e. the peer is using the unversioned protocol).\n        if (!(_greeting_recv[9] & 0x01)) {\n            unversioned = true;\n            break;\n        }\n\n        //  The peer is using versioned protocol.\n        receive_greeting_versioned ();\n    }\n    return unversioned ? 1 : 0;\n}\n\nvoid zmq::zmtp_engine_t::receive_greeting_versioned ()\n{\n    //  Send the major version number.\n    if (_outpos + _outsize == _greeting_send + signature_size) {\n        if (_outsize == 0)\n            set_pollout ();\n        _outpos[_outsize++] = 3; //  Major version number\n    }\n\n    if (_greeting_bytes_read > signature_size) {\n        if (_outpos + _outsize == _greeting_send + signature_size + 1) {\n            if (_outsize == 0)\n                set_pollout ();\n\n            //  Use ZMTP/2.0 to talk to older peers.\n            if (_greeting_recv[revision_pos] == ZMTP_1_0\n                || _greeting_recv[revision_pos] == ZMTP_2_0)\n                _outpos[_outsize++] = _options.type;\n            else {\n                _outpos[_outsize++] = 1; //  Minor version number\n                memset (_outpos + _outsize, 0, 20);\n\n                zmq_assert (_options.mechanism == ZMQ_NULL\n                            || _options.mechanism == ZMQ_PLAIN\n                            || _options.mechanism == ZMQ_CURVE\n                            || _options.mechanism == ZMQ_GSSAPI);\n\n                if (_options.mechanism == ZMQ_NULL)\n                    memcpy (_outpos + _outsize, \"NULL\", 4);\n                else if (_options.mechanism == ZMQ_PLAIN)\n                    memcpy (_outpos + _outsize, \"PLAIN\", 5);\n                else if (_options.mechanism == ZMQ_GSSAPI)\n                    memcpy (_outpos + _outsize, \"GSSAPI\", 6);\n                else if (_options.mechanism == ZMQ_CURVE)\n                    memcpy (_outpos + _outsize, \"CURVE\", 5);\n                _outsize += 20;\n                memset (_outpos + _outsize, 0, 32);\n                _outsize += 32;\n                _greeting_size = v3_greeting_size;\n            }\n        }\n    }\n}\n\nzmq::zmtp_engine_t::handshake_fun_t zmq::zmtp_engine_t::select_handshake_fun (\n  bool unversioned_, unsigned char revision_, unsigned char minor_)\n{\n    //  Is the peer using ZMTP/1.0 with no revision number?\n    if (unversioned_) {\n        return &zmtp_engine_t::handshake_v1_0_unversioned;\n    }\n    switch (revision_) {\n        case ZMTP_1_0:\n            return &zmtp_engine_t::handshake_v1_0;\n        case ZMTP_2_0:\n            return &zmtp_engine_t::handshake_v2_0;\n        case ZMTP_3_x:\n            switch (minor_) {\n                case 0:\n                    return &zmtp_engine_t::handshake_v3_0;\n                default:\n                    return &zmtp_engine_t::handshake_v3_1;\n            }\n        default:\n            return &zmtp_engine_t::handshake_v3_1;\n    }\n}",
        "func": "zmq::zmtp_engine_t::zmtp_engine_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),\n    _greeting_size (v2_greeting_size),\n    _greeting_bytes_read (0),\n    _subscription_required (false),\n    _heartbeat_timeout (0)\n{\n    _next_msg = static_cast<int (stream_engine_base_t::*) (msg_t *)> (\n      &zmtp_engine_t::routing_id_msg);\n    _process_msg = static_cast<int (stream_engine_base_t::*) (msg_t *)> (\n      &zmtp_engine_t::process_routing_id_msg);\n\n    int rc = _pong_msg.init ();\n    errno_assert (rc == 0);\n\n    rc = _routing_id_msg.init ();\n    errno_assert (rc == 0);\n\n    if (_options.heartbeat_interval > 0) {\n        _heartbeat_timeout = _options.heartbeat_timeout;\n        if (_heartbeat_timeout == -1)\n            _heartbeat_timeout = _options.heartbeat_interval;\n    }\n}\n\nzmq::zmtp_engine_t::~zmtp_engine_t ()\n{\n    const int rc = _routing_id_msg.close ();\n    errno_assert (rc == 0);\n}\n\nvoid zmq::zmtp_engine_t::plug_internal ()\n{\n    // start optional timer, to prevent handshake hanging on no input\n    set_handshake_timer ();\n\n    //  Send the 'length' and 'flags' fields of the routing id message.\n    //  The 'length' field is encoded in the long format.\n    _outpos = _greeting_send;\n    _outpos[_outsize++] = UCHAR_MAX;\n    put_uint64 (&_outpos[_outsize], _options.routing_id_size + 1);\n    _outsize += 8;\n    _outpos[_outsize++] = 0x7f;\n\n    set_pollin ();\n    set_pollout ();\n    //  Flush all the data that may have been already received downstream.\n    in_event ();\n}\n\n//  Position of the revision and minor fields in the greeting.\nconst size_t revision_pos = 10;\nconst size_t minor_pos = 11;\n\nbool zmq::zmtp_engine_t::handshake ()\n{\n    zmq_assert (_greeting_bytes_read < _greeting_size);\n    //  Receive the greeting.\n    const int rc = receive_greeting ();\n    if (rc == -1)\n        return false;\n    const bool unversioned = rc != 0;\n\n    if (!(this\n            ->*select_handshake_fun (unversioned, _greeting_recv[revision_pos],\n                                     _greeting_recv[minor_pos])) ())\n        return false;\n\n    // Start polling for output if necessary.\n    if (_outsize == 0)\n        set_pollout ();\n\n    if (_has_handshake_timer) {\n        cancel_timer (handshake_timer_id);\n        _has_handshake_timer = false;\n    }\n\n    return true;\n}\n\nint zmq::zmtp_engine_t::receive_greeting ()\n{\n    bool unversioned = false;\n    while (_greeting_bytes_read < _greeting_size) {\n        const int n = read (_greeting_recv + _greeting_bytes_read,\n                            _greeting_size - _greeting_bytes_read);\n        if (n == -1) {\n            if (errno != EAGAIN)\n                error (connection_error);\n            return -1;\n        }\n\n        _greeting_bytes_read += n;\n\n        //  We have received at least one byte from the peer.\n        //  If the first byte is not 0xff, we know that the\n        //  peer is using unversioned protocol.\n        if (_greeting_recv[0] != 0xff) {\n            unversioned = true;\n            break;\n        }\n\n        if (_greeting_bytes_read < signature_size)\n            continue;\n\n        //  Inspect the right-most bit of the 10th byte (which coincides\n        //  with the 'flags' field if a regular message was sent).\n        //  Zero indicates this is a header of a routing id message\n        //  (i.e. the peer is using the unversioned protocol).\n        if (!(_greeting_recv[9] & 0x01)) {\n            unversioned = true;\n            break;\n        }\n\n        //  The peer is using versioned protocol.\n        receive_greeting_versioned ();\n    }\n    return unversioned ? 1 : 0;\n}\n\nvoid zmq::zmtp_engine_t::receive_greeting_versioned ()\n{\n    //  Send the major version number.\n    if (_outpos + _outsize == _greeting_send + signature_size) {\n        if (_outsize == 0)\n            set_pollout ();\n        _outpos[_outsize++] = 3; //  Major version number\n    }\n\n    if (_greeting_bytes_read > signature_size) {\n        if (_outpos + _outsize == _greeting_send + signature_size + 1) {\n            if (_outsize == 0)\n                set_pollout ();\n\n            //  Use ZMTP/2.0 to talk to older peers.\n            if (_greeting_recv[revision_pos] == ZMTP_1_0\n                || _greeting_recv[revision_pos] == ZMTP_2_0)\n                _outpos[_outsize++] = _options.type;\n            else {\n                _outpos[_outsize++] = 1; //  Minor version number\n                memset (_outpos + _outsize, 0, 20);\n\n                zmq_assert (_options.mechanism == ZMQ_NULL\n                            || _options.mechanism == ZMQ_PLAIN\n                            || _options.mechanism == ZMQ_CURVE\n                            || _options.mechanism == ZMQ_GSSAPI);\n\n                if (_options.mechanism == ZMQ_NULL)\n                    memcpy (_outpos + _outsize, \"NULL\", 4);\n                else if (_options.mechanism == ZMQ_PLAIN)\n                    memcpy (_outpos + _outsize, \"PLAIN\", 5);\n                else if (_options.mechanism == ZMQ_GSSAPI)\n                    memcpy (_outpos + _outsize, \"GSSAPI\", 6);\n                else if (_options.mechanism == ZMQ_CURVE)\n                    memcpy (_outpos + _outsize, \"CURVE\", 5);\n                _outsize += 20;\n                memset (_outpos + _outsize, 0, 32);\n                _outsize += 32;\n                _greeting_size = v3_greeting_size;\n            }\n        }\n    }\n}\n\nzmq::zmtp_engine_t::handshake_fun_t zmq::zmtp_engine_t::select_handshake_fun (\n  bool unversioned_, unsigned char revision_, unsigned char minor_)\n{\n    //  Is the peer using ZMTP/1.0 with no revision number?\n    if (unversioned_) {\n        return &zmtp_engine_t::handshake_v1_0_unversioned;\n    }\n    switch (revision_) {\n        case ZMTP_1_0:\n            return &zmtp_engine_t::handshake_v1_0;\n        case ZMTP_2_0:\n            return &zmtp_engine_t::handshake_v2_0;\n        case ZMTP_3_x:\n            switch (minor_) {\n                case 0:\n                    return &zmtp_engine_t::handshake_v3_0;\n                default:\n                    return &zmtp_engine_t::handshake_v3_1;\n            }\n        default:\n            return &zmtp_engine_t::handshake_v3_1;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n   fd_t fd_,\n   const options_t &options_,\n   const endpoint_uri_pair_t &endpoint_uri_pair_) :\n-    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),\n+    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),\n     _greeting_size (v2_greeting_size),\n     _greeting_bytes_read (0),\n     _subscription_required (false),",
        "diff_line_info": {
            "deleted_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),"
            ],
            "added_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::raw_engine_t::raw_engine_t",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "zmq::raw_engine_t::raw_engine_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_)\n{\n}",
        "func": "zmq::raw_engine_t::raw_engine_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, false)\n{\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,6 @@\n   fd_t fd_,\n   const options_t &options_,\n   const endpoint_uri_pair_t &endpoint_uri_pair_) :\n-    stream_engine_base_t (fd_, options_, endpoint_uri_pair_)\n+    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, false)\n {\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_)"
            ],
            "added_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, false)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::session_base_t::process_attach",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "void zmq::session_base_t::process_attach (i_engine *engine_)\n{\n    zmq_assert (engine_ != NULL);\n\n    //  Create the pipe if it does not exist yet.\n    if (!_pipe && !is_terminating ()) {\n        object_t *parents[2] = {this, _socket};\n        pipe_t *pipes[2] = {NULL, NULL};\n\n        const bool conflate = get_effective_conflate_option (options);\n\n        int hwms[2] = {conflate ? -1 : options.rcvhwm,\n                       conflate ? -1 : options.sndhwm};\n        bool conflates[2] = {conflate, conflate};\n        const int rc = pipepair (parents, pipes, hwms, conflates);\n        errno_assert (rc == 0);\n\n        //  Plug the local end of the pipe.\n        pipes[0]->set_event_sink (this);\n\n        //  Remember the local end of the pipe.\n        zmq_assert (!_pipe);\n        _pipe = pipes[0];\n\n        //  The endpoints strings are not set on bind, set them here so that\n        //  events can use them.\n        pipes[0]->set_endpoint_pair (engine_->get_endpoint ());\n        pipes[1]->set_endpoint_pair (engine_->get_endpoint ());\n\n        //  Ask socket to plug into the remote end of the pipe.\n        send_bind (_socket, pipes[1]);\n    }\n\n    //  Plug in the engine.\n    zmq_assert (!_engine);\n    _engine = engine_;\n    _engine->plug (_io_thread, this);\n}",
        "func": "void zmq::session_base_t::process_attach (i_engine *engine_)\n{\n    zmq_assert (engine_ != NULL);\n    zmq_assert (!_engine);\n    _engine = engine_;\n\n    if (!engine_->has_handshake_stage ())\n        engine_ready ();\n\n    //  Plug in the engine.\n    _engine->plug (_io_thread, this);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,38 +1,12 @@\n void zmq::session_base_t::process_attach (i_engine *engine_)\n {\n     zmq_assert (engine_ != NULL);\n+    zmq_assert (!_engine);\n+    _engine = engine_;\n \n-    //  Create the pipe if it does not exist yet.\n-    if (!_pipe && !is_terminating ()) {\n-        object_t *parents[2] = {this, _socket};\n-        pipe_t *pipes[2] = {NULL, NULL};\n-\n-        const bool conflate = get_effective_conflate_option (options);\n-\n-        int hwms[2] = {conflate ? -1 : options.rcvhwm,\n-                       conflate ? -1 : options.sndhwm};\n-        bool conflates[2] = {conflate, conflate};\n-        const int rc = pipepair (parents, pipes, hwms, conflates);\n-        errno_assert (rc == 0);\n-\n-        //  Plug the local end of the pipe.\n-        pipes[0]->set_event_sink (this);\n-\n-        //  Remember the local end of the pipe.\n-        zmq_assert (!_pipe);\n-        _pipe = pipes[0];\n-\n-        //  The endpoints strings are not set on bind, set them here so that\n-        //  events can use them.\n-        pipes[0]->set_endpoint_pair (engine_->get_endpoint ());\n-        pipes[1]->set_endpoint_pair (engine_->get_endpoint ());\n-\n-        //  Ask socket to plug into the remote end of the pipe.\n-        send_bind (_socket, pipes[1]);\n-    }\n+    if (!engine_->has_handshake_stage ())\n+        engine_ready ();\n \n     //  Plug in the engine.\n-    zmq_assert (!_engine);\n-    _engine = engine_;\n     _engine->plug (_io_thread, this);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    //  Create the pipe if it does not exist yet.",
                "    if (!_pipe && !is_terminating ()) {",
                "        object_t *parents[2] = {this, _socket};",
                "        pipe_t *pipes[2] = {NULL, NULL};",
                "",
                "        const bool conflate = get_effective_conflate_option (options);",
                "",
                "        int hwms[2] = {conflate ? -1 : options.rcvhwm,",
                "                       conflate ? -1 : options.sndhwm};",
                "        bool conflates[2] = {conflate, conflate};",
                "        const int rc = pipepair (parents, pipes, hwms, conflates);",
                "        errno_assert (rc == 0);",
                "",
                "        //  Plug the local end of the pipe.",
                "        pipes[0]->set_event_sink (this);",
                "",
                "        //  Remember the local end of the pipe.",
                "        zmq_assert (!_pipe);",
                "        _pipe = pipes[0];",
                "",
                "        //  The endpoints strings are not set on bind, set them here so that",
                "        //  events can use them.",
                "        pipes[0]->set_endpoint_pair (engine_->get_endpoint ());",
                "        pipes[1]->set_endpoint_pair (engine_->get_endpoint ());",
                "",
                "        //  Ask socket to plug into the remote end of the pipe.",
                "        send_bind (_socket, pipes[1]);",
                "    }",
                "    zmq_assert (!_engine);",
                "    _engine = engine_;"
            ],
            "added_lines": [
                "    zmq_assert (!_engine);",
                "    _engine = engine_;",
                "    if (!engine_->has_handshake_stage ())",
                "        engine_ready ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::ws_engine_t::ws_engine_t",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "zmq::ws_engine_t::ws_engine_t (fd_t fd_,\n                               const options_t &options_,\n                               const endpoint_uri_pair_t &endpoint_uri_pair_,\n                               const ws_address_t &address_,\n                               bool client_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),\n    _client (client_),\n    _address (address_),\n    _client_handshake_state (client_handshake_initial),\n    _server_handshake_state (handshake_initial),\n    _header_name_position (0),\n    _header_value_position (0),\n    _header_upgrade_websocket (false),\n    _header_connection_upgrade (false),\n    _heartbeat_timeout (0)\n{\n    memset (_websocket_key, 0, MAX_HEADER_VALUE_LENGTH + 1);\n    memset (_websocket_accept, 0, MAX_HEADER_VALUE_LENGTH + 1);\n    memset (_websocket_protocol, 0, 256);\n\n    _next_msg = &ws_engine_t::next_handshake_command;\n    _process_msg = &ws_engine_t::process_handshake_command;\n    _close_msg.init ();\n\n    if (_options.heartbeat_interval > 0) {\n        _heartbeat_timeout = _options.heartbeat_timeout;\n        if (_heartbeat_timeout == -1)\n            _heartbeat_timeout = _options.heartbeat_interval;\n    }\n}",
        "func": "zmq::ws_engine_t::ws_engine_t (fd_t fd_,\n                               const options_t &options_,\n                               const endpoint_uri_pair_t &endpoint_uri_pair_,\n                               const ws_address_t &address_,\n                               bool client_) :\n    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),\n    _client (client_),\n    _address (address_),\n    _client_handshake_state (client_handshake_initial),\n    _server_handshake_state (handshake_initial),\n    _header_name_position (0),\n    _header_value_position (0),\n    _header_upgrade_websocket (false),\n    _header_connection_upgrade (false),\n    _heartbeat_timeout (0)\n{\n    memset (_websocket_key, 0, MAX_HEADER_VALUE_LENGTH + 1);\n    memset (_websocket_accept, 0, MAX_HEADER_VALUE_LENGTH + 1);\n    memset (_websocket_protocol, 0, 256);\n\n    _next_msg = &ws_engine_t::next_handshake_command;\n    _process_msg = &ws_engine_t::process_handshake_command;\n    _close_msg.init ();\n\n    if (_options.heartbeat_interval > 0) {\n        _heartbeat_timeout = _options.heartbeat_timeout;\n        if (_heartbeat_timeout == -1)\n            _heartbeat_timeout = _options.heartbeat_interval;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n                                const endpoint_uri_pair_t &endpoint_uri_pair_,\n                                const ws_address_t &address_,\n                                bool client_) :\n-    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),\n+    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),\n     _client (client_),\n     _address (address_),\n     _client_handshake_state (client_handshake_initial),",
        "diff_line_info": {
            "deleted_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_),"
            ],
            "added_lines": [
                "    stream_engine_base_t (fd_, options_, endpoint_uri_pair_, true),"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::stream_engine_base_t::in_event_internal",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "bool zmq::stream_engine_base_t::in_event_internal ()\n{\n    zmq_assert (!_io_error);\n\n    //  If still handshaking, receive and process the greeting message.\n    if (unlikely (_handshaking)) {\n        if (handshake ()) {\n            //  Handshaking was successful.\n            //  Switch into the normal message flow.\n            _handshaking = false;\n        } else\n            return false;\n    }\n\n\n    zmq_assert (_decoder);\n\n    //  If there has been an I/O error, stop polling.\n    if (_input_stopped) {\n        rm_fd (_handle);\n        _io_error = true;\n        return true; // TODO or return false in this case too?\n    }\n\n    //  If there's no data to process in the buffer...\n    if (!_insize) {\n        //  Retrieve the buffer and read as much data as possible.\n        //  Note that buffer can be arbitrarily large. However, we assume\n        //  the underlying TCP layer has fixed buffer size and thus the\n        //  number of bytes read will be always limited.\n        size_t bufsize = 0;\n        _decoder->get_buffer (&_inpos, &bufsize);\n\n        const int rc = read (_inpos, bufsize);\n\n        if (rc == -1) {\n            if (errno != EAGAIN) {\n                error (connection_error);\n                return false;\n            }\n            return true;\n        }\n\n        //  Adjust input size\n        _insize = static_cast<size_t> (rc);\n        // Adjust buffer size to received bytes\n        _decoder->resize_buffer (_insize);\n    }\n\n    int rc = 0;\n    size_t processed = 0;\n\n    while (_insize > 0) {\n        rc = _decoder->decode (_inpos, _insize, processed);\n        zmq_assert (processed <= _insize);\n        _inpos += processed;\n        _insize -= processed;\n        if (rc == 0 || rc == -1)\n            break;\n        rc = (this->*_process_msg) (_decoder->msg ());\n        if (rc == -1)\n            break;\n    }\n\n    //  Tear down the connection if we have failed to decode input data\n    //  or the session has rejected the message.\n    if (rc == -1) {\n        if (errno != EAGAIN) {\n            error (protocol_error);\n            return false;\n        }\n        _input_stopped = true;\n        reset_pollin (_handle);\n    }\n\n    _session->flush ();\n    return true;\n}",
        "func": "bool zmq::stream_engine_base_t::in_event_internal ()\n{\n    zmq_assert (!_io_error);\n\n    //  If still handshaking, receive and process the greeting message.\n    if (unlikely (_handshaking)) {\n        if (handshake ()) {\n            //  Handshaking was successful.\n            //  Switch into the normal message flow.\n            _handshaking = false;\n\n            if (_mechanism == NULL && _has_handshake_stage)\n                _session->engine_ready ();\n        } else\n            return false;\n    }\n\n\n    zmq_assert (_decoder);\n\n    //  If there has been an I/O error, stop polling.\n    if (_input_stopped) {\n        rm_fd (_handle);\n        _io_error = true;\n        return true; // TODO or return false in this case too?\n    }\n\n    //  If there's no data to process in the buffer...\n    if (!_insize) {\n        //  Retrieve the buffer and read as much data as possible.\n        //  Note that buffer can be arbitrarily large. However, we assume\n        //  the underlying TCP layer has fixed buffer size and thus the\n        //  number of bytes read will be always limited.\n        size_t bufsize = 0;\n        _decoder->get_buffer (&_inpos, &bufsize);\n\n        const int rc = read (_inpos, bufsize);\n\n        if (rc == -1) {\n            if (errno != EAGAIN) {\n                error (connection_error);\n                return false;\n            }\n            return true;\n        }\n\n        //  Adjust input size\n        _insize = static_cast<size_t> (rc);\n        // Adjust buffer size to received bytes\n        _decoder->resize_buffer (_insize);\n    }\n\n    int rc = 0;\n    size_t processed = 0;\n\n    while (_insize > 0) {\n        rc = _decoder->decode (_inpos, _insize, processed);\n        zmq_assert (processed <= _insize);\n        _inpos += processed;\n        _insize -= processed;\n        if (rc == 0 || rc == -1)\n            break;\n        rc = (this->*_process_msg) (_decoder->msg ());\n        if (rc == -1)\n            break;\n    }\n\n    //  Tear down the connection if we have failed to decode input data\n    //  or the session has rejected the message.\n    if (rc == -1) {\n        if (errno != EAGAIN) {\n            error (protocol_error);\n            return false;\n        }\n        _input_stopped = true;\n        reset_pollin (_handle);\n    }\n\n    _session->flush ();\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,9 @@\n             //  Handshaking was successful.\n             //  Switch into the normal message flow.\n             _handshaking = false;\n+\n+            if (_mechanism == NULL && _has_handshake_stage)\n+                _session->engine_ready ();\n         } else\n             return false;\n     }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "            if (_mechanism == NULL && _has_handshake_stage)",
                "                _session->engine_ready ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::stream_engine_base_t::stream_engine_base_t",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "zmq::stream_engine_base_t::stream_engine_base_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n    _options (options_),\n    _inpos (NULL),\n    _insize (0),\n    _decoder (NULL),\n    _outpos (NULL),\n    _outsize (0),\n    _encoder (NULL),\n    _mechanism (NULL),\n    _next_msg (NULL),\n    _process_msg (NULL),\n    _metadata (NULL),\n    _input_stopped (false),\n    _output_stopped (false),\n    _endpoint_uri_pair (endpoint_uri_pair_),\n    _has_handshake_timer (false),\n    _has_ttl_timer (false),\n    _has_timeout_timer (false),\n    _has_heartbeat_timer (false),\n    _peer_address (get_peer_address (fd_)),\n    _s (fd_),\n    _handle (static_cast<handle_t> (NULL)),\n    _plugged (false),\n    _handshaking (true),\n    _io_error (false),\n    _session (NULL),\n    _socket (NULL)\n{\n    const int rc = _tx_msg.init ();\n    errno_assert (rc == 0);\n\n    //  Put the socket into non-blocking mode.\n    unblock_socket (_s);\n}",
        "func": "zmq::stream_engine_base_t::stream_engine_base_t (\n  fd_t fd_,\n  const options_t &options_,\n  const endpoint_uri_pair_t &endpoint_uri_pair_,\n  bool has_handshake_stage_) :\n    _options (options_),\n    _inpos (NULL),\n    _insize (0),\n    _decoder (NULL),\n    _outpos (NULL),\n    _outsize (0),\n    _encoder (NULL),\n    _mechanism (NULL),\n    _next_msg (NULL),\n    _process_msg (NULL),\n    _metadata (NULL),\n    _input_stopped (false),\n    _output_stopped (false),\n    _endpoint_uri_pair (endpoint_uri_pair_),\n    _has_handshake_timer (false),\n    _has_ttl_timer (false),\n    _has_timeout_timer (false),\n    _has_heartbeat_timer (false),\n    _peer_address (get_peer_address (fd_)),\n    _s (fd_),\n    _handle (static_cast<handle_t> (NULL)),\n    _plugged (false),\n    _handshaking (true),\n    _io_error (false),\n    _session (NULL),\n    _socket (NULL),\n    _has_handshake_stage (has_handshake_stage_)\n{\n    const int rc = _tx_msg.init ();\n    errno_assert (rc == 0);\n\n    //  Put the socket into non-blocking mode.\n    unblock_socket (_s);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,8 @@\n zmq::stream_engine_base_t::stream_engine_base_t (\n   fd_t fd_,\n   const options_t &options_,\n-  const endpoint_uri_pair_t &endpoint_uri_pair_) :\n+  const endpoint_uri_pair_t &endpoint_uri_pair_,\n+  bool has_handshake_stage_) :\n     _options (options_),\n     _inpos (NULL),\n     _insize (0),\n@@ -27,7 +28,8 @@\n     _handshaking (true),\n     _io_error (false),\n     _session (NULL),\n-    _socket (NULL)\n+    _socket (NULL),\n+    _has_handshake_stage (has_handshake_stage_)\n {\n     const int rc = _tx_msg.init ();\n     errno_assert (rc == 0);",
        "diff_line_info": {
            "deleted_lines": [
                "  const endpoint_uri_pair_t &endpoint_uri_pair_) :",
                "    _socket (NULL)"
            ],
            "added_lines": [
                "  const endpoint_uri_pair_t &endpoint_uri_pair_,",
                "  bool has_handshake_stage_) :",
                "    _socket (NULL),",
                "    _has_handshake_stage (has_handshake_stage_)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::stream_engine_base_t::mechanism_ready",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/e7f0090b161ce6344f6bd35009816a925c070b09",
        "commit_title": "problem: zeromq connects peer before handshake is completed",
        "commit_text": " Solution: delay connecting the peer pipe until the handshake is completed",
        "func_before": "void zmq::stream_engine_base_t::mechanism_ready ()\n{\n    if (_options.heartbeat_interval > 0 && !_has_heartbeat_timer) {\n        add_timer (_options.heartbeat_interval, heartbeat_ivl_timer_id);\n        _has_heartbeat_timer = true;\n    }\n\n    bool flush_session = false;\n\n    if (_options.recv_routing_id) {\n        msg_t routing_id;\n        _mechanism->peer_routing_id (&routing_id);\n        const int rc = _session->push_msg (&routing_id);\n        if (rc == -1 && errno == EAGAIN) {\n            // If the write is failing at this stage with\n            // an EAGAIN the pipe must be being shut down,\n            // so we can just bail out of the routing id set.\n            return;\n        }\n        errno_assert (rc == 0);\n        flush_session = true;\n    }\n\n    if (_options.router_notify & ZMQ_NOTIFY_CONNECT) {\n        msg_t connect_notification;\n        connect_notification.init ();\n        const int rc = _session->push_msg (&connect_notification);\n        if (rc == -1 && errno == EAGAIN) {\n            // If the write is failing at this stage with\n            // an EAGAIN the pipe must be being shut down,\n            // so we can just bail out of the notification.\n            return;\n        }\n        errno_assert (rc == 0);\n        flush_session = true;\n    }\n\n    if (flush_session)\n        _session->flush ();\n\n    _next_msg = &stream_engine_base_t::pull_and_encode;\n    _process_msg = &stream_engine_base_t::write_credential;\n\n    //  Compile metadata.\n    properties_t properties;\n    init_properties (properties);\n\n    //  Add ZAP properties.\n    const properties_t &zap_properties = _mechanism->get_zap_properties ();\n    properties.insert (zap_properties.begin (), zap_properties.end ());\n\n    //  Add ZMTP properties.\n    const properties_t &zmtp_properties = _mechanism->get_zmtp_properties ();\n    properties.insert (zmtp_properties.begin (), zmtp_properties.end ());\n\n    zmq_assert (_metadata == NULL);\n    if (!properties.empty ()) {\n        _metadata = new (std::nothrow) metadata_t (properties);\n        alloc_assert (_metadata);\n    }\n\n    _socket->event_handshake_succeeded (_endpoint_uri_pair, 0);\n}",
        "func": "void zmq::stream_engine_base_t::mechanism_ready ()\n{\n    if (_options.heartbeat_interval > 0 && !_has_heartbeat_timer) {\n        add_timer (_options.heartbeat_interval, heartbeat_ivl_timer_id);\n        _has_heartbeat_timer = true;\n    }\n\n    if (_has_handshake_stage)\n        _session->engine_ready ();\n\n    bool flush_session = false;\n\n    if (_options.recv_routing_id) {\n        msg_t routing_id;\n        _mechanism->peer_routing_id (&routing_id);\n        const int rc = _session->push_msg (&routing_id);\n        if (rc == -1 && errno == EAGAIN) {\n            // If the write is failing at this stage with\n            // an EAGAIN the pipe must be being shut down,\n            // so we can just bail out of the routing id set.\n            return;\n        }\n        errno_assert (rc == 0);\n        flush_session = true;\n    }\n\n    if (_options.router_notify & ZMQ_NOTIFY_CONNECT) {\n        msg_t connect_notification;\n        connect_notification.init ();\n        const int rc = _session->push_msg (&connect_notification);\n        if (rc == -1 && errno == EAGAIN) {\n            // If the write is failing at this stage with\n            // an EAGAIN the pipe must be being shut down,\n            // so we can just bail out of the notification.\n            return;\n        }\n        errno_assert (rc == 0);\n        flush_session = true;\n    }\n\n    if (flush_session)\n        _session->flush ();\n\n    _next_msg = &stream_engine_base_t::pull_and_encode;\n    _process_msg = &stream_engine_base_t::write_credential;\n\n    //  Compile metadata.\n    properties_t properties;\n    init_properties (properties);\n\n    //  Add ZAP properties.\n    const properties_t &zap_properties = _mechanism->get_zap_properties ();\n    properties.insert (zap_properties.begin (), zap_properties.end ());\n\n    //  Add ZMTP properties.\n    const properties_t &zmtp_properties = _mechanism->get_zmtp_properties ();\n    properties.insert (zmtp_properties.begin (), zmtp_properties.end ());\n\n    zmq_assert (_metadata == NULL);\n    if (!properties.empty ()) {\n        _metadata = new (std::nothrow) metadata_t (properties);\n        alloc_assert (_metadata);\n    }\n\n    _socket->event_handshake_succeeded (_endpoint_uri_pair, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,9 @@\n         add_timer (_options.heartbeat_interval, heartbeat_ivl_timer_id);\n         _has_heartbeat_timer = true;\n     }\n+\n+    if (_has_handshake_stage)\n+        _session->engine_ready ();\n \n     bool flush_session = false;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if (_has_handshake_stage)",
                "        _session->engine_ready ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15166",
        "func_name": "zeromq/libzmq/zmq::session_base_t::read_activated",
        "description": "In ZeroMQ before version 4.3.3, there is a denial-of-service vulnerability. Users with TCP transport public endpoints, even with CURVE/ZAP enabled, are impacted. If a raw TCP socket is opened and connected to an endpoint that is fully configured with CURVE/ZAP, legitimate clients will not be able to exchange any message. Handshakes complete successfully, and messages are delivered to the library, but the server application never receives them. This is patched in version 4.3.3.",
        "git_url": "https://github.com/zeromq/libzmq/commit/350b4b34f460b91b8fa8f692cf6bc30d561a5711",
        "commit_title": "Problem: test_security_zap occasionally segfaults",
        "commit_text": " Solution: check if a session's _pipe has been allocated before using it, since as a consequence of creating the pipes after the handshake it's no longer guaranteed to be there.  Fixes #3971",
        "func_before": "void zmq::session_base_t::read_activated (pipe_t *pipe_)\n{\n    // Skip activating if we're detaching this pipe\n    if (unlikely (pipe_ != _pipe && pipe_ != _zap_pipe)) {\n        zmq_assert (_terminating_pipes.count (pipe_) == 1);\n        return;\n    }\n\n    if (unlikely (_engine == NULL)) {\n        _pipe->check_read ();\n        return;\n    }\n\n    if (likely (pipe_ == _pipe))\n        _engine->restart_output ();\n    else {\n        // i.e. pipe_ == zap_pipe\n        _engine->zap_msg_available ();\n    }\n}",
        "func": "void zmq::session_base_t::read_activated (pipe_t *pipe_)\n{\n    // Skip activating if we're detaching this pipe\n    if (unlikely (pipe_ != _pipe && pipe_ != _zap_pipe)) {\n        zmq_assert (_terminating_pipes.count (pipe_) == 1);\n        return;\n    }\n\n    if (unlikely (_engine == NULL)) {\n        if (_pipe)\n            _pipe->check_read ();\n        return;\n    }\n\n    if (likely (pipe_ == _pipe))\n        _engine->restart_output ();\n    else {\n        // i.e. pipe_ == zap_pipe\n        _engine->zap_msg_available ();\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,8 @@\n     }\n \n     if (unlikely (_engine == NULL)) {\n-        _pipe->check_read ();\n+        if (_pipe)\n+            _pipe->check_read ();\n         return;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        _pipe->check_read ();"
            ],
            "added_lines": [
                "        if (_pipe)",
                "            _pipe->check_read ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-30858",
        "func_name": "miniupnp/ngiflib/SDL_LoadAnimatedGif",
        "description": "An issue was discovered in ngiflib 0.4. There is SEGV in SDL_LoadAnimatedGif when use SDLaffgif. poc : ./SDLaffgif CA_file2_0",
        "git_url": "https://github.com/miniupnp/ngiflib/commit/231df822d2ec2096d4ccf8d0b9e6a9c6bbfd3b69",
        "commit_title": "ngiflibSDL.c: do not coredump on GIF without any palette",
        "commit_text": " fixes #22",
        "func_before": "struct ngiflibSDL_animation * SDL_LoadAnimatedGif(const char * file)\n{\n\tSDL_Surface * surface;\n\tstruct ngiflib_gif * gif;\n\tFILE *fgif;\n\tint err,i;\n\tu8 * pdst, * psrc;\n\tu8 * p = NULL;\n#ifdef NGIFLIB_NO_FILE\n\tu8 * buffer;\n\tlong filesize;\n#endif /* NGIFLIB_NO_FILE */\n\tint image_count = 0;\n\tint image_count_max = 50;\n\tstruct ngiflibSDL_animation * animation = NULL;\n\tstruct ngiflib_rgb * current_palette = NULL;\n\tint current_palette_size = 0;\n\n\tfgif = fopen(file, \"rb\");\n\tif(fgif==NULL)\n\t\treturn NULL;\n\tgif = (struct ngiflib_gif *)ngiflib_malloc(sizeof(struct ngiflib_gif));\n#ifdef EXTRA_MALLOC_CHECK\n\tif(gif == NULL) {\n\t\treturn NULL;\n\t}\n#endif /* EXTRA_MALLOC_CHECK */\n\tngiflib_memset(gif, 0, sizeof(struct ngiflib_gif));\n#ifdef NGIFLIB_NO_FILE\n\tfseek(fgif, 0, SEEK_END);\n\tfilesize = ftell(fgif);\n\tfseek(fgif, 0, SEEK_SET);\n\tbuffer = malloc(filesize);\n\tif(buffer == NULL) {\n\t\tGifDestroy(gif);\n\t\treturn NULL;\n\t}\n\tfread(buffer, 1, filesize, fgif);\n\tgif->input.buffer.bytes = buffer;\n\tgif->input.buffer.count = (unsigned long)filesize;\n\tgif->mode = NGIFLIB_MODE_FROM_MEM | NGIFLIB_MODE_INDEXED;\n#else /* NGIFLIB_NO_FILE */\n\tgif->input.file = fgif;\n\t/*gif->mode = NGIFLIB_MODE_FROM_FILE | NGIFLIB_MODE_TRUE_COLOR; */\n\tgif->mode = NGIFLIB_MODE_FROM_FILE | NGIFLIB_MODE_INDEXED;\n#ifdef NGIFLIBSDL_LOG\n\tgif->log = stdout;\n#endif /* NGIFLIBSDL_LOG */\n#endif /* NGIFLIB_NO_FILE */\n\twhile((err = LoadGif(gif)) == 1) {\n\t\tif(animation == NULL) {\n\t\t\tanimation = ngiflib_malloc(sizeof(struct ngiflibSDL_animation) + image_count_max*sizeof(struct ngiflibSDL_image));\n\t\t\tif(animation == NULL)\n\t\t\t\treturn NULL;\n\t\t} else if(image_count >= image_count_max) {\n\t\t\timage_count_max += 50;\n\t\t\tstruct ngiflibSDL_animation * tmp;\n\t\t\ttmp = realloc(animation, sizeof(struct ngiflibSDL_animation) + image_count_max*sizeof(struct ngiflibSDL_image));\n\t\t\tif(tmp == NULL) {\n\t\t\t\tfprintf(stderr, \"realloc() failed, cannot decode more images\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tanimation = tmp;\n\t\t}\n\t\tp = gif->frbuff.p8;\n\t\t/*\n\t\tsurface = SDL_CreateRGBSurfaceFrom(p, gif->width, gif->height,\n\t\t                                   32, gif->width << 2,\n\t\t\t\t\t\t\t\t\t\t   0x00ff0000,\n\t\t\t\t\t\t\t\t\t\t   0x0000ff00,\n\t\t\t\t\t\t\t\t\t\t   0x000000ff,\n\t\t\t\t\t\t\t\t\t\t   0xff000000);\n\t\t*/\n\t\tsurface = SDL_CreateRGBSurface(SDL_SWSURFACE | SDL_SRCCOLORKEY,\n\t\t                               gif->width, gif->height, 8,\n\t\t\t\t\t\t\t\t\t   0,0,0,0);\n\t\tSDL_LockSurface(surface);\n\t\t/*\n\t\tif(gif->transparent_flag) {\n\t\t\tSDL_SetColorKey(surface, SDL_SRCCOLORKEY, gif->transparent_color);\n\t\t}\n\t\t*/\n\t\tif(gif->palette != gif->cur_img->palette) {\n\t\t\tcurrent_palette = gif->cur_img->palette;\n\t\t\tcurrent_palette_size = (1 << gif->cur_img->localpalbits);\n\t\t} else if(current_palette == NULL) {\n\t\t\tcurrent_palette = gif->palette;\n\t\t\tcurrent_palette_size = gif->ncolors;\n\t\t}\n\t\tfor(i = 0; i < current_palette_size; i++) {\n\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;\n\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;\n\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;\n\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);\n\t\t}\n\t\tfor(; i < gif->ncolors; i++) {\n\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;\n\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;\n\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;\n\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);\n\t\t}\n\t\tprintf(\"\\n\");\n\t\tpsrc = p; pdst = surface->pixels;\n\t\tfor(i=0; i<gif->height; i++) {\n\t\t\tngiflib_memcpy(pdst, psrc, gif->width);\n\t\t\tpdst += surface->pitch;\n\t\t\tpsrc += gif->width;\n\t\t}\n\t\tSDL_UnlockSurface(surface);\n\t\tanimation->images[image_count].delay_time = -1;\n\t\tif(gif->cur_img->gce.gce_present) {\n\t\t\tanimation->images[image_count].delay_time = gif->cur_img->gce.delay_time;\n\t\t}\n\t\tanimation->images[image_count].surface = surface;\n\t\timage_count++;\n\t}\n\tfclose(fgif);\n#ifdef NGIFLIB_NO_FILE\n\tfree(buffer);\n#endif /* NGIFLIB_NO_FILE */\n\tGifDestroy(gif);\n\tif(animation) animation->image_count = image_count;\n\treturn animation;\n}",
        "func": "struct ngiflibSDL_animation * SDL_LoadAnimatedGif(const char * file)\n{\n\tSDL_Surface * surface;\n\tstruct ngiflib_gif * gif;\n\tFILE *fgif;\n\tint err,i;\n\tu8 * pdst, * psrc;\n\tu8 * p = NULL;\n#ifdef NGIFLIB_NO_FILE\n\tu8 * buffer;\n\tlong filesize;\n#endif /* NGIFLIB_NO_FILE */\n\tint image_count = 0;\n\tint image_count_max = 50;\n\tstruct ngiflibSDL_animation * animation = NULL;\n\tstruct ngiflib_rgb * current_palette = NULL;\n\tint current_palette_size = 0;\n\n\tfgif = fopen(file, \"rb\");\n\tif(fgif==NULL)\n\t\treturn NULL;\n\tgif = (struct ngiflib_gif *)ngiflib_malloc(sizeof(struct ngiflib_gif));\n#ifdef EXTRA_MALLOC_CHECK\n\tif(gif == NULL) {\n\t\treturn NULL;\n\t}\n#endif /* EXTRA_MALLOC_CHECK */\n\tngiflib_memset(gif, 0, sizeof(struct ngiflib_gif));\n#ifdef NGIFLIB_NO_FILE\n\tfseek(fgif, 0, SEEK_END);\n\tfilesize = ftell(fgif);\n\tfseek(fgif, 0, SEEK_SET);\n\tbuffer = malloc(filesize);\n\tif(buffer == NULL) {\n\t\tGifDestroy(gif);\n\t\treturn NULL;\n\t}\n\tfread(buffer, 1, filesize, fgif);\n\tgif->input.buffer.bytes = buffer;\n\tgif->input.buffer.count = (unsigned long)filesize;\n\tgif->mode = NGIFLIB_MODE_FROM_MEM | NGIFLIB_MODE_INDEXED;\n#else /* NGIFLIB_NO_FILE */\n\tgif->input.file = fgif;\n\t/*gif->mode = NGIFLIB_MODE_FROM_FILE | NGIFLIB_MODE_TRUE_COLOR; */\n\tgif->mode = NGIFLIB_MODE_FROM_FILE | NGIFLIB_MODE_INDEXED;\n#ifdef NGIFLIBSDL_LOG\n\tgif->log = stdout;\n#endif /* NGIFLIBSDL_LOG */\n#endif /* NGIFLIB_NO_FILE */\n\twhile((err = LoadGif(gif)) == 1) {\n\t\tif(animation == NULL) {\n\t\t\tanimation = ngiflib_malloc(sizeof(struct ngiflibSDL_animation) + image_count_max*sizeof(struct ngiflibSDL_image));\n\t\t\tif(animation == NULL)\n\t\t\t\treturn NULL;\n\t\t} else if(image_count >= image_count_max) {\n\t\t\timage_count_max += 50;\n\t\t\tstruct ngiflibSDL_animation * tmp;\n\t\t\ttmp = realloc(animation, sizeof(struct ngiflibSDL_animation) + image_count_max*sizeof(struct ngiflibSDL_image));\n\t\t\tif(tmp == NULL) {\n\t\t\t\tfprintf(stderr, \"realloc() failed, cannot decode more images\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tanimation = tmp;\n\t\t}\n\t\tp = gif->frbuff.p8;\n\t\t/*\n\t\tsurface = SDL_CreateRGBSurfaceFrom(p, gif->width, gif->height,\n\t\t                                   32, gif->width << 2,\n\t\t\t\t\t\t\t\t\t\t   0x00ff0000,\n\t\t\t\t\t\t\t\t\t\t   0x0000ff00,\n\t\t\t\t\t\t\t\t\t\t   0x000000ff,\n\t\t\t\t\t\t\t\t\t\t   0xff000000);\n\t\t*/\n\t\tsurface = SDL_CreateRGBSurface(SDL_SWSURFACE | SDL_SRCCOLORKEY,\n\t\t                               gif->width, gif->height, 8,\n\t\t\t\t\t\t\t\t\t   0,0,0,0);\n\t\tSDL_LockSurface(surface);\n\t\t/*\n\t\tif(gif->transparent_flag) {\n\t\t\tSDL_SetColorKey(surface, SDL_SRCCOLORKEY, gif->transparent_color);\n\t\t}\n\t\t*/\n\t\tif(gif->palette != gif->cur_img->palette) {\n\t\t\tcurrent_palette = gif->cur_img->palette;\n\t\t\tcurrent_palette_size = (1 << gif->cur_img->localpalbits);\n\t\t} else if(current_palette == NULL) {\n\t\t\tcurrent_palette = gif->palette;\n\t\t\tcurrent_palette_size = gif->ncolors;\n\t\t}\n\t\tif (current_palette != NULL) {\n\t\t\tfor(i = 0; i < current_palette_size; i++) {\n\t\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;\n\t\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;\n\t\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;\n\t\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);\n\t\t\t}\n\t\t\tfor(; i < gif->ncolors; i++) {\n\t\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;\n\t\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;\n\t\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;\n\t\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);\n\t\t\t}\n\t\t\t//printf(\"\\n\");\n\t\t} else {\n\t\t\tfprintf(stderr, \"no palette in GIF\\n\");\n\t\t}\n\t\tpsrc = p; pdst = surface->pixels;\n\t\tfor(i=0; i<gif->height; i++) {\n\t\t\tngiflib_memcpy(pdst, psrc, gif->width);\n\t\t\tpdst += surface->pitch;\n\t\t\tpsrc += gif->width;\n\t\t}\n\t\tSDL_UnlockSurface(surface);\n\t\tanimation->images[image_count].delay_time = -1;\n\t\tif(gif->cur_img->gce.gce_present) {\n\t\t\tanimation->images[image_count].delay_time = gif->cur_img->gce.delay_time;\n\t\t}\n\t\tanimation->images[image_count].surface = surface;\n\t\timage_count++;\n\t}\n\tfclose(fgif);\n#ifdef NGIFLIB_NO_FILE\n\tfree(buffer);\n#endif /* NGIFLIB_NO_FILE */\n\tGifDestroy(gif);\n\tif(animation) animation->image_count = image_count;\n\treturn animation;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -87,19 +87,23 @@\n \t\t\tcurrent_palette = gif->palette;\n \t\t\tcurrent_palette_size = gif->ncolors;\n \t\t}\n-\t\tfor(i = 0; i < current_palette_size; i++) {\n-\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;\n-\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;\n-\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;\n-\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);\n+\t\tif (current_palette != NULL) {\n+\t\t\tfor(i = 0; i < current_palette_size; i++) {\n+\t\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;\n+\t\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;\n+\t\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;\n+\t\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);\n+\t\t\t}\n+\t\t\tfor(; i < gif->ncolors; i++) {\n+\t\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;\n+\t\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;\n+\t\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;\n+\t\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);\n+\t\t\t}\n+\t\t\t//printf(\"\\n\");\n+\t\t} else {\n+\t\t\tfprintf(stderr, \"no palette in GIF\\n\");\n \t\t}\n-\t\tfor(; i < gif->ncolors; i++) {\n-\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;\n-\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;\n-\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;\n-\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);\n-\t\t}\n-\t\tprintf(\"\\n\");\n \t\tpsrc = p; pdst = surface->pixels;\n \t\tfor(i=0; i<gif->height; i++) {\n \t\t\tngiflib_memcpy(pdst, psrc, gif->width);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tfor(i = 0; i < current_palette_size; i++) {",
                "\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;",
                "\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;",
                "\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;",
                "\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);",
                "\t\tfor(; i < gif->ncolors; i++) {",
                "\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;",
                "\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;",
                "\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;",
                "\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);",
                "\t\t}",
                "\t\tprintf(\"\\n\");"
            ],
            "added_lines": [
                "\t\tif (current_palette != NULL) {",
                "\t\t\tfor(i = 0; i < current_palette_size; i++) {",
                "\t\t\t\tsurface->format->palette->colors[i].r = current_palette[i].r;",
                "\t\t\t\tsurface->format->palette->colors[i].g = current_palette[i].g;",
                "\t\t\t\tsurface->format->palette->colors[i].b = current_palette[i].b;",
                "\t\t\t\t//printf(\"#%02x%02x%02x \",  current_palette[i].r,  current_palette[i].g,  current_palette[i].b);",
                "\t\t\t}",
                "\t\t\tfor(; i < gif->ncolors; i++) {",
                "\t\t\t\tsurface->format->palette->colors[i].r = gif->palette[i].r;",
                "\t\t\t\tsurface->format->palette->colors[i].g = gif->palette[i].g;",
                "\t\t\t\tsurface->format->palette->colors[i].b = gif->palette[i].b;",
                "\t\t\t\t//printf(\"#%02x%02x%02x \",  gif->palette[i].r,  gif->palette[i].g,  gif->palette[i].b);",
                "\t\t\t}",
                "\t\t\t//printf(\"\\n\");",
                "\t\t} else {",
                "\t\t\tfprintf(stderr, \"no palette in GIF\\n\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-4037",
        "func_name": "qemu/ehci_advance_state",
        "description": "The ehci_advance_state function in hw/usb/hcd-ehci.c in QEMU allows local guest OS administrators to cause a denial of service (infinite loop and CPU consumption) via a circular split isochronous transfer descriptor (siTD) list, a related issue to CVE-2015-8558.",
        "git_url": "https://github.com/qemu/qemu/commit/1ae3f2f178087711f9591350abad133525ba93f2",
        "commit_title": "ehci: apply limit to iTD/sidt descriptors",
        "commit_text": " Commit \"156a2e4 ehci: make idt processing more robust\" tries to avoid a DoS by the guest (create a circular iTD queue and let qemu ehci emulation run in circles forever).  Unfortunately this has two problems: First it misses the case of siTDs, and second it reportedly breaks FreeBSD.  So lets go for a different approach: just count the number of iTDs and siTDs we have seen per frame and apply a limit.  That should really catch all cases now. ",
        "func_before": "static void ehci_advance_state(EHCIState *ehci, int async)\n{\n    EHCIQueue *q = NULL;\n    int again;\n\n    do {\n        switch(ehci_get_state(ehci, async)) {\n        case EST_WAITLISTHEAD:\n            again = ehci_state_waitlisthead(ehci, async);\n            break;\n\n        case EST_FETCHENTRY:\n            again = ehci_state_fetchentry(ehci, async);\n            break;\n\n        case EST_FETCHQH:\n            q = ehci_state_fetchqh(ehci, async);\n            if (q != NULL) {\n                assert(q->async == async);\n                again = 1;\n            } else {\n                again = 0;\n            }\n            break;\n\n        case EST_FETCHITD:\n            again = ehci_state_fetchitd(ehci, async);\n            break;\n\n        case EST_FETCHSITD:\n            again = ehci_state_fetchsitd(ehci, async);\n            break;\n\n        case EST_ADVANCEQUEUE:\n            assert(q != NULL);\n            again = ehci_state_advqueue(q);\n            break;\n\n        case EST_FETCHQTD:\n            assert(q != NULL);\n            again = ehci_state_fetchqtd(q);\n            break;\n\n        case EST_HORIZONTALQH:\n            assert(q != NULL);\n            again = ehci_state_horizqh(q);\n            break;\n\n        case EST_EXECUTE:\n            assert(q != NULL);\n            again = ehci_state_execute(q);\n            if (async) {\n                ehci->async_stepdown = 0;\n            }\n            break;\n\n        case EST_EXECUTING:\n            assert(q != NULL);\n            if (async) {\n                ehci->async_stepdown = 0;\n            }\n            again = ehci_state_executing(q);\n            break;\n\n        case EST_WRITEBACK:\n            assert(q != NULL);\n            again = ehci_state_writeback(q);\n            if (!async) {\n                ehci->periodic_sched_active = PERIODIC_ACTIVE;\n            }\n            break;\n\n        default:\n            fprintf(stderr, \"Bad state!\\n\");\n            again = -1;\n            g_assert_not_reached();\n            break;\n        }\n\n        if (again < 0) {\n            fprintf(stderr, \"processing error - resetting ehci HC\\n\");\n            ehci_reset(ehci);\n            again = 0;\n        }\n    }\n    while (again);\n}",
        "func": "static void ehci_advance_state(EHCIState *ehci, int async)\n{\n    EHCIQueue *q = NULL;\n    int itd_count = 0;\n    int again;\n\n    do {\n        switch(ehci_get_state(ehci, async)) {\n        case EST_WAITLISTHEAD:\n            again = ehci_state_waitlisthead(ehci, async);\n            break;\n\n        case EST_FETCHENTRY:\n            again = ehci_state_fetchentry(ehci, async);\n            break;\n\n        case EST_FETCHQH:\n            q = ehci_state_fetchqh(ehci, async);\n            if (q != NULL) {\n                assert(q->async == async);\n                again = 1;\n            } else {\n                again = 0;\n            }\n            break;\n\n        case EST_FETCHITD:\n            again = ehci_state_fetchitd(ehci, async);\n            itd_count++;\n            break;\n\n        case EST_FETCHSITD:\n            again = ehci_state_fetchsitd(ehci, async);\n            itd_count++;\n            break;\n\n        case EST_ADVANCEQUEUE:\n            assert(q != NULL);\n            again = ehci_state_advqueue(q);\n            break;\n\n        case EST_FETCHQTD:\n            assert(q != NULL);\n            again = ehci_state_fetchqtd(q);\n            break;\n\n        case EST_HORIZONTALQH:\n            assert(q != NULL);\n            again = ehci_state_horizqh(q);\n            break;\n\n        case EST_EXECUTE:\n            assert(q != NULL);\n            again = ehci_state_execute(q);\n            if (async) {\n                ehci->async_stepdown = 0;\n            }\n            break;\n\n        case EST_EXECUTING:\n            assert(q != NULL);\n            if (async) {\n                ehci->async_stepdown = 0;\n            }\n            again = ehci_state_executing(q);\n            break;\n\n        case EST_WRITEBACK:\n            assert(q != NULL);\n            again = ehci_state_writeback(q);\n            if (!async) {\n                ehci->periodic_sched_active = PERIODIC_ACTIVE;\n            }\n            break;\n\n        default:\n            fprintf(stderr, \"Bad state!\\n\");\n            again = -1;\n            g_assert_not_reached();\n            break;\n        }\n\n        if (again < 0 || itd_count > 16) {\n            /* TODO: notify guest (raise HSE irq?) */\n            fprintf(stderr, \"processing error - resetting ehci HC\\n\");\n            ehci_reset(ehci);\n            again = 0;\n        }\n    }\n    while (again);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n static void ehci_advance_state(EHCIState *ehci, int async)\n {\n     EHCIQueue *q = NULL;\n+    int itd_count = 0;\n     int again;\n \n     do {\n@@ -25,10 +26,12 @@\n \n         case EST_FETCHITD:\n             again = ehci_state_fetchitd(ehci, async);\n+            itd_count++;\n             break;\n \n         case EST_FETCHSITD:\n             again = ehci_state_fetchsitd(ehci, async);\n+            itd_count++;\n             break;\n \n         case EST_ADVANCEQUEUE:\n@@ -77,7 +80,8 @@\n             break;\n         }\n \n-        if (again < 0) {\n+        if (again < 0 || itd_count > 16) {\n+            /* TODO: notify guest (raise HSE irq?) */\n             fprintf(stderr, \"processing error - resetting ehci HC\\n\");\n             ehci_reset(ehci);\n             again = 0;",
        "diff_line_info": {
            "deleted_lines": [
                "        if (again < 0) {"
            ],
            "added_lines": [
                "    int itd_count = 0;",
                "            itd_count++;",
                "            itd_count++;",
                "        if (again < 0 || itd_count > 16) {",
                "            /* TODO: notify guest (raise HSE irq?) */"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20030",
        "func_name": "libexif/exif_data_load_data_content",
        "description": "An error when processing the EXIF_IFD_INTEROPERABILITY and EXIF_IFD_EXIF tags within libexif version 0.6.21 can be exploited to exhaust available CPU resources.",
        "git_url": "https://github.com/libexif/libexif/commit/6aa11df549114ebda520dde4cdaea2f9357b2c89",
        "commit_title": "Improve deep recursion detection in exif_data_load_data_content.",
        "commit_text": " The existing detection was still vulnerable to pathological cases causing DoS by wasting CPU. The new algorithm takes the number of tags into account to make it harder to abuse by cases using shallow recursion but with a very large number of tags.  This improves on commit 5d28011c which wasn't sufficient to counter this kind of case.  The limitation in the previous fix was discovered by Laurent Delosieres, Secunia Research at Flexera (Secunia Advisory SA84652) and is assigned the identifier CVE-2018-20030.",
        "func_before": "static void\nexif_data_load_data_content (ExifData *data, ExifIfd ifd,\n\t\t\t     const unsigned char *d,\n\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_depth)\n{\n\tExifLong o, thumbnail_offset = 0, thumbnail_length = 0;\n\tExifShort n;\n\tExifEntry *entry;\n\tunsigned int i;\n\tExifTag tag;\n\n\tif (!data || !data->priv) \n\t\treturn;\n\n\t/* check for valid ExifIfd enum range */\n\tif ((((int)ifd) < 0) || ( ((int)ifd) >= EXIF_IFD_COUNT))\n\t  return;\n\n\tif (recursion_depth > 12) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_CORRUPT_DATA, \"ExifData\",\n\t\t\t  \"Deep recursion detected!\");\n\t\treturn;\n\t}\n\n\t/* Read the number of entries */\n\tif ((offset + 2 < offset) || (offset + 2 < 2) || (offset + 2 > ds)) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_CORRUPT_DATA, \"ExifData\",\n\t\t\t  \"Tag data past end of buffer (%u > %u)\", offset+2, ds);\n\t\treturn;\n\t}\n\tn = exif_get_short (d + offset, data->priv->order);\n\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t          \"Loading %hu entries...\", n);\n\toffset += 2;\n\n\t/* Check if we have enough data. */\n\tif (offset + 12 * n > ds) {\n\t\tn = (ds - offset) / 12;\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t  \"Short data; only loading %hu entries...\", n);\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\n\t\ttag = exif_get_short (d + offset + 12 * i, data->priv->order);\n\t\tswitch (tag) {\n\t\tcase EXIF_TAG_EXIF_IFD_POINTER:\n\t\tcase EXIF_TAG_GPS_INFO_IFD_POINTER:\n\t\tcase EXIF_TAG_INTEROPERABILITY_IFD_POINTER:\n\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LENGTH:\n\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT:\n\t\t\to = exif_get_long (d + offset + 12 * i + 8,\n\t\t\t\t\t   data->priv->order);\n\t\t\t/* FIXME: IFD_POINTER tags aren't marked as being in a\n\t\t\t * specific IFD, so exif_tag_get_name_in_ifd won't work\n\t\t\t */\n\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t  \"Sub-IFD entry 0x%x ('%s') at %u.\", tag,\n\t\t\t\t  exif_tag_get_name(tag), o);\n\t\t\tswitch (tag) {\n\t\t\tcase EXIF_TAG_EXIF_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_EXIF);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o, recursion_depth + 1);\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_GPS_INFO_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_GPS);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o, recursion_depth + 1);\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_INTEROPERABILITY_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_INTEROPERABILITY);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o, recursion_depth + 1);\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT:\n\t\t\t\tthumbnail_offset = o;\n\t\t\t\tif (thumbnail_offset && thumbnail_length)\n\t\t\t\t\texif_data_load_data_thumbnail (data, d,\n\t\t\t\t\t\t\t\t       ds, thumbnail_offset,\n\t\t\t\t\t\t\t\t       thumbnail_length);\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LENGTH:\n\t\t\t\tthumbnail_length = o;\n\t\t\t\tif (thumbnail_offset && thumbnail_length)\n\t\t\t\t\texif_data_load_data_thumbnail (data, d,\n\t\t\t\t\t\t\t\t       ds, thumbnail_offset,\n\t\t\t\t\t\t\t\t       thumbnail_length);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\n\t\t\t/*\n\t\t\t * If we don't know the tag, don't fail. It could be that new \n\t\t\t * versions of the standard have defined additional tags. Note that\n\t\t\t * 0 is a valid tag in the GPS IFD.\n\t\t\t */\n\t\t\tif (!exif_tag_get_name_in_ifd (tag, ifd)) {\n\n\t\t\t\t/*\n\t\t\t\t * Special case: Tag and format 0. That's against specification\n\t\t\t\t * (at least up to 2.2). But Photoshop writes it anyways.\n\t\t\t\t */\n\t\t\t\tif (!memcmp (d + offset + 12 * i, \"\\0\\0\\0\\0\", 4)) {\n\t\t\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t\t\t  \"Skipping empty entry at position %u in '%s'.\", i, \n\t\t\t\t\t\t  exif_ifd_get_name (ifd));\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t\t  \"Unknown tag 0x%04x (entry %u in '%s'). Please report this tag \"\n\t\t\t\t\t  \"to <libexif-devel@lists.sourceforge.net>.\", tag, i,\n\t\t\t\t\t  exif_ifd_get_name (ifd));\n\t\t\t\tif (data->priv->options & EXIF_DATA_OPTION_IGNORE_UNKNOWN_TAGS)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tentry = exif_entry_new_mem (data->priv->mem);\n\t\t\tif (!entry) {\n\t\t\t\t  exif_log (data->priv->log, EXIF_LOG_CODE_NO_MEMORY, \"ExifData\",\n                                          \"Could not allocate memory\");\n\t\t\t\t  return;\n\t\t\t}\n\t\t\tif (exif_data_load_data_entry (data, entry, d, ds,\n\t\t\t\t\t\t   offset + 12 * i))\n\t\t\t\texif_content_add_entry (data->ifd[ifd], entry);\n\t\t\texif_entry_unref (entry);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "func": "static void\nexif_data_load_data_content (ExifData *data, ExifIfd ifd,\n\t\t\t     const unsigned char *d,\n\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_cost)\n{\n\tExifLong o, thumbnail_offset = 0, thumbnail_length = 0;\n\tExifShort n;\n\tExifEntry *entry;\n\tunsigned int i;\n\tExifTag tag;\n\n\tif (!data || !data->priv) \n\t\treturn;\n\n\t/* check for valid ExifIfd enum range */\n\tif ((((int)ifd) < 0) || ( ((int)ifd) >= EXIF_IFD_COUNT))\n\t  return;\n\n\tif (recursion_cost > 170) {\n\t\t/*\n\t\t * recursion_cost is a logarithmic-scale indicator of how expensive this\n\t\t * recursive call might end up being. It is an indicator of the depth of\n\t\t * recursion as well as the potential for worst-case future recursive\n\t\t * calls. Since it's difficult to tell ahead of time how often recursion\n\t\t * will occur, this assumes the worst by assuming every tag could end up\n\t\t * causing recursion.\n\t\t * The value of 170 was chosen to limit typical EXIF structures to a\n\t\t * recursive depth of about 6, but pathological ones (those with very\n\t\t * many tags) to only 2.\n\t\t */\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_CORRUPT_DATA, \"ExifData\",\n\t\t\t  \"Deep/expensive recursion detected!\");\n\t\treturn;\n\t}\n\n\t/* Read the number of entries */\n\tif ((offset + 2 < offset) || (offset + 2 < 2) || (offset + 2 > ds)) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_CORRUPT_DATA, \"ExifData\",\n\t\t\t  \"Tag data past end of buffer (%u > %u)\", offset+2, ds);\n\t\treturn;\n\t}\n\tn = exif_get_short (d + offset, data->priv->order);\n\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t          \"Loading %hu entries...\", n);\n\toffset += 2;\n\n\t/* Check if we have enough data. */\n\tif (offset + 12 * n > ds) {\n\t\tn = (ds - offset) / 12;\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t  \"Short data; only loading %hu entries...\", n);\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\n\t\ttag = exif_get_short (d + offset + 12 * i, data->priv->order);\n\t\tswitch (tag) {\n\t\tcase EXIF_TAG_EXIF_IFD_POINTER:\n\t\tcase EXIF_TAG_GPS_INFO_IFD_POINTER:\n\t\tcase EXIF_TAG_INTEROPERABILITY_IFD_POINTER:\n\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LENGTH:\n\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT:\n\t\t\to = exif_get_long (d + offset + 12 * i + 8,\n\t\t\t\t\t   data->priv->order);\n\t\t\t/* FIXME: IFD_POINTER tags aren't marked as being in a\n\t\t\t * specific IFD, so exif_tag_get_name_in_ifd won't work\n\t\t\t */\n\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t  \"Sub-IFD entry 0x%x ('%s') at %u.\", tag,\n\t\t\t\t  exif_tag_get_name(tag), o);\n\t\t\tswitch (tag) {\n\t\t\tcase EXIF_TAG_EXIF_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_EXIF);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o,\n\t\t\t\t\trecursion_cost + level_cost(n));\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_GPS_INFO_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_GPS);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o,\n\t\t\t\t\trecursion_cost + level_cost(n));\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_INTEROPERABILITY_IFD_POINTER:\n\t\t\t\tCHECK_REC (EXIF_IFD_INTEROPERABILITY);\n\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o,\n\t\t\t\t\trecursion_cost + level_cost(n));\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT:\n\t\t\t\tthumbnail_offset = o;\n\t\t\t\tif (thumbnail_offset && thumbnail_length)\n\t\t\t\t\texif_data_load_data_thumbnail (data, d,\n\t\t\t\t\t\t\t\t       ds, thumbnail_offset,\n\t\t\t\t\t\t\t\t       thumbnail_length);\n\t\t\t\tbreak;\n\t\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LENGTH:\n\t\t\t\tthumbnail_length = o;\n\t\t\t\tif (thumbnail_offset && thumbnail_length)\n\t\t\t\t\texif_data_load_data_thumbnail (data, d,\n\t\t\t\t\t\t\t\t       ds, thumbnail_offset,\n\t\t\t\t\t\t\t\t       thumbnail_length);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\n\t\t\t/*\n\t\t\t * If we don't know the tag, don't fail. It could be that new \n\t\t\t * versions of the standard have defined additional tags. Note that\n\t\t\t * 0 is a valid tag in the GPS IFD.\n\t\t\t */\n\t\t\tif (!exif_tag_get_name_in_ifd (tag, ifd)) {\n\n\t\t\t\t/*\n\t\t\t\t * Special case: Tag and format 0. That's against specification\n\t\t\t\t * (at least up to 2.2). But Photoshop writes it anyways.\n\t\t\t\t */\n\t\t\t\tif (!memcmp (d + offset + 12 * i, \"\\0\\0\\0\\0\", 4)) {\n\t\t\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t\t\t  \"Skipping empty entry at position %u in '%s'.\", i, \n\t\t\t\t\t\t  exif_ifd_get_name (ifd));\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\",\n\t\t\t\t\t  \"Unknown tag 0x%04x (entry %u in '%s'). Please report this tag \"\n\t\t\t\t\t  \"to <libexif-devel@lists.sourceforge.net>.\", tag, i,\n\t\t\t\t\t  exif_ifd_get_name (ifd));\n\t\t\t\tif (data->priv->options & EXIF_DATA_OPTION_IGNORE_UNKNOWN_TAGS)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tentry = exif_entry_new_mem (data->priv->mem);\n\t\t\tif (!entry) {\n\t\t\t\t  exif_log (data->priv->log, EXIF_LOG_CODE_NO_MEMORY, \"ExifData\",\n                                          \"Could not allocate memory\");\n\t\t\t\t  return;\n\t\t\t}\n\t\t\tif (exif_data_load_data_entry (data, entry, d, ds,\n\t\t\t\t\t\t   offset + 12 * i))\n\t\t\t\texif_content_add_entry (data->ifd[ifd], entry);\n\t\t\texif_entry_unref (entry);\n\t\t\tbreak;\n\t\t}\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static void\n exif_data_load_data_content (ExifData *data, ExifIfd ifd,\n \t\t\t     const unsigned char *d,\n-\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_depth)\n+\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_cost)\n {\n \tExifLong o, thumbnail_offset = 0, thumbnail_length = 0;\n \tExifShort n;\n@@ -16,9 +16,20 @@\n \tif ((((int)ifd) < 0) || ( ((int)ifd) >= EXIF_IFD_COUNT))\n \t  return;\n \n-\tif (recursion_depth > 12) {\n+\tif (recursion_cost > 170) {\n+\t\t/*\n+\t\t * recursion_cost is a logarithmic-scale indicator of how expensive this\n+\t\t * recursive call might end up being. It is an indicator of the depth of\n+\t\t * recursion as well as the potential for worst-case future recursive\n+\t\t * calls. Since it's difficult to tell ahead of time how often recursion\n+\t\t * will occur, this assumes the worst by assuming every tag could end up\n+\t\t * causing recursion.\n+\t\t * The value of 170 was chosen to limit typical EXIF structures to a\n+\t\t * recursive depth of about 6, but pathological ones (those with very\n+\t\t * many tags) to only 2.\n+\t\t */\n \t\texif_log (data->priv->log, EXIF_LOG_CODE_CORRUPT_DATA, \"ExifData\",\n-\t\t\t  \"Deep recursion detected!\");\n+\t\t\t  \"Deep/expensive recursion detected!\");\n \t\treturn;\n \t}\n \n@@ -60,15 +71,18 @@\n \t\t\tswitch (tag) {\n \t\t\tcase EXIF_TAG_EXIF_IFD_POINTER:\n \t\t\t\tCHECK_REC (EXIF_IFD_EXIF);\n-\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o, recursion_depth + 1);\n+\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o,\n+\t\t\t\t\trecursion_cost + level_cost(n));\n \t\t\t\tbreak;\n \t\t\tcase EXIF_TAG_GPS_INFO_IFD_POINTER:\n \t\t\t\tCHECK_REC (EXIF_IFD_GPS);\n-\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o, recursion_depth + 1);\n+\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o,\n+\t\t\t\t\trecursion_cost + level_cost(n));\n \t\t\t\tbreak;\n \t\t\tcase EXIF_TAG_INTEROPERABILITY_IFD_POINTER:\n \t\t\t\tCHECK_REC (EXIF_IFD_INTEROPERABILITY);\n-\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o, recursion_depth + 1);\n+\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o,\n+\t\t\t\t\trecursion_cost + level_cost(n));\n \t\t\t\tbreak;\n \t\t\tcase EXIF_TAG_JPEG_INTERCHANGE_FORMAT:\n \t\t\t\tthumbnail_offset = o;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_depth)",
                "\tif (recursion_depth > 12) {",
                "\t\t\t  \"Deep recursion detected!\");",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o, recursion_depth + 1);",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o, recursion_depth + 1);",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o, recursion_depth + 1);"
            ],
            "added_lines": [
                "\t\t\t     unsigned int ds, unsigned int offset, unsigned int recursion_cost)",
                "\tif (recursion_cost > 170) {",
                "\t\t/*",
                "\t\t * recursion_cost is a logarithmic-scale indicator of how expensive this",
                "\t\t * recursive call might end up being. It is an indicator of the depth of",
                "\t\t * recursion as well as the potential for worst-case future recursive",
                "\t\t * calls. Since it's difficult to tell ahead of time how often recursion",
                "\t\t * will occur, this assumes the worst by assuming every tag could end up",
                "\t\t * causing recursion.",
                "\t\t * The value of 170 was chosen to limit typical EXIF structures to a",
                "\t\t * recursive depth of about 6, but pathological ones (those with very",
                "\t\t * many tags) to only 2.",
                "\t\t */",
                "\t\t\t  \"Deep/expensive recursion detected!\");",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_EXIF, d, ds, o,",
                "\t\t\t\t\trecursion_cost + level_cost(n));",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_GPS, d, ds, o,",
                "\t\t\t\t\trecursion_cost + level_cost(n));",
                "\t\t\t\texif_data_load_data_content (data, EXIF_IFD_INTEROPERABILITY, d, ds, o,",
                "\t\t\t\t\trecursion_cost + level_cost(n));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11470",
        "func_name": "ImageMagick/ReadCINImage",
        "description": "The cineon parsing component in ImageMagick 7.0.8-26 Q16 allows attackers to cause a denial-of-service (uncontrolled resource consumption) by crafting a Cineon image with an incorrect claimed image size. This occurs because ReadCINImage in coders/cin.c lacks a check for insufficient image data in a file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/e3cdce6fe12193f235b8c0ae5efe6880a25eb957",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/1472",
        "commit_text": "",
        "func_before": "static Image *ReadCINImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n#define MonoColorType  1\n#define RGBColorType  3\n\n  char\n    property[MagickPathExtent];\n\n  CINInfo\n    cin;\n\n  const unsigned char\n    *pixels;\n\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  register Quantum\n    *q;\n\n  size_t\n    length;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    magick[4];\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    File information.\n  */\n  offset=0;\n  count=ReadBlob(image,4,magick);\n  offset+=count;\n  if ((count != 4) ||\n      ((LocaleNCompare((char *) magick,\"\\200\\052\\137\\327\",4) != 0)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  memset(&cin,0,sizeof(cin));\n  image->endian=(magick[0] == 0x80) && (magick[1] == 0x2a) &&\n    (magick[2] == 0x5f) && (magick[3] == 0xd7) ? MSBEndian : LSBEndian;\n  cin.file.image_offset=ReadBlobLong(image);\n  offset+=4;\n  cin.file.generic_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.industry_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.user_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.file_size=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.file.version),(unsigned char *)\n    cin.file.version);\n  (void) CopyMagickString(property,cin.file.version,sizeof(cin.file.version));\n  (void) SetImageProperty(image,\"dpx:file.version\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.filename),(unsigned char *)\n    cin.file.filename);\n  (void) CopyMagickString(property,cin.file.filename,sizeof(cin.file.filename));\n  (void) SetImageProperty(image,\"dpx:file.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_date),(unsigned char *)\n    cin.file.create_date);\n  (void) CopyMagickString(property,cin.file.create_date,\n    sizeof(cin.file.create_date));\n  (void) SetImageProperty(image,\"dpx:file.create_date\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_time),(unsigned char *)\n    cin.file.create_time);\n  (void) CopyMagickString(property,cin.file.create_time,\n    sizeof(cin.file.create_time));\n  (void) SetImageProperty(image,\"dpx:file.create_time\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.reserve),(unsigned char *)\n    cin.file.reserve);\n  /*\n    Image information.\n  */\n  cin.image.orientation=(unsigned char) ReadBlobByte(image);\n  offset++;\n  if (cin.image.orientation != (unsigned char) (~0))\n    (void) FormatImageProperty(image,\"dpx:image.orientation\",\"%d\",\n      cin.image.orientation);\n  switch (cin.image.orientation)\n  {\n    default:\n    case 0: image->orientation=TopLeftOrientation; break;\n    case 1: image->orientation=TopRightOrientation; break;\n    case 2: image->orientation=BottomLeftOrientation; break;\n    case 3: image->orientation=BottomRightOrientation; break;\n    case 4: image->orientation=LeftTopOrientation; break;\n    case 5: image->orientation=RightTopOrientation; break;\n    case 6: image->orientation=LeftBottomOrientation; break;\n    case 7: image->orientation=RightBottomOrientation; break;\n  }\n  cin.image.number_channels=(unsigned char) ReadBlobByte(image);\n  offset++;\n  offset+=ReadBlob(image,sizeof(cin.image.reserve1),(unsigned char *)\n    cin.image.reserve1);\n  for (i=0; i < 8; i++)\n  {\n    cin.image.channel[i].designator[0]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].designator[1]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].bits_per_pixel=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].reserve=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].pixels_per_line=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].lines_per_image=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].min_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].min_quantity=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_quantity=ReadBlobFloat(image);\n    offset+=4;\n  }\n  cin.image.white_point[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[0]) != MagickFalse)\n    image->chromaticity.white_point.x=cin.image.white_point[0];\n  cin.image.white_point[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[1]) != MagickFalse)\n    image->chromaticity.white_point.y=cin.image.white_point[1];\n  cin.image.red_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.red_primary_chromaticity[0];\n  cin.image.red_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.red_primary.y=cin.image.red_primary_chromaticity[1];\n  cin.image.green_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.green_primary_chromaticity[0];\n  cin.image.green_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.green_primary.y=cin.image.green_primary_chromaticity[1];\n  cin.image.blue_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.blue_primary.x=cin.image.blue_primary_chromaticity[0];\n  cin.image.blue_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.blue_primary.y=cin.image.blue_primary_chromaticity[1];\n  offset+=ReadBlob(image,sizeof(cin.image.label),(unsigned char *)\n    cin.image.label);\n  (void) CopyMagickString(property,cin.image.label,sizeof(cin.image.label));\n  (void) SetImageProperty(image,\"dpx:image.label\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.image.reserve),(unsigned char *)\n    cin.image.reserve);\n  /*\n    Image data format information.\n  */\n  cin.data_format.interleave=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.packing=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sign=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sense=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.line_pad=ReadBlobLong(image);\n  offset+=4;\n  cin.data_format.channel_pad=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.data_format.reserve),(unsigned char *)\n    cin.data_format.reserve);\n  /*\n    Image origination information.\n  */\n  cin.origination.x_offset=ReadBlobSignedLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.x_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.x_offset\",\"%.20g\",\n      (double) cin.origination.x_offset);\n  cin.origination.y_offset=(ssize_t) ReadBlobLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.y_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.y_offset\",\"%.20g\",\n      (double) cin.origination.y_offset);\n  offset+=ReadBlob(image,sizeof(cin.origination.filename),(unsigned char *)\n    cin.origination.filename);\n  (void) CopyMagickString(property,cin.origination.filename,\n    sizeof(cin.origination.filename));\n  (void) SetImageProperty(image,\"dpx:origination.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_date),(unsigned char *)\n    cin.origination.create_date);\n  (void) CopyMagickString(property,cin.origination.create_date,\n    sizeof(cin.origination.create_date));\n  (void) SetImageProperty(image,\"dpx:origination.create_date\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_time),(unsigned char *)\n    cin.origination.create_time);\n  (void) CopyMagickString(property,cin.origination.create_time,\n    sizeof(cin.origination.create_time));\n  (void) SetImageProperty(image,\"dpx:origination.create_time\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.device),(unsigned char *)\n    cin.origination.device);\n  (void) CopyMagickString(property,cin.origination.device,\n    sizeof(cin.origination.device));\n  (void) SetImageProperty(image,\"dpx:origination.device\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.model),(unsigned char *)\n    cin.origination.model);\n  (void) CopyMagickString(property,cin.origination.model,\n    sizeof(cin.origination.model));\n  (void) SetImageProperty(image,\"dpx:origination.model\",property,exception);\n  (void) memset(cin.origination.serial,0, \n    sizeof(cin.origination.serial));\n  offset+=ReadBlob(image,sizeof(cin.origination.serial),(unsigned char *)\n    cin.origination.serial);\n  (void) CopyMagickString(property,cin.origination.serial,\n    sizeof(cin.origination.serial));\n  (void) SetImageProperty(image,\"dpx:origination.serial\",property,exception);\n  cin.origination.x_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.y_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.gamma=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.origination.gamma) != MagickFalse)\n    image->gamma=cin.origination.gamma;\n  offset+=ReadBlob(image,sizeof(cin.origination.reserve),(unsigned char *)\n    cin.origination.reserve);\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      int\n        c;\n\n      /*\n        Image film information.\n      */\n      cin.film.id=ReadBlobByte(image);\n      offset++;\n      c=cin.film.id;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.id\",\"%d\",cin.film.id);\n      cin.film.type=ReadBlobByte(image);\n      offset++;\n      c=cin.film.type;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.type\",\"%d\",cin.film.type);\n      cin.film.offset=ReadBlobByte(image);\n      offset++;\n      c=cin.film.offset;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.offset\",\"%d\",\n          cin.film.offset);\n      cin.film.reserve1=ReadBlobByte(image);\n      offset++;\n      cin.film.prefix=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.prefix != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.prefix\",\"%.20g\",(double)\n          cin.film.prefix);\n      cin.film.count=ReadBlobLong(image);\n      offset+=4;\n      offset+=ReadBlob(image,sizeof(cin.film.format),(unsigned char *)\n        cin.film.format);\n      (void) CopyMagickString(property,cin.film.format,sizeof(cin.film.format));\n      (void) SetImageProperty(image,\"dpx:film.format\",property,exception);\n      cin.film.frame_position=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.frame_position != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.frame_position\",\"%.20g\",\n          (double) cin.film.frame_position);\n      cin.film.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(cin.film.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.frame_rate\",\"%g\",\n          cin.film.frame_rate);\n      offset+=ReadBlob(image,sizeof(cin.film.frame_id),(unsigned char *)\n        cin.film.frame_id);\n      (void) CopyMagickString(property,cin.film.frame_id,\n        sizeof(cin.film.frame_id));\n      (void) SetImageProperty(image,\"dpx:film.frame_id\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.slate_info),(unsigned char *)\n        cin.film.slate_info);\n      (void) CopyMagickString(property,cin.film.slate_info,\n        sizeof(cin.film.slate_info));\n      (void) SetImageProperty(image,\"dpx:film.slate_info\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.reserve),(unsigned char *)\n        cin.film.reserve);\n    }\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      StringInfo\n        *profile;\n\n      /*\n        User defined data.\n      */\n      if (cin.file.user_length > GetBlobSize(image))\n        ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n      profile=BlobToStringInfo((const unsigned char *) NULL,\n        cin.file.user_length);\n      if (profile == (StringInfo *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      offset+=ReadBlob(image,GetStringInfoLength(profile),\n        GetStringInfoDatum(profile));\n      (void) SetImageProfile(image,\"dpx:user.data\",profile,exception);\n      profile=DestroyStringInfo(profile);\n    }\n  image->depth=cin.image.channel[0].bits_per_pixel;\n  image->columns=cin.image.channel[0].pixels_per_line;\n  image->rows=cin.image.channel[0].lines_per_image;\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(image);\n    }\n  for ( ; offset < (MagickOffsetType) cin.file.image_offset; offset++)\n  {\n    int\n      c;\n\n    c=ReadBlobByte(image);\n    if (c == EOF)\n      break;\n  }\n  if (offset < (MagickOffsetType) cin.file.image_offset)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  (void) SetImageBackgroundColor(image,exception);\n  /*\n    Convert CIN raster image to pixel packets.\n  */\n  quantum_info=AcquireQuantumInfo(image_info,image);\n  if (quantum_info == (QuantumInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n  quantum_info->quantum=32;\n  quantum_info->pack=MagickFalse;\n  quantum_type=RGBQuantum;\n  length=GetQuantumExtent(image,quantum_info,quantum_type);\n  length=GetBytesPerRow(image->columns,3,image->depth,MagickTrue);\n  if (cin.image.number_channels == 1)\n    {\n      quantum_type=GrayQuantum;\n      length=GetBytesPerRow(image->columns,1,image->depth,MagickTrue);\n    }\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n    if (q == (Quantum *) NULL)\n      break;\n    pixels=(const unsigned char *) ReadBlobStream(image,length,\n      GetQuantumPixels(quantum_info),&count);\n    if ((size_t) count != length)\n      break;\n    (void) ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,\n      quantum_type,pixels,exception);\n    if (SyncAuthenticPixels(image,exception) == MagickFalse)\n      break;\n    if (image->previous == (Image *) NULL)\n      {\n        status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n          image->rows);\n        if (status == MagickFalse)\n          break;\n      }\n  }\n  SetQuantumImageType(image,quantum_type);\n  quantum_info=DestroyQuantumInfo(quantum_info);\n  if (EOFBlob(image) != MagickFalse)\n    ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n      image->filename);\n  SetImageColorspace(image,LogColorspace,exception);\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}",
        "func": "static Image *ReadCINImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n#define MonoColorType  1\n#define RGBColorType  3\n\n  char\n    property[MagickPathExtent];\n\n  CINInfo\n    cin;\n\n  const unsigned char\n    *pixels;\n\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  register Quantum\n    *q;\n\n  size_t\n    length;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    magick[4];\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    File information.\n  */\n  offset=0;\n  count=ReadBlob(image,4,magick);\n  offset+=count;\n  if ((count != 4) ||\n      ((LocaleNCompare((char *) magick,\"\\200\\052\\137\\327\",4) != 0)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  memset(&cin,0,sizeof(cin));\n  image->endian=(magick[0] == 0x80) && (magick[1] == 0x2a) &&\n    (magick[2] == 0x5f) && (magick[3] == 0xd7) ? MSBEndian : LSBEndian;\n  cin.file.image_offset=ReadBlobLong(image);\n  offset+=4;\n  cin.file.generic_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.industry_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.user_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.file_size=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.file.version),(unsigned char *)\n    cin.file.version);\n  (void) CopyMagickString(property,cin.file.version,sizeof(cin.file.version));\n  (void) SetImageProperty(image,\"dpx:file.version\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.filename),(unsigned char *)\n    cin.file.filename);\n  (void) CopyMagickString(property,cin.file.filename,sizeof(cin.file.filename));\n  (void) SetImageProperty(image,\"dpx:file.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_date),(unsigned char *)\n    cin.file.create_date);\n  (void) CopyMagickString(property,cin.file.create_date,\n    sizeof(cin.file.create_date));\n  (void) SetImageProperty(image,\"dpx:file.create_date\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_time),(unsigned char *)\n    cin.file.create_time);\n  (void) CopyMagickString(property,cin.file.create_time,\n    sizeof(cin.file.create_time));\n  (void) SetImageProperty(image,\"dpx:file.create_time\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.reserve),(unsigned char *)\n    cin.file.reserve);\n  /*\n    Image information.\n  */\n  cin.image.orientation=(unsigned char) ReadBlobByte(image);\n  offset++;\n  if (cin.image.orientation != (unsigned char) (~0))\n    (void) FormatImageProperty(image,\"dpx:image.orientation\",\"%d\",\n      cin.image.orientation);\n  switch (cin.image.orientation)\n  {\n    default:\n    case 0: image->orientation=TopLeftOrientation; break;\n    case 1: image->orientation=TopRightOrientation; break;\n    case 2: image->orientation=BottomLeftOrientation; break;\n    case 3: image->orientation=BottomRightOrientation; break;\n    case 4: image->orientation=LeftTopOrientation; break;\n    case 5: image->orientation=RightTopOrientation; break;\n    case 6: image->orientation=LeftBottomOrientation; break;\n    case 7: image->orientation=RightBottomOrientation; break;\n  }\n  cin.image.number_channels=(unsigned char) ReadBlobByte(image);\n  offset++;\n  offset+=ReadBlob(image,sizeof(cin.image.reserve1),(unsigned char *)\n    cin.image.reserve1);\n  for (i=0; i < 8; i++)\n  {\n    cin.image.channel[i].designator[0]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].designator[1]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].bits_per_pixel=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].reserve=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].pixels_per_line=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].lines_per_image=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].min_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].min_quantity=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_quantity=ReadBlobFloat(image);\n    offset+=4;\n  }\n  cin.image.white_point[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[0]) != MagickFalse)\n    image->chromaticity.white_point.x=cin.image.white_point[0];\n  cin.image.white_point[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[1]) != MagickFalse)\n    image->chromaticity.white_point.y=cin.image.white_point[1];\n  cin.image.red_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.red_primary_chromaticity[0];\n  cin.image.red_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.red_primary.y=cin.image.red_primary_chromaticity[1];\n  cin.image.green_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.green_primary_chromaticity[0];\n  cin.image.green_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.green_primary.y=cin.image.green_primary_chromaticity[1];\n  cin.image.blue_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.blue_primary.x=cin.image.blue_primary_chromaticity[0];\n  cin.image.blue_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.blue_primary.y=cin.image.blue_primary_chromaticity[1];\n  offset+=ReadBlob(image,sizeof(cin.image.label),(unsigned char *)\n    cin.image.label);\n  (void) CopyMagickString(property,cin.image.label,sizeof(cin.image.label));\n  (void) SetImageProperty(image,\"dpx:image.label\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.image.reserve),(unsigned char *)\n    cin.image.reserve);\n  /*\n    Image data format information.\n  */\n  cin.data_format.interleave=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.packing=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sign=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sense=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.line_pad=ReadBlobLong(image);\n  offset+=4;\n  cin.data_format.channel_pad=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.data_format.reserve),(unsigned char *)\n    cin.data_format.reserve);\n  /*\n    Image origination information.\n  */\n  cin.origination.x_offset=ReadBlobSignedLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.x_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.x_offset\",\"%.20g\",\n      (double) cin.origination.x_offset);\n  cin.origination.y_offset=(ssize_t) ReadBlobLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.y_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.y_offset\",\"%.20g\",\n      (double) cin.origination.y_offset);\n  offset+=ReadBlob(image,sizeof(cin.origination.filename),(unsigned char *)\n    cin.origination.filename);\n  (void) CopyMagickString(property,cin.origination.filename,\n    sizeof(cin.origination.filename));\n  (void) SetImageProperty(image,\"dpx:origination.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_date),(unsigned char *)\n    cin.origination.create_date);\n  (void) CopyMagickString(property,cin.origination.create_date,\n    sizeof(cin.origination.create_date));\n  (void) SetImageProperty(image,\"dpx:origination.create_date\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_time),(unsigned char *)\n    cin.origination.create_time);\n  (void) CopyMagickString(property,cin.origination.create_time,\n    sizeof(cin.origination.create_time));\n  (void) SetImageProperty(image,\"dpx:origination.create_time\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.device),(unsigned char *)\n    cin.origination.device);\n  (void) CopyMagickString(property,cin.origination.device,\n    sizeof(cin.origination.device));\n  (void) SetImageProperty(image,\"dpx:origination.device\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.model),(unsigned char *)\n    cin.origination.model);\n  (void) CopyMagickString(property,cin.origination.model,\n    sizeof(cin.origination.model));\n  (void) SetImageProperty(image,\"dpx:origination.model\",property,exception);\n  (void) memset(cin.origination.serial,0, \n    sizeof(cin.origination.serial));\n  offset+=ReadBlob(image,sizeof(cin.origination.serial),(unsigned char *)\n    cin.origination.serial);\n  (void) CopyMagickString(property,cin.origination.serial,\n    sizeof(cin.origination.serial));\n  (void) SetImageProperty(image,\"dpx:origination.serial\",property,exception);\n  cin.origination.x_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.y_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.gamma=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.origination.gamma) != MagickFalse)\n    image->gamma=cin.origination.gamma;\n  offset+=ReadBlob(image,sizeof(cin.origination.reserve),(unsigned char *)\n    cin.origination.reserve);\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      int\n        c;\n\n      /*\n        Image film information.\n      */\n      cin.film.id=ReadBlobByte(image);\n      offset++;\n      c=cin.film.id;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.id\",\"%d\",cin.film.id);\n      cin.film.type=ReadBlobByte(image);\n      offset++;\n      c=cin.film.type;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.type\",\"%d\",cin.film.type);\n      cin.film.offset=ReadBlobByte(image);\n      offset++;\n      c=cin.film.offset;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.offset\",\"%d\",\n          cin.film.offset);\n      cin.film.reserve1=ReadBlobByte(image);\n      offset++;\n      cin.film.prefix=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.prefix != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.prefix\",\"%.20g\",(double)\n          cin.film.prefix);\n      cin.film.count=ReadBlobLong(image);\n      offset+=4;\n      offset+=ReadBlob(image,sizeof(cin.film.format),(unsigned char *)\n        cin.film.format);\n      (void) CopyMagickString(property,cin.film.format,sizeof(cin.film.format));\n      (void) SetImageProperty(image,\"dpx:film.format\",property,exception);\n      cin.film.frame_position=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.frame_position != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.frame_position\",\"%.20g\",\n          (double) cin.film.frame_position);\n      cin.film.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(cin.film.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.frame_rate\",\"%g\",\n          cin.film.frame_rate);\n      offset+=ReadBlob(image,sizeof(cin.film.frame_id),(unsigned char *)\n        cin.film.frame_id);\n      (void) CopyMagickString(property,cin.film.frame_id,\n        sizeof(cin.film.frame_id));\n      (void) SetImageProperty(image,\"dpx:film.frame_id\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.slate_info),(unsigned char *)\n        cin.film.slate_info);\n      (void) CopyMagickString(property,cin.film.slate_info,\n        sizeof(cin.film.slate_info));\n      (void) SetImageProperty(image,\"dpx:film.slate_info\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.reserve),(unsigned char *)\n        cin.film.reserve);\n    }\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      StringInfo\n        *profile;\n\n      /*\n        User defined data.\n      */\n      if (cin.file.user_length > GetBlobSize(image))\n        ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n      profile=BlobToStringInfo((const unsigned char *) NULL,\n        cin.file.user_length);\n      if (profile == (StringInfo *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      offset+=ReadBlob(image,GetStringInfoLength(profile),\n        GetStringInfoDatum(profile));\n      (void) SetImageProfile(image,\"dpx:user.data\",profile,exception);\n      profile=DestroyStringInfo(profile);\n    }\n  image->depth=cin.image.channel[0].bits_per_pixel;\n  image->columns=cin.image.channel[0].pixels_per_line;\n  image->rows=cin.image.channel[0].lines_per_image;\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(image);\n    }\n  if (((MagickSizeType) image->columns*image->rows) > GetBlobSize(image))\n    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n  for ( ; offset < (MagickOffsetType) cin.file.image_offset; offset++)\n  {\n    int\n      c;\n\n    c=ReadBlobByte(image);\n    if (c == EOF)\n      break;\n  }\n  if (offset < (MagickOffsetType) cin.file.image_offset)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  (void) SetImageBackgroundColor(image,exception);\n  /*\n    Convert CIN raster image to pixel packets.\n  */\n  quantum_info=AcquireQuantumInfo(image_info,image);\n  if (quantum_info == (QuantumInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n  quantum_info->quantum=32;\n  quantum_info->pack=MagickFalse;\n  quantum_type=RGBQuantum;\n  length=GetQuantumExtent(image,quantum_info,quantum_type);\n  length=GetBytesPerRow(image->columns,3,image->depth,MagickTrue);\n  if (cin.image.number_channels == 1)\n    {\n      quantum_type=GrayQuantum;\n      length=GetBytesPerRow(image->columns,1,image->depth,MagickTrue);\n    }\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n    if (q == (Quantum *) NULL)\n      break;\n    pixels=(const unsigned char *) ReadBlobStream(image,length,\n      GetQuantumPixels(quantum_info),&count);\n    if ((size_t) count != length)\n      break;\n    (void) ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,\n      quantum_type,pixels,exception);\n    if (SyncAuthenticPixels(image,exception) == MagickFalse)\n      break;\n    if (image->previous == (Image *) NULL)\n      {\n        status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n          image->rows);\n        if (status == MagickFalse)\n          break;\n      }\n  }\n  SetQuantumImageType(image,quantum_type);\n  quantum_info=DestroyQuantumInfo(quantum_info);\n  if (EOFBlob(image) != MagickFalse)\n    ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n      image->filename);\n  SetImageColorspace(image,LogColorspace,exception);\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -348,6 +348,8 @@\n       (void) CloseBlob(image);\n       return(image);\n     }\n+  if (((MagickSizeType) image->columns*image->rows) > GetBlobSize(image))\n+    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n   for ( ; offset < (MagickOffsetType) cin.file.image_offset; offset++)\n   {\n     int",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (((MagickSizeType) image->columns*image->rows) > GetBlobSize(image))",
                "    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2689",
        "func_name": "torvalds/linux/gfs2_fallocate",
        "description": "The gfs2_fallocate function in fs/gfs2/file.c in the Linux kernel before 3.0-rc1 does not ensure that the size of a chunk allocation is a multiple of the block size, which allows local users to cause a denial of service (BUG and system crash) by arranging for all resource groups to have too little free space.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=6905d9e4dda6112f007e9090bca80507da158e63",
        "commit_title": "The GFS2 fallocate code chooses a target size to for allocating chunks of",
        "commit_text": "space.  Whenever it can't find any resource groups with enough space free, it halves its target. Since this target is in bytes, eventually it will no longer be a multiple of blksize.  As long as there is more space available in the resource group than the target, this isn't a problem, since gfs2 will use the actual space available, which is always a multiple of blksize.  However, when gfs couldn't fallocate a bigger chunk than the target, it was using the non-blksize aligned number. This caused a BUG in later code that required blksize aligned offsets.  GFS2 now ensures that bytes is always a multiple of blksize  ",
        "func_before": "static long gfs2_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t   loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\tunsigned int data_blocks = 0, ind_blocks = 0, rblocks;\n\tloff_t bytes, max_bytes;\n\tstruct gfs2_alloc *al;\n\tint error;\n\tloff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;\n\tnext = (next + 1) << sdp->sd_sb.sb_bsize_shift;\n\n\t/* We only support the FALLOC_FL_KEEP_SIZE mode */\n\tif (mode & ~FALLOC_FL_KEEP_SIZE)\n\t\treturn -EOPNOTSUPP;\n\n\toffset = (offset >> sdp->sd_sb.sb_bsize_shift) <<\n\t\t sdp->sd_sb.sb_bsize_shift;\n\n\tlen = next - offset;\n\tbytes = sdp->sd_max_rg_data * sdp->sd_sb.sb_bsize / 2;\n\tif (!bytes)\n\t\tbytes = UINT_MAX;\n\n\tgfs2_holder_init(ip->i_gl, LM_ST_EXCLUSIVE, 0, &ip->i_gh);\n\terror = gfs2_glock_nq(&ip->i_gh);\n\tif (unlikely(error))\n\t\tgoto out_uninit;\n\n\tif (!gfs2_write_alloc_required(ip, offset, len))\n\t\tgoto out_unlock;\n\n\twhile (len > 0) {\n\t\tif (len < bytes)\n\t\t\tbytes = len;\n\t\tal = gfs2_alloc_get(ip);\n\t\tif (!al) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terror = gfs2_quota_lock_check(ip);\n\t\tif (error)\n\t\t\tgoto out_alloc_put;\n\nretry:\n\t\tgfs2_write_calc_reserv(ip, bytes, &data_blocks, &ind_blocks);\n\n\t\tal->al_requested = data_blocks + ind_blocks;\n\t\terror = gfs2_inplace_reserve(ip);\n\t\tif (error) {\n\t\t\tif (error == -ENOSPC && bytes > sdp->sd_sb.sb_bsize) {\n\t\t\t\tbytes >>= 1;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tgoto out_qunlock;\n\t\t}\n\t\tmax_bytes = bytes;\n\t\tcalc_max_reserv(ip, len, &max_bytes, &data_blocks, &ind_blocks);\n\t\tal->al_requested = data_blocks + ind_blocks;\n\n\t\trblocks = RES_DINODE + ind_blocks + RES_STATFS + RES_QUOTA +\n\t\t\t  RES_RG_HDR + gfs2_rg_blocks(al);\n\t\tif (gfs2_is_jdata(ip))\n\t\t\trblocks += data_blocks ? data_blocks : 1;\n\n\t\terror = gfs2_trans_begin(sdp, rblocks,\n\t\t\t\t\t PAGE_CACHE_SIZE/sdp->sd_sb.sb_bsize);\n\t\tif (error)\n\t\t\tgoto out_trans_fail;\n\n\t\terror = fallocate_chunk(inode, offset, max_bytes, mode);\n\t\tgfs2_trans_end(sdp);\n\n\t\tif (error)\n\t\t\tgoto out_trans_fail;\n\n\t\tlen -= max_bytes;\n\t\toffset += max_bytes;\n\t\tgfs2_inplace_release(ip);\n\t\tgfs2_quota_unlock(ip);\n\t\tgfs2_alloc_put(ip);\n\t}\n\tgoto out_unlock;\n\nout_trans_fail:\n\tgfs2_inplace_release(ip);\nout_qunlock:\n\tgfs2_quota_unlock(ip);\nout_alloc_put:\n\tgfs2_alloc_put(ip);\nout_unlock:\n\tgfs2_glock_dq(&ip->i_gh);\nout_uninit:\n\tgfs2_holder_uninit(&ip->i_gh);\n\treturn error;\n}",
        "func": "static long gfs2_fallocate(struct file *file, int mode, loff_t offset,\n\t\t\t   loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\tunsigned int data_blocks = 0, ind_blocks = 0, rblocks;\n\tloff_t bytes, max_bytes;\n\tstruct gfs2_alloc *al;\n\tint error;\n\tloff_t bsize_mask = ~((loff_t)sdp->sd_sb.sb_bsize - 1);\n\tloff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;\n\tnext = (next + 1) << sdp->sd_sb.sb_bsize_shift;\n\n\t/* We only support the FALLOC_FL_KEEP_SIZE mode */\n\tif (mode & ~FALLOC_FL_KEEP_SIZE)\n\t\treturn -EOPNOTSUPP;\n\n\toffset &= bsize_mask;\n\n\tlen = next - offset;\n\tbytes = sdp->sd_max_rg_data * sdp->sd_sb.sb_bsize / 2;\n\tif (!bytes)\n\t\tbytes = UINT_MAX;\n\tbytes &= bsize_mask;\n\tif (bytes == 0)\n\t\tbytes = sdp->sd_sb.sb_bsize;\n\n\tgfs2_holder_init(ip->i_gl, LM_ST_EXCLUSIVE, 0, &ip->i_gh);\n\terror = gfs2_glock_nq(&ip->i_gh);\n\tif (unlikely(error))\n\t\tgoto out_uninit;\n\n\tif (!gfs2_write_alloc_required(ip, offset, len))\n\t\tgoto out_unlock;\n\n\twhile (len > 0) {\n\t\tif (len < bytes)\n\t\t\tbytes = len;\n\t\tal = gfs2_alloc_get(ip);\n\t\tif (!al) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terror = gfs2_quota_lock_check(ip);\n\t\tif (error)\n\t\t\tgoto out_alloc_put;\n\nretry:\n\t\tgfs2_write_calc_reserv(ip, bytes, &data_blocks, &ind_blocks);\n\n\t\tal->al_requested = data_blocks + ind_blocks;\n\t\terror = gfs2_inplace_reserve(ip);\n\t\tif (error) {\n\t\t\tif (error == -ENOSPC && bytes > sdp->sd_sb.sb_bsize) {\n\t\t\t\tbytes >>= 1;\n\t\t\t\tbytes &= bsize_mask;\n\t\t\t\tif (bytes == 0)\n\t\t\t\t\tbytes = sdp->sd_sb.sb_bsize;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tgoto out_qunlock;\n\t\t}\n\t\tmax_bytes = bytes;\n\t\tcalc_max_reserv(ip, len, &max_bytes, &data_blocks, &ind_blocks);\n\t\tal->al_requested = data_blocks + ind_blocks;\n\n\t\trblocks = RES_DINODE + ind_blocks + RES_STATFS + RES_QUOTA +\n\t\t\t  RES_RG_HDR + gfs2_rg_blocks(al);\n\t\tif (gfs2_is_jdata(ip))\n\t\t\trblocks += data_blocks ? data_blocks : 1;\n\n\t\terror = gfs2_trans_begin(sdp, rblocks,\n\t\t\t\t\t PAGE_CACHE_SIZE/sdp->sd_sb.sb_bsize);\n\t\tif (error)\n\t\t\tgoto out_trans_fail;\n\n\t\terror = fallocate_chunk(inode, offset, max_bytes, mode);\n\t\tgfs2_trans_end(sdp);\n\n\t\tif (error)\n\t\t\tgoto out_trans_fail;\n\n\t\tlen -= max_bytes;\n\t\toffset += max_bytes;\n\t\tgfs2_inplace_release(ip);\n\t\tgfs2_quota_unlock(ip);\n\t\tgfs2_alloc_put(ip);\n\t}\n\tgoto out_unlock;\n\nout_trans_fail:\n\tgfs2_inplace_release(ip);\nout_qunlock:\n\tgfs2_quota_unlock(ip);\nout_alloc_put:\n\tgfs2_alloc_put(ip);\nout_unlock:\n\tgfs2_glock_dq(&ip->i_gh);\nout_uninit:\n\tgfs2_holder_uninit(&ip->i_gh);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n \tloff_t bytes, max_bytes;\n \tstruct gfs2_alloc *al;\n \tint error;\n+\tloff_t bsize_mask = ~((loff_t)sdp->sd_sb.sb_bsize - 1);\n \tloff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;\n \tnext = (next + 1) << sdp->sd_sb.sb_bsize_shift;\n \n@@ -15,13 +16,15 @@\n \tif (mode & ~FALLOC_FL_KEEP_SIZE)\n \t\treturn -EOPNOTSUPP;\n \n-\toffset = (offset >> sdp->sd_sb.sb_bsize_shift) <<\n-\t\t sdp->sd_sb.sb_bsize_shift;\n+\toffset &= bsize_mask;\n \n \tlen = next - offset;\n \tbytes = sdp->sd_max_rg_data * sdp->sd_sb.sb_bsize / 2;\n \tif (!bytes)\n \t\tbytes = UINT_MAX;\n+\tbytes &= bsize_mask;\n+\tif (bytes == 0)\n+\t\tbytes = sdp->sd_sb.sb_bsize;\n \n \tgfs2_holder_init(ip->i_gl, LM_ST_EXCLUSIVE, 0, &ip->i_gh);\n \terror = gfs2_glock_nq(&ip->i_gh);\n@@ -52,6 +55,9 @@\n \t\tif (error) {\n \t\t\tif (error == -ENOSPC && bytes > sdp->sd_sb.sb_bsize) {\n \t\t\t\tbytes >>= 1;\n+\t\t\t\tbytes &= bsize_mask;\n+\t\t\t\tif (bytes == 0)\n+\t\t\t\t\tbytes = sdp->sd_sb.sb_bsize;\n \t\t\t\tgoto retry;\n \t\t\t}\n \t\t\tgoto out_qunlock;",
        "diff_line_info": {
            "deleted_lines": [
                "\toffset = (offset >> sdp->sd_sb.sb_bsize_shift) <<",
                "\t\t sdp->sd_sb.sb_bsize_shift;"
            ],
            "added_lines": [
                "\tloff_t bsize_mask = ~((loff_t)sdp->sd_sb.sb_bsize - 1);",
                "\toffset &= bsize_mask;",
                "\tbytes &= bsize_mask;",
                "\tif (bytes == 0)",
                "\t\tbytes = sdp->sd_sb.sb_bsize;",
                "\t\t\t\tbytes &= bsize_mask;",
                "\t\t\t\tif (bytes == 0)",
                "\t\t\t\t\tbytes = sdp->sd_sb.sb_bsize;"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/cleanup_net",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=2b035b39970740722598f7a9d548835f9bdd730f",
        "commit_title": "It is fairly common to kill several network namespaces at once.  Either",
        "commit_text": "because they are nested one inside the other or because they are cooperating in multiple machine networking experiments.  As the network stack control logic does not parallelize easily batch up multiple network namespaces existing together.  To get the full benefit of batching the virtual network devices to be removed must be all removed in one batch.  For that purpose I have added a loop after the last network device operations have run that batches up all remaining network devices and deletes them.  An extra benefit is that the reorganization slightly shrinks the size of the per network namespace data structures replaceing a work_struct with a list_head.  In a trivial test with 4K namespaces this change reduced the cost of a destroying 4K namespaces from 7+ minutes (at 12% cpu) to 44 seconds (at 60% cpu).  The bulk of that 44s was spent in inet_twsk_purge.  ",
        "func_before": "static void cleanup_net(struct work_struct *work)\n{\n\tstruct pernet_operations *ops;\n\tstruct net *net;\n\n\tnet = container_of(work, struct net, work);\n\n\tmutex_lock(&net_mutex);\n\n\t/* Don't let anyone else find us. */\n\trtnl_lock();\n\tlist_del_rcu(&net->list);\n\trtnl_unlock();\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit)\n\t\t\tops->exit(net);\n\t}\n\n\tmutex_unlock(&net_mutex);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tnet_free(net);\n}",
        "func": "static void cleanup_net(struct work_struct *work)\n{\n\tstruct pernet_operations *ops;\n\tstruct net *net, *tmp;\n\tLIST_HEAD(net_kill_list);\n\n\t/* Atomically snapshot the list of namespaces to cleanup */\n\tspin_lock_irq(&cleanup_list_lock);\n\tlist_replace_init(&cleanup_list, &net_kill_list);\n\tspin_unlock_irq(&cleanup_list_lock);\n\n\tmutex_lock(&net_mutex);\n\n\t/* Don't let anyone else find us. */\n\trtnl_lock();\n\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\tlist_del_rcu(&net->list);\n\trtnl_unlock();\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit) {\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tops->exit(net);\n\t\t}\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\n\tmutex_unlock(&net_mutex);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tlist_for_each_entry_safe(net, tmp, &net_kill_list, cleanup_list) {\n\t\tlist_del_init(&net->cleanup_list);\n\t\tnet_free(net);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,15 +1,20 @@\n static void cleanup_net(struct work_struct *work)\n {\n \tstruct pernet_operations *ops;\n-\tstruct net *net;\n+\tstruct net *net, *tmp;\n+\tLIST_HEAD(net_kill_list);\n \n-\tnet = container_of(work, struct net, work);\n+\t/* Atomically snapshot the list of namespaces to cleanup */\n+\tspin_lock_irq(&cleanup_list_lock);\n+\tlist_replace_init(&cleanup_list, &net_kill_list);\n+\tspin_unlock_irq(&cleanup_list_lock);\n \n \tmutex_lock(&net_mutex);\n \n \t/* Don't let anyone else find us. */\n \trtnl_lock();\n-\tlist_del_rcu(&net->list);\n+\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n+\t\tlist_del_rcu(&net->list);\n \trtnl_unlock();\n \n \t/*\n@@ -21,8 +26,18 @@\n \n \t/* Run all of the network namespace exit methods */\n \tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n-\t\tif (ops->exit)\n-\t\t\tops->exit(net);\n+\t\tif (ops->exit) {\n+\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n+\t\t\t\tops->exit(net);\n+\t\t}\n+\t\tif (&ops->list == first_device) {\n+\t\t\tLIST_HEAD(dev_kill_list);\n+\t\t\trtnl_lock();\n+\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n+\t\t\t\tunregister_netdevices(net, &dev_kill_list);\n+\t\t\tunregister_netdevice_many(&dev_kill_list);\n+\t\t\trtnl_unlock();\n+\t\t}\n \t}\n \n \tmutex_unlock(&net_mutex);\n@@ -33,5 +48,8 @@\n \trcu_barrier();\n \n \t/* Finally it is safe to free my network namespace structure */\n-\tnet_free(net);\n+\tlist_for_each_entry_safe(net, tmp, &net_kill_list, cleanup_list) {\n+\t\tlist_del_init(&net->cleanup_list);\n+\t\tnet_free(net);\n+\t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net;",
                "\tnet = container_of(work, struct net, work);",
                "\tlist_del_rcu(&net->list);",
                "\t\tif (ops->exit)",
                "\t\t\tops->exit(net);",
                "\tnet_free(net);"
            ],
            "added_lines": [
                "\tstruct net *net, *tmp;",
                "\tLIST_HEAD(net_kill_list);",
                "\t/* Atomically snapshot the list of namespaces to cleanup */",
                "\tspin_lock_irq(&cleanup_list_lock);",
                "\tlist_replace_init(&cleanup_list, &net_kill_list);",
                "\tspin_unlock_irq(&cleanup_list_lock);",
                "\tlist_for_each_entry(net, &net_kill_list, cleanup_list)",
                "\t\tlist_del_rcu(&net->list);",
                "\t\tif (ops->exit) {",
                "\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)",
                "\t\t\t\tops->exit(net);",
                "\t\t}",
                "\t\tif (&ops->list == first_device) {",
                "\t\t\tLIST_HEAD(dev_kill_list);",
                "\t\t\trtnl_lock();",
                "\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)",
                "\t\t\t\tunregister_netdevices(net, &dev_kill_list);",
                "\t\t\tunregister_netdevice_many(&dev_kill_list);",
                "\t\t\trtnl_unlock();",
                "\t\t}",
                "\tlist_for_each_entry_safe(net, tmp, &net_kill_list, cleanup_list) {",
                "\t\tlist_del_init(&net->cleanup_list);",
                "\t\tnet_free(net);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/setup_net",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=2b035b39970740722598f7a9d548835f9bdd730f",
        "commit_title": "It is fairly common to kill several network namespaces at once.  Either",
        "commit_text": "because they are nested one inside the other or because they are cooperating in multiple machine networking experiments.  As the network stack control logic does not parallelize easily batch up multiple network namespaces existing together.  To get the full benefit of batching the virtual network devices to be removed must be all removed in one batch.  For that purpose I have added a loop after the last network device operations have run that batches up all remaining network devices and deletes them.  An extra benefit is that the reorganization slightly shrinks the size of the per network namespace data structures replaceing a work_struct with a list_head.  In a trivial test with 4K namespaces this change reduced the cost of a destroying 4K namespaces from 7+ minutes (at 12% cpu) to 44 seconds (at 60% cpu).  The bulk of that 44s was spent in inet_twsk_purge.  ",
        "func_before": "static __net_init int setup_net(struct net *net)\n{\n\t/* Must be called with net_mutex held */\n\tstruct pernet_operations *ops;\n\tint error = 0;\n\n\tatomic_set(&net->count, 1);\n\n#ifdef NETNS_REFCNT_DEBUG\n\tatomic_set(&net->use_count, 0);\n#endif\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\tif (ops->init) {\n\t\t\terror = ops->init(net);\n\t\t\tif (error < 0)\n\t\t\t\tgoto out_undo;\n\t\t}\n\t}\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit)\n\t\t\tops->exit(net);\n\t}\n\n\trcu_barrier();\n\tgoto out;\n}",
        "func": "static __net_init int setup_net(struct net *net)\n{\n\t/* Must be called with net_mutex held */\n\tstruct pernet_operations *ops;\n\tint error = 0;\n\n\tatomic_set(&net->count, 1);\n\n#ifdef NETNS_REFCNT_DEBUG\n\tatomic_set(&net->use_count, 0);\n#endif\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\tif (ops->init) {\n\t\t\terror = ops->init(net);\n\t\t\tif (error < 0)\n\t\t\t\tgoto out_undo;\n\t\t}\n\t}\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit)\n\t\t\tops->exit(net);\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\n\trcu_barrier();\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,13 @@\n \tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n \t\tif (ops->exit)\n \t\t\tops->exit(net);\n+\t\tif (&ops->list == first_device) {\n+\t\t\tLIST_HEAD(dev_kill_list);\n+\t\t\trtnl_lock();\n+\t\t\tunregister_netdevices(net, &dev_kill_list);\n+\t\t\tunregister_netdevice_many(&dev_kill_list);\n+\t\t\trtnl_unlock();\n+\t\t}\n \t}\n \n \trcu_barrier();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tif (&ops->list == first_device) {",
                "\t\t\tLIST_HEAD(dev_kill_list);",
                "\t\t\trtnl_lock();",
                "\t\t\tunregister_netdevices(net, &dev_kill_list);",
                "\t\t\tunregister_netdevice_many(&dev_kill_list);",
                "\t\t\trtnl_unlock();",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/__put_net",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=2b035b39970740722598f7a9d548835f9bdd730f",
        "commit_title": "It is fairly common to kill several network namespaces at once.  Either",
        "commit_text": "because they are nested one inside the other or because they are cooperating in multiple machine networking experiments.  As the network stack control logic does not parallelize easily batch up multiple network namespaces existing together.  To get the full benefit of batching the virtual network devices to be removed must be all removed in one batch.  For that purpose I have added a loop after the last network device operations have run that batches up all remaining network devices and deletes them.  An extra benefit is that the reorganization slightly shrinks the size of the per network namespace data structures replaceing a work_struct with a list_head.  In a trivial test with 4K namespaces this change reduced the cost of a destroying 4K namespaces from 7+ minutes (at 12% cpu) to 44 seconds (at 60% cpu).  The bulk of that 44s was spent in inet_twsk_purge.  ",
        "func_before": "void __put_net(struct net *net)\n{\n\t/* Cleanup the network namespace in process context */\n\tINIT_WORK(&net->work, cleanup_net);\n\tqueue_work(netns_wq, &net->work);\n}",
        "func": "void __put_net(struct net *net)\n{\n\t/* Cleanup the network namespace in process context */\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cleanup_list_lock, flags);\n\tlist_add(&net->cleanup_list, &cleanup_list);\n\tspin_unlock_irqrestore(&cleanup_list_lock, flags);\n\n\tqueue_work(netns_wq, &net_cleanup_work);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,11 @@\n void __put_net(struct net *net)\n {\n \t/* Cleanup the network namespace in process context */\n-\tINIT_WORK(&net->work, cleanup_net);\n-\tqueue_work(netns_wq, &net->work);\n+\tunsigned long flags;\n+\n+\tspin_lock_irqsave(&cleanup_list_lock, flags);\n+\tlist_add(&net->cleanup_list, &cleanup_list);\n+\tspin_unlock_irqrestore(&cleanup_list_lock, flags);\n+\n+\tqueue_work(netns_wq, &net_cleanup_work);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tINIT_WORK(&net->work, cleanup_net);",
                "\tqueue_work(netns_wq, &net->work);"
            ],
            "added_lines": [
                "\tunsigned long flags;",
                "",
                "\tspin_lock_irqsave(&cleanup_list_lock, flags);",
                "\tlist_add(&net->cleanup_list, &cleanup_list);",
                "\tspin_unlock_irqrestore(&cleanup_list_lock, flags);",
                "",
                "\tqueue_work(netns_wq, &net_cleanup_work);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/cleanup_net",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=f875bae065334907796da12523f9df85c89f5712",
        "commit_title": "To get the full benefit of batched network namespace cleanup netowrk",
        "commit_text": "device deletion needs to be performed by the generic code.  When using register_pernet_gen_device and freeing the data in exit_net it is impossible to delay allocation until after exit_net has called as the device uninit methods are no longer safe.  To correct this, and to simplify working with per network namespace data I have moved allocation and deletion of per network namespace data into the network namespace core.  The core now frees the data only after all of the network namespace exit routines have run.  Now it is only required to set the new fields .id and .size in the pernet_operations structure if you want network namespace data to be managed for you automatically.  This makes the current register_pernet_gen_device and register_pernet_gen_subsys routines unnecessary.  For the moment I have left them as compatibility wrappers in net_namespace.h They will be removed once all of the users have been updated.  ",
        "func_before": "static void cleanup_net(struct work_struct *work)\n{\n\tstruct pernet_operations *ops;\n\tstruct net *net, *tmp;\n\tLIST_HEAD(net_kill_list);\n\n\t/* Atomically snapshot the list of namespaces to cleanup */\n\tspin_lock_irq(&cleanup_list_lock);\n\tlist_replace_init(&cleanup_list, &net_kill_list);\n\tspin_unlock_irq(&cleanup_list_lock);\n\n\tmutex_lock(&net_mutex);\n\n\t/* Don't let anyone else find us. */\n\trtnl_lock();\n\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\tlist_del_rcu(&net->list);\n\trtnl_unlock();\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit) {\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tops->exit(net);\n\t\t}\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\n\tmutex_unlock(&net_mutex);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tlist_for_each_entry_safe(net, tmp, &net_kill_list, cleanup_list) {\n\t\tlist_del_init(&net->cleanup_list);\n\t\tnet_free(net);\n\t}\n}",
        "func": "static void cleanup_net(struct work_struct *work)\n{\n\tconst struct pernet_operations *ops;\n\tstruct net *net, *tmp;\n\tLIST_HEAD(net_kill_list);\n\n\t/* Atomically snapshot the list of namespaces to cleanup */\n\tspin_lock_irq(&cleanup_list_lock);\n\tlist_replace_init(&cleanup_list, &net_kill_list);\n\tspin_unlock_irq(&cleanup_list_lock);\n\n\tmutex_lock(&net_mutex);\n\n\t/* Don't let anyone else find us. */\n\trtnl_lock();\n\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\tlist_del_rcu(&net->list);\n\trtnl_unlock();\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit) {\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tops->exit(net);\n\t\t}\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\t/* Free the net generic variables */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n\t\tif (ops->size && ops->id) {\n\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n\t\t\t\tops_free(ops, net);\n\t\t}\n\t}\n\n\tmutex_unlock(&net_mutex);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tlist_for_each_entry_safe(net, tmp, &net_kill_list, cleanup_list) {\n\t\tlist_del_init(&net->cleanup_list);\n\t\tnet_free(net);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static void cleanup_net(struct work_struct *work)\n {\n-\tstruct pernet_operations *ops;\n+\tconst struct pernet_operations *ops;\n \tstruct net *net, *tmp;\n \tLIST_HEAD(net_kill_list);\n \n@@ -39,6 +39,13 @@\n \t\t\trtnl_unlock();\n \t\t}\n \t}\n+\t/* Free the net generic variables */\n+\tlist_for_each_entry_reverse(ops, &pernet_list, list) {\n+\t\tif (ops->size && ops->id) {\n+\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)\n+\t\t\t\tops_free(ops, net);\n+\t\t}\n+\t}\n \n \tmutex_unlock(&net_mutex);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct pernet_operations *ops;"
            ],
            "added_lines": [
                "\tconst struct pernet_operations *ops;",
                "\t/* Free the net generic variables */",
                "\tlist_for_each_entry_reverse(ops, &pernet_list, list) {",
                "\t\tif (ops->size && ops->id) {",
                "\t\t\tlist_for_each_entry(net, &net_kill_list, cleanup_list)",
                "\t\t\t\tops_free(ops, net);",
                "\t\t}",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/setup_net",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=f875bae065334907796da12523f9df85c89f5712",
        "commit_title": "To get the full benefit of batched network namespace cleanup netowrk",
        "commit_text": "device deletion needs to be performed by the generic code.  When using register_pernet_gen_device and freeing the data in exit_net it is impossible to delay allocation until after exit_net has called as the device uninit methods are no longer safe.  To correct this, and to simplify working with per network namespace data I have moved allocation and deletion of per network namespace data into the network namespace core.  The core now frees the data only after all of the network namespace exit routines have run.  Now it is only required to set the new fields .id and .size in the pernet_operations structure if you want network namespace data to be managed for you automatically.  This makes the current register_pernet_gen_device and register_pernet_gen_subsys routines unnecessary.  For the moment I have left them as compatibility wrappers in net_namespace.h They will be removed once all of the users have been updated.  ",
        "func_before": "static __net_init int setup_net(struct net *net)\n{\n\t/* Must be called with net_mutex held */\n\tstruct pernet_operations *ops;\n\tint error = 0;\n\n\tatomic_set(&net->count, 1);\n\n#ifdef NETNS_REFCNT_DEBUG\n\tatomic_set(&net->use_count, 0);\n#endif\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\tif (ops->init) {\n\t\t\terror = ops->init(net);\n\t\t\tif (error < 0)\n\t\t\t\tgoto out_undo;\n\t\t}\n\t}\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit)\n\t\t\tops->exit(net);\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\n\trcu_barrier();\n\tgoto out;\n}",
        "func": "static __net_init int setup_net(struct net *net)\n{\n\t/* Must be called with net_mutex held */\n\tconst struct pernet_operations *ops, *saved_ops;\n\tint error = 0;\n\n\tatomic_set(&net->count, 1);\n\n#ifdef NETNS_REFCNT_DEBUG\n\tatomic_set(&net->use_count, 0);\n#endif\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\terror = ops_init(ops, net);\n\t\tif (error < 0)\n\t\t\tgoto out_undo;\n\t}\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tsaved_ops = ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n\t\tif (ops->exit)\n\t\t\tops->exit(net);\n\t\tif (&ops->list == first_device) {\n\t\t\tLIST_HEAD(dev_kill_list);\n\t\t\trtnl_lock();\n\t\t\tunregister_netdevices(net, &dev_kill_list);\n\t\t\tunregister_netdevice_many(&dev_kill_list);\n\t\t\trtnl_unlock();\n\t\t}\n\t}\n\tops = saved_ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n\t\tops_free(ops, net);\n\n\trcu_barrier();\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static __net_init int setup_net(struct net *net)\n {\n \t/* Must be called with net_mutex held */\n-\tstruct pernet_operations *ops;\n+\tconst struct pernet_operations *ops, *saved_ops;\n \tint error = 0;\n \n \tatomic_set(&net->count, 1);\n@@ -11,11 +11,9 @@\n #endif\n \n \tlist_for_each_entry(ops, &pernet_list, list) {\n-\t\tif (ops->init) {\n-\t\t\terror = ops->init(net);\n-\t\t\tif (error < 0)\n-\t\t\t\tgoto out_undo;\n-\t\t}\n+\t\terror = ops_init(ops, net);\n+\t\tif (error < 0)\n+\t\t\tgoto out_undo;\n \t}\n out:\n \treturn error;\n@@ -24,6 +22,7 @@\n \t/* Walk through the list backwards calling the exit functions\n \t * for the pernet modules whose init functions did not fail.\n \t */\n+\tsaved_ops = ops;\n \tlist_for_each_entry_continue_reverse(ops, &pernet_list, list) {\n \t\tif (ops->exit)\n \t\t\tops->exit(net);\n@@ -35,6 +34,9 @@\n \t\t\trtnl_unlock();\n \t\t}\n \t}\n+\tops = saved_ops;\n+\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n+\t\tops_free(ops, net);\n \n \trcu_barrier();\n \tgoto out;",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct pernet_operations *ops;",
                "\t\tif (ops->init) {",
                "\t\t\terror = ops->init(net);",
                "\t\t\tif (error < 0)",
                "\t\t\t\tgoto out_undo;",
                "\t\t}"
            ],
            "added_lines": [
                "\tconst struct pernet_operations *ops, *saved_ops;",
                "\t\terror = ops_init(ops, net);",
                "\t\tif (error < 0)",
                "\t\t\tgoto out_undo;",
                "\tsaved_ops = ops;",
                "\tops = saved_ops;",
                "\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)",
                "\t\tops_free(ops, net);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/unregister_pernet_operations",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=f875bae065334907796da12523f9df85c89f5712",
        "commit_title": "To get the full benefit of batched network namespace cleanup netowrk",
        "commit_text": "device deletion needs to be performed by the generic code.  When using register_pernet_gen_device and freeing the data in exit_net it is impossible to delay allocation until after exit_net has called as the device uninit methods are no longer safe.  To correct this, and to simplify working with per network namespace data I have moved allocation and deletion of per network namespace data into the network namespace core.  The core now frees the data only after all of the network namespace exit routines have run.  Now it is only required to set the new fields .id and .size in the pernet_operations structure if you want network namespace data to be managed for you automatically.  This makes the current register_pernet_gen_device and register_pernet_gen_subsys routines unnecessary.  For the moment I have left them as compatibility wrappers in net_namespace.h They will be removed once all of the users have been updated.  ",
        "func_before": "static void unregister_pernet_operations(struct pernet_operations *ops)\n{\n\tstruct net *net;\n\n\tlist_del(&ops->list);\n\tif (ops->exit)\n\t\tfor_each_net(net)\n\t\t\tops->exit(net);\n}",
        "func": "static void unregister_pernet_operations(struct pernet_operations *ops)\n{\n\t\n\t__unregister_pernet_operations(ops);\n\tif (ops->id)\n\t\tida_remove(&net_generic_ids, *ops->id);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,7 @@\n static void unregister_pernet_operations(struct pernet_operations *ops)\n {\n-\tstruct net *net;\n-\n-\tlist_del(&ops->list);\n-\tif (ops->exit)\n-\t\tfor_each_net(net)\n-\t\t\tops->exit(net);\n+\t\n+\t__unregister_pernet_operations(ops);\n+\tif (ops->id)\n+\t\tida_remove(&net_generic_ids, *ops->id);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net;",
                "",
                "\tlist_del(&ops->list);",
                "\tif (ops->exit)",
                "\t\tfor_each_net(net)",
                "\t\t\tops->exit(net);"
            ],
            "added_lines": [
                "\t",
                "\t__unregister_pernet_operations(ops);",
                "\tif (ops->id)",
                "\t\tida_remove(&net_generic_ids, *ops->id);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-2189",
        "func_name": "torvalds/linux/register_pernet_operations",
        "description": "net/core/net_namespace.c in the Linux kernel 2.6.32 and earlier does not properly handle a high rate of creation and cleanup of network namespaces, which makes it easier for remote attackers to cause a denial of service (memory consumption) via requests to a daemon that requires a separate namespace per connection, as demonstrated by vsftpd.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=f875bae065334907796da12523f9df85c89f5712",
        "commit_title": "To get the full benefit of batched network namespace cleanup netowrk",
        "commit_text": "device deletion needs to be performed by the generic code.  When using register_pernet_gen_device and freeing the data in exit_net it is impossible to delay allocation until after exit_net has called as the device uninit methods are no longer safe.  To correct this, and to simplify working with per network namespace data I have moved allocation and deletion of per network namespace data into the network namespace core.  The core now frees the data only after all of the network namespace exit routines have run.  Now it is only required to set the new fields .id and .size in the pernet_operations structure if you want network namespace data to be managed for you automatically.  This makes the current register_pernet_gen_device and register_pernet_gen_subsys routines unnecessary.  For the moment I have left them as compatibility wrappers in net_namespace.h They will be removed once all of the users have been updated.  ",
        "func_before": "static int register_pernet_operations(struct list_head *list,\n\t\t\t\t      struct pernet_operations *ops)\n{\n\tstruct net *net, *undo_net;\n\tint error;\n\n\tlist_add_tail(&ops->list, list);\n\tif (ops->init) {\n\t\tfor_each_net(net) {\n\t\t\terror = ops->init(net);\n\t\t\tif (error)\n\t\t\t\tgoto out_undo;\n\t\t}\n\t}\n\treturn 0;\n\nout_undo:\n\t/* If I have an error cleanup all namespaces I initialized */\n\tlist_del(&ops->list);\n\tif (ops->exit) {\n\t\tfor_each_net(undo_net) {\n\t\t\tif (net_eq(undo_net, net))\n\t\t\t\tgoto undone;\n\t\t\tops->exit(undo_net);\n\t\t}\n\t}\nundone:\n\treturn error;\n}",
        "func": "static int register_pernet_operations(struct list_head *list,\n\t\t\t\t      struct pernet_operations *ops)\n{\n\tint error;\n\n\tif (ops->id) {\nagain:\n\t\terror = ida_get_new_above(&net_generic_ids, 1, ops->id);\n\t\tif (error < 0) {\n\t\t\tif (error == -EAGAIN) {\n\t\t\t\tida_pre_get(&net_generic_ids, GFP_KERNEL);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\treturn error;\n\t\t}\n\t}\n\terror = __register_pernet_operations(list, ops);\n\tif (error && ops->id)\n\t\tida_remove(&net_generic_ids, *ops->id);\n\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,29 +1,22 @@\n static int register_pernet_operations(struct list_head *list,\n \t\t\t\t      struct pernet_operations *ops)\n {\n-\tstruct net *net, *undo_net;\n \tint error;\n \n-\tlist_add_tail(&ops->list, list);\n-\tif (ops->init) {\n-\t\tfor_each_net(net) {\n-\t\t\terror = ops->init(net);\n-\t\t\tif (error)\n-\t\t\t\tgoto out_undo;\n+\tif (ops->id) {\n+again:\n+\t\terror = ida_get_new_above(&net_generic_ids, 1, ops->id);\n+\t\tif (error < 0) {\n+\t\t\tif (error == -EAGAIN) {\n+\t\t\t\tida_pre_get(&net_generic_ids, GFP_KERNEL);\n+\t\t\t\tgoto again;\n+\t\t\t}\n+\t\t\treturn error;\n \t\t}\n \t}\n-\treturn 0;\n+\terror = __register_pernet_operations(list, ops);\n+\tif (error && ops->id)\n+\t\tida_remove(&net_generic_ids, *ops->id);\n \n-out_undo:\n-\t/* If I have an error cleanup all namespaces I initialized */\n-\tlist_del(&ops->list);\n-\tif (ops->exit) {\n-\t\tfor_each_net(undo_net) {\n-\t\t\tif (net_eq(undo_net, net))\n-\t\t\t\tgoto undone;\n-\t\t\tops->exit(undo_net);\n-\t\t}\n-\t}\n-undone:\n \treturn error;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct net *net, *undo_net;",
                "\tlist_add_tail(&ops->list, list);",
                "\tif (ops->init) {",
                "\t\tfor_each_net(net) {",
                "\t\t\terror = ops->init(net);",
                "\t\t\tif (error)",
                "\t\t\t\tgoto out_undo;",
                "\treturn 0;",
                "out_undo:",
                "\t/* If I have an error cleanup all namespaces I initialized */",
                "\tlist_del(&ops->list);",
                "\tif (ops->exit) {",
                "\t\tfor_each_net(undo_net) {",
                "\t\t\tif (net_eq(undo_net, net))",
                "\t\t\t\tgoto undone;",
                "\t\t\tops->exit(undo_net);",
                "\t\t}",
                "\t}",
                "undone:"
            ],
            "added_lines": [
                "\tif (ops->id) {",
                "again:",
                "\t\terror = ida_get_new_above(&net_generic_ids, 1, ops->id);",
                "\t\tif (error < 0) {",
                "\t\t\tif (error == -EAGAIN) {",
                "\t\t\t\tida_pre_get(&net_generic_ids, GFP_KERNEL);",
                "\t\t\t\tgoto again;",
                "\t\t\t}",
                "\t\t\treturn error;",
                "\terror = __register_pernet_operations(list, ops);",
                "\tif (error && ops->id)",
                "\t\tida_remove(&net_generic_ids, *ops->id);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8610",
        "func_name": "openssl/dtls1_read_bytes",
        "description": "A denial of service flaw was found in OpenSSL 0.9.8, 1.0.1, 1.0.2 through 1.0.2h, and 1.1.0 in the way the TLS/SSL protocol defined processing of ALERT packets during a connection handshake. A remote attacker could use this flaw to make a TLS/SSL server consume an excessive amount of CPU and fail to accept connections from other clients.",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=af58be768ebb690f78530f796e92b8ae5c9a4401",
        "commit_title": "",
        "commit_text": "Don't allow too many consecutive warning alerts  Certain warning alerts are ignored if they are received. This can mean that no progress will be made if one peer continually sends those warning alerts. Implement a count so that we abort the connection if we receive too many.  Issue reported by Shi Lei.  ",
        "func_before": "int dtls1_read_bytes(SSL *s, int type, int *recvd_type, unsigned char *buf,\n                     int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (!SSL3_BUFFER_is_initialised(&s->rlayer.rbuf)) {\n        /* Not initialized yet */\n        if (!ssl3_setup_buffers(s))\n            return (-1);\n    }\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA) &&\n         (type != SSL3_RT_HANDSHAKE)) ||\n        (peek && (type != SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    /*\n     * check whether there's a handshake message (client hello?) waiting\n     */\n    if ((ret = have_handshake_fragment(s, type, buf, len)))\n        return ret;\n\n    /*\n     * Now s->rlayer.d->handshake_fragment_len == 0 if\n     * type == SSL3_RT_HANDSHAKE.\n     */\n\n#ifndef OPENSSL_NO_SCTP\n    /*\n     * Continue handshake if it had to be interrupted to read app data with\n     * SCTP.\n     */\n    if ((!ossl_statem_get_in_handshake(s) && SSL_in_init(s)) ||\n        (BIO_dgram_is_sctp(SSL_get_rbio(s))\n         && ossl_statem_in_sctp_read_sock(s)\n         && s->s3->in_read_app_data != 2))\n#else\n    if (!ossl_statem_get_in_handshake(s) && SSL_in_init(s))\n#endif\n    {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = s->rlayer.rrec;\n\n    /*\n     * We are not handshaking and have no data yet, so process data buffered\n     * during the last handshake in advance, if any.\n     */\n    if (SSL_is_init_finished(s) && SSL3_RECORD_get_length(rr) == 0) {\n        pitem *item;\n        item = pqueue_pop(s->rlayer.d->buffered_app_data.q);\n        if (item) {\n#ifndef OPENSSL_NO_SCTP\n            /* Restore bio_dgram_sctp_rcvinfo struct */\n            if (BIO_dgram_is_sctp(SSL_get_rbio(s))) {\n                DTLS1_RECORD_DATA *rdata = (DTLS1_RECORD_DATA *)item->data;\n                BIO_ctrl(SSL_get_rbio(s), BIO_CTRL_DGRAM_SCTP_SET_RCVINFO,\n                         sizeof(rdata->recordinfo), &rdata->recordinfo);\n            }\n#endif\n\n            dtls1_copy_record(s, item);\n\n            OPENSSL_free(item->data);\n            pitem_free(item);\n        }\n    }\n\n    /* Check for timeout */\n    if (dtls1_handle_timeout(s) > 0)\n        goto start;\n\n    /* get new packet if necessary */\n    if ((SSL3_RECORD_get_length(rr) == 0)\n        || (s->rlayer.rstate == SSL_ST_READ_BODY)) {\n        ret = dtls1_get_record(s);\n        if (ret <= 0) {\n            ret = dtls1_read_failed(s, ret);\n            /* anything other than a timeout is an error */\n            if (ret <= 0)\n                return (ret);\n            else\n                goto start;\n        }\n    }\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (SSL3_RECORD_get_type(rr) != SSL3_RT_HANDSHAKE)) {\n        /*\n         * We now have application data between CCS and Finished. Most likely\n         * the packets were reordered on their way, so buffer the application\n         * data for later processing rather than dropping the connection.\n         */\n        if (dtls1_buffer_record(s, &(s->rlayer.d->buffered_app_data),\n                                SSL3_RECORD_get_seq_num(rr)) < 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n            return -1;\n        }\n        SSL3_RECORD_set_length(rr, 0);\n        goto start;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        SSL3_RECORD_set_length(rr, 0);\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == SSL3_RECORD_get_type(rr)\n        || (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && type == SSL3_RT_HANDSHAKE && recvd_type != NULL)) {\n        /*\n         * SSL3_RT_APPLICATION_DATA or\n         * SSL3_RT_HANDSHAKE or\n         * SSL3_RT_CHANGE_CIPHER_SPEC\n         */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RECORD_get_type(rr);\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > SSL3_RECORD_get_length(rr))\n            n = SSL3_RECORD_get_length(rr);\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)]), n);\n        if (!peek) {\n            SSL3_RECORD_sub_length(rr, n);\n            SSL3_RECORD_add_off(rr, n);\n            if (SSL3_RECORD_get_length(rr) == 0) {\n                s->rlayer.rstate = SSL_ST_READ_HEADER;\n                SSL3_RECORD_set_off(rr, 0);\n            }\n        }\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * We were about to renegotiate but had to read belated application\n         * data first, so retry.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            SSL3_RECORD_get_type(rr) == SSL3_RT_APPLICATION_DATA &&\n            ossl_statem_in_sctp_read_sock(s)) {\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n        }\n\n        /*\n         * We might had to delay a close_notify alert because of reordered\n         * app data. If there was an alert and there is no message to read\n         * anymore, finally set shutdown.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            s->d1->shutdown_received\n            && !BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            return (0);\n        }\n#endif\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int k, dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (SSL3_RECORD_get_type(rr) == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof s->rlayer.d->handshake_fragment;\n            dest = s->rlayer.d->handshake_fragment;\n            dest_len = &s->rlayer.d->handshake_fragment_len;\n        } else if (SSL3_RECORD_get_type(rr) == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->rlayer.d->alert_fragment);\n            dest = s->rlayer.d->alert_fragment;\n            dest_len = &s->rlayer.d->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (SSL3_RECORD_get_type(rr) == DTLS1_RT_HEARTBEAT) {\n            /* We allow a 0 return */\n            if (dtls1_process_heartbeat(s, SSL3_RECORD_get_data(rr),\n                                        SSL3_RECORD_get_length(rr)) < 0) {\n                return -1;\n            }\n            /* Exit and notify application to read again */\n            SSL3_RECORD_set_length(rr, 0);\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n        /* else it's a CCS message, or application data or wrong */\n        else if (SSL3_RECORD_get_type(rr) != SSL3_RT_CHANGE_CIPHER_SPEC) {\n            /*\n             * Application data while renegotiating is allowed. Try again\n             * reading.\n             */\n            if (SSL3_RECORD_get_type(rr) == SSL3_RT_APPLICATION_DATA) {\n                BIO *bio;\n                s->s3->in_read_app_data = 2;\n                bio = SSL_get_rbio(s);\n                s->rwstate = SSL_READING;\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n\n            /* Not certain if this is the right error handling */\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n\n        if (dest_maxlen > 0) {\n            /*\n             * XDTLS: In a pathological case, the Client Hello may be\n             * fragmented--don't always expect dest_maxlen bytes\n             */\n            if (SSL3_RECORD_get_length(rr) < dest_maxlen) {\n#ifdef DTLS1_AD_MISSING_HANDSHAKE_MESSAGE\n                /*\n                 * for normal alerts rr->length is 2, while\n                 * dest_maxlen is 7 if we were to handle this\n                 * non-existing alert...\n                 */\n                FIX ME;\n#endif\n                s->rlayer.rstate = SSL_ST_READ_HEADER;\n                SSL3_RECORD_set_length(rr, 0);\n                goto start;\n            }\n\n            /* now move 'n' bytes: */\n            for (k = 0; k < dest_maxlen; k++) {\n                dest[k] = SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)];\n                SSL3_RECORD_add_off(rr, 1);\n                SSL3_RECORD_add_length(rr, -1);\n            }\n            *dest_len = dest_maxlen;\n        }\n    }\n\n    /*-\n     * s->rlayer.d->handshake_fragment_len == 12  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->rlayer.d->alert_fragment_len == 7      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->rlayer.d->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        (s->rlayer.d->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->rlayer.d->handshake_fragment_len = 0;\n\n        if ((s->rlayer.d->handshake_fragment[1] != 0) ||\n            (s->rlayer.d->handshake_fragment[2] != 0) ||\n            (s->rlayer.d->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        /*\n         * no need to check sequence number on HELLO REQUEST messages\n         */\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->rlayer.d->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            s->d1->handshake_read_seq++;\n            s->new_session = 1;\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (SSL3_BUFFER_get_left(&s->rlayer.rbuf) == 0) {\n                        /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    if (s->rlayer.d->alert_fragment_len >= DTLS1_AL_HEADER_LENGTH) {\n        int alert_level = s->rlayer.d->alert_fragment[0];\n        int alert_descr = s->rlayer.d->alert_fragment[1];\n\n        s->rlayer.d->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->rlayer.d->alert_fragment, 2, s,\n                            s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n#ifndef OPENSSL_NO_SCTP\n                /*\n                 * With SCTP and streams the socket may deliver app data\n                 * after a close_notify alert. We have to check this first so\n                 * that nothing gets discarded.\n                 */\n                if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n                    BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n                    s->d1->shutdown_received = 1;\n                    s->rwstate = SSL_READING;\n                    BIO_clear_retry_flags(SSL_get_rbio(s));\n                    BIO_set_retry_read(SSL_get_rbio(s));\n                    return -1;\n                }\n#endif\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n#if 0\n            /* XXX: this is a possible improvement in the future */\n            /* now check if it's a missing record */\n            if (alert_descr == DTLS1_AD_MISSING_HANDSHAKE_MESSAGE) {\n                unsigned short seq;\n                unsigned int frag_off;\n                unsigned char *p = &(s->rlayer.d->alert_fragment[2]);\n\n                n2s(p, seq);\n                n2l3(p, frag_off);\n\n                dtls1_retransmit_message(s,\n                                         dtls1_get_queue_priority\n                                         (frag->msg_header.seq, 0), frag_off,\n                                         &found);\n                if (!found && SSL_in_init(s)) {\n                    /*\n                     * fprintf( stderr,\"in init = %d\\n\", SSL_in_init(s));\n                     */\n                    /*\n                     * requested a message not yet sent, send an alert\n                     * ourselves\n                     */\n                    ssl3_send_alert(s, SSL3_AL_WARNING,\n                                    DTLS1_AD_MISSING_HANDSHAKE_MESSAGE);\n                }\n            }\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof tmp, \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        SSL3_RECORD_set_length(rr, 0);\n        return (0);\n    }\n\n    if (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        /*\n         * We can't process a CCS now, because previous handshake messages\n         * are still missing, so just drop it.\n         */\n        SSL3_RECORD_set_length(rr, 0);\n        goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->rlayer.d->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        !ossl_statem_get_in_handshake(s)) {\n        struct hm_header_st msg_hdr;\n\n        /* this may just be a stale retransmit */\n        dtls1_get_message_header(rr->data, &msg_hdr);\n        if (SSL3_RECORD_get_epoch(rr) != s->rlayer.d->r_epoch) {\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n\n        /*\n         * If we are server, we may have a repeated FINISHED of the client\n         * here, then retransmit our CCS and FINISHED.\n         */\n        if (msg_hdr.type == SSL3_MT_FINISHED) {\n            if (dtls1_check_timeout_num(s) < 0)\n                return -1;\n\n            dtls1_retransmit_buffered_messages(s);\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n            ossl_statem_set_in_init(s, 1);\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (SSL3_BUFFER_get_left(&s->rlayer.rbuf) == 0) {\n                /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (SSL3_RECORD_get_type(rr)) {\n    default:\n        /* TLS just ignores unknown message types */\n        if (s->version == TLS1_VERSION) {\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when ossl_statem_get_in_handshake(s) is true, but\n         * that should not happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            ossl_statem_app_data_allowed(s)) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n    return (-1);\n}",
        "func": "int dtls1_read_bytes(SSL *s, int type, int *recvd_type, unsigned char *buf,\n                     int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (!SSL3_BUFFER_is_initialised(&s->rlayer.rbuf)) {\n        /* Not initialized yet */\n        if (!ssl3_setup_buffers(s))\n            return (-1);\n    }\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA) &&\n         (type != SSL3_RT_HANDSHAKE)) ||\n        (peek && (type != SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    /*\n     * check whether there's a handshake message (client hello?) waiting\n     */\n    if ((ret = have_handshake_fragment(s, type, buf, len)))\n        return ret;\n\n    /*\n     * Now s->rlayer.d->handshake_fragment_len == 0 if\n     * type == SSL3_RT_HANDSHAKE.\n     */\n\n#ifndef OPENSSL_NO_SCTP\n    /*\n     * Continue handshake if it had to be interrupted to read app data with\n     * SCTP.\n     */\n    if ((!ossl_statem_get_in_handshake(s) && SSL_in_init(s)) ||\n        (BIO_dgram_is_sctp(SSL_get_rbio(s))\n         && ossl_statem_in_sctp_read_sock(s)\n         && s->s3->in_read_app_data != 2))\n#else\n    if (!ossl_statem_get_in_handshake(s) && SSL_in_init(s))\n#endif\n    {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = s->rlayer.rrec;\n\n    /*\n     * We are not handshaking and have no data yet, so process data buffered\n     * during the last handshake in advance, if any.\n     */\n    if (SSL_is_init_finished(s) && SSL3_RECORD_get_length(rr) == 0) {\n        pitem *item;\n        item = pqueue_pop(s->rlayer.d->buffered_app_data.q);\n        if (item) {\n#ifndef OPENSSL_NO_SCTP\n            /* Restore bio_dgram_sctp_rcvinfo struct */\n            if (BIO_dgram_is_sctp(SSL_get_rbio(s))) {\n                DTLS1_RECORD_DATA *rdata = (DTLS1_RECORD_DATA *)item->data;\n                BIO_ctrl(SSL_get_rbio(s), BIO_CTRL_DGRAM_SCTP_SET_RCVINFO,\n                         sizeof(rdata->recordinfo), &rdata->recordinfo);\n            }\n#endif\n\n            dtls1_copy_record(s, item);\n\n            OPENSSL_free(item->data);\n            pitem_free(item);\n        }\n    }\n\n    /* Check for timeout */\n    if (dtls1_handle_timeout(s) > 0)\n        goto start;\n\n    /* get new packet if necessary */\n    if ((SSL3_RECORD_get_length(rr) == 0)\n        || (s->rlayer.rstate == SSL_ST_READ_BODY)) {\n        ret = dtls1_get_record(s);\n        if (ret <= 0) {\n            ret = dtls1_read_failed(s, ret);\n            /* anything other than a timeout is an error */\n            if (ret <= 0)\n                return (ret);\n            else\n                goto start;\n        }\n    }\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT\n            && SSL3_RECORD_get_length(rr) != 0)\n        s->rlayer.alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (SSL3_RECORD_get_type(rr) != SSL3_RT_HANDSHAKE)) {\n        /*\n         * We now have application data between CCS and Finished. Most likely\n         * the packets were reordered on their way, so buffer the application\n         * data for later processing rather than dropping the connection.\n         */\n        if (dtls1_buffer_record(s, &(s->rlayer.d->buffered_app_data),\n                                SSL3_RECORD_get_seq_num(rr)) < 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n            return -1;\n        }\n        SSL3_RECORD_set_length(rr, 0);\n        goto start;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        SSL3_RECORD_set_length(rr, 0);\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == SSL3_RECORD_get_type(rr)\n        || (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && type == SSL3_RT_HANDSHAKE && recvd_type != NULL)) {\n        /*\n         * SSL3_RT_APPLICATION_DATA or\n         * SSL3_RT_HANDSHAKE or\n         * SSL3_RT_CHANGE_CIPHER_SPEC\n         */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RECORD_get_type(rr);\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > SSL3_RECORD_get_length(rr))\n            n = SSL3_RECORD_get_length(rr);\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)]), n);\n        if (!peek) {\n            SSL3_RECORD_sub_length(rr, n);\n            SSL3_RECORD_add_off(rr, n);\n            if (SSL3_RECORD_get_length(rr) == 0) {\n                s->rlayer.rstate = SSL_ST_READ_HEADER;\n                SSL3_RECORD_set_off(rr, 0);\n            }\n        }\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * We were about to renegotiate but had to read belated application\n         * data first, so retry.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            SSL3_RECORD_get_type(rr) == SSL3_RT_APPLICATION_DATA &&\n            ossl_statem_in_sctp_read_sock(s)) {\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n        }\n\n        /*\n         * We might had to delay a close_notify alert because of reordered\n         * app data. If there was an alert and there is no message to read\n         * anymore, finally set shutdown.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            s->d1->shutdown_received\n            && !BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            return (0);\n        }\n#endif\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int k, dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (SSL3_RECORD_get_type(rr) == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof s->rlayer.d->handshake_fragment;\n            dest = s->rlayer.d->handshake_fragment;\n            dest_len = &s->rlayer.d->handshake_fragment_len;\n        } else if (SSL3_RECORD_get_type(rr) == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->rlayer.d->alert_fragment);\n            dest = s->rlayer.d->alert_fragment;\n            dest_len = &s->rlayer.d->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (SSL3_RECORD_get_type(rr) == DTLS1_RT_HEARTBEAT) {\n            /* We allow a 0 return */\n            if (dtls1_process_heartbeat(s, SSL3_RECORD_get_data(rr),\n                                        SSL3_RECORD_get_length(rr)) < 0) {\n                return -1;\n            }\n            /* Exit and notify application to read again */\n            SSL3_RECORD_set_length(rr, 0);\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n        /* else it's a CCS message, or application data or wrong */\n        else if (SSL3_RECORD_get_type(rr) != SSL3_RT_CHANGE_CIPHER_SPEC) {\n            /*\n             * Application data while renegotiating is allowed. Try again\n             * reading.\n             */\n            if (SSL3_RECORD_get_type(rr) == SSL3_RT_APPLICATION_DATA) {\n                BIO *bio;\n                s->s3->in_read_app_data = 2;\n                bio = SSL_get_rbio(s);\n                s->rwstate = SSL_READING;\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n\n            /* Not certain if this is the right error handling */\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n\n        if (dest_maxlen > 0) {\n            /*\n             * XDTLS: In a pathological case, the Client Hello may be\n             * fragmented--don't always expect dest_maxlen bytes\n             */\n            if (SSL3_RECORD_get_length(rr) < dest_maxlen) {\n#ifdef DTLS1_AD_MISSING_HANDSHAKE_MESSAGE\n                /*\n                 * for normal alerts rr->length is 2, while\n                 * dest_maxlen is 7 if we were to handle this\n                 * non-existing alert...\n                 */\n                FIX ME;\n#endif\n                s->rlayer.rstate = SSL_ST_READ_HEADER;\n                SSL3_RECORD_set_length(rr, 0);\n                goto start;\n            }\n\n            /* now move 'n' bytes: */\n            for (k = 0; k < dest_maxlen; k++) {\n                dest[k] = SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)];\n                SSL3_RECORD_add_off(rr, 1);\n                SSL3_RECORD_add_length(rr, -1);\n            }\n            *dest_len = dest_maxlen;\n        }\n    }\n\n    /*-\n     * s->rlayer.d->handshake_fragment_len == 12  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->rlayer.d->alert_fragment_len == 7      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->rlayer.d->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        (s->rlayer.d->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->rlayer.d->handshake_fragment_len = 0;\n\n        if ((s->rlayer.d->handshake_fragment[1] != 0) ||\n            (s->rlayer.d->handshake_fragment[2] != 0) ||\n            (s->rlayer.d->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        /*\n         * no need to check sequence number on HELLO REQUEST messages\n         */\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->rlayer.d->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            s->d1->handshake_read_seq++;\n            s->new_session = 1;\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (SSL3_BUFFER_get_left(&s->rlayer.rbuf) == 0) {\n                        /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    if (s->rlayer.d->alert_fragment_len >= DTLS1_AL_HEADER_LENGTH) {\n        int alert_level = s->rlayer.d->alert_fragment[0];\n        int alert_descr = s->rlayer.d->alert_fragment[1];\n\n        s->rlayer.d->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->rlayer.d->alert_fragment, 2, s,\n                            s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n\n            s->rlayer.alert_count++;\n            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n#ifndef OPENSSL_NO_SCTP\n                /*\n                 * With SCTP and streams the socket may deliver app data\n                 * after a close_notify alert. We have to check this first so\n                 * that nothing gets discarded.\n                 */\n                if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n                    BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n                    s->d1->shutdown_received = 1;\n                    s->rwstate = SSL_READING;\n                    BIO_clear_retry_flags(SSL_get_rbio(s));\n                    BIO_set_retry_read(SSL_get_rbio(s));\n                    return -1;\n                }\n#endif\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n#if 0\n            /* XXX: this is a possible improvement in the future */\n            /* now check if it's a missing record */\n            if (alert_descr == DTLS1_AD_MISSING_HANDSHAKE_MESSAGE) {\n                unsigned short seq;\n                unsigned int frag_off;\n                unsigned char *p = &(s->rlayer.d->alert_fragment[2]);\n\n                n2s(p, seq);\n                n2l3(p, frag_off);\n\n                dtls1_retransmit_message(s,\n                                         dtls1_get_queue_priority\n                                         (frag->msg_header.seq, 0), frag_off,\n                                         &found);\n                if (!found && SSL_in_init(s)) {\n                    /*\n                     * fprintf( stderr,\"in init = %d\\n\", SSL_in_init(s));\n                     */\n                    /*\n                     * requested a message not yet sent, send an alert\n                     * ourselves\n                     */\n                    ssl3_send_alert(s, SSL3_AL_WARNING,\n                                    DTLS1_AD_MISSING_HANDSHAKE_MESSAGE);\n                }\n            }\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof tmp, \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        SSL3_RECORD_set_length(rr, 0);\n        return (0);\n    }\n\n    if (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        /*\n         * We can't process a CCS now, because previous handshake messages\n         * are still missing, so just drop it.\n         */\n        SSL3_RECORD_set_length(rr, 0);\n        goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->rlayer.d->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        !ossl_statem_get_in_handshake(s)) {\n        struct hm_header_st msg_hdr;\n\n        /* this may just be a stale retransmit */\n        dtls1_get_message_header(rr->data, &msg_hdr);\n        if (SSL3_RECORD_get_epoch(rr) != s->rlayer.d->r_epoch) {\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n\n        /*\n         * If we are server, we may have a repeated FINISHED of the client\n         * here, then retransmit our CCS and FINISHED.\n         */\n        if (msg_hdr.type == SSL3_MT_FINISHED) {\n            if (dtls1_check_timeout_num(s) < 0)\n                return -1;\n\n            dtls1_retransmit_buffered_messages(s);\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n            ossl_statem_set_in_init(s, 1);\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (SSL3_BUFFER_get_left(&s->rlayer.rbuf) == 0) {\n                /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (SSL3_RECORD_get_type(rr)) {\n    default:\n        /* TLS just ignores unknown message types */\n        if (s->version == TLS1_VERSION) {\n            SSL3_RECORD_set_length(rr, 0);\n            goto start;\n        }\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when ossl_statem_get_in_handshake(s) is true, but\n         * that should not happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            ossl_statem_app_data_allowed(s)) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -105,6 +105,14 @@\n                 goto start;\n         }\n     }\n+\n+    /*\n+     * Reset the count of consecutive warning alerts if we've got a non-empty\n+     * record that isn't an alert.\n+     */\n+    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT\n+            && SSL3_RECORD_get_length(rr) != 0)\n+        s->rlayer.alert_count = 0;\n \n     /* we now have a packet which can be read and processed */\n \n@@ -385,6 +393,14 @@\n \n         if (alert_level == SSL3_AL_WARNING) {\n             s->s3->warn_alert = alert_descr;\n+\n+            s->rlayer.alert_count++;\n+            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {\n+                al = SSL_AD_UNEXPECTED_MESSAGE;\n+                SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n+                goto f_err;\n+            }\n+\n             if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n #ifndef OPENSSL_NO_SCTP\n                 /*",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /*",
                "     * Reset the count of consecutive warning alerts if we've got a non-empty",
                "     * record that isn't an alert.",
                "     */",
                "    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT",
                "            && SSL3_RECORD_get_length(rr) != 0)",
                "        s->rlayer.alert_count = 0;",
                "",
                "            s->rlayer.alert_count++;",
                "            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {",
                "                al = SSL_AD_UNEXPECTED_MESSAGE;",
                "                SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);",
                "                goto f_err;",
                "            }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8610",
        "func_name": "openssl/ssl3_read_bytes",
        "description": "A denial of service flaw was found in OpenSSL 0.9.8, 1.0.1, 1.0.2 through 1.0.2h, and 1.1.0 in the way the TLS/SSL protocol defined processing of ALERT packets during a connection handshake. A remote attacker could use this flaw to make a TLS/SSL server consume an excessive amount of CPU and fail to accept connections from other clients.",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=af58be768ebb690f78530f796e92b8ae5c9a4401",
        "commit_title": "",
        "commit_text": "Don't allow too many consecutive warning alerts  Certain warning alerts are ignored if they are received. This can mean that no progress will be made if one peer continually sends those warning alerts. Implement a count so that we abort the connection if we receive too many.  Issue reported by Shi Lei.  ",
        "func_before": "int ssl3_read_bytes(SSL *s, int type, int *recvd_type, unsigned char *buf,\n                    int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n, curr_rec, num_recs, read_bytes;\n    SSL3_RECORD *rr;\n    SSL3_BUFFER *rbuf;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    rbuf = &s->rlayer.rbuf;\n\n    if (!SSL3_BUFFER_is_initialised(rbuf)) {\n        /* Not initialized yet */\n        if (!ssl3_setup_read_buffer(s))\n            return (-1);\n    }\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA)\n         && (type != SSL3_RT_HANDSHAKE)) || (peek\n                                             && (type !=\n                                                 SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    if ((type == SSL3_RT_HANDSHAKE) && (s->rlayer.handshake_fragment_len > 0))\n        /* (partially) satisfy request from storage */\n    {\n        unsigned char *src = s->rlayer.handshake_fragment;\n        unsigned char *dst = buf;\n        unsigned int k;\n\n        /* peek == 0 */\n        n = 0;\n        while ((len > 0) && (s->rlayer.handshake_fragment_len > 0)) {\n            *dst++ = *src++;\n            len--;\n            s->rlayer.handshake_fragment_len--;\n            n++;\n        }\n        /* move any remaining fragment bytes: */\n        for (k = 0; k < s->rlayer.handshake_fragment_len; k++)\n            s->rlayer.handshake_fragment[k] = *src++;\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RT_HANDSHAKE;\n\n        return n;\n    }\n\n    /*\n     * Now s->rlayer.handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n    if (!ossl_statem_get_in_handshake(s) && SSL_in_init(s)) {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * For each record 'i' up to |num_recs]\n     * rr[i].type     - is the type of record\n     * rr[i].data,    - data\n     * rr[i].off,     - offset into 'data' for next read\n     * rr[i].length,  - number of bytes.\n     */\n    rr = s->rlayer.rrec;\n    num_recs = RECORD_LAYER_get_numrpipes(&s->rlayer);\n\n    do {\n        /* get new records if necessary */\n        if (num_recs == 0) {\n            ret = ssl3_get_record(s);\n            if (ret <= 0)\n                return (ret);\n            num_recs = RECORD_LAYER_get_numrpipes(&s->rlayer);\n            if (num_recs == 0) {\n                /* Shouldn't happen */\n                al = SSL_AD_INTERNAL_ERROR;\n                SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n                goto f_err;\n            }\n        }\n        /* Skip over any records we have already read */\n        for (curr_rec = 0;\n             curr_rec < num_recs && SSL3_RECORD_is_read(&rr[curr_rec]);\n             curr_rec++) ;\n        if (curr_rec == num_recs) {\n            RECORD_LAYER_set_numrpipes(&s->rlayer, 0);\n            num_recs = 0;\n            curr_rec = 0;\n        }\n    } while (num_recs == 0);\n    rr = &rr[curr_rec];\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (SSL3_RECORD_get_type(rr) != SSL3_RT_HANDSHAKE)) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_DATA_BETWEEN_CCS_AND_FINISHED);\n        goto f_err;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        SSL3_RECORD_set_length(rr, 0);\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == SSL3_RECORD_get_type(rr)\n        || (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && type == SSL3_RT_HANDSHAKE && recvd_type != NULL)) {\n        /*\n         * SSL3_RT_APPLICATION_DATA or\n         * SSL3_RT_HANDSHAKE or\n         * SSL3_RT_CHANGE_CIPHER_SPEC\n         */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (type == SSL3_RT_HANDSHAKE\n            && SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && s->rlayer.handshake_fragment_len > 0) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RECORD_get_type(rr);\n\n        if (len <= 0)\n            return (len);\n\n        read_bytes = 0;\n        do {\n            if ((unsigned int)len - read_bytes > SSL3_RECORD_get_length(rr))\n                n = SSL3_RECORD_get_length(rr);\n            else\n                n = (unsigned int)len - read_bytes;\n\n            memcpy(buf, &(rr->data[rr->off]), n);\n            buf += n;\n            if (!peek) {\n                SSL3_RECORD_sub_length(rr, n);\n                SSL3_RECORD_add_off(rr, n);\n                if (SSL3_RECORD_get_length(rr) == 0) {\n                    s->rlayer.rstate = SSL_ST_READ_HEADER;\n                    SSL3_RECORD_set_off(rr, 0);\n                    SSL3_RECORD_set_read(rr);\n                }\n            }\n            if (SSL3_RECORD_get_length(rr) == 0\n                || (peek && n == SSL3_RECORD_get_length(rr))) {\n                curr_rec++;\n                rr++;\n            }\n            read_bytes += n;\n        } while (type == SSL3_RT_APPLICATION_DATA && curr_rec < num_recs\n                 && read_bytes < (unsigned int)len);\n        if (read_bytes == 0) {\n            /* We must have read empty records. Get more data */\n            goto start;\n        }\n        if (!peek && curr_rec == num_recs\n            && (s->mode & SSL_MODE_RELEASE_BUFFERS)\n            && SSL3_BUFFER_get_left(rbuf) == 0)\n            ssl3_release_read_buffer(s);\n        return read_bytes;\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello) or invalid (we\n     * were actually expecting a CCS).\n     */\n\n    /*\n     * Lets just double check that we've not got an SSLv2 record\n     */\n    if (rr->rec_version == SSL2_VERSION) {\n        /*\n         * Should never happen. ssl3_get_record() should only give us an SSLv2\n         * record back if this is the first packet and we are looking for an\n         * initial ClientHello. Therefore |type| should always be equal to\n         * |rr->type|. If not then something has gone horribly wrong\n         */\n        al = SSL_AD_INTERNAL_ERROR;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    }\n\n    if (s->method->version == TLS_ANY_VERSION\n        && (s->server || rr->type != SSL3_RT_ALERT)) {\n        /*\n         * If we've got this far and still haven't decided on what version\n         * we're using then this must be a client side alert we're dealing with\n         * (we don't allow heartbeats yet). We shouldn't be receiving anything\n         * other than a ClientHello if we are a server.\n         */\n        s->version = rr->rec_version;\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_MESSAGE);\n        goto f_err;\n    }\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (SSL3_RECORD_get_type(rr) == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof s->rlayer.handshake_fragment;\n            dest = s->rlayer.handshake_fragment;\n            dest_len = &s->rlayer.handshake_fragment_len;\n        } else if (SSL3_RECORD_get_type(rr) == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof s->rlayer.alert_fragment;\n            dest = s->rlayer.alert_fragment;\n            dest_len = &s->rlayer.alert_fragment_len;\n        }\n\n        if (dest_maxlen > 0) {\n            n = dest_maxlen - *dest_len; /* available space in 'dest' */\n            if (SSL3_RECORD_get_length(rr) < n)\n                n = SSL3_RECORD_get_length(rr); /* available bytes */\n\n            /* now move 'n' bytes: */\n            while (n-- > 0) {\n                dest[(*dest_len)++] =\n                    SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)];\n                SSL3_RECORD_add_off(rr, 1);\n                SSL3_RECORD_add_length(rr, -1);\n            }\n\n            if (*dest_len < dest_maxlen) {\n                SSL3_RECORD_set_read(rr);\n                goto start;     /* fragment was too small */\n            }\n        }\n    }\n\n    /*-\n     * s->rlayer.handshake_fragment_len == 4  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->rlayer.alert_fragment_len == 2      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->rlayer.handshake_fragment_len >= 4) &&\n        (s->rlayer.handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->rlayer.handshake_fragment_len = 0;\n\n        if ((s->rlayer.handshake_fragment[1] != 0) ||\n            (s->rlayer.handshake_fragment[2] != 0) ||\n            (s->rlayer.handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->rlayer.handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (SSL3_BUFFER_get_left(rbuf) == 0) {\n                        /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on. WARNING:\n     * experimental code, needs reviewing (steve)\n     */\n    if (s->server &&\n        SSL_is_init_finished(s) &&\n        !s->s3->send_connection_binding &&\n        (s->version > SSL3_VERSION) &&\n        (s->rlayer.handshake_fragment_len >= 4) &&\n        (s->rlayer.handshake_fragment[0] == SSL3_MT_CLIENT_HELLO) &&\n        (s->session != NULL) && (s->session->cipher != NULL) &&\n        !(s->ctx->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION)) {\n        SSL3_RECORD_set_length(rr, 0);\n        SSL3_RECORD_set_read(rr);\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n    if (s->rlayer.alert_fragment_len >= 2) {\n        int alert_level = s->rlayer.alert_fragment[0];\n        int alert_descr = s->rlayer.alert_fragment[1];\n\n        s->rlayer.alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->rlayer.alert_fragment, 2, s,\n                            s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n            SSL3_RECORD_set_read(rr);\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n            /*\n             * This is a warning but we receive it if we requested\n             * renegotiation and the peer denied it. Terminate with a fatal\n             * alert because if application tried to renegotiate it\n             * presumably had a good reason and expects it to succeed. In\n             * future we might have a renegotiation where we don't care if\n             * the peer refused it where we carry on.\n             */\n            else if (alert_descr == SSL_AD_NO_RENEGOTIATION) {\n                al = SSL_AD_HANDSHAKE_FAILURE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_NO_RENEGOTIATION);\n                goto f_err;\n            }\n#ifdef SSL_AD_MISSING_SRP_USERNAME\n            else if (alert_descr == SSL_AD_MISSING_SRP_USERNAME)\n                return (0);\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof tmp, \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL3_RECORD_set_read(rr);\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        SSL3_RECORD_set_length(rr, 0);\n        SSL3_RECORD_set_read(rr);\n        return (0);\n    }\n\n    if (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n        goto f_err;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->rlayer.handshake_fragment_len >= 4)\n        && !ossl_statem_get_in_handshake(s)) {\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n            ossl_statem_set_in_init(s, 1);\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (SSL3_BUFFER_get_left(rbuf) == 0) {\n                /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (SSL3_RECORD_get_type(rr)) {\n    default:\n        /*\n         * TLS up to v1.1 just ignores unknown message types: TLS v1.2 give\n         * an unexpected message alert.\n         */\n        if (s->version >= TLS1_VERSION && s->version <= TLS1_1_VERSION) {\n            SSL3_RECORD_set_length(rr, 0);\n            SSL3_RECORD_set_read(rr);\n            goto start;\n        }\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when ossl_statem_get_in_handshake(s) is true, but\n         * that should not happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (ossl_statem_app_data_allowed(s)) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n    return (-1);\n}",
        "func": "int ssl3_read_bytes(SSL *s, int type, int *recvd_type, unsigned char *buf,\n                    int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n, curr_rec, num_recs, read_bytes;\n    SSL3_RECORD *rr;\n    SSL3_BUFFER *rbuf;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    rbuf = &s->rlayer.rbuf;\n\n    if (!SSL3_BUFFER_is_initialised(rbuf)) {\n        /* Not initialized yet */\n        if (!ssl3_setup_read_buffer(s))\n            return (-1);\n    }\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA)\n         && (type != SSL3_RT_HANDSHAKE)) || (peek\n                                             && (type !=\n                                                 SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    if ((type == SSL3_RT_HANDSHAKE) && (s->rlayer.handshake_fragment_len > 0))\n        /* (partially) satisfy request from storage */\n    {\n        unsigned char *src = s->rlayer.handshake_fragment;\n        unsigned char *dst = buf;\n        unsigned int k;\n\n        /* peek == 0 */\n        n = 0;\n        while ((len > 0) && (s->rlayer.handshake_fragment_len > 0)) {\n            *dst++ = *src++;\n            len--;\n            s->rlayer.handshake_fragment_len--;\n            n++;\n        }\n        /* move any remaining fragment bytes: */\n        for (k = 0; k < s->rlayer.handshake_fragment_len; k++)\n            s->rlayer.handshake_fragment[k] = *src++;\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RT_HANDSHAKE;\n\n        return n;\n    }\n\n    /*\n     * Now s->rlayer.handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n    if (!ossl_statem_get_in_handshake(s) && SSL_in_init(s)) {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * For each record 'i' up to |num_recs]\n     * rr[i].type     - is the type of record\n     * rr[i].data,    - data\n     * rr[i].off,     - offset into 'data' for next read\n     * rr[i].length,  - number of bytes.\n     */\n    rr = s->rlayer.rrec;\n    num_recs = RECORD_LAYER_get_numrpipes(&s->rlayer);\n\n    do {\n        /* get new records if necessary */\n        if (num_recs == 0) {\n            ret = ssl3_get_record(s);\n            if (ret <= 0)\n                return (ret);\n            num_recs = RECORD_LAYER_get_numrpipes(&s->rlayer);\n            if (num_recs == 0) {\n                /* Shouldn't happen */\n                al = SSL_AD_INTERNAL_ERROR;\n                SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n                goto f_err;\n            }\n        }\n        /* Skip over any records we have already read */\n        for (curr_rec = 0;\n             curr_rec < num_recs && SSL3_RECORD_is_read(&rr[curr_rec]);\n             curr_rec++) ;\n        if (curr_rec == num_recs) {\n            RECORD_LAYER_set_numrpipes(&s->rlayer, 0);\n            num_recs = 0;\n            curr_rec = 0;\n        }\n    } while (num_recs == 0);\n    rr = &rr[curr_rec];\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT\n            && SSL3_RECORD_get_length(rr) != 0)\n        s->rlayer.alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (SSL3_RECORD_get_type(rr) != SSL3_RT_HANDSHAKE)) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_DATA_BETWEEN_CCS_AND_FINISHED);\n        goto f_err;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        SSL3_RECORD_set_length(rr, 0);\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == SSL3_RECORD_get_type(rr)\n        || (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && type == SSL3_RT_HANDSHAKE && recvd_type != NULL)) {\n        /*\n         * SSL3_RT_APPLICATION_DATA or\n         * SSL3_RT_HANDSHAKE or\n         * SSL3_RT_CHANGE_CIPHER_SPEC\n         */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (type == SSL3_RT_HANDSHAKE\n            && SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC\n            && s->rlayer.handshake_fragment_len > 0) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        if (recvd_type != NULL)\n            *recvd_type = SSL3_RECORD_get_type(rr);\n\n        if (len <= 0)\n            return (len);\n\n        read_bytes = 0;\n        do {\n            if ((unsigned int)len - read_bytes > SSL3_RECORD_get_length(rr))\n                n = SSL3_RECORD_get_length(rr);\n            else\n                n = (unsigned int)len - read_bytes;\n\n            memcpy(buf, &(rr->data[rr->off]), n);\n            buf += n;\n            if (!peek) {\n                SSL3_RECORD_sub_length(rr, n);\n                SSL3_RECORD_add_off(rr, n);\n                if (SSL3_RECORD_get_length(rr) == 0) {\n                    s->rlayer.rstate = SSL_ST_READ_HEADER;\n                    SSL3_RECORD_set_off(rr, 0);\n                    SSL3_RECORD_set_read(rr);\n                }\n            }\n            if (SSL3_RECORD_get_length(rr) == 0\n                || (peek && n == SSL3_RECORD_get_length(rr))) {\n                curr_rec++;\n                rr++;\n            }\n            read_bytes += n;\n        } while (type == SSL3_RT_APPLICATION_DATA && curr_rec < num_recs\n                 && read_bytes < (unsigned int)len);\n        if (read_bytes == 0) {\n            /* We must have read empty records. Get more data */\n            goto start;\n        }\n        if (!peek && curr_rec == num_recs\n            && (s->mode & SSL_MODE_RELEASE_BUFFERS)\n            && SSL3_BUFFER_get_left(rbuf) == 0)\n            ssl3_release_read_buffer(s);\n        return read_bytes;\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello) or invalid (we\n     * were actually expecting a CCS).\n     */\n\n    /*\n     * Lets just double check that we've not got an SSLv2 record\n     */\n    if (rr->rec_version == SSL2_VERSION) {\n        /*\n         * Should never happen. ssl3_get_record() should only give us an SSLv2\n         * record back if this is the first packet and we are looking for an\n         * initial ClientHello. Therefore |type| should always be equal to\n         * |rr->type|. If not then something has gone horribly wrong\n         */\n        al = SSL_AD_INTERNAL_ERROR;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    }\n\n    if (s->method->version == TLS_ANY_VERSION\n        && (s->server || rr->type != SSL3_RT_ALERT)) {\n        /*\n         * If we've got this far and still haven't decided on what version\n         * we're using then this must be a client side alert we're dealing with\n         * (we don't allow heartbeats yet). We shouldn't be receiving anything\n         * other than a ClientHello if we are a server.\n         */\n        s->version = rr->rec_version;\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_MESSAGE);\n        goto f_err;\n    }\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (SSL3_RECORD_get_type(rr) == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof s->rlayer.handshake_fragment;\n            dest = s->rlayer.handshake_fragment;\n            dest_len = &s->rlayer.handshake_fragment_len;\n        } else if (SSL3_RECORD_get_type(rr) == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof s->rlayer.alert_fragment;\n            dest = s->rlayer.alert_fragment;\n            dest_len = &s->rlayer.alert_fragment_len;\n        }\n\n        if (dest_maxlen > 0) {\n            n = dest_maxlen - *dest_len; /* available space in 'dest' */\n            if (SSL3_RECORD_get_length(rr) < n)\n                n = SSL3_RECORD_get_length(rr); /* available bytes */\n\n            /* now move 'n' bytes: */\n            while (n-- > 0) {\n                dest[(*dest_len)++] =\n                    SSL3_RECORD_get_data(rr)[SSL3_RECORD_get_off(rr)];\n                SSL3_RECORD_add_off(rr, 1);\n                SSL3_RECORD_add_length(rr, -1);\n            }\n\n            if (*dest_len < dest_maxlen) {\n                SSL3_RECORD_set_read(rr);\n                goto start;     /* fragment was too small */\n            }\n        }\n    }\n\n    /*-\n     * s->rlayer.handshake_fragment_len == 4  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->rlayer.alert_fragment_len == 2      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->rlayer.handshake_fragment_len >= 4) &&\n        (s->rlayer.handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->rlayer.handshake_fragment_len = 0;\n\n        if ((s->rlayer.handshake_fragment[1] != 0) ||\n            (s->rlayer.handshake_fragment[2] != 0) ||\n            (s->rlayer.handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->rlayer.handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (SSL3_BUFFER_get_left(rbuf) == 0) {\n                        /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on. WARNING:\n     * experimental code, needs reviewing (steve)\n     */\n    if (s->server &&\n        SSL_is_init_finished(s) &&\n        !s->s3->send_connection_binding &&\n        (s->version > SSL3_VERSION) &&\n        (s->rlayer.handshake_fragment_len >= 4) &&\n        (s->rlayer.handshake_fragment[0] == SSL3_MT_CLIENT_HELLO) &&\n        (s->session != NULL) && (s->session->cipher != NULL) &&\n        !(s->ctx->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION)) {\n        SSL3_RECORD_set_length(rr, 0);\n        SSL3_RECORD_set_read(rr);\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n    if (s->rlayer.alert_fragment_len >= 2) {\n        int alert_level = s->rlayer.alert_fragment[0];\n        int alert_descr = s->rlayer.alert_fragment[1];\n\n        s->rlayer.alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->rlayer.alert_fragment, 2, s,\n                            s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n            SSL3_RECORD_set_read(rr);\n\n            s->rlayer.alert_count++;\n            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n            /*\n             * This is a warning but we receive it if we requested\n             * renegotiation and the peer denied it. Terminate with a fatal\n             * alert because if application tried to renegotiate it\n             * presumably had a good reason and expects it to succeed. In\n             * future we might have a renegotiation where we don't care if\n             * the peer refused it where we carry on.\n             */\n            else if (alert_descr == SSL_AD_NO_RENEGOTIATION) {\n                al = SSL_AD_HANDSHAKE_FAILURE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_NO_RENEGOTIATION);\n                goto f_err;\n            }\n#ifdef SSL_AD_MISSING_SRP_USERNAME\n            else if (alert_descr == SSL_AD_MISSING_SRP_USERNAME)\n                return (0);\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof tmp, \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL3_RECORD_set_read(rr);\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        SSL3_RECORD_set_length(rr, 0);\n        SSL3_RECORD_set_read(rr);\n        return (0);\n    }\n\n    if (SSL3_RECORD_get_type(rr) == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n        goto f_err;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->rlayer.handshake_fragment_len >= 4)\n        && !ossl_statem_get_in_handshake(s)) {\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n            ossl_statem_set_in_init(s, 1);\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (SSL3_BUFFER_get_left(rbuf) == 0) {\n                /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (SSL3_RECORD_get_type(rr)) {\n    default:\n        /*\n         * TLS up to v1.1 just ignores unknown message types: TLS v1.2 give\n         * an unexpected message alert.\n         */\n        if (s->version >= TLS1_VERSION && s->version <= TLS1_1_VERSION) {\n            SSL3_RECORD_set_length(rr, 0);\n            SSL3_RECORD_set_read(rr);\n            goto start;\n        }\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when ossl_statem_get_in_handshake(s) is true, but\n         * that should not happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (ossl_statem_app_data_allowed(s)) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -100,6 +100,14 @@\n         }\n     } while (num_recs == 0);\n     rr = &rr[curr_rec];\n+\n+    /*\n+     * Reset the count of consecutive warning alerts if we've got a non-empty\n+     * record that isn't an alert.\n+     */\n+    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT\n+            && SSL3_RECORD_get_length(rr) != 0)\n+        s->rlayer.alert_count = 0;\n \n     /* we now have a packet which can be read and processed */\n \n@@ -371,6 +379,14 @@\n         if (alert_level == SSL3_AL_WARNING) {\n             s->s3->warn_alert = alert_descr;\n             SSL3_RECORD_set_read(rr);\n+\n+            s->rlayer.alert_count++;\n+            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {\n+                al = SSL_AD_UNEXPECTED_MESSAGE;\n+                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n+                goto f_err;\n+            }\n+\n             if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n                 s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                 return (0);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /*",
                "     * Reset the count of consecutive warning alerts if we've got a non-empty",
                "     * record that isn't an alert.",
                "     */",
                "    if (SSL3_RECORD_get_type(rr) != SSL3_RT_ALERT",
                "            && SSL3_RECORD_get_length(rr) != 0)",
                "        s->rlayer.alert_count = 0;",
                "",
                "            s->rlayer.alert_count++;",
                "            if (s->rlayer.alert_count == MAX_WARN_ALERT_COUNT) {",
                "                al = SSL_AD_UNEXPECTED_MESSAGE;",
                "                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);",
                "                goto f_err;",
                "            }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12190",
        "func_name": "torvalds/linux/bio_map_user_iov",
        "description": "The bio_map_user_iov and bio_unmap_user functions in block/bio.c in the Linux kernel before 4.13.8 do unbalanced refcounting when a SCSI I/O vector has small consecutive buffers belonging to the same page. The bio_add_pc_page function merges them into one, but the page reference is never dropped. This causes a memory leak and possible system lockup (exploitable against the host OS by a guest OS user, if a SCSI disk is passed through to a virtual machine) due to an out-of-memory condition.",
        "git_url": "https://github.com/torvalds/linux/commit/2b04e8f6bbb196cab4b232af0f8d48ff2c7a8058",
        "commit_title": "more bio_map_user_iov() leak fixes",
        "commit_text": " we need to take care of failure exit as well - pages already in bio should be dropped by analogue of bio_unmap_pages(), since their refcounts had been bumped only once per reference in bio.  Cc: stable@vger.kernel.org",
        "func_before": "struct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
        "func": "struct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\tstruct bio_vec *bvec;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (unlikely(ret < local_nr_pages)) {\n\t\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\t\tif (!pages[j])\n\t\t\t\t\tbreak;\n\t\t\t\tput_page(pages[j]);\n\t\t\t}\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tbio_for_each_segment_all(bvec, bio, j) {\n\t\tput_page(bvec->bv_page);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,7 @@\n \tint ret, offset;\n \tstruct iov_iter i;\n \tstruct iovec iov;\n+\tstruct bio_vec *bvec;\n \n \tiov_for_each(iov, i, *iter) {\n \t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n@@ -54,7 +55,12 @@\n \t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n \t\t\t\t(iter->type & WRITE) != WRITE,\n \t\t\t\t&pages[cur_page]);\n-\t\tif (ret < local_nr_pages) {\n+\t\tif (unlikely(ret < local_nr_pages)) {\n+\t\t\tfor (j = cur_page; j < page_limit; j++) {\n+\t\t\t\tif (!pages[j])\n+\t\t\t\t\tbreak;\n+\t\t\t\tput_page(pages[j]);\n+\t\t\t}\n \t\t\tret = -EFAULT;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -110,10 +116,8 @@\n \treturn bio;\n \n  out_unmap:\n-\tfor (j = 0; j < nr_pages; j++) {\n-\t\tif (!pages[j])\n-\t\t\tbreak;\n-\t\tput_page(pages[j]);\n+\tbio_for_each_segment_all(bvec, bio, j) {\n+\t\tput_page(bvec->bv_page);\n \t}\n  out:\n \tkfree(pages);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (ret < local_nr_pages) {",
                "\tfor (j = 0; j < nr_pages; j++) {",
                "\t\tif (!pages[j])",
                "\t\t\tbreak;",
                "\t\tput_page(pages[j]);"
            ],
            "added_lines": [
                "\tstruct bio_vec *bvec;",
                "\t\tif (unlikely(ret < local_nr_pages)) {",
                "\t\t\tfor (j = cur_page; j < page_limit; j++) {",
                "\t\t\t\tif (!pages[j])",
                "\t\t\t\t\tbreak;",
                "\t\t\t\tput_page(pages[j]);",
                "\t\t\t}",
                "\tbio_for_each_segment_all(bvec, bio, j) {",
                "\t\tput_page(bvec->bv_page);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-12190",
        "func_name": "torvalds/linux/bio_map_user_iov",
        "description": "The bio_map_user_iov and bio_unmap_user functions in block/bio.c in the Linux kernel before 4.13.8 do unbalanced refcounting when a SCSI I/O vector has small consecutive buffers belonging to the same page. The bio_add_pc_page function merges them into one, but the page reference is never dropped. This causes a memory leak and possible system lockup (exploitable against the host OS by a guest OS user, if a SCSI disk is passed through to a virtual machine) due to an out-of-memory condition.",
        "git_url": "https://github.com/torvalds/linux/commit/95d78c28b5a85bacbc29b8dba7c04babb9b0d467",
        "commit_title": "fix unbalanced page refcounting in bio_map_user_iov",
        "commit_text": " bio_map_user_iov and bio_unmap_user do unbalanced pages refcounting if IO vector has small consecutive buffers belonging to the same page. bio_add_pc_page merges them into one, but the page reference is never dropped.  Cc: stable@vger.kernel.org",
        "func_before": "struct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
        "func": "struct bio *bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask)\n{\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -62,6 +62,7 @@\n \t\toffset = offset_in_page(uaddr);\n \t\tfor (j = cur_page; j < page_limit; j++) {\n \t\t\tunsigned int bytes = PAGE_SIZE - offset;\n+\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n \n \t\t\tif (len <= 0)\n \t\t\t\tbreak;\n@@ -75,6 +76,13 @@\n \t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n \t\t\t\t\t    bytes)\n \t\t\t\tbreak;\n+\n+\t\t\t/*\n+\t\t\t * check if vector was merged with previous\n+\t\t\t * drop page reference if needed\n+\t\t\t */\n+\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n+\t\t\t\tput_page(pages[j]);\n \n \t\t\tlen -= bytes;\n \t\t\toffset = 0;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;",
                "",
                "\t\t\t/*",
                "\t\t\t * check if vector was merged with previous",
                "\t\t\t * drop page reference if needed",
                "\t\t\t */",
                "\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)",
                "\t\t\t\tput_page(pages[j]);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17682",
        "func_name": "ImageMagick/ExtractPostscript",
        "description": "In ImageMagick 7.0.7-12 Q16, a large loop vulnerability was found in the function ExtractPostscript in coders/wpg.c, which allows attackers to cause a denial of service (CPU exhaustion) via a crafted wpg image file that triggers a ReadWPGImage call.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/da649f031e36753c69268c5c027e695b8ae45e9a",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/870",
        "commit_text": "",
        "func_before": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while(PS_Size-- > 0)\n    {\n      (void) fputc(ReadBlobByte(image),ps_file);\n    }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) strncpy(clone_info->magick,magic_info->name,MaxTextExtent-1);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickString(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickString(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickString(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}",
        "func": "static Image *ExtractPostscript(Image *image,const ImageInfo *image_info,\n  MagickOffsetType PS_Offset,ssize_t PS_Size,ExceptionInfo *exception)\n{\n  char\n    postscript_file[MaxTextExtent];\n\n  const MagicInfo\n    *magic_info;\n\n  FILE\n    *ps_file;\n\n  int\n    c;\n\n  ImageInfo\n    *clone_info;\n\n  Image\n    *image2;\n\n  unsigned char\n    magick[2*MaxTextExtent];\n\n\n  if ((clone_info=CloneImageInfo(image_info)) == NULL)\n    return(image);\n  clone_info->blob=(void *) NULL;\n  clone_info->length=0;\n\n  /* Obtain temporary file */\n  (void) AcquireUniqueFilename(postscript_file);\n  ps_file=fopen_utf8(postscript_file,\"wb\");\n  if (ps_file == (FILE *) NULL)\n    goto FINISH;\n\n  /* Copy postscript to temporary file */\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  (void) ReadBlob(image, 2*MaxTextExtent, magick);\n\n  (void) SeekBlob(image,PS_Offset,SEEK_SET);\n  while (PS_Size-- > 0)\n  {\n    c=ReadBlobByte(image);\n    if (c == EOF)\n      break;\n    (void) fputc(c,ps_file);\n  }\n  (void) fclose(ps_file);\n\n    /* Detect file format - Check magic.mgk configuration file. */\n  magic_info=GetMagicInfo(magick,2*MaxTextExtent,exception);\n  if(magic_info == (const MagicInfo *) NULL) goto FINISH_UNL;\n  /*     printf(\"Detected:%s  \\n\",magic_info->name); */\n  if(exception->severity != UndefinedException) goto FINISH_UNL;\n  if(magic_info->name == (char *) NULL) goto FINISH_UNL;\n\n  (void) strncpy(clone_info->magick,magic_info->name,MaxTextExtent-1);\n\n    /* Read nested image */\n  /*FormatString(clone_info->filename,\"%s:%s\",magic_info->name,postscript_file);*/\n  FormatLocaleString(clone_info->filename,MaxTextExtent,\"%s\",postscript_file);\n  image2=ReadImage(clone_info,exception);\n\n  if (!image2)\n    goto FINISH_UNL;\n\n  /*\n    Replace current image with new image while copying base image\n    attributes.\n  */\n  (void) CopyMagickString(image2->filename,image->filename,MaxTextExtent);\n  (void) CopyMagickString(image2->magick_filename,image->magick_filename,MaxTextExtent);\n  (void) CopyMagickString(image2->magick,image->magick,MaxTextExtent);\n  image2->depth=image->depth;\n  DestroyBlob(image2);\n  image2->blob=ReferenceBlob(image->blob);\n\n  if ((image->rows == 0) || (image->columns == 0))\n    DeleteImageFromList(&image);\n\n  AppendImageToList(&image,image2);\n\n FINISH_UNL:\n  (void) RelinquishUniqueFileResource(postscript_file);\n FINISH:\n  DestroyImageInfo(clone_info);\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,9 @@\n \n   FILE\n     *ps_file;\n+\n+  int\n+    c;\n \n   ImageInfo\n     *clone_info;\n@@ -36,10 +39,13 @@\n   (void) ReadBlob(image, 2*MaxTextExtent, magick);\n \n   (void) SeekBlob(image,PS_Offset,SEEK_SET);\n-  while(PS_Size-- > 0)\n-    {\n-      (void) fputc(ReadBlobByte(image),ps_file);\n-    }\n+  while (PS_Size-- > 0)\n+  {\n+    c=ReadBlobByte(image);\n+    if (c == EOF)\n+      break;\n+    (void) fputc(c,ps_file);\n+  }\n   (void) fclose(ps_file);\n \n     /* Detect file format - Check magic.mgk configuration file. */",
        "diff_line_info": {
            "deleted_lines": [
                "  while(PS_Size-- > 0)",
                "    {",
                "      (void) fputc(ReadBlobByte(image),ps_file);",
                "    }"
            ],
            "added_lines": [
                "",
                "  int",
                "    c;",
                "  while (PS_Size-- > 0)",
                "  {",
                "    c=ReadBlobByte(image);",
                "    if (c == EOF)",
                "      break;",
                "    (void) fputc(c,ps_file);",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-1000476",
        "func_name": "ImageMagick/ReadDDSInfo",
        "description": "ImageMagick 7.0.7-12 Q16, a CPU exhaustion vulnerability was found in the function ReadDDSInfo in coders/dds.c, which allows attackers to cause a denial of service.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/9da3b39525982ad98f7e8f5e8183b9c613927e61",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/867",
        "commit_text": "",
        "func_before": "static MagickBooleanType ReadDDSInfo(Image *image, DDSInfo *dds_info)\n{\n  size_t\n    hdr_size,\n    required;\n  \n  /* Seek to start of header */\n  (void) SeekBlob(image, 4, SEEK_SET);\n  \n  /* Check header field */\n  hdr_size = ReadBlobLSBLong(image);\n  if (hdr_size != 124)\n    return MagickFalse;\n  \n  /* Fill in DDS info struct */\n  dds_info->flags = ReadBlobLSBLong(image);\n  \n  /* Check required flags */\n  required=(size_t) (DDSD_WIDTH | DDSD_HEIGHT | DDSD_PIXELFORMAT);\n  if ((dds_info->flags & required) != required)\n    return MagickFalse;\n  \n  dds_info->height = ReadBlobLSBLong(image);\n  dds_info->width = ReadBlobLSBLong(image);\n  dds_info->pitchOrLinearSize = ReadBlobLSBLong(image);\n  dds_info->depth = ReadBlobLSBLong(image);\n  dds_info->mipmapcount = ReadBlobLSBLong(image);\n  \n  (void) SeekBlob(image, 44, SEEK_CUR);   /* reserved region of 11 DWORDs */\n  \n  /* Read pixel format structure */\n  hdr_size = ReadBlobLSBLong(image);\n  if (hdr_size != 32)\n    return MagickFalse;\n  \n  dds_info->pixelformat.flags = ReadBlobLSBLong(image);\n  dds_info->pixelformat.fourcc = ReadBlobLSBLong(image);\n  dds_info->pixelformat.rgb_bitcount = ReadBlobLSBLong(image);\n  dds_info->pixelformat.r_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.g_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.b_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.alpha_bitmask = ReadBlobLSBLong(image);\n  \n  dds_info->ddscaps1 = ReadBlobLSBLong(image);\n  dds_info->ddscaps2 = ReadBlobLSBLong(image);\n  (void) SeekBlob(image, 12, SEEK_CUR); /* 3 reserved DWORDs */\n  \n  return MagickTrue;\n}",
        "func": "static MagickBooleanType ReadDDSInfo(Image *image, DDSInfo *dds_info)\n{\n  size_t\n    hdr_size,\n    required;\n  \n  /* Seek to start of header */\n  (void) SeekBlob(image, 4, SEEK_SET);\n  \n  /* Check header field */\n  hdr_size = ReadBlobLSBLong(image);\n  if (hdr_size != 124)\n    return MagickFalse;\n  \n  /* Fill in DDS info struct */\n  dds_info->flags = ReadBlobLSBLong(image);\n  \n  /* Check required flags */\n  required=(size_t) (DDSD_WIDTH | DDSD_HEIGHT | DDSD_PIXELFORMAT);\n  if ((dds_info->flags & required) != required)\n    return MagickFalse;\n  \n  dds_info->height = ReadBlobLSBLong(image);\n  dds_info->width = ReadBlobLSBLong(image);\n  dds_info->pitchOrLinearSize = ReadBlobLSBLong(image);\n  dds_info->depth = ReadBlobLSBLong(image);\n  dds_info->mipmapcount = ReadBlobLSBLong(image);\n  if (dds_info->mipmapcount > GetBlobSize(image))\n    return MagickFalse;\n  \n  (void) SeekBlob(image, 44, SEEK_CUR);   /* reserved region of 11 DWORDs */\n  \n  /* Read pixel format structure */\n  hdr_size = ReadBlobLSBLong(image);\n  if (hdr_size != 32)\n    return MagickFalse;\n  \n  dds_info->pixelformat.flags = ReadBlobLSBLong(image);\n  dds_info->pixelformat.fourcc = ReadBlobLSBLong(image);\n  dds_info->pixelformat.rgb_bitcount = ReadBlobLSBLong(image);\n  dds_info->pixelformat.r_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.g_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.b_bitmask = ReadBlobLSBLong(image);\n  dds_info->pixelformat.alpha_bitmask = ReadBlobLSBLong(image);\n  \n  dds_info->ddscaps1 = ReadBlobLSBLong(image);\n  dds_info->ddscaps2 = ReadBlobLSBLong(image);\n  (void) SeekBlob(image, 12, SEEK_CUR); /* 3 reserved DWORDs */\n  \n  return MagickTrue;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,8 @@\n   dds_info->pitchOrLinearSize = ReadBlobLSBLong(image);\n   dds_info->depth = ReadBlobLSBLong(image);\n   dds_info->mipmapcount = ReadBlobLSBLong(image);\n+  if (dds_info->mipmapcount > GetBlobSize(image))\n+    return MagickFalse;\n   \n   (void) SeekBlob(image, 44, SEEK_CUR);   /* reserved region of 11 DWORDs */\n   ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (dds_info->mipmapcount > GetBlobSize(image))",
                "    return MagickFalse;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0476",
        "func_name": "radareorg/radare2/read_module",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/27fe8031782d3a06c3998eaa94354867864f9f1b",
        "commit_title": "Fix DoS in the minidump parser ##crash",
        "commit_text": " * Reported by lazymio via huntr.dev * Reproducer: mdmp-dos",
        "func_before": "static void read_module(RBuffer *b, ut64 addr, struct minidump_module *module) {\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tr_buf_seek (b, addr, R_BUF_SET);\n\tmodule->base_of_image = r_buf_read_le64 (b);\n\tmodule->size_of_image = r_buf_read_le32 (b);\n\tmodule->check_sum = r_buf_read_le32 (b);\n\tmodule->time_date_stamp = r_buf_read_le32 (b);\n\tmodule->module_name_rva = r_buf_read_le32 (b);\n\tmodule->version_info.dw_signature = r_buf_read_le32 (b);\n\tmodule->version_info.dw_struc_version = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_version_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_version_ls = r_buf_read_le32 (b);\n\tmodule->version_info.dw_product_version_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_product_version_ls = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_flags_mask = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_flags = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_os = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_type = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_subtype = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_date_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_date_ls = r_buf_read_le32 (b);\n\tmodule->cv_record.data_size = r_buf_read_le32 (b);\n\tmodule->cv_record.rva = r_buf_read_le32 (b);\n\tmodule->misc_record.data_size = r_buf_read_le32 (b);\n\tmodule->misc_record.rva = r_buf_read_le32 (b);\n\tmodule->reserved_0 = r_buf_read_le64 (b);\n\tmodule->reserved_1 = r_buf_read_le64 (b);\n\tr_buf_seek (b, o_addr, R_BUF_SET);\n}",
        "func": "static struct minidump_module *read_module(RBuffer *b, ut64 addr) {\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tif (r_buf_seek (b, addr, R_BUF_SET) == -1) {\n\t\treturn NULL;\n\t}\n\tstruct minidump_module *module = R_NEW0 (struct minidump_module);\n\tif (!module) {\n\t\treturn NULL;\n\t}\n\tmodule->base_of_image = r_buf_read_le64 (b);\n\tmodule->size_of_image = r_buf_read_le32 (b);\n\tmodule->check_sum = r_buf_read_le32 (b);\n\tmodule->time_date_stamp = r_buf_read_le32 (b);\n\tmodule->module_name_rva = r_buf_read_le32 (b);\n\tmodule->version_info.dw_signature = r_buf_read_le32 (b);\n\tmodule->version_info.dw_struc_version = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_version_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_version_ls = r_buf_read_le32 (b);\n\tmodule->version_info.dw_product_version_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_product_version_ls = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_flags_mask = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_flags = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_os = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_type = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_subtype = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_date_ms = r_buf_read_le32 (b);\n\tmodule->version_info.dw_file_date_ls = r_buf_read_le32 (b);\n\tmodule->cv_record.data_size = r_buf_read_le32 (b);\n\tmodule->cv_record.rva = r_buf_read_le32 (b);\n\tmodule->misc_record.data_size = r_buf_read_le32 (b);\n\tmodule->misc_record.rva = r_buf_read_le32 (b);\n\tmodule->reserved_0 = r_buf_read_le64 (b);\n\tmodule->reserved_1 = r_buf_read_le64 (b);\n\tr_buf_seek (b, o_addr, R_BUF_SET);\n\treturn module;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,12 @@\n-static void read_module(RBuffer *b, ut64 addr, struct minidump_module *module) {\n+static struct minidump_module *read_module(RBuffer *b, ut64 addr) {\n \tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n-\tr_buf_seek (b, addr, R_BUF_SET);\n+\tif (r_buf_seek (b, addr, R_BUF_SET) == -1) {\n+\t\treturn NULL;\n+\t}\n+\tstruct minidump_module *module = R_NEW0 (struct minidump_module);\n+\tif (!module) {\n+\t\treturn NULL;\n+\t}\n \tmodule->base_of_image = r_buf_read_le64 (b);\n \tmodule->size_of_image = r_buf_read_le32 (b);\n \tmodule->check_sum = r_buf_read_le32 (b);\n@@ -26,4 +32,5 @@\n \tmodule->reserved_0 = r_buf_read_le64 (b);\n \tmodule->reserved_1 = r_buf_read_le64 (b);\n \tr_buf_seek (b, o_addr, R_BUF_SET);\n+\treturn module;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void read_module(RBuffer *b, ut64 addr, struct minidump_module *module) {",
                "\tr_buf_seek (b, addr, R_BUF_SET);"
            ],
            "added_lines": [
                "static struct minidump_module *read_module(RBuffer *b, ut64 addr) {",
                "\tif (r_buf_seek (b, addr, R_BUF_SET) == -1) {",
                "\t\treturn NULL;",
                "\t}",
                "\tstruct minidump_module *module = R_NEW0 (struct minidump_module);",
                "\tif (!module) {",
                "\t\treturn NULL;",
                "\t}",
                "\treturn module;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0476",
        "func_name": "radareorg/radare2/r_bin_mdmp_init_directory_entry",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/27fe8031782d3a06c3998eaa94354867864f9f1b",
        "commit_title": "Fix DoS in the minidump parser ##crash",
        "commit_text": " * Reported by lazymio via huntr.dev * Reproducer: mdmp-dos",
        "func_before": "static bool r_bin_mdmp_init_directory_entry(struct r_bin_mdmp_obj *obj, struct minidump_directory *entry) {\n\tr_strf_buffer (128);\n\tstruct minidump_handle_operation_list handle_operation_list;\n\tstruct minidump_memory_list memory_list;\n\tstruct minidump_memory64_list memory64_list;\n\tstruct minidump_memory_info_list memory_info_list;\n\tstruct minidump_module_list module_list;\n\tstruct minidump_thread_list thread_list;\n\tstruct minidump_thread_ex_list thread_ex_list;\n\tstruct minidump_thread_info_list thread_info_list;\n\tstruct minidump_token_info_list token_info_list;\n\tstruct minidump_unloaded_module_list unloaded_module_list;\n\tut64 offset;\n\tint i, r;\n\n\t/* We could confirm data sizes but a malcious MDMP will always get around\n\t** this! But we can ensure that the data is not outside of the file */\n\tif ((ut64)entry->location.rva + entry->location.data_size > r_buf_size (obj->b)) {\n\t\teprintf (\"[ERROR] Size Mismatch - Stream data is larger than file size!\\n\");\n\t\treturn false;\n\t}\n\n\tswitch (entry->stream_type) {\n\tcase THREAD_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_list, sizeof (thread_list));\n\t\tif (r != sizeof (thread_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread.format\", \"ddddq?? \"\n\t\t\t\"ThreadId SuspendCount PriorityClass Priority \"\n\t\t\t\"Teb (mdmp_memory_descriptor)Stack \"\n\t\t\t\"(mdmp_location_descriptor)ThreadContext\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfThreads (mdmp_thread)Threads\",\n\t\t\t\tthread_list.number_of_threads),\n\t\t\t0);\n\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tbreak;\n\tcase MODULE_LIST_STREAM:\n\t\tmodule_list.number_of_modules = r_buf_read_le32_at (obj->b, entry->location.rva);\n\n\t\tsdb_set (obj->kv, \"mdmp_module.format\", \"qddtd???qq \"\n\t\t\t\"BaseOfImage SizeOfImage CheckSum \"\n\t\t\t\"TimeDateStamp ModuleNameRVA \"\n\t\t\t\"(mdmp_vs_fixedfileinfo)VersionInfo \"\n\t\t\t\"(mdmp_location_descriptor)CvRecord \"\n\t\t\t\"(mdmp_location_descriptor)MiscRecord \"\n\t\t\t\"Reserved0 Reserved1\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_module_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_module_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfModule (mdmp_module)Modules\",\n\t\t\t\tmodule_list.number_of_modules),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (module_list);\n\t\tfor (i = 0; i < module_list.number_of_modules; i++) {\n\t\t\tstruct minidump_module *module = R_NEW (struct minidump_module);\n\t\t\tif (!module) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tread_module (obj->b, offset, module);\n\t\t\tr_list_append (obj->streams.modules, module);\n\t\t\toffset += sizeof (*module);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&memory_list, sizeof (memory_list));\n\t\tif (r != sizeof (memory_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_memory_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfMemoryRanges \"\n\t\t\t\t\"(mdmp_memory_descriptor)MemoryRanges \",\n\t\t\t\tmemory_list.number_of_memory_ranges),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (memory_list);\n\t\tfor (i = 0; i < memory_list.number_of_memory_ranges; i++) {\n\t\t\tstruct minidump_memory_descriptor *desc = R_NEW (struct minidump_memory_descriptor);\n\t\t\tif (!desc) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)desc, sizeof (*desc));\n\t\t\tif (r != sizeof (*desc)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.memories, desc);\n\t\t\toffset += sizeof (*desc);\n\t\t}\n\t\tbreak;\n\tcase EXCEPTION_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.exception = R_NEW (struct minidump_exception_stream);\n\t\tif (!obj->streams.exception) {\n\t\t\tbreak;\n\t\t}\n\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.exception, sizeof (*obj->streams.exception));\n\t\tif (r != sizeof (*obj->streams.exception)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_exception.format\", \"[4]E[4]Eqqdd[15]q \"\n\t\t\t\t\t\t\t   \"(mdmp_exception_code)ExceptionCode \"\n\t\t\t\t\t\t\t   \"(mdmp_exception_flags)ExceptionFlags \"\n\t\t\t\t\t\t\t   \"ExceptionRecord ExceptionAddress \"\n\t\t\t\t\t\t\t   \"NumberParameters __UnusedAlignment \"\n\t\t\t\t\t\t\t   \"ExceptionInformation\",\n\t\t\t0);\n\t\tsdb_num_set (obj->kv, \"mdmp_exception_stream.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_exception_stream.format\", \"dd?? \"\n\t\t\t\t\t\t\t\t  \"ThreadId __Alignment \"\n\t\t\t\t\t\t\t\t  \"(mdmp_exception)ExceptionRecord \"\n\t\t\t\t\t\t\t\t  \"(mdmp_location_descriptor)ThreadContext\",\n\t\t\t0);\n\n\t\tbreak;\n\tcase SYSTEM_INFO_STREAM:\n\t\tobj->streams.system_info = R_NEW (struct minidump_system_info);\n\t\tif (!obj->streams.system_info) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.system_info, sizeof (*obj->streams.system_info));\n\t\tif (r != sizeof (*obj->streams.system_info)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_system_info.offset\",\n\t\t\tentry->location.rva, 0);\n\t\t/* TODO: We need E as a byte! */\n\t\tsdb_set (obj->kv, \"mdmp_system_info.format\", \"[2]EwwbBddd[4]Ed[2]Ew[2]q \"\n\t\t\t\"(mdmp_processor_architecture)ProcessorArchitecture \"\n\t\t\t\"ProcessorLevel ProcessorRevision NumberOfProcessors \"\n\t\t\t\"(mdmp_product_type)ProductType \"\n\t\t\t\"MajorVersion MinorVersion BuildNumber (mdmp_platform_id)PlatformId \"\n\t\t\t\"CsdVersionRva (mdmp_suite_mask)SuiteMask Reserved2 ProcessorFeatures\", 0);\n\n\t\tbreak;\n\tcase THREAD_EX_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_ex_list, sizeof (thread_ex_list));\n\t\tif (r != sizeof (thread_ex_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread_ex.format\", \"ddddq??? \"\n\t\t\t\"ThreadId SuspendCount PriorityClass Priority \"\n\t\t\t\"Teb (mdmp_memory_descriptor)Stack \"\n\t\t\t\"(mdmp_location_descriptor)ThreadContext \"\n\t\t\t\"(mdmp_memory_descriptor)BackingStore\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_ex_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_ex_list.format\",\n\t\t\tr_strf (\"d[%d]? NumberOfThreads \"\n\t\t\t\t\"(mdmp_thread_ex)Threads\",\n\t\t\t\tthread_ex_list.number_of_threads),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (thread_ex_list);\n\t\tfor (i = 0; i < thread_ex_list.number_of_threads; i++) {\n\t\t\tstruct minidump_thread_ex *thread = R_NEW (struct minidump_thread_ex);\n\t\t\tif (!thread) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)thread, sizeof (*thread));\n\t\t\tif (r != sizeof (*thread)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.ex_threads, thread);\n\t\t\toffset += sizeof (*thread);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_64_LIST_STREAM:\n\t\tread_memory64_list (obj->b, entry->location.rva, &memory64_list);\n\n\t\tsdb_num_set (obj->kv, \"mdmp_memory64_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory64_list.format\",\n\t\t\tr_strf (\"qq[%\"PFMT64d\"]? NumberOfMemoryRanges \"\n\t\t\t\t\"BaseRva \"\n\t\t\t\t\"(mdmp_memory_descriptor64)MemoryRanges\",\n\t\t\t\tmemory64_list.number_of_memory_ranges),\n\t\t\t0);\n\n\t\tobj->streams.memories64.base_rva = memory64_list.base_rva;\n\t\toffset = entry->location.rva + sizeof (memory64_list);\n\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges; i++) {\n\t\t\tstruct minidump_memory_descriptor64 *desc = R_NEW (struct minidump_memory_descriptor64);\n\t\t\tif (!desc) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tread_desc (obj->b, offset, desc);\n\t\t\tr_list_append (obj->streams.memories64.memories, desc);\n\t\t\toffset += sizeof (*desc);\n\t\t}\n\t\tbreak;\n\tcase COMMENT_STREAM_A:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.comments_a = R_NEWS (ut8, COMMENTS_SIZE);\n\t\tif (!obj->streams.comments_a) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, obj->streams.comments_a, COMMENTS_SIZE);\n\t\tif (r != COMMENTS_SIZE) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_comment_stream_a.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_comment_stream_a.format\",\n\t\t\t\"s CommentA\", 0);\n\n\t\tbreak;\n\tcase COMMENT_STREAM_W:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.comments_w = R_NEWS (ut8, COMMENTS_SIZE);\n\t\tif (!obj->streams.comments_w) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, obj->streams.comments_w, COMMENTS_SIZE);\n\t\tif (r != COMMENTS_SIZE) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_comment_stream_w.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_comment_stream_w.format\",\n\t\t\t\t\"s CommentW\", 0);\n\n\t\tbreak;\n\tcase HANDLE_DATA_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.handle_data = R_NEW (struct minidump_handle_data_stream);\n\t\tif (!obj->streams.handle_data) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.handle_data, sizeof (*obj->streams.handle_data));\n\t\tif (r != sizeof (*obj->streams.handle_data)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_handle_data_stream.offset\",\n\t\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_handle_data_stream.format\", \"dddd \"\n\t\t\t\t\"SizeOfHeader SizeOfDescriptor \"\n\t\t\t\t\"NumberOfDescriptors Reserved\", 0);\n\t\tbreak;\n\tcase FUNCTION_TABLE_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.function_table = R_NEW (struct minidump_function_table_stream);\n\t\tif (!obj->streams.function_table) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.function_table, sizeof (*obj->streams.function_table));\n\t\tif (r != sizeof (*obj->streams.function_table)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_function_table_stream.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_function_table_stream.format\", \"dddddd \"\n\t\t\t\"SizeOfHeader SizeOfDescriptor SizeOfNativeDescriptor \"\n\t\t\t\"SizeOfFunctionEntry NumberOfDescriptors SizeOfAlignPad\",\n\t\t\t0);\n\t\tbreak;\n\tcase UNLOADED_MODULE_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&unloaded_module_list, sizeof (unloaded_module_list));\n\t\tif (r != sizeof (unloaded_module_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_unloaded_module.format\", \"qddtd \"\n\t\t\t\"BaseOfImage SizeOfImage CheckSum TimeDateStamp \"\n\t\t\t\"ModuleNameRva\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_unloaded_module_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_unloaded_module_list.format\", \"ddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n\n\t\toffset = entry->location.rva + sizeof (unloaded_module_list);\n\t\tfor (i = 0; i < unloaded_module_list.number_of_entries; i++) {\n\t\t\tstruct minidump_unloaded_module *module = R_NEW (struct minidump_unloaded_module);\n\t\t\tif (!module) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)module, sizeof (*module));\n\t\t\tif (r != sizeof (*module)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.unloaded_modules, module);\n\t\t\toffset += sizeof (*module);\n\t\t}\n\t\tbreak;\n\tcase MISC_INFO_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.misc_info.misc_info_1 = R_NEW (struct minidump_misc_info);\n\t\tif (!obj->streams.misc_info.misc_info_1) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.misc_info.misc_info_1, sizeof (*obj->streams.misc_info.misc_info_1));\n\t\tif (r != sizeof (*obj->streams.misc_info.misc_info_1)) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* TODO: Handle different sizes */\n\t\tsdb_num_set (obj->kv, \"mdmp_misc_info.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_misc_info.format\", \"d[4]Bdtttddddd \"\n\t\t\t\"SizeOfInfo (mdmp_misc1_flags)Flags1 ProcessId \"\n\t\t\t\"ProcessCreateTime ProcessUserTime ProcessKernelTime \"\n\t\t\t\"ProcessorMaxMhz ProcessorCurrentMhz \"\n\t\t\t\"ProcessorMhzLimit ProcessorMaxIdleState \"\n\t\t\t\"ProcessorCurrentIdleState\", 0);\n\n\t\tbreak;\n\tcase MEMORY_INFO_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&memory_info_list, sizeof (memory_info_list));\n\t\tif (r != sizeof (memory_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_memory_info.format\",\n\t\t\t\"qq[4]Edq[4]E[4]E[4]Ed BaseAddress AllocationBase \"\n\t\t\t\"(mdmp_page_protect)AllocationProtect __Alignment1 RegionSize \"\n\t\t\t\"(mdmp_mem_state)State (mdmp_page_protect)Protect \"\n\t\t\t\"(mdmp_mem_type)Type __Alignment2\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_memory_info_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory_info_list.format\",\n\t\t\tr_strf (\"ddq[%\"PFMT64d\"]? SizeOfHeader SizeOfEntry \"\n\t\t\t\t\"NumberOfEntries (mdmp_memory_info)MemoryInfo\",\n\t\t\t\tmemory_info_list.number_of_entries),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (memory_info_list);\n\t\tfor (i = 0; i < memory_info_list.number_of_entries; i++) {\n\t\t\tstruct minidump_memory_info *info = R_NEW (struct minidump_memory_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.memory_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\tcase THREAD_INFO_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_info_list, sizeof (thread_info_list));\n\t\tif (r != sizeof (thread_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread_info.format\", \"ddddttttqq \"\n\t\t\t\"ThreadId DumpFlags DumpError ExitStatus CreateTime \"\n\t\t\t\"ExitTime KernelTime UserTime StartAddress Affinity\",\n\t\t\t0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_info_list.offset\",\n\t\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_info_list.format\", \"ddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n\n\t\toffset = entry->location.rva + sizeof (thread_info_list);\n\t\tfor (i = 0; i < thread_info_list.number_of_entries; i++) {\n\t\t\tstruct minidump_thread_info *info = R_NEW (struct minidump_thread_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.thread_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\tcase HANDLE_OPERATION_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&handle_operation_list, sizeof (handle_operation_list));\n\t\tif (r != sizeof (handle_operation_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_handle_operation_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_handle_operation_list.format\", \"dddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries Reserved\", 0);\n\n\t\toffset = entry->location.rva + sizeof (handle_operation_list);\n\t\tfor (i = 0; i < handle_operation_list.number_of_entries; i++) {\n\t\t\tstruct avrf_handle_operation *op = R_NEW (struct avrf_handle_operation);\n\t\t\tif (!op) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)op, sizeof (*op));\n\t\t\tif (r != sizeof (*op)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.operations, op);\n\t\t\toffset += sizeof (*op);\n\t\t}\n\n\t\tbreak;\n\tcase TOKEN_STREAM:\n\t\t/* TODO: Not fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&token_info_list, sizeof (token_info_list));\n\t\tif (r != sizeof (token_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_token_info.format\", \"ddq \"\n\t\t\t\"TokenSize TokenId TokenHandle\", 0);\n\n\t\tsdb_num_set (obj->kv, \"mdmp_token_info_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_token_info_list.format\", \"dddd \"\n\t\t\t\"TokenListSize TokenListEntries ListHeaderSize ElementHeaderSize\", 0);\n\n\t\toffset = entry->location.rva + sizeof (token_info_list);\n\t\tfor (i = 0; i < token_info_list.number_of_entries; i++) {\n\t\t\tstruct minidump_token_info *info = R_NEW (struct minidump_token_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.token_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\n\tcase LAST_RESERVED_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tbreak;\n\tcase UNUSED_STREAM:\n\tcase RESERVED_STREAM_0:\n\tcase RESERVED_STREAM_1:\n\t\t/* Silently ignore reserved streams */\n\t\tbreak;\n\tdefault:\n\t\teprintf (\"[WARN] Invalid or unsupported enumeration encountered %d\\n\", entry->stream_type);\n\t\tbreak;\n\t}\n\treturn true;\n}",
        "func": "static bool r_bin_mdmp_init_directory_entry(struct r_bin_mdmp_obj *obj, struct minidump_directory *entry) {\n\tr_strf_buffer (128);\n\tstruct minidump_handle_operation_list handle_operation_list;\n\tstruct minidump_memory_list memory_list;\n\tstruct minidump_memory64_list memory64_list;\n\tstruct minidump_memory_info_list memory_info_list;\n\tstruct minidump_module_list module_list;\n\tstruct minidump_thread_list thread_list;\n\tstruct minidump_thread_ex_list thread_ex_list;\n\tstruct minidump_thread_info_list thread_info_list;\n\tstruct minidump_token_info_list token_info_list;\n\tstruct minidump_unloaded_module_list unloaded_module_list;\n\tut64 offset;\n\tint i, r;\n\n\t/* We could confirm data sizes but a malcious MDMP will always get around\n\t** this! But we can ensure that the data is not outside of the file */\n\tif ((ut64)entry->location.rva + entry->location.data_size > r_buf_size (obj->b)) {\n\t\teprintf (\"[ERROR] Size Mismatch - Stream data is larger than file size!\\n\");\n\t\treturn false;\n\t}\n\n\tswitch (entry->stream_type) {\n\tcase THREAD_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_list, sizeof (thread_list));\n\t\tif (r != sizeof (thread_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread.format\", \"ddddq?? \"\n\t\t\t\"ThreadId SuspendCount PriorityClass Priority \"\n\t\t\t\"Teb (mdmp_memory_descriptor)Stack \"\n\t\t\t\"(mdmp_location_descriptor)ThreadContext\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfThreads (mdmp_thread)Threads\",\n\t\t\t\tthread_list.number_of_threads),\n\t\t\t0);\n\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tbreak;\n\tcase MODULE_LIST_STREAM:\n\t\tmodule_list.number_of_modules = r_buf_read_le32_at (obj->b, entry->location.rva);\n\n\t\tsdb_set (obj->kv, \"mdmp_module.format\", \"qddtd???qq \"\n\t\t\t\"BaseOfImage SizeOfImage CheckSum \"\n\t\t\t\"TimeDateStamp ModuleNameRVA \"\n\t\t\t\"(mdmp_vs_fixedfileinfo)VersionInfo \"\n\t\t\t\"(mdmp_location_descriptor)CvRecord \"\n\t\t\t\"(mdmp_location_descriptor)MiscRecord \"\n\t\t\t\"Reserved0 Reserved1\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_module_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_module_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfModule (mdmp_module)Modules\",\n\t\t\t\tmodule_list.number_of_modules),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (module_list);\n\t\tfor (i = 0; i < module_list.number_of_modules && offset < obj->size; i++) {\n\t\t\tstruct minidump_module *module = read_module (obj->b, offset);\n\t\t\tif (!module) {\n\t\t\t\tbreak;\t\n\t\t\t}\n\t\t\tr_list_append (obj->streams.modules, module);\n\t\t\toffset += sizeof (*module);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&memory_list, sizeof (memory_list));\n\t\tif (r != sizeof (memory_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_memory_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory_list.format\",\n\t\t\tr_strf (\"d[%d]? \"\n\t\t\t\t\"NumberOfMemoryRanges \"\n\t\t\t\t\"(mdmp_memory_descriptor)MemoryRanges \",\n\t\t\t\tmemory_list.number_of_memory_ranges),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (memory_list);\n\t\tfor (i = 0; i < memory_list.number_of_memory_ranges && offset < obj->size; i++) {\n\t\t\tstruct minidump_memory_descriptor *desc = R_NEW (struct minidump_memory_descriptor);\n\t\t\tif (!desc) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)desc, sizeof (*desc));\n\t\t\tif (r != sizeof (*desc)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.memories, desc);\n\t\t\toffset += sizeof (*desc);\n\t\t}\n\t\tbreak;\n\tcase EXCEPTION_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.exception = R_NEW (struct minidump_exception_stream);\n\t\tif (!obj->streams.exception) {\n\t\t\tbreak;\n\t\t}\n\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.exception, sizeof (*obj->streams.exception));\n\t\tif (r != sizeof (*obj->streams.exception)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_exception.format\", \"[4]E[4]Eqqdd[15]q \"\n\t\t\t\t\t\t\t   \"(mdmp_exception_code)ExceptionCode \"\n\t\t\t\t\t\t\t   \"(mdmp_exception_flags)ExceptionFlags \"\n\t\t\t\t\t\t\t   \"ExceptionRecord ExceptionAddress \"\n\t\t\t\t\t\t\t   \"NumberParameters __UnusedAlignment \"\n\t\t\t\t\t\t\t   \"ExceptionInformation\",\n\t\t\t0);\n\t\tsdb_num_set (obj->kv, \"mdmp_exception_stream.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_exception_stream.format\", \"dd?? \"\n\t\t\t\t\t\t\t\t  \"ThreadId __Alignment \"\n\t\t\t\t\t\t\t\t  \"(mdmp_exception)ExceptionRecord \"\n\t\t\t\t\t\t\t\t  \"(mdmp_location_descriptor)ThreadContext\",\n\t\t\t0);\n\n\t\tbreak;\n\tcase SYSTEM_INFO_STREAM:\n\t\tobj->streams.system_info = R_NEW (struct minidump_system_info);\n\t\tif (!obj->streams.system_info) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.system_info, sizeof (*obj->streams.system_info));\n\t\tif (r != sizeof (*obj->streams.system_info)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_system_info.offset\",\n\t\t\tentry->location.rva, 0);\n\t\t/* TODO: We need E as a byte! */\n\t\tsdb_set (obj->kv, \"mdmp_system_info.format\", \"[2]EwwbBddd[4]Ed[2]Ew[2]q \"\n\t\t\t\"(mdmp_processor_architecture)ProcessorArchitecture \"\n\t\t\t\"ProcessorLevel ProcessorRevision NumberOfProcessors \"\n\t\t\t\"(mdmp_product_type)ProductType \"\n\t\t\t\"MajorVersion MinorVersion BuildNumber (mdmp_platform_id)PlatformId \"\n\t\t\t\"CsdVersionRva (mdmp_suite_mask)SuiteMask Reserved2 ProcessorFeatures\", 0);\n\n\t\tbreak;\n\tcase THREAD_EX_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_ex_list, sizeof (thread_ex_list));\n\t\tif (r != sizeof (thread_ex_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread_ex.format\", \"ddddq??? \"\n\t\t\t\"ThreadId SuspendCount PriorityClass Priority \"\n\t\t\t\"Teb (mdmp_memory_descriptor)Stack \"\n\t\t\t\"(mdmp_location_descriptor)ThreadContext \"\n\t\t\t\"(mdmp_memory_descriptor)BackingStore\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_ex_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_ex_list.format\",\n\t\t\tr_strf (\"d[%d]? NumberOfThreads \"\n\t\t\t\t\"(mdmp_thread_ex)Threads\",\n\t\t\t\tthread_ex_list.number_of_threads),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (thread_ex_list);\n\t\tfor (i = 0; i < thread_ex_list.number_of_threads && offset < obj->size; i++) {\n\t\t\tstruct minidump_thread_ex *thread = R_NEW (struct minidump_thread_ex);\n\t\t\tif (!thread) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)thread, sizeof (*thread));\n\t\t\tif (r != sizeof (*thread)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.ex_threads, thread);\n\t\t\toffset += sizeof (*thread);\n\t\t}\n\t\tbreak;\n\tcase MEMORY_64_LIST_STREAM:\n\t\tread_memory64_list (obj->b, entry->location.rva, &memory64_list);\n\n\t\tsdb_num_set (obj->kv, \"mdmp_memory64_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory64_list.format\",\n\t\t\tr_strf (\"qq[%\"PFMT64d\"]? NumberOfMemoryRanges \"\n\t\t\t\t\"BaseRva \"\n\t\t\t\t\"(mdmp_memory_descriptor64)MemoryRanges\",\n\t\t\t\tmemory64_list.number_of_memory_ranges),\n\t\t\t0);\n\n\t\tobj->streams.memories64.base_rva = memory64_list.base_rva;\n\t\toffset = entry->location.rva + sizeof (memory64_list);\n\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges && offset < obj->size; i++) {\n\t\t\tstruct minidump_memory_descriptor64 *desc = R_NEW (struct minidump_memory_descriptor64);\n\t\t\tif (!desc) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tread_desc (obj->b, offset, desc);\n\t\t\tr_list_append (obj->streams.memories64.memories, desc);\n\t\t\toffset += sizeof (*desc);\n\t\t}\n\t\tbreak;\n\tcase COMMENT_STREAM_A:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.comments_a = R_NEWS (ut8, COMMENTS_SIZE);\n\t\tif (!obj->streams.comments_a) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, obj->streams.comments_a, COMMENTS_SIZE);\n\t\tif (r != COMMENTS_SIZE) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_comment_stream_a.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_comment_stream_a.format\",\n\t\t\t\"s CommentA\", 0);\n\n\t\tbreak;\n\tcase COMMENT_STREAM_W:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.comments_w = R_NEWS (ut8, COMMENTS_SIZE);\n\t\tif (!obj->streams.comments_w) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, obj->streams.comments_w, COMMENTS_SIZE);\n\t\tif (r != COMMENTS_SIZE) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_comment_stream_w.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_comment_stream_w.format\",\n\t\t\t\t\"s CommentW\", 0);\n\n\t\tbreak;\n\tcase HANDLE_DATA_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.handle_data = R_NEW (struct minidump_handle_data_stream);\n\t\tif (!obj->streams.handle_data) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.handle_data, sizeof (*obj->streams.handle_data));\n\t\tif (r != sizeof (*obj->streams.handle_data)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_handle_data_stream.offset\",\n\t\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_handle_data_stream.format\", \"dddd \"\n\t\t\t\t\"SizeOfHeader SizeOfDescriptor \"\n\t\t\t\t\"NumberOfDescriptors Reserved\", 0);\n\t\tbreak;\n\tcase FUNCTION_TABLE_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.function_table = R_NEW (struct minidump_function_table_stream);\n\t\tif (!obj->streams.function_table) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.function_table, sizeof (*obj->streams.function_table));\n\t\tif (r != sizeof (*obj->streams.function_table)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_function_table_stream.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_function_table_stream.format\", \"dddddd \"\n\t\t\t\"SizeOfHeader SizeOfDescriptor SizeOfNativeDescriptor \"\n\t\t\t\"SizeOfFunctionEntry NumberOfDescriptors SizeOfAlignPad\",\n\t\t\t0);\n\t\tbreak;\n\tcase UNLOADED_MODULE_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&unloaded_module_list, sizeof (unloaded_module_list));\n\t\tif (r != sizeof (unloaded_module_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_unloaded_module.format\", \"qddtd \"\n\t\t\t\"BaseOfImage SizeOfImage CheckSum TimeDateStamp \"\n\t\t\t\"ModuleNameRva\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_unloaded_module_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_unloaded_module_list.format\", \"ddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n\n\t\toffset = entry->location.rva + sizeof (unloaded_module_list);\n\t\tfor (i = 0; i < unloaded_module_list.number_of_entries && offset < obj->size; i++) {\n\t\t\tstruct minidump_unloaded_module *module = R_NEW (struct minidump_unloaded_module);\n\t\t\tif (!module) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)module, sizeof (*module));\n\t\t\tif (r != sizeof (*module)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.unloaded_modules, module);\n\t\t\toffset += sizeof (*module);\n\t\t}\n\t\tbreak;\n\tcase MISC_INFO_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tobj->streams.misc_info.misc_info_1 = R_NEW (struct minidump_misc_info);\n\t\tif (!obj->streams.misc_info.misc_info_1) {\n\t\t\tbreak;\n\t\t}\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)obj->streams.misc_info.misc_info_1, sizeof (*obj->streams.misc_info.misc_info_1));\n\t\tif (r != sizeof (*obj->streams.misc_info.misc_info_1)) {\n\t\t\tbreak;\n\t\t}\n\n\t\t/* TODO: Handle different sizes */\n\t\tsdb_num_set (obj->kv, \"mdmp_misc_info.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_misc_info.format\", \"d[4]Bdtttddddd \"\n\t\t\t\"SizeOfInfo (mdmp_misc1_flags)Flags1 ProcessId \"\n\t\t\t\"ProcessCreateTime ProcessUserTime ProcessKernelTime \"\n\t\t\t\"ProcessorMaxMhz ProcessorCurrentMhz \"\n\t\t\t\"ProcessorMhzLimit ProcessorMaxIdleState \"\n\t\t\t\"ProcessorCurrentIdleState\", 0);\n\n\t\tbreak;\n\tcase MEMORY_INFO_LIST_STREAM:\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&memory_info_list, sizeof (memory_info_list));\n\t\tif (r != sizeof (memory_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_memory_info.format\",\n\t\t\t\"qq[4]Edq[4]E[4]E[4]Ed BaseAddress AllocationBase \"\n\t\t\t\"(mdmp_page_protect)AllocationProtect __Alignment1 RegionSize \"\n\t\t\t\"(mdmp_mem_state)State (mdmp_page_protect)Protect \"\n\t\t\t\"(mdmp_mem_type)Type __Alignment2\", 0);\n\t\tsdb_num_set (obj->kv, \"mdmp_memory_info_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_memory_info_list.format\",\n\t\t\tr_strf (\"ddq[%\"PFMT64d\"]? SizeOfHeader SizeOfEntry \"\n\t\t\t\t\"NumberOfEntries (mdmp_memory_info)MemoryInfo\",\n\t\t\t\tmemory_info_list.number_of_entries),\n\t\t\t0);\n\n\t\toffset = entry->location.rva + sizeof (memory_info_list);\n\t\tfor (i = 0; i < memory_info_list.number_of_entries && offset < obj->size; i++) {\n\t\t\tstruct minidump_memory_info *info = R_NEW (struct minidump_memory_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.memory_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\tcase THREAD_INFO_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&thread_info_list, sizeof (thread_info_list));\n\t\tif (r != sizeof (thread_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_thread_info.format\", \"ddddttttqq \"\n\t\t\t\"ThreadId DumpFlags DumpError ExitStatus CreateTime \"\n\t\t\t\"ExitTime KernelTime UserTime StartAddress Affinity\",\n\t\t\t0);\n\t\tsdb_num_set (obj->kv, \"mdmp_thread_info_list.offset\",\n\t\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_thread_info_list.format\", \"ddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n\n\t\toffset = entry->location.rva + sizeof (thread_info_list);\n\t\tfor (i = 0; i < thread_info_list.number_of_entries && offset < obj->size; i++) {\n\t\t\tstruct minidump_thread_info *info = R_NEW (struct minidump_thread_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.thread_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\tcase HANDLE_OPERATION_LIST_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&handle_operation_list, sizeof (handle_operation_list));\n\t\tif (r != sizeof (handle_operation_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_num_set (obj->kv, \"mdmp_handle_operation_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_handle_operation_list.format\", \"dddd \"\n\t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries Reserved\", 0);\n\n\t\toffset = entry->location.rva + sizeof (handle_operation_list);\n\t\tfor (i = 0; i < handle_operation_list.number_of_entries && offset < obj->size; i++) {\n\t\t\tstruct avrf_handle_operation *op = R_NEW (struct avrf_handle_operation);\n\t\t\tif (!op) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)op, sizeof (*op));\n\t\t\tif (r != sizeof (*op)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.operations, op);\n\t\t\toffset += sizeof (*op);\n\t\t}\n\n\t\tbreak;\n\tcase TOKEN_STREAM:\n\t\t/* TODO: Not fully parsed or utilised */\n\t\tr = r_buf_read_at (obj->b, entry->location.rva, (ut8 *)&token_info_list, sizeof (token_info_list));\n\t\tif (r != sizeof (token_info_list)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tsdb_set (obj->kv, \"mdmp_token_info.format\", \"ddq \"\n\t\t\t\"TokenSize TokenId TokenHandle\", 0);\n\n\t\tsdb_num_set (obj->kv, \"mdmp_token_info_list.offset\",\n\t\t\tentry->location.rva, 0);\n\t\tsdb_set (obj->kv, \"mdmp_token_info_list.format\", \"dddd \"\n\t\t\t\"TokenListSize TokenListEntries ListHeaderSize ElementHeaderSize\", 0);\n\n\t\toffset = entry->location.rva + sizeof (token_info_list);\n\t\tfor (i = 0; i < token_info_list.number_of_entries && offset < obj->size; i++) {\n\t\t\tstruct minidump_token_info *info = R_NEW (struct minidump_token_info);\n\t\t\tif (!info) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr = r_buf_read_at (obj->b, offset, (ut8 *)info, sizeof (*info));\n\t\t\tif (r != sizeof (*info)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tr_list_append (obj->streams.token_infos, info);\n\t\t\toffset += sizeof (*info);\n\t\t}\n\t\tbreak;\n\n\tcase LAST_RESERVED_STREAM:\n\t\t/* TODO: Not yet fully parsed or utilised */\n\t\tbreak;\n\tcase UNUSED_STREAM:\n\tcase RESERVED_STREAM_0:\n\tcase RESERVED_STREAM_1:\n\t\t/* Silently ignore reserved streams */\n\t\tbreak;\n\tdefault:\n\t\teprintf (\"[WARN] Invalid or unsupported enumeration encountered %d\\n\", entry->stream_type);\n\t\tbreak;\n\t}\n\treturn true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -60,12 +60,11 @@\n \t\t\t0);\n \n \t\toffset = entry->location.rva + sizeof (module_list);\n-\t\tfor (i = 0; i < module_list.number_of_modules; i++) {\n-\t\t\tstruct minidump_module *module = R_NEW (struct minidump_module);\n+\t\tfor (i = 0; i < module_list.number_of_modules && offset < obj->size; i++) {\n+\t\t\tstruct minidump_module *module = read_module (obj->b, offset);\n \t\t\tif (!module) {\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tread_module (obj->b, offset, module);\n+\t\t\t\tbreak;\t\n+\t\t\t}\n \t\t\tr_list_append (obj->streams.modules, module);\n \t\t\toffset += sizeof (*module);\n \t\t}\n@@ -86,7 +85,7 @@\n \t\t\t0);\n \n \t\toffset = entry->location.rva + sizeof (memory_list);\n-\t\tfor (i = 0; i < memory_list.number_of_memory_ranges; i++) {\n+\t\tfor (i = 0; i < memory_list.number_of_memory_ranges && offset < obj->size; i++) {\n \t\t\tstruct minidump_memory_descriptor *desc = R_NEW (struct minidump_memory_descriptor);\n \t\t\tif (!desc) {\n \t\t\t\tbreak;\n@@ -169,7 +168,7 @@\n \t\t\t0);\n \n \t\toffset = entry->location.rva + sizeof (thread_ex_list);\n-\t\tfor (i = 0; i < thread_ex_list.number_of_threads; i++) {\n+\t\tfor (i = 0; i < thread_ex_list.number_of_threads && offset < obj->size; i++) {\n \t\t\tstruct minidump_thread_ex *thread = R_NEW (struct minidump_thread_ex);\n \t\t\tif (!thread) {\n \t\t\t\tbreak;\n@@ -196,7 +195,7 @@\n \n \t\tobj->streams.memories64.base_rva = memory64_list.base_rva;\n \t\toffset = entry->location.rva + sizeof (memory64_list);\n-\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges; i++) {\n+\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges && offset < obj->size; i++) {\n \t\t\tstruct minidump_memory_descriptor64 *desc = R_NEW (struct minidump_memory_descriptor64);\n \t\t\tif (!desc) {\n \t\t\t\tbreak;\n@@ -291,7 +290,7 @@\n \t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n \n \t\toffset = entry->location.rva + sizeof (unloaded_module_list);\n-\t\tfor (i = 0; i < unloaded_module_list.number_of_entries; i++) {\n+\t\tfor (i = 0; i < unloaded_module_list.number_of_entries && offset < obj->size; i++) {\n \t\t\tstruct minidump_unloaded_module *module = R_NEW (struct minidump_unloaded_module);\n \t\t\tif (!module) {\n \t\t\t\tbreak;\n@@ -346,7 +345,7 @@\n \t\t\t0);\n \n \t\toffset = entry->location.rva + sizeof (memory_info_list);\n-\t\tfor (i = 0; i < memory_info_list.number_of_entries; i++) {\n+\t\tfor (i = 0; i < memory_info_list.number_of_entries && offset < obj->size; i++) {\n \t\t\tstruct minidump_memory_info *info = R_NEW (struct minidump_memory_info);\n \t\t\tif (!info) {\n \t\t\t\tbreak;\n@@ -376,7 +375,7 @@\n \t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries\", 0);\n \n \t\toffset = entry->location.rva + sizeof (thread_info_list);\n-\t\tfor (i = 0; i < thread_info_list.number_of_entries; i++) {\n+\t\tfor (i = 0; i < thread_info_list.number_of_entries && offset < obj->size; i++) {\n \t\t\tstruct minidump_thread_info *info = R_NEW (struct minidump_thread_info);\n \t\t\tif (!info) {\n \t\t\t\tbreak;\n@@ -402,7 +401,7 @@\n \t\t\t\"SizeOfHeader SizeOfEntry NumberOfEntries Reserved\", 0);\n \n \t\toffset = entry->location.rva + sizeof (handle_operation_list);\n-\t\tfor (i = 0; i < handle_operation_list.number_of_entries; i++) {\n+\t\tfor (i = 0; i < handle_operation_list.number_of_entries && offset < obj->size; i++) {\n \t\t\tstruct avrf_handle_operation *op = R_NEW (struct avrf_handle_operation);\n \t\t\tif (!op) {\n \t\t\t\tbreak;\n@@ -432,7 +431,7 @@\n \t\t\t\"TokenListSize TokenListEntries ListHeaderSize ElementHeaderSize\", 0);\n \n \t\toffset = entry->location.rva + sizeof (token_info_list);\n-\t\tfor (i = 0; i < token_info_list.number_of_entries; i++) {\n+\t\tfor (i = 0; i < token_info_list.number_of_entries && offset < obj->size; i++) {\n \t\t\tstruct minidump_token_info *info = R_NEW (struct minidump_token_info);\n \t\t\tif (!info) {\n \t\t\t\tbreak;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tfor (i = 0; i < module_list.number_of_modules; i++) {",
                "\t\t\tstruct minidump_module *module = R_NEW (struct minidump_module);",
                "\t\t\t\tbreak;",
                "\t\t\t}",
                "\t\t\tread_module (obj->b, offset, module);",
                "\t\tfor (i = 0; i < memory_list.number_of_memory_ranges; i++) {",
                "\t\tfor (i = 0; i < thread_ex_list.number_of_threads; i++) {",
                "\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges; i++) {",
                "\t\tfor (i = 0; i < unloaded_module_list.number_of_entries; i++) {",
                "\t\tfor (i = 0; i < memory_info_list.number_of_entries; i++) {",
                "\t\tfor (i = 0; i < thread_info_list.number_of_entries; i++) {",
                "\t\tfor (i = 0; i < handle_operation_list.number_of_entries; i++) {",
                "\t\tfor (i = 0; i < token_info_list.number_of_entries; i++) {"
            ],
            "added_lines": [
                "\t\tfor (i = 0; i < module_list.number_of_modules && offset < obj->size; i++) {",
                "\t\t\tstruct minidump_module *module = read_module (obj->b, offset);",
                "\t\t\t\tbreak;\t",
                "\t\t\t}",
                "\t\tfor (i = 0; i < memory_list.number_of_memory_ranges && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < thread_ex_list.number_of_threads && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < memory64_list.number_of_memory_ranges && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < unloaded_module_list.number_of_entries && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < memory_info_list.number_of_entries && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < thread_info_list.number_of_entries && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < handle_operation_list.number_of_entries && offset < obj->size; i++) {",
                "\t\tfor (i = 0; i < token_info_list.number_of_entries && offset < obj->size; i++) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/estimate_slide",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static ut64 estimate_slide(RBinFile *bf, RDyldCache *cache, ut64 value_mask, ut64 value_add) {\n\tut64 slide = 0;\n\tif (cache->n_hdr > 1) {\n\t\treturn slide;\n\t}\n\tut64 *classlist = malloc (64);\n\tif (!classlist) {\n\t\tgoto beach;\n\t}\n\n\tRListIter *iter;\n\tRDyldBinImage *bin;\n\tr_list_foreach (cache->bins, iter, bin) {\n\t\tbool found_sample = false;\n\n\t\tstruct MACH0_(opts_t) opts = {0};\n\t\topts.verbose = bf->rbin->verbose;\n\t\topts.header_at = bin->header_at;\n\t\topts.symbols_off = 0;\n\n\t\tstruct MACH0_(obj_t) *mach0 = MACH0_(new_buf) (cache->buf, &opts);\n\t\tif (!mach0) {\n\t\t\tgoto beach;\n\t\t}\n\n\t\tstruct section_t *sections = NULL;\n\t\tif (!(sections = MACH0_(get_sections) (mach0))) {\n\t\t\tMACH0_(mach0_free) (mach0);\n\t\t\tgoto beach;\n\t\t}\n\n\t\tint i;\n\t\tint incomplete = 2;\n\t\tint classlist_idx = 0, data_idx = 0;\n\t\tfor (i = 0; !sections[i].last && incomplete; i++) {\n\t\t\tif (sections[i].size == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (strstr (sections[i].name, \"__objc_classlist\")) {\n\t\t\t\tincomplete--;\n\t\t\t\tclasslist_idx = i;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (strstr (sections[i].name, \"__objc_data\")) {\n\t\t\t\tincomplete--;\n\t\t\t\tdata_idx = i;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (incomplete) {\n\t\t\tgoto next_bin;\n\t\t}\n\n\t\tint classlist_sample_size = R_MIN (64, sections[classlist_idx].size);\n\t\tint n_classes = classlist_sample_size / 8;\n\t\tut64 sect_offset = sections[classlist_idx].offset + bin->hdr_offset;\n\n\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) < classlist_sample_size) {\n\t\t\tgoto next_bin;\n\t\t}\n\n\t\tut64 data_addr = sections[data_idx].addr;\n\t\tut64 data_tail = data_addr & 0xfff;\n\t\tut64 data_tail_end = (data_addr + sections[data_idx].size) & 0xfff;\n\t\tfor (i = 0; i < n_classes; i++) {\n\t\t\tut64 cl_addr = (classlist[i] & value_mask) + value_add;\n\t\t\tut64 cl_tail = cl_addr & 0xfff;\n\t\t\tif (cl_tail >= data_tail && cl_tail < data_tail_end) {\n\t\t\t\tut64 off = cl_tail - data_tail;\n\t\t\t\tslide = ((cl_addr - off) & value_mask) - (data_addr & value_mask);\n\t\t\t\tfound_sample = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nnext_bin:\n\t\tMACH0_(mach0_free) (mach0);\n\t\tR_FREE (sections);\n\n\t\tif (found_sample) {\n\t\t\tbreak;\n\t\t}\n\t}\n\nbeach:\n\tR_FREE (classlist);\n\treturn slide;\n}",
        "func": "static ut64 estimate_slide(RBinFile *bf, RDyldCache *cache, ut64 value_mask, ut64 value_add) {\n\tut64 slide = 0;\n\tif (cache->n_hdr > 1) {\n\t\treturn slide;\n\t}\n\tut64 *classlist = malloc (64);\n\tif (!classlist) {\n\t\tgoto beach;\n\t}\n\n\tRListIter *iter;\n\tRDyldBinImage *bin;\n\tr_list_foreach (cache->bins, iter, bin) {\n\t\tbool found_sample = false;\n\n\t\tstruct MACH0_(opts_t) opts = {0};\n\t\topts.verbose = bf->rbin->verbose;\n\t\topts.header_at = bin->header_at;\n\t\topts.symbols_off = 0;\n\n\t\tstruct MACH0_(obj_t) *mach0 = MACH0_(new_buf) (cache->buf, &opts);\n\t\tif (!mach0) {\n\t\t\tgoto beach;\n\t\t}\n\n\t\tstruct section_t *sections = NULL;\n\t\tif (!(sections = MACH0_(get_sections) (mach0))) {\n\t\t\tMACH0_(mach0_free) (mach0);\n\t\t\tgoto beach;\n\t\t}\n\n\t\tint i;\n\t\tint incomplete = 2;\n\t\tint classlist_idx = 0, data_idx = 0;\n\t\tfor (i = 0; !sections[i].last && incomplete; i++) {\n\t\t\tif (sections[i].size == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (strstr (sections[i].name, \"__objc_classlist\")) {\n\t\t\t\tincomplete--;\n\t\t\t\tclasslist_idx = i;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (strstr (sections[i].name, \"__objc_data\")) {\n\t\t\t\tincomplete--;\n\t\t\t\tdata_idx = i;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (incomplete) {\n\t\t\tgoto next_bin;\n\t\t}\n\n\t\tint classlist_sample_size = R_MIN (64, sections[classlist_idx].size);\n\t\tint n_classes = classlist_sample_size / 8;\n\t\tut64 sect_offset = sections[classlist_idx].offset + bin->hdr_offset;\n\n\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) != classlist_sample_size) {\n\t\t\tgoto next_bin;\n\t\t}\n\n\t\tut64 data_addr = sections[data_idx].addr;\n\t\tut64 data_tail = data_addr & 0xfff;\n\t\tut64 data_tail_end = (data_addr + sections[data_idx].size) & 0xfff;\n\t\tfor (i = 0; i < n_classes; i++) {\n\t\t\tut64 cl_addr = (classlist[i] & value_mask) + value_add;\n\t\t\tut64 cl_tail = cl_addr & 0xfff;\n\t\t\tif (cl_tail >= data_tail && cl_tail < data_tail_end) {\n\t\t\t\tut64 off = cl_tail - data_tail;\n\t\t\t\tslide = ((cl_addr - off) & value_mask) - (data_addr & value_mask);\n\t\t\t\tfound_sample = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\nnext_bin:\n\t\tMACH0_(mach0_free) (mach0);\n\t\tR_FREE (sections);\n\n\t\tif (found_sample) {\n\t\t\tbreak;\n\t\t}\n\t}\n\nbeach:\n\tR_FREE (classlist);\n\treturn slide;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,7 +56,7 @@\n \t\tint n_classes = classlist_sample_size / 8;\n \t\tut64 sect_offset = sections[classlist_idx].offset + bin->hdr_offset;\n \n-\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) < classlist_sample_size) {\n+\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) != classlist_sample_size) {\n \t\t\tgoto next_bin;\n \t\t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) < classlist_sample_size) {"
            ],
            "added_lines": [
                "\t\tif (r_buf_fread_at (cache->buf, sect_offset, (ut8*) classlist, \"l\", n_classes) != classlist_sample_size) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/bin_pe_parse_imports",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static int bin_pe_parse_imports(RBinPEObj* pe,\n                                struct r_bin_pe_import_t** importp, int* nimp,\n                                const char* dll_name,\n                                PE_DWord OriginalFirstThunk,\n                                PE_DWord FirstThunk) {\n\tchar import_name[PE_NAME_LENGTH + 1];\n\tchar name[PE_NAME_LENGTH + 1];\n\tPE_Word import_hint, import_ordinal = 0;\n\tPE_DWord import_table = 0, off = 0;\n\tint i = 0, len;\n\tSdb* db = NULL;\n\tchar* sdb_module = NULL;\n\tchar* symname = NULL;\n\tchar* symdllname = NULL;\n\n\tif (!dll_name || !*dll_name || *dll_name == '0') {\n\t\treturn 0;\n\t}\n\n\tif (!(off = PE_(va2pa) (pe, OriginalFirstThunk)) &&\n\t!(off = PE_(va2pa) (pe, FirstThunk))) {\n\t\treturn 0;\n\t}\n\tdo {\n\t\tif (import_ordinal >= UT16_MAX) {\n\t\t\tbreak;\n\t\t}\n\t\tif (off + i * sizeof (PE_DWord) > pe->size) {\n\t\t\tbreak;\n\t\t}\n\t\timport_table = R_BUF_READ_PE_DWORD_AT (pe->b, off + i * sizeof (PE_DWord));\n\t\tif (import_table == PE_DWORD_MAX) {\n\t\t\tpe_printf (\"Warning: read (import table)\\n\");\n\t\t\tgoto error;\n\t\t} else if (import_table) {\n\t\t\tif (import_table & ILT_MASK1) {\n\t\t\t\timport_ordinal = import_table & ILT_MASK2;\n\t\t\t\timport_hint = 0;\n\t\t\t\tsnprintf (import_name, PE_NAME_LENGTH, \"Ordinal_%i\", import_ordinal);\n\t\t\t\tfree (symdllname);\n\t\t\t\tstrncpy (name, dll_name, sizeof (name) - 1);\n\t\t\t\tname[sizeof (name) - 1] = 0;\n\t\t\t\tsymdllname = strdup (name);\n\n\t\t\t\t// remove the trailling \".dll\"\n\t\t\t\tsize_t len = strlen (symdllname);\n\t\t\t\tr_str_case (symdllname, 0);\n\t\t\t\tlen = len < 4? 0: len - 4;\n\t\t\t\tsymdllname[len] = 0;\n\n\t\t\t\tchar* filename = NULL;\n\t\t\t\tif (!sdb_module || strcmp (symdllname, sdb_module)) {\n\t\t\t\t\tsdb_free (db);\n\t\t\t\t\tdb = NULL;\n\t\t\t\t\tfree (sdb_module);\n\t\t\t\t\tsdb_module = strdup (symdllname);\n\t\t\t\t\tfilename = r_str_newf (\"%s.sdb\", symdllname);\n\t\t\t\t\tif (filename && r_file_exists (filename)) {\n\t\t\t\t\t\tdb = sdb_new (NULL, filename, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tconst char *dirPrefix = r_sys_prefix (NULL);\n\t\t\t\t\t\tchar *lower_symdllname = strdup (symdllname);\n\t\t\t\t\t\tr_str_case (lower_symdllname, false);\n\t\t\t\t\t\tfilename = r_str_newf (R_JOIN_4_PATHS (\"%s\", R2_SDB_FORMAT, \"dll\", \"%s.sdb\"),\n\t\t\t\t\t\t\tdirPrefix, lower_symdllname);\n\t\t\t\t\t\tfree (lower_symdllname);\n\t\t\t\t\t\tif (r_file_exists (filename)) {\n\t\t\t\t\t\t\tdb = sdb_new (NULL, filename, 0);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (db) {\n\t\t\t\t\tsymname = resolveModuleOrdinal (db, symdllname, import_ordinal);\n\t\t\t\t\tif (symname) {\n\t\t\t\t\t\tsnprintf (import_name, PE_NAME_LENGTH, \"%s\", symname);\n\t\t\t\t\t\tR_FREE (symname);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tpe_printf (\"Cannot find %s\\n\", filename);\n\t\t\t\t}\n\t\t\t\tfree (filename);\n\t\t\t} else {\n\t\t\t\timport_ordinal++;\n\t\t\t\tconst ut64 off = PE_(va2pa) (pe, import_table);\n\t\t\t\tif (off > pe->size || (off + sizeof (PE_Word)) > pe->size) {\n\t\t\t\t\tpe_printf (\"Warning: off > pe->size\\n\");\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\timport_hint = r_buf_read_le16_at (pe->b, off);\n\t\t\t\tif (import_hint == UT16_MAX) {\n\t\t\t\t\tpe_printf (\"Warning: read import hint at 0x%08\"PFMT64x \"\\n\", off);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tname[0] = '\\0';\n\t\t\t\tlen = r_buf_read_at (pe->b, off + sizeof (PE_Word), (ut8*) name, PE_NAME_LENGTH);\n\t\t\t\tif (len < 1) {\n\t\t\t\t\tpe_printf (\"Warning: read (import name)\\n\");\n\t\t\t\t\tgoto error;\n\t\t\t\t} else if (!*name) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tname[PE_NAME_LENGTH] = '\\0';\n\t\t\t\tint len = snprintf (import_name, sizeof (import_name), \"%s\" , name);\n\t\t\t\tif (len >= sizeof (import_name)) {\n\t\t\t\t\teprintf (\"Import name '%s' has been truncated.\\n\", import_name);\n\t\t\t\t}\n\t\t\t}\n\t\t\tstruct r_bin_pe_import_t *new_importp = realloc (*importp, (*nimp + 1) * sizeof (struct r_bin_pe_import_t));\n\t\t\tif (!new_importp) {\n\t\t\t\tr_sys_perror (\"realloc (import)\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t*importp = new_importp;\n\t\t\tmemcpy ((*importp)[*nimp].name, import_name, PE_NAME_LENGTH);\n\t\t\t(*importp)[*nimp].name[PE_NAME_LENGTH] = '\\0';\n\t\t\tmemcpy ((*importp)[*nimp].libname, dll_name, PE_NAME_LENGTH);\n\t\t\t(*importp)[*nimp].libname[PE_NAME_LENGTH] = '\\0';\n\t\t\t(*importp)[*nimp].vaddr = bin_pe_rva_to_va (pe, FirstThunk + i * sizeof (PE_DWord));\n\t\t\t(*importp)[*nimp].paddr = PE_(va2pa) (pe, FirstThunk) + i * sizeof (PE_DWord);\n\t\t\t(*importp)[*nimp].hint = import_hint;\n\t\t\t(*importp)[*nimp].ordinal = import_ordinal;\n\t\t\t(*importp)[*nimp].last = 0;\n\t\t\t(*nimp)++;\n\t\t\ti++;\n\t\t}\n\t} while (import_table);\n\n\tif (db) {\n\t\tsdb_free (db);\n\t\tdb = NULL;\n\t}\n\tfree (symdllname);\n\tfree (sdb_module);\n\treturn i;\n\nerror:\n\tif (db) {\n\t\tsdb_free (db);\n\t\tdb = NULL;\n\t}\n\tfree (symdllname);\n\tfree (sdb_module);\n\treturn false;\n}",
        "func": "static int bin_pe_parse_imports(RBinPEObj* pe,\n                                struct r_bin_pe_import_t** importp, int* nimp,\n                                const char* dll_name,\n                                PE_DWord OriginalFirstThunk,\n                                PE_DWord FirstThunk) {\n\tchar import_name[PE_NAME_LENGTH + 1];\n\tchar name[PE_NAME_LENGTH + 1];\n\tPE_Word import_hint, import_ordinal = 0;\n\tPE_DWord import_table = 0, off = 0;\n\tint i = 0, len;\n\tSdb* db = NULL;\n\tchar* sdb_module = NULL;\n\tchar* symname = NULL;\n\tchar* symdllname = NULL;\n\n\tif (!dll_name || !*dll_name || *dll_name == '0') {\n\t\treturn 0;\n\t}\n\n\tif (!(off = PE_(va2pa) (pe, OriginalFirstThunk)) &&\n\t!(off = PE_(va2pa) (pe, FirstThunk))) {\n\t\treturn 0;\n\t}\n\tdo {\n\t\tif (import_ordinal >= UT16_MAX) {\n\t\t\tbreak;\n\t\t}\n\t\tif (off + i * sizeof (PE_DWord) > pe->size) {\n\t\t\tbreak;\n\t\t}\n\t\timport_table = R_BUF_READ_PE_DWORD_AT (pe->b, off + i * sizeof (PE_DWord));\n\t\tif (import_table == PE_DWORD_MAX) {\n\t\t\tpe_printf (\"Warning: read (import table)\\n\");\n\t\t\tgoto error;\n\t\t} else if (import_table) {\n\t\t\tif (import_table & ILT_MASK1) {\n\t\t\t\timport_ordinal = import_table & ILT_MASK2;\n\t\t\t\timport_hint = 0;\n\t\t\t\tsnprintf (import_name, PE_NAME_LENGTH, \"Ordinal_%i\", import_ordinal);\n\t\t\t\tfree (symdllname);\n\t\t\t\tstrncpy (name, dll_name, sizeof (name) - 1);\n\t\t\t\tname[sizeof (name) - 1] = 0;\n\t\t\t\tsymdllname = strdup (name);\n\n\t\t\t\t// remove the trailling \".dll\"\n\t\t\t\tsize_t len = strlen (symdllname);\n\t\t\t\tr_str_case (symdllname, 0);\n\t\t\t\tlen = len < 4? 0: len - 4;\n\t\t\t\tsymdllname[len] = 0;\n\n\t\t\t\tchar* filename = NULL;\n\t\t\t\tif (!sdb_module || strcmp (symdllname, sdb_module)) {\n\t\t\t\t\tsdb_free (db);\n\t\t\t\t\tdb = NULL;\n\t\t\t\t\tfree (sdb_module);\n\t\t\t\t\tsdb_module = strdup (symdllname);\n\t\t\t\t\tfilename = r_str_newf (\"%s.sdb\", symdllname);\n\t\t\t\t\tif (filename && r_file_exists (filename)) {\n\t\t\t\t\t\tdb = sdb_new (NULL, filename, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tconst char *dirPrefix = r_sys_prefix (NULL);\n\t\t\t\t\t\tchar *lower_symdllname = strdup (symdllname);\n\t\t\t\t\t\tr_str_case (lower_symdllname, false);\n\t\t\t\t\t\tfilename = r_str_newf (R_JOIN_4_PATHS (\"%s\", R2_SDB_FORMAT, \"dll\", \"%s.sdb\"),\n\t\t\t\t\t\t\tdirPrefix, lower_symdllname);\n\t\t\t\t\t\tfree (lower_symdllname);\n\t\t\t\t\t\tif (r_file_exists (filename)) {\n\t\t\t\t\t\t\tdb = sdb_new (NULL, filename, 0);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (db) {\n\t\t\t\t\tsymname = resolveModuleOrdinal (db, symdllname, import_ordinal);\n\t\t\t\t\tif (symname) {\n\t\t\t\t\t\tsnprintf (import_name, PE_NAME_LENGTH, \"%s\", symname);\n\t\t\t\t\t\tR_FREE (symname);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tpe_printf (\"Cannot find %s\\n\", filename);\n\t\t\t\t}\n\t\t\t\tfree (filename);\n\t\t\t} else {\n\t\t\t\timport_ordinal++;\n\t\t\t\tconst ut64 off = PE_(va2pa) (pe, import_table);\n\t\t\t\tif (off > pe->size || (off + sizeof (PE_Word)) > pe->size) {\n\t\t\t\t\tpe_printf (\"Warning: off > pe->size\\n\");\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\timport_hint = r_buf_read_le16_at (pe->b, off);\n\t\t\t\tif (import_hint == UT16_MAX) {\n\t\t\t\t\tpe_printf (\"Warning: read import hint at 0x%08\"PFMT64x \"\\n\", off);\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tname[0] = '\\0';\n\t\t\t\tlen = r_buf_read_at (pe->b, off + sizeof (PE_Word), (ut8*) name, PE_NAME_LENGTH);\n\t\t\t\tif (len < 1) {\n\t\t\t\t\tpe_printf (\"Warning: read (import name)\\n\");\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tif (!*name) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tname[PE_NAME_LENGTH] = '\\0';\n\t\t\t\tint len = snprintf (import_name, sizeof (import_name), \"%s\" , name);\n\t\t\t\tif (len >= sizeof (import_name)) {\n\t\t\t\t\teprintf (\"Import name '%s' has been truncated.\\n\", import_name);\n\t\t\t\t}\n\t\t\t}\n\t\t\tstruct r_bin_pe_import_t *new_importp = realloc (*importp, (*nimp + 1) * sizeof (struct r_bin_pe_import_t));\n\t\t\tif (!new_importp) {\n\t\t\t\tr_sys_perror (\"realloc (import)\");\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\t*importp = new_importp;\n\t\t\tmemcpy ((*importp)[*nimp].name, import_name, PE_NAME_LENGTH);\n\t\t\t(*importp)[*nimp].name[PE_NAME_LENGTH] = '\\0';\n\t\t\tmemcpy ((*importp)[*nimp].libname, dll_name, PE_NAME_LENGTH);\n\t\t\t(*importp)[*nimp].libname[PE_NAME_LENGTH] = '\\0';\n\t\t\t(*importp)[*nimp].vaddr = bin_pe_rva_to_va (pe, FirstThunk + i * sizeof (PE_DWord));\n\t\t\t(*importp)[*nimp].paddr = PE_(va2pa) (pe, FirstThunk) + i * sizeof (PE_DWord);\n\t\t\t(*importp)[*nimp].hint = import_hint;\n\t\t\t(*importp)[*nimp].ordinal = import_ordinal;\n\t\t\t(*importp)[*nimp].last = 0;\n\t\t\t(*nimp)++;\n\t\t\ti++;\n\t\t}\n\t} while (import_table);\n\n\tif (db) {\n\t\tsdb_free (db);\n\t\tdb = NULL;\n\t}\n\tfree (symdllname);\n\tfree (sdb_module);\n\treturn i;\n\nerror:\n\tif (db) {\n\t\tsdb_free (db);\n\t\tdb = NULL;\n\t}\n\tfree (symdllname);\n\tfree (sdb_module);\n\treturn false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -96,7 +96,8 @@\n \t\t\t\tif (len < 1) {\n \t\t\t\t\tpe_printf (\"Warning: read (import name)\\n\");\n \t\t\t\t\tgoto error;\n-\t\t\t\t} else if (!*name) {\n+\t\t\t\t}\n+\t\t\t\tif (!*name) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\t\tname[PE_NAME_LENGTH] = '\\0';",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t} else if (!*name) {"
            ],
            "added_lines": [
                "\t\t\t\t}",
                "\t\t\t\tif (!*name) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/bin_pe_init_metadata_hdr",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static bool bin_pe_init_metadata_hdr(RBinPEObj* pe) {\n\tPE_(image_metadata_header) * metadata = R_NEW0 (PE_(image_metadata_header));\n\tif (!metadata) {\n\t\treturn false;\n\t}\n\tPE_DWord metadata_directory = pe->clr_hdr? PE_(va2pa) (pe, pe->clr_hdr->MetaDataDirectoryAddress): 0;\n\tif (!metadata_directory) {\n\t\tfree (metadata);\n\t\treturn false;\n\t}\n\tint rr = r_buf_fread_at (pe->b, metadata_directory,\n\t\t(ut8*) metadata, pe->big_endian? \"1I2S\": \"1i2s\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\trr = r_buf_fread_at (pe->b, metadata_directory + 8,\n\t\t(ut8*) (&metadata->Reserved), pe->big_endian? \"1I\": \"1i\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\trr = r_buf_fread_at (pe->b, metadata_directory + 12,\n\t\t(ut8*) (&metadata->VersionStringLength), pe->big_endian? \"1I\": \"1i\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\teprintf (\"Metadata Signature: 0x%\"PFMT64x\" 0x%\"PFMT64x\" %d\\n\",\n\t\t(ut64)metadata_directory, (ut64)metadata->Signature, (int)metadata->VersionStringLength);\n\n\t// read the version string\n\tint len = metadata->VersionStringLength; // XXX: dont trust this length\n\tif (len > 0) {\n\t\tmetadata->VersionString = calloc (1, len + 1);\n\t\tif (!metadata->VersionString) {\n\t\t\tgoto fail;\n\t\t}\n\n\t\trr = r_buf_read_at (pe->b, metadata_directory + 16, (ut8*)(metadata->VersionString), len);\n\t\tif (rr != len) {\n\t\t\teprintf (\"Warning: read (metadata header) - cannot parse version string\\n\");\n\t\t\tfree (metadata->VersionString);\n\t\t\tfree (metadata);\n\t\t\treturn 0;\n\t\t}\n\t\teprintf (\".NET Version: %s\\n\", metadata->VersionString);\n\t}\n\n\t// read the header after the string\n\trr = r_buf_fread_at (pe->b, metadata_directory + 16 + metadata->VersionStringLength,\n\t\t(ut8*) (&metadata->Flags), pe->big_endian? \"2S\": \"2s\", 1);\n\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\teprintf (\"Number of Metadata Streams: %d\\n\", metadata->NumberOfStreams);\n\tpe->metadata_header = metadata;\n\n\n\t// read metadata streams\n\tint stream_addr = metadata_directory + 20 + metadata->VersionStringLength;\n\tPE_(image_metadata_stream) * stream;\n\tPE_(image_metadata_stream) **streams = calloc (sizeof (PE_(image_metadata_stream)*), metadata->NumberOfStreams);\n\tif (!streams) {\n\t\tgoto fail;\n\t}\n\tint count;\n\tfor (count = 0; count < metadata->NumberOfStreams; count++) {\n\t\tstream = R_NEW0 (PE_(image_metadata_stream));\n\t\tif (!stream) {\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (r_buf_size (pe->b) < (stream_addr + 8 + MAX_METADATA_STRING_LENGTH)) {\n\t\t\teprintf (\"Truncated\\n\");\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (r_buf_fread_at (pe->b, stream_addr, (ut8*) stream, pe->big_endian? \"2I\": \"2i\", 1) < 1) {\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\teprintf (\"DirectoryAddress: %x Size: %x\\n\", stream->Offset, stream->Size);\n\t\tchar* stream_name = calloc (1, MAX_METADATA_STRING_LENGTH + 1);\n\n\t\tif (!stream_name) {\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tint c = bin_pe_read_metadata_string (stream_name, pe->b, stream_addr + 8);\n\t\tif (c == 0) {\n\t\t\tfree (stream_name);\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\teprintf (\"Stream name: %s %d\\n\", stream_name, c);\n\t\tstream->Name = stream_name;\n\t\tstreams[count] = stream;\n\t\tstream_addr += 8 + c;\n\t}\n\tpe->streams = streams;\n\treturn true;\nfail:\n\teprintf (\"Warning: read (metadata header)\\n\");\n\tfree (metadata);\n\treturn false;\n}",
        "func": "static bool bin_pe_init_metadata_hdr(RBinPEObj* pe) {\n\tPE_(image_metadata_header) * metadata = R_NEW0 (PE_(image_metadata_header));\n\tif (!metadata) {\n\t\treturn false;\n\t}\n\tPE_DWord metadata_directory = pe->clr_hdr? PE_(va2pa) (pe, pe->clr_hdr->MetaDataDirectoryAddress): 0;\n\tif (!metadata_directory) {\n\t\tfree (metadata);\n\t\treturn false;\n\t}\n\tint rr = r_buf_fread_at (pe->b, metadata_directory,\n\t\t(ut8*) metadata, pe->big_endian? \"1I2S\": \"1i2s\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\trr = r_buf_fread_at (pe->b, metadata_directory + 8,\n\t\t(ut8*) (&metadata->Reserved), pe->big_endian? \"1I\": \"1i\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\trr = r_buf_fread_at (pe->b, metadata_directory + 12,\n\t\t(ut8*) (&metadata->VersionStringLength), pe->big_endian? \"1I\": \"1i\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\teprintf (\"Metadata Signature: 0x%\"PFMT64x\" 0x%\"PFMT64x\" %d\\n\",\n\t\t(ut64)metadata_directory, (ut64)metadata->Signature, (int)metadata->VersionStringLength);\n\n\t// read the version string\n\tint len = metadata->VersionStringLength; // XXX: dont trust this length\n\tif (len > 0) {\n\t\tmetadata->VersionString = calloc (1, len + 1);\n\t\tif (!metadata->VersionString) {\n\t\t\tgoto fail;\n\t\t}\n\n\t\trr = r_buf_read_at (pe->b, metadata_directory + 16, (ut8*)(metadata->VersionString), len);\n\t\tif (rr != len) {\n\t\t\teprintf (\"Warning: read (metadata header) - cannot parse version string\\n\");\n\t\t\tfree (metadata->VersionString);\n\t\t\tfree (metadata);\n\t\t\treturn 0;\n\t\t}\n\t\teprintf (\".NET Version: %s\\n\", metadata->VersionString);\n\t}\n\n\t// read the header after the string\n\trr = r_buf_fread_at (pe->b, metadata_directory + 16 + metadata->VersionStringLength,\n\t\t(ut8*) (&metadata->Flags), pe->big_endian? \"2S\": \"2s\", 1);\n\tif (rr < 1) {\n\t\tgoto fail;\n\t}\n\n\teprintf (\"Number of Metadata Streams: %d\\n\", metadata->NumberOfStreams);\n\tpe->metadata_header = metadata;\n\n\n\t// read metadata streams\n\tint stream_addr = metadata_directory + 20 + metadata->VersionStringLength;\n\tPE_(image_metadata_stream) * stream;\n\tPE_(image_metadata_stream) **streams = calloc (sizeof (PE_(image_metadata_stream)*), metadata->NumberOfStreams);\n\tif (!streams) {\n\t\tgoto fail;\n\t}\n\tint count;\n\tfor (count = 0; count < metadata->NumberOfStreams; count++) {\n\t\tstream = R_NEW0 (PE_(image_metadata_stream));\n\t\tif (!stream) {\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (r_buf_size (pe->b) < (stream_addr + 8 + MAX_METADATA_STRING_LENGTH)) {\n\t\t\teprintf (\"Truncated\\n\");\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (r_buf_fread_at (pe->b, stream_addr, (ut8*) stream, pe->big_endian? \"2I\": \"2i\", 1) < 1) {\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\teprintf (\"DirectoryAddress: %x Size: %x\\n\", stream->Offset, stream->Size);\n\t\tchar* stream_name = calloc (1, MAX_METADATA_STRING_LENGTH + 1);\n\n\t\tif (!stream_name) {\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tint c = bin_pe_read_metadata_string (stream_name, pe->b, stream_addr + 8);\n\t\tif (c == 0) {\n\t\t\tfree (stream_name);\n\t\t\tfree (stream);\n\t\t\tfree (streams);\n\t\t\tgoto fail;\n\t\t}\n\t\teprintf (\"Stream name: %s %d\\n\", stream_name, c);\n\t\tstream->Name = stream_name;\n\t\tstreams[count] = stream;\n\t\tstream_addr += 8 + c;\n\t}\n\tpe->streams = streams;\n\treturn true;\nfail:\n\teprintf (\"Warning: read (metadata header)\\n\");\n\tfree (metadata);\n\treturn false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,7 +50,6 @@\n \t// read the header after the string\n \trr = r_buf_fread_at (pe->b, metadata_directory + 16 + metadata->VersionStringLength,\n \t\t(ut8*) (&metadata->Flags), pe->big_endian? \"2S\": \"2s\", 1);\n-\n \tif (rr < 1) {\n \t\tgoto fail;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/Pe_r_bin_pe_parse_var",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static Var* Pe_r_bin_pe_parse_var(RBinPEObj* pe, PE_DWord* curAddr) {\n\tVar* var = calloc (1, sizeof (*var));\n\tif (!var) {\n\t\tpe_printf (\"Warning: calloc (Var)\\n\");\n\t\treturn NULL;\n\t}\n\tif ((var->wLength = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wLength)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wLength);\n\tif ((var->wValueLength = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wValueLength)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wValueLength);\n\tif ((var->wType = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wType)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wType);\n\tif (var->wType != 0 && var->wType != 1) {\n\t\tpe_printf (\"Warning: check (Var wType)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\n\tvar->szKey = (ut16*) malloc (UT16_ALIGN (TRANSLATION_UTF_16_LEN));  //L\"Translation\"\n\tif (!var->szKey) {\n\t\tpe_printf (\"Warning: malloc (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) < 1) {\n\t\tpe_printf (\"Warning: read (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += TRANSLATION_UTF_16_LEN;\n\tif (memcmp (var->szKey, TRANSLATION_UTF_16, TRANSLATION_UTF_16_LEN)) {\n\t\tpe_printf (\"Warning: check (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\talign32 (*curAddr);\n\tvar->numOfValues = var->wValueLength / 4;\n\tif (!var->numOfValues) {\n\t\tpe_printf (\"Warning: check (Var numOfValues)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tvar->Value = (ut32*) malloc (var->wValueLength);\n\tif (!var->Value) {\n\t\tpe_printf (\"Warning: malloc (Var Value)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->Value, var->wValueLength) != var->wValueLength) {\n\t\tpe_printf (\"Warning: read (Var Value)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += var->wValueLength;\n\treturn var;\n}",
        "func": "static Var* Pe_r_bin_pe_parse_var(RBinPEObj* pe, PE_DWord* curAddr) {\n\tVar* var = calloc (1, sizeof (*var));\n\tif (!var) {\n\t\tpe_printf (\"Warning: calloc (Var)\\n\");\n\t\treturn NULL;\n\t}\n\tif ((var->wLength = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wLength)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wLength);\n\tif ((var->wValueLength = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wValueLength)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wValueLength);\n\tif ((var->wType = r_buf_read_le16_at (pe->b, *curAddr)) == UT16_MAX) {\n\t\tpe_printf (\"Warning: read (Var wType)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += sizeof (var->wType);\n\tif (var->wType != 0 && var->wType != 1) {\n\t\tpe_printf (\"Warning: check (Var wType)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\n\tvar->szKey = (ut16*) malloc (UT16_ALIGN (TRANSLATION_UTF_16_LEN));  //L\"Translation\"\n\tif (!var->szKey) {\n\t\tpe_printf (\"Warning: malloc (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) != TRANSLATION_UTF_16_LEN) {\n\t\tpe_printf (\"Warning: read (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += TRANSLATION_UTF_16_LEN;\n\tif (memcmp (var->szKey, TRANSLATION_UTF_16, TRANSLATION_UTF_16_LEN)) {\n\t\tpe_printf (\"Warning: check (Var szKey)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\talign32 (*curAddr);\n\tvar->numOfValues = var->wValueLength / 4;\n\tif (!var->numOfValues) {\n\t\tpe_printf (\"Warning: check (Var numOfValues)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tvar->Value = (ut32*) malloc (var->wValueLength);\n\tif (!var->Value) {\n\t\tpe_printf (\"Warning: malloc (Var Value)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->Value, var->wValueLength) != var->wValueLength) {\n\t\tpe_printf (\"Warning: read (Var Value)\\n\");\n\t\tfree_Var (var);\n\t\treturn NULL;\n\t}\n\t*curAddr += var->wValueLength;\n\treturn var;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -34,7 +34,7 @@\n \t\tfree_Var (var);\n \t\treturn NULL;\n \t}\n-\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) < 1) {\n+\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) != TRANSLATION_UTF_16_LEN) {\n \t\tpe_printf (\"Warning: read (Var szKey)\\n\");\n \t\tfree_Var (var);\n \t\treturn NULL;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) < 1) {"
            ],
            "added_lines": [
                "\tif (r_buf_read_at (pe->b, *curAddr, (ut8*) var->szKey, TRANSLATION_UTF_16_LEN) != TRANSLATION_UTF_16_LEN) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/r_buf_fread_at",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "R_API st64 r_buf_fread_at(RBuffer *b, ut64 addr, ut8 *buf, const char *fmt, int n) {\n\tr_return_val_if_fail (b && buf && fmt, -1);\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tst64 r = r_buf_seek (b, addr, R_BUF_SET);\n\tif (r < 0) {\n\t\treturn r;\n\t}\n\tr = r_buf_fread (b, buf, fmt, n);\n\tr_buf_seek (b, o_addr, R_BUF_SET);\n\treturn r;\n}",
        "func": "R_API st64 r_buf_fread_at(RBuffer *b, ut64 addr, ut8 *buf, const char *fmt, int n) {\n\tr_return_val_if_fail (b && buf && fmt, -1);\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tst64 r = r_buf_seek (b, addr, R_BUF_SET);\n\tif (r < 0) {\n\t\treturn r;\n\t}\n\tr = r_buf_fread (b, buf, fmt, n);\n\t(void)r_buf_seek (b, o_addr, R_BUF_SET);\n\treturn r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,6 @@\n \t\treturn r;\n \t}\n \tr = r_buf_fread (b, buf, fmt, n);\n-\tr_buf_seek (b, o_addr, R_BUF_SET);\n+\t(void)r_buf_seek (b, o_addr, R_BUF_SET);\n \treturn r;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tr_buf_seek (b, o_addr, R_BUF_SET);"
            ],
            "added_lines": [
                "\t(void)r_buf_seek (b, o_addr, R_BUF_SET);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/r_buf_read_at",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "R_API st64 r_buf_read_at(RBuffer *b, ut64 addr, ut8 *buf, ut64 len) {\n\tr_return_val_if_fail (b && buf, -1);\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tst64 r = r_buf_seek (b, addr, R_BUF_SET);\n\tif (r < 0) {\n\t\treturn r;\n\t}\n\n\tr = r_buf_read (b, buf, len);\n\tr_buf_seek (b, o_addr, R_BUF_SET);\n\treturn r;\n}",
        "func": "R_API st64 r_buf_read_at(RBuffer *b, ut64 addr, ut8 *buf, ut64 len) {\n\tr_return_val_if_fail (b && buf, -1);\n\tst64 o_addr = r_buf_seek (b, 0, R_BUF_CUR);\n\tst64 r = r_buf_seek (b, addr, R_BUF_SET);\n\tif (r < 0) {\n\t\treturn r;\n\t}\n\tr = r_buf_read (b, buf, len);\n\tr_buf_seek (b, o_addr, R_BUF_SET);\n\treturn r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,6 @@\n \tif (r < 0) {\n \t\treturn r;\n \t}\n-\n \tr = r_buf_read (b, buf, len);\n \tr_buf_seek (b, o_addr, R_BUF_SET);\n \treturn r;",
        "diff_line_info": {
            "deleted_lines": [
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/buf_format",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static st64 buf_format(RBuffer *dst, RBuffer *src, const char *fmt, int n) {\n\tst64 res = 0;\n\tint i;\n\tfor (i = 0; i < n; i++) {\n\t\tint j;\n\t\tint m = 1;\n\t\tint tsize = 2;\n\t\tbool bigendian = true;\n\n\t\tfor (j = 0; fmt[j]; j++) {\n\t\t\tswitch (fmt[j]) {\n\t\t\tcase '0':\n\t\t\tcase '1':\n\t\t\tcase '2':\n\t\t\tcase '3':\n\t\t\tcase '4':\n\t\t\tcase '5':\n\t\t\tcase '6':\n\t\t\tcase '7':\n\t\t\tcase '8':\n\t\t\tcase '9':\n\t\t\t\tif (m == 1) {\n\t\t\t\t\tm = r_num_get (NULL, &fmt[j]);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\tcase 's': tsize = 2; bigendian = false; break;\n\t\t\tcase 'S': tsize = 2; bigendian = true; break;\n\t\t\tcase 'i': tsize = 4; bigendian = false; break;\n\t\t\tcase 'I': tsize = 4; bigendian = true; break;\n\t\t\tcase 'l': tsize = 8; bigendian = false; break;\n\t\t\tcase 'L': tsize = 8; bigendian = true; break;\n\t\t\tcase 'c': tsize = 1; bigendian = false; break;\n\t\t\tdefault: return -1;\n\t\t\t}\n\n\t\t\tint k;\n\t\t\tfor (k = 0; k < m; k++) {\n\t\t\t\tut8 tmp[sizeof (ut64)];\n\t\t\t\tut8 d1;\n\t\t\t\tut16 d2;\n\t\t\t\tut32 d3;\n\t\t\t\tut64 d4;\n\t\t\t\tst64 r = r_buf_read (src, tmp, tsize);\n\t\t\t\tif (r < tsize) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\n\t\t\t\tswitch (tsize) {\n\t\t\t\tcase 1:\n\t\t\t\t\td1 = r_read_ble8 (tmp);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d1, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 2:\n\t\t\t\t\td2 = r_read_ble16 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d2, 2);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 4:\n\t\t\t\t\td3 = r_read_ble32 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d3, 4);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 8:\n\t\t\t\t\td4 = r_read_ble64 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d4, 8);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (r < 0) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tres += r;\n\t\t\t}\n\t\t\tm = 1;\n\t\t}\n\t}\n\treturn res;\n}",
        "func": "static st64 buf_format(RBuffer *dst, RBuffer *src, const char *fmt, int n) {\n\tst64 res = 0;\n\tint i;\n\tfor (i = 0; i < n; i++) {\n\t\tint j;\n\t\tint m = 1;\n\t\tint tsize = 2;\n\t\tbool bigendian = true;\n\n\t\tfor (j = 0; fmt[j]; j++) {\n\t\t\tswitch (fmt[j]) {\n\t\t\tcase '0':\n\t\t\tcase '1':\n\t\t\tcase '2':\n\t\t\tcase '3':\n\t\t\tcase '4':\n\t\t\tcase '5':\n\t\t\tcase '6':\n\t\t\tcase '7':\n\t\t\tcase '8':\n\t\t\tcase '9':\n\t\t\t\tif (m == 1) {\n\t\t\t\t\tm = r_num_get (NULL, &fmt[j]);\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\tcase 's': tsize = 2; bigendian = false; break;\n\t\t\tcase 'S': tsize = 2; bigendian = true; break;\n\t\t\tcase 'i': tsize = 4; bigendian = false; break;\n\t\t\tcase 'I': tsize = 4; bigendian = true; break;\n\t\t\tcase 'l': tsize = 8; bigendian = false; break;\n\t\t\tcase 'L': tsize = 8; bigendian = true; break;\n\t\t\tcase 'c': tsize = 1; bigendian = false; break;\n\t\t\tdefault: return -1;\n\t\t\t}\n\n\t\t\tint k;\n\t\t\tfor (k = 0; k < m; k++) {\n\t\t\t\tut8 tmp[sizeof (ut64)];\n\t\t\t\tut8 d1;\n\t\t\t\tut16 d2;\n\t\t\t\tut32 d3;\n\t\t\t\tut64 d4;\n\t\t\t\tst64 r = r_buf_read (src, tmp, tsize);\n\t\t\t\tif (r != tsize) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tswitch (tsize) {\n\t\t\t\tcase 1:\n\t\t\t\t\td1 = r_read_ble8 (tmp);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d1, 1);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 2:\n\t\t\t\t\td2 = r_read_ble16 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d2, 2);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 4:\n\t\t\t\t\td3 = r_read_ble32 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d3, 4);\n\t\t\t\t\tbreak;\n\t\t\t\tcase 8:\n\t\t\t\t\td4 = r_read_ble64 (tmp, bigendian);\n\t\t\t\t\tr = r_buf_write (dst, (ut8 *)&d4, 8);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (r < 0) {\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tres += r;\n\t\t\t}\n\t\t\tm = 1;\n\t\t}\n\t}\n\treturn res;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,10 +41,9 @@\n \t\t\t\tut32 d3;\n \t\t\t\tut64 d4;\n \t\t\t\tst64 r = r_buf_read (src, tmp, tsize);\n-\t\t\t\tif (r < tsize) {\n+\t\t\t\tif (r != tsize) {\n \t\t\t\t\treturn -1;\n \t\t\t\t}\n-\n \t\t\t\tswitch (tsize) {\n \t\t\t\tcase 1:\n \t\t\t\t\td1 = r_read_ble8 (tmp);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tif (r < tsize) {",
                ""
            ],
            "added_lines": [
                "\t\t\t\tif (r != tsize) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/sections",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static RList* sections(RBinFile* bf) {\n\tRList* ret = NULL;\n\tRBinSection* sect = NULL;\n\tpsxexe_header psxheader = {0};\n\tut64 sz = 0;\n\n\tif (!(ret = r_list_new ())) {\n\t\treturn NULL;\n\t}\n\n\tif (!(sect = R_NEW0 (RBinSection))) {\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {\n\t\teprintf (\"Truncated Header\\n\");\n\t\tfree (sect);\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tsz = r_buf_size (bf->buf);\n\n\tsect->name = strdup (\"TEXT\");\n\tsect->paddr = PSXEXE_TEXTSECTION_OFFSET;\n\tsect->size = sz - PSXEXE_TEXTSECTION_OFFSET;\n\tsect->vaddr = psxheader.t_addr;\n\tsect->vsize = psxheader.t_size;\n\tsect->perm = R_PERM_RX;\n\tsect->add = true;\n\tsect->has_strings = true;\n\n\tr_list_append (ret, sect);\n\treturn ret;\n}",
        "func": "static RList* sections(RBinFile* bf) {\n\tRList* ret = NULL;\n\tRBinSection* sect = NULL;\n\tpsxexe_header psxheader = {0};\n\tut64 sz = 0;\n\n\tif (!(ret = r_list_new ())) {\n\t\treturn NULL;\n\t}\n\n\tif (!(sect = R_NEW0 (RBinSection))) {\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {\n\t\teprintf (\"Truncated Header\\n\");\n\t\tfree (sect);\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tsz = r_buf_size (bf->buf);\n\n\tsect->name = strdup (\"TEXT\");\n\tsect->paddr = PSXEXE_TEXTSECTION_OFFSET;\n\tsect->size = sz - PSXEXE_TEXTSECTION_OFFSET;\n\tsect->vaddr = psxheader.t_addr;\n\tsect->vsize = psxheader.t_size;\n\tsect->perm = R_PERM_RX;\n\tsect->add = true;\n\tsect->has_strings = true;\n\n\tr_list_append (ret, sect);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \t\treturn NULL;\n \t}\n \n-\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {\n+\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {\n \t\teprintf (\"Truncated Header\\n\");\n \t\tfree (sect);\n \t\tr_list_free (ret);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {"
            ],
            "added_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/entries",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static RList* entries(RBinFile* bf) {\n\tRList* ret = NULL;\n\tRBinAddr* addr = NULL;\n\tpsxexe_header psxheader;\n\n\tif (!(ret = r_list_new ())) {\n\t\treturn NULL;\n\t}\n\n\tif (!(addr = R_NEW0 (RBinAddr))) {\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {\n\t\teprintf (\"PSXEXE Header truncated\\n\");\n\t\tr_list_free (ret);\n\t\tfree (addr);\n\t\treturn NULL;\n\t}\n\n\taddr->paddr = (psxheader.pc0 - psxheader.t_addr) + PSXEXE_TEXTSECTION_OFFSET;\n\taddr->vaddr = psxheader.pc0;\n\n\tr_list_append (ret, addr);\n\treturn ret;\n}",
        "func": "static RList* entries(RBinFile* bf) {\n\tRList* ret = NULL;\n\tRBinAddr* addr = NULL;\n\tpsxexe_header psxheader;\n\n\tif (!(ret = r_list_new ())) {\n\t\treturn NULL;\n\t}\n\n\tif (!(addr = R_NEW0 (RBinAddr))) {\n\t\tr_list_free (ret);\n\t\treturn NULL;\n\t}\n\n\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {\n\t\teprintf (\"PSXEXE Header truncated\\n\");\n\t\tr_list_free (ret);\n\t\tfree (addr);\n\t\treturn NULL;\n\t}\n\n\taddr->paddr = (psxheader.pc0 - psxheader.t_addr) + PSXEXE_TEXTSECTION_OFFSET;\n\taddr->vaddr = psxheader.pc0;\n\n\tr_list_append (ret, addr);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n \t\treturn NULL;\n \t}\n \n-\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {\n+\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {\n \t\teprintf (\"PSXEXE Header truncated\\n\");\n \t\tr_list_free (ret);\n \t\tfree (addr);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) < sizeof (psxexe_header)) {"
            ],
            "added_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8*)&psxheader, \"8c17i\", 1) != sizeof (psxexe_header)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/lmf_header_load",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static int lmf_header_load(lmf_header *lmfh, RBuffer *buf, Sdb *db) {\n\tif (r_buf_size (buf) < sizeof (lmf_header)) {\n\t\treturn false;\n\t}\n\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) < QNX_HDR_SIZE) {\n\t\treturn false;\n\t}\n\tr_strf_buffer (32);\n\tsdb_set (db, \"qnx.version\", r_strf (\"0x%xH\", lmfh->version), 0);\n\tsdb_set (db, \"qnx.cflags\", r_strf (\"0x%xH\", lmfh->cflags), 0);\n\tsdb_set (db, \"qnx.cpu\", r_strf (\"0x%xH\", lmfh->cpu), 0);\n\tsdb_set (db, \"qnx.fpu\", r_strf (\"0x%xH\", lmfh->fpu), 0);\n\tsdb_set (db, \"qnx.code_index\", r_strf (\"0x%x\", lmfh->code_index), 0);\n\tsdb_set (db, \"qnx.stack_index\", r_strf (\"0x%x\", lmfh->stack_index), 0);\n\tsdb_set (db, \"qnx.heap_index\", r_strf (\"0x%x\", lmfh->heap_index), 0);\n\tsdb_set (db, \"qnx.argv_index\", r_strf (\"0x%x\", lmfh->argv_index), 0);\n\tsdb_set (db, \"qnx.code_offset\", r_strf (\"0x%x\", lmfh->code_offset), 0);\n\tsdb_set (db, \"qnx.stack_nbytes\", r_strf (\"0x%x\", lmfh->stack_nbytes), 0);\n\tsdb_set (db, \"qnx.heap_nbytes\", r_strf (\"0x%x\", lmfh->heap_nbytes), 0);\n\tsdb_set (db, \"qnx.image_base\", r_strf (\"0x%x\", lmfh->image_base), 0);\n\treturn true;\n}",
        "func": "static int lmf_header_load(lmf_header *lmfh, RBuffer *buf, Sdb *db) {\n\tif (r_buf_size (buf) < sizeof (lmf_header)) {\n\t\treturn false;\n\t}\n\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) != QNX_HDR_SIZE) {\n\t\treturn false;\n\t}\n\tr_strf_buffer (32);\n\tsdb_set (db, \"qnx.version\", r_strf (\"0x%xH\", lmfh->version), 0);\n\tsdb_set (db, \"qnx.cflags\", r_strf (\"0x%xH\", lmfh->cflags), 0);\n\tsdb_set (db, \"qnx.cpu\", r_strf (\"0x%xH\", lmfh->cpu), 0);\n\tsdb_set (db, \"qnx.fpu\", r_strf (\"0x%xH\", lmfh->fpu), 0);\n\tsdb_set (db, \"qnx.code_index\", r_strf (\"0x%x\", lmfh->code_index), 0);\n\tsdb_set (db, \"qnx.stack_index\", r_strf (\"0x%x\", lmfh->stack_index), 0);\n\tsdb_set (db, \"qnx.heap_index\", r_strf (\"0x%x\", lmfh->heap_index), 0);\n\tsdb_set (db, \"qnx.argv_index\", r_strf (\"0x%x\", lmfh->argv_index), 0);\n\tsdb_set (db, \"qnx.code_offset\", r_strf (\"0x%x\", lmfh->code_offset), 0);\n\tsdb_set (db, \"qnx.stack_nbytes\", r_strf (\"0x%x\", lmfh->stack_nbytes), 0);\n\tsdb_set (db, \"qnx.heap_nbytes\", r_strf (\"0x%x\", lmfh->heap_nbytes), 0);\n\tsdb_set (db, \"qnx.image_base\", r_strf (\"0x%x\", lmfh->image_base), 0);\n\treturn true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n \tif (r_buf_size (buf) < sizeof (lmf_header)) {\n \t\treturn false;\n \t}\n-\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) < QNX_HDR_SIZE) {\n+\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) != QNX_HDR_SIZE) {\n \t\treturn false;\n \t}\n \tr_strf_buffer (32);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) < QNX_HDR_SIZE) {"
            ],
            "added_lines": [
                "\tif (r_buf_fread_at (buf, QNX_HEADER_ADDR, (ut8 *) lmfh, \"iiiiiiiicccciiiicc\", 1) != QNX_HDR_SIZE) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0695",
        "func_name": "radareorg/radare2/load_buffer",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.6.4.",
        "git_url": "https://github.com/radareorg/radare2/commit/634b886e84a5c568d243e744becc6b3223e089cf",
        "commit_title": "Fix DoS in PE/QNX/DYLDCACHE/PSX parsers ##crash",
        "commit_text": " * Reported by lazymio * Reproducer: AAA4AAAAAB4=",
        "func_before": "static bool load_buffer(RBinFile *bf, void **bin_obj, RBuffer *buf, ut64 loadaddr, Sdb *sdb) {\n\tQnxObj *qo = R_NEW0 (QnxObj);\n\tif (!qo) {\n\t\treturn false;\n\t}\n\tlmf_record lrec;\n\tlmf_resource lres;\n\tlmf_data ldata;\n\tut64 offset = QNX_RECORD_SIZE;\n\tRList *sections = NULL;\n\tRList *fixups = NULL;\n\n\tif (!qo) {\n\t\tgoto beach;\n\t}\n\tif (!(sections = r_list_newf ((RListFree)r_bin_section_free)) || !(fixups = r_list_new ())) {\n\t\tgoto beach;\n\t}\n\tqo->kv = sdb_new0 ();\n\tif (!qo->kv) {\n\t\tgoto beach;\n\t}\n\t// Read the first record\n\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {\n\t\tgoto beach;\n\t}\n\t// Load the header\n\tlmf_header_load (&qo->lmfh, bf->buf, qo->kv);\n\toffset += lrec.data_nbytes;\n\n\tfor (;;) {\n\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {\n\t\t\tgoto beach;\n\t\t}\n\t\toffset += sizeof (lmf_record);\n\n\t\tif (lrec.rec_type == LMF_IMAGE_END_REC) {\n\t\t\tbreak;\n\t\t} else if (lrec.rec_type == LMF_RESOURCE_REC) {\n\t\t\tRBinSection *ptr = R_NEW0 (RBinSection);\n\t\t\tif (!ptr) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) < sizeof (lmf_resource)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->name = strdup (\"LMF_RESOURCE\");\n\t\t\tptr->paddr = offset;\n\t\t\tptr->vsize = lrec.data_nbytes - sizeof (lmf_resource);\n\t\t\tptr->size = ptr->vsize;\n\t\t\tptr->add = true;\n\t\t \tr_list_append (sections, ptr);\n\t\t} else if (lrec.rec_type == LMF_LOAD_REC) {\n\t\t\tRBinSection *ptr = R_NEW0 (RBinSection);\n\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tif (!ptr) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->name = strdup (\"LMF_LOAD\");\n\t\t\tptr->paddr = offset;\n\t\t\tptr->vaddr = ldata.offset;\n\t\t\tptr->vsize = lrec.data_nbytes - sizeof (lmf_data);\n\t\t\tptr->size = ptr->vsize;\n\t\t\tptr->add = true;\n\t\t \tr_list_append (sections, ptr);\n\t\t} else if (lrec.rec_type == LMF_FIXUP_REC) {\n\t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->vaddr = ptr->paddr = ldata.offset;\n\t\t\tptr->type = 'f'; // \"LMF_FIXUP\";\n\t\t\tr_list_append (fixups, ptr);\n\t\t} else if (lrec.rec_type == LMF_8087_FIXUP_REC) {\n\t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->vaddr = ptr->paddr = ldata.offset;\n\t\t\tptr->type = 'F'; // \"LMF_8087_FIXUP\";\n\t\t\tr_list_append (fixups, ptr);\n\t\t} else if (lrec.rec_type == LMF_RW_END_REC) {\n\t\t\tr_buf_fread_at (bf->buf, offset, (ut8 *)&qo->rwend, \"si\", 1);\n\t\t}\n\t\toffset += lrec.data_nbytes;\n\t}\n\tsdb_ns_set (sdb, \"info\", qo->kv);\n\tqo->sections = sections;\n\tqo->fixups = fixups;\n\t*bin_obj = qo;\n\treturn true;\nbeach:\n\tfree (qo);\n\tr_list_free (fixups);\n\tr_list_free (sections);\n\treturn false;\n}",
        "func": "static bool load_buffer(RBinFile *bf, void **bin_obj, RBuffer *buf, ut64 loadaddr, Sdb *sdb) {\n\tQnxObj *qo = R_NEW0 (QnxObj);\n\tif (!qo) {\n\t\treturn false;\n\t}\n\tlmf_record lrec;\n\tlmf_resource lres;\n\tlmf_data ldata;\n\tut64 offset = QNX_RECORD_SIZE;\n\tRList *sections = NULL;\n\tRList *fixups = NULL;\n\n\tif (!qo) {\n\t\tgoto beach;\n\t}\n\tif (!(sections = r_list_newf ((RListFree)r_bin_section_free)) || !(fixups = r_list_new ())) {\n\t\tgoto beach;\n\t}\n\tqo->kv = sdb_new0 ();\n\tif (!qo->kv) {\n\t\tgoto beach;\n\t}\n\t// Read the first record\n\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {\n\t\tgoto beach;\n\t}\n\t// Load the header\n\tlmf_header_load (&qo->lmfh, bf->buf, qo->kv);\n\toffset += lrec.data_nbytes;\n\n\tfor (;;) {\n\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {\n\t\t\tgoto beach;\n\t\t}\n\t\toffset += sizeof (lmf_record);\n\n\t\tif (lrec.rec_type == LMF_IMAGE_END_REC) {\n\t\t\tbreak;\n\t\t} else if (lrec.rec_type == LMF_RESOURCE_REC) {\n\t\t\tRBinSection *ptr = R_NEW0 (RBinSection);\n\t\t\tif (!ptr) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) != sizeof (lmf_resource)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->name = strdup (\"LMF_RESOURCE\");\n\t\t\tptr->paddr = offset;\n\t\t\tptr->vsize = lrec.data_nbytes - sizeof (lmf_resource);\n\t\t\tptr->size = ptr->vsize;\n\t\t\tptr->add = true;\n\t\t \tr_list_append (sections, ptr);\n\t\t} else if (lrec.rec_type == LMF_LOAD_REC) {\n\t\t\tRBinSection *ptr = R_NEW0 (RBinSection);\n\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tif (!ptr) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->name = strdup (\"LMF_LOAD\");\n\t\t\tptr->paddr = offset;\n\t\t\tptr->vaddr = ldata.offset;\n\t\t\tptr->vsize = lrec.data_nbytes - sizeof (lmf_data);\n\t\t\tptr->size = ptr->vsize;\n\t\t\tptr->add = true;\n\t\t \tr_list_append (sections, ptr);\n\t\t} else if (lrec.rec_type == LMF_FIXUP_REC) {\n\t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->vaddr = ptr->paddr = ldata.offset;\n\t\t\tptr->type = 'f'; // \"LMF_FIXUP\";\n\t\t\tr_list_append (fixups, ptr);\n\t\t} else if (lrec.rec_type == LMF_8087_FIXUP_REC) {\n\t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n\t\t\t\tgoto beach;\n\t\t\t}\n\t\t\tptr->vaddr = ptr->paddr = ldata.offset;\n\t\t\tptr->type = 'F'; // \"LMF_8087_FIXUP\";\n\t\t\tr_list_append (fixups, ptr);\n\t\t} else if (lrec.rec_type == LMF_RW_END_REC) {\n\t\t\tr_buf_fread_at (bf->buf, offset, (ut8 *)&qo->rwend, \"si\", 1);\n\t\t}\n\t\toffset += lrec.data_nbytes;\n\t}\n\tsdb_ns_set (sdb, \"info\", qo->kv);\n\tqo->sections = sections;\n\tqo->fixups = fixups;\n\t*bin_obj = qo;\n\treturn true;\nbeach:\n\tfree (qo);\n\tr_list_free (fixups);\n\tr_list_free (sections);\n\treturn false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,7 @@\n \t\tgoto beach;\n \t}\n \t// Read the first record\n-\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {\n+\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {\n \t\tgoto beach;\n \t}\n \t// Load the header\n@@ -29,7 +29,7 @@\n \toffset += lrec.data_nbytes;\n \n \tfor (;;) {\n-\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {\n+\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {\n \t\t\tgoto beach;\n \t\t}\n \t\toffset += sizeof (lmf_record);\n@@ -41,7 +41,7 @@\n \t\t\tif (!ptr) {\n \t\t\t\tgoto beach;\n \t\t\t}\n-\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) < sizeof (lmf_resource)) {\n+\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) != sizeof (lmf_resource)) {\n \t\t\t\tgoto beach;\n \t\t\t}\n \t\t\tptr->name = strdup (\"LMF_RESOURCE\");\n@@ -52,7 +52,7 @@\n \t\t \tr_list_append (sections, ptr);\n \t\t} else if (lrec.rec_type == LMF_LOAD_REC) {\n \t\t\tRBinSection *ptr = R_NEW0 (RBinSection);\n-\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n+\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n \t\t\t\tgoto beach;\n \t\t\t}\n \t\t\tif (!ptr) {\n@@ -67,7 +67,7 @@\n \t\t \tr_list_append (sections, ptr);\n \t\t} else if (lrec.rec_type == LMF_FIXUP_REC) {\n \t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n-\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n+\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n \t\t\t\tgoto beach;\n \t\t\t}\n \t\t\tptr->vaddr = ptr->paddr = ldata.offset;\n@@ -75,7 +75,7 @@\n \t\t\tr_list_append (fixups, ptr);\n \t\t} else if (lrec.rec_type == LMF_8087_FIXUP_REC) {\n \t\t\tRBinReloc *ptr = R_NEW0 (RBinReloc);\n-\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {\n+\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {\n \t\t\t\tgoto beach;\n \t\t\t}\n \t\t\tptr->vaddr = ptr->paddr = ldata.offset;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {",
                "\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) < QNX_RECORD_SIZE) {",
                "\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) < sizeof (lmf_resource)) {",
                "\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {",
                "\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {",
                "\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) < sizeof (lmf_data)) {"
            ],
            "added_lines": [
                "\tif (r_buf_fread_at (bf->buf, 0, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {",
                "\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lrec, \"ccss\", 1) != QNX_RECORD_SIZE) {",
                "\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&lres, \"ssss\", 1) != sizeof (lmf_resource)) {",
                "\t\t\tif (r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {",
                "\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {",
                "\t\t\tif (!ptr || r_buf_fread_at (bf->buf, offset, (ut8 *)&ldata, \"si\", 1) != sizeof (lmf_data)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9253",
        "func_name": "php/php-src/fpm_child_init",
        "description": "An issue was discovered in PHP 7.3.x before 7.3.0alpha3, 7.2.x before 7.2.8, and before 7.1.20. The php-fpm master process restarts a child process in an endless loop when using program execution functions (e.g., passthru, exec, shell_exec, or system) with a non-blocking STDIN stream, causing this master process to consume 100% of the CPU, and consume disk space with a large volume of error logs, as demonstrated by an attack by a customer of a shared-hosting facility.",
        "git_url": "https://github.com/php/php-src/commit/69dee5c732fe982c82edb17d0dbc3e79a47748d8",
        "commit_title": "Fixed bug #73342",
        "commit_text": " Directly listen on socket, instead of duping it to STDIN and listening on that.",
        "func_before": "static void fpm_child_init(struct fpm_worker_pool_s *wp) /* {{{ */\n{\n\tfpm_globals.max_requests = wp->config->pm_max_requests;\n\n\tif (0 > fpm_stdio_init_child(wp)  ||\n\t    0 > fpm_log_init_child(wp)    ||\n\t    0 > fpm_status_init_child(wp) ||\n\t    0 > fpm_unix_init_child(wp)   ||\n\t    0 > fpm_signals_init_child()  ||\n\t    0 > fpm_env_init_child(wp)    ||\n\t    0 > fpm_php_init_child(wp)) {\n\n\t\tzlog(ZLOG_ERROR, \"[pool %s] child failed to initialize\", wp->config->name);\n\t\texit(FPM_EXIT_SOFTWARE);\n\t}\n}",
        "func": "static void fpm_child_init(struct fpm_worker_pool_s *wp) /* {{{ */\n{\n\tfpm_globals.max_requests = wp->config->pm_max_requests;\n\tfpm_globals.listening_socket = dup(wp->listening_socket);\n\n\tif (0 > fpm_stdio_init_child(wp)  ||\n\t    0 > fpm_log_init_child(wp)    ||\n\t    0 > fpm_status_init_child(wp) ||\n\t    0 > fpm_unix_init_child(wp)   ||\n\t    0 > fpm_signals_init_child()  ||\n\t    0 > fpm_env_init_child(wp)    ||\n\t    0 > fpm_php_init_child(wp)) {\n\n\t\tzlog(ZLOG_ERROR, \"[pool %s] child failed to initialize\", wp->config->name);\n\t\texit(FPM_EXIT_SOFTWARE);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n static void fpm_child_init(struct fpm_worker_pool_s *wp) /* {{{ */\n {\n \tfpm_globals.max_requests = wp->config->pm_max_requests;\n+\tfpm_globals.listening_socket = dup(wp->listening_socket);\n \n \tif (0 > fpm_stdio_init_child(wp)  ||\n \t    0 > fpm_log_init_child(wp)    ||",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tfpm_globals.listening_socket = dup(wp->listening_socket);"
            ]
        }
    },
    {
        "cve_id": "CVE-2015-9253",
        "func_name": "php/php-src/fpm_stdio_init_child",
        "description": "An issue was discovered in PHP 7.3.x before 7.3.0alpha3, 7.2.x before 7.2.8, and before 7.1.20. The php-fpm master process restarts a child process in an endless loop when using program execution functions (e.g., passthru, exec, shell_exec, or system) with a non-blocking STDIN stream, causing this master process to consume 100% of the CPU, and consume disk space with a large volume of error logs, as demonstrated by an attack by a customer of a shared-hosting facility.",
        "git_url": "https://github.com/php/php-src/commit/69dee5c732fe982c82edb17d0dbc3e79a47748d8",
        "commit_title": "Fixed bug #73342",
        "commit_text": " Directly listen on socket, instead of duping it to STDIN and listening on that.",
        "func_before": "int fpm_stdio_init_child(struct fpm_worker_pool_s *wp) /* {{{ */\n{\n#ifdef HAVE_SYSLOG_H\n\tif (fpm_globals.error_log_fd == ZLOG_SYSLOG) {\n\t\tcloselog(); /* ensure to close syslog not to interrupt with PHP syslog code */\n\t} else\n#endif\n\n\t/* Notice: child cannot use master error_log\n\t * because not aware when being reopen\n\t * else, should use if (!fpm_use_error_log())\n\t */\n\tif (fpm_globals.error_log_fd > 0) {\n\t\tclose(fpm_globals.error_log_fd);\n\t}\n\tfpm_globals.error_log_fd = -1;\n\tzlog_set_fd(-1);\n\n\tif (wp->listening_socket != STDIN_FILENO) {\n\t\tif (0 > dup2(wp->listening_socket, STDIN_FILENO)) {\n\t\t\tzlog(ZLOG_SYSERROR, \"failed to init child stdio: dup2()\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}",
        "func": "int fpm_stdio_init_child(struct fpm_worker_pool_s *wp) /* {{{ */\n{\n#ifdef HAVE_SYSLOG_H\n\tif (fpm_globals.error_log_fd == ZLOG_SYSLOG) {\n\t\tcloselog(); /* ensure to close syslog not to interrupt with PHP syslog code */\n\t} else\n#endif\n\n\t/* Notice: child cannot use master error_log\n\t * because not aware when being reopen\n\t * else, should use if (!fpm_use_error_log())\n\t */\n\tif (fpm_globals.error_log_fd > 0) {\n\t\tclose(fpm_globals.error_log_fd);\n\t}\n\tfpm_globals.error_log_fd = -1;\n\tzlog_set_fd(-1);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,11 +16,5 @@\n \tfpm_globals.error_log_fd = -1;\n \tzlog_set_fd(-1);\n \n-\tif (wp->listening_socket != STDIN_FILENO) {\n-\t\tif (0 > dup2(wp->listening_socket, STDIN_FILENO)) {\n-\t\t\tzlog(ZLOG_SYSERROR, \"failed to init child stdio: dup2()\");\n-\t\t\treturn -1;\n-\t\t}\n-\t}\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (wp->listening_socket != STDIN_FILENO) {",
                "\t\tif (0 > dup2(wp->listening_socket, STDIN_FILENO)) {",
                "\t\t\tzlog(ZLOG_SYSERROR, \"failed to init child stdio: dup2()\");",
                "\t\t\treturn -1;",
                "\t\t}",
                "\t}"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/declareArguments",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/a014f4c224a7b21f1c648257d1fd1128413129aa",
        "commit_title": "Add limits to the size of received AXFR, in megabytes",
        "commit_text": " This prevents resource exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "void declareArguments()\n{\n  ::arg().set(\"local-port\",\"The port on which we listen\")=\"53\";\n  ::arg().setSwitch(\"experimental-dnsupdate\",\"Enable/Disable DNS update (RFC2136) support. Default is no.\")=\"no\";\n  ::arg().set(\"allow-dnsupdate-from\",\"A global setting to allow DNS updates from these IP ranges.\")=\"127.0.0.0/8,::1\";\n  ::arg().setSwitch(\"forward-dnsupdate\",\"A global setting to allow DNS update packages that are for a Slave domain, to be forwarded to the master.\")=\"yes\";\n  ::arg().setSwitch(\"log-dns-details\",\"If PDNS should log DNS non-erroneous details\")=\"no\";\n  ::arg().setSwitch(\"log-dns-queries\",\"If PDNS should log all incoming DNS queries\")=\"no\";\n  ::arg().set(\"local-address\",\"Local IP addresses to which we bind\")=\"0.0.0.0\";\n  ::arg().setSwitch(\"local-address-nonexist-fail\",\"Fail to start if one or more of the local-address's do not exist on this server\")=\"yes\";\n  ::arg().set(\"local-ipv6\",\"Local IP address to which we bind\")=\"\";\n  ::arg().setSwitch(\"reuseport\",\"Enable higher performance on compliant kernels by using SO_REUSEPORT allowing each receiver thread to open its own socket\")=\"no\";\n  ::arg().setSwitch(\"local-ipv6-nonexist-fail\",\"Fail to start if one or more of the local-ipv6 addresses do not exist on this server\")=\"yes\";\n  ::arg().set(\"query-local-address\",\"Source IP address for sending queries\")=\"0.0.0.0\";\n  ::arg().set(\"query-local-address6\",\"Source IPv6 address for sending queries\")=\"::\";\n  ::arg().set(\"overload-queue-length\",\"Maximum queuelength moving to packetcache only\")=\"0\";\n  ::arg().set(\"max-queue-length\",\"Maximum queuelength before considering situation lost\")=\"5000\";\n\n  ::arg().set(\"retrieval-threads\", \"Number of AXFR-retrieval threads for slave operation\")=\"2\";\n  ::arg().setSwitch(\"experimental-json-interface\", \"If the webserver should serve JSON data\")=\"no\";\n  ::arg().setSwitch(\"experimental-api-readonly\", \"If the JSON API should disallow data modification\")=\"no\";\n  ::arg().set(\"experimental-api-key\", \"REST API Static authentication key (required for API use)\")=\"\";\n  ::arg().setSwitch(\"experimental-dname-processing\", \"If we should support DNAME records\")=\"no\";\n\n  ::arg().setCmd(\"help\",\"Provide a helpful message\");\n  ::arg().setCmd(\"version\",\"Output version and compilation date\");\n  ::arg().setCmd(\"config\",\"Provide configuration file on standard output\");\n  ::arg().setCmd(\"list-modules\",\"Lists all modules available\");\n  ::arg().setCmd(\"no-config\",\"Don't parse configuration file\");\n  \n  ::arg().set(\"version-string\",\"PowerDNS version in packets - full, anonymous, powerdns or custom\")=\"full\"; \n  ::arg().set(\"control-console\",\"Debugging switch - don't use\")=\"no\"; // but I know you will!\n  ::arg().set(\"loglevel\",\"Amount of logging. Higher is more. Do not set below 3\")=\"4\";\n  ::arg().set(\"disable-syslog\",\"Disable logging to syslog, useful when running inside a supervisor that logs stdout\")=\"no\";\n  ::arg().set(\"default-soa-name\",\"name to insert in the SOA record if none set in the backend\")=\"a.misconfigured.powerdns.server\";\n  ::arg().set(\"default-soa-mail\",\"mail address to insert in the SOA record if none set in the backend\")=\"\";\n  ::arg().set(\"distributor-threads\",\"Default number of Distributor (backend) threads to start\")=\"3\";\n  ::arg().set(\"signing-threads\",\"Default number of signer threads to start\")=\"3\";\n  ::arg().set(\"receiver-threads\",\"Default number of receiver threads to start\")=\"1\";\n  ::arg().set(\"queue-limit\",\"Maximum number of milliseconds to queue a query\")=\"1500\"; \n  ::arg().set(\"recursor\",\"If recursion is desired, IP address of a recursing nameserver\")=\"no\"; \n  ::arg().set(\"allow-recursion\",\"List of subnets that are allowed to recurse\")=\"0.0.0.0/0\";\n  ::arg().set(\"pipebackend-abi-version\",\"Version of the pipe backend ABI\")=\"1\";\n  ::arg().set(\"udp-truncation-threshold\", \"Maximum UDP response size before we truncate\")=\"1680\";\n  ::arg().set(\"disable-tcp\",\"Do not listen to TCP queries\")=\"no\";\n  \n  ::arg().set(\"config-name\",\"Name of this virtual configuration - will rename the binary image\")=\"\";\n\n  ::arg().set(\"load-modules\",\"Load this module - supply absolute or relative path\")=\"\";\n  ::arg().set(\"launch\",\"Which backends to launch and order to query them in\")=\"\";\n  ::arg().setSwitch(\"disable-axfr\",\"Disable zonetransfers but do allow TCP queries\")=\"no\";\n  ::arg().set(\"allow-axfr-ips\",\"Allow zonetransfers only to these subnets\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"only-notify\", \"Only send AXFR NOTIFY to these IP addresses or netmasks\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"also-notify\", \"When notifying a domain, also notify these nameservers\")=\"\";\n  ::arg().set(\"allow-notify-from\",\"Allow AXFR NOTIFY from these IP ranges. If empty, drop all incoming notifies.\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"slave-cycle-interval\",\"Schedule slave freshness checks once every .. seconds\")=\"60\";\n\n  ::arg().set(\"tcp-control-address\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"\";\n  ::arg().set(\"tcp-control-port\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"53000\";\n  ::arg().set(\"tcp-control-secret\",\"If set, PowerDNS can be controlled over TCP after passing this secret\")=\"\";\n  ::arg().set(\"tcp-control-range\",\"If set, remote control of PowerDNS is possible over these networks only\")=\"127.0.0.0/8, 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12, ::1/128, fe80::/10\";\n  \n  ::arg().setSwitch(\"slave\",\"Act as a slave\")=\"no\";\n  ::arg().setSwitch(\"master\",\"Act as a master\")=\"no\";\n  ::arg().setSwitch(\"disable-axfr-rectify\",\"Disable the rectify step during an outgoing AXFR. Only required for regression testing.\")=\"no\";\n  ::arg().setSwitch(\"guardian\",\"Run within a guardian process\")=\"no\";\n  ::arg().setSwitch(\"send-root-referral\",\"Send out old-fashioned root-referral instead of ServFail in case of no authority\")=\"no\";\n  ::arg().setSwitch(\"prevent-self-notification\",\"Don't send notifications to what we think is ourself\")=\"yes\";\n  ::arg().setSwitch(\"webserver\",\"Start a webserver for monitoring\")=\"no\"; \n  ::arg().setSwitch(\"webserver-print-arguments\",\"If the webserver should print arguments\")=\"no\"; \n  ::arg().setSwitch(\"edns-subnet-processing\",\"If we should act on EDNS Subnet options\")=\"no\"; \n  ::arg().setSwitch(\"any-to-tcp\",\"Answer ANY queries with tc=1, shunting to TCP\")=\"no\"; \n  ::arg().set(\"webserver-address\",\"IP Address of webserver to listen on\")=\"127.0.0.1\";\n  ::arg().set(\"webserver-port\",\"Port of webserver to listen on\")=\"8081\";\n  ::arg().set(\"webserver-password\",\"Password required for accessing the webserver\")=\"\";\n  ::arg().set(\"webserver-allow-from\",\"Webserver access is only allowed from these subnets\")=\"0.0.0.0/0,::/0\";\n\n  ::arg().setSwitch(\"out-of-zone-additional-processing\",\"Do out of zone additional processing\")=\"yes\";\n  ::arg().setSwitch(\"do-ipv6-additional-processing\", \"Do AAAA additional processing\")=\"yes\";\n  ::arg().setSwitch(\"query-logging\",\"Hint backends that queries should be logged\")=\"no\";\n\n  ::arg().set(\"carbon-ourname\", \"If set, overrides our reported hostname for carbon stats\")=\"\";\n  ::arg().set(\"carbon-server\", \"If set, send metrics in carbon (graphite) format to this server\")=\"\";\n  ::arg().set(\"carbon-interval\", \"Number of seconds between carbon (graphite) updates\")=\"30\";\n\n  ::arg().set(\"cache-ttl\",\"Seconds to store packets in the PacketCache\")=\"20\";\n  ::arg().set(\"recursive-cache-ttl\",\"Seconds to store packets for recursive queries in the PacketCache\")=\"10\";\n  ::arg().set(\"negquery-cache-ttl\",\"Seconds to store negative query results in the QueryCache\")=\"60\";\n  ::arg().set(\"query-cache-ttl\",\"Seconds to store query results in the QueryCache\")=\"20\";\n  ::arg().set(\"soa-minimum-ttl\",\"Default SOA minimum ttl\")=\"3600\";\n  ::arg().set(\"server-id\", \"Returned when queried for 'server.id' TXT or NSID, defaults to hostname - disabled or custom\")=\"\";\n  ::arg().set(\"soa-refresh-default\",\"Default SOA refresh\")=\"10800\";\n  ::arg().set(\"soa-retry-default\",\"Default SOA retry\")=\"3600\";\n  ::arg().set(\"soa-expire-default\",\"Default SOA expire\")=\"604800\";\n  ::arg().set(\"default-soa-edit\",\"Default SOA-EDIT value\")=\"\";\n  ::arg().set(\"default-soa-edit-signed\",\"Default SOA-EDIT value for signed zones\")=\"\";\n\n  ::arg().set(\"trusted-notification-proxy\", \"IP address of incoming notification proxy\")=\"\";\n  ::arg().set(\"slave-renotify\", \"If we should send out notifications for slaved updates\")=\"no\";\n\n  ::arg().set(\"default-ttl\",\"Seconds a result is valid if not set otherwise\")=\"3600\";\n  ::arg().set(\"max-tcp-connections\",\"Maximum number of TCP connections\")=\"10\";\n  ::arg().setSwitch(\"no-shuffle\",\"Set this to prevent random shuffling of answers - for regression testing\")=\"off\";\n\n  ::arg().set(\"experimental-logfile\", \"Filename of the log file for JSON parser\" )= \"/var/log/pdns.log\";\n  ::arg().set(\"setuid\",\"If set, change user id to this uid for more security\")=\"\";\n  ::arg().set(\"setgid\",\"If set, change group id to this gid for more security\")=\"\";\n\n  ::arg().set(\"max-cache-entries\", \"Maximum number of cache entries\")=\"1000000\";\n  ::arg().set(\"max-signature-cache-entries\", \"Maximum number of signatures cache entries\")=\"\";\n  ::arg().set(\"max-ent-entries\", \"Maximum number of empty non-terminals in a zone\")=\"100000\";\n  ::arg().set(\"entropy-source\", \"If set, read entropy from this file\")=\"/dev/urandom\";\n\n  ::arg().set(\"lua-prequery-script\", \"Lua script with prequery handler\")=\"\";\n\n  ::arg().setSwitch(\"traceback-handler\",\"Enable the traceback handler (Linux only)\")=\"yes\";\n  ::arg().setSwitch(\"direct-dnskey\",\"Fetch DNSKEY RRs from backend during DNSKEY synthesis\")=\"no\";\n  ::arg().set(\"default-ksk-algorithms\",\"Default KSK algorithms\")=\"rsasha256\";\n  ::arg().set(\"default-ksk-size\",\"Default KSK size (0 means default)\")=\"0\";\n  ::arg().set(\"default-zsk-algorithms\",\"Default ZSK algorithms\")=\"rsasha256\";\n  ::arg().set(\"default-zsk-size\",\"Default ZSK size (0 means default)\")=\"0\";\n  ::arg().set(\"max-nsec3-iterations\",\"Limit the number of NSEC3 hash iterations\")=\"500\"; // RFC5155 10.3\n\n  ::arg().set(\"include-dir\",\"Include *.conf files from this directory\");\n  ::arg().set(\"security-poll-suffix\",\"Domain name from which to query security update notifications\")=\"secpoll.powerdns.com.\";\n}",
        "func": "void declareArguments()\n{\n  ::arg().set(\"local-port\",\"The port on which we listen\")=\"53\";\n  ::arg().setSwitch(\"experimental-dnsupdate\",\"Enable/Disable DNS update (RFC2136) support. Default is no.\")=\"no\";\n  ::arg().set(\"allow-dnsupdate-from\",\"A global setting to allow DNS updates from these IP ranges.\")=\"127.0.0.0/8,::1\";\n  ::arg().setSwitch(\"forward-dnsupdate\",\"A global setting to allow DNS update packages that are for a Slave domain, to be forwarded to the master.\")=\"yes\";\n  ::arg().setSwitch(\"log-dns-details\",\"If PDNS should log DNS non-erroneous details\")=\"no\";\n  ::arg().setSwitch(\"log-dns-queries\",\"If PDNS should log all incoming DNS queries\")=\"no\";\n  ::arg().set(\"local-address\",\"Local IP addresses to which we bind\")=\"0.0.0.0\";\n  ::arg().setSwitch(\"local-address-nonexist-fail\",\"Fail to start if one or more of the local-address's do not exist on this server\")=\"yes\";\n  ::arg().set(\"local-ipv6\",\"Local IP address to which we bind\")=\"\";\n  ::arg().setSwitch(\"reuseport\",\"Enable higher performance on compliant kernels by using SO_REUSEPORT allowing each receiver thread to open its own socket\")=\"no\";\n  ::arg().setSwitch(\"local-ipv6-nonexist-fail\",\"Fail to start if one or more of the local-ipv6 addresses do not exist on this server\")=\"yes\";\n  ::arg().set(\"query-local-address\",\"Source IP address for sending queries\")=\"0.0.0.0\";\n  ::arg().set(\"query-local-address6\",\"Source IPv6 address for sending queries\")=\"::\";\n  ::arg().set(\"overload-queue-length\",\"Maximum queuelength moving to packetcache only\")=\"0\";\n  ::arg().set(\"max-queue-length\",\"Maximum queuelength before considering situation lost\")=\"5000\";\n\n  ::arg().set(\"retrieval-threads\", \"Number of AXFR-retrieval threads for slave operation\")=\"2\";\n  ::arg().setSwitch(\"experimental-json-interface\", \"If the webserver should serve JSON data\")=\"no\";\n  ::arg().setSwitch(\"experimental-api-readonly\", \"If the JSON API should disallow data modification\")=\"no\";\n  ::arg().set(\"experimental-api-key\", \"REST API Static authentication key (required for API use)\")=\"\";\n  ::arg().setSwitch(\"experimental-dname-processing\", \"If we should support DNAME records\")=\"no\";\n\n  ::arg().setCmd(\"help\",\"Provide a helpful message\");\n  ::arg().setCmd(\"version\",\"Output version and compilation date\");\n  ::arg().setCmd(\"config\",\"Provide configuration file on standard output\");\n  ::arg().setCmd(\"list-modules\",\"Lists all modules available\");\n  ::arg().setCmd(\"no-config\",\"Don't parse configuration file\");\n  \n  ::arg().set(\"version-string\",\"PowerDNS version in packets - full, anonymous, powerdns or custom\")=\"full\"; \n  ::arg().set(\"control-console\",\"Debugging switch - don't use\")=\"no\"; // but I know you will!\n  ::arg().set(\"loglevel\",\"Amount of logging. Higher is more. Do not set below 3\")=\"4\";\n  ::arg().set(\"disable-syslog\",\"Disable logging to syslog, useful when running inside a supervisor that logs stdout\")=\"no\";\n  ::arg().set(\"default-soa-name\",\"name to insert in the SOA record if none set in the backend\")=\"a.misconfigured.powerdns.server\";\n  ::arg().set(\"default-soa-mail\",\"mail address to insert in the SOA record if none set in the backend\")=\"\";\n  ::arg().set(\"distributor-threads\",\"Default number of Distributor (backend) threads to start\")=\"3\";\n  ::arg().set(\"signing-threads\",\"Default number of signer threads to start\")=\"3\";\n  ::arg().set(\"receiver-threads\",\"Default number of receiver threads to start\")=\"1\";\n  ::arg().set(\"queue-limit\",\"Maximum number of milliseconds to queue a query\")=\"1500\"; \n  ::arg().set(\"recursor\",\"If recursion is desired, IP address of a recursing nameserver\")=\"no\"; \n  ::arg().set(\"allow-recursion\",\"List of subnets that are allowed to recurse\")=\"0.0.0.0/0\";\n  ::arg().set(\"pipebackend-abi-version\",\"Version of the pipe backend ABI\")=\"1\";\n  ::arg().set(\"udp-truncation-threshold\", \"Maximum UDP response size before we truncate\")=\"1680\";\n  ::arg().set(\"disable-tcp\",\"Do not listen to TCP queries\")=\"no\";\n  \n  ::arg().set(\"config-name\",\"Name of this virtual configuration - will rename the binary image\")=\"\";\n\n  ::arg().set(\"load-modules\",\"Load this module - supply absolute or relative path\")=\"\";\n  ::arg().set(\"launch\",\"Which backends to launch and order to query them in\")=\"\";\n  ::arg().setSwitch(\"disable-axfr\",\"Disable zonetransfers but do allow TCP queries\")=\"no\";\n  ::arg().set(\"allow-axfr-ips\",\"Allow zonetransfers only to these subnets\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"only-notify\", \"Only send AXFR NOTIFY to these IP addresses or netmasks\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"also-notify\", \"When notifying a domain, also notify these nameservers\")=\"\";\n  ::arg().set(\"allow-notify-from\",\"Allow AXFR NOTIFY from these IP ranges. If empty, drop all incoming notifies.\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"slave-cycle-interval\",\"Schedule slave freshness checks once every .. seconds\")=\"60\";\n\n  ::arg().set(\"tcp-control-address\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"\";\n  ::arg().set(\"tcp-control-port\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"53000\";\n  ::arg().set(\"tcp-control-secret\",\"If set, PowerDNS can be controlled over TCP after passing this secret\")=\"\";\n  ::arg().set(\"tcp-control-range\",\"If set, remote control of PowerDNS is possible over these networks only\")=\"127.0.0.0/8, 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12, ::1/128, fe80::/10\";\n  \n  ::arg().setSwitch(\"slave\",\"Act as a slave\")=\"no\";\n  ::arg().setSwitch(\"master\",\"Act as a master\")=\"no\";\n  ::arg().setSwitch(\"disable-axfr-rectify\",\"Disable the rectify step during an outgoing AXFR. Only required for regression testing.\")=\"no\";\n  ::arg().setSwitch(\"guardian\",\"Run within a guardian process\")=\"no\";\n  ::arg().setSwitch(\"send-root-referral\",\"Send out old-fashioned root-referral instead of ServFail in case of no authority\")=\"no\";\n  ::arg().setSwitch(\"prevent-self-notification\",\"Don't send notifications to what we think is ourself\")=\"yes\";\n  ::arg().setSwitch(\"webserver\",\"Start a webserver for monitoring\")=\"no\"; \n  ::arg().setSwitch(\"webserver-print-arguments\",\"If the webserver should print arguments\")=\"no\"; \n  ::arg().setSwitch(\"edns-subnet-processing\",\"If we should act on EDNS Subnet options\")=\"no\"; \n  ::arg().setSwitch(\"any-to-tcp\",\"Answer ANY queries with tc=1, shunting to TCP\")=\"no\"; \n  ::arg().set(\"webserver-address\",\"IP Address of webserver to listen on\")=\"127.0.0.1\";\n  ::arg().set(\"webserver-port\",\"Port of webserver to listen on\")=\"8081\";\n  ::arg().set(\"webserver-password\",\"Password required for accessing the webserver\")=\"\";\n  ::arg().set(\"webserver-allow-from\",\"Webserver access is only allowed from these subnets\")=\"0.0.0.0/0,::/0\";\n\n  ::arg().setSwitch(\"out-of-zone-additional-processing\",\"Do out of zone additional processing\")=\"yes\";\n  ::arg().setSwitch(\"do-ipv6-additional-processing\", \"Do AAAA additional processing\")=\"yes\";\n  ::arg().setSwitch(\"query-logging\",\"Hint backends that queries should be logged\")=\"no\";\n\n  ::arg().set(\"carbon-ourname\", \"If set, overrides our reported hostname for carbon stats\")=\"\";\n  ::arg().set(\"carbon-server\", \"If set, send metrics in carbon (graphite) format to this server\")=\"\";\n  ::arg().set(\"carbon-interval\", \"Number of seconds between carbon (graphite) updates\")=\"30\";\n\n  ::arg().set(\"cache-ttl\",\"Seconds to store packets in the PacketCache\")=\"20\";\n  ::arg().set(\"recursive-cache-ttl\",\"Seconds to store packets for recursive queries in the PacketCache\")=\"10\";\n  ::arg().set(\"negquery-cache-ttl\",\"Seconds to store negative query results in the QueryCache\")=\"60\";\n  ::arg().set(\"query-cache-ttl\",\"Seconds to store query results in the QueryCache\")=\"20\";\n  ::arg().set(\"soa-minimum-ttl\",\"Default SOA minimum ttl\")=\"3600\";\n  ::arg().set(\"server-id\", \"Returned when queried for 'server.id' TXT or NSID, defaults to hostname - disabled or custom\")=\"\";\n  ::arg().set(\"soa-refresh-default\",\"Default SOA refresh\")=\"10800\";\n  ::arg().set(\"soa-retry-default\",\"Default SOA retry\")=\"3600\";\n  ::arg().set(\"soa-expire-default\",\"Default SOA expire\")=\"604800\";\n  ::arg().set(\"default-soa-edit\",\"Default SOA-EDIT value\")=\"\";\n  ::arg().set(\"default-soa-edit-signed\",\"Default SOA-EDIT value for signed zones\")=\"\";\n\n  ::arg().set(\"trusted-notification-proxy\", \"IP address of incoming notification proxy\")=\"\";\n  ::arg().set(\"slave-renotify\", \"If we should send out notifications for slaved updates\")=\"no\";\n\n  ::arg().set(\"default-ttl\",\"Seconds a result is valid if not set otherwise\")=\"3600\";\n  ::arg().set(\"max-tcp-connections\",\"Maximum number of TCP connections\")=\"10\";\n  ::arg().setSwitch(\"no-shuffle\",\"Set this to prevent random shuffling of answers - for regression testing\")=\"off\";\n\n  ::arg().set(\"experimental-logfile\", \"Filename of the log file for JSON parser\" )= \"/var/log/pdns.log\";\n  ::arg().set(\"setuid\",\"If set, change user id to this uid for more security\")=\"\";\n  ::arg().set(\"setgid\",\"If set, change group id to this gid for more security\")=\"\";\n\n  ::arg().set(\"max-cache-entries\", \"Maximum number of cache entries\")=\"1000000\";\n  ::arg().set(\"max-signature-cache-entries\", \"Maximum number of signatures cache entries\")=\"\";\n  ::arg().set(\"max-ent-entries\", \"Maximum number of empty non-terminals in a zone\")=\"100000\";\n  ::arg().set(\"entropy-source\", \"If set, read entropy from this file\")=\"/dev/urandom\";\n\n  ::arg().set(\"lua-prequery-script\", \"Lua script with prequery handler\")=\"\";\n\n  ::arg().setSwitch(\"traceback-handler\",\"Enable the traceback handler (Linux only)\")=\"yes\";\n  ::arg().setSwitch(\"direct-dnskey\",\"Fetch DNSKEY RRs from backend during DNSKEY synthesis\")=\"no\";\n  ::arg().set(\"default-ksk-algorithms\",\"Default KSK algorithms\")=\"rsasha256\";\n  ::arg().set(\"default-ksk-size\",\"Default KSK size (0 means default)\")=\"0\";\n  ::arg().set(\"default-zsk-algorithms\",\"Default ZSK algorithms\")=\"rsasha256\";\n  ::arg().set(\"default-zsk-size\",\"Default ZSK size (0 means default)\")=\"0\";\n  ::arg().set(\"max-nsec3-iterations\",\"Limit the number of NSEC3 hash iterations\")=\"500\"; // RFC5155 10.3\n\n  ::arg().set(\"include-dir\",\"Include *.conf files from this directory\");\n  ::arg().set(\"security-poll-suffix\",\"Domain name from which to query security update notifications\")=\"secpoll.powerdns.com.\";\n\n  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming AXFR\")=\"100\";\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -123,4 +123,6 @@\n \n   ::arg().set(\"include-dir\",\"Include *.conf files from this directory\");\n   ::arg().set(\"security-poll-suffix\",\"Domain name from which to query security update notifications\")=\"secpoll.powerdns.com.\";\n+\n+  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming AXFR\")=\"100\";\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming AXFR\")=\"100\";"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/CommunicatorClass::suck",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/a014f4c224a7b21f1c648257d1fd1128413129aa",
        "commit_title": "Add limits to the size of received AXFR, in megabytes",
        "commit_text": " This prevents resource exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "void CommunicatorClass::suck(const string &domain,const string &remote)\n{\n  L<<Logger::Error<<\"Initiating transfer of '\"<<domain<<\"' from remote '\"<<remote<<\"'\"<<endl;\n  UeberBackend B; // fresh UeberBackend\n\n  DomainInfo di;\n  di.backend=0;\n  bool transaction=false;\n  try {\n    DNSSECKeeper dk (&B); // reuse our UeberBackend copy for DNSSECKeeper\n\n    if(!B.getDomainInfo(domain, di) || !di.backend) { // di.backend and B are mostly identical\n      L<<Logger::Error<<\"Can't determine backend for domain '\"<<domain<<\"'\"<<endl;\n      return;\n    }\n    uint32_t domain_id=di.id;\n\n\n    string tsigkeyname, tsigalgorithm, tsigsecret;\n    if(dk.getTSIGForAccess(domain, remote, &tsigkeyname)) {\n      string tsigsecret64;\n      if(B.getTSIGKey(tsigkeyname, &tsigalgorithm, &tsigsecret64)) {\n        B64Decode(tsigsecret64, tsigsecret);\n      } else {\n        L<<Logger::Error<<\"TSIG key '\"<<tsigkeyname<<\"' for domain '\"<<domain<<\"' not found\"<<endl;\n        return;\n      }\n    }\n\n\n    scoped_ptr<AuthLua> pdl;\n    vector<string> scripts;\n    if(B.getDomainMetadata(domain, \"LUA-AXFR-SCRIPT\", scripts) && !scripts.empty()) {\n      try {\n        pdl.reset(new AuthLua(scripts[0]));\n        L<<Logger::Info<<\"Loaded Lua script '\"<<scripts[0]<<\"' to edit the incoming AXFR of '\"<<domain<<\"'\"<<endl;\n      }\n      catch(std::exception& e) {\n        L<<Logger::Error<<\"Failed to load Lua editing script '\"<<scripts[0]<<\"' for incoming AXFR of '\"<<domain<<\"': \"<<e.what()<<endl;\n        return;\n      }\n    }\n\n\n    vector<string> localaddr;\n    ComboAddress laddr;\n    if(B.getDomainMetadata(domain, \"AXFR-SOURCE\", localaddr) && !localaddr.empty()) {\n      try {\n        laddr = ComboAddress(localaddr[0]);\n        L<<Logger::Info<<\"AXFR source for domain '\"<<domain<<\"' set to \"<<localaddr[0]<<endl;\n      }\n      catch(std::exception& e) {\n        L<<Logger::Error<<\"Failed to load AXFR source '\"<<localaddr[0]<<\"' for incoming AXFR of '\"<<domain<<\"': \"<<e.what()<<endl;\n        return;\n      }\n    } else {\n      laddr.sin4.sin_family = 0;\n    }\n\n\n    bool hadDnssecZone = false;\n    bool hadPresigned = false;\n    bool hadNSEC3 = false;\n    NSEC3PARAMRecordContent ns3pr, hadNs3pr;\n    bool isNarrow, hadNarrow=false;\n\n    if(dk.isSecuredZone(domain)) {\n      hadDnssecZone=true;\n      hadPresigned=dk.isPresigned(domain);\n      if (dk.getNSEC3PARAM(domain, &ns3pr, &isNarrow)) {\n        hadNSEC3 = true;\n        hadNs3pr = ns3pr;\n        hadNarrow = isNarrow;\n      }\n    }\n\n\n    bool isDnssecZone = false;\n    bool isPresigned = false;\n    bool isNSEC3 = false;\n    bool optOutFlag = false;\n\n    bool first=true;\n    bool firstNSEC3=true;\n    unsigned int soa_serial = 0;\n    set<string> nsset, qnames, secured;\n    vector<DNSResourceRecord> rrs;\n\n    ComboAddress raddr(remote, 53);\n    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr);\n    Resolver::res_t recs;\n    while(retriever.getChunk(recs)) {\n      if(first) {\n        L<<Logger::Error<<\"AXFR started for '\"<<domain<<\"'\"<<endl;\n        first=false;\n      }\n\n      for(Resolver::res_t::iterator i=recs.begin();i!=recs.end();++i) {\n        if(i->qtype.getCode() == QType::OPT || i->qtype.getCode() == QType::TSIG) // ignore EDNS0 & TSIG\n          continue;\n\n        if(!endsOn(i->qname, domain)) {\n          L<<Logger::Error<<\"Remote \"<<remote<<\" tried to sneak in out-of-zone data '\"<<i->qname<<\"'|\"<<i->qtype.getName()<<\" during AXFR of zone '\"<<domain<<\"', ignoring\"<<endl;\n          continue;\n        }\n\n        vector<DNSResourceRecord> out;\n        if(!pdl || !pdl->axfrfilter(raddr, domain, *i, out)) {\n          out.push_back(*i);\n        }\n\n        BOOST_FOREACH(DNSResourceRecord& rr, out) {\n          switch(rr.qtype.getCode()) {\n            case QType::NSEC3PARAM: {\n              ns3pr = NSEC3PARAMRecordContent(rr.content);\n              isDnssecZone = isNSEC3 = true;\n              isNarrow = false;\n              continue;\n            }\n            case QType::NSEC3: {\n              NSEC3RecordContent ns3rc(rr.content);\n              if (firstNSEC3) {\n                isDnssecZone = isPresigned = true;\n                firstNSEC3 = false;\n              } else if (optOutFlag != (ns3rc.d_flags & 1))\n                throw PDNSException(\"Zones with a mixture of Opt-Out NSEC3 RRs and non-Opt-Out NSEC3 RRs are not supported.\");\n              optOutFlag = ns3rc.d_flags & 1;\n              if (ns3rc.d_set.count(QType::NS) && !pdns_iequals(rr.qname, domain))\n                secured.insert(toLower(makeRelative(rr.qname, domain)));\n              continue;\n            }\n            case QType::NSEC: {\n              isDnssecZone = isPresigned = true;\n              continue;\n            }\n            case QType::SOA: {\n              if(soa_serial != 0)\n                continue; //skip the last SOA\n              SOAData sd;\n              fillSOAData(rr.content,sd);\n              soa_serial = sd.serial;\n              break;\n            }\n            case QType::NS: {\n              if(!pdns_iequals(rr.qname, domain))\n                nsset.insert(rr.qname);\n              break;\n            }\n            default:\n              break;\n          }\n\n          qnames.insert(rr.qname);\n\n          rr.domain_id=domain_id;\n          rrs.push_back(rr);\n        }\n      }\n    }\n\n    if(isNSEC3) {\n      ns3pr.d_flags = optOutFlag ? 1 : 0;\n    }\n\n\n    if(!isPresigned) {\n      DNSSECKeeper::keyset_t keys = dk.getKeys(domain);\n      if(!keys.empty()) {\n        isDnssecZone = true;\n        isNSEC3 = hadNSEC3;\n        ns3pr = hadNs3pr;\n        optOutFlag = (hadNs3pr.d_flags & 1);\n        isNarrow = hadNarrow;\n      }\n    }\n\n\n    if(isDnssecZone) {\n      if(!isNSEC3)\n        L<<Logger::Info<<\"Adding NSEC ordering information\"<<endl;\n      else if(!isNarrow)\n        L<<Logger::Info<<\"Adding NSEC3 hashed ordering information for '\"<<domain<<\"'\"<<endl;\n      else\n        L<<Logger::Info<<\"Erasing NSEC3 ordering since we are narrow, only setting 'auth' fields\"<<endl;\n    }\n\n\n    transaction=di.backend->startTransaction(domain, domain_id);\n    L<<Logger::Error<<\"Transaction started for '\"<<domain<<\"'\"<<endl;\n\n    // update the presigned flag and NSEC3PARAM\n    if (isDnssecZone) {\n      // update presigned if there was a change\n      if (isPresigned && !hadPresigned) {\n        // zone is now presigned\n        dk.setPresigned(domain);\n      } else if (hadPresigned && !isPresigned) {\n        // zone is no longer presigned\n        dk.unsetPresigned(domain);\n      }\n      // update NSEC3PARAM\n      if (isNSEC3) {\n        // zone is NSEC3, only update if there was a change\n        if (!hadNSEC3 || (hadNarrow  != isNarrow) ||\n            (ns3pr.d_algorithm != hadNs3pr.d_algorithm) ||\n            (ns3pr.d_flags != hadNs3pr.d_flags) ||\n            (ns3pr.d_iterations != hadNs3pr.d_iterations) ||\n            (ns3pr.d_salt != hadNs3pr.d_salt)) {\n          dk.setNSEC3PARAM(domain, ns3pr, isNarrow);\n        }\n      } else if (hadNSEC3 ) {\n         // zone is no longer NSEC3\n         dk.unsetNSEC3PARAM(domain);\n      }\n    } else if (hadDnssecZone) {\n      // zone is no longer signed\n      if (hadPresigned) {\n        // remove presigned\n        dk.unsetPresigned(domain);\n      }\n      if (hadNSEC3) {\n        // unset NSEC3PARAM\n        dk.unsetNSEC3PARAM(domain);\n      }\n    }\n\n    bool doent=true;\n    uint32_t maxent = ::arg().asNum(\"max-ent-entries\");\n    string ordername, shorter;\n    set<string> rrterm;\n    map<string,bool> nonterm;\n\n\n    BOOST_FOREACH(DNSResourceRecord& rr, rrs) {\n\n      if(!isPresigned) {\n        if (rr.qtype.getCode() == QType::RRSIG)\n          continue;\n        if(isDnssecZone && rr.qtype.getCode() == QType::DNSKEY && !::arg().mustDo(\"direct-dnskey\"))\n          continue;\n      }\n\n      // Figure out auth and ents\n      rr.auth=true;\n      shorter=rr.qname;\n      rrterm.clear();\n      do {\n        if(doent) {\n          if (!qnames.count(shorter))\n            rrterm.insert(shorter);\n        }\n        if(nsset.count(shorter) && rr.qtype.getCode() != QType::DS)\n          rr.auth=false;\n\n        if (pdns_iequals(shorter, domain)) // stop at apex\n          break;\n      }while(chopOff(shorter));\n\n      // Insert ents\n      if(doent && !rrterm.empty()) {\n        bool auth;\n        if (!rr.auth && rr.qtype.getCode() == QType::NS) {\n          if (isNSEC3)\n            ordername=toBase32Hex(hashQNameWithSalt(ns3pr.d_iterations, ns3pr.d_salt, rr.qname));\n          auth=(!isNSEC3 || !optOutFlag || secured.count(ordername));\n        } else\n          auth=rr.auth;\n\n        BOOST_FOREACH(const string nt, rrterm){\n          if (!nonterm.count(nt))\n              nonterm.insert(pair<string, bool>(nt, auth));\n            else if (auth)\n              nonterm[nt]=true;\n        }\n\n        if(nonterm.size() > maxent) {\n          L<<Logger::Error<<\"AXFR zone \"<<domain<<\" has too many empty non terminals.\"<<endl;\n          nonterm.clear();\n          doent=false;\n        }\n      }\n\n      // RRSIG is always auth, even inside a delegation\n      if (rr.qtype.getCode() == QType::RRSIG)\n        rr.auth=true;\n\n      // Add ordername and insert record\n      if (isDnssecZone && rr.qtype.getCode() != QType::RRSIG) {\n        if (isNSEC3) {\n          // NSEC3\n          ordername=toBase32Hex(hashQNameWithSalt(ns3pr.d_iterations, ns3pr.d_salt, rr.qname));\n          if(!isNarrow && (rr.auth || (rr.qtype.getCode() == QType::NS && (!optOutFlag || secured.count(ordername))))) {\n            di.backend->feedRecord(rr, &ordername);\n          } else\n            di.backend->feedRecord(rr);\n        } else {\n          // NSEC\n          if (rr.auth || rr.qtype.getCode() == QType::NS) {\n            ordername=toLower(labelReverse(makeRelative(rr.qname, domain)));\n            di.backend->feedRecord(rr, &ordername);\n          } else\n            di.backend->feedRecord(rr);\n        }\n      } else\n        di.backend->feedRecord(rr);\n    }\n\n    // Insert empty non-terminals\n    if(doent && !nonterm.empty()) {\n      if (isNSEC3) {\n        di.backend->feedEnts3(domain_id, domain, nonterm, ns3pr.d_iterations, ns3pr.d_salt, isNarrow);\n      } else\n        di.backend->feedEnts(domain_id, nonterm);\n    }\n\n    di.backend->commitTransaction();\n    transaction = false;\n    di.backend->setFresh(domain_id);\n    PC.purge(domain+\"$\");\n\n\n    L<<Logger::Error<<\"AXFR done for '\"<<domain<<\"', zone committed with serial number \"<<soa_serial<<endl;\n    if(::arg().mustDo(\"slave-renotify\"))\n      notifyDomain(domain);\n  }\n  catch(DBException &re) {\n    L<<Logger::Error<<\"Unable to feed record during incoming AXFR of '\"+domain+\"': \"<<re.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(MOADNSException &re) {\n    L<<Logger::Error<<\"Unable to parse record during incoming AXFR of '\"+domain+\"' (MOADNSException): \"<<re.what()<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(std::exception &re) {\n    L<<Logger::Error<<\"Unable to parse record during incoming AXFR of '\"+domain+\"' (std::exception): \"<<re.what()<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(ResolverException &re) {\n    L<<Logger::Error<<\"Unable to AXFR zone '\"+domain+\"' from remote '\"<<remote<<\"' (resolver): \"<<re.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(PDNSException &ae) {\n    L<<Logger::Error<<\"Unable to AXFR zone '\"+domain+\"' from remote '\"<<remote<<\"' (PDNSException): \"<<ae.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n}",
        "func": "void CommunicatorClass::suck(const string &domain,const string &remote)\n{\n  L<<Logger::Error<<\"Initiating transfer of '\"<<domain<<\"' from remote '\"<<remote<<\"'\"<<endl;\n  UeberBackend B; // fresh UeberBackend\n\n  DomainInfo di;\n  di.backend=0;\n  bool transaction=false;\n  try {\n    DNSSECKeeper dk (&B); // reuse our UeberBackend copy for DNSSECKeeper\n\n    if(!B.getDomainInfo(domain, di) || !di.backend) { // di.backend and B are mostly identical\n      L<<Logger::Error<<\"Can't determine backend for domain '\"<<domain<<\"'\"<<endl;\n      return;\n    }\n    uint32_t domain_id=di.id;\n\n\n    string tsigkeyname, tsigalgorithm, tsigsecret;\n    if(dk.getTSIGForAccess(domain, remote, &tsigkeyname)) {\n      string tsigsecret64;\n      if(B.getTSIGKey(tsigkeyname, &tsigalgorithm, &tsigsecret64)) {\n        B64Decode(tsigsecret64, tsigsecret);\n      } else {\n        L<<Logger::Error<<\"TSIG key '\"<<tsigkeyname<<\"' for domain '\"<<domain<<\"' not found\"<<endl;\n        return;\n      }\n    }\n\n\n    scoped_ptr<AuthLua> pdl;\n    vector<string> scripts;\n    if(B.getDomainMetadata(domain, \"LUA-AXFR-SCRIPT\", scripts) && !scripts.empty()) {\n      try {\n        pdl.reset(new AuthLua(scripts[0]));\n        L<<Logger::Info<<\"Loaded Lua script '\"<<scripts[0]<<\"' to edit the incoming AXFR of '\"<<domain<<\"'\"<<endl;\n      }\n      catch(std::exception& e) {\n        L<<Logger::Error<<\"Failed to load Lua editing script '\"<<scripts[0]<<\"' for incoming AXFR of '\"<<domain<<\"': \"<<e.what()<<endl;\n        return;\n      }\n    }\n\n\n    vector<string> localaddr;\n    ComboAddress laddr;\n    if(B.getDomainMetadata(domain, \"AXFR-SOURCE\", localaddr) && !localaddr.empty()) {\n      try {\n        laddr = ComboAddress(localaddr[0]);\n        L<<Logger::Info<<\"AXFR source for domain '\"<<domain<<\"' set to \"<<localaddr[0]<<endl;\n      }\n      catch(std::exception& e) {\n        L<<Logger::Error<<\"Failed to load AXFR source '\"<<localaddr[0]<<\"' for incoming AXFR of '\"<<domain<<\"': \"<<e.what()<<endl;\n        return;\n      }\n    } else {\n      laddr.sin4.sin_family = 0;\n    }\n\n\n    bool hadDnssecZone = false;\n    bool hadPresigned = false;\n    bool hadNSEC3 = false;\n    NSEC3PARAMRecordContent ns3pr, hadNs3pr;\n    bool isNarrow, hadNarrow=false;\n\n    if(dk.isSecuredZone(domain)) {\n      hadDnssecZone=true;\n      hadPresigned=dk.isPresigned(domain);\n      if (dk.getNSEC3PARAM(domain, &ns3pr, &isNarrow)) {\n        hadNSEC3 = true;\n        hadNs3pr = ns3pr;\n        hadNarrow = isNarrow;\n      }\n    }\n\n\n    bool isDnssecZone = false;\n    bool isPresigned = false;\n    bool isNSEC3 = false;\n    bool optOutFlag = false;\n\n    bool first=true;\n    bool firstNSEC3=true;\n    unsigned int soa_serial = 0;\n    set<string> nsset, qnames, secured;\n    vector<DNSResourceRecord> rrs;\n\n    ComboAddress raddr(remote, 53);\n    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n    Resolver::res_t recs;\n    while(retriever.getChunk(recs)) {\n      if(first) {\n        L<<Logger::Error<<\"AXFR started for '\"<<domain<<\"'\"<<endl;\n        first=false;\n      }\n\n      for(Resolver::res_t::iterator i=recs.begin();i!=recs.end();++i) {\n        if(i->qtype.getCode() == QType::OPT || i->qtype.getCode() == QType::TSIG) // ignore EDNS0 & TSIG\n          continue;\n\n        if(!endsOn(i->qname, domain)) {\n          L<<Logger::Error<<\"Remote \"<<remote<<\" tried to sneak in out-of-zone data '\"<<i->qname<<\"'|\"<<i->qtype.getName()<<\" during AXFR of zone '\"<<domain<<\"', ignoring\"<<endl;\n          continue;\n        }\n\n        vector<DNSResourceRecord> out;\n        if(!pdl || !pdl->axfrfilter(raddr, domain, *i, out)) {\n          out.push_back(*i);\n        }\n\n        BOOST_FOREACH(DNSResourceRecord& rr, out) {\n          switch(rr.qtype.getCode()) {\n            case QType::NSEC3PARAM: {\n              ns3pr = NSEC3PARAMRecordContent(rr.content);\n              isDnssecZone = isNSEC3 = true;\n              isNarrow = false;\n              continue;\n            }\n            case QType::NSEC3: {\n              NSEC3RecordContent ns3rc(rr.content);\n              if (firstNSEC3) {\n                isDnssecZone = isPresigned = true;\n                firstNSEC3 = false;\n              } else if (optOutFlag != (ns3rc.d_flags & 1))\n                throw PDNSException(\"Zones with a mixture of Opt-Out NSEC3 RRs and non-Opt-Out NSEC3 RRs are not supported.\");\n              optOutFlag = ns3rc.d_flags & 1;\n              if (ns3rc.d_set.count(QType::NS) && !pdns_iequals(rr.qname, domain))\n                secured.insert(toLower(makeRelative(rr.qname, domain)));\n              continue;\n            }\n            case QType::NSEC: {\n              isDnssecZone = isPresigned = true;\n              continue;\n            }\n            case QType::SOA: {\n              if(soa_serial != 0)\n                continue; //skip the last SOA\n              SOAData sd;\n              fillSOAData(rr.content,sd);\n              soa_serial = sd.serial;\n              break;\n            }\n            case QType::NS: {\n              if(!pdns_iequals(rr.qname, domain))\n                nsset.insert(rr.qname);\n              break;\n            }\n            default:\n              break;\n          }\n\n          qnames.insert(rr.qname);\n\n          rr.domain_id=domain_id;\n          rrs.push_back(rr);\n        }\n      }\n    }\n\n    if(isNSEC3) {\n      ns3pr.d_flags = optOutFlag ? 1 : 0;\n    }\n\n\n    if(!isPresigned) {\n      DNSSECKeeper::keyset_t keys = dk.getKeys(domain);\n      if(!keys.empty()) {\n        isDnssecZone = true;\n        isNSEC3 = hadNSEC3;\n        ns3pr = hadNs3pr;\n        optOutFlag = (hadNs3pr.d_flags & 1);\n        isNarrow = hadNarrow;\n      }\n    }\n\n\n    if(isDnssecZone) {\n      if(!isNSEC3)\n        L<<Logger::Info<<\"Adding NSEC ordering information\"<<endl;\n      else if(!isNarrow)\n        L<<Logger::Info<<\"Adding NSEC3 hashed ordering information for '\"<<domain<<\"'\"<<endl;\n      else\n        L<<Logger::Info<<\"Erasing NSEC3 ordering since we are narrow, only setting 'auth' fields\"<<endl;\n    }\n\n\n    transaction=di.backend->startTransaction(domain, domain_id);\n    L<<Logger::Error<<\"Transaction started for '\"<<domain<<\"'\"<<endl;\n\n    // update the presigned flag and NSEC3PARAM\n    if (isDnssecZone) {\n      // update presigned if there was a change\n      if (isPresigned && !hadPresigned) {\n        // zone is now presigned\n        dk.setPresigned(domain);\n      } else if (hadPresigned && !isPresigned) {\n        // zone is no longer presigned\n        dk.unsetPresigned(domain);\n      }\n      // update NSEC3PARAM\n      if (isNSEC3) {\n        // zone is NSEC3, only update if there was a change\n        if (!hadNSEC3 || (hadNarrow  != isNarrow) ||\n            (ns3pr.d_algorithm != hadNs3pr.d_algorithm) ||\n            (ns3pr.d_flags != hadNs3pr.d_flags) ||\n            (ns3pr.d_iterations != hadNs3pr.d_iterations) ||\n            (ns3pr.d_salt != hadNs3pr.d_salt)) {\n          dk.setNSEC3PARAM(domain, ns3pr, isNarrow);\n        }\n      } else if (hadNSEC3 ) {\n         // zone is no longer NSEC3\n         dk.unsetNSEC3PARAM(domain);\n      }\n    } else if (hadDnssecZone) {\n      // zone is no longer signed\n      if (hadPresigned) {\n        // remove presigned\n        dk.unsetPresigned(domain);\n      }\n      if (hadNSEC3) {\n        // unset NSEC3PARAM\n        dk.unsetNSEC3PARAM(domain);\n      }\n    }\n\n    bool doent=true;\n    uint32_t maxent = ::arg().asNum(\"max-ent-entries\");\n    string ordername, shorter;\n    set<string> rrterm;\n    map<string,bool> nonterm;\n\n\n    BOOST_FOREACH(DNSResourceRecord& rr, rrs) {\n\n      if(!isPresigned) {\n        if (rr.qtype.getCode() == QType::RRSIG)\n          continue;\n        if(isDnssecZone && rr.qtype.getCode() == QType::DNSKEY && !::arg().mustDo(\"direct-dnskey\"))\n          continue;\n      }\n\n      // Figure out auth and ents\n      rr.auth=true;\n      shorter=rr.qname;\n      rrterm.clear();\n      do {\n        if(doent) {\n          if (!qnames.count(shorter))\n            rrterm.insert(shorter);\n        }\n        if(nsset.count(shorter) && rr.qtype.getCode() != QType::DS)\n          rr.auth=false;\n\n        if (pdns_iequals(shorter, domain)) // stop at apex\n          break;\n      }while(chopOff(shorter));\n\n      // Insert ents\n      if(doent && !rrterm.empty()) {\n        bool auth;\n        if (!rr.auth && rr.qtype.getCode() == QType::NS) {\n          if (isNSEC3)\n            ordername=toBase32Hex(hashQNameWithSalt(ns3pr.d_iterations, ns3pr.d_salt, rr.qname));\n          auth=(!isNSEC3 || !optOutFlag || secured.count(ordername));\n        } else\n          auth=rr.auth;\n\n        BOOST_FOREACH(const string nt, rrterm){\n          if (!nonterm.count(nt))\n              nonterm.insert(pair<string, bool>(nt, auth));\n            else if (auth)\n              nonterm[nt]=true;\n        }\n\n        if(nonterm.size() > maxent) {\n          L<<Logger::Error<<\"AXFR zone \"<<domain<<\" has too many empty non terminals.\"<<endl;\n          nonterm.clear();\n          doent=false;\n        }\n      }\n\n      // RRSIG is always auth, even inside a delegation\n      if (rr.qtype.getCode() == QType::RRSIG)\n        rr.auth=true;\n\n      // Add ordername and insert record\n      if (isDnssecZone && rr.qtype.getCode() != QType::RRSIG) {\n        if (isNSEC3) {\n          // NSEC3\n          ordername=toBase32Hex(hashQNameWithSalt(ns3pr.d_iterations, ns3pr.d_salt, rr.qname));\n          if(!isNarrow && (rr.auth || (rr.qtype.getCode() == QType::NS && (!optOutFlag || secured.count(ordername))))) {\n            di.backend->feedRecord(rr, &ordername);\n          } else\n            di.backend->feedRecord(rr);\n        } else {\n          // NSEC\n          if (rr.auth || rr.qtype.getCode() == QType::NS) {\n            ordername=toLower(labelReverse(makeRelative(rr.qname, domain)));\n            di.backend->feedRecord(rr, &ordername);\n          } else\n            di.backend->feedRecord(rr);\n        }\n      } else\n        di.backend->feedRecord(rr);\n    }\n\n    // Insert empty non-terminals\n    if(doent && !nonterm.empty()) {\n      if (isNSEC3) {\n        di.backend->feedEnts3(domain_id, domain, nonterm, ns3pr.d_iterations, ns3pr.d_salt, isNarrow);\n      } else\n        di.backend->feedEnts(domain_id, nonterm);\n    }\n\n    di.backend->commitTransaction();\n    transaction = false;\n    di.backend->setFresh(domain_id);\n    PC.purge(domain+\"$\");\n\n\n    L<<Logger::Error<<\"AXFR done for '\"<<domain<<\"', zone committed with serial number \"<<soa_serial<<endl;\n    if(::arg().mustDo(\"slave-renotify\"))\n      notifyDomain(domain);\n  }\n  catch(DBException &re) {\n    L<<Logger::Error<<\"Unable to feed record during incoming AXFR of '\"+domain+\"': \"<<re.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(MOADNSException &re) {\n    L<<Logger::Error<<\"Unable to parse record during incoming AXFR of '\"+domain+\"' (MOADNSException): \"<<re.what()<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(std::exception &re) {\n    L<<Logger::Error<<\"Unable to parse record during incoming AXFR of '\"+domain+\"' (std::exception): \"<<re.what()<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(ResolverException &re) {\n    L<<Logger::Error<<\"Unable to AXFR zone '\"+domain+\"' from remote '\"<<remote<<\"' (resolver): \"<<re.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n  catch(PDNSException &ae) {\n    L<<Logger::Error<<\"Unable to AXFR zone '\"+domain+\"' from remote '\"<<remote<<\"' (PDNSException): \"<<ae.reason<<endl;\n    if(di.backend && transaction) {\n      L<<Logger::Error<<\"Aborting possible open transaction for domain '\"<<domain<<\"' AXFR\"<<endl;\n      di.backend->abortTransaction();\n    }\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -87,7 +87,7 @@\n     vector<DNSResourceRecord> rrs;\n \n     ComboAddress raddr(remote, 53);\n-    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr);\n+    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n     Resolver::res_t recs;\n     while(retriever.getChunk(recs)) {\n       if(first) {",
        "diff_line_info": {
            "deleted_lines": [
                "    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr);"
            ],
            "added_lines": [
                "    AXFRRetriever retriever(raddr, domain.c_str(), tsigkeyname, tsigalgorithm, tsigsecret, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/AXFRRetriever::AXFRRetriever",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/a014f4c224a7b21f1c648257d1fd1128413129aa",
        "commit_title": "Add limits to the size of received AXFR, in megabytes",
        "commit_text": " This prevents resource exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "AXFRRetriever::AXFRRetriever(const ComboAddress& remote,\n        const string& domain,\n        const string& tsigkeyname,\n        const string& tsigalgorithm, \n        const string& tsigsecret,\n        const ComboAddress* laddr)\n: d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_tsigPos(0), d_nonSignedMessages(0)\n{\n  ComboAddress local;\n  if (laddr != NULL) {\n    local = (ComboAddress) (*laddr);\n  } else {\n    if(remote.sin4.sin_family == AF_INET)\n      local=ComboAddress(::arg()[\"query-local-address\"]);\n    else if(!::arg()[\"query-local-address6\"].empty())\n      local=ComboAddress(::arg()[\"query-local-address6\"]);\n    else\n      local=ComboAddress(\"::\");\n  }\n  d_sock = -1;\n  try {\n    d_sock = makeQuerySocket(local, false); // make a TCP socket\n    d_buf = shared_array<char>(new char[65536]);\n    d_remote = remote; // mostly for error reporting\n    this->connect();\n    d_soacount = 0;\n  \n    vector<uint8_t> packet;\n    DNSPacketWriter pw(packet, domain, QType::AXFR);\n    pw.getHeader()->id = dns_random(0xffff);\n  \n    if(!tsigkeyname.empty()) {\n      if (tsigalgorithm == \"hmac-md5\")\n        d_trc.d_algoName = tsigalgorithm + \".sig-alg.reg.int.\";\n      else\n        d_trc.d_algoName = tsigalgorithm;\n      d_trc.d_time = time(0);\n      d_trc.d_fudge = 300;\n      d_trc.d_origID=ntohs(pw.getHeader()->id);\n      d_trc.d_eRcode=0;\n      addTSIG(pw, &d_trc, tsigkeyname, tsigsecret, \"\", false);\n    }\n  \n    uint16_t replen=htons(packet.size());\n    Utility::iovec iov[2];\n    iov[0].iov_base=(char*)&replen;\n    iov[0].iov_len=2;\n    iov[1].iov_base=(char*)&packet[0];\n    iov[1].iov_len=packet.size();\n  \n    int ret=Utility::writev(d_sock, iov, 2);\n    if(ret < 0)\n      throw ResolverException(\"Error sending question to \"+d_remote.toStringWithPort()+\": \"+stringerror());\n    if(ret != (int)(2+packet.size())) {\n      throw ResolverException(\"Partial write on AXFR request to \"+d_remote.toStringWithPort());\n    }\n  \n    int res = waitForData(d_sock, 10, 0);\n    \n    if(!res)\n      throw ResolverException(\"Timeout waiting for answer from \"+d_remote.toStringWithPort()+\" during AXFR\");\n    if(res<0)\n      throw ResolverException(\"Error waiting for answer from \"+d_remote.toStringWithPort()+\": \"+stringerror());\n  }\n  catch(...) {\n    if(d_sock >= 0)\n      close(d_sock);\n    throw;\n  }\n}",
        "func": "AXFRRetriever::AXFRRetriever(const ComboAddress& remote,\n        const string& domain,\n        const string& tsigkeyname,\n        const string& tsigalgorithm, \n        const string& tsigsecret,\n        const ComboAddress* laddr,\n        size_t maxReceivedBytes)\n  : d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)\n{\n  ComboAddress local;\n  if (laddr != NULL) {\n    local = (ComboAddress) (*laddr);\n  } else {\n    if(remote.sin4.sin_family == AF_INET)\n      local=ComboAddress(::arg()[\"query-local-address\"]);\n    else if(!::arg()[\"query-local-address6\"].empty())\n      local=ComboAddress(::arg()[\"query-local-address6\"]);\n    else\n      local=ComboAddress(\"::\");\n  }\n  d_sock = -1;\n  try {\n    d_sock = makeQuerySocket(local, false); // make a TCP socket\n    d_buf = shared_array<char>(new char[65536]);\n    d_remote = remote; // mostly for error reporting\n    this->connect();\n    d_soacount = 0;\n  \n    vector<uint8_t> packet;\n    DNSPacketWriter pw(packet, domain, QType::AXFR);\n    pw.getHeader()->id = dns_random(0xffff);\n  \n    if(!tsigkeyname.empty()) {\n      if (tsigalgorithm == \"hmac-md5\")\n        d_trc.d_algoName = tsigalgorithm + \".sig-alg.reg.int.\";\n      else\n        d_trc.d_algoName = tsigalgorithm;\n      d_trc.d_time = time(0);\n      d_trc.d_fudge = 300;\n      d_trc.d_origID=ntohs(pw.getHeader()->id);\n      d_trc.d_eRcode=0;\n      addTSIG(pw, &d_trc, tsigkeyname, tsigsecret, \"\", false);\n    }\n  \n    uint16_t replen=htons(packet.size());\n    Utility::iovec iov[2];\n    iov[0].iov_base=(char*)&replen;\n    iov[0].iov_len=2;\n    iov[1].iov_base=(char*)&packet[0];\n    iov[1].iov_len=packet.size();\n  \n    int ret=Utility::writev(d_sock, iov, 2);\n    if(ret < 0)\n      throw ResolverException(\"Error sending question to \"+d_remote.toStringWithPort()+\": \"+stringerror());\n    if(ret != (int)(2+packet.size())) {\n      throw ResolverException(\"Partial write on AXFR request to \"+d_remote.toStringWithPort());\n    }\n  \n    int res = waitForData(d_sock, 10, 0);\n    \n    if(!res)\n      throw ResolverException(\"Timeout waiting for answer from \"+d_remote.toStringWithPort()+\" during AXFR\");\n    if(res<0)\n      throw ResolverException(\"Error waiting for answer from \"+d_remote.toStringWithPort()+\": \"+stringerror());\n  }\n  catch(...) {\n    if(d_sock >= 0)\n      close(d_sock);\n    throw;\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,8 +3,9 @@\n         const string& tsigkeyname,\n         const string& tsigalgorithm, \n         const string& tsigsecret,\n-        const ComboAddress* laddr)\n-: d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_tsigPos(0), d_nonSignedMessages(0)\n+        const ComboAddress* laddr,\n+        size_t maxReceivedBytes)\n+  : d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)\n {\n   ComboAddress local;\n   if (laddr != NULL) {",
        "diff_line_info": {
            "deleted_lines": [
                "        const ComboAddress* laddr)",
                ": d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_tsigPos(0), d_nonSignedMessages(0)"
            ],
            "added_lines": [
                "        const ComboAddress* laddr,",
                "        size_t maxReceivedBytes)",
                "  : d_tsigkeyname(tsigkeyname), d_tsigsecret(tsigsecret), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/AXFRRetriever::getChunk",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/a014f4c224a7b21f1c648257d1fd1128413129aa",
        "commit_title": "Add limits to the size of received AXFR, in megabytes",
        "commit_text": " This prevents resource exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "int AXFRRetriever::getChunk(Resolver::res_t &res) // Implementation is making sure RFC2845 4.4 is followed.\n{\n  if(d_soacount > 1)\n    return false;\n\n  // d_sock is connected and is about to spit out a packet\n  int len=getLength();\n  if(len<0)\n    throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n  \n  timeoutReadn(len); \n  MOADNSParser mdp(d_buf.get(), len);\n\n  int err = parseResult(mdp, \"\", 0, 0, &res);\n  if(err) \n    throw ResolverException(\"AXFR chunk error: \" + RCode::to_s(err));\n\n  BOOST_FOREACH(const MOADNSParser::answers_t::value_type& answer, mdp.d_answers)\n    if (answer.first.d_type == QType::SOA)\n      d_soacount++;\n \n  if(!d_tsigkeyname.empty()) { // TSIG verify message\n    // If we have multiple messages, we need to concatenate them together. We also need to make sure we know the location of \n    // the TSIG record so we can remove it in makeTSIGMessageFromTSIGPacket\n    d_signData.append(d_buf.get(), len);\n    if (mdp.getTSIGPos() == 0)\n      d_tsigPos += len;\n    else \n      d_tsigPos += mdp.getTSIGPos();\n\n    string theirMac;\n    bool checkTSIG = false;\n    \n    BOOST_FOREACH(const MOADNSParser::answers_t::value_type& answer, mdp.d_answers) {\n      if (answer.first.d_type == QType::SOA)  // A SOA is either the first or the last record. We need to check TSIG if that's the case.\n        checkTSIG = true;\n      \n      if(answer.first.d_type == QType::TSIG) {\n        shared_ptr<TSIGRecordContent> trc = boost::dynamic_pointer_cast<TSIGRecordContent>(answer.first.d_content);\n        theirMac = trc->d_mac;\n        d_trc.d_time = trc->d_time;\n        checkTSIG = true;\n      }\n    }\n\n    if( ! checkTSIG && d_nonSignedMessages > 99) { // We're allowed to get 100 digest without a TSIG.\n      throw ResolverException(\"No TSIG message received in last 100 messages of AXFR transfer.\");\n    }\n\n    if (checkTSIG) {\n      if (theirMac.empty())\n        throw ResolverException(\"No TSIG on AXFR response from \"+d_remote.toStringWithPort()+\" , should be signed with TSIG key '\"+d_tsigkeyname+\"'\");\n\n      string message;\n      if (!d_prevMac.empty()) {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tsigkeyname, d_trc, d_prevMac, true, d_signData.size()-len);\n      } else {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tsigkeyname, d_trc, d_trc.d_mac, false);\n      }\n\n      TSIGHashEnum algo;\n      if (!getTSIGHashEnum(d_trc.d_algoName, algo)) {\n        throw ResolverException(\"Unsupported TSIG HMAC algorithm \" + d_trc.d_algoName);\n      }\n\n      string ourMac=calculateHMAC(d_tsigsecret, message, algo);\n\n      // ourMac[0]++; // sabotage == for testing :-)\n      if(ourMac != theirMac) {\n        throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tsigkeyname+\"'\");\n      }\n\n      // Reset and store some values for the next chunks. \n      d_prevMac = theirMac;\n      d_nonSignedMessages = 0;\n      d_signData.clear();\n      d_tsigPos = 0;\n    }\n    else\n      d_nonSignedMessages++;\n  }\n  \n  return true;\n}",
        "func": "int AXFRRetriever::getChunk(Resolver::res_t &res) // Implementation is making sure RFC2845 4.4 is followed.\n{\n  if(d_soacount > 1)\n    return false;\n\n  // d_sock is connected and is about to spit out a packet\n  int len=getLength();\n  if(len<0)\n    throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n\n  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)\n    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");\n\n  timeoutReadn(len);\n\n  d_receivedBytes += (uint16_t) len;\n\n  MOADNSParser mdp(d_buf.get(), len);\n\n  int err = parseResult(mdp, \"\", 0, 0, &res);\n  if(err) \n    throw ResolverException(\"AXFR chunk error: \" + RCode::to_s(err));\n\n  BOOST_FOREACH(const MOADNSParser::answers_t::value_type& answer, mdp.d_answers)\n    if (answer.first.d_type == QType::SOA)\n      d_soacount++;\n \n  if(!d_tsigkeyname.empty()) { // TSIG verify message\n    // If we have multiple messages, we need to concatenate them together. We also need to make sure we know the location of \n    // the TSIG record so we can remove it in makeTSIGMessageFromTSIGPacket\n    d_signData.append(d_buf.get(), len);\n    if (mdp.getTSIGPos() == 0)\n      d_tsigPos += len;\n    else \n      d_tsigPos += mdp.getTSIGPos();\n\n    string theirMac;\n    bool checkTSIG = false;\n    \n    BOOST_FOREACH(const MOADNSParser::answers_t::value_type& answer, mdp.d_answers) {\n      if (answer.first.d_type == QType::SOA)  // A SOA is either the first or the last record. We need to check TSIG if that's the case.\n        checkTSIG = true;\n      \n      if(answer.first.d_type == QType::TSIG) {\n        shared_ptr<TSIGRecordContent> trc = boost::dynamic_pointer_cast<TSIGRecordContent>(answer.first.d_content);\n        theirMac = trc->d_mac;\n        d_trc.d_time = trc->d_time;\n        checkTSIG = true;\n      }\n    }\n\n    if( ! checkTSIG && d_nonSignedMessages > 99) { // We're allowed to get 100 digest without a TSIG.\n      throw ResolverException(\"No TSIG message received in last 100 messages of AXFR transfer.\");\n    }\n\n    if (checkTSIG) {\n      if (theirMac.empty())\n        throw ResolverException(\"No TSIG on AXFR response from \"+d_remote.toStringWithPort()+\" , should be signed with TSIG key '\"+d_tsigkeyname+\"'\");\n\n      string message;\n      if (!d_prevMac.empty()) {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tsigkeyname, d_trc, d_prevMac, true, d_signData.size()-len);\n      } else {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tsigkeyname, d_trc, d_trc.d_mac, false);\n      }\n\n      TSIGHashEnum algo;\n      if (!getTSIGHashEnum(d_trc.d_algoName, algo)) {\n        throw ResolverException(\"Unsupported TSIG HMAC algorithm \" + d_trc.d_algoName);\n      }\n\n      string ourMac=calculateHMAC(d_tsigsecret, message, algo);\n\n      // ourMac[0]++; // sabotage == for testing :-)\n      if(ourMac != theirMac) {\n        throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tsigkeyname+\"'\");\n      }\n\n      // Reset and store some values for the next chunks. \n      d_prevMac = theirMac;\n      d_nonSignedMessages = 0;\n      d_signData.clear();\n      d_tsigPos = 0;\n    }\n    else\n      d_nonSignedMessages++;\n  }\n  \n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,8 +7,14 @@\n   int len=getLength();\n   if(len<0)\n     throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n-  \n-  timeoutReadn(len); \n+\n+  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)\n+    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");\n+\n+  timeoutReadn(len);\n+\n+  d_receivedBytes += (uint16_t) len;\n+\n   MOADNSParser mdp(d_buf.get(), len);\n \n   int err = parseResult(mdp, \"\", 0, 0, &res);",
        "diff_line_info": {
            "deleted_lines": [
                "  ",
                "  timeoutReadn(len); "
            ],
            "added_lines": [
                "",
                "  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)",
                "    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");",
                "",
                "  timeoutReadn(len);",
                "",
                "  d_receivedBytes += (uint16_t) len;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/doAxfr",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "vector<DNSResourceRecord> doAxfr(const ComboAddress& raddr, const DNSName& domain, const TSIGTriplet& tt, const ComboAddress& laddr,  scoped_ptr<AuthLua>& pdl, ZoneStatus& zs)\n{\n  vector<DNSResourceRecord> rrs;\n  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr);\n  Resolver::res_t recs;\n  bool first=true;\n  bool firstNSEC3{true};\n  bool soa_received {false};\n  while(retriever.getChunk(recs)) {\n    if(first) {\n      L<<Logger::Error<<\"AXFR started for '\"<<domain<<\"'\"<<endl;\n      first=false;\n    }\n\n    for(Resolver::res_t::iterator i=recs.begin();i!=recs.end();++i) {\n      if(i->qtype.getCode() == QType::OPT || i->qtype.getCode() == QType::TSIG) // ignore EDNS0 & TSIG\n        continue;\n\n      if(!i->qname.isPartOf(domain)) {\n        L<<Logger::Error<<\"Remote \"<<raddr.toStringWithPort()<<\" tried to sneak in out-of-zone data '\"<<i->qname<<\"'|\"<<i->qtype.getName()<<\" during AXFR of zone '\"<<domain<<\"', ignoring\"<<endl;\n        continue;\n      }\n\n      vector<DNSResourceRecord> out;\n      if(!pdl || !pdl->axfrfilter(raddr, domain, *i, out)) {\n        out.push_back(*i); // if axfrfilter didn't do anything, we put our record in 'out' ourselves\n      }\n\n      for(DNSResourceRecord& rr :  out) {\n        if(!processRecordForZS(domain, firstNSEC3, rr, zs))\n          continue;\n        if(rr.qtype.getCode() == QType::SOA) {\n          if(soa_received)\n            continue; //skip the last SOA\n          SOAData sd;\n          fillSOAData(rr.content,sd);\n          zs.soa_serial = sd.serial;\n          soa_received = true;\n        }\n\n        rrs.push_back(rr);\n\n      }\n    }\n  }\n  return rrs;\n}",
        "func": "vector<DNSResourceRecord> doAxfr(const ComboAddress& raddr, const DNSName& domain, const TSIGTriplet& tt, const ComboAddress& laddr,  scoped_ptr<AuthLua>& pdl, ZoneStatus& zs)\n{\n  vector<DNSResourceRecord> rrs;\n  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n  Resolver::res_t recs;\n  bool first=true;\n  bool firstNSEC3{true};\n  bool soa_received {false};\n  while(retriever.getChunk(recs)) {\n    if(first) {\n      L<<Logger::Error<<\"AXFR started for '\"<<domain<<\"'\"<<endl;\n      first=false;\n    }\n\n    for(Resolver::res_t::iterator i=recs.begin();i!=recs.end();++i) {\n      if(i->qtype.getCode() == QType::OPT || i->qtype.getCode() == QType::TSIG) // ignore EDNS0 & TSIG\n        continue;\n\n      if(!i->qname.isPartOf(domain)) {\n        L<<Logger::Error<<\"Remote \"<<raddr.toStringWithPort()<<\" tried to sneak in out-of-zone data '\"<<i->qname<<\"'|\"<<i->qtype.getName()<<\" during AXFR of zone '\"<<domain<<\"', ignoring\"<<endl;\n        continue;\n      }\n\n      vector<DNSResourceRecord> out;\n      if(!pdl || !pdl->axfrfilter(raddr, domain, *i, out)) {\n        out.push_back(*i); // if axfrfilter didn't do anything, we put our record in 'out' ourselves\n      }\n\n      for(DNSResourceRecord& rr :  out) {\n        if(!processRecordForZS(domain, firstNSEC3, rr, zs))\n          continue;\n        if(rr.qtype.getCode() == QType::SOA) {\n          if(soa_received)\n            continue; //skip the last SOA\n          SOAData sd;\n          fillSOAData(rr.content,sd);\n          zs.soa_serial = sd.serial;\n          soa_received = true;\n        }\n\n        rrs.push_back(rr);\n\n      }\n    }\n  }\n  return rrs;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n vector<DNSResourceRecord> doAxfr(const ComboAddress& raddr, const DNSName& domain, const TSIGTriplet& tt, const ComboAddress& laddr,  scoped_ptr<AuthLua>& pdl, ZoneStatus& zs)\n {\n   vector<DNSResourceRecord> rrs;\n-  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr);\n+  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n   Resolver::res_t recs;\n   bool first=true;\n   bool firstNSEC3{true};",
        "diff_line_info": {
            "deleted_lines": [
                "  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr);"
            ],
            "added_lines": [
                "  AXFRRetriever retriever(raddr, domain, tt, (laddr.sin4.sin_family == 0) ? NULL : &laddr, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/CommunicatorClass::ixfrSuck",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "void CommunicatorClass::ixfrSuck(const DNSName &domain, const TSIGTriplet& tt, const ComboAddress& laddr, const ComboAddress& remote, scoped_ptr<AuthLua>& pdl,\n                                 ZoneStatus& zs, vector<DNSRecord>* axfr)\n{\n  UeberBackend B; // fresh UeberBackend\n\n  DomainInfo di;\n  di.backend=0;\n  //  bool transaction=false;\n  try {\n    DNSSECKeeper dk (&B); // reuse our UeberBackend copy for DNSSECKeeper\n\n    if(!B.getDomainInfo(domain, di) || !di.backend) { // di.backend and B are mostly identical\n      L<<Logger::Error<<\"Can't determine backend for domain '\"<<domain<<\"'\"<<endl;\n      return;\n    }\n\n    soatimes st;\n    memset(&st, 0, sizeof(st));\n    st.serial=di.serial;\n\n    DNSRecord dr;\n    dr.d_content = std::make_shared<SOARecordContent>(DNSName(\".\"), DNSName(\".\"), st);\n    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0);\n    zs.numDeltas=deltas.size();\n    //    cout<<\"Got \"<<deltas.size()<<\" deltas from serial \"<<di.serial<<\", applying..\"<<endl;\n    \n    for(const auto& d : deltas) {\n      const auto& remove = d.first;\n      const auto& add = d.second;\n      //      cout<<\"Delta sizes: \"<<remove.size()<<\", \"<<add.size()<<endl;\n      \n      if(remove.empty()) { // we got passed an AXFR!\n        *axfr = add;\n        return;\n      }\n        \n\n      // our hammer is 'replaceRRSet(domain_id, qname, qt, vector<DNSResourceRecord>& rrset)\n      // which thinks in terms of RRSETs\n      // however, IXFR does not, and removes and adds *records* (bummer)\n      // this means that we must group updates by {qname,qtype}, retrieve the RRSET, apply\n      // the add/remove updates, and replaceRRSet the whole thing. \n      \n      \n      map<pair<DNSName,uint16_t>, pair<vector<DNSRecord>, vector<DNSRecord> > > grouped;\n      \n      for(const auto& x: remove)\n        grouped[{x.d_name, x.d_type}].first.push_back(x);\n      for(const auto& x: add)\n        grouped[{x.d_name, x.d_type}].second.push_back(x);\n\n      di.backend->startTransaction(domain, -1);\n      for(const auto g : grouped) {\n        DNSResourceRecord rr;\n        vector<DNSRecord> rrset;\n        B.lookup(QType(g.first.second), g.first.first, 0, di.id);\n        while(B.get(rr)) {\n          rrset.push_back(DNSRecord{rr});\n        }\n        // O(N^2)!\n        rrset.erase(remove_if(rrset.begin(), rrset.end(), \n                              [&g](const DNSRecord& dr) {\n                                return count(g.second.first.cbegin(), \n                                             g.second.first.cend(), dr);\n                              }), rrset.end());\n        // the DNSRecord== operator compares on name, type, class and lowercase content representation\n\n        for(const auto& x : g.second.second) {\n          rrset.push_back(x);\n        }\n\n        vector<DNSResourceRecord> replacement;\n        for(const auto& x : rrset) {\n          DNSResourceRecord dr(x);\n          dr.qname += domain;\n          dr.domain_id = di.id;\n          if(x.d_type == QType::SOA) {\n            //            cout<<\"New SOA: \"<<x.d_content->getZoneRepresentation()<<endl;\n            auto sr = getRR<SOARecordContent>(x);\n            zs.soa_serial=sr->d_st.serial;\n          }\n          \n          replacement.push_back(dr);\n        }\n\n        di.backend->replaceRRSet(di.id, g.first.first+domain, QType(g.first.second), replacement);\n      }\n      di.backend->commitTransaction();\n    }\n  }\n  catch(std::exception& p) {\n    L<<Logger::Error<<\"Got exception during IXFR: \"<<p.what()<<endl;\n    throw;\n  }\n  catch(PDNSException& p) {\n    L<<Logger::Error<<\"Got exception during IXFR: \"<<p.reason<<endl;\n    throw;\n  }  \n}",
        "func": "void CommunicatorClass::ixfrSuck(const DNSName &domain, const TSIGTriplet& tt, const ComboAddress& laddr, const ComboAddress& remote, scoped_ptr<AuthLua>& pdl,\n                                 ZoneStatus& zs, vector<DNSRecord>* axfr)\n{\n  UeberBackend B; // fresh UeberBackend\n\n  DomainInfo di;\n  di.backend=0;\n  //  bool transaction=false;\n  try {\n    DNSSECKeeper dk (&B); // reuse our UeberBackend copy for DNSSECKeeper\n\n    if(!B.getDomainInfo(domain, di) || !di.backend) { // di.backend and B are mostly identical\n      L<<Logger::Error<<\"Can't determine backend for domain '\"<<domain<<\"'\"<<endl;\n      return;\n    }\n\n    soatimes st;\n    memset(&st, 0, sizeof(st));\n    st.serial=di.serial;\n\n    DNSRecord dr;\n    dr.d_content = std::make_shared<SOARecordContent>(DNSName(\".\"), DNSName(\".\"), st);\n    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n    zs.numDeltas=deltas.size();\n    //    cout<<\"Got \"<<deltas.size()<<\" deltas from serial \"<<di.serial<<\", applying..\"<<endl;\n    \n    for(const auto& d : deltas) {\n      const auto& remove = d.first;\n      const auto& add = d.second;\n      //      cout<<\"Delta sizes: \"<<remove.size()<<\", \"<<add.size()<<endl;\n      \n      if(remove.empty()) { // we got passed an AXFR!\n        *axfr = add;\n        return;\n      }\n        \n\n      // our hammer is 'replaceRRSet(domain_id, qname, qt, vector<DNSResourceRecord>& rrset)\n      // which thinks in terms of RRSETs\n      // however, IXFR does not, and removes and adds *records* (bummer)\n      // this means that we must group updates by {qname,qtype}, retrieve the RRSET, apply\n      // the add/remove updates, and replaceRRSet the whole thing. \n      \n      \n      map<pair<DNSName,uint16_t>, pair<vector<DNSRecord>, vector<DNSRecord> > > grouped;\n      \n      for(const auto& x: remove)\n        grouped[{x.d_name, x.d_type}].first.push_back(x);\n      for(const auto& x: add)\n        grouped[{x.d_name, x.d_type}].second.push_back(x);\n\n      di.backend->startTransaction(domain, -1);\n      for(const auto g : grouped) {\n        DNSResourceRecord rr;\n        vector<DNSRecord> rrset;\n        B.lookup(QType(g.first.second), g.first.first, 0, di.id);\n        while(B.get(rr)) {\n          rrset.push_back(DNSRecord{rr});\n        }\n        // O(N^2)!\n        rrset.erase(remove_if(rrset.begin(), rrset.end(), \n                              [&g](const DNSRecord& dr) {\n                                return count(g.second.first.cbegin(), \n                                             g.second.first.cend(), dr);\n                              }), rrset.end());\n        // the DNSRecord== operator compares on name, type, class and lowercase content representation\n\n        for(const auto& x : g.second.second) {\n          rrset.push_back(x);\n        }\n\n        vector<DNSResourceRecord> replacement;\n        for(const auto& x : rrset) {\n          DNSResourceRecord dr(x);\n          dr.qname += domain;\n          dr.domain_id = di.id;\n          if(x.d_type == QType::SOA) {\n            //            cout<<\"New SOA: \"<<x.d_content->getZoneRepresentation()<<endl;\n            auto sr = getRR<SOARecordContent>(x);\n            zs.soa_serial=sr->d_st.serial;\n          }\n          \n          replacement.push_back(dr);\n        }\n\n        di.backend->replaceRRSet(di.id, g.first.first+domain, QType(g.first.second), replacement);\n      }\n      di.backend->commitTransaction();\n    }\n  }\n  catch(std::exception& p) {\n    L<<Logger::Error<<\"Got exception during IXFR: \"<<p.what()<<endl;\n    throw;\n  }\n  catch(PDNSException& p) {\n    L<<Logger::Error<<\"Got exception during IXFR: \"<<p.reason<<endl;\n    throw;\n  }  \n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,7 +20,7 @@\n \n     DNSRecord dr;\n     dr.d_content = std::make_shared<SOARecordContent>(DNSName(\".\"), DNSName(\".\"), st);\n-    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0);\n+    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);\n     zs.numDeltas=deltas.size();\n     //    cout<<\"Got \"<<deltas.size()<<\" deltas from serial \"<<di.serial<<\", applying..\"<<endl;\n     ",
        "diff_line_info": {
            "deleted_lines": [
                "    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0);"
            ],
            "added_lines": [
                "    auto deltas = getIXFRDeltas(remote, domain, dr, tt, laddr.sin4.sin_family ? &laddr : 0, ((size_t) ::arg().asNum(\"xfr-max-received-mbytes\")) * 1024 * 1024);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/RPZIXFRTracker",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr)\n{\n  int refresh = oursr->d_st.refresh;\n  for(;;) {\n    DNSRecord dr;\n    dr.d_content=oursr;\n\n    sleep(refresh);\n    \n    L<<Logger::Info<<\"Getting IXFR deltas for \"<<zone<<\" from \"<<master.toStringWithPort()<<\", our serial: \"<<getRR<SOARecordContent>(dr)->d_st.serial<<endl;\n    vector<pair<vector<DNSRecord>, vector<DNSRecord> > > deltas;\n    try {\n      deltas = getIXFRDeltas(master, zone, dr, tt);\n    } catch(std::runtime_error& e ){\n      L<<Logger::Warning<<e.what()<<endl;\n      continue;\n    }\n    if(deltas.empty())\n      continue;\n    L<<Logger::Info<<\"Processing \"<<deltas.size()<<\" delta\"<<addS(deltas)<<\" for RPZ \"<<zone<<endl;\n\n    auto luaconfsCopy = g_luaconfs.getCopy();\n    int totremove=0, totadd=0;\n    for(const auto& delta : deltas) {\n      const auto& remove = delta.first;\n      const auto& add = delta.second;\n      if(remove.empty()) {\n        L<<Logger::Warning<<\"IXFR update is a whole new zone\"<<endl;\n        luaconfsCopy.dfe.clear(0);\n      }\n      for(const auto& rr : remove) { // should always contain the SOA\n\ttotremove++;\n\tif(rr.d_type == QType::SOA) {\n\t  auto oldsr = getRR<SOARecordContent>(rr);\n\t  if(oldsr && oldsr->d_st.serial == oursr->d_st.serial) {\n\t    //\t    cout<<\"Got good removal of SOA serial \"<<oldsr->d_st.serial<<endl;\n\t  }\n\t  else\n\t    L<<Logger::Error<<\"GOT WRONG SOA SERIAL REMOVAL, SHOULD TRIGGER WHOLE RELOAD\"<<endl;\n\t}\n\telse {\n\t  L<<Logger::Info<<\"Had removal of \"<<rr.d_name<<endl;\n\t  RPZRecordToPolicy(rr, luaconfsCopy.dfe, polName, false, boost::optional<DNSFilterEngine::Policy>(), 0);\n\t}\n      }\n\n      for(const auto& rr : add) { // should always contain the new SOA\n\ttotadd++;\n\tif(rr.d_type == QType::SOA) {\n\t  auto newsr = getRR<SOARecordContent>(rr);\n\t  //\t  L<<Logger::Info<<\"New SOA serial for \"<<zone<<\": \"<<newsr->d_st.serial<<endl;\n\t  if (newsr) {\n\t    oursr = newsr;\n\t  }\n\t}\n\telse {\n\t  L<<Logger::Info<<\"Had addition of \"<<rr.d_name<<endl;\n\t  RPZRecordToPolicy(rr, luaconfsCopy.dfe, polName, true, boost::optional<DNSFilterEngine::Policy>(), 0);\n\t}\n      }\n    }\n    L<<Logger::Info<<\"Had \"<<totremove<<\" RPZ removal\"<<addS(totremove)<<\", \"<<totadd<<\" addition\"<<addS(totadd)<<\" for \"<<zone<<\" New serial: \"<<oursr->d_st.serial<<endl;\n    g_luaconfs.setState(luaconfsCopy);\n  }\n}",
        "func": "void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr, size_t maxReceivedBytes)\n{\n  int refresh = oursr->d_st.refresh;\n  for(;;) {\n    DNSRecord dr;\n    dr.d_content=oursr;\n\n    sleep(refresh);\n    \n    L<<Logger::Info<<\"Getting IXFR deltas for \"<<zone<<\" from \"<<master.toStringWithPort()<<\", our serial: \"<<getRR<SOARecordContent>(dr)->d_st.serial<<endl;\n    vector<pair<vector<DNSRecord>, vector<DNSRecord> > > deltas;\n    try {\n      deltas = getIXFRDeltas(master, zone, dr, tt, nullptr, maxReceivedBytes);\n    } catch(std::runtime_error& e ){\n      L<<Logger::Warning<<e.what()<<endl;\n      continue;\n    }\n    if(deltas.empty())\n      continue;\n    L<<Logger::Info<<\"Processing \"<<deltas.size()<<\" delta\"<<addS(deltas)<<\" for RPZ \"<<zone<<endl;\n\n    auto luaconfsCopy = g_luaconfs.getCopy();\n    int totremove=0, totadd=0;\n    for(const auto& delta : deltas) {\n      const auto& remove = delta.first;\n      const auto& add = delta.second;\n      if(remove.empty()) {\n        L<<Logger::Warning<<\"IXFR update is a whole new zone\"<<endl;\n        luaconfsCopy.dfe.clear(0);\n      }\n      for(const auto& rr : remove) { // should always contain the SOA\n\ttotremove++;\n\tif(rr.d_type == QType::SOA) {\n\t  auto oldsr = getRR<SOARecordContent>(rr);\n\t  if(oldsr && oldsr->d_st.serial == oursr->d_st.serial) {\n\t    //\t    cout<<\"Got good removal of SOA serial \"<<oldsr->d_st.serial<<endl;\n\t  }\n\t  else\n\t    L<<Logger::Error<<\"GOT WRONG SOA SERIAL REMOVAL, SHOULD TRIGGER WHOLE RELOAD\"<<endl;\n\t}\n\telse {\n\t  L<<Logger::Info<<\"Had removal of \"<<rr.d_name<<endl;\n\t  RPZRecordToPolicy(rr, luaconfsCopy.dfe, polName, false, boost::optional<DNSFilterEngine::Policy>(), 0);\n\t}\n      }\n\n      for(const auto& rr : add) { // should always contain the new SOA\n\ttotadd++;\n\tif(rr.d_type == QType::SOA) {\n\t  auto newsr = getRR<SOARecordContent>(rr);\n\t  //\t  L<<Logger::Info<<\"New SOA serial for \"<<zone<<\": \"<<newsr->d_st.serial<<endl;\n\t  if (newsr) {\n\t    oursr = newsr;\n\t  }\n\t}\n\telse {\n\t  L<<Logger::Info<<\"Had addition of \"<<rr.d_name<<endl;\n\t  RPZRecordToPolicy(rr, luaconfsCopy.dfe, polName, true, boost::optional<DNSFilterEngine::Policy>(), 0);\n\t}\n      }\n    }\n    L<<Logger::Info<<\"Had \"<<totremove<<\" RPZ removal\"<<addS(totremove)<<\", \"<<totadd<<\" addition\"<<addS(totadd)<<\" for \"<<zone<<\" New serial: \"<<oursr->d_st.serial<<endl;\n    g_luaconfs.setState(luaconfsCopy);\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr)\n+void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr, size_t maxReceivedBytes)\n {\n   int refresh = oursr->d_st.refresh;\n   for(;;) {\n@@ -10,7 +10,7 @@\n     L<<Logger::Info<<\"Getting IXFR deltas for \"<<zone<<\" from \"<<master.toStringWithPort()<<\", our serial: \"<<getRR<SOARecordContent>(dr)->d_st.serial<<endl;\n     vector<pair<vector<DNSRecord>, vector<DNSRecord> > > deltas;\n     try {\n-      deltas = getIXFRDeltas(master, zone, dr, tt);\n+      deltas = getIXFRDeltas(master, zone, dr, tt, nullptr, maxReceivedBytes);\n     } catch(std::runtime_error& e ){\n       L<<Logger::Warning<<e.what()<<endl;\n       continue;",
        "diff_line_info": {
            "deleted_lines": [
                "void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr)",
                "      deltas = getIXFRDeltas(master, zone, dr, tt);"
            ],
            "added_lines": [
                "void RPZIXFRTracker(const ComboAddress& master, const DNSName& zone, const std::string& polName, const TSIGTriplet& tt, shared_ptr<SOARecordContent> oursr, size_t maxReceivedBytes)",
                "      deltas = getIXFRDeltas(master, zone, dr, tt, nullptr, maxReceivedBytes);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/AXFRRetriever::AXFRRetriever",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "AXFRRetriever::AXFRRetriever(const ComboAddress& remote,\n                             const DNSName& domain,\n                             const TSIGTriplet& tt, \n                             const ComboAddress* laddr)\n  : d_tt(tt), d_tsigPos(0), d_nonSignedMessages(0)\n{\n  ComboAddress local;\n  if (laddr != NULL) {\n    local = (ComboAddress) (*laddr);\n  } else {\n    if(remote.sin4.sin_family == AF_INET)\n      local=ComboAddress(::arg()[\"query-local-address\"]);\n    else if(!::arg()[\"query-local-address6\"].empty())\n      local=ComboAddress(::arg()[\"query-local-address6\"]);\n    else\n      local=ComboAddress(\"::\");\n  }\n  d_sock = -1;\n  try {\n    d_sock = makeQuerySocket(local, false); // make a TCP socket\n    if (d_sock < 0)\n      throw ResolverException(\"Error creating socket for AXFR request to \"+d_remote.toStringWithPort());\n    d_buf = shared_array<char>(new char[65536]);\n    d_remote = remote; // mostly for error reporting\n    this->connect();\n    d_soacount = 0;\n  \n    vector<uint8_t> packet;\n    DNSPacketWriter pw(packet, domain, QType::AXFR);\n    pw.getHeader()->id = dns_random(0xffff);\n  \n    if(!tt.name.empty()) {\n      if (tt.algo == DNSName(\"hmac-md5\"))\n        d_trc.d_algoName = tt.algo + DNSName(\"sig-alg.reg.int\");\n      else\n        d_trc.d_algoName = tt.algo;\n      d_trc.d_time = time(0);\n      d_trc.d_fudge = 300;\n      d_trc.d_origID=ntohs(pw.getHeader()->id);\n      d_trc.d_eRcode=0;\n      addTSIG(pw, &d_trc, tt.name, tt.secret, \"\", false);\n    }\n  \n    uint16_t replen=htons(packet.size());\n    Utility::iovec iov[2];\n    iov[0].iov_base=reinterpret_cast<char*>(&replen);\n    iov[0].iov_len=2;\n    iov[1].iov_base=packet.data();\n    iov[1].iov_len=packet.size();\n  \n    int ret=Utility::writev(d_sock, iov, 2);\n    if(ret < 0)\n      throw ResolverException(\"Error sending question to \"+d_remote.toStringWithPort()+\": \"+stringerror());\n    if(ret != (int)(2+packet.size())) {\n      throw ResolverException(\"Partial write on AXFR request to \"+d_remote.toStringWithPort());\n    }\n  \n    int res = waitForData(d_sock, 10, 0);\n    \n    if(!res)\n      throw ResolverException(\"Timeout waiting for answer from \"+d_remote.toStringWithPort()+\" during AXFR\");\n    if(res<0)\n      throw ResolverException(\"Error waiting for answer from \"+d_remote.toStringWithPort()+\": \"+stringerror());\n  }\n  catch(...) {\n    if(d_sock >= 0)\n      close(d_sock);\n    d_sock = -1;\n    throw;\n  }\n}",
        "func": "AXFRRetriever::AXFRRetriever(const ComboAddress& remote,\n                             const DNSName& domain,\n                             const TSIGTriplet& tt, \n                             const ComboAddress* laddr,\n                             size_t maxReceivedBytes)\n  : d_tt(tt), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)\n{\n  ComboAddress local;\n  if (laddr != NULL) {\n    local = (ComboAddress) (*laddr);\n  } else {\n    if(remote.sin4.sin_family == AF_INET)\n      local=ComboAddress(::arg()[\"query-local-address\"]);\n    else if(!::arg()[\"query-local-address6\"].empty())\n      local=ComboAddress(::arg()[\"query-local-address6\"]);\n    else\n      local=ComboAddress(\"::\");\n  }\n  d_sock = -1;\n  try {\n    d_sock = makeQuerySocket(local, false); // make a TCP socket\n    if (d_sock < 0)\n      throw ResolverException(\"Error creating socket for AXFR request to \"+d_remote.toStringWithPort());\n    d_buf = shared_array<char>(new char[65536]);\n    d_remote = remote; // mostly for error reporting\n    this->connect();\n    d_soacount = 0;\n  \n    vector<uint8_t> packet;\n    DNSPacketWriter pw(packet, domain, QType::AXFR);\n    pw.getHeader()->id = dns_random(0xffff);\n  \n    if(!tt.name.empty()) {\n      if (tt.algo == DNSName(\"hmac-md5\"))\n        d_trc.d_algoName = tt.algo + DNSName(\"sig-alg.reg.int\");\n      else\n        d_trc.d_algoName = tt.algo;\n      d_trc.d_time = time(0);\n      d_trc.d_fudge = 300;\n      d_trc.d_origID=ntohs(pw.getHeader()->id);\n      d_trc.d_eRcode=0;\n      addTSIG(pw, &d_trc, tt.name, tt.secret, \"\", false);\n    }\n  \n    uint16_t replen=htons(packet.size());\n    Utility::iovec iov[2];\n    iov[0].iov_base=reinterpret_cast<char*>(&replen);\n    iov[0].iov_len=2;\n    iov[1].iov_base=packet.data();\n    iov[1].iov_len=packet.size();\n  \n    int ret=Utility::writev(d_sock, iov, 2);\n    if(ret < 0)\n      throw ResolverException(\"Error sending question to \"+d_remote.toStringWithPort()+\": \"+stringerror());\n    if(ret != (int)(2+packet.size())) {\n      throw ResolverException(\"Partial write on AXFR request to \"+d_remote.toStringWithPort());\n    }\n  \n    int res = waitForData(d_sock, 10, 0);\n    \n    if(!res)\n      throw ResolverException(\"Timeout waiting for answer from \"+d_remote.toStringWithPort()+\" during AXFR\");\n    if(res<0)\n      throw ResolverException(\"Error waiting for answer from \"+d_remote.toStringWithPort()+\": \"+stringerror());\n  }\n  catch(...) {\n    if(d_sock >= 0)\n      close(d_sock);\n    d_sock = -1;\n    throw;\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,9 @@\n AXFRRetriever::AXFRRetriever(const ComboAddress& remote,\n                              const DNSName& domain,\n                              const TSIGTriplet& tt, \n-                             const ComboAddress* laddr)\n-  : d_tt(tt), d_tsigPos(0), d_nonSignedMessages(0)\n+                             const ComboAddress* laddr,\n+                             size_t maxReceivedBytes)\n+  : d_tt(tt), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)\n {\n   ComboAddress local;\n   if (laddr != NULL) {",
        "diff_line_info": {
            "deleted_lines": [
                "                             const ComboAddress* laddr)",
                "  : d_tt(tt), d_tsigPos(0), d_nonSignedMessages(0)"
            ],
            "added_lines": [
                "                             const ComboAddress* laddr,",
                "                             size_t maxReceivedBytes)",
                "  : d_tt(tt), d_receivedBytes(0), d_maxReceivedBytes(maxReceivedBytes), d_tsigPos(0), d_nonSignedMessages(0)"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/AXFRRetriever::getChunk",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "int AXFRRetriever::getChunk(Resolver::res_t &res, vector<DNSRecord>* records) // Implementation is making sure RFC2845 4.4 is followed.\n{\n  if(d_soacount > 1)\n    return false;\n\n  // d_sock is connected and is about to spit out a packet\n  int len=getLength();\n  if(len<0)\n    throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n  \n  timeoutReadn(len); \n  MOADNSParser mdp(d_buf.get(), len);\n\n  int err;\n  if(!records)\n    err=parseResult(mdp, DNSName(), 0, 0, &res);\n  else {\n    records->clear();\n    for(const auto& r: mdp.d_answers)\n      records->push_back(r.first);\n    err = mdp.d_header.rcode;\n  }\n  \n  if(err) \n    throw ResolverException(\"AXFR chunk error: \" + RCode::to_s(err));\n\n  for(const MOADNSParser::answers_t::value_type& answer :  mdp.d_answers)\n    if (answer.first.d_type == QType::SOA)\n      d_soacount++;\n \n  if(!d_tt.name.empty()) { // TSIG verify message\n    // If we have multiple messages, we need to concatenate them together. We also need to make sure we know the location of \n    // the TSIG record so we can remove it in makeTSIGMessageFromTSIGPacket\n    d_signData.append(d_buf.get(), len);\n    if (mdp.getTSIGPos() == 0)\n      d_tsigPos += len;\n    else \n      d_tsigPos += mdp.getTSIGPos();\n\n    string theirMac;\n    bool checkTSIG = false;\n    \n    for(const MOADNSParser::answers_t::value_type& answer :  mdp.d_answers) {\n      if (answer.first.d_type == QType::SOA)  // A SOA is either the first or the last record. We need to check TSIG if that's the case.\n        checkTSIG = true;\n      \n      if(answer.first.d_type == QType::TSIG) {\n        shared_ptr<TSIGRecordContent> trc = getRR<TSIGRecordContent>(answer.first);\n        if(trc) {\n          theirMac = trc->d_mac;\n          d_trc.d_time = trc->d_time;\n          checkTSIG = true;\n        }\n      }\n    }\n\n    if( ! checkTSIG && d_nonSignedMessages > 99) { // We're allowed to get 100 digest without a TSIG.\n      throw ResolverException(\"No TSIG message received in last 100 messages of AXFR transfer.\");\n    }\n\n    if (checkTSIG) {\n      if (theirMac.empty())\n        throw ResolverException(\"No TSIG on AXFR response from \"+d_remote.toStringWithPort()+\" , should be signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n\n      string message;\n      if (!d_prevMac.empty()) {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tt.name, d_trc, d_prevMac, true, d_signData.size()-len);\n      } else {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tt.name, d_trc, d_trc.d_mac, false);\n      }\n\n      TSIGHashEnum algo;\n      if (!getTSIGHashEnum(d_trc.d_algoName, algo)) {\n        throw ResolverException(\"Unsupported TSIG HMAC algorithm \" + d_trc.d_algoName.toString());\n      }\n\n      if (algo == TSIG_GSS) {\n        GssContext gssctx(d_tt.name);\n        if (!gss_verify_signature(d_tt.name, message, theirMac)) {\n          throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n        }\n      } else {\n        string ourMac=calculateHMAC(d_tt.secret, message, algo);\n\n        // ourMac[0]++; // sabotage == for testing :-)\n        if(ourMac != theirMac) {\n          throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n        }\n      }\n\n      // Reset and store some values for the next chunks. \n      d_prevMac = theirMac;\n      d_nonSignedMessages = 0;\n      d_signData.clear();\n      d_tsigPos = 0;\n    }\n    else\n      d_nonSignedMessages++;\n  }\n  \n  return true;\n}",
        "func": "int AXFRRetriever::getChunk(Resolver::res_t &res, vector<DNSRecord>* records) // Implementation is making sure RFC2845 4.4 is followed.\n{\n  if(d_soacount > 1)\n    return false;\n\n  // d_sock is connected and is about to spit out a packet\n  int len=getLength();\n  if(len<0)\n    throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n\n  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)\n    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");\n\n  timeoutReadn(len);\n\n  d_receivedBytes += (uint16_t) len;\n\n  MOADNSParser mdp(d_buf.get(), len);\n\n  int err;\n  if(!records)\n    err=parseResult(mdp, DNSName(), 0, 0, &res);\n  else {\n    records->clear();\n    for(const auto& r: mdp.d_answers)\n      records->push_back(r.first);\n    err = mdp.d_header.rcode;\n  }\n  \n  if(err) \n    throw ResolverException(\"AXFR chunk error: \" + RCode::to_s(err));\n\n  for(const MOADNSParser::answers_t::value_type& answer :  mdp.d_answers)\n    if (answer.first.d_type == QType::SOA)\n      d_soacount++;\n \n  if(!d_tt.name.empty()) { // TSIG verify message\n    // If we have multiple messages, we need to concatenate them together. We also need to make sure we know the location of \n    // the TSIG record so we can remove it in makeTSIGMessageFromTSIGPacket\n    d_signData.append(d_buf.get(), len);\n    if (mdp.getTSIGPos() == 0)\n      d_tsigPos += len;\n    else \n      d_tsigPos += mdp.getTSIGPos();\n\n    string theirMac;\n    bool checkTSIG = false;\n    \n    for(const MOADNSParser::answers_t::value_type& answer :  mdp.d_answers) {\n      if (answer.first.d_type == QType::SOA)  // A SOA is either the first or the last record. We need to check TSIG if that's the case.\n        checkTSIG = true;\n      \n      if(answer.first.d_type == QType::TSIG) {\n        shared_ptr<TSIGRecordContent> trc = getRR<TSIGRecordContent>(answer.first);\n        if(trc) {\n          theirMac = trc->d_mac;\n          d_trc.d_time = trc->d_time;\n          checkTSIG = true;\n        }\n      }\n    }\n\n    if( ! checkTSIG && d_nonSignedMessages > 99) { // We're allowed to get 100 digest without a TSIG.\n      throw ResolverException(\"No TSIG message received in last 100 messages of AXFR transfer.\");\n    }\n\n    if (checkTSIG) {\n      if (theirMac.empty())\n        throw ResolverException(\"No TSIG on AXFR response from \"+d_remote.toStringWithPort()+\" , should be signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n\n      string message;\n      if (!d_prevMac.empty()) {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tt.name, d_trc, d_prevMac, true, d_signData.size()-len);\n      } else {\n        message = makeTSIGMessageFromTSIGPacket(d_signData, d_tsigPos, d_tt.name, d_trc, d_trc.d_mac, false);\n      }\n\n      TSIGHashEnum algo;\n      if (!getTSIGHashEnum(d_trc.d_algoName, algo)) {\n        throw ResolverException(\"Unsupported TSIG HMAC algorithm \" + d_trc.d_algoName.toString());\n      }\n\n      if (algo == TSIG_GSS) {\n        GssContext gssctx(d_tt.name);\n        if (!gss_verify_signature(d_tt.name, message, theirMac)) {\n          throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n        }\n      } else {\n        string ourMac=calculateHMAC(d_tt.secret, message, algo);\n\n        // ourMac[0]++; // sabotage == for testing :-)\n        if(ourMac != theirMac) {\n          throw ResolverException(\"Signature failed to validate on AXFR response from \"+d_remote.toStringWithPort()+\" signed with TSIG key '\"+d_tt.name.toString()+\"'\");\n        }\n      }\n\n      // Reset and store some values for the next chunks. \n      d_prevMac = theirMac;\n      d_nonSignedMessages = 0;\n      d_signData.clear();\n      d_tsigPos = 0;\n    }\n    else\n      d_nonSignedMessages++;\n  }\n  \n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,8 +7,14 @@\n   int len=getLength();\n   if(len<0)\n     throw ResolverException(\"EOF trying to read axfr chunk from remote TCP client\");\n-  \n-  timeoutReadn(len); \n+\n+  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)\n+    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");\n+\n+  timeoutReadn(len);\n+\n+  d_receivedBytes += (uint16_t) len;\n+\n   MOADNSParser mdp(d_buf.get(), len);\n \n   int err;",
        "diff_line_info": {
            "deleted_lines": [
                "  ",
                "  timeoutReadn(len); "
            ],
            "added_lines": [
                "",
                "  if (d_maxReceivedBytes > 0 && (d_maxReceivedBytes - d_receivedBytes) < (size_t) len)",
                "    throw ResolverException(\"Reached the maximum number of received bytes during AXFR\");",
                "",
                "  timeoutReadn(len);",
                "",
                "  d_receivedBytes += (uint16_t) len;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/declareArguments",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "void declareArguments()\n{\n  ::arg().set(\"local-port\",\"The port on which we listen\")=\"53\";\n  ::arg().setSwitch(\"dnsupdate\",\"Enable/Disable DNS update (RFC2136) support. Default is no.\")=\"no\";\n  ::arg().setSwitch(\"write-pid\",\"Write a PID file\")=\"yes\";\n  ::arg().set(\"allow-dnsupdate-from\",\"A global setting to allow DNS updates from these IP ranges.\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"allow-unsigned-notify\",\"Allow unsigned notifications for TSIG secured domains\")=\"yes\"; //FIXME: change to 'no' later\n  ::arg().set(\"allow-unsigned-supermaster\", \"Allow supermasters to create zones without TSIG signed NOTIFY\")=\"yes\";\n  ::arg().setSwitch(\"forward-dnsupdate\",\"A global setting to allow DNS update packages that are for a Slave domain, to be forwarded to the master.\")=\"yes\";\n  ::arg().setSwitch(\"log-dns-details\",\"If PDNS should log DNS non-erroneous details\")=\"no\";\n  ::arg().setSwitch(\"log-dns-queries\",\"If PDNS should log all incoming DNS queries\")=\"no\";\n  ::arg().set(\"local-address\",\"Local IP addresses to which we bind\")=\"0.0.0.0\";\n  ::arg().setSwitch(\"local-address-nonexist-fail\",\"Fail to start if one or more of the local-address's do not exist on this server\")=\"yes\";\n  ::arg().setSwitch(\"non-local-bind\", \"Enable binding to non-local addresses by using FREEBIND / BINDANY socket options\")=\"no\";\n  ::arg().set(\"local-ipv6\",\"Local IP address to which we bind\")=\"::\";\n  ::arg().setSwitch(\"reuseport\",\"Enable higher performance on compliant kernels by using SO_REUSEPORT allowing each receiver thread to open its own socket\")=\"no\";\n  ::arg().setSwitch(\"local-ipv6-nonexist-fail\",\"Fail to start if one or more of the local-ipv6 addresses do not exist on this server\")=\"yes\";\n  ::arg().set(\"query-local-address\",\"Source IP address for sending queries\")=\"0.0.0.0\";\n  ::arg().set(\"query-local-address6\",\"Source IPv6 address for sending queries\")=\"::\";\n  ::arg().set(\"overload-queue-length\",\"Maximum queuelength moving to packetcache only\")=\"0\";\n  ::arg().set(\"max-queue-length\",\"Maximum queuelength before considering situation lost\")=\"5000\";\n\n  ::arg().set(\"retrieval-threads\", \"Number of AXFR-retrieval threads for slave operation\")=\"2\";\n  ::arg().setSwitch(\"api\", \"Enable/disable the REST API\")=\"no\";\n  ::arg().set(\"api-key\", \"Static pre-shared authentication key for access to the REST API\")=\"\";\n  ::arg().set(\"api-logfile\", \"Location of the server logfile (used by the REST API)\")=\"/var/log/pdns.log\";\n  ::arg().setSwitch(\"api-readonly\", \"Disallow data modification through the REST API when set\")=\"no\";\n  ::arg().setSwitch(\"dname-processing\", \"If we should support DNAME records\")=\"no\";\n\n  ::arg().setCmd(\"help\",\"Provide a helpful message\");\n  ::arg().setCmd(\"version\",\"Output version and compilation date\");\n  ::arg().setCmd(\"config\",\"Provide configuration file on standard output\");\n  ::arg().setCmd(\"list-modules\",\"Lists all modules available\");\n  ::arg().setCmd(\"no-config\",\"Don't parse configuration file\");\n  \n  ::arg().set(\"version-string\",\"PowerDNS version in packets - full, anonymous, powerdns or custom\")=\"full\"; \n  ::arg().set(\"control-console\",\"Debugging switch - don't use\")=\"no\"; // but I know you will!\n  ::arg().set(\"loglevel\",\"Amount of logging. Higher is more. Do not set below 3\")=\"4\";\n  ::arg().set(\"disable-syslog\",\"Disable logging to syslog, useful when running inside a supervisor that logs stdout\")=\"no\";\n  ::arg().set(\"default-soa-name\",\"name to insert in the SOA record if none set in the backend\")=\"a.misconfigured.powerdns.server\";\n  ::arg().set(\"default-soa-mail\",\"mail address to insert in the SOA record if none set in the backend\")=\"\";\n  ::arg().set(\"distributor-threads\",\"Default number of Distributor (backend) threads to start\")=\"3\";\n  ::arg().set(\"signing-threads\",\"Default number of signer threads to start\")=\"3\";\n  ::arg().set(\"receiver-threads\",\"Default number of receiver threads to start\")=\"1\";\n  ::arg().set(\"queue-limit\",\"Maximum number of milliseconds to queue a query\")=\"1500\"; \n  ::arg().set(\"recursor\",\"If recursion is desired, IP address of a recursing nameserver\")=\"no\"; \n  ::arg().set(\"allow-recursion\",\"List of subnets that are allowed to recurse\")=\"0.0.0.0/0\";\n  ::arg().set(\"udp-truncation-threshold\", \"Maximum UDP response size before we truncate\")=\"1680\";\n  ::arg().set(\"disable-tcp\",\"Do not listen to TCP queries\")=\"no\";\n  \n  ::arg().set(\"config-name\",\"Name of this virtual configuration - will rename the binary image\")=\"\";\n\n  ::arg().set(\"load-modules\",\"Load this module - supply absolute or relative path\")=\"\";\n  ::arg().set(\"launch\",\"Which backends to launch and order to query them in\")=\"\";\n  ::arg().setSwitch(\"disable-axfr\",\"Disable zonetransfers but do allow TCP queries\")=\"no\";\n  ::arg().set(\"allow-axfr-ips\",\"Allow zonetransfers only to these subnets\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"only-notify\", \"Only send AXFR NOTIFY to these IP addresses or netmasks\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"also-notify\", \"When notifying a domain, also notify these nameservers\")=\"\";\n  ::arg().set(\"allow-notify-from\",\"Allow AXFR NOTIFY from these IP ranges. If empty, drop all incoming notifies.\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"slave-cycle-interval\",\"Schedule slave freshness checks once every .. seconds\")=\"60\";\n\n  ::arg().set(\"tcp-control-address\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"\";\n  ::arg().set(\"tcp-control-port\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"53000\";\n  ::arg().set(\"tcp-control-secret\",\"If set, PowerDNS can be controlled over TCP after passing this secret\")=\"\";\n  ::arg().set(\"tcp-control-range\",\"If set, remote control of PowerDNS is possible over these networks only\")=\"127.0.0.0/8, 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12, ::1/128, fe80::/10\";\n  \n  ::arg().setSwitch(\"slave\",\"Act as a slave\")=\"no\";\n  ::arg().setSwitch(\"master\",\"Act as a master\")=\"no\";\n  ::arg().setSwitch(\"disable-axfr-rectify\",\"Disable the rectify step during an outgoing AXFR. Only required for regression testing.\")=\"no\";\n  ::arg().setSwitch(\"guardian\",\"Run within a guardian process\")=\"no\";\n  ::arg().setSwitch(\"prevent-self-notification\",\"Don't send notifications to what we think is ourself\")=\"yes\";\n  ::arg().setSwitch(\"webserver\",\"Start a webserver for monitoring\")=\"no\"; \n  ::arg().setSwitch(\"webserver-print-arguments\",\"If the webserver should print arguments\")=\"no\"; \n  ::arg().setSwitch(\"edns-subnet-processing\",\"If we should act on EDNS Subnet options\")=\"no\"; \n  ::arg().setSwitch(\"any-to-tcp\",\"Answer ANY queries with tc=1, shunting to TCP\")=\"no\"; \n  ::arg().set(\"webserver-address\",\"IP Address of webserver to listen on\")=\"127.0.0.1\";\n  ::arg().set(\"webserver-port\",\"Port of webserver to listen on\")=\"8081\";\n  ::arg().set(\"webserver-password\",\"Password required for accessing the webserver\")=\"\";\n  ::arg().set(\"webserver-allow-from\",\"Webserver access is only allowed from these subnets\")=\"0.0.0.0/0,::/0\";\n\n  ::arg().setSwitch(\"out-of-zone-additional-processing\",\"Do out of zone additional processing\")=\"yes\";\n  ::arg().setSwitch(\"do-ipv6-additional-processing\", \"Do AAAA additional processing\")=\"yes\";\n  ::arg().setSwitch(\"query-logging\",\"Hint backends that queries should be logged\")=\"no\";\n\n  ::arg().set(\"carbon-ourname\", \"If set, overrides our reported hostname for carbon stats\")=\"\";\n  ::arg().set(\"carbon-server\", \"If set, send metrics in carbon (graphite) format to this server\")=\"\";\n  ::arg().set(\"carbon-interval\", \"Number of seconds between carbon (graphite) updates\")=\"30\";\n\n  ::arg().set(\"cache-ttl\",\"Seconds to store packets in the PacketCache\")=\"20\";\n  ::arg().set(\"recursive-cache-ttl\",\"Seconds to store packets for recursive queries in the PacketCache\")=\"10\";\n  ::arg().set(\"negquery-cache-ttl\",\"Seconds to store negative query results in the QueryCache\")=\"60\";\n  ::arg().set(\"query-cache-ttl\",\"Seconds to store query results in the QueryCache\")=\"20\";\n  ::arg().set(\"soa-minimum-ttl\",\"Default SOA minimum ttl\")=\"3600\";\n  ::arg().set(\"server-id\", \"Returned when queried for 'server.id' TXT or NSID, defaults to hostname - disabled or custom\")=\"\";\n  ::arg().set(\"soa-refresh-default\",\"Default SOA refresh\")=\"10800\";\n  ::arg().set(\"soa-retry-default\",\"Default SOA retry\")=\"3600\";\n  ::arg().set(\"soa-expire-default\",\"Default SOA expire\")=\"604800\";\n  ::arg().set(\"default-soa-edit\",\"Default SOA-EDIT value\")=\"\";\n  ::arg().set(\"default-soa-edit-signed\",\"Default SOA-EDIT value for signed zones\")=\"\";\n  ::arg().set(\"dnssec-key-cache-ttl\",\"Seconds to cache DNSSEC keys from the database\")=\"30\";\n  ::arg().set(\"domain-metadata-cache-ttl\",\"Seconds to cache domain metadata from the database\")=\"60\";\n\n  ::arg().set(\"trusted-notification-proxy\", \"IP address of incoming notification proxy\")=\"\";\n  ::arg().set(\"slave-renotify\", \"If we should send out notifications for slaved updates\")=\"no\";\n\n  ::arg().set(\"default-ttl\",\"Seconds a result is valid if not set otherwise\")=\"3600\";\n  ::arg().set(\"max-tcp-connections\",\"Maximum number of TCP connections\")=\"20\";\n  ::arg().setSwitch(\"no-shuffle\",\"Set this to prevent random shuffling of answers - for regression testing\")=\"off\";\n\n  ::arg().set(\"setuid\",\"If set, change user id to this uid for more security\")=\"\";\n  ::arg().set(\"setgid\",\"If set, change group id to this gid for more security\")=\"\";\n\n  ::arg().set(\"max-cache-entries\", \"Maximum number of cache entries\")=\"1000000\";\n  ::arg().set(\"max-signature-cache-entries\", \"Maximum number of signatures cache entries\")=\"\";\n  ::arg().set(\"max-ent-entries\", \"Maximum number of empty non-terminals in a zone\")=\"100000\";\n  ::arg().set(\"entropy-source\", \"If set, read entropy from this file\")=\"/dev/urandom\";\n\n  ::arg().set(\"lua-prequery-script\", \"Lua script with prequery handler (DO NOT USE)\")=\"\";\n  ::arg().set(\"experimental-lua-policy-script\", \"Lua script for the policy engine\")=\"\";\n\n  ::arg().setSwitch(\"traceback-handler\",\"Enable the traceback handler (Linux only)\")=\"yes\";\n  ::arg().setSwitch(\"direct-dnskey\",\"Fetch DNSKEY RRs from backend during DNSKEY synthesis\")=\"no\";\n  ::arg().set(\"default-ksk-algorithms\",\"Default KSK algorithms\")=\"ecdsa256\";\n  ::arg().set(\"default-ksk-size\",\"Default KSK size (0 means default)\")=\"0\";\n  ::arg().set(\"default-zsk-algorithms\",\"Default ZSK algorithms\")=\"\";\n  ::arg().set(\"default-zsk-size\",\"Default ZSK size (0 means default)\")=\"0\";\n  ::arg().set(\"max-nsec3-iterations\",\"Limit the number of NSEC3 hash iterations\")=\"500\"; // RFC5155 10.3\n\n  ::arg().set(\"include-dir\",\"Include *.conf files from this directory\");\n  ::arg().set(\"security-poll-suffix\",\"Domain name from which to query security update notifications\")=\"secpoll.powerdns.com.\";\n\n  ::arg().setSwitch(\"outgoing-axfr-expand-alias\", \"Expand ALIAS records during outgoing AXFR\")=\"no\";\n  ::arg().setSwitch(\"8bit-dns\", \"Allow 8bit dns queries\")=\"no\";\n}",
        "func": "void declareArguments()\n{\n  ::arg().set(\"local-port\",\"The port on which we listen\")=\"53\";\n  ::arg().setSwitch(\"dnsupdate\",\"Enable/Disable DNS update (RFC2136) support. Default is no.\")=\"no\";\n  ::arg().setSwitch(\"write-pid\",\"Write a PID file\")=\"yes\";\n  ::arg().set(\"allow-dnsupdate-from\",\"A global setting to allow DNS updates from these IP ranges.\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"allow-unsigned-notify\",\"Allow unsigned notifications for TSIG secured domains\")=\"yes\"; //FIXME: change to 'no' later\n  ::arg().set(\"allow-unsigned-supermaster\", \"Allow supermasters to create zones without TSIG signed NOTIFY\")=\"yes\";\n  ::arg().setSwitch(\"forward-dnsupdate\",\"A global setting to allow DNS update packages that are for a Slave domain, to be forwarded to the master.\")=\"yes\";\n  ::arg().setSwitch(\"log-dns-details\",\"If PDNS should log DNS non-erroneous details\")=\"no\";\n  ::arg().setSwitch(\"log-dns-queries\",\"If PDNS should log all incoming DNS queries\")=\"no\";\n  ::arg().set(\"local-address\",\"Local IP addresses to which we bind\")=\"0.0.0.0\";\n  ::arg().setSwitch(\"local-address-nonexist-fail\",\"Fail to start if one or more of the local-address's do not exist on this server\")=\"yes\";\n  ::arg().setSwitch(\"non-local-bind\", \"Enable binding to non-local addresses by using FREEBIND / BINDANY socket options\")=\"no\";\n  ::arg().set(\"local-ipv6\",\"Local IP address to which we bind\")=\"::\";\n  ::arg().setSwitch(\"reuseport\",\"Enable higher performance on compliant kernels by using SO_REUSEPORT allowing each receiver thread to open its own socket\")=\"no\";\n  ::arg().setSwitch(\"local-ipv6-nonexist-fail\",\"Fail to start if one or more of the local-ipv6 addresses do not exist on this server\")=\"yes\";\n  ::arg().set(\"query-local-address\",\"Source IP address for sending queries\")=\"0.0.0.0\";\n  ::arg().set(\"query-local-address6\",\"Source IPv6 address for sending queries\")=\"::\";\n  ::arg().set(\"overload-queue-length\",\"Maximum queuelength moving to packetcache only\")=\"0\";\n  ::arg().set(\"max-queue-length\",\"Maximum queuelength before considering situation lost\")=\"5000\";\n\n  ::arg().set(\"retrieval-threads\", \"Number of AXFR-retrieval threads for slave operation\")=\"2\";\n  ::arg().setSwitch(\"api\", \"Enable/disable the REST API\")=\"no\";\n  ::arg().set(\"api-key\", \"Static pre-shared authentication key for access to the REST API\")=\"\";\n  ::arg().set(\"api-logfile\", \"Location of the server logfile (used by the REST API)\")=\"/var/log/pdns.log\";\n  ::arg().setSwitch(\"api-readonly\", \"Disallow data modification through the REST API when set\")=\"no\";\n  ::arg().setSwitch(\"dname-processing\", \"If we should support DNAME records\")=\"no\";\n\n  ::arg().setCmd(\"help\",\"Provide a helpful message\");\n  ::arg().setCmd(\"version\",\"Output version and compilation date\");\n  ::arg().setCmd(\"config\",\"Provide configuration file on standard output\");\n  ::arg().setCmd(\"list-modules\",\"Lists all modules available\");\n  ::arg().setCmd(\"no-config\",\"Don't parse configuration file\");\n  \n  ::arg().set(\"version-string\",\"PowerDNS version in packets - full, anonymous, powerdns or custom\")=\"full\"; \n  ::arg().set(\"control-console\",\"Debugging switch - don't use\")=\"no\"; // but I know you will!\n  ::arg().set(\"loglevel\",\"Amount of logging. Higher is more. Do not set below 3\")=\"4\";\n  ::arg().set(\"disable-syslog\",\"Disable logging to syslog, useful when running inside a supervisor that logs stdout\")=\"no\";\n  ::arg().set(\"default-soa-name\",\"name to insert in the SOA record if none set in the backend\")=\"a.misconfigured.powerdns.server\";\n  ::arg().set(\"default-soa-mail\",\"mail address to insert in the SOA record if none set in the backend\")=\"\";\n  ::arg().set(\"distributor-threads\",\"Default number of Distributor (backend) threads to start\")=\"3\";\n  ::arg().set(\"signing-threads\",\"Default number of signer threads to start\")=\"3\";\n  ::arg().set(\"receiver-threads\",\"Default number of receiver threads to start\")=\"1\";\n  ::arg().set(\"queue-limit\",\"Maximum number of milliseconds to queue a query\")=\"1500\"; \n  ::arg().set(\"recursor\",\"If recursion is desired, IP address of a recursing nameserver\")=\"no\"; \n  ::arg().set(\"allow-recursion\",\"List of subnets that are allowed to recurse\")=\"0.0.0.0/0\";\n  ::arg().set(\"udp-truncation-threshold\", \"Maximum UDP response size before we truncate\")=\"1680\";\n  ::arg().set(\"disable-tcp\",\"Do not listen to TCP queries\")=\"no\";\n  \n  ::arg().set(\"config-name\",\"Name of this virtual configuration - will rename the binary image\")=\"\";\n\n  ::arg().set(\"load-modules\",\"Load this module - supply absolute or relative path\")=\"\";\n  ::arg().set(\"launch\",\"Which backends to launch and order to query them in\")=\"\";\n  ::arg().setSwitch(\"disable-axfr\",\"Disable zonetransfers but do allow TCP queries\")=\"no\";\n  ::arg().set(\"allow-axfr-ips\",\"Allow zonetransfers only to these subnets\")=\"127.0.0.0/8,::1\";\n  ::arg().set(\"only-notify\", \"Only send AXFR NOTIFY to these IP addresses or netmasks\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"also-notify\", \"When notifying a domain, also notify these nameservers\")=\"\";\n  ::arg().set(\"allow-notify-from\",\"Allow AXFR NOTIFY from these IP ranges. If empty, drop all incoming notifies.\")=\"0.0.0.0/0,::/0\";\n  ::arg().set(\"slave-cycle-interval\",\"Schedule slave freshness checks once every .. seconds\")=\"60\";\n\n  ::arg().set(\"tcp-control-address\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"\";\n  ::arg().set(\"tcp-control-port\",\"If set, PowerDNS can be controlled over TCP on this address\")=\"53000\";\n  ::arg().set(\"tcp-control-secret\",\"If set, PowerDNS can be controlled over TCP after passing this secret\")=\"\";\n  ::arg().set(\"tcp-control-range\",\"If set, remote control of PowerDNS is possible over these networks only\")=\"127.0.0.0/8, 10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12, ::1/128, fe80::/10\";\n  \n  ::arg().setSwitch(\"slave\",\"Act as a slave\")=\"no\";\n  ::arg().setSwitch(\"master\",\"Act as a master\")=\"no\";\n  ::arg().setSwitch(\"disable-axfr-rectify\",\"Disable the rectify step during an outgoing AXFR. Only required for regression testing.\")=\"no\";\n  ::arg().setSwitch(\"guardian\",\"Run within a guardian process\")=\"no\";\n  ::arg().setSwitch(\"prevent-self-notification\",\"Don't send notifications to what we think is ourself\")=\"yes\";\n  ::arg().setSwitch(\"webserver\",\"Start a webserver for monitoring\")=\"no\"; \n  ::arg().setSwitch(\"webserver-print-arguments\",\"If the webserver should print arguments\")=\"no\"; \n  ::arg().setSwitch(\"edns-subnet-processing\",\"If we should act on EDNS Subnet options\")=\"no\"; \n  ::arg().setSwitch(\"any-to-tcp\",\"Answer ANY queries with tc=1, shunting to TCP\")=\"no\"; \n  ::arg().set(\"webserver-address\",\"IP Address of webserver to listen on\")=\"127.0.0.1\";\n  ::arg().set(\"webserver-port\",\"Port of webserver to listen on\")=\"8081\";\n  ::arg().set(\"webserver-password\",\"Password required for accessing the webserver\")=\"\";\n  ::arg().set(\"webserver-allow-from\",\"Webserver access is only allowed from these subnets\")=\"0.0.0.0/0,::/0\";\n\n  ::arg().setSwitch(\"out-of-zone-additional-processing\",\"Do out of zone additional processing\")=\"yes\";\n  ::arg().setSwitch(\"do-ipv6-additional-processing\", \"Do AAAA additional processing\")=\"yes\";\n  ::arg().setSwitch(\"query-logging\",\"Hint backends that queries should be logged\")=\"no\";\n\n  ::arg().set(\"carbon-ourname\", \"If set, overrides our reported hostname for carbon stats\")=\"\";\n  ::arg().set(\"carbon-server\", \"If set, send metrics in carbon (graphite) format to this server\")=\"\";\n  ::arg().set(\"carbon-interval\", \"Number of seconds between carbon (graphite) updates\")=\"30\";\n\n  ::arg().set(\"cache-ttl\",\"Seconds to store packets in the PacketCache\")=\"20\";\n  ::arg().set(\"recursive-cache-ttl\",\"Seconds to store packets for recursive queries in the PacketCache\")=\"10\";\n  ::arg().set(\"negquery-cache-ttl\",\"Seconds to store negative query results in the QueryCache\")=\"60\";\n  ::arg().set(\"query-cache-ttl\",\"Seconds to store query results in the QueryCache\")=\"20\";\n  ::arg().set(\"soa-minimum-ttl\",\"Default SOA minimum ttl\")=\"3600\";\n  ::arg().set(\"server-id\", \"Returned when queried for 'server.id' TXT or NSID, defaults to hostname - disabled or custom\")=\"\";\n  ::arg().set(\"soa-refresh-default\",\"Default SOA refresh\")=\"10800\";\n  ::arg().set(\"soa-retry-default\",\"Default SOA retry\")=\"3600\";\n  ::arg().set(\"soa-expire-default\",\"Default SOA expire\")=\"604800\";\n  ::arg().set(\"default-soa-edit\",\"Default SOA-EDIT value\")=\"\";\n  ::arg().set(\"default-soa-edit-signed\",\"Default SOA-EDIT value for signed zones\")=\"\";\n  ::arg().set(\"dnssec-key-cache-ttl\",\"Seconds to cache DNSSEC keys from the database\")=\"30\";\n  ::arg().set(\"domain-metadata-cache-ttl\",\"Seconds to cache domain metadata from the database\")=\"60\";\n\n  ::arg().set(\"trusted-notification-proxy\", \"IP address of incoming notification proxy\")=\"\";\n  ::arg().set(\"slave-renotify\", \"If we should send out notifications for slaved updates\")=\"no\";\n\n  ::arg().set(\"default-ttl\",\"Seconds a result is valid if not set otherwise\")=\"3600\";\n  ::arg().set(\"max-tcp-connections\",\"Maximum number of TCP connections\")=\"20\";\n  ::arg().setSwitch(\"no-shuffle\",\"Set this to prevent random shuffling of answers - for regression testing\")=\"off\";\n\n  ::arg().set(\"setuid\",\"If set, change user id to this uid for more security\")=\"\";\n  ::arg().set(\"setgid\",\"If set, change group id to this gid for more security\")=\"\";\n\n  ::arg().set(\"max-cache-entries\", \"Maximum number of cache entries\")=\"1000000\";\n  ::arg().set(\"max-signature-cache-entries\", \"Maximum number of signatures cache entries\")=\"\";\n  ::arg().set(\"max-ent-entries\", \"Maximum number of empty non-terminals in a zone\")=\"100000\";\n  ::arg().set(\"entropy-source\", \"If set, read entropy from this file\")=\"/dev/urandom\";\n\n  ::arg().set(\"lua-prequery-script\", \"Lua script with prequery handler (DO NOT USE)\")=\"\";\n  ::arg().set(\"experimental-lua-policy-script\", \"Lua script for the policy engine\")=\"\";\n\n  ::arg().setSwitch(\"traceback-handler\",\"Enable the traceback handler (Linux only)\")=\"yes\";\n  ::arg().setSwitch(\"direct-dnskey\",\"Fetch DNSKEY RRs from backend during DNSKEY synthesis\")=\"no\";\n  ::arg().set(\"default-ksk-algorithms\",\"Default KSK algorithms\")=\"ecdsa256\";\n  ::arg().set(\"default-ksk-size\",\"Default KSK size (0 means default)\")=\"0\";\n  ::arg().set(\"default-zsk-algorithms\",\"Default ZSK algorithms\")=\"\";\n  ::arg().set(\"default-zsk-size\",\"Default ZSK size (0 means default)\")=\"0\";\n  ::arg().set(\"max-nsec3-iterations\",\"Limit the number of NSEC3 hash iterations\")=\"500\"; // RFC5155 10.3\n\n  ::arg().set(\"include-dir\",\"Include *.conf files from this directory\");\n  ::arg().set(\"security-poll-suffix\",\"Domain name from which to query security update notifications\")=\"secpoll.powerdns.com.\";\n\n  ::arg().setSwitch(\"outgoing-axfr-expand-alias\", \"Expand ALIAS records during outgoing AXFR\")=\"no\";\n  ::arg().setSwitch(\"8bit-dns\", \"Allow 8bit dns queries\")=\"no\";\n\n  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming XFR\")=\"100\";\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -131,4 +131,6 @@\n \n   ::arg().setSwitch(\"outgoing-axfr-expand-alias\", \"Expand ALIAS records during outgoing AXFR\")=\"no\";\n   ::arg().setSwitch(\"8bit-dns\", \"Allow 8bit dns queries\")=\"no\";\n+\n+  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming XFR\")=\"100\";\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  ::arg().set(\"xfr-max-received-mbytes\", \"Maximum number of megabytes received from an incoming XFR\")=\"100\";"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/getIXFRDeltas",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "vector<pair<vector<DNSRecord>, vector<DNSRecord> > > getIXFRDeltas(const ComboAddress& master, const DNSName& zone, const DNSRecord& oursr, \n                                                                   const TSIGTriplet& tt, const ComboAddress* laddr)\n{\n  vector<pair<vector<DNSRecord>, vector<DNSRecord> > >  ret;\n  vector<uint8_t> packet;\n  DNSPacketWriter pw(packet, zone, QType::IXFR);\n  pw.getHeader()->qr=0;\n  pw.getHeader()->rd=0;\n  pw.getHeader()->id=dns_random(0xffff);\n  pw.startRecord(zone, QType::SOA, 0, QClass::IN, DNSResourceRecord::AUTHORITY);\n  oursr.d_content->toPacket(pw);\n\n  pw.commit();\n  if(!tt.algo.empty()) {\n    TSIGHashEnum the;\n    getTSIGHashEnum(tt.algo, the);\n    TSIGRecordContent trc;\n    try {\n      trc.d_algoName = getTSIGAlgoName(the);\n    } catch(PDNSException& pe) {\n      throw std::runtime_error(\"TSIG algorithm '\"+tt.algo.toString()+\"' is unknown.\");\n    }\n    trc.d_time = time((time_t*)NULL);\n    trc.d_fudge = 300;\n    trc.d_origID=ntohs(pw.getHeader()->id);\n    trc.d_eRcode=0;\n    addTSIG(pw, &trc, tt.name, tt.secret, \"\", false);\n  }\n  uint16_t len=htons(packet.size());\n  string msg((const char*)&len, 2);\n  msg.append((const char*)&packet[0], packet.size());\n\n  Socket s(master.sin4.sin_family, SOCK_STREAM);\n  //  cout<<\"going to connect\"<<endl;\n  if(laddr)\n    s.bind(*laddr);\n  s.connect(master);\n  //  cout<<\"Connected\"<<endl;\n  s.writen(msg);\n\n  // CURRENT MASTER SOA\n  // REPEAT:\n  //   SOA WHERE THIS DELTA STARTS\n  //   RECORDS TO REMOVE\n  //   SOA WHERE THIS DELTA GOES\n  //   RECORDS TO ADD\n  // CURRENT MASTER SOA \n  shared_ptr<SOARecordContent> masterSOA;\n  vector<DNSRecord> records;\n  for(;;) {\n    if(s.read((char*)&len, 2)!=2)\n      break;\n    len=ntohs(len);\n    //    cout<<\"Got chunk of \"<<len<<\" bytes\"<<endl;\n    if(!len)\n      break;\n    char reply[len]; \n    readn2(s.getHandle(), reply, len);\n    MOADNSParser mdp(string(reply, len));\n    if(mdp.d_header.rcode) \n      throw std::runtime_error(\"Got an error trying to IXFR zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort()+\"': \"+RCode::to_s(mdp.d_header.rcode));\n\n    //    cout<<\"Got a response, rcode: \"<<mdp.d_header.rcode<<\", got \"<<mdp.d_answers.size()<<\" answers\"<<endl;\n    for(auto& r: mdp.d_answers) {\n      if(r.first.d_type == QType::TSIG) \n        continue;\n      //      cout<<r.first.d_name<< \" \" <<r.first.d_content->getZoneRepresentation()<<endl;\n      r.first.d_name = r.first.d_name.makeRelative(zone);\n      records.push_back(r.first);\n      if(r.first.d_type == QType::SOA) {\n\tauto sr = getRR<SOARecordContent>(r.first);\n\tif(sr) {\n\t  if(!masterSOA) {\n\t    if(sr->d_st.serial == std::dynamic_pointer_cast<SOARecordContent>(oursr.d_content)->d_st.serial) { // we are up to date\n\t      goto done;\n\t    }\n\t    masterSOA=sr;\n\t  }\n\t  else if(sr->d_st.serial == masterSOA->d_st.serial)\n\t    goto done;\n\t}\n      }\n    }\n  }\n  //  cout<<\"Got \"<<records.size()<<\" records\"<<endl;\n done:;\n  for(unsigned int pos = 1;pos < records.size();) {\n    auto sr = getRR<SOARecordContent>(records[pos]);\n    vector<DNSRecord> remove, add;\n    if(!sr) { // this is an actual AXFR!\n      return {{remove, records}};\n    }\n    if(sr->d_st.serial == masterSOA->d_st.serial)\n      break;\n    \n\n    remove.push_back(records[pos]); // this adds the SOA\n    for(pos++; pos < records.size() && records[pos].d_type != QType::SOA; ++pos) {\n      remove.push_back(records[pos]);\n    }\n    sr = getRR<SOARecordContent>(records[pos]);\n\n    add.push_back(records[pos]); // this adds the new SOA\n    for(pos++; pos < records.size() && records[pos].d_type != QType::SOA; ++pos)  {\n      add.push_back(records[pos]);\n    }\n    ret.push_back(make_pair(remove,add));\n  }\n  return ret;\n}",
        "func": "vector<pair<vector<DNSRecord>, vector<DNSRecord> > > getIXFRDeltas(const ComboAddress& master, const DNSName& zone, const DNSRecord& oursr, \n                                                                   const TSIGTriplet& tt, const ComboAddress* laddr, size_t maxReceivedBytes)\n{\n  vector<pair<vector<DNSRecord>, vector<DNSRecord> > >  ret;\n  vector<uint8_t> packet;\n  DNSPacketWriter pw(packet, zone, QType::IXFR);\n  pw.getHeader()->qr=0;\n  pw.getHeader()->rd=0;\n  pw.getHeader()->id=dns_random(0xffff);\n  pw.startRecord(zone, QType::SOA, 0, QClass::IN, DNSResourceRecord::AUTHORITY);\n  oursr.d_content->toPacket(pw);\n\n  pw.commit();\n  if(!tt.algo.empty()) {\n    TSIGHashEnum the;\n    getTSIGHashEnum(tt.algo, the);\n    TSIGRecordContent trc;\n    try {\n      trc.d_algoName = getTSIGAlgoName(the);\n    } catch(PDNSException& pe) {\n      throw std::runtime_error(\"TSIG algorithm '\"+tt.algo.toString()+\"' is unknown.\");\n    }\n    trc.d_time = time((time_t*)NULL);\n    trc.d_fudge = 300;\n    trc.d_origID=ntohs(pw.getHeader()->id);\n    trc.d_eRcode=0;\n    addTSIG(pw, &trc, tt.name, tt.secret, \"\", false);\n  }\n  uint16_t len=htons(packet.size());\n  string msg((const char*)&len, 2);\n  msg.append((const char*)&packet[0], packet.size());\n\n  Socket s(master.sin4.sin_family, SOCK_STREAM);\n  //  cout<<\"going to connect\"<<endl;\n  if(laddr)\n    s.bind(*laddr);\n  s.connect(master);\n  //  cout<<\"Connected\"<<endl;\n  s.writen(msg);\n\n  // CURRENT MASTER SOA\n  // REPEAT:\n  //   SOA WHERE THIS DELTA STARTS\n  //   RECORDS TO REMOVE\n  //   SOA WHERE THIS DELTA GOES\n  //   RECORDS TO ADD\n  // CURRENT MASTER SOA \n  shared_ptr<SOARecordContent> masterSOA;\n  vector<DNSRecord> records;\n  size_t receivedBytes = 0;\n  for(;;) {\n    if(s.read((char*)&len, 2)!=2)\n      break;\n    len=ntohs(len);\n    //    cout<<\"Got chunk of \"<<len<<\" bytes\"<<endl;\n    if(!len)\n      break;\n\n    if (maxReceivedBytes > 0 && (maxReceivedBytes - receivedBytes) < (size_t) len)\n      throw std::runtime_error(\"Reached the maximum number of received bytes in an IXFR delta for zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort());\n\n    char reply[len]; \n    readn2(s.getHandle(), reply, len);\n    receivedBytes += len;\n    MOADNSParser mdp(string(reply, len));\n    if(mdp.d_header.rcode) \n      throw std::runtime_error(\"Got an error trying to IXFR zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort()+\"': \"+RCode::to_s(mdp.d_header.rcode));\n\n    //    cout<<\"Got a response, rcode: \"<<mdp.d_header.rcode<<\", got \"<<mdp.d_answers.size()<<\" answers\"<<endl;\n    for(auto& r: mdp.d_answers) {\n      if(r.first.d_type == QType::TSIG) \n        continue;\n      //      cout<<r.first.d_name<< \" \" <<r.first.d_content->getZoneRepresentation()<<endl;\n      r.first.d_name = r.first.d_name.makeRelative(zone);\n      records.push_back(r.first);\n      if(r.first.d_type == QType::SOA) {\n\tauto sr = getRR<SOARecordContent>(r.first);\n\tif(sr) {\n\t  if(!masterSOA) {\n\t    if(sr->d_st.serial == std::dynamic_pointer_cast<SOARecordContent>(oursr.d_content)->d_st.serial) { // we are up to date\n\t      goto done;\n\t    }\n\t    masterSOA=sr;\n\t  }\n\t  else if(sr->d_st.serial == masterSOA->d_st.serial)\n\t    goto done;\n\t}\n      }\n    }\n  }\n  //  cout<<\"Got \"<<records.size()<<\" records\"<<endl;\n done:;\n  for(unsigned int pos = 1;pos < records.size();) {\n    auto sr = getRR<SOARecordContent>(records[pos]);\n    vector<DNSRecord> remove, add;\n    if(!sr) { // this is an actual AXFR!\n      return {{remove, records}};\n    }\n    if(sr->d_st.serial == masterSOA->d_st.serial)\n      break;\n    \n\n    remove.push_back(records[pos]); // this adds the SOA\n    for(pos++; pos < records.size() && records[pos].d_type != QType::SOA; ++pos) {\n      remove.push_back(records[pos]);\n    }\n    sr = getRR<SOARecordContent>(records[pos]);\n\n    add.push_back(records[pos]); // this adds the new SOA\n    for(pos++; pos < records.size() && records[pos].d_type != QType::SOA; ++pos)  {\n      add.push_back(records[pos]);\n    }\n    ret.push_back(make_pair(remove,add));\n  }\n  return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n vector<pair<vector<DNSRecord>, vector<DNSRecord> > > getIXFRDeltas(const ComboAddress& master, const DNSName& zone, const DNSRecord& oursr, \n-                                                                   const TSIGTriplet& tt, const ComboAddress* laddr)\n+                                                                   const TSIGTriplet& tt, const ComboAddress* laddr, size_t maxReceivedBytes)\n {\n   vector<pair<vector<DNSRecord>, vector<DNSRecord> > >  ret;\n   vector<uint8_t> packet;\n@@ -47,6 +47,7 @@\n   // CURRENT MASTER SOA \n   shared_ptr<SOARecordContent> masterSOA;\n   vector<DNSRecord> records;\n+  size_t receivedBytes = 0;\n   for(;;) {\n     if(s.read((char*)&len, 2)!=2)\n       break;\n@@ -54,8 +55,13 @@\n     //    cout<<\"Got chunk of \"<<len<<\" bytes\"<<endl;\n     if(!len)\n       break;\n+\n+    if (maxReceivedBytes > 0 && (maxReceivedBytes - receivedBytes) < (size_t) len)\n+      throw std::runtime_error(\"Reached the maximum number of received bytes in an IXFR delta for zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort());\n+\n     char reply[len]; \n     readn2(s.getHandle(), reply, len);\n+    receivedBytes += len;\n     MOADNSParser mdp(string(reply, len));\n     if(mdp.d_header.rcode) \n       throw std::runtime_error(\"Got an error trying to IXFR zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort()+\"': \"+RCode::to_s(mdp.d_header.rcode));",
        "diff_line_info": {
            "deleted_lines": [
                "                                                                   const TSIGTriplet& tt, const ComboAddress* laddr)"
            ],
            "added_lines": [
                "                                                                   const TSIGTriplet& tt, const ComboAddress* laddr, size_t maxReceivedBytes)",
                "  size_t receivedBytes = 0;",
                "",
                "    if (maxReceivedBytes > 0 && (maxReceivedBytes - receivedBytes) < (size_t) len)",
                "      throw std::runtime_error(\"Reached the maximum number of received bytes in an IXFR delta for zone '\"+zone.toString()+\"' from master '\"+master.toStringWithPort());",
                "",
                "    receivedBytes += len;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6172",
        "func_name": "PowerDNS/pdns/loadRPZFromServer",
        "description": "PowerDNS (aka pdns) Authoritative Server before 4.0.1 allows remote primary DNS servers to cause a denial of service (memory exhaustion and secondary DNS server crash) via a large (1) AXFR or (2) IXFR response.",
        "git_url": "https://github.com/PowerDNS/pdns/commit/db8f9152168acf5d548d4f256789eae783e01667",
        "commit_title": "Add limits to the size of received {A,I}XFR, in megabytes",
        "commit_text": " This prevents memory exhaustion in case the master is sending a very large amount of data in an update.",
        "func_before": "shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt)\n{\n  L<<Logger::Warning<<\"Loading RPZ zone '\"<<zone<<\"' from \"<<master.toStringWithPort()<<endl;\n  if(!tt.name.empty())\n    L<<Logger::Warning<<\"With TSIG key '\"<<tt.name<<\"' of algorithm '\"<<tt.algo<<\"'\"<<endl;\n\n  ComboAddress local= master.sin4.sin_family == AF_INET ? ComboAddress(\"0.0.0.0\") : ComboAddress(\"::\"); // should be configurable\n  AXFRRetriever axfr(master, zone, tt, &local);\n  unsigned int nrecords=0;\n  Resolver::res_t nop;\n  vector<DNSRecord> chunk;\n  time_t last=0;\n  shared_ptr<SOARecordContent> sr;\n  while(axfr.getChunk(nop, &chunk)) {\n    for(auto& dr : chunk) {\n      if(dr.d_type==QType::NS || dr.d_type==QType::TSIG) {\n\tcontinue;\n      }\n\n      dr.d_name.makeUsRelative(zone);\n      if(dr.d_type==QType::SOA) {\n\tsr = getRR<SOARecordContent>(dr);\n\tcontinue;\n      }\n\n      RPZRecordToPolicy(dr, target, polName, true, defpol, place);\n      nrecords++;\n    } \n    if(last != time(0)) {\n      L<<Logger::Info<<\"Loaded & indexed \"<<nrecords<<\" policy records so far\"<<endl;\n      last=time(0);\n    }\n  }\n  L<<Logger::Info<<\"Done: \"<<nrecords<<\" policy records active, SOA: \"<<sr->getZoneRepresentation()<<endl;\n  return sr;\n}",
        "func": "shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt, size_t maxReceivedBytes)\n{\n  L<<Logger::Warning<<\"Loading RPZ zone '\"<<zone<<\"' from \"<<master.toStringWithPort()<<endl;\n  if(!tt.name.empty())\n    L<<Logger::Warning<<\"With TSIG key '\"<<tt.name<<\"' of algorithm '\"<<tt.algo<<\"'\"<<endl;\n\n  ComboAddress local= master.sin4.sin_family == AF_INET ? ComboAddress(\"0.0.0.0\") : ComboAddress(\"::\"); // should be configurable\n  AXFRRetriever axfr(master, zone, tt, &local, maxReceivedBytes);\n  unsigned int nrecords=0;\n  Resolver::res_t nop;\n  vector<DNSRecord> chunk;\n  time_t last=0;\n  shared_ptr<SOARecordContent> sr;\n  while(axfr.getChunk(nop, &chunk)) {\n    for(auto& dr : chunk) {\n      if(dr.d_type==QType::NS || dr.d_type==QType::TSIG) {\n\tcontinue;\n      }\n\n      dr.d_name.makeUsRelative(zone);\n      if(dr.d_type==QType::SOA) {\n\tsr = getRR<SOARecordContent>(dr);\n\tcontinue;\n      }\n\n      RPZRecordToPolicy(dr, target, polName, true, defpol, place);\n      nrecords++;\n    } \n    if(last != time(0)) {\n      L<<Logger::Info<<\"Loaded & indexed \"<<nrecords<<\" policy records so far\"<<endl;\n      last=time(0);\n    }\n  }\n  L<<Logger::Info<<\"Done: \"<<nrecords<<\" policy records active, SOA: \"<<sr->getZoneRepresentation()<<endl;\n  return sr;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,11 @@\n-shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt)\n+shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt, size_t maxReceivedBytes)\n {\n   L<<Logger::Warning<<\"Loading RPZ zone '\"<<zone<<\"' from \"<<master.toStringWithPort()<<endl;\n   if(!tt.name.empty())\n     L<<Logger::Warning<<\"With TSIG key '\"<<tt.name<<\"' of algorithm '\"<<tt.algo<<\"'\"<<endl;\n \n   ComboAddress local= master.sin4.sin_family == AF_INET ? ComboAddress(\"0.0.0.0\") : ComboAddress(\"::\"); // should be configurable\n-  AXFRRetriever axfr(master, zone, tt, &local);\n+  AXFRRetriever axfr(master, zone, tt, &local, maxReceivedBytes);\n   unsigned int nrecords=0;\n   Resolver::res_t nop;\n   vector<DNSRecord> chunk;",
        "diff_line_info": {
            "deleted_lines": [
                "shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt)",
                "  AXFRRetriever axfr(master, zone, tt, &local);"
            ],
            "added_lines": [
                "shared_ptr<SOARecordContent> loadRPZFromServer(const ComboAddress& master, const DNSName& zone, DNSFilterEngine& target, const std::string& polName, boost::optional<DNSFilterEngine::Policy> defpol, int place,  const TSIGTriplet& tt, size_t maxReceivedBytes)",
                "  AXFRRetriever axfr(master, zone, tt, &local, maxReceivedBytes);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6307",
        "func_name": "openssl/tls_get_message_header",
        "description": "The state-machine implementation in OpenSSL 1.1.0 before 1.1.0a allocates memory before checking for an excessive length, which might allow remote attackers to cause a denial of service (memory consumption) via crafted TLS messages, related to statem/statem.c and statem/statem_lib.c.",
        "git_url": "https://git.openssl.org/?p=openssl.git;a=commit;h=4b390b6c3f8df925dc92a3dd6b022baa9a2f4650",
        "commit_title": "",
        "commit_text": "Excessive allocation of memory in tls_get_message_header()  A TLS message includes 3 bytes for its length in the header for the message. This would allow for messages up to 16Mb in length. Messages of this length are excessive and OpenSSL includes a check to ensure that a peer is sending reasonably sized messages in order to avoid too much memory being consumed to service a connection. A flaw in the logic of version 1.1.0 means that memory for the message is allocated too early, prior to the excessive message length check. Due to way memory is allocated in OpenSSL this could mean an attacker could force up to 21Mb to be allocated to service a connection. This could lead to a Denial of Service through memory exhaustion. However, the excessive message length check still takes place, and this would cause the connection to immediately fail. Assuming that the application calls SSL_free() on the failed conneciton in a timely manner then the 21Mb of allocated memory will then be immediately freed again. Therefore the excessive memory allocation will be transitory in nature. This then means that there is only a security impact if:  1) The application does not call SSL_free() in a timely manner in the event that the connection fails or 2) The application is working in a constrained environment where there is very little free memory or 3) The attacker initiates multiple connection attempts such that there are multiple connections in a state where memory has been allocated for the connection; SSL_free() has not yet been called; and there is insufficient memory to service the multiple requests.  Except in the instance of (1) above any Denial Of Service is likely to be transitory because as soon as the connection fails the memory is subsequently freed again in the SSL_free() call. However there is an increased risk during this period of application crashes due to the lack of memory - which would then mean a more serious Denial of Service.  This issue does not affect DTLS users.  Issue was reported by Shi Lei (Gear Team, Qihoo 360 Inc.).  CVE-2016-6307  (cherry picked from commit c1ef7c971d0bbf117c3c80f65b5875e2e7b024b1) ",
        "func_before": "int tls_get_message_header(SSL *s, int *mt)\n{\n    /* s->init_num < SSL3_HM_HEADER_LENGTH */\n    int skip_message, i, recvd_type, al;\n    unsigned char *p;\n    unsigned long l;\n\n    p = (unsigned char *)s->init_buf->data;\n\n    do {\n        while (s->init_num < SSL3_HM_HEADER_LENGTH) {\n            i = s->method->ssl_read_bytes(s, SSL3_RT_HANDSHAKE, &recvd_type,\n                                          &p[s->init_num],\n                                          SSL3_HM_HEADER_LENGTH - s->init_num,\n                                          0);\n            if (i <= 0) {\n                s->rwstate = SSL_READING;\n                return 0;\n            }\n            if (recvd_type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n                /*\n                 * A ChangeCipherSpec must be a single byte and may not occur\n                 * in the middle of a handshake message.\n                 */\n                if (s->init_num != 0 || i != 1 || p[0] != SSL3_MT_CCS) {\n                    al = SSL_AD_UNEXPECTED_MESSAGE;\n                    SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER,\n                           SSL_R_BAD_CHANGE_CIPHER_SPEC);\n                    goto f_err;\n                }\n                s->s3->tmp.message_type = *mt = SSL3_MT_CHANGE_CIPHER_SPEC;\n                s->init_num = i - 1;\n                s->s3->tmp.message_size = i;\n                return 1;\n            } else if (recvd_type != SSL3_RT_HANDSHAKE) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, SSL_R_CCS_RECEIVED_EARLY);\n                goto f_err;\n            }\n            s->init_num += i;\n        }\n\n        skip_message = 0;\n        if (!s->server)\n            if (p[0] == SSL3_MT_HELLO_REQUEST)\n                /*\n                 * The server may always send 'Hello Request' messages --\n                 * we are doing a handshake anyway now, so ignore them if\n                 * their format is correct. Does not count for 'Finished'\n                 * MAC.\n                 */\n                if (p[1] == 0 && p[2] == 0 && p[3] == 0) {\n                    s->init_num = 0;\n                    skip_message = 1;\n\n                    if (s->msg_callback)\n                        s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                                        p, SSL3_HM_HEADER_LENGTH, s,\n                                        s->msg_callback_arg);\n                }\n    } while (skip_message);\n    /* s->init_num == SSL3_HM_HEADER_LENGTH */\n\n    *mt = *p;\n    s->s3->tmp.message_type = *(p++);\n\n    if (RECORD_LAYER_is_sslv2_record(&s->rlayer)) {\n        /*\n         * Only happens with SSLv3+ in an SSLv2 backward compatible\n         * ClientHello\n         *\n         * Total message size is the remaining record bytes to read\n         * plus the SSL3_HM_HEADER_LENGTH bytes that we already read\n         */\n        l = RECORD_LAYER_get_rrec_length(&s->rlayer)\n            + SSL3_HM_HEADER_LENGTH;\n        if (l && !BUF_MEM_grow_clean(s->init_buf, (int)l)) {\n            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);\n            goto err;\n        }\n        s->s3->tmp.message_size = l;\n\n        s->init_msg = s->init_buf->data;\n        s->init_num = SSL3_HM_HEADER_LENGTH;\n    } else {\n        n2l3(p, l);\n        /* BUF_MEM_grow takes an 'int' parameter */\n        if (l > (INT_MAX - SSL3_HM_HEADER_LENGTH)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, SSL_R_EXCESSIVE_MESSAGE_SIZE);\n            goto f_err;\n        }\n        if (l && !BUF_MEM_grow_clean(s->init_buf,\n                                     (int)l + SSL3_HM_HEADER_LENGTH)) {\n            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);\n            goto err;\n        }\n        s->s3->tmp.message_size = l;\n\n        s->init_msg = s->init_buf->data + SSL3_HM_HEADER_LENGTH;\n        s->init_num = 0;\n    }\n\n    return 1;\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    return 0;\n}",
        "func": "int tls_get_message_header(SSL *s, int *mt)\n{\n    /* s->init_num < SSL3_HM_HEADER_LENGTH */\n    int skip_message, i, recvd_type, al;\n    unsigned char *p;\n    unsigned long l;\n\n    p = (unsigned char *)s->init_buf->data;\n\n    do {\n        while (s->init_num < SSL3_HM_HEADER_LENGTH) {\n            i = s->method->ssl_read_bytes(s, SSL3_RT_HANDSHAKE, &recvd_type,\n                                          &p[s->init_num],\n                                          SSL3_HM_HEADER_LENGTH - s->init_num,\n                                          0);\n            if (i <= 0) {\n                s->rwstate = SSL_READING;\n                return 0;\n            }\n            if (recvd_type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n                /*\n                 * A ChangeCipherSpec must be a single byte and may not occur\n                 * in the middle of a handshake message.\n                 */\n                if (s->init_num != 0 || i != 1 || p[0] != SSL3_MT_CCS) {\n                    al = SSL_AD_UNEXPECTED_MESSAGE;\n                    SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER,\n                           SSL_R_BAD_CHANGE_CIPHER_SPEC);\n                    goto f_err;\n                }\n                s->s3->tmp.message_type = *mt = SSL3_MT_CHANGE_CIPHER_SPEC;\n                s->init_num = i - 1;\n                s->s3->tmp.message_size = i;\n                return 1;\n            } else if (recvd_type != SSL3_RT_HANDSHAKE) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, SSL_R_CCS_RECEIVED_EARLY);\n                goto f_err;\n            }\n            s->init_num += i;\n        }\n\n        skip_message = 0;\n        if (!s->server)\n            if (p[0] == SSL3_MT_HELLO_REQUEST)\n                /*\n                 * The server may always send 'Hello Request' messages --\n                 * we are doing a handshake anyway now, so ignore them if\n                 * their format is correct. Does not count for 'Finished'\n                 * MAC.\n                 */\n                if (p[1] == 0 && p[2] == 0 && p[3] == 0) {\n                    s->init_num = 0;\n                    skip_message = 1;\n\n                    if (s->msg_callback)\n                        s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                                        p, SSL3_HM_HEADER_LENGTH, s,\n                                        s->msg_callback_arg);\n                }\n    } while (skip_message);\n    /* s->init_num == SSL3_HM_HEADER_LENGTH */\n\n    *mt = *p;\n    s->s3->tmp.message_type = *(p++);\n\n    if (RECORD_LAYER_is_sslv2_record(&s->rlayer)) {\n        /*\n         * Only happens with SSLv3+ in an SSLv2 backward compatible\n         * ClientHello\n         *\n         * Total message size is the remaining record bytes to read\n         * plus the SSL3_HM_HEADER_LENGTH bytes that we already read\n         */\n        l = RECORD_LAYER_get_rrec_length(&s->rlayer)\n            + SSL3_HM_HEADER_LENGTH;\n        s->s3->tmp.message_size = l;\n\n        s->init_msg = s->init_buf->data;\n        s->init_num = SSL3_HM_HEADER_LENGTH;\n    } else {\n        n2l3(p, l);\n        /* BUF_MEM_grow takes an 'int' parameter */\n        if (l > (INT_MAX - SSL3_HM_HEADER_LENGTH)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, SSL_R_EXCESSIVE_MESSAGE_SIZE);\n            goto f_err;\n        }\n        s->s3->tmp.message_size = l;\n\n        s->init_msg = s->init_buf->data + SSL3_HM_HEADER_LENGTH;\n        s->init_num = 0;\n    }\n\n    return 1;\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -74,10 +74,6 @@\n          */\n         l = RECORD_LAYER_get_rrec_length(&s->rlayer)\n             + SSL3_HM_HEADER_LENGTH;\n-        if (l && !BUF_MEM_grow_clean(s->init_buf, (int)l)) {\n-            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);\n-            goto err;\n-        }\n         s->s3->tmp.message_size = l;\n \n         s->init_msg = s->init_buf->data;\n@@ -90,11 +86,6 @@\n             SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, SSL_R_EXCESSIVE_MESSAGE_SIZE);\n             goto f_err;\n         }\n-        if (l && !BUF_MEM_grow_clean(s->init_buf,\n-                                     (int)l + SSL3_HM_HEADER_LENGTH)) {\n-            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);\n-            goto err;\n-        }\n         s->s3->tmp.message_size = l;\n \n         s->init_msg = s->init_buf->data + SSL3_HM_HEADER_LENGTH;\n@@ -104,6 +95,5 @@\n     return 1;\n  f_err:\n     ssl3_send_alert(s, SSL3_AL_FATAL, al);\n- err:\n     return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        if (l && !BUF_MEM_grow_clean(s->init_buf, (int)l)) {",
                "            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);",
                "            goto err;",
                "        }",
                "        if (l && !BUF_MEM_grow_clean(s->init_buf,",
                "                                     (int)l + SSL3_HM_HEADER_LENGTH)) {",
                "            SSLerr(SSL_F_TLS_GET_MESSAGE_HEADER, ERR_R_BUF_LIB);",
                "            goto err;",
                "        }",
                " err:"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-8666",
        "func_name": "torvalds/linux/dev_gro_receive",
        "description": "The IP stack in the Linux kernel before 4.6 allows remote attackers to cause a denial of service (stack consumption and panic) or possibly have unspecified other impact by triggering use of the GRO path for packets with tunnel stacking, as demonstrated by interleaved IPv4 headers and GRE headers, a related issue to CVE-2016-7039.",
        "git_url": "https://github.com/torvalds/linux/commit/fac8e0f579695a3ecbc4d3cac369139d7f819971",
        "commit_title": "tunnels: Don't apply GRO to multiple layers of encapsulation.",
        "commit_text": " When drivers express support for TSO of encapsulated packets, they only mean that they can do it for one layer of encapsulation. Supporting additional levels would mean updating, at a minimum, more IP length fields and they are unaware of this.  No encapsulation device expresses support for handling offloaded encapsulated packets, so we won't generate these types of frames in the transmit path. However, GRO doesn't have a check for multiple levels of encapsulation and will attempt to build them.  UDP tunnel GRO actually does prevent this situation but it only handles multiple UDP tunnels stacked on top of each other. This generalizes that solution to prevent any kind of tunnel stacking that would cause problems. ",
        "func_before": "static enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (!(skb->dev->features & NETIF_F_GRO))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb) || skb->csum_bad)\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->udp_mark = 0;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}",
        "func": "static enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (!(skb->dev->features & NETIF_F_GRO))\n\t\tgoto normal;\n\n\tif (skb_is_gso(skb) || skb_has_frag_list(skb) || skb->csum_bad)\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = 0;\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,7 +26,7 @@\n \t\tNAPI_GRO_CB(skb)->same_flow = 0;\n \t\tNAPI_GRO_CB(skb)->flush = 0;\n \t\tNAPI_GRO_CB(skb)->free = 0;\n-\t\tNAPI_GRO_CB(skb)->udp_mark = 0;\n+\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n \t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n \n \t\t/* Setup for GRO checksum validation */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tNAPI_GRO_CB(skb)->udp_mark = 0;"
            ],
            "added_lines": [
                "\t\tNAPI_GRO_CB(skb)->encap_mark = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8666",
        "func_name": "torvalds/linux/gre_gro_receive",
        "description": "The IP stack in the Linux kernel before 4.6 allows remote attackers to cause a denial of service (stack consumption and panic) or possibly have unspecified other impact by triggering use of the GRO path for packets with tunnel stacking, as demonstrated by interleaved IPv4 headers and GRE headers, a related issue to CVE-2016-7039.",
        "git_url": "https://github.com/torvalds/linux/commit/fac8e0f579695a3ecbc4d3cac369139d7f819971",
        "commit_title": "tunnels: Don't apply GRO to multiple layers of encapsulation.",
        "commit_text": " When drivers express support for TSO of encapsulated packets, they only mean that they can do it for one layer of encapsulation. Supporting additional levels would mean updating, at a minimum, more IP length fields and they are unaware of this.  No encapsulation device expresses support for handling offloaded encapsulated packets, so we won't generate these types of frames in the transmit path. However, GRO doesn't have a check for multiple levels of encapsulation and will attempt to build them.  UDP tunnel GRO actually does prevent this situation but it only handles multiple UDP tunnels stacked on top of each other. This generalizes that solution to prevent any kind of tunnel stacking that would cause problems. ",
        "func_before": "static struct sk_buff **gre_gro_receive(struct sk_buff **head,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct gre_base_hdr *greh;\n\tunsigned int hlen, grehlen;\n\tunsigned int off;\n\tint flush = 1;\n\tstruct packet_offload *ptype;\n\t__be16 type;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*greh);\n\tgreh = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out;\n\t}\n\n\t/* Only support version 0 and K (key), C (csum) flags. Note that\n\t * although the support for the S (seq#) flag can be added easily\n\t * for GRO, this is problematic for GSO hence can not be enabled\n\t * here because a GRO pkt may end up in the forwarding path, thus\n\t * requiring GSO support to break it up correctly.\n\t */\n\tif ((greh->flags & ~(GRE_KEY|GRE_CSUM)) != 0)\n\t\tgoto out;\n\n\ttype = greh->protocol;\n\n\trcu_read_lock();\n\tptype = gro_find_receive_by_type(type);\n\tif (!ptype)\n\t\tgoto out_unlock;\n\n\tgrehlen = GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\thlen = off + grehlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif ((greh->flags & GRE_CSUM) && !NAPI_GRO_CB(skb)->flush) {\n\t\tif (skb_gro_checksum_simple_validate(skb))\n\t\t\tgoto out_unlock;\n\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_GRE, 0,\n\t\t\t\t\t     null_compute_pseudo);\n\t}\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct gre_base_hdr *greh2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\t/* The following checks are needed to ensure only pkts\n\t\t * from the same tunnel are considered for aggregation.\n\t\t * The criteria for \"the same tunnel\" includes:\n\t\t * 1) same version (we only support version 0 here)\n\t\t * 2) same protocol (we only support ETH_P_IP for now)\n\t\t * 3) same set of flags\n\t\t * 4) same key if the key field is present.\n\t\t */\n\t\tgreh2 = (struct gre_base_hdr *)(p->data + off);\n\n\t\tif (greh2->flags != greh->flags ||\n\t\t    greh2->protocol != greh->protocol) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (greh->flags & GRE_KEY) {\n\t\t\t/* compare keys */\n\t\t\tif (*(__be32 *)(greh2+1) != *(__be32 *)(greh+1)) {\n\t\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, grehlen);\n\n\t/* Adjusted NAPI_GRO_CB(skb)->csum after skb_gro_pull()*/\n\tskb_gro_postpull_rcsum(skb, greh, grehlen);\n\n\tpp = ptype->callbacks.gro_receive(head, skb);\n\tflush = 0;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}",
        "func": "static struct sk_buff **gre_gro_receive(struct sk_buff **head,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct sk_buff *p;\n\tconst struct gre_base_hdr *greh;\n\tunsigned int hlen, grehlen;\n\tunsigned int off;\n\tint flush = 1;\n\tstruct packet_offload *ptype;\n\t__be16 type;\n\n\tif (NAPI_GRO_CB(skb)->encap_mark)\n\t\tgoto out;\n\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\toff = skb_gro_offset(skb);\n\thlen = off + sizeof(*greh);\n\tgreh = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out;\n\t}\n\n\t/* Only support version 0 and K (key), C (csum) flags. Note that\n\t * although the support for the S (seq#) flag can be added easily\n\t * for GRO, this is problematic for GSO hence can not be enabled\n\t * here because a GRO pkt may end up in the forwarding path, thus\n\t * requiring GSO support to break it up correctly.\n\t */\n\tif ((greh->flags & ~(GRE_KEY|GRE_CSUM)) != 0)\n\t\tgoto out;\n\n\ttype = greh->protocol;\n\n\trcu_read_lock();\n\tptype = gro_find_receive_by_type(type);\n\tif (!ptype)\n\t\tgoto out_unlock;\n\n\tgrehlen = GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_KEY)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\tif (greh->flags & GRE_CSUM)\n\t\tgrehlen += GRE_HEADER_SECTION;\n\n\thlen = off + grehlen;\n\tif (skb_gro_header_hard(skb, hlen)) {\n\t\tgreh = skb_gro_header_slow(skb, hlen, off);\n\t\tif (unlikely(!greh))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Don't bother verifying checksum if we're going to flush anyway. */\n\tif ((greh->flags & GRE_CSUM) && !NAPI_GRO_CB(skb)->flush) {\n\t\tif (skb_gro_checksum_simple_validate(skb))\n\t\t\tgoto out_unlock;\n\n\t\tskb_gro_checksum_try_convert(skb, IPPROTO_GRE, 0,\n\t\t\t\t\t     null_compute_pseudo);\n\t}\n\n\tfor (p = *head; p; p = p->next) {\n\t\tconst struct gre_base_hdr *greh2;\n\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\t/* The following checks are needed to ensure only pkts\n\t\t * from the same tunnel are considered for aggregation.\n\t\t * The criteria for \"the same tunnel\" includes:\n\t\t * 1) same version (we only support version 0 here)\n\t\t * 2) same protocol (we only support ETH_P_IP for now)\n\t\t * 3) same set of flags\n\t\t * 4) same key if the key field is present.\n\t\t */\n\t\tgreh2 = (struct gre_base_hdr *)(p->data + off);\n\n\t\tif (greh2->flags != greh->flags ||\n\t\t    greh2->protocol != greh->protocol) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (greh->flags & GRE_KEY) {\n\t\t\t/* compare keys */\n\t\t\tif (*(__be32 *)(greh2+1) != *(__be32 *)(greh+1)) {\n\t\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, grehlen);\n\n\t/* Adjusted NAPI_GRO_CB(skb)->csum after skb_gro_pull()*/\n\tskb_gro_postpull_rcsum(skb, greh, grehlen);\n\n\tpp = ptype->callbacks.gro_receive(head, skb);\n\tflush = 0;\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\n\treturn pp;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,11 @@\n \tint flush = 1;\n \tstruct packet_offload *ptype;\n \t__be16 type;\n+\n+\tif (NAPI_GRO_CB(skb)->encap_mark)\n+\t\tgoto out;\n+\n+\tNAPI_GRO_CB(skb)->encap_mark = 1;\n \n \toff = skb_gro_offset(skb);\n \thlen = off + sizeof(*greh);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (NAPI_GRO_CB(skb)->encap_mark)",
                "\t\tgoto out;",
                "",
                "\tNAPI_GRO_CB(skb)->encap_mark = 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-8666",
        "func_name": "torvalds/linux/udp_gro_receive",
        "description": "The IP stack in the Linux kernel before 4.6 allows remote attackers to cause a denial of service (stack consumption and panic) or possibly have unspecified other impact by triggering use of the GRO path for packets with tunnel stacking, as demonstrated by interleaved IPv4 headers and GRE headers, a related issue to CVE-2016-7039.",
        "git_url": "https://github.com/torvalds/linux/commit/fac8e0f579695a3ecbc4d3cac369139d7f819971",
        "commit_title": "tunnels: Don't apply GRO to multiple layers of encapsulation.",
        "commit_text": " When drivers express support for TSO of encapsulated packets, they only mean that they can do it for one layer of encapsulation. Supporting additional levels would mean updating, at a minimum, more IP length fields and they are unaware of this.  No encapsulation device expresses support for handling offloaded encapsulated packets, so we won't generate these types of frames in the transmit path. However, GRO doesn't have a check for multiple levels of encapsulation and will attempt to build them.  UDP tunnel GRO actually does prevent this situation but it only handles multiple UDP tunnels stacked on top of each other. This generalizes that solution to prevent any kind of tunnel stacking that would cause problems. ",
        "func_before": "struct sk_buff **udp_gro_receive(struct sk_buff **head, struct sk_buff *skb,\n\t\t\t\t struct udphdr *uh)\n{\n\tstruct udp_offload_priv *uo_priv;\n\tstruct sk_buff *p, **pp = NULL;\n\tstruct udphdr *uh2;\n\tunsigned int off = skb_gro_offset(skb);\n\tint flush = 1;\n\n\tif (NAPI_GRO_CB(skb)->udp_mark ||\n\t    (skb->ip_summed != CHECKSUM_PARTIAL &&\n\t     NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t     !NAPI_GRO_CB(skb)->csum_valid))\n\t\tgoto out;\n\n\t/* mark that this skb passed once through the udp gro layer */\n\tNAPI_GRO_CB(skb)->udp_mark = 1;\n\n\trcu_read_lock();\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_receive)\n\t\t\tgoto unflush;\n\t}\n\tgoto out_unlock;\n\nunflush:\n\tflush = 0;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = (struct udphdr   *)(p->data + off);\n\n\t\t/* Match ports and either checksums are either both zero\n\t\t * or nonzero.\n\t\t */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source) ||\n\t\t    (!uh->check ^ !uh2->check)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(struct udphdr)); /* pull encapsulating udp header */\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\tpp = uo_priv->offload->callbacks.gro_receive(head, skb,\n\t\t\t\t\t\t     uo_priv->offload);\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\treturn pp;\n}",
        "func": "struct sk_buff **udp_gro_receive(struct sk_buff **head, struct sk_buff *skb,\n\t\t\t\t struct udphdr *uh)\n{\n\tstruct udp_offload_priv *uo_priv;\n\tstruct sk_buff *p, **pp = NULL;\n\tstruct udphdr *uh2;\n\tunsigned int off = skb_gro_offset(skb);\n\tint flush = 1;\n\n\tif (NAPI_GRO_CB(skb)->encap_mark ||\n\t    (skb->ip_summed != CHECKSUM_PARTIAL &&\n\t     NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t     !NAPI_GRO_CB(skb)->csum_valid))\n\t\tgoto out;\n\n\t/* mark that this skb passed once through the tunnel gro layer */\n\tNAPI_GRO_CB(skb)->encap_mark = 1;\n\n\trcu_read_lock();\n\tuo_priv = rcu_dereference(udp_offload_base);\n\tfor (; uo_priv != NULL; uo_priv = rcu_dereference(uo_priv->next)) {\n\t\tif (net_eq(read_pnet(&uo_priv->net), dev_net(skb->dev)) &&\n\t\t    uo_priv->offload->port == uh->dest &&\n\t\t    uo_priv->offload->callbacks.gro_receive)\n\t\t\tgoto unflush;\n\t}\n\tgoto out_unlock;\n\nunflush:\n\tflush = 0;\n\n\tfor (p = *head; p; p = p->next) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = (struct udphdr   *)(p->data + off);\n\n\t\t/* Match ports and either checksums are either both zero\n\t\t * or nonzero.\n\t\t */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source) ||\n\t\t    (!uh->check ^ !uh2->check)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tskb_gro_pull(skb, sizeof(struct udphdr)); /* pull encapsulating udp header */\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\tNAPI_GRO_CB(skb)->proto = uo_priv->offload->ipproto;\n\tpp = uo_priv->offload->callbacks.gro_receive(head, skb,\n\t\t\t\t\t\t     uo_priv->offload);\n\nout_unlock:\n\trcu_read_unlock();\nout:\n\tNAPI_GRO_CB(skb)->flush |= flush;\n\treturn pp;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,14 +7,14 @@\n \tunsigned int off = skb_gro_offset(skb);\n \tint flush = 1;\n \n-\tif (NAPI_GRO_CB(skb)->udp_mark ||\n+\tif (NAPI_GRO_CB(skb)->encap_mark ||\n \t    (skb->ip_summed != CHECKSUM_PARTIAL &&\n \t     NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n \t     !NAPI_GRO_CB(skb)->csum_valid))\n \t\tgoto out;\n \n-\t/* mark that this skb passed once through the udp gro layer */\n-\tNAPI_GRO_CB(skb)->udp_mark = 1;\n+\t/* mark that this skb passed once through the tunnel gro layer */\n+\tNAPI_GRO_CB(skb)->encap_mark = 1;\n \n \trcu_read_lock();\n \tuo_priv = rcu_dereference(udp_offload_base);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (NAPI_GRO_CB(skb)->udp_mark ||",
                "\t/* mark that this skb passed once through the udp gro layer */",
                "\tNAPI_GRO_CB(skb)->udp_mark = 1;"
            ],
            "added_lines": [
                "\tif (NAPI_GRO_CB(skb)->encap_mark ||",
                "\t/* mark that this skb passed once through the tunnel gro layer */",
                "\tNAPI_GRO_CB(skb)->encap_mark = 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/create_mnt_ns",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}",
        "func": "static struct mnt_namespace *create_mnt_ns(struct vfsmount *m)\n{\n\tstruct mnt_namespace *new_ns = alloc_mnt_ns(&init_user_ns);\n\tif (!IS_ERR(new_ns)) {\n\t\tstruct mount *mnt = real_mount(m);\n\t\tmnt->mnt_ns = new_ns;\n\t\tnew_ns->root = mnt;\n\t\tnew_ns->mounts++;\n\t\tlist_add(&mnt->mnt_list, &new_ns->list);\n\t} else {\n\t\tmntput(m);\n\t}\n\treturn new_ns;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n \t\tstruct mount *mnt = real_mount(m);\n \t\tmnt->mnt_ns = new_ns;\n \t\tnew_ns->root = mnt;\n+\t\tnew_ns->mounts++;\n \t\tlist_add(&mnt->mnt_list, &new_ns->list);\n \t} else {\n \t\tmntput(m);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tnew_ns->mounts++;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/attach_recursive_mnt",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\treturn err;\n}",
        "func": "static int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tstruct path *parent_path)\n{\n\tHLIST_HEAD(tree_list);\n\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\t/* Is there space to add these mounts to the mount namespace? */\n\tif (!parent_path) {\n\t\terr = count_mounts(ns, source_mnt);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (parent_path) {\n\t\tdetach_mnt(source_mnt, parent_path);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt, NULL);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt_last(&child->mnt_parent->mnt,\n\t\t\t\t      child->mnt_mountpoint);\n\t\tcommit_tree(child, q);\n\t}\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\tns->pending_mounts = 0;\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,17 @@\n \t\t\tstruct path *parent_path)\n {\n \tHLIST_HEAD(tree_list);\n+\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;\n \tstruct mount *child, *p;\n \tstruct hlist_node *n;\n \tint err;\n+\n+\t/* Is there space to add these mounts to the mount namespace? */\n+\tif (!parent_path) {\n+\t\terr = count_mounts(ns, source_mnt);\n+\t\tif (err)\n+\t\t\tgoto out;\n+\t}\n \n \tif (IS_MNT_SHARED(dest_mnt)) {\n \t\terr = invent_group_ids(source_mnt, true);\n@@ -44,10 +52,12 @@\n  out_cleanup_ids:\n \twhile (!hlist_empty(&tree_list)) {\n \t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n+\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;\n \t\tumount_tree(child, UMOUNT_SYNC);\n \t}\n \tunlock_mount_hash();\n \tcleanup_group_ids(source_mnt, NULL);\n  out:\n+\tns->pending_mounts = 0;\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;",
                "",
                "\t/* Is there space to add these mounts to the mount namespace? */",
                "\tif (!parent_path) {",
                "\t\terr = count_mounts(ns, source_mnt);",
                "\t\tif (err)",
                "\t\t\tgoto out;",
                "\t}",
                "\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;",
                "\tns->pending_mounts = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/commit_tree",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tattach_shadowed(mnt, parent, shadows);\n\ttouch_mnt_namespace(n);\n}",
        "func": "static void commit_tree(struct mount *mnt, struct mount *shadows)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tn->mounts += n->pending_mounts;\n\tn->pending_mounts = 0;\n\n\tattach_shadowed(mnt, parent, shadows);\n\ttouch_mnt_namespace(n);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,9 @@\n \n \tlist_splice(&head, n->list.prev);\n \n+\tn->mounts += n->pending_mounts;\n+\tn->pending_mounts = 0;\n+\n \tattach_shadowed(mnt, parent, shadows);\n \ttouch_mnt_namespace(n);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tn->mounts += n->pending_mounts;",
                "\tn->pending_mounts = 0;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/umount_tree",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\t__touch_mnt_namespace(p->mnt_ns);\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}",
        "func": "static void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tstruct mnt_namespace *ns;\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\tns = p->mnt_ns;\n\t\tif (ns) {\n\t\t\tns->mounts--;\n\t\t\t__touch_mnt_namespace(ns);\n\t\t}\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\n\t\tpin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,\n\t\t\t\t disconnect ? &unmounted : NULL);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,11 +22,16 @@\n \t\tpropagate_umount(&tmp_list);\n \n \twhile (!list_empty(&tmp_list)) {\n+\t\tstruct mnt_namespace *ns;\n \t\tbool disconnect;\n \t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n \t\tlist_del_init(&p->mnt_expire);\n \t\tlist_del_init(&p->mnt_list);\n-\t\t__touch_mnt_namespace(p->mnt_ns);\n+\t\tns = p->mnt_ns;\n+\t\tif (ns) {\n+\t\t\tns->mounts--;\n+\t\t\t__touch_mnt_namespace(ns);\n+\t\t}\n \t\tp->mnt_ns = NULL;\n \t\tif (how & UMOUNT_SYNC)\n \t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t__touch_mnt_namespace(p->mnt_ns);"
            ],
            "added_lines": [
                "\t\tstruct mnt_namespace *ns;",
                "\t\tns = p->mnt_ns;",
                "\t\tif (ns) {",
                "\t\t\tns->mounts--;",
                "\t\t\t__touch_mnt_namespace(ns);",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/alloc_mnt_ns",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tret = ns_alloc_inum(&new_ns->ns);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\treturn new_ns;\n}",
        "func": "static struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kmalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tret = ns_alloc_inum(&new_ns->ns);\n\tif (ret) {\n\t\tkfree(new_ns);\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(ret);\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\tatomic_set(&new_ns->count, 1);\n\tnew_ns->root = NULL;\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tnew_ns->event = 0;\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\tnew_ns->mounts = 0;\n\tnew_ns->pending_mounts = 0;\n\treturn new_ns;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,5 +28,7 @@\n \tnew_ns->event = 0;\n \tnew_ns->user_ns = get_user_ns(user_ns);\n \tnew_ns->ucounts = ucounts;\n+\tnew_ns->mounts = 0;\n+\tnew_ns->pending_mounts = 0;\n \treturn new_ns;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tnew_ns->mounts = 0;",
                "\tnew_ns->pending_mounts = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/copy_mnt_ns",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}",
        "func": "struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tnew_ns->mounts++;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -44,6 +44,7 @@\n \tq = new;\n \twhile (p) {\n \t\tq->mnt_ns = new_ns;\n+\t\tnew_ns->mounts++;\n \t\tif (new_fs) {\n \t\t\tif (&p->mnt == new_fs->root.mnt) {\n \t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tnew_ns->mounts++;"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-6213",
        "func_name": "torvalds/linux/propagate_one",
        "description": "fs/namespace.c in the Linux kernel before 4.9 does not restrict how many mounts may exist in a mount namespace, which allows local users to cause a denial of service (memory consumption and deadlock) via MS_BIND mount system calls, as demonstrated by a loop that triggers exponential growth in the number of mounts.",
        "git_url": "https://github.com/torvalds/linux/commit/d29216842a85c7970c536108e093963f02714498",
        "commit_title": "mnt: Add a per mount namespace limit on the number of mounts",
        "commit_text": " CAI Qian <caiqian@redhat.com> pointed out that the semantics of shared subtrees make it possible to create an exponentially increasing number of mounts in a mount namespace.      mkdir /tmp/1 /tmp/2     mount --make-rshared /     for i in $(seq 1 20) ; do mount --bind /tmp/1 /tmp/2 ; done  Will create create 2^20 or 1048576 mounts, which is a practical problem as some people have managed to hit this by accident.  As such CVE-2016-6213 was assigned.  Ian Kent <raven@themaw.net> described the situation for autofs users as follows:  > The number of mounts for direct mount maps is usually not very large because of > the way they are implemented, large direct mount maps can have performance > problems. There can be anywhere from a few (likely case a few hundred) to less > than 10000, plus mounts that have been triggered and not yet expired. > > Indirect mounts have one autofs mount at the root plus the number of mounts that > have been triggered and not yet expired. > > The number of autofs indirect map entries can range from a few to the common > case of several thousand and in rare cases up to between 30000 and 50000. I've > not heard of people with maps larger than 50000 entries. > > The larger the number of map entries the greater the possibility for a large > number of active mounts so it's not hard to expect cases of a 1000 or somewhat > more active mounts.  So I am setting the default number of mounts allowed per mount namespace at 100,000.  This is more than enough for any use case I know of, but small enough to quickly stop an exponential increase in mounts.  Which should be perfect to catch misconfigurations and malfunctioning programs.  For anyone who needs a higher limit this can be changed by writing to the new /proc/sys/fs/mount-max sysctl. ",
        "func_before": "static int propagate_one(struct mount *m)\n{\n\tstruct mount *child;\n\tint type;\n\t/* skip ones added by this propagate_mnt() */\n\tif (IS_MNT_NEW(m))\n\t\treturn 0;\n\t/* skip if mountpoint isn't covered by it */\n\tif (!is_subdir(mp->m_dentry, m->mnt.mnt_root))\n\t\treturn 0;\n\tif (peers(m, last_dest)) {\n\t\ttype = CL_MAKE_SHARED;\n\t} else {\n\t\tstruct mount *n, *p;\n\t\tbool done;\n\t\tfor (n = m; ; n = p) {\n\t\t\tp = n->mnt_master;\n\t\t\tif (p == dest_master || IS_MNT_MARKED(p))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tstruct mount *parent = last_source->mnt_parent;\n\t\t\tif (last_source == first_source)\n\t\t\t\tbreak;\n\t\t\tdone = parent->mnt_master == p;\n\t\t\tif (done && peers(n, parent))\n\t\t\t\tbreak;\n\t\t\tlast_source = last_source->mnt_master;\n\t\t} while (!done);\n\n\t\ttype = CL_SLAVE;\n\t\t/* beginning of peer group among the slaves? */\n\t\tif (IS_MNT_SHARED(m))\n\t\t\ttype |= CL_MAKE_SHARED;\n\t}\n\t\t\n\t/* Notice when we are propagating across user namespaces */\n\tif (m->mnt_ns->user_ns != user_ns)\n\t\ttype |= CL_UNPRIVILEGED;\n\tchild = copy_tree(last_source, last_source->mnt.mnt_root, type);\n\tif (IS_ERR(child))\n\t\treturn PTR_ERR(child);\n\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\tmnt_set_mountpoint(m, mp, child);\n\tlast_dest = m;\n\tlast_source = child;\n\tif (m->mnt_master != dest_master) {\n\t\tread_seqlock_excl(&mount_lock);\n\t\tSET_MNT_MARK(m->mnt_master);\n\t\tread_sequnlock_excl(&mount_lock);\n\t}\n\thlist_add_head(&child->mnt_hash, list);\n\treturn 0;\n}",
        "func": "static int propagate_one(struct mount *m)\n{\n\tstruct mount *child;\n\tint type;\n\t/* skip ones added by this propagate_mnt() */\n\tif (IS_MNT_NEW(m))\n\t\treturn 0;\n\t/* skip if mountpoint isn't covered by it */\n\tif (!is_subdir(mp->m_dentry, m->mnt.mnt_root))\n\t\treturn 0;\n\tif (peers(m, last_dest)) {\n\t\ttype = CL_MAKE_SHARED;\n\t} else {\n\t\tstruct mount *n, *p;\n\t\tbool done;\n\t\tfor (n = m; ; n = p) {\n\t\t\tp = n->mnt_master;\n\t\t\tif (p == dest_master || IS_MNT_MARKED(p))\n\t\t\t\tbreak;\n\t\t}\n\t\tdo {\n\t\t\tstruct mount *parent = last_source->mnt_parent;\n\t\t\tif (last_source == first_source)\n\t\t\t\tbreak;\n\t\t\tdone = parent->mnt_master == p;\n\t\t\tif (done && peers(n, parent))\n\t\t\t\tbreak;\n\t\t\tlast_source = last_source->mnt_master;\n\t\t} while (!done);\n\n\t\ttype = CL_SLAVE;\n\t\t/* beginning of peer group among the slaves? */\n\t\tif (IS_MNT_SHARED(m))\n\t\t\ttype |= CL_MAKE_SHARED;\n\t}\n\t\t\n\t/* Notice when we are propagating across user namespaces */\n\tif (m->mnt_ns->user_ns != user_ns)\n\t\ttype |= CL_UNPRIVILEGED;\n\tchild = copy_tree(last_source, last_source->mnt.mnt_root, type);\n\tif (IS_ERR(child))\n\t\treturn PTR_ERR(child);\n\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\tmnt_set_mountpoint(m, mp, child);\n\tlast_dest = m;\n\tlast_source = child;\n\tif (m->mnt_master != dest_master) {\n\t\tread_seqlock_excl(&mount_lock);\n\t\tSET_MNT_MARK(m->mnt_master);\n\t\tread_sequnlock_excl(&mount_lock);\n\t}\n\thlist_add_head(&child->mnt_hash, list);\n\treturn count_mounts(m->mnt_ns, child);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,5 +50,5 @@\n \t\tread_sequnlock_excl(&mount_lock);\n \t}\n \thlist_add_head(&child->mnt_hash, list);\n-\treturn 0;\n+\treturn count_mounts(m->mnt_ns, child);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn 0;"
            ],
            "added_lines": [
                "\treturn count_mounts(m->mnt_ns, child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9685",
        "func_name": "torvalds/linux/xfs_attr3_leaf_list_int",
        "description": "Multiple memory leaks in error paths in fs/xfs/xfs_attr_list.c in the Linux kernel before 4.5.1 allow local users to cause a denial of service (memory consumption) via crafted XFS filesystem operations.",
        "git_url": "https://github.com/torvalds/linux/commit/2e83b79b2d6c78bf1b4aa227938a214dcbddc83f",
        "commit_title": "xfs: fix two memory leaks in xfs_attr_list.c error paths",
        "commit_text": " This plugs 2 trivial leaks in xfs_attr_shortform_list and xfs_attr3_leaf_list_int.  Cc: <stable@vger.kernel.org>",
        "func_before": "int\nxfs_attr3_leaf_list_int(\n\tstruct xfs_buf\t\t\t*bp,\n\tstruct xfs_attr_list_context\t*context)\n{\n\tstruct attrlist_cursor_kern\t*cursor;\n\tstruct xfs_attr_leafblock\t*leaf;\n\tstruct xfs_attr3_icleaf_hdr\tichdr;\n\tstruct xfs_attr_leaf_entry\t*entries;\n\tstruct xfs_attr_leaf_entry\t*entry;\n\tint\t\t\t\tretval;\n\tint\t\t\t\ti;\n\tstruct xfs_mount\t\t*mp = context->dp->i_mount;\n\n\ttrace_xfs_attr_list_leaf(context);\n\n\tleaf = bp->b_addr;\n\txfs_attr3_leaf_hdr_from_disk(mp->m_attr_geo, &ichdr, leaf);\n\tentries = xfs_attr3_leaf_entryp(leaf);\n\n\tcursor = context->cursor;\n\tcursor->initted = 1;\n\n\t/*\n\t * Re-find our place in the leaf block if this is a new syscall.\n\t */\n\tif (context->resynch) {\n\t\tentry = &entries[0];\n\t\tfor (i = 0; i < ichdr.count; entry++, i++) {\n\t\t\tif (be32_to_cpu(entry->hashval) == cursor->hashval) {\n\t\t\t\tif (cursor->offset == context->dupcnt) {\n\t\t\t\t\tcontext->dupcnt = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontext->dupcnt++;\n\t\t\t} else if (be32_to_cpu(entry->hashval) >\n\t\t\t\t\tcursor->hashval) {\n\t\t\t\tcontext->dupcnt = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == ichdr.count) {\n\t\t\ttrace_xfs_attr_list_notfound(context);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tentry = &entries[0];\n\t\ti = 0;\n\t}\n\tcontext->resynch = 0;\n\n\t/*\n\t * We have found our place, start copying out the new attributes.\n\t */\n\tretval = 0;\n\tfor (; i < ichdr.count; entry++, i++) {\n\t\tif (be32_to_cpu(entry->hashval) != cursor->hashval) {\n\t\t\tcursor->hashval = be32_to_cpu(entry->hashval);\n\t\t\tcursor->offset = 0;\n\t\t}\n\n\t\tif (entry->flags & XFS_ATTR_INCOMPLETE)\n\t\t\tcontinue;\t\t/* skip incomplete entries */\n\n\t\tif (entry->flags & XFS_ATTR_LOCAL) {\n\t\t\txfs_attr_leaf_name_local_t *name_loc =\n\t\t\t\txfs_attr3_leaf_name_local(leaf, i);\n\n\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\tname_loc->nameval,\n\t\t\t\t\t\t(int)name_loc->namelen,\n\t\t\t\t\t\tbe16_to_cpu(name_loc->valuelen),\n\t\t\t\t\t\t&name_loc->nameval[name_loc->namelen]);\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\t\t} else {\n\t\t\txfs_attr_leaf_name_remote_t *name_rmt =\n\t\t\t\txfs_attr3_leaf_name_remote(leaf, i);\n\n\t\t\tint valuelen = be32_to_cpu(name_rmt->valuelen);\n\n\t\t\tif (context->put_value) {\n\t\t\t\txfs_da_args_t args;\n\n\t\t\t\tmemset((char *)&args, 0, sizeof(args));\n\t\t\t\targs.geo = context->dp->i_mount->m_attr_geo;\n\t\t\t\targs.dp = context->dp;\n\t\t\t\targs.whichfork = XFS_ATTR_FORK;\n\t\t\t\targs.valuelen = valuelen;\n\t\t\t\targs.rmtvaluelen = valuelen;\n\t\t\t\targs.value = kmem_alloc(valuelen, KM_SLEEP | KM_NOFS);\n\t\t\t\targs.rmtblkno = be32_to_cpu(name_rmt->valueblk);\n\t\t\t\targs.rmtblkcnt = xfs_attr3_rmt_blocks(\n\t\t\t\t\t\t\targs.dp->i_mount, valuelen);\n\t\t\t\tretval = xfs_attr_rmtval_get(&args);\n\t\t\t\tif (retval)\n\t\t\t\t\treturn retval;\n\t\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\tname_rmt->name,\n\t\t\t\t\t\t(int)name_rmt->namelen,\n\t\t\t\t\t\tvaluelen,\n\t\t\t\t\t\targs.value);\n\t\t\t\tkmem_free(args.value);\n\t\t\t} else {\n\t\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\tname_rmt->name,\n\t\t\t\t\t\t(int)name_rmt->namelen,\n\t\t\t\t\t\tvaluelen,\n\t\t\t\t\t\tNULL);\n\t\t\t}\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\t\t}\n\t\tif (context->seen_enough)\n\t\t\tbreak;\n\t\tcursor->offset++;\n\t}\n\ttrace_xfs_attr_list_leaf_end(context);\n\treturn retval;\n}",
        "func": "int\nxfs_attr3_leaf_list_int(\n\tstruct xfs_buf\t\t\t*bp,\n\tstruct xfs_attr_list_context\t*context)\n{\n\tstruct attrlist_cursor_kern\t*cursor;\n\tstruct xfs_attr_leafblock\t*leaf;\n\tstruct xfs_attr3_icleaf_hdr\tichdr;\n\tstruct xfs_attr_leaf_entry\t*entries;\n\tstruct xfs_attr_leaf_entry\t*entry;\n\tint\t\t\t\tretval;\n\tint\t\t\t\ti;\n\tstruct xfs_mount\t\t*mp = context->dp->i_mount;\n\n\ttrace_xfs_attr_list_leaf(context);\n\n\tleaf = bp->b_addr;\n\txfs_attr3_leaf_hdr_from_disk(mp->m_attr_geo, &ichdr, leaf);\n\tentries = xfs_attr3_leaf_entryp(leaf);\n\n\tcursor = context->cursor;\n\tcursor->initted = 1;\n\n\t/*\n\t * Re-find our place in the leaf block if this is a new syscall.\n\t */\n\tif (context->resynch) {\n\t\tentry = &entries[0];\n\t\tfor (i = 0; i < ichdr.count; entry++, i++) {\n\t\t\tif (be32_to_cpu(entry->hashval) == cursor->hashval) {\n\t\t\t\tif (cursor->offset == context->dupcnt) {\n\t\t\t\t\tcontext->dupcnt = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontext->dupcnt++;\n\t\t\t} else if (be32_to_cpu(entry->hashval) >\n\t\t\t\t\tcursor->hashval) {\n\t\t\t\tcontext->dupcnt = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == ichdr.count) {\n\t\t\ttrace_xfs_attr_list_notfound(context);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tentry = &entries[0];\n\t\ti = 0;\n\t}\n\tcontext->resynch = 0;\n\n\t/*\n\t * We have found our place, start copying out the new attributes.\n\t */\n\tretval = 0;\n\tfor (; i < ichdr.count; entry++, i++) {\n\t\tif (be32_to_cpu(entry->hashval) != cursor->hashval) {\n\t\t\tcursor->hashval = be32_to_cpu(entry->hashval);\n\t\t\tcursor->offset = 0;\n\t\t}\n\n\t\tif (entry->flags & XFS_ATTR_INCOMPLETE)\n\t\t\tcontinue;\t\t/* skip incomplete entries */\n\n\t\tif (entry->flags & XFS_ATTR_LOCAL) {\n\t\t\txfs_attr_leaf_name_local_t *name_loc =\n\t\t\t\txfs_attr3_leaf_name_local(leaf, i);\n\n\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\tname_loc->nameval,\n\t\t\t\t\t\t(int)name_loc->namelen,\n\t\t\t\t\t\tbe16_to_cpu(name_loc->valuelen),\n\t\t\t\t\t\t&name_loc->nameval[name_loc->namelen]);\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\t\t} else {\n\t\t\txfs_attr_leaf_name_remote_t *name_rmt =\n\t\t\t\txfs_attr3_leaf_name_remote(leaf, i);\n\n\t\t\tint valuelen = be32_to_cpu(name_rmt->valuelen);\n\n\t\t\tif (context->put_value) {\n\t\t\t\txfs_da_args_t args;\n\n\t\t\t\tmemset((char *)&args, 0, sizeof(args));\n\t\t\t\targs.geo = context->dp->i_mount->m_attr_geo;\n\t\t\t\targs.dp = context->dp;\n\t\t\t\targs.whichfork = XFS_ATTR_FORK;\n\t\t\t\targs.valuelen = valuelen;\n\t\t\t\targs.rmtvaluelen = valuelen;\n\t\t\t\targs.value = kmem_alloc(valuelen, KM_SLEEP | KM_NOFS);\n\t\t\t\targs.rmtblkno = be32_to_cpu(name_rmt->valueblk);\n\t\t\t\targs.rmtblkcnt = xfs_attr3_rmt_blocks(\n\t\t\t\t\t\t\targs.dp->i_mount, valuelen);\n\t\t\t\tretval = xfs_attr_rmtval_get(&args);\n\t\t\t\tif (!retval)\n\t\t\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\t\tname_rmt->name,\n\t\t\t\t\t\t\t(int)name_rmt->namelen,\n\t\t\t\t\t\t\tvaluelen,\n\t\t\t\t\t\t\targs.value);\n\t\t\t\tkmem_free(args.value);\n\t\t\t} else {\n\t\t\t\tretval = context->put_listent(context,\n\t\t\t\t\t\tentry->flags,\n\t\t\t\t\t\tname_rmt->name,\n\t\t\t\t\t\t(int)name_rmt->namelen,\n\t\t\t\t\t\tvaluelen,\n\t\t\t\t\t\tNULL);\n\t\t\t}\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\t\t}\n\t\tif (context->seen_enough)\n\t\t\tbreak;\n\t\tcursor->offset++;\n\t}\n\ttrace_xfs_attr_list_leaf_end(context);\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -94,14 +94,13 @@\n \t\t\t\targs.rmtblkcnt = xfs_attr3_rmt_blocks(\n \t\t\t\t\t\t\targs.dp->i_mount, valuelen);\n \t\t\t\tretval = xfs_attr_rmtval_get(&args);\n-\t\t\t\tif (retval)\n-\t\t\t\t\treturn retval;\n-\t\t\t\tretval = context->put_listent(context,\n-\t\t\t\t\t\tentry->flags,\n-\t\t\t\t\t\tname_rmt->name,\n-\t\t\t\t\t\t(int)name_rmt->namelen,\n-\t\t\t\t\t\tvaluelen,\n-\t\t\t\t\t\targs.value);\n+\t\t\t\tif (!retval)\n+\t\t\t\t\tretval = context->put_listent(context,\n+\t\t\t\t\t\t\tentry->flags,\n+\t\t\t\t\t\t\tname_rmt->name,\n+\t\t\t\t\t\t\t(int)name_rmt->namelen,\n+\t\t\t\t\t\t\tvaluelen,\n+\t\t\t\t\t\t\targs.value);\n \t\t\t\tkmem_free(args.value);\n \t\t\t} else {\n \t\t\t\tretval = context->put_listent(context,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tif (retval)",
                "\t\t\t\t\treturn retval;",
                "\t\t\t\tretval = context->put_listent(context,",
                "\t\t\t\t\t\tentry->flags,",
                "\t\t\t\t\t\tname_rmt->name,",
                "\t\t\t\t\t\t(int)name_rmt->namelen,",
                "\t\t\t\t\t\tvaluelen,",
                "\t\t\t\t\t\targs.value);"
            ],
            "added_lines": [
                "\t\t\t\tif (!retval)",
                "\t\t\t\t\tretval = context->put_listent(context,",
                "\t\t\t\t\t\t\tentry->flags,",
                "\t\t\t\t\t\t\tname_rmt->name,",
                "\t\t\t\t\t\t\t(int)name_rmt->namelen,",
                "\t\t\t\t\t\t\tvaluelen,",
                "\t\t\t\t\t\t\targs.value);"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9685",
        "func_name": "torvalds/linux/xfs_attr_shortform_list",
        "description": "Multiple memory leaks in error paths in fs/xfs/xfs_attr_list.c in the Linux kernel before 4.5.1 allow local users to cause a denial of service (memory consumption) via crafted XFS filesystem operations.",
        "git_url": "https://github.com/torvalds/linux/commit/2e83b79b2d6c78bf1b4aa227938a214dcbddc83f",
        "commit_title": "xfs: fix two memory leaks in xfs_attr_list.c error paths",
        "commit_text": " This plugs 2 trivial leaks in xfs_attr_shortform_list and xfs_attr3_leaf_list_int.  Cc: <stable@vger.kernel.org>",
        "func_before": "int\nxfs_attr_shortform_list(xfs_attr_list_context_t *context)\n{\n\tattrlist_cursor_kern_t *cursor;\n\txfs_attr_sf_sort_t *sbuf, *sbp;\n\txfs_attr_shortform_t *sf;\n\txfs_attr_sf_entry_t *sfe;\n\txfs_inode_t *dp;\n\tint sbsize, nsbuf, count, i;\n\tint error;\n\n\tASSERT(context != NULL);\n\tdp = context->dp;\n\tASSERT(dp != NULL);\n\tASSERT(dp->i_afp != NULL);\n\tsf = (xfs_attr_shortform_t *)dp->i_afp->if_u1.if_data;\n\tASSERT(sf != NULL);\n\tif (!sf->hdr.count)\n\t\treturn 0;\n\tcursor = context->cursor;\n\tASSERT(cursor != NULL);\n\n\ttrace_xfs_attr_list_sf(context);\n\n\t/*\n\t * If the buffer is large enough and the cursor is at the start,\n\t * do not bother with sorting since we will return everything in\n\t * one buffer and another call using the cursor won't need to be\n\t * made.\n\t * Note the generous fudge factor of 16 overhead bytes per entry.\n\t * If bufsize is zero then put_listent must be a search function\n\t * and can just scan through what we have.\n\t */\n\tif (context->bufsize == 0 ||\n\t    (XFS_ISRESET_CURSOR(cursor) &&\n             (dp->i_afp->if_bytes + sf->hdr.count * 16) < context->bufsize)) {\n\t\tfor (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) {\n\t\t\terror = context->put_listent(context,\n\t\t\t\t\t   sfe->flags,\n\t\t\t\t\t   sfe->nameval,\n\t\t\t\t\t   (int)sfe->namelen,\n\t\t\t\t\t   (int)sfe->valuelen,\n\t\t\t\t\t   &sfe->nameval[sfe->namelen]);\n\n\t\t\t/*\n\t\t\t * Either search callback finished early or\n\t\t\t * didn't fit it all in the buffer after all.\n\t\t\t */\n\t\t\tif (context->seen_enough)\n\t\t\t\tbreak;\n\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\tsfe = XFS_ATTR_SF_NEXTENTRY(sfe);\n\t\t}\n\t\ttrace_xfs_attr_list_sf_all(context);\n\t\treturn 0;\n\t}\n\n\t/* do no more for a search callback */\n\tif (context->bufsize == 0)\n\t\treturn 0;\n\n\t/*\n\t * It didn't all fit, so we have to sort everything on hashval.\n\t */\n\tsbsize = sf->hdr.count * sizeof(*sbuf);\n\tsbp = sbuf = kmem_alloc(sbsize, KM_SLEEP | KM_NOFS);\n\n\t/*\n\t * Scan the attribute list for the rest of the entries, storing\n\t * the relevant info from only those that match into a buffer.\n\t */\n\tnsbuf = 0;\n\tfor (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) {\n\t\tif (unlikely(\n\t\t    ((char *)sfe < (char *)sf) ||\n\t\t    ((char *)sfe >= ((char *)sf + dp->i_afp->if_bytes)))) {\n\t\t\tXFS_CORRUPTION_ERROR(\"xfs_attr_shortform_list\",\n\t\t\t\t\t     XFS_ERRLEVEL_LOW,\n\t\t\t\t\t     context->dp->i_mount, sfe);\n\t\t\tkmem_free(sbuf);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tsbp->entno = i;\n\t\tsbp->hash = xfs_da_hashname(sfe->nameval, sfe->namelen);\n\t\tsbp->name = sfe->nameval;\n\t\tsbp->namelen = sfe->namelen;\n\t\t/* These are bytes, and both on-disk, don't endian-flip */\n\t\tsbp->valuelen = sfe->valuelen;\n\t\tsbp->flags = sfe->flags;\n\t\tsfe = XFS_ATTR_SF_NEXTENTRY(sfe);\n\t\tsbp++;\n\t\tnsbuf++;\n\t}\n\n\t/*\n\t * Sort the entries on hash then entno.\n\t */\n\txfs_sort(sbuf, nsbuf, sizeof(*sbuf), xfs_attr_shortform_compare);\n\n\t/*\n\t * Re-find our place IN THE SORTED LIST.\n\t */\n\tcount = 0;\n\tcursor->initted = 1;\n\tcursor->blkno = 0;\n\tfor (sbp = sbuf, i = 0; i < nsbuf; i++, sbp++) {\n\t\tif (sbp->hash == cursor->hashval) {\n\t\t\tif (cursor->offset == count) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount++;\n\t\t} else if (sbp->hash > cursor->hashval) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (i == nsbuf) {\n\t\tkmem_free(sbuf);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Loop putting entries into the user buffer.\n\t */\n\tfor ( ; i < nsbuf; i++, sbp++) {\n\t\tif (cursor->hashval != sbp->hash) {\n\t\t\tcursor->hashval = sbp->hash;\n\t\t\tcursor->offset = 0;\n\t\t}\n\t\terror = context->put_listent(context,\n\t\t\t\t\tsbp->flags,\n\t\t\t\t\tsbp->name,\n\t\t\t\t\tsbp->namelen,\n\t\t\t\t\tsbp->valuelen,\n\t\t\t\t\t&sbp->name[sbp->namelen]);\n\t\tif (error)\n\t\t\treturn error;\n\t\tif (context->seen_enough)\n\t\t\tbreak;\n\t\tcursor->offset++;\n\t}\n\n\tkmem_free(sbuf);\n\treturn 0;\n}",
        "func": "int\nxfs_attr_shortform_list(xfs_attr_list_context_t *context)\n{\n\tattrlist_cursor_kern_t *cursor;\n\txfs_attr_sf_sort_t *sbuf, *sbp;\n\txfs_attr_shortform_t *sf;\n\txfs_attr_sf_entry_t *sfe;\n\txfs_inode_t *dp;\n\tint sbsize, nsbuf, count, i;\n\tint error;\n\n\tASSERT(context != NULL);\n\tdp = context->dp;\n\tASSERT(dp != NULL);\n\tASSERT(dp->i_afp != NULL);\n\tsf = (xfs_attr_shortform_t *)dp->i_afp->if_u1.if_data;\n\tASSERT(sf != NULL);\n\tif (!sf->hdr.count)\n\t\treturn 0;\n\tcursor = context->cursor;\n\tASSERT(cursor != NULL);\n\n\ttrace_xfs_attr_list_sf(context);\n\n\t/*\n\t * If the buffer is large enough and the cursor is at the start,\n\t * do not bother with sorting since we will return everything in\n\t * one buffer and another call using the cursor won't need to be\n\t * made.\n\t * Note the generous fudge factor of 16 overhead bytes per entry.\n\t * If bufsize is zero then put_listent must be a search function\n\t * and can just scan through what we have.\n\t */\n\tif (context->bufsize == 0 ||\n\t    (XFS_ISRESET_CURSOR(cursor) &&\n             (dp->i_afp->if_bytes + sf->hdr.count * 16) < context->bufsize)) {\n\t\tfor (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) {\n\t\t\terror = context->put_listent(context,\n\t\t\t\t\t   sfe->flags,\n\t\t\t\t\t   sfe->nameval,\n\t\t\t\t\t   (int)sfe->namelen,\n\t\t\t\t\t   (int)sfe->valuelen,\n\t\t\t\t\t   &sfe->nameval[sfe->namelen]);\n\n\t\t\t/*\n\t\t\t * Either search callback finished early or\n\t\t\t * didn't fit it all in the buffer after all.\n\t\t\t */\n\t\t\tif (context->seen_enough)\n\t\t\t\tbreak;\n\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\tsfe = XFS_ATTR_SF_NEXTENTRY(sfe);\n\t\t}\n\t\ttrace_xfs_attr_list_sf_all(context);\n\t\treturn 0;\n\t}\n\n\t/* do no more for a search callback */\n\tif (context->bufsize == 0)\n\t\treturn 0;\n\n\t/*\n\t * It didn't all fit, so we have to sort everything on hashval.\n\t */\n\tsbsize = sf->hdr.count * sizeof(*sbuf);\n\tsbp = sbuf = kmem_alloc(sbsize, KM_SLEEP | KM_NOFS);\n\n\t/*\n\t * Scan the attribute list for the rest of the entries, storing\n\t * the relevant info from only those that match into a buffer.\n\t */\n\tnsbuf = 0;\n\tfor (i = 0, sfe = &sf->list[0]; i < sf->hdr.count; i++) {\n\t\tif (unlikely(\n\t\t    ((char *)sfe < (char *)sf) ||\n\t\t    ((char *)sfe >= ((char *)sf + dp->i_afp->if_bytes)))) {\n\t\t\tXFS_CORRUPTION_ERROR(\"xfs_attr_shortform_list\",\n\t\t\t\t\t     XFS_ERRLEVEL_LOW,\n\t\t\t\t\t     context->dp->i_mount, sfe);\n\t\t\tkmem_free(sbuf);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tsbp->entno = i;\n\t\tsbp->hash = xfs_da_hashname(sfe->nameval, sfe->namelen);\n\t\tsbp->name = sfe->nameval;\n\t\tsbp->namelen = sfe->namelen;\n\t\t/* These are bytes, and both on-disk, don't endian-flip */\n\t\tsbp->valuelen = sfe->valuelen;\n\t\tsbp->flags = sfe->flags;\n\t\tsfe = XFS_ATTR_SF_NEXTENTRY(sfe);\n\t\tsbp++;\n\t\tnsbuf++;\n\t}\n\n\t/*\n\t * Sort the entries on hash then entno.\n\t */\n\txfs_sort(sbuf, nsbuf, sizeof(*sbuf), xfs_attr_shortform_compare);\n\n\t/*\n\t * Re-find our place IN THE SORTED LIST.\n\t */\n\tcount = 0;\n\tcursor->initted = 1;\n\tcursor->blkno = 0;\n\tfor (sbp = sbuf, i = 0; i < nsbuf; i++, sbp++) {\n\t\tif (sbp->hash == cursor->hashval) {\n\t\t\tif (cursor->offset == count) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount++;\n\t\t} else if (sbp->hash > cursor->hashval) {\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (i == nsbuf) {\n\t\tkmem_free(sbuf);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Loop putting entries into the user buffer.\n\t */\n\tfor ( ; i < nsbuf; i++, sbp++) {\n\t\tif (cursor->hashval != sbp->hash) {\n\t\t\tcursor->hashval = sbp->hash;\n\t\t\tcursor->offset = 0;\n\t\t}\n\t\terror = context->put_listent(context,\n\t\t\t\t\tsbp->flags,\n\t\t\t\t\tsbp->name,\n\t\t\t\t\tsbp->namelen,\n\t\t\t\t\tsbp->valuelen,\n\t\t\t\t\t&sbp->name[sbp->namelen]);\n\t\tif (error) {\n\t\t\tkmem_free(sbuf);\n\t\t\treturn error;\n\t\t}\n\t\tif (context->seen_enough)\n\t\t\tbreak;\n\t\tcursor->offset++;\n\t}\n\n\tkmem_free(sbuf);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -135,8 +135,10 @@\n \t\t\t\t\tsbp->namelen,\n \t\t\t\t\tsbp->valuelen,\n \t\t\t\t\t&sbp->name[sbp->namelen]);\n-\t\tif (error)\n+\t\tif (error) {\n+\t\t\tkmem_free(sbuf);\n \t\t\treturn error;\n+\t\t}\n \t\tif (context->seen_enough)\n \t\t\tbreak;\n \t\tcursor->offset++;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (error)"
            ],
            "added_lines": [
                "\t\tif (error) {",
                "\t\t\tkmem_free(sbuf);",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3687",
        "func_name": "torvalds/linux/sctp_assoc_lookup_asconf_ack",
        "description": "The sctp_assoc_lookup_asconf_ack function in net/sctp/associola.c in the SCTP implementation in the Linux kernel through 3.17.2 allows remote attackers to cause a denial of service (panic) via duplicate ASCONF chunks that trigger an incorrect uncork within the side-effect interpreter.",
        "git_url": "https://github.com/torvalds/linux/commit/b69040d8e39f20d5215a03502a8e8b4c6ab78395",
        "commit_title": "net: sctp: fix panic on duplicate ASCONF chunks",
        "commit_text": " When receiving a e.g. semi-good formed connection scan in the form of ...    -------------- INIT[ASCONF; ASCONF_ACK] ------------->   <----------- INIT-ACK[ASCONF; ASCONF_ACK] ------------   -------------------- COOKIE-ECHO -------------------->   <-------------------- COOKIE-ACK ---------------------   ---------------- ASCONF_a; ASCONF_b ----------------->  ... where ASCONF_a equals ASCONF_b chunk (at least both serials need to be equal), we panic an SCTP server!  The problem is that good-formed ASCONF chunks that we reply with ASCONF_ACK chunks are cached per serial. Thus, when we receive a same ASCONF chunk twice (e.g. through a lost ASCONF_ACK), we do not need to process them again on the server side (that was the idea, also proposed in the RFC). Instead, we know it was cached and we just resend the cached chunk instead. So far, so good.  Where things get nasty is in SCTP's side effect interpreter, that is, sctp_cmd_interpreter():  While incoming ASCONF_a (chunk = event_arg) is being marked !end_of_packet and !singleton, and we have an association context, we do not flush the outqueue the first time after processing the ASCONF_ACK singleton chunk via SCTP_CMD_REPLY. Instead, we keep it queued up, although we set local_cork to 1. Commit 2e3216cd54b1 changed the precedence, so that as long as we get bundled, incoming chunks we try possible bundling on outgoing queue as well. Before this commit, we would just flush the output queue.  Now, while ASCONF_a's ASCONF_ACK sits in the corked outq, we continue to process the same ASCONF_b chunk from the packet. As we have cached the previous ASCONF_ACK, we find it, grab it and do another SCTP_CMD_REPLY command on it. So, effectively, we rip the chunk->list pointers and requeue the same ASCONF_ACK chunk another time. Since we process ASCONF_b, it's correctly marked with end_of_packet and we enforce an uncork, and thus flush, thus crashing the kernel.  Fix it by testing if the ASCONF_ACK is currently pending and if that is the case, do not requeue it. When flushing the output queue we may relink the chunk for preparing an outgoing packet, but eventually unlink it when it's copied into the skb right before transmission.  Joint work with Vlad Yasevich. ",
        "func_before": "struct sctp_chunk *sctp_assoc_lookup_asconf_ack(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__be32 serial)\n{\n\tstruct sctp_chunk *ack;\n\n\t/* Walk through the list of cached ASCONF-ACKs and find the\n\t * ack chunk whose serial number matches that of the request.\n\t */\n\tlist_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {\n\t\tif (ack->subh.addip_hdr->serial == serial) {\n\t\t\tsctp_chunk_hold(ack);\n\t\t\treturn ack;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
        "func": "struct sctp_chunk *sctp_assoc_lookup_asconf_ack(\n\t\t\t\t\tconst struct sctp_association *asoc,\n\t\t\t\t\t__be32 serial)\n{\n\tstruct sctp_chunk *ack;\n\n\t/* Walk through the list of cached ASCONF-ACKs and find the\n\t * ack chunk whose serial number matches that of the request.\n\t */\n\tlist_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {\n\t\tif (sctp_chunk_pending(ack))\n\t\t\tcontinue;\n\t\tif (ack->subh.addip_hdr->serial == serial) {\n\t\t\tsctp_chunk_hold(ack);\n\t\t\treturn ack;\n\t\t}\n\t}\n\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,8 @@\n \t * ack chunk whose serial number matches that of the request.\n \t */\n \tlist_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {\n+\t\tif (sctp_chunk_pending(ack))\n+\t\t\tcontinue;\n \t\tif (ack->subh.addip_hdr->serial == serial) {\n \t\t\tsctp_chunk_hold(ack);\n \t\t\treturn ack;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tif (sctp_chunk_pending(ack))",
                "\t\t\tcontinue;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3690",
        "func_name": "torvalds/linux/vmx_set_constant_host_state",
        "description": "arch/x86/kvm/vmx.c in the KVM subsystem in the Linux kernel before 3.17.2 on Intel processors does not ensure that the value in the CR4 control register remains the same after a VM entry, which allows host OS users to kill arbitrary processes or cause a denial of service (system disruption) by leveraging /dev/kvm access, as demonstrated by PR_SET_TSC prctl calls within a modified copy of QEMU.",
        "git_url": "https://github.com/torvalds/linux/commit/d974baa398f34393db76be45f7d4d04fbdbb4a0a",
        "commit_title": "x86,kvm,vmx: Preserve CR4 across VM entry",
        "commit_text": " CR4 isn't constant; at least the TSD and PCE bits can vary.  TBH, treating CR0 and CR3 as constant scares me a bit, too, but it looks like it's correct.  This adds a branch and a read from cr4 to each vm entry.  Because it is extremely likely that consecutive entries into the same vcpu will have the same host cr4 value, this fixes up the vmcs instead of restoring cr4 after the fact.  A subsequent patch will add a kernel-wide cr4 shadow, reducing the overhead in the common case to just two memory reads and a branch.  Cc: stable@vger.kernel.org Cc: Petr Matousek <pmatouse@redhat.com> Cc: Gleb Natapov <gleb@kernel.org>",
        "func_before": "static void vmx_set_constant_host_state(struct vcpu_vmx *vmx)\n{\n\tu32 low32, high32;\n\tunsigned long tmpl;\n\tstruct desc_ptr dt;\n\n\tvmcs_writel(HOST_CR0, read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */\n\tvmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */\n\tvmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\t/*\n\t * Load null selectors, so we can avoid reloading them in\n\t * __vmx_load_host_state(), in case userspace uses the null selectors\n\t * too (the expected case).\n\t */\n\tvmcs_write16(HOST_DS_SELECTOR, 0);\n\tvmcs_write16(HOST_ES_SELECTOR, 0);\n#else\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#endif\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tnative_store_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\tvmx->host_idt_base = dt.address;\n\n\tvmcs_writel(HOST_RIP, vmx_return); /* 22.2.5 */\n\n\trdmsr(MSR_IA32_SYSENTER_CS, low32, high32);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, low32);\n\trdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, low32, high32);\n\t\tvmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));\n\t}\n}",
        "func": "static void vmx_set_constant_host_state(struct vcpu_vmx *vmx)\n{\n\tu32 low32, high32;\n\tunsigned long tmpl;\n\tstruct desc_ptr dt;\n\tunsigned long cr4;\n\n\tvmcs_writel(HOST_CR0, read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */\n\tvmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */\n\n\t/* Save the most likely value for this task's CR4 in the VMCS. */\n\tcr4 = read_cr4();\n\tvmcs_writel(HOST_CR4, cr4);\t\t\t/* 22.2.3, 22.2.5 */\n\tvmx->host_state.vmcs_host_cr4 = cr4;\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\t/*\n\t * Load null selectors, so we can avoid reloading them in\n\t * __vmx_load_host_state(), in case userspace uses the null selectors\n\t * too (the expected case).\n\t */\n\tvmcs_write16(HOST_DS_SELECTOR, 0);\n\tvmcs_write16(HOST_ES_SELECTOR, 0);\n#else\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#endif\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tnative_store_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\tvmx->host_idt_base = dt.address;\n\n\tvmcs_writel(HOST_RIP, vmx_return); /* 22.2.5 */\n\n\trdmsr(MSR_IA32_SYSENTER_CS, low32, high32);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, low32);\n\trdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, low32, high32);\n\t\tvmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,10 +3,15 @@\n \tu32 low32, high32;\n \tunsigned long tmpl;\n \tstruct desc_ptr dt;\n+\tunsigned long cr4;\n \n \tvmcs_writel(HOST_CR0, read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */\n-\tvmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */\n \tvmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */\n+\n+\t/* Save the most likely value for this task's CR4 in the VMCS. */\n+\tcr4 = read_cr4();\n+\tvmcs_writel(HOST_CR4, cr4);\t\t\t/* 22.2.3, 22.2.5 */\n+\tvmx->host_state.vmcs_host_cr4 = cr4;\n \n \tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n #ifdef CONFIG_X86_64",
        "diff_line_info": {
            "deleted_lines": [
                "\tvmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */"
            ],
            "added_lines": [
                "\tunsigned long cr4;",
                "",
                "\t/* Save the most likely value for this task's CR4 in the VMCS. */",
                "\tcr4 = read_cr4();",
                "\tvmcs_writel(HOST_CR4, cr4);\t\t\t/* 22.2.3, 22.2.5 */",
                "\tvmx->host_state.vmcs_host_cr4 = cr4;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-3690",
        "func_name": "torvalds/linux/vmx_vcpu_run",
        "description": "arch/x86/kvm/vmx.c in the KVM subsystem in the Linux kernel before 3.17.2 on Intel processors does not ensure that the value in the CR4 control register remains the same after a VM entry, which allows host OS users to kill arbitrary processes or cause a denial of service (system disruption) by leveraging /dev/kvm access, as demonstrated by PR_SET_TSC prctl calls within a modified copy of QEMU.",
        "git_url": "https://github.com/torvalds/linux/commit/d974baa398f34393db76be45f7d4d04fbdbb4a0a",
        "commit_title": "x86,kvm,vmx: Preserve CR4 across VM entry",
        "commit_text": " CR4 isn't constant; at least the TSD and PCE bits can vary.  TBH, treating CR0 and CR3 as constant scares me a bit, too, but it looks like it's correct.  This adds a branch and a read from cr4 to each vm entry.  Because it is extremely likely that consecutive entries into the same vcpu will have the same host cr4 value, this fixes up the vmcs instead of restoring cr4 after the fact.  A subsequent patch will add a kernel-wide cr4 shadow, reducing the overhead in the common case to just two memory reads and a branch.  Cc: stable@vger.kernel.org Cc: Petr Matousek <pmatouse@redhat.com> Cc: Gleb Natapov <gleb@kernel.org>",
        "func_before": "static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long debugctlmsr;\n\n\t/* Record the guest's net vcpu time for enforced NMI injections. */\n\tif (unlikely(!cpu_has_virtual_nmis() && vmx->soft_vnmi_blocked))\n\t\tvmx->entry_time = ktime_get();\n\n\t/* Don't enter VMX if guest state is invalid, let the exit handler\n\t   start emulation until we arrive back to a valid state */\n\tif (vmx->emulation_required)\n\t\treturn;\n\n\tif (vmx->ple_window_dirty) {\n\t\tvmx->ple_window_dirty = false;\n\t\tvmcs_write32(PLE_WINDOW, vmx->ple_window);\n\t}\n\n\tif (vmx->nested.sync_shadow_vmcs) {\n\t\tcopy_vmcs12_to_shadow(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t}\n\n\tif (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n\tif (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n\n\t/* When single-stepping over STI and MOV SS, we must clear the\n\t * corresponding interruptibility bits in the guest state. Otherwise\n\t * vmentry fails as it then expects bit 14 (BS) in pending debug\n\t * exceptions being set, but that's not correct for the guest debugging\n\t * case. */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\tatomic_switch_perf_msrs(vmx);\n\tdebugctlmsr = get_debugctlmsr();\n\n\tvmx->__launched = vmx->loaded_vmcs->launched;\n\tasm(\n\t\t/* Store host registers */\n\t\t\"push %%\" _ASM_DX \"; push %%\" _ASM_BP \";\"\n\t\t\"push %%\" _ASM_CX \" \\n\\t\" /* placeholder for guest rcx */\n\t\t\"push %%\" _ASM_CX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t\"je 1f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t__ex(ASM_VMX_VMWRITE_RSP_RDX) \"\\n\\t\"\n\t\t\"1: \\n\\t\"\n\t\t/* Reload cr2 if changed */\n\t\t\"mov %c[cr2](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %%cr2, %%\" _ASM_DX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_AX \", %%\" _ASM_DX \" \\n\\t\"\n\t\t\"je 2f \\n\\t\"\n\t\t\"mov %%\" _ASM_AX\", %%cr2 \\n\\t\"\n\t\t\"2: \\n\\t\"\n\t\t/* Check if vmlaunch of vmresume is needed */\n\t\t\"cmpl $0, %c[launched](%0) \\n\\t\"\n\t\t/* Load guest registers.  Don't clobber flags. */\n\t\t\"mov %c[rax](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %c[rbx](%0), %%\" _ASM_BX \" \\n\\t\"\n\t\t\"mov %c[rdx](%0), %%\" _ASM_DX \" \\n\\t\"\n\t\t\"mov %c[rsi](%0), %%\" _ASM_SI \" \\n\\t\"\n\t\t\"mov %c[rdi](%0), %%\" _ASM_DI \" \\n\\t\"\n\t\t\"mov %c[rbp](%0), %%\" _ASM_BP \" \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%0),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%0),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%0), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%0), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%0), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%0), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%0), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%0), %%r15 \\n\\t\"\n#endif\n\t\t\"mov %c[rcx](%0), %%\" _ASM_CX \" \\n\\t\" /* kills %0 (ecx) */\n\n\t\t/* Enter guest mode */\n\t\t\"jne 1f \\n\\t\"\n\t\t__ex(ASM_VMX_VMLAUNCH) \"\\n\\t\"\n\t\t\"jmp 2f \\n\\t\"\n\t\t\"1: \" __ex(ASM_VMX_VMRESUME) \"\\n\\t\"\n\t\t\"2: \"\n\t\t/* Save guest registers, load host registers, keep flags */\n\t\t\"mov %0, %c[wordsize](%%\" _ASM_SP \") \\n\\t\"\n\t\t\"pop %0 \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[rax](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BX \", %c[rbx](%0) \\n\\t\"\n\t\t__ASM_SIZE(pop) \" %c[rcx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DX \", %c[rdx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_SI \", %c[rsi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DI \", %c[rdi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BP \", %c[rbp](%0) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%0) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%0) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%0) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%0) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%0) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%0) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%0) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%0) \\n\\t\"\n#endif\n\t\t\"mov %%cr2, %%\" _ASM_AX \"   \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[cr2](%0) \\n\\t\"\n\n\t\t\"pop  %%\" _ASM_BP \"; pop  %%\" _ASM_DX \" \\n\\t\"\n\t\t\"setbe %c[fail](%0) \\n\\t\"\n\t\t\".pushsection .rodata \\n\\t\"\n\t\t\".global vmx_return \\n\\t\"\n\t\t\"vmx_return: \" _ASM_PTR \" 2b \\n\\t\"\n\t\t\".popsection\"\n\t      : : \"c\"(vmx), \"d\"((unsigned long)HOST_RSP),\n\t\t[launched]\"i\"(offsetof(struct vcpu_vmx, __launched)),\n\t\t[fail]\"i\"(offsetof(struct vcpu_vmx, fail)),\n\t\t[host_rsp]\"i\"(offsetof(struct vcpu_vmx, host_rsp)),\n\t\t[rax]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),\n\t\t[rbx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t[rcx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t[rdx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t[rsi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t[rdi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t[rbp]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),\n#ifdef CONFIG_X86_64\n\t\t[r8]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t[r9]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t[r10]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t[r11]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t[r12]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t[r13]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t[r14]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t[r15]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),\n#endif\n\t\t[cr2]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),\n\t\t[wordsize]\"i\"(sizeof(ulong))\n\t      : \"cc\", \"memory\"\n#ifdef CONFIG_X86_64\n\t\t, \"rax\", \"rbx\", \"rdi\", \"rsi\"\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\", \"r12\", \"r13\", \"r14\", \"r15\"\n#else\n\t\t, \"eax\", \"ebx\", \"edi\", \"esi\"\n#endif\n\t      );\n\n\t/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */\n\tif (debugctlmsr)\n\t\tupdate_debugctlmsr(debugctlmsr);\n\n#ifndef CONFIG_X86_64\n\t/*\n\t * The sysexit path does not restore ds/es, so we must set them to\n\t * a reasonable value ourselves.\n\t *\n\t * We can't defer this to vmx_load_host_state() since that function\n\t * may be executed in interrupt context, which saves and restore segments\n\t * around it, nullifying its effect.\n\t */\n\tloadsegment(ds, __USER_DS);\n\tloadsegment(es, __USER_DS);\n#endif\n\n\tvcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)\n\t\t\t\t  | (1 << VCPU_EXREG_RFLAGS)\n\t\t\t\t  | (1 << VCPU_EXREG_PDPTR)\n\t\t\t\t  | (1 << VCPU_EXREG_SEGMENTS)\n\t\t\t\t  | (1 << VCPU_EXREG_CR3));\n\tvcpu->arch.regs_dirty = 0;\n\n\tvmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);\n\n\tvmx->loaded_vmcs->launched = 1;\n\n\tvmx->exit_reason = vmcs_read32(VM_EXIT_REASON);\n\ttrace_kvm_exit(vmx->exit_reason, vcpu, KVM_ISA_VMX);\n\n\t/*\n\t * the KVM_REQ_EVENT optimization bit is only on for one entry, and if\n\t * we did not inject a still-pending event to L1 now because of\n\t * nested_run_pending, we need to re-enable this bit.\n\t */\n\tif (vmx->nested.nested_run_pending)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvmx->nested.nested_run_pending = 0;\n\n\tvmx_complete_atomic_exit(vmx);\n\tvmx_recover_nmi_blocking(vmx);\n\tvmx_complete_interrupts(vmx);\n}",
        "func": "static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long debugctlmsr, cr4;\n\n\t/* Record the guest's net vcpu time for enforced NMI injections. */\n\tif (unlikely(!cpu_has_virtual_nmis() && vmx->soft_vnmi_blocked))\n\t\tvmx->entry_time = ktime_get();\n\n\t/* Don't enter VMX if guest state is invalid, let the exit handler\n\t   start emulation until we arrive back to a valid state */\n\tif (vmx->emulation_required)\n\t\treturn;\n\n\tif (vmx->ple_window_dirty) {\n\t\tvmx->ple_window_dirty = false;\n\t\tvmcs_write32(PLE_WINDOW, vmx->ple_window);\n\t}\n\n\tif (vmx->nested.sync_shadow_vmcs) {\n\t\tcopy_vmcs12_to_shadow(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t}\n\n\tif (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n\tif (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n\n\tcr4 = read_cr4();\n\tif (unlikely(cr4 != vmx->host_state.vmcs_host_cr4)) {\n\t\tvmcs_writel(HOST_CR4, cr4);\n\t\tvmx->host_state.vmcs_host_cr4 = cr4;\n\t}\n\n\t/* When single-stepping over STI and MOV SS, we must clear the\n\t * corresponding interruptibility bits in the guest state. Otherwise\n\t * vmentry fails as it then expects bit 14 (BS) in pending debug\n\t * exceptions being set, but that's not correct for the guest debugging\n\t * case. */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\tatomic_switch_perf_msrs(vmx);\n\tdebugctlmsr = get_debugctlmsr();\n\n\tvmx->__launched = vmx->loaded_vmcs->launched;\n\tasm(\n\t\t/* Store host registers */\n\t\t\"push %%\" _ASM_DX \"; push %%\" _ASM_BP \";\"\n\t\t\"push %%\" _ASM_CX \" \\n\\t\" /* placeholder for guest rcx */\n\t\t\"push %%\" _ASM_CX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t\"je 1f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t__ex(ASM_VMX_VMWRITE_RSP_RDX) \"\\n\\t\"\n\t\t\"1: \\n\\t\"\n\t\t/* Reload cr2 if changed */\n\t\t\"mov %c[cr2](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %%cr2, %%\" _ASM_DX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_AX \", %%\" _ASM_DX \" \\n\\t\"\n\t\t\"je 2f \\n\\t\"\n\t\t\"mov %%\" _ASM_AX\", %%cr2 \\n\\t\"\n\t\t\"2: \\n\\t\"\n\t\t/* Check if vmlaunch of vmresume is needed */\n\t\t\"cmpl $0, %c[launched](%0) \\n\\t\"\n\t\t/* Load guest registers.  Don't clobber flags. */\n\t\t\"mov %c[rax](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %c[rbx](%0), %%\" _ASM_BX \" \\n\\t\"\n\t\t\"mov %c[rdx](%0), %%\" _ASM_DX \" \\n\\t\"\n\t\t\"mov %c[rsi](%0), %%\" _ASM_SI \" \\n\\t\"\n\t\t\"mov %c[rdi](%0), %%\" _ASM_DI \" \\n\\t\"\n\t\t\"mov %c[rbp](%0), %%\" _ASM_BP \" \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%0),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%0),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%0), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%0), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%0), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%0), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%0), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%0), %%r15 \\n\\t\"\n#endif\n\t\t\"mov %c[rcx](%0), %%\" _ASM_CX \" \\n\\t\" /* kills %0 (ecx) */\n\n\t\t/* Enter guest mode */\n\t\t\"jne 1f \\n\\t\"\n\t\t__ex(ASM_VMX_VMLAUNCH) \"\\n\\t\"\n\t\t\"jmp 2f \\n\\t\"\n\t\t\"1: \" __ex(ASM_VMX_VMRESUME) \"\\n\\t\"\n\t\t\"2: \"\n\t\t/* Save guest registers, load host registers, keep flags */\n\t\t\"mov %0, %c[wordsize](%%\" _ASM_SP \") \\n\\t\"\n\t\t\"pop %0 \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[rax](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BX \", %c[rbx](%0) \\n\\t\"\n\t\t__ASM_SIZE(pop) \" %c[rcx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DX \", %c[rdx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_SI \", %c[rsi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DI \", %c[rdi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BP \", %c[rbp](%0) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%0) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%0) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%0) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%0) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%0) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%0) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%0) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%0) \\n\\t\"\n#endif\n\t\t\"mov %%cr2, %%\" _ASM_AX \"   \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[cr2](%0) \\n\\t\"\n\n\t\t\"pop  %%\" _ASM_BP \"; pop  %%\" _ASM_DX \" \\n\\t\"\n\t\t\"setbe %c[fail](%0) \\n\\t\"\n\t\t\".pushsection .rodata \\n\\t\"\n\t\t\".global vmx_return \\n\\t\"\n\t\t\"vmx_return: \" _ASM_PTR \" 2b \\n\\t\"\n\t\t\".popsection\"\n\t      : : \"c\"(vmx), \"d\"((unsigned long)HOST_RSP),\n\t\t[launched]\"i\"(offsetof(struct vcpu_vmx, __launched)),\n\t\t[fail]\"i\"(offsetof(struct vcpu_vmx, fail)),\n\t\t[host_rsp]\"i\"(offsetof(struct vcpu_vmx, host_rsp)),\n\t\t[rax]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),\n\t\t[rbx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t[rcx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t[rdx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t[rsi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t[rdi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t[rbp]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),\n#ifdef CONFIG_X86_64\n\t\t[r8]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t[r9]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t[r10]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t[r11]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t[r12]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t[r13]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t[r14]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t[r15]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),\n#endif\n\t\t[cr2]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),\n\t\t[wordsize]\"i\"(sizeof(ulong))\n\t      : \"cc\", \"memory\"\n#ifdef CONFIG_X86_64\n\t\t, \"rax\", \"rbx\", \"rdi\", \"rsi\"\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\", \"r12\", \"r13\", \"r14\", \"r15\"\n#else\n\t\t, \"eax\", \"ebx\", \"edi\", \"esi\"\n#endif\n\t      );\n\n\t/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */\n\tif (debugctlmsr)\n\t\tupdate_debugctlmsr(debugctlmsr);\n\n#ifndef CONFIG_X86_64\n\t/*\n\t * The sysexit path does not restore ds/es, so we must set them to\n\t * a reasonable value ourselves.\n\t *\n\t * We can't defer this to vmx_load_host_state() since that function\n\t * may be executed in interrupt context, which saves and restore segments\n\t * around it, nullifying its effect.\n\t */\n\tloadsegment(ds, __USER_DS);\n\tloadsegment(es, __USER_DS);\n#endif\n\n\tvcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)\n\t\t\t\t  | (1 << VCPU_EXREG_RFLAGS)\n\t\t\t\t  | (1 << VCPU_EXREG_PDPTR)\n\t\t\t\t  | (1 << VCPU_EXREG_SEGMENTS)\n\t\t\t\t  | (1 << VCPU_EXREG_CR3));\n\tvcpu->arch.regs_dirty = 0;\n\n\tvmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);\n\n\tvmx->loaded_vmcs->launched = 1;\n\n\tvmx->exit_reason = vmcs_read32(VM_EXIT_REASON);\n\ttrace_kvm_exit(vmx->exit_reason, vcpu, KVM_ISA_VMX);\n\n\t/*\n\t * the KVM_REQ_EVENT optimization bit is only on for one entry, and if\n\t * we did not inject a still-pending event to L1 now because of\n\t * nested_run_pending, we need to re-enable this bit.\n\t */\n\tif (vmx->nested.nested_run_pending)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvmx->nested.nested_run_pending = 0;\n\n\tvmx_complete_atomic_exit(vmx);\n\tvmx_recover_nmi_blocking(vmx);\n\tvmx_complete_interrupts(vmx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)\n {\n \tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n-\tunsigned long debugctlmsr;\n+\tunsigned long debugctlmsr, cr4;\n \n \t/* Record the guest's net vcpu time for enforced NMI injections. */\n \tif (unlikely(!cpu_has_virtual_nmis() && vmx->soft_vnmi_blocked))\n@@ -26,6 +26,12 @@\n \t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n \tif (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))\n \t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n+\n+\tcr4 = read_cr4();\n+\tif (unlikely(cr4 != vmx->host_state.vmcs_host_cr4)) {\n+\t\tvmcs_writel(HOST_CR4, cr4);\n+\t\tvmx->host_state.vmcs_host_cr4 = cr4;\n+\t}\n \n \t/* When single-stepping over STI and MOV SS, we must clear the\n \t * corresponding interruptibility bits in the guest state. Otherwise",
        "diff_line_info": {
            "deleted_lines": [
                "\tunsigned long debugctlmsr;"
            ],
            "added_lines": [
                "\tunsigned long debugctlmsr, cr4;",
                "",
                "\tcr4 = read_cr4();",
                "\tif (unlikely(cr4 != vmx->host_state.vmcs_host_cr4)) {",
                "\t\tvmcs_writel(HOST_CR4, cr4);",
                "\t\tvmx->host_state.vmcs_host_cr4 = cr4;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/__dcache_readdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static int __dcache_readdir(struct file *file,  struct dir_context *ctx,\n\t\t\t    u32 shared_gen)\n{\n\tstruct ceph_file_info *fi = file->private_data;\n\tstruct dentry *parent = file->f_dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct list_head *p;\n\tstruct dentry *dentry, *last;\n\tstruct ceph_dentry_info *di;\n\tint err = 0;\n\n\t/* claim ref on last dentry we returned */\n\tlast = fi->dentry;\n\tfi->dentry = NULL;\n\n\tdout(\"__dcache_readdir %p v%u at %llu (last %p)\\n\",\n\t     dir, shared_gen, ctx->pos, last);\n\n\tspin_lock(&parent->d_lock);\n\n\t/* start at beginning? */\n\tif (ctx->pos == 2 || last == NULL ||\n\t    fpos_cmp(ctx->pos, ceph_dentry(last)->offset) < 0) {\n\t\tif (list_empty(&parent->d_subdirs))\n\t\t\tgoto out_unlock;\n\t\tp = parent->d_subdirs.prev;\n\t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n\t} else {\n\t\tp = last->d_u.d_child.prev;\n\t}\n\nmore:\n\tdentry = list_entry(p, struct dentry, d_u.d_child);\n\tdi = ceph_dentry(dentry);\n\twhile (1) {\n\t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n\t\t     d_unhashed(dentry) ? \"!hashed\" : \"hashed\",\n\t\t     parent->d_subdirs.prev, parent->d_subdirs.next);\n\t\tif (p == &parent->d_subdirs) {\n\t\t\tfi->flags |= CEPH_F_ATEND;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (di->lease_shared_gen == shared_gen &&\n\t\t    !d_unhashed(dentry) && dentry->d_inode &&\n\t\t    ceph_snap(dentry->d_inode) != CEPH_SNAPDIR &&\n\t\t    ceph_ino(dentry->d_inode) != CEPH_INO_CEPH &&\n\t\t    fpos_cmp(ctx->pos, di->offset) <= 0)\n\t\t\tbreak;\n\t\tdout(\" skipping %p %.*s at %llu (%llu)%s%s\\n\", dentry,\n\t\t     dentry->d_name.len, dentry->d_name.name, di->offset,\n\t\t     ctx->pos, d_unhashed(dentry) ? \" unhashed\" : \"\",\n\t\t     !dentry->d_inode ? \" null\" : \"\");\n\t\tspin_unlock(&dentry->d_lock);\n\t\tp = p->prev;\n\t\tdentry = list_entry(p, struct dentry, d_u.d_child);\n\t\tdi = ceph_dentry(dentry);\n\t}\n\n\tdget_dlock(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&parent->d_lock);\n\n\t/* make sure a dentry wasn't dropped while we didn't have parent lock */\n\tif (!ceph_dir_is_complete(dir)) {\n\t\tdout(\" lost dir complete on %p; falling back to mds\\n\", dir);\n\t\tdput(dentry);\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tdout(\" %llu (%llu) dentry %p %.*s %p\\n\", di->offset, ctx->pos,\n\t     dentry, dentry->d_name.len, dentry->d_name.name, dentry->d_inode);\n\tif (!dir_emit(ctx, dentry->d_name.name,\n\t\t      dentry->d_name.len,\n\t\t      ceph_translate_ino(dentry->d_sb, dentry->d_inode->i_ino),\n\t\t      dentry->d_inode->i_mode >> 12)) {\n\t\tif (last) {\n\t\t\t/* remember our position */\n\t\t\tfi->dentry = last;\n\t\t\tfi->next_offset = fpos_off(di->offset);\n\t\t}\n\t\tdput(dentry);\n\t\treturn 0;\n\t}\n\n\tctx->pos = di->offset + 1;\n\n\tif (last)\n\t\tdput(last);\n\tlast = dentry;\n\n\tspin_lock(&parent->d_lock);\n\tp = p->prev;\t/* advance to next dentry */\n\tgoto more;\n\nout_unlock:\n\tspin_unlock(&parent->d_lock);\nout:\n\tif (last)\n\t\tdput(last);\n\treturn err;\n}",
        "func": "static int __dcache_readdir(struct file *file,  struct dir_context *ctx,\n\t\t\t    u32 shared_gen)\n{\n\tstruct ceph_file_info *fi = file->private_data;\n\tstruct dentry *parent = file->f_dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct list_head *p;\n\tstruct dentry *dentry, *last;\n\tstruct ceph_dentry_info *di;\n\tint err = 0;\n\n\t/* claim ref on last dentry we returned */\n\tlast = fi->dentry;\n\tfi->dentry = NULL;\n\n\tdout(\"__dcache_readdir %p v%u at %llu (last %p)\\n\",\n\t     dir, shared_gen, ctx->pos, last);\n\n\tspin_lock(&parent->d_lock);\n\n\t/* start at beginning? */\n\tif (ctx->pos == 2 || last == NULL ||\n\t    fpos_cmp(ctx->pos, ceph_dentry(last)->offset) < 0) {\n\t\tif (list_empty(&parent->d_subdirs))\n\t\t\tgoto out_unlock;\n\t\tp = parent->d_subdirs.prev;\n\t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n\t} else {\n\t\tp = last->d_child.prev;\n\t}\n\nmore:\n\tdentry = list_entry(p, struct dentry, d_child);\n\tdi = ceph_dentry(dentry);\n\twhile (1) {\n\t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n\t\t     d_unhashed(dentry) ? \"!hashed\" : \"hashed\",\n\t\t     parent->d_subdirs.prev, parent->d_subdirs.next);\n\t\tif (p == &parent->d_subdirs) {\n\t\t\tfi->flags |= CEPH_F_ATEND;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (di->lease_shared_gen == shared_gen &&\n\t\t    !d_unhashed(dentry) && dentry->d_inode &&\n\t\t    ceph_snap(dentry->d_inode) != CEPH_SNAPDIR &&\n\t\t    ceph_ino(dentry->d_inode) != CEPH_INO_CEPH &&\n\t\t    fpos_cmp(ctx->pos, di->offset) <= 0)\n\t\t\tbreak;\n\t\tdout(\" skipping %p %.*s at %llu (%llu)%s%s\\n\", dentry,\n\t\t     dentry->d_name.len, dentry->d_name.name, di->offset,\n\t\t     ctx->pos, d_unhashed(dentry) ? \" unhashed\" : \"\",\n\t\t     !dentry->d_inode ? \" null\" : \"\");\n\t\tspin_unlock(&dentry->d_lock);\n\t\tp = p->prev;\n\t\tdentry = list_entry(p, struct dentry, d_child);\n\t\tdi = ceph_dentry(dentry);\n\t}\n\n\tdget_dlock(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&parent->d_lock);\n\n\t/* make sure a dentry wasn't dropped while we didn't have parent lock */\n\tif (!ceph_dir_is_complete(dir)) {\n\t\tdout(\" lost dir complete on %p; falling back to mds\\n\", dir);\n\t\tdput(dentry);\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tdout(\" %llu (%llu) dentry %p %.*s %p\\n\", di->offset, ctx->pos,\n\t     dentry, dentry->d_name.len, dentry->d_name.name, dentry->d_inode);\n\tif (!dir_emit(ctx, dentry->d_name.name,\n\t\t      dentry->d_name.len,\n\t\t      ceph_translate_ino(dentry->d_sb, dentry->d_inode->i_ino),\n\t\t      dentry->d_inode->i_mode >> 12)) {\n\t\tif (last) {\n\t\t\t/* remember our position */\n\t\t\tfi->dentry = last;\n\t\t\tfi->next_offset = fpos_off(di->offset);\n\t\t}\n\t\tdput(dentry);\n\t\treturn 0;\n\t}\n\n\tctx->pos = di->offset + 1;\n\n\tif (last)\n\t\tdput(last);\n\tlast = dentry;\n\n\tspin_lock(&parent->d_lock);\n\tp = p->prev;\t/* advance to next dentry */\n\tgoto more;\n\nout_unlock:\n\tspin_unlock(&parent->d_lock);\nout:\n\tif (last)\n\t\tdput(last);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,11 +26,11 @@\n \t\tp = parent->d_subdirs.prev;\n \t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n \t} else {\n-\t\tp = last->d_u.d_child.prev;\n+\t\tp = last->d_child.prev;\n \t}\n \n more:\n-\tdentry = list_entry(p, struct dentry, d_u.d_child);\n+\tdentry = list_entry(p, struct dentry, d_child);\n \tdi = ceph_dentry(dentry);\n \twhile (1) {\n \t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n@@ -53,7 +53,7 @@\n \t\t     !dentry->d_inode ? \" null\" : \"\");\n \t\tspin_unlock(&dentry->d_lock);\n \t\tp = p->prev;\n-\t\tdentry = list_entry(p, struct dentry, d_u.d_child);\n+\t\tdentry = list_entry(p, struct dentry, d_child);\n \t\tdi = ceph_dentry(dentry);\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tp = last->d_u.d_child.prev;",
                "\tdentry = list_entry(p, struct dentry, d_u.d_child);",
                "\t\tdentry = list_entry(p, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\tp = last->d_child.prev;",
                "\tdentry = list_entry(p, struct dentry, d_child);",
                "\t\tdentry = list_entry(p, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/coda_flag_children",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void coda_flag_children(struct dentry *parent, int flag)\n{\n\tstruct dentry *de;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(de, &parent->d_subdirs, d_u.d_child) {\n\t\t/* don't know what to do with negative dentries */\n\t\tif (de->d_inode ) \n\t\t\tcoda_flag_inode(de->d_inode, flag);\n\t}\n\tspin_unlock(&parent->d_lock);\n\treturn; \n}",
        "func": "static void coda_flag_children(struct dentry *parent, int flag)\n{\n\tstruct dentry *de;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(de, &parent->d_subdirs, d_child) {\n\t\t/* don't know what to do with negative dentries */\n\t\tif (de->d_inode ) \n\t\t\tcoda_flag_inode(de->d_inode, flag);\n\t}\n\tspin_unlock(&parent->d_lock);\n\treturn; \n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct dentry *de;\n \n \tspin_lock(&parent->d_lock);\n-\tlist_for_each_entry(de, &parent->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry(de, &parent->d_subdirs, d_child) {\n \t\t/* don't know what to do with negative dentries */\n \t\tif (de->d_inode ) \n \t\t\tcoda_flag_inode(de->d_inode, flag);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry(de, &parent->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry(de, &parent->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/find_acceptable_alias",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static struct dentry *\nfind_acceptable_alias(struct dentry *result,\n\t\tint (*acceptable)(void *context, struct dentry *dentry),\n\t\tvoid *context)\n{\n\tstruct dentry *dentry, *toput = NULL;\n\tstruct inode *inode;\n\n\tif (acceptable(context, result))\n\t\treturn result;\n\n\tinode = result->d_inode;\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {\n\t\tdget(dentry);\n\t\tspin_unlock(&inode->i_lock);\n\t\tif (toput)\n\t\t\tdput(toput);\n\t\tif (dentry != result && acceptable(context, dentry)) {\n\t\t\tdput(result);\n\t\t\treturn dentry;\n\t\t}\n\t\tspin_lock(&inode->i_lock);\n\t\ttoput = dentry;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (toput)\n\t\tdput(toput);\n\treturn NULL;\n}",
        "func": "static struct dentry *\nfind_acceptable_alias(struct dentry *result,\n\t\tint (*acceptable)(void *context, struct dentry *dentry),\n\t\tvoid *context)\n{\n\tstruct dentry *dentry, *toput = NULL;\n\tstruct inode *inode;\n\n\tif (acceptable(context, result))\n\t\treturn result;\n\n\tinode = result->d_inode;\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {\n\t\tdget(dentry);\n\t\tspin_unlock(&inode->i_lock);\n\t\tif (toput)\n\t\t\tdput(toput);\n\t\tif (dentry != result && acceptable(context, dentry)) {\n\t\t\tdput(result);\n\t\t\treturn dentry;\n\t\t}\n\t\tspin_lock(&inode->i_lock);\n\t\ttoput = dentry;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (toput)\n\t\tdput(toput);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n \n \tinode = result->d_inode;\n \tspin_lock(&inode->i_lock);\n-\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {\n+\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {\n \t\tdget(dentry);\n \t\tspin_unlock(&inode->i_lock);\n \t\tif (toput)",
        "diff_line_info": {
            "deleted_lines": [
                "\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/nfs_superblock_set_dummy_root",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static int nfs_superblock_set_dummy_root(struct super_block *sb, struct inode *inode)\n{\n\t/* The mntroot acts as the dummy root dentry for this superblock */\n\tif (sb->s_root == NULL) {\n\t\tsb->s_root = d_make_root(inode);\n\t\tif (sb->s_root == NULL)\n\t\t\treturn -ENOMEM;\n\t\tihold(inode);\n\t\t/*\n\t\t * Ensure that this dentry is invisible to d_find_alias().\n\t\t * Otherwise, it may be spliced into the tree by\n\t\t * d_materialise_unique if a parent directory from the same\n\t\t * filesystem gets mounted at a later time.\n\t\t * This again causes shrink_dcache_for_umount_subtree() to\n\t\t * Oops, since the test for IS_ROOT() will fail.\n\t\t */\n\t\tspin_lock(&sb->s_root->d_inode->i_lock);\n\t\tspin_lock(&sb->s_root->d_lock);\n\t\thlist_del_init(&sb->s_root->d_alias);\n\t\tspin_unlock(&sb->s_root->d_lock);\n\t\tspin_unlock(&sb->s_root->d_inode->i_lock);\n\t}\n\treturn 0;\n}",
        "func": "static int nfs_superblock_set_dummy_root(struct super_block *sb, struct inode *inode)\n{\n\t/* The mntroot acts as the dummy root dentry for this superblock */\n\tif (sb->s_root == NULL) {\n\t\tsb->s_root = d_make_root(inode);\n\t\tif (sb->s_root == NULL)\n\t\t\treturn -ENOMEM;\n\t\tihold(inode);\n\t\t/*\n\t\t * Ensure that this dentry is invisible to d_find_alias().\n\t\t * Otherwise, it may be spliced into the tree by\n\t\t * d_materialise_unique if a parent directory from the same\n\t\t * filesystem gets mounted at a later time.\n\t\t * This again causes shrink_dcache_for_umount_subtree() to\n\t\t * Oops, since the test for IS_ROOT() will fail.\n\t\t */\n\t\tspin_lock(&sb->s_root->d_inode->i_lock);\n\t\tspin_lock(&sb->s_root->d_lock);\n\t\thlist_del_init(&sb->s_root->d_u.d_alias);\n\t\tspin_unlock(&sb->s_root->d_lock);\n\t\tspin_unlock(&sb->s_root->d_inode->i_lock);\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,7 +16,7 @@\n \t\t */\n \t\tspin_lock(&sb->s_root->d_inode->i_lock);\n \t\tspin_lock(&sb->s_root->d_lock);\n-\t\thlist_del_init(&sb->s_root->d_alias);\n+\t\thlist_del_init(&sb->s_root->d_u.d_alias);\n \t\tspin_unlock(&sb->s_root->d_lock);\n \t\tspin_unlock(&sb->s_root->d_inode->i_lock);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\thlist_del_init(&sb->s_root->d_alias);"
            ],
            "added_lines": [
                "\t\thlist_del_init(&sb->s_root->d_u.d_alias);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/autofs_clear_leaf_automount_flags",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void autofs_clear_leaf_automount_flags(struct dentry *dentry)\n{\n\tstruct list_head *d_child;\n\tstruct dentry *parent;\n\n\t/* flags for dentrys in the root are handled elsewhere */\n\tif (IS_ROOT(dentry->d_parent))\n\t\treturn;\n\n\tmanaged_dentry_clear_managed(dentry);\n\n\tparent = dentry->d_parent;\n\t/* only consider parents below dentrys in the root */\n\tif (IS_ROOT(parent->d_parent))\n\t\treturn;\n\td_child = &dentry->d_u.d_child;\n\t/* Set parent managed if it's becoming empty */\n\tif (d_child->next == &parent->d_subdirs &&\n\t    d_child->prev == &parent->d_subdirs)\n\t\tmanaged_dentry_set_managed(parent);\n\treturn;\n}",
        "func": "static void autofs_clear_leaf_automount_flags(struct dentry *dentry)\n{\n\tstruct list_head *d_child;\n\tstruct dentry *parent;\n\n\t/* flags for dentrys in the root are handled elsewhere */\n\tif (IS_ROOT(dentry->d_parent))\n\t\treturn;\n\n\tmanaged_dentry_clear_managed(dentry);\n\n\tparent = dentry->d_parent;\n\t/* only consider parents below dentrys in the root */\n\tif (IS_ROOT(parent->d_parent))\n\t\treturn;\n\td_child = &dentry->d_child;\n\t/* Set parent managed if it's becoming empty */\n\tif (d_child->next == &parent->d_subdirs &&\n\t    d_child->prev == &parent->d_subdirs)\n\t\tmanaged_dentry_set_managed(parent);\n\treturn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \t/* only consider parents below dentrys in the root */\n \tif (IS_ROOT(parent->d_parent))\n \t\treturn;\n-\td_child = &dentry->d_u.d_child;\n+\td_child = &dentry->d_child;\n \t/* Set parent managed if it's becoming empty */\n \tif (d_child->next == &parent->d_subdirs &&\n \t    d_child->prev == &parent->d_subdirs)",
        "diff_line_info": {
            "deleted_lines": [
                "\td_child = &dentry->d_u.d_child;"
            ],
            "added_lines": [
                "\td_child = &dentry->d_child;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/spufs_prune_dir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "func": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct dentry *dentry, *tmp;\n \n \tmutex_lock(&dir->d_inode->i_mutex);\n-\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n \t\tspin_lock(&dentry->d_lock);\n \t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n \t\t\tdget_dlock(dentry);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/spufs_prune_dir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "func": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct dentry *dentry, *tmp;\n \n \tmutex_lock(&dir->d_inode->i_mutex);\n-\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n \t\tspin_lock(&dentry->d_lock);\n \t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n \t\t\tdget_dlock(dentry);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/affs_fix_dcache",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void\naffs_fix_dcache(struct inode *inode, u32 entry_ino)\n{\n\tstruct dentry *dentry;\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {\n\t\tif (entry_ino == (u32)(long)dentry->d_fsdata) {\n\t\t\tdentry->d_fsdata = (void *)inode->i_ino;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
        "func": "static void\naffs_fix_dcache(struct inode *inode, u32 entry_ino)\n{\n\tstruct dentry *dentry;\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {\n\t\tif (entry_ino == (u32)(long)dentry->d_fsdata) {\n\t\t\tdentry->d_fsdata = (void *)inode->i_ino;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n {\n \tstruct dentry *dentry;\n \tspin_lock(&inode->i_lock);\n-\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {\n+\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {\n \t\tif (entry_ino == (u32)(long)dentry->d_fsdata) {\n \t\t\tdentry->d_fsdata = (void *)inode->i_ino;\n \t\t\tbreak;",
        "diff_line_info": {
            "deleted_lines": [
                "\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/instance_rmdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static int instance_rmdir(struct inode *inode, struct dentry *dentry)\n{\n\tstruct dentry *parent;\n\tint ret;\n\n\t/* Paranoid: Make sure the parent is the \"instances\" directory */\n\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);\n\tif (WARN_ON_ONCE(parent != trace_instance_dir))\n\t\treturn -ENOENT;\n\n\t/* The caller did a dget() on dentry */\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n\n\t/*\n\t * The inode mutex is locked, but debugfs_create_dir() will also\n\t * take the mutex. As the instances directory can not be destroyed\n\t * or changed in any other way, it is safe to unlock it, and\n\t * let the dentry try. If two users try to make the same dir at\n\t * the same time, then the instance_delete() will determine the\n\t * winner.\n\t */\n\tmutex_unlock(&inode->i_mutex);\n\n\tret = instance_delete(dentry->d_iname);\n\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\n\treturn ret;\n}",
        "func": "static int instance_rmdir(struct inode *inode, struct dentry *dentry)\n{\n\tstruct dentry *parent;\n\tint ret;\n\n\t/* Paranoid: Make sure the parent is the \"instances\" directory */\n\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n\tif (WARN_ON_ONCE(parent != trace_instance_dir))\n\t\treturn -ENOENT;\n\n\t/* The caller did a dget() on dentry */\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n\n\t/*\n\t * The inode mutex is locked, but debugfs_create_dir() will also\n\t * take the mutex. As the instances directory can not be destroyed\n\t * or changed in any other way, it is safe to unlock it, and\n\t * let the dentry try. If two users try to make the same dir at\n\t * the same time, then the instance_delete() will determine the\n\t * winner.\n\t */\n\tmutex_unlock(&inode->i_mutex);\n\n\tret = instance_delete(dentry->d_iname);\n\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tint ret;\n \n \t/* Paranoid: Make sure the parent is the \"instances\" directory */\n-\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);\n+\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n \tif (WARN_ON_ONCE(parent != trace_instance_dir))\n \t\treturn -ENOENT;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);"
            ],
            "added_lines": [
                "\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/instance_mkdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static int instance_mkdir (struct inode *inode, struct dentry *dentry, umode_t mode)\n{\n\tstruct dentry *parent;\n\tint ret;\n\n\t/* Paranoid: Make sure the parent is the \"instances\" directory */\n\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);\n\tif (WARN_ON_ONCE(parent != trace_instance_dir))\n\t\treturn -ENOENT;\n\n\t/*\n\t * The inode mutex is locked, but debugfs_create_dir() will also\n\t * take the mutex. As the instances directory can not be destroyed\n\t * or changed in any other way, it is safe to unlock it, and\n\t * let the dentry try. If two users try to make the same dir at\n\t * the same time, then the new_instance_create() will determine the\n\t * winner.\n\t */\n\tmutex_unlock(&inode->i_mutex);\n\n\tret = new_instance_create(dentry->d_iname);\n\n\tmutex_lock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "func": "static int instance_mkdir (struct inode *inode, struct dentry *dentry, umode_t mode)\n{\n\tstruct dentry *parent;\n\tint ret;\n\n\t/* Paranoid: Make sure the parent is the \"instances\" directory */\n\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n\tif (WARN_ON_ONCE(parent != trace_instance_dir))\n\t\treturn -ENOENT;\n\n\t/*\n\t * The inode mutex is locked, but debugfs_create_dir() will also\n\t * take the mutex. As the instances directory can not be destroyed\n\t * or changed in any other way, it is safe to unlock it, and\n\t * let the dentry try. If two users try to make the same dir at\n\t * the same time, then the new_instance_create() will determine the\n\t * winner.\n\t */\n\tmutex_unlock(&inode->i_mutex);\n\n\tret = new_instance_create(dentry->d_iname);\n\n\tmutex_lock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tint ret;\n \n \t/* Paranoid: Make sure the parent is the \"instances\" directory */\n-\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);\n+\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n \tif (WARN_ON_ONCE(parent != trace_instance_dir))\n \t\treturn -ENOENT;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);"
            ],
            "added_lines": [
                "\tparent = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/sel_remove_entries",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void sel_remove_entries(struct dentry *de)\n{\n\tstruct list_head *node;\n\n\tspin_lock(&de->d_lock);\n\tnode = de->d_subdirs.next;\n\twhile (node != &de->d_subdirs) {\n\t\tstruct dentry *d = list_entry(node, struct dentry, d_u.d_child);\n\n\t\tspin_lock_nested(&d->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tlist_del_init(node);\n\n\t\tif (d->d_inode) {\n\t\t\tdget_dlock(d);\n\t\t\tspin_unlock(&de->d_lock);\n\t\t\tspin_unlock(&d->d_lock);\n\t\t\td_delete(d);\n\t\t\tsimple_unlink(de->d_inode, d);\n\t\t\tdput(d);\n\t\t\tspin_lock(&de->d_lock);\n\t\t} else\n\t\t\tspin_unlock(&d->d_lock);\n\t\tnode = de->d_subdirs.next;\n\t}\n\n\tspin_unlock(&de->d_lock);\n}",
        "func": "static void sel_remove_entries(struct dentry *de)\n{\n\tstruct list_head *node;\n\n\tspin_lock(&de->d_lock);\n\tnode = de->d_subdirs.next;\n\twhile (node != &de->d_subdirs) {\n\t\tstruct dentry *d = list_entry(node, struct dentry, d_child);\n\n\t\tspin_lock_nested(&d->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tlist_del_init(node);\n\n\t\tif (d->d_inode) {\n\t\t\tdget_dlock(d);\n\t\t\tspin_unlock(&de->d_lock);\n\t\t\tspin_unlock(&d->d_lock);\n\t\t\td_delete(d);\n\t\t\tsimple_unlink(de->d_inode, d);\n\t\t\tdput(d);\n\t\t\tspin_lock(&de->d_lock);\n\t\t} else\n\t\t\tspin_unlock(&d->d_lock);\n\t\tnode = de->d_subdirs.next;\n\t}\n\n\tspin_unlock(&de->d_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \tspin_lock(&de->d_lock);\n \tnode = de->d_subdirs.next;\n \twhile (node != &de->d_subdirs) {\n-\t\tstruct dentry *d = list_entry(node, struct dentry, d_u.d_child);\n+\t\tstruct dentry *d = list_entry(node, struct dentry, d_child);\n \n \t\tspin_lock_nested(&d->d_lock, DENTRY_D_LOCK_NESTED);\n \t\tlist_del_init(node);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tstruct dentry *d = list_entry(node, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\tstruct dentry *d = list_entry(node, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/sel_remove_classes",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void sel_remove_classes(void)\n{\n\tstruct list_head *class_node;\n\n\tlist_for_each(class_node, &class_dir->d_subdirs) {\n\t\tstruct dentry *class_subdir = list_entry(class_node,\n\t\t\t\t\tstruct dentry, d_u.d_child);\n\t\tstruct list_head *class_subdir_node;\n\n\t\tlist_for_each(class_subdir_node, &class_subdir->d_subdirs) {\n\t\t\tstruct dentry *d = list_entry(class_subdir_node,\n\t\t\t\t\t\tstruct dentry, d_u.d_child);\n\n\t\t\tif (d->d_inode)\n\t\t\t\tif (d->d_inode->i_mode & S_IFDIR)\n\t\t\t\t\tsel_remove_entries(d);\n\t\t}\n\n\t\tsel_remove_entries(class_subdir);\n\t}\n\n\tsel_remove_entries(class_dir);\n}",
        "func": "static void sel_remove_classes(void)\n{\n\tstruct list_head *class_node;\n\n\tlist_for_each(class_node, &class_dir->d_subdirs) {\n\t\tstruct dentry *class_subdir = list_entry(class_node,\n\t\t\t\t\tstruct dentry, d_child);\n\t\tstruct list_head *class_subdir_node;\n\n\t\tlist_for_each(class_subdir_node, &class_subdir->d_subdirs) {\n\t\t\tstruct dentry *d = list_entry(class_subdir_node,\n\t\t\t\t\t\tstruct dentry, d_child);\n\n\t\t\tif (d->d_inode)\n\t\t\t\tif (d->d_inode->i_mode & S_IFDIR)\n\t\t\t\t\tsel_remove_entries(d);\n\t\t}\n\n\t\tsel_remove_entries(class_subdir);\n\t}\n\n\tsel_remove_entries(class_dir);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,12 +4,12 @@\n \n \tlist_for_each(class_node, &class_dir->d_subdirs) {\n \t\tstruct dentry *class_subdir = list_entry(class_node,\n-\t\t\t\t\tstruct dentry, d_u.d_child);\n+\t\t\t\t\tstruct dentry, d_child);\n \t\tstruct list_head *class_subdir_node;\n \n \t\tlist_for_each(class_subdir_node, &class_subdir->d_subdirs) {\n \t\t\tstruct dentry *d = list_entry(class_subdir_node,\n-\t\t\t\t\t\tstruct dentry, d_u.d_child);\n+\t\t\t\t\t\tstruct dentry, d_child);\n \n \t\t\tif (d->d_inode)\n \t\t\t\tif (d->d_inode->i_mode & S_IFDIR)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\tstruct dentry, d_u.d_child);",
                "\t\t\t\t\t\tstruct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\t\t\t\tstruct dentry, d_child);",
                "\t\t\t\t\t\tstruct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ncp_renew_dentries",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static inline void\nncp_renew_dentries(struct dentry *parent)\n{\n\tstruct ncp_server *server = NCP_SERVER(parent->d_inode);\n\tstruct dentry *dentry;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {\n\t\tif (dentry->d_fsdata == NULL)\n\t\t\tncp_age_dentry(server, dentry);\n\t\telse\n\t\t\tncp_new_dentry(dentry);\n\t}\n\tspin_unlock(&parent->d_lock);\n}",
        "func": "static inline void\nncp_renew_dentries(struct dentry *parent)\n{\n\tstruct ncp_server *server = NCP_SERVER(parent->d_inode);\n\tstruct dentry *dentry;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {\n\t\tif (dentry->d_fsdata == NULL)\n\t\t\tncp_age_dentry(server, dentry);\n\t\telse\n\t\t\tncp_new_dentry(dentry);\n\t}\n\tspin_unlock(&parent->d_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \tstruct dentry *dentry;\n \n \tspin_lock(&parent->d_lock);\n-\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {\n \t\tif (dentry->d_fsdata == NULL)\n \t\t\tncp_age_dentry(server, dentry);\n \t\telse",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ncp_invalidate_dircache_entries",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static inline void\nncp_invalidate_dircache_entries(struct dentry *parent)\n{\n\tstruct ncp_server *server = NCP_SERVER(parent->d_inode);\n\tstruct dentry *dentry;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {\n\t\tdentry->d_fsdata = NULL;\n\t\tncp_age_dentry(server, dentry);\n\t}\n\tspin_unlock(&parent->d_lock);\n}",
        "func": "static inline void\nncp_invalidate_dircache_entries(struct dentry *parent)\n{\n\tstruct ncp_server *server = NCP_SERVER(parent->d_inode);\n\tstruct dentry *dentry;\n\n\tspin_lock(&parent->d_lock);\n\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {\n\t\tdentry->d_fsdata = NULL;\n\t\tncp_age_dentry(server, dentry);\n\t}\n\tspin_unlock(&parent->d_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \tstruct dentry *dentry;\n \n \tspin_lock(&parent->d_lock);\n-\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {\n \t\tdentry->d_fsdata = NULL;\n \t\tncp_age_dentry(server, dentry);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry(dentry, &parent->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry(dentry, &parent->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/__fsnotify_update_child_dentry_flags",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "void __fsnotify_update_child_dentry_flags(struct inode *inode)\n{\n\tstruct dentry *alias;\n\tint watched;\n\n\tif (!S_ISDIR(inode->i_mode))\n\t\treturn;\n\n\t/* determine if the children should tell inode about their events */\n\twatched = fsnotify_inode_watches_children(inode);\n\n\tspin_lock(&inode->i_lock);\n\t/* run all of the dentries associated with this inode.  Since this is a\n\t * directory, there damn well better only be one item on this list */\n\thlist_for_each_entry(alias, &inode->i_dentry, d_alias) {\n\t\tstruct dentry *child;\n\n\t\t/* run all of the children of the original inode and fix their\n\t\t * d_flags to indicate parental interest (their parent is the\n\t\t * original inode) */\n\t\tspin_lock(&alias->d_lock);\n\t\tlist_for_each_entry(child, &alias->d_subdirs, d_u.d_child) {\n\t\t\tif (!child->d_inode)\n\t\t\t\tcontinue;\n\n\t\t\tspin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\tif (watched)\n\t\t\t\tchild->d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED;\n\t\t\telse\n\t\t\t\tchild->d_flags &= ~DCACHE_FSNOTIFY_PARENT_WATCHED;\n\t\t\tspin_unlock(&child->d_lock);\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
        "func": "void __fsnotify_update_child_dentry_flags(struct inode *inode)\n{\n\tstruct dentry *alias;\n\tint watched;\n\n\tif (!S_ISDIR(inode->i_mode))\n\t\treturn;\n\n\t/* determine if the children should tell inode about their events */\n\twatched = fsnotify_inode_watches_children(inode);\n\n\tspin_lock(&inode->i_lock);\n\t/* run all of the dentries associated with this inode.  Since this is a\n\t * directory, there damn well better only be one item on this list */\n\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n\t\tstruct dentry *child;\n\n\t\t/* run all of the children of the original inode and fix their\n\t\t * d_flags to indicate parental interest (their parent is the\n\t\t * original inode) */\n\t\tspin_lock(&alias->d_lock);\n\t\tlist_for_each_entry(child, &alias->d_subdirs, d_child) {\n\t\t\tif (!child->d_inode)\n\t\t\t\tcontinue;\n\n\t\t\tspin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\tif (watched)\n\t\t\t\tchild->d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED;\n\t\t\telse\n\t\t\t\tchild->d_flags &= ~DCACHE_FSNOTIFY_PARENT_WATCHED;\n\t\t\tspin_unlock(&child->d_lock);\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,14 +12,14 @@\n \tspin_lock(&inode->i_lock);\n \t/* run all of the dentries associated with this inode.  Since this is a\n \t * directory, there damn well better only be one item on this list */\n-\thlist_for_each_entry(alias, &inode->i_dentry, d_alias) {\n+\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n \t\tstruct dentry *child;\n \n \t\t/* run all of the children of the original inode and fix their\n \t\t * d_flags to indicate parental interest (their parent is the\n \t\t * original inode) */\n \t\tspin_lock(&alias->d_lock);\n-\t\tlist_for_each_entry(child, &alias->d_subdirs, d_u.d_child) {\n+\t\tlist_for_each_entry(child, &alias->d_subdirs, d_child) {\n \t\t\tif (!child->d_inode)\n \t\t\t\tcontinue;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\thlist_for_each_entry(alias, &inode->i_dentry, d_alias) {",
                "\t\tlist_for_each_entry(child, &alias->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {",
                "\t\tlist_for_each_entry(child, &alias->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_find_alias",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static struct dentry *ll_find_alias(struct inode *inode, struct dentry *dentry)\n{\n\tstruct dentry *alias, *discon_alias, *invalid_alias;\n\tstruct ll_d_hlist_node *p;\n\n\tif (ll_d_hlist_empty(&inode->i_dentry))\n\t\treturn NULL;\n\n\tdiscon_alias = invalid_alias = NULL;\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_alias) {\n\t\tLASSERT(alias != dentry);\n\n\t\tspin_lock(&alias->d_lock);\n\t\tif (alias->d_flags & DCACHE_DISCONNECTED)\n\t\t\t/* LASSERT(last_discon == NULL); LU-405, bz 20055 */\n\t\t\tdiscon_alias = alias;\n\t\telse if (alias->d_parent == dentry->d_parent\t     &&\n\t\t\t alias->d_name.hash == dentry->d_name.hash       &&\n\t\t\t alias->d_name.len == dentry->d_name.len\t &&\n\t\t\t memcmp(alias->d_name.name, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len) == 0)\n\t\t\tinvalid_alias = alias;\n\t\tspin_unlock(&alias->d_lock);\n\n\t\tif (invalid_alias)\n\t\t\tbreak;\n\t}\n\talias = invalid_alias ?: discon_alias ?: NULL;\n\tif (alias) {\n\t\tspin_lock(&alias->d_lock);\n\t\tdget_dlock(alias);\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tll_unlock_dcache(inode);\n\n\treturn alias;\n}",
        "func": "static struct dentry *ll_find_alias(struct inode *inode, struct dentry *dentry)\n{\n\tstruct dentry *alias, *discon_alias, *invalid_alias;\n\tstruct ll_d_hlist_node *p;\n\n\tif (ll_d_hlist_empty(&inode->i_dentry))\n\t\treturn NULL;\n\n\tdiscon_alias = invalid_alias = NULL;\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_u.d_alias) {\n\t\tLASSERT(alias != dentry);\n\n\t\tspin_lock(&alias->d_lock);\n\t\tif (alias->d_flags & DCACHE_DISCONNECTED)\n\t\t\t/* LASSERT(last_discon == NULL); LU-405, bz 20055 */\n\t\t\tdiscon_alias = alias;\n\t\telse if (alias->d_parent == dentry->d_parent\t     &&\n\t\t\t alias->d_name.hash == dentry->d_name.hash       &&\n\t\t\t alias->d_name.len == dentry->d_name.len\t &&\n\t\t\t memcmp(alias->d_name.name, dentry->d_name.name,\n\t\t\t\tdentry->d_name.len) == 0)\n\t\t\tinvalid_alias = alias;\n\t\tspin_unlock(&alias->d_lock);\n\n\t\tif (invalid_alias)\n\t\t\tbreak;\n\t}\n\talias = invalid_alias ?: discon_alias ?: NULL;\n\tif (alias) {\n\t\tspin_lock(&alias->d_lock);\n\t\tdget_dlock(alias);\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tll_unlock_dcache(inode);\n\n\treturn alias;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \tdiscon_alias = invalid_alias = NULL;\n \n \tll_lock_dcache(inode);\n-\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_alias) {\n+\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_u.d_alias) {\n \t\tLASSERT(alias != dentry);\n \n \t\tspin_lock(&alias->d_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\tll_d_hlist_for_each_entry(alias, p, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_get_child_fid",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void ll_get_child_fid(struct inode * dir, struct qstr *name,\n\t\t\t     struct lu_fid *fid)\n{\n\tstruct dentry *parent, *child;\n\n\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_alias);\n\tchild = d_lookup(parent, name);\n\tif (child) {\n\t\tif (child->d_inode)\n\t\t\t*fid = *ll_inode2fid(child->d_inode);\n\t\tdput(child);\n\t}\n}",
        "func": "static void ll_get_child_fid(struct inode * dir, struct qstr *name,\n\t\t\t     struct lu_fid *fid)\n{\n\tstruct dentry *parent, *child;\n\n\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_u.d_alias);\n\tchild = d_lookup(parent, name);\n\tif (child) {\n\t\tif (child->d_inode)\n\t\t\t*fid = *ll_inode2fid(child->d_inode);\n\t\tdput(child);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n {\n \tstruct dentry *parent, *child;\n \n-\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_alias);\n+\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_u.d_alias);\n \tchild = d_lookup(parent, name);\n \tif (child) {\n \t\tif (child->d_inode)",
        "diff_line_info": {
            "deleted_lines": [
                "\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_alias);"
            ],
            "added_lines": [
                "\tparent = ll_d_hlist_entry(dir->i_dentry, struct dentry, d_u.d_alias);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_invalidate_negative_children",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void ll_invalidate_negative_children(struct inode *dir)\n{\n\tstruct dentry *dentry, *tmp_subdir;\n\tstruct ll_d_hlist_node *p;\n\n\tll_lock_dcache(dir);\n\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_alias) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!list_empty(&dentry->d_subdirs)) {\n\t\t\tstruct dentry *child;\n\n\t\t\tlist_for_each_entry_safe(child, tmp_subdir,\n\t\t\t\t\t\t &dentry->d_subdirs,\n\t\t\t\t\t\t d_u.d_child) {\n\t\t\t\tif (child->d_inode == NULL)\n\t\t\t\t\td_lustre_invalidate(child, 1);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\tll_unlock_dcache(dir);\n}",
        "func": "static void ll_invalidate_negative_children(struct inode *dir)\n{\n\tstruct dentry *dentry, *tmp_subdir;\n\tstruct ll_d_hlist_node *p;\n\n\tll_lock_dcache(dir);\n\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_u.d_alias) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!list_empty(&dentry->d_subdirs)) {\n\t\t\tstruct dentry *child;\n\n\t\t\tlist_for_each_entry_safe(child, tmp_subdir,\n\t\t\t\t\t\t &dentry->d_subdirs,\n\t\t\t\t\t\t d_child) {\n\t\t\t\tif (child->d_inode == NULL)\n\t\t\t\t\td_lustre_invalidate(child, 1);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\tll_unlock_dcache(dir);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,14 +4,14 @@\n \tstruct ll_d_hlist_node *p;\n \n \tll_lock_dcache(dir);\n-\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_alias) {\n+\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_u.d_alias) {\n \t\tspin_lock(&dentry->d_lock);\n \t\tif (!list_empty(&dentry->d_subdirs)) {\n \t\t\tstruct dentry *child;\n \n \t\t\tlist_for_each_entry_safe(child, tmp_subdir,\n \t\t\t\t\t\t &dentry->d_subdirs,\n-\t\t\t\t\t\t d_u.d_child) {\n+\t\t\t\t\t\t d_child) {\n \t\t\t\tif (child->d_inode == NULL)\n \t\t\t\t\td_lustre_invalidate(child, 1);\n \t\t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_alias) {",
                "\t\t\t\t\t\t d_u.d_child) {"
            ],
            "added_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &dir->i_dentry, d_u.d_alias) {",
                "\t\t\t\t\t\t d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_invalidate_aliases",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "func": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \t       inode->i_ino, inode->i_generation, inode);\n \n \tll_lock_dcache(inode);\n-\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n+\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n \t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n \t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n \t\t       dentry->d_name.name, dentry, dentry->d_parent,",
        "diff_line_info": {
            "deleted_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/remove_event_file_dir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void remove_event_file_dir(struct ftrace_event_file *file)\n{\n\tstruct dentry *dir = file->dir;\n\tstruct dentry *child;\n\n\tif (dir) {\n\t\tspin_lock(&dir->d_lock);\t/* probably unneeded */\n\t\tlist_for_each_entry(child, &dir->d_subdirs, d_u.d_child) {\n\t\t\tif (child->d_inode)\t/* probably unneeded */\n\t\t\t\tchild->d_inode->i_private = NULL;\n\t\t}\n\t\tspin_unlock(&dir->d_lock);\n\n\t\tdebugfs_remove_recursive(dir);\n\t}\n\n\tlist_del(&file->list);\n\tremove_subsystem(file->system);\n\tfree_event_filter(file->filter);\n\tkmem_cache_free(file_cachep, file);\n}",
        "func": "static void remove_event_file_dir(struct ftrace_event_file *file)\n{\n\tstruct dentry *dir = file->dir;\n\tstruct dentry *child;\n\n\tif (dir) {\n\t\tspin_lock(&dir->d_lock);\t/* probably unneeded */\n\t\tlist_for_each_entry(child, &dir->d_subdirs, d_child) {\n\t\t\tif (child->d_inode)\t/* probably unneeded */\n\t\t\t\tchild->d_inode->i_private = NULL;\n\t\t}\n\t\tspin_unlock(&dir->d_lock);\n\n\t\tdebugfs_remove_recursive(dir);\n\t}\n\n\tlist_del(&file->list);\n\tremove_subsystem(file->system);\n\tfree_event_filter(file->filter);\n\tkmem_cache_free(file_cachep, file);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \n \tif (dir) {\n \t\tspin_lock(&dir->d_lock);\t/* probably unneeded */\n-\t\tlist_for_each_entry(child, &dir->d_subdirs, d_u.d_child) {\n+\t\tlist_for_each_entry(child, &dir->d_subdirs, d_child) {\n \t\t\tif (child->d_inode)\t/* probably unneeded */\n \t\t\t\tchild->d_inode->i_private = NULL;\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tlist_for_each_entry(child, &dir->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\t\tlist_for_each_entry(child, &dir->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/spufs_prune_dir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "func": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct dentry *dentry, *tmp;\n \n \tmutex_lock(&dir->d_inode->i_mutex);\n-\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n \t\tspin_lock(&dentry->d_lock);\n \t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n \t\t\tdget_dlock(dentry);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/get_next_positive_dentry",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static struct dentry *get_next_positive_dentry(struct dentry *prev,\n\t\t\t\t\t\tstruct dentry *root)\n{\n\tstruct autofs_sb_info *sbi = autofs4_sbi(root->d_sb);\n\tstruct list_head *next;\n\tstruct dentry *p, *ret;\n\n\tif (prev == NULL)\n\t\treturn dget(root);\n\n\tspin_lock(&sbi->lookup_lock);\nrelock:\n\tp = prev;\n\tspin_lock(&p->d_lock);\nagain:\n\tnext = p->d_subdirs.next;\n\tif (next == &p->d_subdirs) {\n\t\twhile (1) {\n\t\t\tstruct dentry *parent;\n\n\t\t\tif (p == root) {\n\t\t\t\tspin_unlock(&p->d_lock);\n\t\t\t\tspin_unlock(&sbi->lookup_lock);\n\t\t\t\tdput(prev);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\tparent = p->d_parent;\n\t\t\tif (!spin_trylock(&parent->d_lock)) {\n\t\t\t\tspin_unlock(&p->d_lock);\n\t\t\t\tcpu_relax();\n\t\t\t\tgoto relock;\n\t\t\t}\n\t\t\tspin_unlock(&p->d_lock);\n\t\t\tnext = p->d_u.d_child.next;\n\t\t\tp = parent;\n\t\t\tif (next != &parent->d_subdirs)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tret = list_entry(next, struct dentry, d_u.d_child);\n\n\tspin_lock_nested(&ret->d_lock, DENTRY_D_LOCK_NESTED);\n\t/* Negative dentry - try next */\n\tif (!simple_positive(ret)) {\n\t\tspin_unlock(&p->d_lock);\n\t\tlock_set_subclass(&ret->d_lock.dep_map, 0, _RET_IP_);\n\t\tp = ret;\n\t\tgoto again;\n\t}\n\tdget_dlock(ret);\n\tspin_unlock(&ret->d_lock);\n\tspin_unlock(&p->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\n\tdput(prev);\n\n\treturn ret;\n}",
        "func": "static struct dentry *get_next_positive_dentry(struct dentry *prev,\n\t\t\t\t\t\tstruct dentry *root)\n{\n\tstruct autofs_sb_info *sbi = autofs4_sbi(root->d_sb);\n\tstruct list_head *next;\n\tstruct dentry *p, *ret;\n\n\tif (prev == NULL)\n\t\treturn dget(root);\n\n\tspin_lock(&sbi->lookup_lock);\nrelock:\n\tp = prev;\n\tspin_lock(&p->d_lock);\nagain:\n\tnext = p->d_subdirs.next;\n\tif (next == &p->d_subdirs) {\n\t\twhile (1) {\n\t\t\tstruct dentry *parent;\n\n\t\t\tif (p == root) {\n\t\t\t\tspin_unlock(&p->d_lock);\n\t\t\t\tspin_unlock(&sbi->lookup_lock);\n\t\t\t\tdput(prev);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\tparent = p->d_parent;\n\t\t\tif (!spin_trylock(&parent->d_lock)) {\n\t\t\t\tspin_unlock(&p->d_lock);\n\t\t\t\tcpu_relax();\n\t\t\t\tgoto relock;\n\t\t\t}\n\t\t\tspin_unlock(&p->d_lock);\n\t\t\tnext = p->d_child.next;\n\t\t\tp = parent;\n\t\t\tif (next != &parent->d_subdirs)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tret = list_entry(next, struct dentry, d_child);\n\n\tspin_lock_nested(&ret->d_lock, DENTRY_D_LOCK_NESTED);\n\t/* Negative dentry - try next */\n\tif (!simple_positive(ret)) {\n\t\tspin_unlock(&p->d_lock);\n\t\tlock_set_subclass(&ret->d_lock.dep_map, 0, _RET_IP_);\n\t\tp = ret;\n\t\tgoto again;\n\t}\n\tdget_dlock(ret);\n\tspin_unlock(&ret->d_lock);\n\tspin_unlock(&p->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\n\tdput(prev);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,13 +32,13 @@\n \t\t\t\tgoto relock;\n \t\t\t}\n \t\t\tspin_unlock(&p->d_lock);\n-\t\t\tnext = p->d_u.d_child.next;\n+\t\t\tnext = p->d_child.next;\n \t\t\tp = parent;\n \t\t\tif (next != &parent->d_subdirs)\n \t\t\t\tbreak;\n \t\t}\n \t}\n-\tret = list_entry(next, struct dentry, d_u.d_child);\n+\tret = list_entry(next, struct dentry, d_child);\n \n \tspin_lock_nested(&ret->d_lock, DENTRY_D_LOCK_NESTED);\n \t/* Negative dentry - try next */",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tnext = p->d_u.d_child.next;",
                "\tret = list_entry(next, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\t\tnext = p->d_child.next;",
                "\tret = list_entry(next, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/get_next_positive_subdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static struct dentry *get_next_positive_subdir(struct dentry *prev,\n\t\t\t\t\t\tstruct dentry *root)\n{\n\tstruct autofs_sb_info *sbi = autofs4_sbi(root->d_sb);\n\tstruct list_head *next;\n\tstruct dentry *q;\n\n\tspin_lock(&sbi->lookup_lock);\n\tspin_lock(&root->d_lock);\n\n\tif (prev)\n\t\tnext = prev->d_u.d_child.next;\n\telse {\n\t\tprev = dget_dlock(root);\n\t\tnext = prev->d_subdirs.next;\n\t}\n\ncont:\n\tif (next == &root->d_subdirs) {\n\t\tspin_unlock(&root->d_lock);\n\t\tspin_unlock(&sbi->lookup_lock);\n\t\tdput(prev);\n\t\treturn NULL;\n\t}\n\n\tq = list_entry(next, struct dentry, d_u.d_child);\n\n\tspin_lock_nested(&q->d_lock, DENTRY_D_LOCK_NESTED);\n\t/* Already gone or negative dentry (under construction) - try next */\n\tif (!d_count(q) || !simple_positive(q)) {\n\t\tspin_unlock(&q->d_lock);\n\t\tnext = q->d_u.d_child.next;\n\t\tgoto cont;\n\t}\n\tdget_dlock(q);\n\tspin_unlock(&q->d_lock);\n\tspin_unlock(&root->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\n\tdput(prev);\n\n\treturn q;\n}",
        "func": "static struct dentry *get_next_positive_subdir(struct dentry *prev,\n\t\t\t\t\t\tstruct dentry *root)\n{\n\tstruct autofs_sb_info *sbi = autofs4_sbi(root->d_sb);\n\tstruct list_head *next;\n\tstruct dentry *q;\n\n\tspin_lock(&sbi->lookup_lock);\n\tspin_lock(&root->d_lock);\n\n\tif (prev)\n\t\tnext = prev->d_child.next;\n\telse {\n\t\tprev = dget_dlock(root);\n\t\tnext = prev->d_subdirs.next;\n\t}\n\ncont:\n\tif (next == &root->d_subdirs) {\n\t\tspin_unlock(&root->d_lock);\n\t\tspin_unlock(&sbi->lookup_lock);\n\t\tdput(prev);\n\t\treturn NULL;\n\t}\n\n\tq = list_entry(next, struct dentry, d_child);\n\n\tspin_lock_nested(&q->d_lock, DENTRY_D_LOCK_NESTED);\n\t/* Already gone or negative dentry (under construction) - try next */\n\tif (!d_count(q) || !simple_positive(q)) {\n\t\tspin_unlock(&q->d_lock);\n\t\tnext = q->d_child.next;\n\t\tgoto cont;\n\t}\n\tdget_dlock(q);\n\tspin_unlock(&q->d_lock);\n\tspin_unlock(&root->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\n\tdput(prev);\n\n\treturn q;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \tspin_lock(&root->d_lock);\n \n \tif (prev)\n-\t\tnext = prev->d_u.d_child.next;\n+\t\tnext = prev->d_child.next;\n \telse {\n \t\tprev = dget_dlock(root);\n \t\tnext = prev->d_subdirs.next;\n@@ -23,13 +23,13 @@\n \t\treturn NULL;\n \t}\n \n-\tq = list_entry(next, struct dentry, d_u.d_child);\n+\tq = list_entry(next, struct dentry, d_child);\n \n \tspin_lock_nested(&q->d_lock, DENTRY_D_LOCK_NESTED);\n \t/* Already gone or negative dentry (under construction) - try next */\n \tif (!d_count(q) || !simple_positive(q)) {\n \t\tspin_unlock(&q->d_lock);\n-\t\tnext = q->d_u.d_child.next;\n+\t\tnext = q->d_child.next;\n \t\tgoto cont;\n \t}\n \tdget_dlock(q);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tnext = prev->d_u.d_child.next;",
                "\tq = list_entry(next, struct dentry, d_u.d_child);",
                "\t\tnext = q->d_u.d_child.next;"
            ],
            "added_lines": [
                "\t\tnext = prev->d_child.next;",
                "\tq = list_entry(next, struct dentry, d_child);",
                "\t\tnext = q->d_child.next;"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/autofs4_expire_indirect",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "struct dentry *autofs4_expire_indirect(struct super_block *sb,\n\t\t\t\t       struct vfsmount *mnt,\n\t\t\t\t       struct autofs_sb_info *sbi,\n\t\t\t\t       int how)\n{\n\tunsigned long timeout;\n\tstruct dentry *root = sb->s_root;\n\tstruct dentry *dentry;\n\tstruct dentry *expired;\n\tstruct autofs_info *ino;\n\n\tif (!root)\n\t\treturn NULL;\n\n\tnow = jiffies;\n\ttimeout = sbi->exp_timeout;\n\n\tdentry = NULL;\n\twhile ((dentry = get_next_positive_subdir(dentry, root))) {\n\t\tspin_lock(&sbi->fs_lock);\n\t\tino = autofs4_dentry_ino(dentry);\n\t\tif (ino->flags & AUTOFS_INF_NO_RCU)\n\t\t\texpired = NULL;\n\t\telse\n\t\t\texpired = should_expire(dentry, mnt, timeout, how);\n\t\tif (!expired) {\n\t\t\tspin_unlock(&sbi->fs_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tino = autofs4_dentry_ino(expired);\n\t\tino->flags |= AUTOFS_INF_NO_RCU;\n\t\tspin_unlock(&sbi->fs_lock);\n\t\tsynchronize_rcu();\n\t\tspin_lock(&sbi->fs_lock);\n\t\tif (should_expire(expired, mnt, timeout, how)) {\n\t\t\tif (expired != dentry)\n\t\t\t\tdput(dentry);\n\t\t\tgoto found;\n\t\t}\n\n\t\tino->flags &= ~AUTOFS_INF_NO_RCU;\n\t\tif (expired != dentry)\n\t\t\tdput(expired);\n\t\tspin_unlock(&sbi->fs_lock);\n\t}\n\treturn NULL;\n\nfound:\n\tDPRINTK(\"returning %p %.*s\",\n\t\texpired, (int)expired->d_name.len, expired->d_name.name);\n\tino->flags |= AUTOFS_INF_EXPIRING;\n\tsmp_mb();\n\tino->flags &= ~AUTOFS_INF_NO_RCU;\n\tinit_completion(&ino->expire_complete);\n\tspin_unlock(&sbi->fs_lock);\n\tspin_lock(&sbi->lookup_lock);\n\tspin_lock(&expired->d_parent->d_lock);\n\tspin_lock_nested(&expired->d_lock, DENTRY_D_LOCK_NESTED);\n\tlist_move(&expired->d_parent->d_subdirs, &expired->d_u.d_child);\n\tspin_unlock(&expired->d_lock);\n\tspin_unlock(&expired->d_parent->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\treturn expired;\n}",
        "func": "struct dentry *autofs4_expire_indirect(struct super_block *sb,\n\t\t\t\t       struct vfsmount *mnt,\n\t\t\t\t       struct autofs_sb_info *sbi,\n\t\t\t\t       int how)\n{\n\tunsigned long timeout;\n\tstruct dentry *root = sb->s_root;\n\tstruct dentry *dentry;\n\tstruct dentry *expired;\n\tstruct autofs_info *ino;\n\n\tif (!root)\n\t\treturn NULL;\n\n\tnow = jiffies;\n\ttimeout = sbi->exp_timeout;\n\n\tdentry = NULL;\n\twhile ((dentry = get_next_positive_subdir(dentry, root))) {\n\t\tspin_lock(&sbi->fs_lock);\n\t\tino = autofs4_dentry_ino(dentry);\n\t\tif (ino->flags & AUTOFS_INF_NO_RCU)\n\t\t\texpired = NULL;\n\t\telse\n\t\t\texpired = should_expire(dentry, mnt, timeout, how);\n\t\tif (!expired) {\n\t\t\tspin_unlock(&sbi->fs_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tino = autofs4_dentry_ino(expired);\n\t\tino->flags |= AUTOFS_INF_NO_RCU;\n\t\tspin_unlock(&sbi->fs_lock);\n\t\tsynchronize_rcu();\n\t\tspin_lock(&sbi->fs_lock);\n\t\tif (should_expire(expired, mnt, timeout, how)) {\n\t\t\tif (expired != dentry)\n\t\t\t\tdput(dentry);\n\t\t\tgoto found;\n\t\t}\n\n\t\tino->flags &= ~AUTOFS_INF_NO_RCU;\n\t\tif (expired != dentry)\n\t\t\tdput(expired);\n\t\tspin_unlock(&sbi->fs_lock);\n\t}\n\treturn NULL;\n\nfound:\n\tDPRINTK(\"returning %p %.*s\",\n\t\texpired, (int)expired->d_name.len, expired->d_name.name);\n\tino->flags |= AUTOFS_INF_EXPIRING;\n\tsmp_mb();\n\tino->flags &= ~AUTOFS_INF_NO_RCU;\n\tinit_completion(&ino->expire_complete);\n\tspin_unlock(&sbi->fs_lock);\n\tspin_lock(&sbi->lookup_lock);\n\tspin_lock(&expired->d_parent->d_lock);\n\tspin_lock_nested(&expired->d_lock, DENTRY_D_LOCK_NESTED);\n\tlist_move(&expired->d_parent->d_subdirs, &expired->d_child);\n\tspin_unlock(&expired->d_lock);\n\tspin_unlock(&expired->d_parent->d_lock);\n\tspin_unlock(&sbi->lookup_lock);\n\treturn expired;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,7 +56,7 @@\n \tspin_lock(&sbi->lookup_lock);\n \tspin_lock(&expired->d_parent->d_lock);\n \tspin_lock_nested(&expired->d_lock, DENTRY_D_LOCK_NESTED);\n-\tlist_move(&expired->d_parent->d_subdirs, &expired->d_u.d_child);\n+\tlist_move(&expired->d_parent->d_subdirs, &expired->d_child);\n \tspin_unlock(&expired->d_lock);\n \tspin_unlock(&expired->d_parent->d_lock);\n \tspin_unlock(&sbi->lookup_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_move(&expired->d_parent->d_subdirs, &expired->d_u.d_child);"
            ],
            "added_lines": [
                "\tlist_move(&expired->d_parent->d_subdirs, &expired->d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/dcache_readdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "int dcache_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tstruct dentry *cursor = file->private_data;\n\tstruct list_head *p, *q = &cursor->d_u.d_child;\n\n\tif (!dir_emit_dots(file, ctx))\n\t\treturn 0;\n\tspin_lock(&dentry->d_lock);\n\tif (ctx->pos == 2)\n\t\tlist_move(q, &dentry->d_subdirs);\n\n\tfor (p = q->next; p != &dentry->d_subdirs; p = p->next) {\n\t\tstruct dentry *next = list_entry(p, struct dentry, d_u.d_child);\n\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (!simple_positive(next)) {\n\t\t\tspin_unlock(&next->d_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock(&next->d_lock);\n\t\tspin_unlock(&dentry->d_lock);\n\t\tif (!dir_emit(ctx, next->d_name.name, next->d_name.len,\n\t\t\t      next->d_inode->i_ino, dt_type(next->d_inode)))\n\t\t\treturn 0;\n\t\tspin_lock(&dentry->d_lock);\n\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t/* next is still alive */\n\t\tlist_move(q, p);\n\t\tspin_unlock(&next->d_lock);\n\t\tp = q;\n\t\tctx->pos++;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\treturn 0;\n}",
        "func": "int dcache_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tstruct dentry *cursor = file->private_data;\n\tstruct list_head *p, *q = &cursor->d_child;\n\n\tif (!dir_emit_dots(file, ctx))\n\t\treturn 0;\n\tspin_lock(&dentry->d_lock);\n\tif (ctx->pos == 2)\n\t\tlist_move(q, &dentry->d_subdirs);\n\n\tfor (p = q->next; p != &dentry->d_subdirs; p = p->next) {\n\t\tstruct dentry *next = list_entry(p, struct dentry, d_child);\n\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (!simple_positive(next)) {\n\t\t\tspin_unlock(&next->d_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_unlock(&next->d_lock);\n\t\tspin_unlock(&dentry->d_lock);\n\t\tif (!dir_emit(ctx, next->d_name.name, next->d_name.len,\n\t\t\t      next->d_inode->i_ino, dt_type(next->d_inode)))\n\t\t\treturn 0;\n\t\tspin_lock(&dentry->d_lock);\n\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t/* next is still alive */\n\t\tlist_move(q, p);\n\t\tspin_unlock(&next->d_lock);\n\t\tp = q;\n\t\tctx->pos++;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n {\n \tstruct dentry *dentry = file->f_path.dentry;\n \tstruct dentry *cursor = file->private_data;\n-\tstruct list_head *p, *q = &cursor->d_u.d_child;\n+\tstruct list_head *p, *q = &cursor->d_child;\n \n \tif (!dir_emit_dots(file, ctx))\n \t\treturn 0;\n@@ -11,7 +11,7 @@\n \t\tlist_move(q, &dentry->d_subdirs);\n \n \tfor (p = q->next; p != &dentry->d_subdirs; p = p->next) {\n-\t\tstruct dentry *next = list_entry(p, struct dentry, d_u.d_child);\n+\t\tstruct dentry *next = list_entry(p, struct dentry, d_child);\n \t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n \t\tif (!simple_positive(next)) {\n \t\t\tspin_unlock(&next->d_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct list_head *p, *q = &cursor->d_u.d_child;",
                "\t\tstruct dentry *next = list_entry(p, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\tstruct list_head *p, *q = &cursor->d_child;",
                "\t\tstruct dentry *next = list_entry(p, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/dcache_dir_lseek",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "loff_t dcache_dir_lseek(struct file *file, loff_t offset, int whence)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\tswitch (whence) {\n\t\tcase 1:\n\t\t\toffset += file->f_pos;\n\t\tcase 0:\n\t\t\tif (offset >= 0)\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\t\treturn -EINVAL;\n\t}\n\tif (offset != file->f_pos) {\n\t\tfile->f_pos = offset;\n\t\tif (file->f_pos >= 2) {\n\t\t\tstruct list_head *p;\n\t\t\tstruct dentry *cursor = file->private_data;\n\t\t\tloff_t n = file->f_pos - 2;\n\n\t\t\tspin_lock(&dentry->d_lock);\n\t\t\t/* d_lock not required for cursor */\n\t\t\tlist_del(&cursor->d_u.d_child);\n\t\t\tp = dentry->d_subdirs.next;\n\t\t\twhile (n && p != &dentry->d_subdirs) {\n\t\t\t\tstruct dentry *next;\n\t\t\t\tnext = list_entry(p, struct dentry, d_u.d_child);\n\t\t\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\t\tif (simple_positive(next))\n\t\t\t\t\tn--;\n\t\t\t\tspin_unlock(&next->d_lock);\n\t\t\t\tp = p->next;\n\t\t\t}\n\t\t\tlist_add_tail(&cursor->d_u.d_child, p);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n\treturn offset;\n}",
        "func": "loff_t dcache_dir_lseek(struct file *file, loff_t offset, int whence)\n{\n\tstruct dentry *dentry = file->f_path.dentry;\n\tmutex_lock(&dentry->d_inode->i_mutex);\n\tswitch (whence) {\n\t\tcase 1:\n\t\t\toffset += file->f_pos;\n\t\tcase 0:\n\t\t\tif (offset >= 0)\n\t\t\t\tbreak;\n\t\tdefault:\n\t\t\tmutex_unlock(&dentry->d_inode->i_mutex);\n\t\t\treturn -EINVAL;\n\t}\n\tif (offset != file->f_pos) {\n\t\tfile->f_pos = offset;\n\t\tif (file->f_pos >= 2) {\n\t\t\tstruct list_head *p;\n\t\t\tstruct dentry *cursor = file->private_data;\n\t\t\tloff_t n = file->f_pos - 2;\n\n\t\t\tspin_lock(&dentry->d_lock);\n\t\t\t/* d_lock not required for cursor */\n\t\t\tlist_del(&cursor->d_child);\n\t\t\tp = dentry->d_subdirs.next;\n\t\t\twhile (n && p != &dentry->d_subdirs) {\n\t\t\t\tstruct dentry *next;\n\t\t\t\tnext = list_entry(p, struct dentry, d_child);\n\t\t\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\t\tif (simple_positive(next))\n\t\t\t\t\tn--;\n\t\t\t\tspin_unlock(&next->d_lock);\n\t\t\t\tp = p->next;\n\t\t\t}\n\t\t\tlist_add_tail(&cursor->d_child, p);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tmutex_unlock(&dentry->d_inode->i_mutex);\n\treturn offset;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,18 +21,18 @@\n \n \t\t\tspin_lock(&dentry->d_lock);\n \t\t\t/* d_lock not required for cursor */\n-\t\t\tlist_del(&cursor->d_u.d_child);\n+\t\t\tlist_del(&cursor->d_child);\n \t\t\tp = dentry->d_subdirs.next;\n \t\t\twhile (n && p != &dentry->d_subdirs) {\n \t\t\t\tstruct dentry *next;\n-\t\t\t\tnext = list_entry(p, struct dentry, d_u.d_child);\n+\t\t\t\tnext = list_entry(p, struct dentry, d_child);\n \t\t\t\tspin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);\n \t\t\t\tif (simple_positive(next))\n \t\t\t\t\tn--;\n \t\t\t\tspin_unlock(&next->d_lock);\n \t\t\t\tp = p->next;\n \t\t\t}\n-\t\t\tlist_add_tail(&cursor->d_u.d_child, p);\n+\t\t\tlist_add_tail(&cursor->d_child, p);\n \t\t\tspin_unlock(&dentry->d_lock);\n \t\t}\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tlist_del(&cursor->d_u.d_child);",
                "\t\t\t\tnext = list_entry(p, struct dentry, d_u.d_child);",
                "\t\t\tlist_add_tail(&cursor->d_u.d_child, p);"
            ],
            "added_lines": [
                "\t\t\tlist_del(&cursor->d_child);",
                "\t\t\t\tnext = list_entry(p, struct dentry, d_child);",
                "\t\t\tlist_add_tail(&cursor->d_child, p);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/simple_empty",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "int simple_empty(struct dentry *dentry)\n{\n\tstruct dentry *child;\n\tint ret = 0;\n\n\tspin_lock(&dentry->d_lock);\n\tlist_for_each_entry(child, &dentry->d_subdirs, d_u.d_child) {\n\t\tspin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (simple_positive(child)) {\n\t\t\tspin_unlock(&child->d_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock(&child->d_lock);\n\t}\n\tret = 1;\nout:\n\tspin_unlock(&dentry->d_lock);\n\treturn ret;\n}",
        "func": "int simple_empty(struct dentry *dentry)\n{\n\tstruct dentry *child;\n\tint ret = 0;\n\n\tspin_lock(&dentry->d_lock);\n\tlist_for_each_entry(child, &dentry->d_subdirs, d_child) {\n\t\tspin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (simple_positive(child)) {\n\t\t\tspin_unlock(&child->d_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock(&child->d_lock);\n\t}\n\tret = 1;\nout:\n\tspin_unlock(&dentry->d_lock);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tint ret = 0;\n \n \tspin_lock(&dentry->d_lock);\n-\tlist_for_each_entry(child, &dentry->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry(child, &dentry->d_subdirs, d_child) {\n \t\tspin_lock_nested(&child->d_lock, DENTRY_D_LOCK_NESTED);\n \t\tif (simple_positive(child)) {\n \t\t\tspin_unlock(&child->d_lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry(child, &dentry->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry(child, &dentry->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_invalidate_aliases",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "func": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \t       inode->i_ino, inode->i_generation, inode);\n \n \tll_lock_dcache(inode);\n-\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n+\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n \t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n \t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n \t\t       dentry->d_name.name, dentry, dentry->d_parent,",
        "diff_line_info": {
            "deleted_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/__dcache_readdir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static int __dcache_readdir(struct file *file,  struct dir_context *ctx,\n\t\t\t    u32 shared_gen)\n{\n\tstruct ceph_file_info *fi = file->private_data;\n\tstruct dentry *parent = file->f_dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct list_head *p;\n\tstruct dentry *dentry, *last;\n\tstruct ceph_dentry_info *di;\n\tint err = 0;\n\n\t/* claim ref on last dentry we returned */\n\tlast = fi->dentry;\n\tfi->dentry = NULL;\n\n\tdout(\"__dcache_readdir %p v%u at %llu (last %p)\\n\",\n\t     dir, shared_gen, ctx->pos, last);\n\n\tspin_lock(&parent->d_lock);\n\n\t/* start at beginning? */\n\tif (ctx->pos == 2 || last == NULL ||\n\t    fpos_cmp(ctx->pos, ceph_dentry(last)->offset) < 0) {\n\t\tif (list_empty(&parent->d_subdirs))\n\t\t\tgoto out_unlock;\n\t\tp = parent->d_subdirs.prev;\n\t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n\t} else {\n\t\tp = last->d_u.d_child.prev;\n\t}\n\nmore:\n\tdentry = list_entry(p, struct dentry, d_u.d_child);\n\tdi = ceph_dentry(dentry);\n\twhile (1) {\n\t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n\t\t     d_unhashed(dentry) ? \"!hashed\" : \"hashed\",\n\t\t     parent->d_subdirs.prev, parent->d_subdirs.next);\n\t\tif (p == &parent->d_subdirs) {\n\t\t\tfi->flags |= CEPH_F_ATEND;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (di->lease_shared_gen == shared_gen &&\n\t\t    !d_unhashed(dentry) && dentry->d_inode &&\n\t\t    ceph_snap(dentry->d_inode) != CEPH_SNAPDIR &&\n\t\t    ceph_ino(dentry->d_inode) != CEPH_INO_CEPH &&\n\t\t    fpos_cmp(ctx->pos, di->offset) <= 0)\n\t\t\tbreak;\n\t\tdout(\" skipping %p %.*s at %llu (%llu)%s%s\\n\", dentry,\n\t\t     dentry->d_name.len, dentry->d_name.name, di->offset,\n\t\t     ctx->pos, d_unhashed(dentry) ? \" unhashed\" : \"\",\n\t\t     !dentry->d_inode ? \" null\" : \"\");\n\t\tspin_unlock(&dentry->d_lock);\n\t\tp = p->prev;\n\t\tdentry = list_entry(p, struct dentry, d_u.d_child);\n\t\tdi = ceph_dentry(dentry);\n\t}\n\n\tdget_dlock(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&parent->d_lock);\n\n\t/* make sure a dentry wasn't dropped while we didn't have parent lock */\n\tif (!ceph_dir_is_complete(dir)) {\n\t\tdout(\" lost dir complete on %p; falling back to mds\\n\", dir);\n\t\tdput(dentry);\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tdout(\" %llu (%llu) dentry %p %.*s %p\\n\", di->offset, ctx->pos,\n\t     dentry, dentry->d_name.len, dentry->d_name.name, dentry->d_inode);\n\tif (!dir_emit(ctx, dentry->d_name.name,\n\t\t      dentry->d_name.len,\n\t\t      ceph_translate_ino(dentry->d_sb, dentry->d_inode->i_ino),\n\t\t      dentry->d_inode->i_mode >> 12)) {\n\t\tif (last) {\n\t\t\t/* remember our position */\n\t\t\tfi->dentry = last;\n\t\t\tfi->next_offset = fpos_off(di->offset);\n\t\t}\n\t\tdput(dentry);\n\t\treturn 0;\n\t}\n\n\tctx->pos = di->offset + 1;\n\n\tif (last)\n\t\tdput(last);\n\tlast = dentry;\n\n\tspin_lock(&parent->d_lock);\n\tp = p->prev;\t/* advance to next dentry */\n\tgoto more;\n\nout_unlock:\n\tspin_unlock(&parent->d_lock);\nout:\n\tif (last)\n\t\tdput(last);\n\treturn err;\n}",
        "func": "static int __dcache_readdir(struct file *file,  struct dir_context *ctx,\n\t\t\t    u32 shared_gen)\n{\n\tstruct ceph_file_info *fi = file->private_data;\n\tstruct dentry *parent = file->f_dentry;\n\tstruct inode *dir = parent->d_inode;\n\tstruct list_head *p;\n\tstruct dentry *dentry, *last;\n\tstruct ceph_dentry_info *di;\n\tint err = 0;\n\n\t/* claim ref on last dentry we returned */\n\tlast = fi->dentry;\n\tfi->dentry = NULL;\n\n\tdout(\"__dcache_readdir %p v%u at %llu (last %p)\\n\",\n\t     dir, shared_gen, ctx->pos, last);\n\n\tspin_lock(&parent->d_lock);\n\n\t/* start at beginning? */\n\tif (ctx->pos == 2 || last == NULL ||\n\t    fpos_cmp(ctx->pos, ceph_dentry(last)->offset) < 0) {\n\t\tif (list_empty(&parent->d_subdirs))\n\t\t\tgoto out_unlock;\n\t\tp = parent->d_subdirs.prev;\n\t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n\t} else {\n\t\tp = last->d_child.prev;\n\t}\n\nmore:\n\tdentry = list_entry(p, struct dentry, d_child);\n\tdi = ceph_dentry(dentry);\n\twhile (1) {\n\t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n\t\t     d_unhashed(dentry) ? \"!hashed\" : \"hashed\",\n\t\t     parent->d_subdirs.prev, parent->d_subdirs.next);\n\t\tif (p == &parent->d_subdirs) {\n\t\t\tfi->flags |= CEPH_F_ATEND;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\tif (di->lease_shared_gen == shared_gen &&\n\t\t    !d_unhashed(dentry) && dentry->d_inode &&\n\t\t    ceph_snap(dentry->d_inode) != CEPH_SNAPDIR &&\n\t\t    ceph_ino(dentry->d_inode) != CEPH_INO_CEPH &&\n\t\t    fpos_cmp(ctx->pos, di->offset) <= 0)\n\t\t\tbreak;\n\t\tdout(\" skipping %p %.*s at %llu (%llu)%s%s\\n\", dentry,\n\t\t     dentry->d_name.len, dentry->d_name.name, di->offset,\n\t\t     ctx->pos, d_unhashed(dentry) ? \" unhashed\" : \"\",\n\t\t     !dentry->d_inode ? \" null\" : \"\");\n\t\tspin_unlock(&dentry->d_lock);\n\t\tp = p->prev;\n\t\tdentry = list_entry(p, struct dentry, d_child);\n\t\tdi = ceph_dentry(dentry);\n\t}\n\n\tdget_dlock(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&parent->d_lock);\n\n\t/* make sure a dentry wasn't dropped while we didn't have parent lock */\n\tif (!ceph_dir_is_complete(dir)) {\n\t\tdout(\" lost dir complete on %p; falling back to mds\\n\", dir);\n\t\tdput(dentry);\n\t\terr = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tdout(\" %llu (%llu) dentry %p %.*s %p\\n\", di->offset, ctx->pos,\n\t     dentry, dentry->d_name.len, dentry->d_name.name, dentry->d_inode);\n\tif (!dir_emit(ctx, dentry->d_name.name,\n\t\t      dentry->d_name.len,\n\t\t      ceph_translate_ino(dentry->d_sb, dentry->d_inode->i_ino),\n\t\t      dentry->d_inode->i_mode >> 12)) {\n\t\tif (last) {\n\t\t\t/* remember our position */\n\t\t\tfi->dentry = last;\n\t\t\tfi->next_offset = fpos_off(di->offset);\n\t\t}\n\t\tdput(dentry);\n\t\treturn 0;\n\t}\n\n\tctx->pos = di->offset + 1;\n\n\tif (last)\n\t\tdput(last);\n\tlast = dentry;\n\n\tspin_lock(&parent->d_lock);\n\tp = p->prev;\t/* advance to next dentry */\n\tgoto more;\n\nout_unlock:\n\tspin_unlock(&parent->d_lock);\nout:\n\tif (last)\n\t\tdput(last);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,11 +26,11 @@\n \t\tp = parent->d_subdirs.prev;\n \t\tdout(\" initial p %p/%p\\n\", p->prev, p->next);\n \t} else {\n-\t\tp = last->d_u.d_child.prev;\n+\t\tp = last->d_child.prev;\n \t}\n \n more:\n-\tdentry = list_entry(p, struct dentry, d_u.d_child);\n+\tdentry = list_entry(p, struct dentry, d_child);\n \tdi = ceph_dentry(dentry);\n \twhile (1) {\n \t\tdout(\" p %p/%p %s d_subdirs %p/%p\\n\", p->prev, p->next,\n@@ -53,7 +53,7 @@\n \t\t     !dentry->d_inode ? \" null\" : \"\");\n \t\tspin_unlock(&dentry->d_lock);\n \t\tp = p->prev;\n-\t\tdentry = list_entry(p, struct dentry, d_u.d_child);\n+\t\tdentry = list_entry(p, struct dentry, d_child);\n \t\tdi = ceph_dentry(dentry);\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tp = last->d_u.d_child.prev;",
                "\tdentry = list_entry(p, struct dentry, d_u.d_child);",
                "\t\tdentry = list_entry(p, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\tp = last->d_child.prev;",
                "\tdentry = list_entry(p, struct dentry, d_child);",
                "\t\tdentry = list_entry(p, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/spufs_prune_dir",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "func": "static void spufs_prune_dir(struct dentry *dir)\n{\n\tstruct dentry *dentry, *tmp;\n\n\tmutex_lock(&dir->d_inode->i_mutex);\n\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n\t\t\tdget_dlock(dentry);\n\t\t\t__d_drop(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tsimple_unlink(dir->d_inode, dentry);\n\t\t\t/* XXX: what was dcache_lock protecting here? Other\n\t\t\t * filesystems (IB, configfs) release dcache_lock\n\t\t\t * before unlink */\n\t\t\tdput(dentry);\n\t\t} else {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t}\n\t}\n\tshrink_dcache_parent(dir);\n\tmutex_unlock(&dir->d_inode->i_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tstruct dentry *dentry, *tmp;\n \n \tmutex_lock(&dir->d_inode->i_mutex);\n-\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {\n+\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {\n \t\tspin_lock(&dentry->d_lock);\n \t\tif (!(d_unhashed(dentry)) && dentry->d_inode) {\n \t\t\tdget_dlock(dentry);",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_u.d_child) {"
            ],
            "added_lines": [
                "\tlist_for_each_entry_safe(dentry, tmp, &dir->d_subdirs, d_child) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/ll_invalidate_aliases",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "func": "void ll_invalidate_aliases(struct inode *inode)\n{\n\tstruct dentry *dentry;\n\tstruct ll_d_hlist_node *p;\n\n\tLASSERT(inode != NULL);\n\n\tCDEBUG(D_INODE, \"marking dentries for ino %lu/%u(%p) invalid\\n\",\n\t       inode->i_ino, inode->i_generation, inode);\n\n\tll_lock_dcache(inode);\n\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n\t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n\t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n\t\t       dentry->d_name.name, dentry, dentry->d_parent,\n\t\t       dentry->d_inode, dentry->d_flags);\n\n\t\tif (unlikely(dentry == dentry->d_sb->s_root)) {\n\t\t\tCERROR(\"%s: called on root dentry=%p, fid=\"DFID\"\\n\",\n\t\t\t       ll_get_fsname(dentry->d_sb, NULL, 0),\n\t\t\t       dentry, PFID(ll_inode2fid(inode)));\n\t\t\tlustre_dump_dentry(dentry, 1);\n\t\t\tdump_stack();\n\t\t}\n\n\t\td_lustre_invalidate(dentry, 0);\n\t}\n\tll_unlock_dcache(inode);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \t       inode->i_ino, inode->i_generation, inode);\n \n \tll_lock_dcache(inode);\n-\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {\n+\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {\n \t\tCDEBUG(D_DENTRY, \"dentry in drop %.*s (%p) parent %p \"\n \t\t       \"inode %p flags %d\\n\", dentry->d_name.len,\n \t\t       dentry->d_name.name, dentry, dentry->d_parent,",
        "diff_line_info": {
            "deleted_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_alias) {"
            ],
            "added_lines": [
                "\tll_d_hlist_for_each_entry(dentry, p, &inode->i_dentry, d_u.d_alias) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/lustre_dump_dentry",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=946e51f2bf37f1656916eb75bd0742ba33983c28",
        "commit_title": "",
        "commit_text": "",
        "func_before": "void lustre_dump_dentry(struct dentry *dentry, int recur)\n{\n\tstruct list_head *tmp;\n\tint subdirs = 0;\n\n\tLASSERT(dentry != NULL);\n\n\tlist_for_each(tmp, &dentry->d_subdirs)\n\t\tsubdirs++;\n\n\tCERROR(\"dentry %p dump: name=%.*s parent=%.*s (%p), inode=%p, count=%u,\"\n\t       \" flags=0x%x, fsdata=%p, %d subdirs\\n\", dentry,\n\t       dentry->d_name.len, dentry->d_name.name,\n\t       dentry->d_parent->d_name.len, dentry->d_parent->d_name.name,\n\t       dentry->d_parent, dentry->d_inode, d_count(dentry),\n\t       dentry->d_flags, dentry->d_fsdata, subdirs);\n\tif (dentry->d_inode != NULL)\n\t\tll_dump_inode(dentry->d_inode);\n\n\tif (recur == 0)\n\t\treturn;\n\n\tlist_for_each(tmp, &dentry->d_subdirs) {\n\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_u.d_child);\n\t\tlustre_dump_dentry(d, recur - 1);\n\t}\n}",
        "func": "void lustre_dump_dentry(struct dentry *dentry, int recur)\n{\n\tstruct list_head *tmp;\n\tint subdirs = 0;\n\n\tLASSERT(dentry != NULL);\n\n\tlist_for_each(tmp, &dentry->d_subdirs)\n\t\tsubdirs++;\n\n\tCERROR(\"dentry %p dump: name=%.*s parent=%.*s (%p), inode=%p, count=%u,\"\n\t       \" flags=0x%x, fsdata=%p, %d subdirs\\n\", dentry,\n\t       dentry->d_name.len, dentry->d_name.name,\n\t       dentry->d_parent->d_name.len, dentry->d_parent->d_name.name,\n\t       dentry->d_parent, dentry->d_inode, d_count(dentry),\n\t       dentry->d_flags, dentry->d_fsdata, subdirs);\n\tif (dentry->d_inode != NULL)\n\t\tll_dump_inode(dentry->d_inode);\n\n\tif (recur == 0)\n\t\treturn;\n\n\tlist_for_each(tmp, &dentry->d_subdirs) {\n\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_child);\n\t\tlustre_dump_dentry(d, recur - 1);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,7 @@\n \t\treturn;\n \n \tlist_for_each(tmp, &dentry->d_subdirs) {\n-\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_u.d_child);\n+\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_child);\n \t\tlustre_dump_dentry(d, recur - 1);\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_u.d_child);"
            ],
            "added_lines": [
                "\t\tstruct dentry *d = list_entry(tmp, struct dentry, d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/__dentry_kill",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=ca5358ef75fc69fee5322a38a340f5739d997c10",
        "commit_title": "... by not hitting rename_retry for reasons other than rename having",
        "commit_text": "happened.  In other words, do _not_ restart when finding that between unlocking the child and locking the parent the former got into __dentry_kill().  Skip the killed siblings instead...  ",
        "func_before": "static void __dentry_kill(struct dentry *dentry)\n{\n\tstruct dentry *parent = NULL;\n\tbool can_free = true;\n\tif (!IS_ROOT(dentry))\n\t\tparent = dentry->d_parent;\n\n\t/*\n\t * The dentry is now unrecoverably dead to the world.\n\t */\n\tlockref_mark_dead(&dentry->d_lockref);\n\n\t/*\n\t * inform the fs via d_prune that this dentry is about to be\n\t * unhashed and destroyed.\n\t */\n\tif (dentry->d_flags & DCACHE_OP_PRUNE)\n\t\tdentry->d_op->d_prune(dentry);\n\n\tif (dentry->d_flags & DCACHE_LRU_LIST) {\n\t\tif (!(dentry->d_flags & DCACHE_SHRINK_LIST))\n\t\t\td_lru_del(dentry);\n\t}\n\t/* if it was on the hash then remove it */\n\t__d_drop(dentry);\n\tlist_del(&dentry->d_child);\n\t/*\n\t * Inform d_walk() that we are no longer attached to the\n\t * dentry tree\n\t */\n\tdentry->d_flags |= DCACHE_DENTRY_KILLED;\n\tif (parent)\n\t\tspin_unlock(&parent->d_lock);\n\tdentry_iput(dentry);\n\t/*\n\t * dentry_iput drops the locks, at which point nobody (except\n\t * transient RCU lookups) can reach this dentry.\n\t */\n\tBUG_ON((int)dentry->d_lockref.count > 0);\n\tthis_cpu_dec(nr_dentry);\n\tif (dentry->d_op && dentry->d_op->d_release)\n\t\tdentry->d_op->d_release(dentry);\n\n\tspin_lock(&dentry->d_lock);\n\tif (dentry->d_flags & DCACHE_SHRINK_LIST) {\n\t\tdentry->d_flags |= DCACHE_MAY_FREE;\n\t\tcan_free = false;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\tif (likely(can_free))\n\t\tdentry_free(dentry);\n}",
        "func": "static void __dentry_kill(struct dentry *dentry)\n{\n\tstruct dentry *parent = NULL;\n\tbool can_free = true;\n\tif (!IS_ROOT(dentry))\n\t\tparent = dentry->d_parent;\n\n\t/*\n\t * The dentry is now unrecoverably dead to the world.\n\t */\n\tlockref_mark_dead(&dentry->d_lockref);\n\n\t/*\n\t * inform the fs via d_prune that this dentry is about to be\n\t * unhashed and destroyed.\n\t */\n\tif (dentry->d_flags & DCACHE_OP_PRUNE)\n\t\tdentry->d_op->d_prune(dentry);\n\n\tif (dentry->d_flags & DCACHE_LRU_LIST) {\n\t\tif (!(dentry->d_flags & DCACHE_SHRINK_LIST))\n\t\t\td_lru_del(dentry);\n\t}\n\t/* if it was on the hash then remove it */\n\t__d_drop(dentry);\n\t__list_del_entry(&dentry->d_child);\n\t/*\n\t * Inform d_walk() that we are no longer attached to the\n\t * dentry tree\n\t */\n\tdentry->d_flags |= DCACHE_DENTRY_KILLED;\n\tif (parent)\n\t\tspin_unlock(&parent->d_lock);\n\tdentry_iput(dentry);\n\t/*\n\t * dentry_iput drops the locks, at which point nobody (except\n\t * transient RCU lookups) can reach this dentry.\n\t */\n\tBUG_ON((int)dentry->d_lockref.count > 0);\n\tthis_cpu_dec(nr_dentry);\n\tif (dentry->d_op && dentry->d_op->d_release)\n\t\tdentry->d_op->d_release(dentry);\n\n\tspin_lock(&dentry->d_lock);\n\tif (dentry->d_flags & DCACHE_SHRINK_LIST) {\n\t\tdentry->d_flags |= DCACHE_MAY_FREE;\n\t\tcan_free = false;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\tif (likely(can_free))\n\t\tdentry_free(dentry);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,7 +23,7 @@\n \t}\n \t/* if it was on the hash then remove it */\n \t__d_drop(dentry);\n-\tlist_del(&dentry->d_child);\n+\t__list_del_entry(&dentry->d_child);\n \t/*\n \t * Inform d_walk() that we are no longer attached to the\n \t * dentry tree",
        "diff_line_info": {
            "deleted_lines": [
                "\tlist_del(&dentry->d_child);"
            ],
            "added_lines": [
                "\t__list_del_entry(&dentry->d_child);"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-8559",
        "func_name": "torvalds/linux/d_walk",
        "description": "The d_walk function in fs/dcache.c in the Linux kernel through 3.17.2 does not properly maintain the semantics of rename_lock, which allows local users to cause a denial of service (deadlock and system hang) via a crafted application.",
        "git_url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?h=ca5358ef75fc69fee5322a38a340f5739d997c10",
        "commit_title": "... by not hitting rename_retry for reasons other than rename having",
        "commit_text": "happened.  In other words, do _not_ restart when finding that between unlocking the child and locking the parent the former got into __dentry_kill().  Skip the killed siblings instead...  ",
        "func_before": "static void d_walk(struct dentry *parent, void *data,\n\t\t   enum d_walk_ret (*enter)(void *, struct dentry *),\n\t\t   void (*finish)(void *))\n{\n\tstruct dentry *this_parent;\n\tstruct list_head *next;\n\tunsigned seq = 0;\n\tenum d_walk_ret ret;\n\tbool retry = true;\n\nagain:\n\tread_seqbegin_or_lock(&rename_lock, &seq);\n\tthis_parent = parent;\n\tspin_lock(&this_parent->d_lock);\n\n\tret = enter(data, this_parent);\n\tswitch (ret) {\n\tcase D_WALK_CONTINUE:\n\t\tbreak;\n\tcase D_WALK_QUIT:\n\tcase D_WALK_SKIP:\n\t\tgoto out_unlock;\n\tcase D_WALK_NORETRY:\n\t\tretry = false;\n\t\tbreak;\n\t}\nrepeat:\n\tnext = this_parent->d_subdirs.next;\nresume:\n\twhile (next != &this_parent->d_subdirs) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct dentry *dentry = list_entry(tmp, struct dentry, d_child);\n\t\tnext = tmp->next;\n\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\n\t\tret = enter(data, dentry);\n\t\tswitch (ret) {\n\t\tcase D_WALK_CONTINUE:\n\t\t\tbreak;\n\t\tcase D_WALK_QUIT:\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tgoto out_unlock;\n\t\tcase D_WALK_NORETRY:\n\t\t\tretry = false;\n\t\t\tbreak;\n\t\tcase D_WALK_SKIP:\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!list_empty(&dentry->d_subdirs)) {\n\t\t\tspin_unlock(&this_parent->d_lock);\n\t\t\tspin_release(&dentry->d_lock.dep_map, 1, _RET_IP_);\n\t\t\tthis_parent = dentry;\n\t\t\tspin_acquire(&this_parent->d_lock.dep_map, 0, 1, _RET_IP_);\n\t\t\tgoto repeat;\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search.\n\t */\n\tif (this_parent != parent) {\n\t\tstruct dentry *child = this_parent;\n\t\tthis_parent = child->d_parent;\n\n\t\trcu_read_lock();\n\t\tspin_unlock(&child->d_lock);\n\t\tspin_lock(&this_parent->d_lock);\n\n\t\t/*\n\t\t * might go back up the wrong parent if we have had a rename\n\t\t * or deletion\n\t\t */\n\t\tif (this_parent != child->d_parent ||\n\t\t\t (child->d_flags & DCACHE_DENTRY_KILLED) ||\n\t\t\t need_seqretry(&rename_lock, seq)) {\n\t\t\tspin_unlock(&this_parent->d_lock);\n\t\t\trcu_read_unlock();\n\t\t\tgoto rename_retry;\n\t\t}\n\t\trcu_read_unlock();\n\t\tnext = child->d_child.next;\n\t\tgoto resume;\n\t}\n\tif (need_seqretry(&rename_lock, seq)) {\n\t\tspin_unlock(&this_parent->d_lock);\n\t\tgoto rename_retry;\n\t}\n\tif (finish)\n\t\tfinish(data);\n\nout_unlock:\n\tspin_unlock(&this_parent->d_lock);\n\tdone_seqretry(&rename_lock, seq);\n\treturn;\n\nrename_retry:\n\tif (!retry)\n\t\treturn;\n\tseq = 1;\n\tgoto again;\n}",
        "func": "static void d_walk(struct dentry *parent, void *data,\n\t\t   enum d_walk_ret (*enter)(void *, struct dentry *),\n\t\t   void (*finish)(void *))\n{\n\tstruct dentry *this_parent;\n\tstruct list_head *next;\n\tunsigned seq = 0;\n\tenum d_walk_ret ret;\n\tbool retry = true;\n\nagain:\n\tread_seqbegin_or_lock(&rename_lock, &seq);\n\tthis_parent = parent;\n\tspin_lock(&this_parent->d_lock);\n\n\tret = enter(data, this_parent);\n\tswitch (ret) {\n\tcase D_WALK_CONTINUE:\n\t\tbreak;\n\tcase D_WALK_QUIT:\n\tcase D_WALK_SKIP:\n\t\tgoto out_unlock;\n\tcase D_WALK_NORETRY:\n\t\tretry = false;\n\t\tbreak;\n\t}\nrepeat:\n\tnext = this_parent->d_subdirs.next;\nresume:\n\twhile (next != &this_parent->d_subdirs) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct dentry *dentry = list_entry(tmp, struct dentry, d_child);\n\t\tnext = tmp->next;\n\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\n\t\tret = enter(data, dentry);\n\t\tswitch (ret) {\n\t\tcase D_WALK_CONTINUE:\n\t\t\tbreak;\n\t\tcase D_WALK_QUIT:\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tgoto out_unlock;\n\t\tcase D_WALK_NORETRY:\n\t\t\tretry = false;\n\t\t\tbreak;\n\t\tcase D_WALK_SKIP:\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!list_empty(&dentry->d_subdirs)) {\n\t\t\tspin_unlock(&this_parent->d_lock);\n\t\t\tspin_release(&dentry->d_lock.dep_map, 1, _RET_IP_);\n\t\t\tthis_parent = dentry;\n\t\t\tspin_acquire(&this_parent->d_lock.dep_map, 0, 1, _RET_IP_);\n\t\t\tgoto repeat;\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search.\n\t */\n\trcu_read_lock();\nascend:\n\tif (this_parent != parent) {\n\t\tstruct dentry *child = this_parent;\n\t\tthis_parent = child->d_parent;\n\n\t\tspin_unlock(&child->d_lock);\n\t\tspin_lock(&this_parent->d_lock);\n\n\t\t/* might go back up the wrong parent if we have had a rename. */\n\t\tif (need_seqretry(&rename_lock, seq))\n\t\t\tgoto rename_retry;\n\t\tnext = child->d_child.next;\n\t\twhile (unlikely(child->d_flags & DCACHE_DENTRY_KILLED)) {\n\t\t\tif (next == &this_parent->d_subdirs)\n\t\t\t\tgoto ascend;\n\t\t\tchild = list_entry(next, struct dentry, d_child);\n\t\t\tnext = next->next;\n\t\t}\n\t\trcu_read_unlock();\n\t\tgoto resume;\n\t}\n\tif (need_seqretry(&rename_lock, seq))\n\t\tgoto rename_retry;\n\trcu_read_unlock();\n\tif (finish)\n\t\tfinish(data);\n\nout_unlock:\n\tspin_unlock(&this_parent->d_lock);\n\tdone_seqretry(&rename_lock, seq);\n\treturn;\n\nrename_retry:\n\tspin_unlock(&this_parent->d_lock);\n\trcu_read_unlock();\n\tBUG_ON(seq & 1);\n\tif (!retry)\n\t\treturn;\n\tseq = 1;\n\tgoto again;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,33 +61,31 @@\n \t/*\n \t * All done at this level ... ascend and resume the search.\n \t */\n+\trcu_read_lock();\n+ascend:\n \tif (this_parent != parent) {\n \t\tstruct dentry *child = this_parent;\n \t\tthis_parent = child->d_parent;\n \n-\t\trcu_read_lock();\n \t\tspin_unlock(&child->d_lock);\n \t\tspin_lock(&this_parent->d_lock);\n \n-\t\t/*\n-\t\t * might go back up the wrong parent if we have had a rename\n-\t\t * or deletion\n-\t\t */\n-\t\tif (this_parent != child->d_parent ||\n-\t\t\t (child->d_flags & DCACHE_DENTRY_KILLED) ||\n-\t\t\t need_seqretry(&rename_lock, seq)) {\n-\t\t\tspin_unlock(&this_parent->d_lock);\n-\t\t\trcu_read_unlock();\n+\t\t/* might go back up the wrong parent if we have had a rename. */\n+\t\tif (need_seqretry(&rename_lock, seq))\n \t\t\tgoto rename_retry;\n+\t\tnext = child->d_child.next;\n+\t\twhile (unlikely(child->d_flags & DCACHE_DENTRY_KILLED)) {\n+\t\t\tif (next == &this_parent->d_subdirs)\n+\t\t\t\tgoto ascend;\n+\t\t\tchild = list_entry(next, struct dentry, d_child);\n+\t\t\tnext = next->next;\n \t\t}\n \t\trcu_read_unlock();\n-\t\tnext = child->d_child.next;\n \t\tgoto resume;\n \t}\n-\tif (need_seqretry(&rename_lock, seq)) {\n-\t\tspin_unlock(&this_parent->d_lock);\n+\tif (need_seqretry(&rename_lock, seq))\n \t\tgoto rename_retry;\n-\t}\n+\trcu_read_unlock();\n \tif (finish)\n \t\tfinish(data);\n \n@@ -97,6 +95,9 @@\n \treturn;\n \n rename_retry:\n+\tspin_unlock(&this_parent->d_lock);\n+\trcu_read_unlock();\n+\tBUG_ON(seq & 1);\n \tif (!retry)\n \t\treturn;\n \tseq = 1;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\trcu_read_lock();",
                "\t\t/*",
                "\t\t * might go back up the wrong parent if we have had a rename",
                "\t\t * or deletion",
                "\t\t */",
                "\t\tif (this_parent != child->d_parent ||",
                "\t\t\t (child->d_flags & DCACHE_DENTRY_KILLED) ||",
                "\t\t\t need_seqretry(&rename_lock, seq)) {",
                "\t\t\tspin_unlock(&this_parent->d_lock);",
                "\t\t\trcu_read_unlock();",
                "\t\tnext = child->d_child.next;",
                "\tif (need_seqretry(&rename_lock, seq)) {",
                "\t\tspin_unlock(&this_parent->d_lock);",
                "\t}"
            ],
            "added_lines": [
                "\trcu_read_lock();",
                "ascend:",
                "\t\t/* might go back up the wrong parent if we have had a rename. */",
                "\t\tif (need_seqretry(&rename_lock, seq))",
                "\t\tnext = child->d_child.next;",
                "\t\twhile (unlikely(child->d_flags & DCACHE_DENTRY_KILLED)) {",
                "\t\t\tif (next == &this_parent->d_subdirs)",
                "\t\t\t\tgoto ascend;",
                "\t\t\tchild = list_entry(next, struct dentry, d_child);",
                "\t\t\tnext = next->next;",
                "\tif (need_seqretry(&rename_lock, seq))",
                "\trcu_read_unlock();",
                "\tspin_unlock(&this_parent->d_lock);",
                "\trcu_read_unlock();",
                "\tBUG_ON(seq & 1);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3698",
        "func_name": "torvalds/linux/svm_vcpu_run",
        "description": "The KVM implementation in the Linux kernel before 2.6.36 does not properly reload the FS and GS segment registers, which allows host OS users to cause a denial of service (host OS crash) via a KVM_RUN ioctl call in conjunction with a modified Local Descriptor Table (LDT).",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=9581d442b9058d3699b4be568b6e5eae38a41493",
        "commit_title": "kvm reloads the host's fs and gs blindly, however the underlying segment",
        "commit_text": "descriptors may be invalid due to the user modifying the ldt after loading them.  Fix by using the safe accessors (loadsegment() and load_gs_index()) instead of home grown unsafe versions.  This is CVE-2010-3698.  KVM-Stable-Tag. ",
        "func_before": "static void svm_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu16 fs_selector;\n\tu16 gs_selector;\n\tu16 ldt_selector;\n\n\tsvm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];\n\tsvm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];\n\tsvm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];\n\n\t/*\n\t * A vmexit emulation is required before the vcpu can be executed\n\t * again.\n\t */\n\tif (unlikely(svm->nested.exit_required))\n\t\treturn;\n\n\tpre_svm_run(svm);\n\n\tsync_lapic_to_cr8(vcpu);\n\n\tsave_host_msrs(vcpu);\n\tfs_selector = kvm_read_fs();\n\tgs_selector = kvm_read_gs();\n\tldt_selector = kvm_read_ldt();\n\tsvm->vmcb->save.cr2 = vcpu->arch.cr2;\n\t/* required for live migration with NPT */\n\tif (npt_enabled)\n\t\tsvm->vmcb->save.cr3 = vcpu->arch.cr3;\n\n\tclgi();\n\n\tlocal_irq_enable();\n\n\tasm volatile (\n\t\t\"push %%\"R\"bp; \\n\\t\"\n\t\t\"mov %c[rbx](%[svm]), %%\"R\"bx \\n\\t\"\n\t\t\"mov %c[rcx](%[svm]), %%\"R\"cx \\n\\t\"\n\t\t\"mov %c[rdx](%[svm]), %%\"R\"dx \\n\\t\"\n\t\t\"mov %c[rsi](%[svm]), %%\"R\"si \\n\\t\"\n\t\t\"mov %c[rdi](%[svm]), %%\"R\"di \\n\\t\"\n\t\t\"mov %c[rbp](%[svm]), %%\"R\"bp \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%[svm]),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%[svm]),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%[svm]), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%[svm]), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%[svm]), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%[svm]), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%[svm]), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%[svm]), %%r15 \\n\\t\"\n#endif\n\n\t\t/* Enter guest mode */\n\t\t\"push %%\"R\"ax \\n\\t\"\n\t\t\"mov %c[vmcb](%[svm]), %%\"R\"ax \\n\\t\"\n\t\t__ex(SVM_VMLOAD) \"\\n\\t\"\n\t\t__ex(SVM_VMRUN) \"\\n\\t\"\n\t\t__ex(SVM_VMSAVE) \"\\n\\t\"\n\t\t\"pop %%\"R\"ax \\n\\t\"\n\n\t\t/* Save guest registers, load host registers */\n\t\t\"mov %%\"R\"bx, %c[rbx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"cx, %c[rcx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"dx, %c[rdx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"si, %c[rsi](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"di, %c[rdi](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"bp, %c[rbp](%[svm]) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%[svm]) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%[svm]) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%[svm]) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%[svm]) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%[svm]) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%[svm]) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%[svm]) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%[svm]) \\n\\t\"\n#endif\n\t\t\"pop %%\"R\"bp\"\n\t\t:\n\t\t: [svm]\"a\"(svm),\n\t\t  [vmcb]\"i\"(offsetof(struct vcpu_svm, vmcb_pa)),\n\t\t  [rbx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t  [rcx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t  [rdx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t  [rsi]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t  [rdi]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t  [rbp]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))\n#ifdef CONFIG_X86_64\n\t\t  , [r8]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t  [r9]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t  [r10]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t  [r11]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t  [r12]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t  [r13]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t  [r14]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t  [r15]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))\n#endif\n\t\t: \"cc\", \"memory\"\n\t\t, R\"bx\", R\"cx\", R\"dx\", R\"si\", R\"di\"\n#ifdef CONFIG_X86_64\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\" , \"r12\", \"r13\", \"r14\", \"r15\"\n#endif\n\t\t);\n\n\tvcpu->arch.cr2 = svm->vmcb->save.cr2;\n\tvcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;\n\tvcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;\n\tvcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;\n\n\tkvm_load_fs(fs_selector);\n\tkvm_load_gs(gs_selector);\n\tkvm_load_ldt(ldt_selector);\n\tload_host_msrs(vcpu);\n\n\treload_tss(vcpu);\n\n\tlocal_irq_disable();\n\n\tstgi();\n\n\tsync_cr8_to_lapic(vcpu);\n\n\tsvm->next_rip = 0;\n\n\tif (npt_enabled) {\n\t\tvcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);\n\t\tvcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);\n\t}\n\n\t/*\n\t * We need to handle MC intercepts here before the vcpu has a chance to\n\t * change the physical cpu\n\t */\n\tif (unlikely(svm->vmcb->control.exit_code ==\n\t\t     SVM_EXIT_EXCP_BASE + MC_VECTOR))\n\t\tsvm_handle_mce(svm);\n}",
        "func": "static void svm_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu16 fs_selector;\n\tu16 gs_selector;\n\tu16 ldt_selector;\n\n\tsvm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];\n\tsvm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];\n\tsvm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];\n\n\t/*\n\t * A vmexit emulation is required before the vcpu can be executed\n\t * again.\n\t */\n\tif (unlikely(svm->nested.exit_required))\n\t\treturn;\n\n\tpre_svm_run(svm);\n\n\tsync_lapic_to_cr8(vcpu);\n\n\tsave_host_msrs(vcpu);\n\tsavesegment(fs, fs_selector);\n\tsavesegment(gs, gs_selector);\n\tldt_selector = kvm_read_ldt();\n\tsvm->vmcb->save.cr2 = vcpu->arch.cr2;\n\t/* required for live migration with NPT */\n\tif (npt_enabled)\n\t\tsvm->vmcb->save.cr3 = vcpu->arch.cr3;\n\n\tclgi();\n\n\tlocal_irq_enable();\n\n\tasm volatile (\n\t\t\"push %%\"R\"bp; \\n\\t\"\n\t\t\"mov %c[rbx](%[svm]), %%\"R\"bx \\n\\t\"\n\t\t\"mov %c[rcx](%[svm]), %%\"R\"cx \\n\\t\"\n\t\t\"mov %c[rdx](%[svm]), %%\"R\"dx \\n\\t\"\n\t\t\"mov %c[rsi](%[svm]), %%\"R\"si \\n\\t\"\n\t\t\"mov %c[rdi](%[svm]), %%\"R\"di \\n\\t\"\n\t\t\"mov %c[rbp](%[svm]), %%\"R\"bp \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%[svm]),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%[svm]),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%[svm]), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%[svm]), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%[svm]), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%[svm]), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%[svm]), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%[svm]), %%r15 \\n\\t\"\n#endif\n\n\t\t/* Enter guest mode */\n\t\t\"push %%\"R\"ax \\n\\t\"\n\t\t\"mov %c[vmcb](%[svm]), %%\"R\"ax \\n\\t\"\n\t\t__ex(SVM_VMLOAD) \"\\n\\t\"\n\t\t__ex(SVM_VMRUN) \"\\n\\t\"\n\t\t__ex(SVM_VMSAVE) \"\\n\\t\"\n\t\t\"pop %%\"R\"ax \\n\\t\"\n\n\t\t/* Save guest registers, load host registers */\n\t\t\"mov %%\"R\"bx, %c[rbx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"cx, %c[rcx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"dx, %c[rdx](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"si, %c[rsi](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"di, %c[rdi](%[svm]) \\n\\t\"\n\t\t\"mov %%\"R\"bp, %c[rbp](%[svm]) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%[svm]) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%[svm]) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%[svm]) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%[svm]) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%[svm]) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%[svm]) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%[svm]) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%[svm]) \\n\\t\"\n#endif\n\t\t\"pop %%\"R\"bp\"\n\t\t:\n\t\t: [svm]\"a\"(svm),\n\t\t  [vmcb]\"i\"(offsetof(struct vcpu_svm, vmcb_pa)),\n\t\t  [rbx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t  [rcx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t  [rdx]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t  [rsi]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t  [rdi]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t  [rbp]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_RBP]))\n#ifdef CONFIG_X86_64\n\t\t  , [r8]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t  [r9]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t  [r10]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t  [r11]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t  [r12]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t  [r13]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t  [r14]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t  [r15]\"i\"(offsetof(struct vcpu_svm, vcpu.arch.regs[VCPU_REGS_R15]))\n#endif\n\t\t: \"cc\", \"memory\"\n\t\t, R\"bx\", R\"cx\", R\"dx\", R\"si\", R\"di\"\n#ifdef CONFIG_X86_64\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\" , \"r12\", \"r13\", \"r14\", \"r15\"\n#endif\n\t\t);\n\n\tvcpu->arch.cr2 = svm->vmcb->save.cr2;\n\tvcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;\n\tvcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;\n\tvcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;\n\n\tload_host_msrs(vcpu);\n\tloadsegment(fs, fs_selector);\n#ifdef CONFIG_X86_64\n\tload_gs_index(gs_selector);\n\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);\n#else\n\tloadsegment(gs, gs_selector);\n#endif\n\tkvm_load_ldt(ldt_selector);\n\n\treload_tss(vcpu);\n\n\tlocal_irq_disable();\n\n\tstgi();\n\n\tsync_cr8_to_lapic(vcpu);\n\n\tsvm->next_rip = 0;\n\n\tif (npt_enabled) {\n\t\tvcpu->arch.regs_avail &= ~(1 << VCPU_EXREG_PDPTR);\n\t\tvcpu->arch.regs_dirty &= ~(1 << VCPU_EXREG_PDPTR);\n\t}\n\n\t/*\n\t * We need to handle MC intercepts here before the vcpu has a chance to\n\t * change the physical cpu\n\t */\n\tif (unlikely(svm->vmcb->control.exit_code ==\n\t\t     SVM_EXIT_EXCP_BASE + MC_VECTOR))\n\t\tsvm_handle_mce(svm);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,8 +21,8 @@\n \tsync_lapic_to_cr8(vcpu);\n \n \tsave_host_msrs(vcpu);\n-\tfs_selector = kvm_read_fs();\n-\tgs_selector = kvm_read_gs();\n+\tsavesegment(fs, fs_selector);\n+\tsavesegment(gs, gs_selector);\n \tldt_selector = kvm_read_ldt();\n \tsvm->vmcb->save.cr2 = vcpu->arch.cr2;\n \t/* required for live migration with NPT */\n@@ -109,10 +109,15 @@\n \tvcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;\n \tvcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;\n \n-\tkvm_load_fs(fs_selector);\n-\tkvm_load_gs(gs_selector);\n+\tload_host_msrs(vcpu);\n+\tloadsegment(fs, fs_selector);\n+#ifdef CONFIG_X86_64\n+\tload_gs_index(gs_selector);\n+\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);\n+#else\n+\tloadsegment(gs, gs_selector);\n+#endif\n \tkvm_load_ldt(ldt_selector);\n-\tload_host_msrs(vcpu);\n \n \treload_tss(vcpu);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tfs_selector = kvm_read_fs();",
                "\tgs_selector = kvm_read_gs();",
                "\tkvm_load_fs(fs_selector);",
                "\tkvm_load_gs(gs_selector);",
                "\tload_host_msrs(vcpu);"
            ],
            "added_lines": [
                "\tsavesegment(fs, fs_selector);",
                "\tsavesegment(gs, gs_selector);",
                "\tload_host_msrs(vcpu);",
                "\tloadsegment(fs, fs_selector);",
                "#ifdef CONFIG_X86_64",
                "\tload_gs_index(gs_selector);",
                "\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);",
                "#else",
                "\tloadsegment(gs, gs_selector);",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3698",
        "func_name": "torvalds/linux/vmx_save_host_state",
        "description": "The KVM implementation in the Linux kernel before 2.6.36 does not properly reload the FS and GS segment registers, which allows host OS users to cause a denial of service (host OS crash) via a KVM_RUN ioctl call in conjunction with a modified Local Descriptor Table (LDT).",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=9581d442b9058d3699b4be568b6e5eae38a41493",
        "commit_title": "kvm reloads the host's fs and gs blindly, however the underlying segment",
        "commit_text": "descriptors may be invalid due to the user modifying the ldt after loading them.  Fix by using the safe accessors (loadsegment() and load_gs_index()) instead of home grown unsafe versions.  This is CVE-2010-3698.  KVM-Stable-Tag. ",
        "func_before": "static void vmx_save_host_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint i;\n\n\tif (vmx->host_state.loaded)\n\t\treturn;\n\n\tvmx->host_state.loaded = 1;\n\t/*\n\t * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not\n\t * allow segment selectors with cpl > 0 or ti == 1.\n\t */\n\tvmx->host_state.ldt_sel = kvm_read_ldt();\n\tvmx->host_state.gs_ldt_reload_needed = vmx->host_state.ldt_sel;\n\tvmx->host_state.fs_sel = kvm_read_fs();\n\tif (!(vmx->host_state.fs_sel & 7)) {\n\t\tvmcs_write16(HOST_FS_SELECTOR, vmx->host_state.fs_sel);\n\t\tvmx->host_state.fs_reload_needed = 0;\n\t} else {\n\t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n\t\tvmx->host_state.fs_reload_needed = 1;\n\t}\n\tvmx->host_state.gs_sel = kvm_read_gs();\n\tif (!(vmx->host_state.gs_sel & 7))\n\t\tvmcs_write16(HOST_GS_SELECTOR, vmx->host_state.gs_sel);\n\telse {\n\t\tvmcs_write16(HOST_GS_SELECTOR, 0);\n\t\tvmx->host_state.gs_ldt_reload_needed = 1;\n\t}\n\n#ifdef CONFIG_X86_64\n\tvmcs_writel(HOST_FS_BASE, read_msr(MSR_FS_BASE));\n\tvmcs_writel(HOST_GS_BASE, read_msr(MSR_GS_BASE));\n#else\n\tvmcs_writel(HOST_FS_BASE, segment_base(vmx->host_state.fs_sel));\n\tvmcs_writel(HOST_GS_BASE, segment_base(vmx->host_state.gs_sel));\n#endif\n\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n\t}\n#endif\n\tfor (i = 0; i < vmx->save_nmsrs; ++i)\n\t\tkvm_set_shared_msr(vmx->guest_msrs[i].index,\n\t\t\t\t   vmx->guest_msrs[i].data,\n\t\t\t\t   vmx->guest_msrs[i].mask);\n}",
        "func": "static void vmx_save_host_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint i;\n\n\tif (vmx->host_state.loaded)\n\t\treturn;\n\n\tvmx->host_state.loaded = 1;\n\t/*\n\t * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not\n\t * allow segment selectors with cpl > 0 or ti == 1.\n\t */\n\tvmx->host_state.ldt_sel = kvm_read_ldt();\n\tvmx->host_state.gs_ldt_reload_needed = vmx->host_state.ldt_sel;\n\tsavesegment(fs, vmx->host_state.fs_sel);\n\tif (!(vmx->host_state.fs_sel & 7)) {\n\t\tvmcs_write16(HOST_FS_SELECTOR, vmx->host_state.fs_sel);\n\t\tvmx->host_state.fs_reload_needed = 0;\n\t} else {\n\t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n\t\tvmx->host_state.fs_reload_needed = 1;\n\t}\n\tsavesegment(gs, vmx->host_state.gs_sel);\n\tif (!(vmx->host_state.gs_sel & 7))\n\t\tvmcs_write16(HOST_GS_SELECTOR, vmx->host_state.gs_sel);\n\telse {\n\t\tvmcs_write16(HOST_GS_SELECTOR, 0);\n\t\tvmx->host_state.gs_ldt_reload_needed = 1;\n\t}\n\n#ifdef CONFIG_X86_64\n\tvmcs_writel(HOST_FS_BASE, read_msr(MSR_FS_BASE));\n\tvmcs_writel(HOST_GS_BASE, read_msr(MSR_GS_BASE));\n#else\n\tvmcs_writel(HOST_FS_BASE, segment_base(vmx->host_state.fs_sel));\n\tvmcs_writel(HOST_GS_BASE, segment_base(vmx->host_state.gs_sel));\n#endif\n\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n\t}\n#endif\n\tfor (i = 0; i < vmx->save_nmsrs; ++i)\n\t\tkvm_set_shared_msr(vmx->guest_msrs[i].index,\n\t\t\t\t   vmx->guest_msrs[i].data,\n\t\t\t\t   vmx->guest_msrs[i].mask);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \t */\n \tvmx->host_state.ldt_sel = kvm_read_ldt();\n \tvmx->host_state.gs_ldt_reload_needed = vmx->host_state.ldt_sel;\n-\tvmx->host_state.fs_sel = kvm_read_fs();\n+\tsavesegment(fs, vmx->host_state.fs_sel);\n \tif (!(vmx->host_state.fs_sel & 7)) {\n \t\tvmcs_write16(HOST_FS_SELECTOR, vmx->host_state.fs_sel);\n \t\tvmx->host_state.fs_reload_needed = 0;\n@@ -21,7 +21,7 @@\n \t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n \t\tvmx->host_state.fs_reload_needed = 1;\n \t}\n-\tvmx->host_state.gs_sel = kvm_read_gs();\n+\tsavesegment(gs, vmx->host_state.gs_sel);\n \tif (!(vmx->host_state.gs_sel & 7))\n \t\tvmcs_write16(HOST_GS_SELECTOR, vmx->host_state.gs_sel);\n \telse {",
        "diff_line_info": {
            "deleted_lines": [
                "\tvmx->host_state.fs_sel = kvm_read_fs();",
                "\tvmx->host_state.gs_sel = kvm_read_gs();"
            ],
            "added_lines": [
                "\tsavesegment(fs, vmx->host_state.fs_sel);",
                "\tsavesegment(gs, vmx->host_state.gs_sel);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3698",
        "func_name": "torvalds/linux/__vmx_load_host_state",
        "description": "The KVM implementation in the Linux kernel before 2.6.36 does not properly reload the FS and GS segment registers, which allows host OS users to cause a denial of service (host OS crash) via a KVM_RUN ioctl call in conjunction with a modified Local Descriptor Table (LDT).",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=9581d442b9058d3699b4be568b6e5eae38a41493",
        "commit_title": "kvm reloads the host's fs and gs blindly, however the underlying segment",
        "commit_text": "descriptors may be invalid due to the user modifying the ldt after loading them.  Fix by using the safe accessors (loadsegment() and load_gs_index()) instead of home grown unsafe versions.  This is CVE-2010-3698.  KVM-Stable-Tag. ",
        "func_before": "static void __vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tunsigned long flags;\n\n\tif (!vmx->host_state.loaded)\n\t\treturn;\n\n\t++vmx->vcpu.stat.host_state_reload;\n\tvmx->host_state.loaded = 0;\n\tif (vmx->host_state.fs_reload_needed)\n\t\tkvm_load_fs(vmx->host_state.fs_sel);\n\tif (vmx->host_state.gs_ldt_reload_needed) {\n\t\tkvm_load_ldt(vmx->host_state.ldt_sel);\n\t\t/*\n\t\t * If we have to reload gs, we must take care to\n\t\t * preserve our gs base.\n\t\t */\n\t\tlocal_irq_save(flags);\n\t\tkvm_load_gs(vmx->host_state.gs_sel);\n#ifdef CONFIG_X86_64\n\t\twrmsrl(MSR_GS_BASE, vmcs_readl(HOST_GS_BASE));\n#endif\n\t\tlocal_irq_restore(flags);\n\t}\n\treload_tss();\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n\t}\n#endif\n\tif (current_thread_info()->status & TS_USEDFPU)\n\t\tclts();\n\tload_gdt(&__get_cpu_var(host_gdt));\n}",
        "func": "static void __vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->host_state.loaded)\n\t\treturn;\n\n\t++vmx->vcpu.stat.host_state_reload;\n\tvmx->host_state.loaded = 0;\n\tif (vmx->host_state.fs_reload_needed)\n\t\tloadsegment(fs, vmx->host_state.fs_sel);\n\tif (vmx->host_state.gs_ldt_reload_needed) {\n\t\tkvm_load_ldt(vmx->host_state.ldt_sel);\n#ifdef CONFIG_X86_64\n\t\tload_gs_index(vmx->host_state.gs_sel);\n\t\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);\n#else\n\t\tloadsegment(gs, vmx->host_state.gs_sel);\n#endif\n\t}\n\treload_tss();\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n\t}\n#endif\n\tif (current_thread_info()->status & TS_USEDFPU)\n\t\tclts();\n\tload_gdt(&__get_cpu_var(host_gdt));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,26 +1,20 @@\n static void __vmx_load_host_state(struct vcpu_vmx *vmx)\n {\n-\tunsigned long flags;\n-\n \tif (!vmx->host_state.loaded)\n \t\treturn;\n \n \t++vmx->vcpu.stat.host_state_reload;\n \tvmx->host_state.loaded = 0;\n \tif (vmx->host_state.fs_reload_needed)\n-\t\tkvm_load_fs(vmx->host_state.fs_sel);\n+\t\tloadsegment(fs, vmx->host_state.fs_sel);\n \tif (vmx->host_state.gs_ldt_reload_needed) {\n \t\tkvm_load_ldt(vmx->host_state.ldt_sel);\n-\t\t/*\n-\t\t * If we have to reload gs, we must take care to\n-\t\t * preserve our gs base.\n-\t\t */\n-\t\tlocal_irq_save(flags);\n-\t\tkvm_load_gs(vmx->host_state.gs_sel);\n #ifdef CONFIG_X86_64\n-\t\twrmsrl(MSR_GS_BASE, vmcs_readl(HOST_GS_BASE));\n+\t\tload_gs_index(vmx->host_state.gs_sel);\n+\t\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);\n+#else\n+\t\tloadsegment(gs, vmx->host_state.gs_sel);\n #endif\n-\t\tlocal_irq_restore(flags);\n \t}\n \treload_tss();\n #ifdef CONFIG_X86_64",
        "diff_line_info": {
            "deleted_lines": [
                "\tunsigned long flags;",
                "",
                "\t\tkvm_load_fs(vmx->host_state.fs_sel);",
                "\t\t/*",
                "\t\t * If we have to reload gs, we must take care to",
                "\t\t * preserve our gs base.",
                "\t\t */",
                "\t\tlocal_irq_save(flags);",
                "\t\tkvm_load_gs(vmx->host_state.gs_sel);",
                "\t\twrmsrl(MSR_GS_BASE, vmcs_readl(HOST_GS_BASE));",
                "\t\tlocal_irq_restore(flags);"
            ],
            "added_lines": [
                "\t\tloadsegment(fs, vmx->host_state.fs_sel);",
                "\t\tload_gs_index(vmx->host_state.gs_sel);",
                "\t\twrmsrl(MSR_KERNEL_GS_BASE, current->thread.gs);",
                "#else",
                "\t\tloadsegment(gs, vmx->host_state.gs_sel);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3698",
        "func_name": "torvalds/linux/vmx_vcpu_setup",
        "description": "The KVM implementation in the Linux kernel before 2.6.36 does not properly reload the FS and GS segment registers, which allows host OS users to cause a denial of service (host OS crash) via a KVM_RUN ioctl call in conjunction with a modified Local Descriptor Table (LDT).",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=9581d442b9058d3699b4be568b6e5eae38a41493",
        "commit_title": "kvm reloads the host's fs and gs blindly, however the underlying segment",
        "commit_text": "descriptors may be invalid due to the user modifying the ldt after loading them.  Fix by using the safe accessors (loadsegment() and load_gs_index()) instead of home grown unsafe versions.  This is CVE-2010-3698.  KVM-Stable-Tag. ",
        "func_before": "static int vmx_vcpu_setup(struct vcpu_vmx *vmx)\n{\n\tu32 host_sysenter_cs, msr_low, msr_high;\n\tu32 junk;\n\tu64 host_pat, tsc_this, tsc_base;\n\tunsigned long a;\n\tstruct desc_ptr dt;\n\tint i;\n\tunsigned long kvm_vmx_return;\n\tu32 exec_control;\n\n\t/* I/O */\n\tvmcs_write64(IO_BITMAP_A, __pa(vmx_io_bitmap_a));\n\tvmcs_write64(IO_BITMAP_B, __pa(vmx_io_bitmap_b));\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx_msr_bitmap_legacy));\n\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */\n\n\t/* Control */\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL,\n\t\tvmcs_config.pin_based_exec_ctrl);\n\n\texec_control = vmcs_config.cpu_based_exec_ctrl;\n\tif (!vm_need_tpr_shadow(vmx->vcpu.kvm)) {\n\t\texec_control &= ~CPU_BASED_TPR_SHADOW;\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR8_LOAD_EXITING;\n#endif\n\t}\n\tif (!enable_ept)\n\t\texec_control |= CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR3_LOAD_EXITING  |\n\t\t\t\tCPU_BASED_INVLPG_EXITING;\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, exec_control);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\texec_control = vmcs_config.cpu_based_2nd_exec_ctrl;\n\t\tif (!vm_need_virtualize_apic_accesses(vmx->vcpu.kvm))\n\t\t\texec_control &=\n\t\t\t\t~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\t\tif (vmx->vpid == 0)\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_VPID;\n\t\tif (!enable_ept) {\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_EPT;\n\t\t\tenable_unrestricted_guest = 0;\n\t\t}\n\t\tif (!enable_unrestricted_guest)\n\t\t\texec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\t\tif (!ple_gap)\n\t\t\texec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);\n\t}\n\n\tif (ple_gap) {\n\t\tvmcs_write32(PLE_GAP, ple_gap);\n\t\tvmcs_write32(PLE_WINDOW, ple_window);\n\t}\n\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, !!bypass_guest_pf);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, !!bypass_guest_pf);\n\tvmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */\n\n\tvmcs_writel(HOST_CR0, read_cr0() | X86_CR0_TS);  /* 22.2.3 */\n\tvmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */\n\tvmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_FS_SELECTOR, kvm_read_fs());    /* 22.2.4 */\n\tvmcs_write16(HOST_GS_SELECTOR, kvm_read_gs());    /* 22.2.4 */\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\trdmsrl(MSR_FS_BASE, a);\n\tvmcs_writel(HOST_FS_BASE, a); /* 22.2.4 */\n\trdmsrl(MSR_GS_BASE, a);\n\tvmcs_writel(HOST_GS_BASE, a); /* 22.2.4 */\n#else\n\tvmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */\n\tvmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */\n#endif\n\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tnative_store_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\n\tasm(\"mov $.Lkvm_vmx_return, %0\" : \"=r\"(kvm_vmx_return));\n\tvmcs_writel(HOST_RIP, kvm_vmx_return); /* 22.2.5 */\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\trdmsr(MSR_IA32_SYSENTER_CS, host_sysenter_cs, junk);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, host_sysenter_cs);\n\trdmsrl(MSR_IA32_SYSENTER_ESP, a);\n\tvmcs_writel(HOST_IA32_SYSENTER_ESP, a);   /* 22.2.3 */\n\trdmsrl(MSR_IA32_SYSENTER_EIP, a);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, a);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, msr_low, msr_high);\n\t\thost_pat = msr_low | ((u64) msr_high << 32);\n\t\tvmcs_write64(HOST_IA32_PAT, host_pat);\n\t}\n\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, msr_low, msr_high);\n\t\thost_pat = msr_low | ((u64) msr_high << 32);\n\t\t/* Write the default value follow host pat */\n\t\tvmcs_write64(GUEST_IA32_PAT, host_pat);\n\t\t/* Keep arch.pat sync with GUEST_IA32_PAT */\n\t\tvmx->vcpu.arch.pat = host_pat;\n\t}\n\n\tfor (i = 0; i < NR_VMX_MSR; ++i) {\n\t\tu32 index = vmx_msr_index[i];\n\t\tu32 data_low, data_high;\n\t\tint j = vmx->nmsrs;\n\n\t\tif (rdmsr_safe(index, &data_low, &data_high) < 0)\n\t\t\tcontinue;\n\t\tif (wrmsr_safe(index, data_low, data_high) < 0)\n\t\t\tcontinue;\n\t\tvmx->guest_msrs[j].index = i;\n\t\tvmx->guest_msrs[j].data = 0;\n\t\tvmx->guest_msrs[j].mask = -1ull;\n\t\t++vmx->nmsrs;\n\t}\n\n\tvmcs_write32(VM_EXIT_CONTROLS, vmcs_config.vmexit_ctrl);\n\n\t/* 22.2.1, 20.8.1 */\n\tvmcs_write32(VM_ENTRY_CONTROLS, vmcs_config.vmentry_ctrl);\n\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);\n\tvmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;\n\tif (enable_ept)\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;\n\tvmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);\n\n\ttsc_base = vmx->vcpu.kvm->arch.vm_init_tsc;\n\trdtscll(tsc_this);\n\tif (tsc_this < vmx->vcpu.kvm->arch.vm_init_tsc)\n\t\ttsc_base = tsc_this;\n\n\tguest_write_tsc(0, tsc_base);\n\n\treturn 0;\n}",
        "func": "static int vmx_vcpu_setup(struct vcpu_vmx *vmx)\n{\n\tu32 host_sysenter_cs, msr_low, msr_high;\n\tu32 junk;\n\tu64 host_pat, tsc_this, tsc_base;\n\tunsigned long a;\n\tstruct desc_ptr dt;\n\tint i;\n\tunsigned long kvm_vmx_return;\n\tu32 exec_control;\n\n\t/* I/O */\n\tvmcs_write64(IO_BITMAP_A, __pa(vmx_io_bitmap_a));\n\tvmcs_write64(IO_BITMAP_B, __pa(vmx_io_bitmap_b));\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx_msr_bitmap_legacy));\n\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */\n\n\t/* Control */\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL,\n\t\tvmcs_config.pin_based_exec_ctrl);\n\n\texec_control = vmcs_config.cpu_based_exec_ctrl;\n\tif (!vm_need_tpr_shadow(vmx->vcpu.kvm)) {\n\t\texec_control &= ~CPU_BASED_TPR_SHADOW;\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR8_LOAD_EXITING;\n#endif\n\t}\n\tif (!enable_ept)\n\t\texec_control |= CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR3_LOAD_EXITING  |\n\t\t\t\tCPU_BASED_INVLPG_EXITING;\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, exec_control);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\texec_control = vmcs_config.cpu_based_2nd_exec_ctrl;\n\t\tif (!vm_need_virtualize_apic_accesses(vmx->vcpu.kvm))\n\t\t\texec_control &=\n\t\t\t\t~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\t\tif (vmx->vpid == 0)\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_VPID;\n\t\tif (!enable_ept) {\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_EPT;\n\t\t\tenable_unrestricted_guest = 0;\n\t\t}\n\t\tif (!enable_unrestricted_guest)\n\t\t\texec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\t\tif (!ple_gap)\n\t\t\texec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);\n\t}\n\n\tif (ple_gap) {\n\t\tvmcs_write32(PLE_GAP, ple_gap);\n\t\tvmcs_write32(PLE_WINDOW, ple_window);\n\t}\n\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, !!bypass_guest_pf);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, !!bypass_guest_pf);\n\tvmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */\n\n\tvmcs_writel(HOST_CR0, read_cr0() | X86_CR0_TS);  /* 22.2.3 */\n\tvmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */\n\tvmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */\n\tvmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\trdmsrl(MSR_FS_BASE, a);\n\tvmcs_writel(HOST_FS_BASE, a); /* 22.2.4 */\n\trdmsrl(MSR_GS_BASE, a);\n\tvmcs_writel(HOST_GS_BASE, a); /* 22.2.4 */\n#else\n\tvmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */\n\tvmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */\n#endif\n\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tnative_store_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\n\tasm(\"mov $.Lkvm_vmx_return, %0\" : \"=r\"(kvm_vmx_return));\n\tvmcs_writel(HOST_RIP, kvm_vmx_return); /* 22.2.5 */\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\trdmsr(MSR_IA32_SYSENTER_CS, host_sysenter_cs, junk);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, host_sysenter_cs);\n\trdmsrl(MSR_IA32_SYSENTER_ESP, a);\n\tvmcs_writel(HOST_IA32_SYSENTER_ESP, a);   /* 22.2.3 */\n\trdmsrl(MSR_IA32_SYSENTER_EIP, a);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, a);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, msr_low, msr_high);\n\t\thost_pat = msr_low | ((u64) msr_high << 32);\n\t\tvmcs_write64(HOST_IA32_PAT, host_pat);\n\t}\n\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, msr_low, msr_high);\n\t\thost_pat = msr_low | ((u64) msr_high << 32);\n\t\t/* Write the default value follow host pat */\n\t\tvmcs_write64(GUEST_IA32_PAT, host_pat);\n\t\t/* Keep arch.pat sync with GUEST_IA32_PAT */\n\t\tvmx->vcpu.arch.pat = host_pat;\n\t}\n\n\tfor (i = 0; i < NR_VMX_MSR; ++i) {\n\t\tu32 index = vmx_msr_index[i];\n\t\tu32 data_low, data_high;\n\t\tint j = vmx->nmsrs;\n\n\t\tif (rdmsr_safe(index, &data_low, &data_high) < 0)\n\t\t\tcontinue;\n\t\tif (wrmsr_safe(index, data_low, data_high) < 0)\n\t\t\tcontinue;\n\t\tvmx->guest_msrs[j].index = i;\n\t\tvmx->guest_msrs[j].data = 0;\n\t\tvmx->guest_msrs[j].mask = -1ull;\n\t\t++vmx->nmsrs;\n\t}\n\n\tvmcs_write32(VM_EXIT_CONTROLS, vmcs_config.vmexit_ctrl);\n\n\t/* 22.2.1, 20.8.1 */\n\tvmcs_write32(VM_ENTRY_CONTROLS, vmcs_config.vmentry_ctrl);\n\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~0UL);\n\tvmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;\n\tif (enable_ept)\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;\n\tvmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);\n\n\ttsc_base = vmx->vcpu.kvm->arch.vm_init_tsc;\n\trdtscll(tsc_this);\n\tif (tsc_this < vmx->vcpu.kvm->arch.vm_init_tsc)\n\t\ttsc_base = tsc_this;\n\n\tguest_write_tsc(0, tsc_base);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -70,8 +70,8 @@\n \tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n \tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n \tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n-\tvmcs_write16(HOST_FS_SELECTOR, kvm_read_fs());    /* 22.2.4 */\n-\tvmcs_write16(HOST_GS_SELECTOR, kvm_read_gs());    /* 22.2.4 */\n+\tvmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */\n+\tvmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */\n \tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n #ifdef CONFIG_X86_64\n \trdmsrl(MSR_FS_BASE, a);",
        "diff_line_info": {
            "deleted_lines": [
                "\tvmcs_write16(HOST_FS_SELECTOR, kvm_read_fs());    /* 22.2.4 */",
                "\tvmcs_write16(HOST_GS_SELECTOR, kvm_read_gs());    /* 22.2.4 */"
            ],
            "added_lines": [
                "\tvmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */",
                "\tvmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3705",
        "func_name": "kernel/git/netdev/net/sctp_auth_asoc_get_hmac",
        "description": "The sctp_auth_asoc_get_hmac function in net/sctp/auth.c in the Linux kernel before 2.6.36 does not properly validate the hmac_ids array of an SCTP peer, which allows remote attackers to cause a denial of service (memory corruption and panic) via a crafted value in the last element of this array.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/davem/net.git;a=commit;h=51e97a12bef19b7e43199fc153cf9bd5f2140362",
        "commit_title": "The sctp_asoc_get_hmac() function iterates through a peer's hmac_ids",
        "commit_text": "array and attempts to ensure that only a supported hmac entry is returned.  The current code fails to do this properly - if the last id in the array is out of range (greater than SCTP_AUTH_HMAC_ID_MAX), the id integer remains set after exiting the loop, and the address of an out-of-bounds entry will be returned and subsequently used in the parent function, causing potentially ugly memory corruption.  This patch resets the id integer to 0 on encountering an invalid id so that NULL will be returned after finishing the loop if no valid ids are found.  ",
        "func_before": "struct sctp_hmac *sctp_auth_asoc_get_hmac(const struct sctp_association *asoc)\n{\n\tstruct sctp_hmac_algo_param *hmacs;\n\t__u16 n_elt;\n\t__u16 id = 0;\n\tint i;\n\n\t/* If we have a default entry, use it */\n\tif (asoc->default_hmac_id)\n\t\treturn &sctp_hmac_list[asoc->default_hmac_id];\n\n\t/* Since we do not have a default entry, find the first entry\n\t * we support and return that.  Do not cache that id.\n\t */\n\thmacs = asoc->peer.peer_hmacs;\n\tif (!hmacs)\n\t\treturn NULL;\n\n\tn_elt = (ntohs(hmacs->param_hdr.length) - sizeof(sctp_paramhdr_t)) >> 1;\n\tfor (i = 0; i < n_elt; i++) {\n\t\tid = ntohs(hmacs->hmac_ids[i]);\n\n\t\t/* Check the id is in the supported range */\n\t\tif (id > SCTP_AUTH_HMAC_ID_MAX)\n\t\t\tcontinue;\n\n\t\t/* See is we support the id.  Supported IDs have name and\n\t\t * length fields set, so that we can allocated and use\n\t\t * them.  We can safely just check for name, for without the\n\t\t * name, we can't allocate the TFM.\n\t\t */\n\t\tif (!sctp_hmac_list[id].hmac_name)\n\t\t\tcontinue;\n\n\t\tbreak;\n\t}\n\n\tif (id == 0)\n\t\treturn NULL;\n\n\treturn &sctp_hmac_list[id];\n}",
        "func": "struct sctp_hmac *sctp_auth_asoc_get_hmac(const struct sctp_association *asoc)\n{\n\tstruct sctp_hmac_algo_param *hmacs;\n\t__u16 n_elt;\n\t__u16 id = 0;\n\tint i;\n\n\t/* If we have a default entry, use it */\n\tif (asoc->default_hmac_id)\n\t\treturn &sctp_hmac_list[asoc->default_hmac_id];\n\n\t/* Since we do not have a default entry, find the first entry\n\t * we support and return that.  Do not cache that id.\n\t */\n\thmacs = asoc->peer.peer_hmacs;\n\tif (!hmacs)\n\t\treturn NULL;\n\n\tn_elt = (ntohs(hmacs->param_hdr.length) - sizeof(sctp_paramhdr_t)) >> 1;\n\tfor (i = 0; i < n_elt; i++) {\n\t\tid = ntohs(hmacs->hmac_ids[i]);\n\n\t\t/* Check the id is in the supported range */\n\t\tif (id > SCTP_AUTH_HMAC_ID_MAX) {\n\t\t\tid = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* See is we support the id.  Supported IDs have name and\n\t\t * length fields set, so that we can allocated and use\n\t\t * them.  We can safely just check for name, for without the\n\t\t * name, we can't allocate the TFM.\n\t\t */\n\t\tif (!sctp_hmac_list[id].hmac_name) {\n\t\t\tid = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbreak;\n\t}\n\n\tif (id == 0)\n\t\treturn NULL;\n\n\treturn &sctp_hmac_list[id];\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,16 +21,20 @@\n \t\tid = ntohs(hmacs->hmac_ids[i]);\n \n \t\t/* Check the id is in the supported range */\n-\t\tif (id > SCTP_AUTH_HMAC_ID_MAX)\n+\t\tif (id > SCTP_AUTH_HMAC_ID_MAX) {\n+\t\t\tid = 0;\n \t\t\tcontinue;\n+\t\t}\n \n \t\t/* See is we support the id.  Supported IDs have name and\n \t\t * length fields set, so that we can allocated and use\n \t\t * them.  We can safely just check for name, for without the\n \t\t * name, we can't allocate the TFM.\n \t\t */\n-\t\tif (!sctp_hmac_list[id].hmac_name)\n+\t\tif (!sctp_hmac_list[id].hmac_name) {\n+\t\t\tid = 0;\n \t\t\tcontinue;\n+\t\t}\n \n \t\tbreak;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (id > SCTP_AUTH_HMAC_ID_MAX)",
                "\t\tif (!sctp_hmac_list[id].hmac_name)"
            ],
            "added_lines": [
                "\t\tif (id > SCTP_AUTH_HMAC_ID_MAX) {",
                "\t\t\tid = 0;",
                "\t\t}",
                "\t\tif (!sctp_hmac_list[id].hmac_name) {",
                "\t\t\tid = 0;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4249",
        "func_name": "kernel/git/netdev/net/wait_for_unix_gc",
        "description": "The wait_for_unix_gc function in net/unix/garbage.c in the Linux kernel before 2.6.37-rc3-next-20101125 does not properly select times for garbage collection of inflight sockets, which allows local users to cause a denial of service (system hang) via crafted use of the socketpair and sendmsg system calls for SOCK_SEQPACKET sockets.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/davem/net.git;a=commit;h=9915672d41273f5b77f1b3c29b391ffb7732b84b",
        "commit_title": "Vegard Nossum found a unix socket OOM was possible, posting an exploit",
        "commit_text": "program.  My analysis is we can eat all LOWMEM memory before unix_gc() being called from unix_release_sock(). Moreover, the thread blocked in unix_gc() can consume huge amount of time to perform cleanup because of huge working set.  One way to handle this is to have a sensible limit on unix_tot_inflight, tested from wait_for_unix_gc() and to force a call to unix_gc() if this limit is hit.  This solves the OOM and also reduce overall latencies, and should not slowdown normal workloads.  ",
        "func_before": "void wait_for_unix_gc(void)\n{\n\twait_event(unix_gc_wait, gc_in_progress == false);\n}",
        "func": "void wait_for_unix_gc(void)\n{\n\t/*\n\t * If number of inflight sockets is insane,\n\t * force a garbage collect right now.\n\t */\n\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)\n\t\tunix_gc();\n\twait_event(unix_gc_wait, gc_in_progress == false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,10 @@\n void wait_for_unix_gc(void)\n {\n+\t/*\n+\t * If number of inflight sockets is insane,\n+\t * force a garbage collect right now.\n+\t */\n+\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)\n+\t\tunix_gc();\n \twait_event(unix_gc_wait, gc_in_progress == false);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/*",
                "\t * If number of inflight sockets is insane,",
                "\t * force a garbage collect right now.",
                "\t */",
                "\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)",
                "\t\tunix_gc();"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-3858",
        "func_name": "torvalds/linux/setup_arg_pages",
        "description": "The setup_arg_pages function in fs/exec.c in the Linux kernel before 2.6.36, when CONFIG_STACK_GROWSDOWN is used, does not properly restrict the stack memory consumption of the (1) arguments and (2) environment for a 32-bit application on a 64-bit platform, which allows local users to cause a denial of service (system crash) via a crafted exec system call, a related issue to CVE-2010-2240.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=1b528181b2ffa14721fb28ad1bd539fe1732c583",
        "commit_title": "The CONFIG_STACK_GROWSDOWN variant of setup_arg_pages() does not",
        "commit_text": "check the size of the argument/environment area on the stack. When it is unworkably large, shift_arg_pages() hits its BUG_ON. This is exploitable with a very large RLIMIT_STACK limit, to create a crash pretty easily.  Check that the initial stack is not too large to make it possible to map in any executable.  We're not checking that the actual executable (or intepreter, for binfmt_elf) will fit.  So those mappings might clobber part of the initial stack mapping.  But that is just userland lossage that userland made happen, not a kernel problem.  ",
        "func_before": "int setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size to 1GB */\n\tstack_base = rlimit_max(RLIMIT_STACK);\n\tif (stack_base > (1 << 30))\n\t\tstack_base = 1 << 30;\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tdown_write(&mm->mmap_sem);\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = rlimit(RLIMIT_STACK) & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}",
        "func": "int setup_arg_pages(struct linux_binprm *bprm,\n\t\t    unsigned long stack_top,\n\t\t    int executable_stack)\n{\n\tunsigned long ret;\n\tunsigned long stack_shift;\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = bprm->vma;\n\tstruct vm_area_struct *prev = NULL;\n\tunsigned long vm_flags;\n\tunsigned long stack_base;\n\tunsigned long stack_size;\n\tunsigned long stack_expand;\n\tunsigned long rlim_stack;\n\n#ifdef CONFIG_STACK_GROWSUP\n\t/* Limit stack size to 1GB */\n\tstack_base = rlimit_max(RLIMIT_STACK);\n\tif (stack_base > (1 << 30))\n\t\tstack_base = 1 << 30;\n\n\t/* Make sure we didn't let the argument array grow too large. */\n\tif (vma->vm_end - vma->vm_start > stack_base)\n\t\treturn -ENOMEM;\n\n\tstack_base = PAGE_ALIGN(stack_top - stack_base);\n\n\tstack_shift = vma->vm_start - stack_base;\n\tmm->arg_start = bprm->p - stack_shift;\n\tbprm->p = vma->vm_end - stack_shift;\n#else\n\tstack_top = arch_align_stack(stack_top);\n\tstack_top = PAGE_ALIGN(stack_top);\n\n\tif (unlikely(stack_top < mmap_min_addr) ||\n\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n\t\treturn -ENOMEM;\n\n\tstack_shift = vma->vm_end - stack_top;\n\n\tbprm->p -= stack_shift;\n\tmm->arg_start = bprm->p;\n#endif\n\n\tif (bprm->loader)\n\t\tbprm->loader -= stack_shift;\n\tbprm->exec -= stack_shift;\n\n\tdown_write(&mm->mmap_sem);\n\tvm_flags = VM_STACK_FLAGS;\n\n\t/*\n\t * Adjust stack execute permissions; explicitly enable for\n\t * EXSTACK_ENABLE_X, disable for EXSTACK_DISABLE_X and leave alone\n\t * (arch default) otherwise.\n\t */\n\tif (unlikely(executable_stack == EXSTACK_ENABLE_X))\n\t\tvm_flags |= VM_EXEC;\n\telse if (executable_stack == EXSTACK_DISABLE_X)\n\t\tvm_flags &= ~VM_EXEC;\n\tvm_flags |= mm->def_flags;\n\tvm_flags |= VM_STACK_INCOMPLETE_SETUP;\n\n\tret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,\n\t\t\tvm_flags);\n\tif (ret)\n\t\tgoto out_unlock;\n\tBUG_ON(prev != vma);\n\n\t/* Move stack pages down in memory. */\n\tif (stack_shift) {\n\t\tret = shift_arg_pages(vma, stack_shift);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* mprotect_fixup is overkill to remove the temporary stack flags */\n\tvma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;\n\n\tstack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */\n\tstack_size = vma->vm_end - vma->vm_start;\n\t/*\n\t * Align this down to a page boundary as expand_stack\n\t * will align it up.\n\t */\n\trlim_stack = rlimit(RLIMIT_STACK) & PAGE_MASK;\n#ifdef CONFIG_STACK_GROWSUP\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_start + rlim_stack;\n\telse\n\t\tstack_base = vma->vm_end + stack_expand;\n#else\n\tif (stack_size + stack_expand > rlim_stack)\n\t\tstack_base = vma->vm_end - rlim_stack;\n\telse\n\t\tstack_base = vma->vm_start - stack_expand;\n#endif\n\tcurrent->mm->start_stack = bprm->p;\n\tret = expand_stack(vma, stack_base);\n\tif (ret)\n\t\tret = -EFAULT;\n\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,6 +31,11 @@\n #else\n \tstack_top = arch_align_stack(stack_top);\n \tstack_top = PAGE_ALIGN(stack_top);\n+\n+\tif (unlikely(stack_top < mmap_min_addr) ||\n+\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))\n+\t\treturn -ENOMEM;\n+\n \tstack_shift = vma->vm_end - stack_top;\n \n \tbprm->p -= stack_shift;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (unlikely(stack_top < mmap_min_addr) ||",
                "\t    unlikely(vma->vm_end - vma->vm_start >= stack_top - mmap_min_addr))",
                "\t\treturn -ENOMEM;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4668",
        "func_name": "torvalds/linux/blk_rq_map_user_iov",
        "description": "The blk_rq_map_user_iov function in block/blk-map.c in the Linux kernel before 2.6.37-rc7 allows local users to cause a denial of service (panic) via a zero-length I/O request in a device ioctl to a SCSI device, related to an unaligned map.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4163.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=5478755616ae2ef1ce144dded589b62b2a50d575",
        "commit_title": "commit 9284bcf checks for proper length of iov entries in",
        "commit_text": "blk_rq_map_user_iov(). But if the map is unaligned, kernel will break out the loop without checking for the proper length. So we need to check the proper length before the unalign check.  Cc: stable@kernel.org ",
        "func_before": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data, struct sg_iovec *iov,\n\t\t\tint iov_count, unsigned int len, gfp_t gfp_mask)\n{\n\tstruct bio *bio;\n\tint i, read = rq_data_dir(rq) == READ;\n\tint unaligned = 0;\n\n\tif (!iov || iov_count <= 0)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < iov_count; i++) {\n\t\tunsigned long uaddr = (unsigned long)iov[i].iov_base;\n\n\t\tif (uaddr & queue_dma_alignment(q)) {\n\t\t\tunaligned = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (!iov[i].iov_len)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (unaligned || (q->dma_pad_mask & len) || map_data)\n\t\tbio = bio_copy_user_iov(q, map_data, iov, iov_count, read,\n\t\t\t\t\tgfp_mask);\n\telse\n\t\tbio = bio_map_user_iov(q, NULL, iov, iov_count, read, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (bio->bi_size != len) {\n\t\t/*\n\t\t * Grab an extra reference to this bio, as bio_unmap_user()\n\t\t * expects to be able to drop it twice as it happens on the\n\t\t * normal IO completion path\n\t\t */\n\t\tbio_get(bio);\n\t\tbio_endio(bio, 0);\n\t\t__blk_rq_unmap_user(bio);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\n\tblk_queue_bounce(q, &bio);\n\tbio_get(bio);\n\tblk_rq_bio_prep(q, rq, bio);\n\trq->buffer = NULL;\n\treturn 0;\n}",
        "func": "int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\n\t\t\tstruct rq_map_data *map_data, struct sg_iovec *iov,\n\t\t\tint iov_count, unsigned int len, gfp_t gfp_mask)\n{\n\tstruct bio *bio;\n\tint i, read = rq_data_dir(rq) == READ;\n\tint unaligned = 0;\n\n\tif (!iov || iov_count <= 0)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < iov_count; i++) {\n\t\tunsigned long uaddr = (unsigned long)iov[i].iov_base;\n\n\t\tif (!iov[i].iov_len)\n\t\t\treturn -EINVAL;\n\n\t\tif (uaddr & queue_dma_alignment(q)) {\n\t\t\tunaligned = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (unaligned || (q->dma_pad_mask & len) || map_data)\n\t\tbio = bio_copy_user_iov(q, map_data, iov, iov_count, read,\n\t\t\t\t\tgfp_mask);\n\telse\n\t\tbio = bio_map_user_iov(q, NULL, iov, iov_count, read, gfp_mask);\n\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\tif (bio->bi_size != len) {\n\t\t/*\n\t\t * Grab an extra reference to this bio, as bio_unmap_user()\n\t\t * expects to be able to drop it twice as it happens on the\n\t\t * normal IO completion path\n\t\t */\n\t\tbio_get(bio);\n\t\tbio_endio(bio, 0);\n\t\t__blk_rq_unmap_user(bio);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!bio_flagged(bio, BIO_USER_MAPPED))\n\t\trq->cmd_flags |= REQ_COPY_USER;\n\n\tblk_queue_bounce(q, &bio);\n\tbio_get(bio);\n\tblk_rq_bio_prep(q, rq, bio);\n\trq->buffer = NULL;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,12 +12,13 @@\n \tfor (i = 0; i < iov_count; i++) {\n \t\tunsigned long uaddr = (unsigned long)iov[i].iov_base;\n \n+\t\tif (!iov[i].iov_len)\n+\t\t\treturn -EINVAL;\n+\n \t\tif (uaddr & queue_dma_alignment(q)) {\n \t\t\tunaligned = 1;\n \t\t\tbreak;\n \t\t}\n-\t\tif (!iov[i].iov_len)\n-\t\t\treturn -EINVAL;\n \t}\n \n \tif (unaligned || (q->dma_pad_mask & len) || map_data)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (!iov[i].iov_len)",
                "\t\t\treturn -EINVAL;"
            ],
            "added_lines": [
                "\t\tif (!iov[i].iov_len)",
                "\t\t\treturn -EINVAL;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4243",
        "func_name": "torvalds/linux/get_arg_page",
        "description": "fs/exec.c in the Linux kernel before 2.6.37 does not enable the OOM Killer to assess use of stack memory by arrays representing the (1) arguments and (2) environment, which allows local users to cause a denial of service (memory consumption) via a crafted exec system call, aka an \"OOM dodging issue,\" a related issue to CVE-2010-3858.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=3c77f845722158206a7209c45ccddc264d19319c",
        "commit_title": "Brad Spengler published a local memory-allocation DoS that",
        "commit_text": "evades the OOM-killer (though not the virtual memory RLIMIT): http://www.grsecurity.net/~spender/64bit_dos.c  execve()->copy_strings() can allocate a lot of memory, but this is not visible to oom-killer, nobody can see the nascent bprm->mm and take it into account.  With this patch get_arg_page() increments current's MM_ANONPAGES counter every time we allocate the new page for argv/envp. When do_execve() succeds or fails, we change this counter back.  Technically this is not 100% correct, we can't know if the new page is swapped out and turn MM_ANONPAGES into MM_SWAPENTS, but I don't think this really matters and everything becomes correct once exec changes ->mm or fails.  Reviewed-and-discussed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: stable@kernel.org ",
        "func_before": "static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_stack_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\tret = get_user_pages(current, bprm->mm, pos,\n\t\t\t1, write, 1, &page, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write) {\n\t\tunsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;\n\t\tstruct rlimit *rlim;\n\n\t\t/*\n\t\t * We've historically supported up to 32 pages (ARG_MAX)\n\t\t * of argument strings even with small stacks\n\t\t */\n\t\tif (size <= ARG_MAX)\n\t\t\treturn page;\n\n\t\t/*\n\t\t * Limit to 1/4-th the stack size for the argv+env strings.\n\t\t * This ensures that:\n\t\t *  - the remaining binfmt code will not run out of stack space,\n\t\t *  - the program will have a reasonable amount of stack left\n\t\t *    to work from.\n\t\t */\n\t\trlim = current->signal->rlim;\n\t\tif (size > ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4) {\n\t\t\tput_page(page);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn page;\n}",
        "func": "static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,\n\t\tint write)\n{\n\tstruct page *page;\n\tint ret;\n\n#ifdef CONFIG_STACK_GROWSUP\n\tif (write) {\n\t\tret = expand_stack_downwards(bprm->vma, pos);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n#endif\n\tret = get_user_pages(current, bprm->mm, pos,\n\t\t\t1, write, 1, &page, NULL);\n\tif (ret <= 0)\n\t\treturn NULL;\n\n\tif (write) {\n\t\tunsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;\n\t\tstruct rlimit *rlim;\n\n\t\tacct_arg_size(bprm, size / PAGE_SIZE);\n\n\t\t/*\n\t\t * We've historically supported up to 32 pages (ARG_MAX)\n\t\t * of argument strings even with small stacks\n\t\t */\n\t\tif (size <= ARG_MAX)\n\t\t\treturn page;\n\n\t\t/*\n\t\t * Limit to 1/4-th the stack size for the argv+env strings.\n\t\t * This ensures that:\n\t\t *  - the remaining binfmt code will not run out of stack space,\n\t\t *  - the program will have a reasonable amount of stack left\n\t\t *    to work from.\n\t\t */\n\t\trlim = current->signal->rlim;\n\t\tif (size > ACCESS_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4) {\n\t\t\tput_page(page);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn page;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,6 +19,8 @@\n \tif (write) {\n \t\tunsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;\n \t\tstruct rlimit *rlim;\n+\n+\t\tacct_arg_size(bprm, size / PAGE_SIZE);\n \n \t\t/*\n \t\t * We've historically supported up to 32 pages (ARG_MAX)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t\tacct_arg_size(bprm, size / PAGE_SIZE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4243",
        "func_name": "torvalds/linux/flush_old_exec",
        "description": "fs/exec.c in the Linux kernel before 2.6.37 does not enable the OOM Killer to assess use of stack memory by arrays representing the (1) arguments and (2) environment, which allows local users to cause a denial of service (memory consumption) via a crafted exec system call, aka an \"OOM dodging issue,\" a related issue to CVE-2010-3858.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=3c77f845722158206a7209c45ccddc264d19319c",
        "commit_title": "Brad Spengler published a local memory-allocation DoS that",
        "commit_text": "evades the OOM-killer (though not the virtual memory RLIMIT): http://www.grsecurity.net/~spender/64bit_dos.c  execve()->copy_strings() can allocate a lot of memory, but this is not visible to oom-killer, nobody can see the nascent bprm->mm and take it into account.  With this patch get_arg_page() increments current's MM_ANONPAGES counter every time we allocate the new page for argv/envp. When do_execve() succeds or fails, we change this counter back.  Technically this is not 100% correct, we can't know if the new page is swapped out and turn MM_ANONPAGES into MM_SWAPENTS, but I don't think this really matters and everything becomes correct once exec changes ->mm or fails.  Reviewed-and-discussed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: stable@kernel.org ",
        "func_before": "int flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\t\t/* We're using it now */\n\n\tcurrent->flags &= ~(PF_RANDOMIZE | PF_KTHREAD);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\treturn 0;\n\nout:\n\treturn retval;\n}",
        "func": "int flush_old_exec(struct linux_binprm * bprm)\n{\n\tint retval;\n\n\t/*\n\t * Make sure we have a private signal table and that\n\t * we are unassociated from the previous thread group.\n\t */\n\tretval = de_thread(current);\n\tif (retval)\n\t\tgoto out;\n\n\tset_mm_exe_file(bprm->mm, bprm->file);\n\n\t/*\n\t * Release all of the old mmap stuff\n\t */\n\tacct_arg_size(bprm, 0);\n\tretval = exec_mmap(bprm->mm);\n\tif (retval)\n\t\tgoto out;\n\n\tbprm->mm = NULL;\t\t/* We're using it now */\n\n\tcurrent->flags &= ~(PF_RANDOMIZE | PF_KTHREAD);\n\tflush_thread();\n\tcurrent->personality &= ~bprm->per_clear;\n\n\treturn 0;\n\nout:\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,6 +15,7 @@\n \t/*\n \t * Release all of the old mmap stuff\n \t */\n+\tacct_arg_size(bprm, 0);\n \tretval = exec_mmap(bprm->mm);\n \tif (retval)\n \t\tgoto out;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tacct_arg_size(bprm, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4243",
        "func_name": "torvalds/linux/do_execve",
        "description": "fs/exec.c in the Linux kernel before 2.6.37 does not enable the OOM Killer to assess use of stack memory by arrays representing the (1) arguments and (2) environment, which allows local users to cause a denial of service (memory consumption) via a crafted exec system call, aka an \"OOM dodging issue,\" a related issue to CVE-2010-3858.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=3c77f845722158206a7209c45ccddc264d19319c",
        "commit_title": "Brad Spengler published a local memory-allocation DoS that",
        "commit_text": "evades the OOM-killer (though not the virtual memory RLIMIT): http://www.grsecurity.net/~spender/64bit_dos.c  execve()->copy_strings() can allocate a lot of memory, but this is not visible to oom-killer, nobody can see the nascent bprm->mm and take it into account.  With this patch get_arg_page() increments current's MM_ANONPAGES counter every time we allocate the new page for argv/envp. When do_execve() succeds or fails, we change this counter back.  Technically this is not 100% correct, we can't know if the new page is swapped out and turn MM_ANONPAGES into MM_SWAPENTS, but I don't think this really matters and everything becomes correct once exec changes ->mm or fails.  Reviewed-and-discussed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com> Cc: stable@kernel.org ",
        "func_before": "int do_execve(const char * filename,\n\tconst char __user *const __user *argv,\n\tconst char __user *const __user *envp,\n\tstruct pt_regs * regs)\n{\n\tstruct linux_binprm *bprm;\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tbool clear_in_exec;\n\tint retval;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tretval = check_unsafe_exec(bprm);\n\tif (retval < 0)\n\t\tgoto out_free;\n\tclear_in_exec = retval;\n\tcurrent->in_execve = 1;\n\n\tfile = open_exec(filename);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tbprm->filename = filename;\n\tbprm->interp = filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_file;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif ((retval = bprm->argc) < 0)\n\t\tgoto out;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif ((retval = bprm->envc) < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = search_binary_handler(bprm,regs);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tacct_update_integrals(current);\n\tfree_bprm(bprm);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm)\n\t\tmmput (bprm->mm);\n\nout_file:\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\nout_unmark:\n\tif (clear_in_exec)\n\t\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\treturn retval;\n}",
        "func": "int do_execve(const char * filename,\n\tconst char __user *const __user *argv,\n\tconst char __user *const __user *envp,\n\tstruct pt_regs * regs)\n{\n\tstruct linux_binprm *bprm;\n\tstruct file *file;\n\tstruct files_struct *displaced;\n\tbool clear_in_exec;\n\tint retval;\n\n\tretval = unshare_files(&displaced);\n\tif (retval)\n\t\tgoto out_ret;\n\n\tretval = -ENOMEM;\n\tbprm = kzalloc(sizeof(*bprm), GFP_KERNEL);\n\tif (!bprm)\n\t\tgoto out_files;\n\n\tretval = prepare_bprm_creds(bprm);\n\tif (retval)\n\t\tgoto out_free;\n\n\tretval = check_unsafe_exec(bprm);\n\tif (retval < 0)\n\t\tgoto out_free;\n\tclear_in_exec = retval;\n\tcurrent->in_execve = 1;\n\n\tfile = open_exec(filename);\n\tretval = PTR_ERR(file);\n\tif (IS_ERR(file))\n\t\tgoto out_unmark;\n\n\tsched_exec();\n\n\tbprm->file = file;\n\tbprm->filename = filename;\n\tbprm->interp = filename;\n\n\tretval = bprm_mm_init(bprm);\n\tif (retval)\n\t\tgoto out_file;\n\n\tbprm->argc = count(argv, MAX_ARG_STRINGS);\n\tif ((retval = bprm->argc) < 0)\n\t\tgoto out;\n\n\tbprm->envc = count(envp, MAX_ARG_STRINGS);\n\tif ((retval = bprm->envc) < 0)\n\t\tgoto out;\n\n\tretval = prepare_binprm(bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings_kernel(1, &bprm->filename, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tbprm->exec = bprm->p;\n\tretval = copy_strings(bprm->envc, envp, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = copy_strings(bprm->argc, argv, bprm);\n\tif (retval < 0)\n\t\tgoto out;\n\n\tretval = search_binary_handler(bprm,regs);\n\tif (retval < 0)\n\t\tgoto out;\n\n\t/* execve succeeded */\n\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\tacct_update_integrals(current);\n\tfree_bprm(bprm);\n\tif (displaced)\n\t\tput_files_struct(displaced);\n\treturn retval;\n\nout:\n\tif (bprm->mm) {\n\t\tacct_arg_size(bprm, 0);\n\t\tmmput(bprm->mm);\n\t}\n\nout_file:\n\tif (bprm->file) {\n\t\tallow_write_access(bprm->file);\n\t\tfput(bprm->file);\n\t}\n\nout_unmark:\n\tif (clear_in_exec)\n\t\tcurrent->fs->in_exec = 0;\n\tcurrent->in_execve = 0;\n\nout_free:\n\tfree_bprm(bprm);\n\nout_files:\n\tif (displaced)\n\t\treset_files_struct(displaced);\nout_ret:\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -82,8 +82,10 @@\n \treturn retval;\n \n out:\n-\tif (bprm->mm)\n-\t\tmmput (bprm->mm);\n+\tif (bprm->mm) {\n+\t\tacct_arg_size(bprm, 0);\n+\t\tmmput(bprm->mm);\n+\t}\n \n out_file:\n \tif (bprm->file) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (bprm->mm)",
                "\t\tmmput (bprm->mm);"
            ],
            "added_lines": [
                "\tif (bprm->mm) {",
                "\t\tacct_arg_size(bprm, 0);",
                "\t\tmmput(bprm->mm);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-0999",
        "func_name": "torvalds/linux/collapse_huge_page",
        "description": "mm/huge_memory.c in the Linux kernel before 2.6.38-rc5 does not prevent creation of a transparent huge page (THP) during the existence of a temporary stack for an exec system call, which allows local users to cause a denial of service (memory consumption) or possibly have unspecified other impact via a crafted application.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=a7d6e4ecdb7648478ddec76d30d87d03d6e22b31",
        "commit_title": "Transparent hugepages can only be created if rmap is fully",
        "commit_text": "functional. So we must prevent hugepages to be created while is_vma_temporary_stack() is true.  This also optmizes away some harmless but unnecessary setting of khugepaged_scan.address and it switches some BUG_ON to VM_BUG_ON.  ",
        "func_before": "static void collapse_huge_page(struct mm_struct *mm,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage,\n\t\t\t       struct vm_area_struct *vma)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpgtable_t pgtable;\n\tstruct page *new_page;\n\tspinlock_t *ptl;\n\tint isolated;\n\tunsigned long hstart, hend;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n#ifndef CONFIG_NUMA\n\tVM_BUG_ON(!*hpage);\n\tnew_page = *hpage;\n#else\n\tVM_BUG_ON(*hpage);\n\t/*\n\t * Allocate the page while the vma is still valid and under\n\t * the mmap_sem read mode so there is no memory allocation\n\t * later when we take the mmap_sem in write mode. This is more\n\t * friendly behavior (OTOH it may actually hide bugs) to\n\t * filesystems in userland with daemons allocating memory in\n\t * the userland I/O paths.  Allocating memory with the\n\t * mmap_sem in read mode is good idea also to allow greater\n\t * scalability.\n\t */\n\tnew_page = alloc_hugepage_vma(khugepaged_defrag(), vma, address);\n\tif (unlikely(!new_page)) {\n\t\tup_read(&mm->mmap_sem);\n\t\t*hpage = ERR_PTR(-ENOMEM);\n\t\treturn;\n\t}\n#endif\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\tput_page(new_page);\n\t\treturn;\n\t}\n\n\t/* after allocating the hugepage upgrade to mmap_sem write mode */\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Prevent all access to pagetables with the exception of\n\t * gup_fast later hanlded by the ptep_clear_flush and the VM\n\t * handled by the anon_vma lock + PG_lock.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tgoto out;\n\n\tvma = find_vma(mm, address);\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (address < hstart || address + HPAGE_PMD_SIZE > hend)\n\t\tgoto out;\n\n\tif ((!(vma->vm_flags & VM_HUGEPAGE) && !khugepaged_always()) ||\n\t    (vma->vm_flags & VM_NOHUGEPAGE))\n\t\tgoto out;\n\n\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\tgoto out;\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\t/* pmd can't go away or become huge under us */\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tanon_vma_lock(vma->anon_vma);\n\n\tpte = pte_offset_map(pmd, address);\n\tptl = pte_lockptr(mm, pmd);\n\n\tspin_lock(&mm->page_table_lock); /* probably unnecessary */\n\t/*\n\t * After this gup_fast can't run anymore. This also removes\n\t * any huge TLB entry from the CPU so we won't allow\n\t * huge and small TLB entries for the same virtual address\n\t * to avoid the risk of CPU bugs in that area.\n\t */\n\t_pmd = pmdp_clear_flush_notify(vma, address, pmd);\n\tspin_unlock(&mm->page_table_lock);\n\n\tspin_lock(ptl);\n\tisolated = __collapse_huge_page_isolate(vma, address, pte);\n\tspin_unlock(ptl);\n\n\tif (unlikely(!isolated)) {\n\t\tpte_unmap(pte);\n\t\tspin_lock(&mm->page_table_lock);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\tset_pmd_at(mm, address, pmd, _pmd);\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock(vma->anon_vma);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * All pages are isolated and locked so anon_vma rmap\n\t * can't run anymore.\n\t */\n\tanon_vma_unlock(vma->anon_vma);\n\n\t__collapse_huge_page_copy(pte, new_page, vma, address, ptl);\n\tpte_unmap(pte);\n\t__SetPageUptodate(new_page);\n\tpgtable = pmd_pgtable(_pmd);\n\tVM_BUG_ON(page_count(pgtable) != 1);\n\tVM_BUG_ON(page_mapcount(pgtable) != 0);\n\n\t_pmd = mk_pmd(new_page, vma->vm_page_prot);\n\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);\n\t_pmd = pmd_mkhuge(_pmd);\n\n\t/*\n\t * spin_lock() below is not the equivalent of smp_wmb(), so\n\t * this is needed to avoid the copy_huge_page writes to become\n\t * visible after the set_pmd_at() write.\n\t */\n\tsmp_wmb();\n\n\tspin_lock(&mm->page_table_lock);\n\tBUG_ON(!pmd_none(*pmd));\n\tpage_add_new_anon_rmap(new_page, vma, address);\n\tset_pmd_at(mm, address, pmd, _pmd);\n\tupdate_mmu_cache(vma, address, entry);\n\tprepare_pmd_huge_pte(pgtable, mm);\n\tmm->nr_ptes--;\n\tspin_unlock(&mm->page_table_lock);\n\n#ifndef CONFIG_NUMA\n\t*hpage = NULL;\n#endif\n\tkhugepaged_pages_collapsed++;\nout_up_write:\n\tup_write(&mm->mmap_sem);\n\treturn;\n\nout:\n\tmem_cgroup_uncharge_page(new_page);\n#ifdef CONFIG_NUMA\n\tput_page(new_page);\n#endif\n\tgoto out_up_write;\n}",
        "func": "static void collapse_huge_page(struct mm_struct *mm,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage,\n\t\t\t       struct vm_area_struct *vma)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpgtable_t pgtable;\n\tstruct page *new_page;\n\tspinlock_t *ptl;\n\tint isolated;\n\tunsigned long hstart, hend;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n#ifndef CONFIG_NUMA\n\tVM_BUG_ON(!*hpage);\n\tnew_page = *hpage;\n#else\n\tVM_BUG_ON(*hpage);\n\t/*\n\t * Allocate the page while the vma is still valid and under\n\t * the mmap_sem read mode so there is no memory allocation\n\t * later when we take the mmap_sem in write mode. This is more\n\t * friendly behavior (OTOH it may actually hide bugs) to\n\t * filesystems in userland with daemons allocating memory in\n\t * the userland I/O paths.  Allocating memory with the\n\t * mmap_sem in read mode is good idea also to allow greater\n\t * scalability.\n\t */\n\tnew_page = alloc_hugepage_vma(khugepaged_defrag(), vma, address);\n\tif (unlikely(!new_page)) {\n\t\tup_read(&mm->mmap_sem);\n\t\t*hpage = ERR_PTR(-ENOMEM);\n\t\treturn;\n\t}\n#endif\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\tput_page(new_page);\n\t\treturn;\n\t}\n\n\t/* after allocating the hugepage upgrade to mmap_sem write mode */\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Prevent all access to pagetables with the exception of\n\t * gup_fast later hanlded by the ptep_clear_flush and the VM\n\t * handled by the anon_vma lock + PG_lock.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tgoto out;\n\n\tvma = find_vma(mm, address);\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (address < hstart || address + HPAGE_PMD_SIZE > hend)\n\t\tgoto out;\n\n\tif ((!(vma->vm_flags & VM_HUGEPAGE) && !khugepaged_always()) ||\n\t    (vma->vm_flags & VM_NOHUGEPAGE))\n\t\tgoto out;\n\n\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\tgoto out;\n\tif (is_vma_temporary_stack(vma))\n\t\tgoto out;\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\t/* pmd can't go away or become huge under us */\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tanon_vma_lock(vma->anon_vma);\n\n\tpte = pte_offset_map(pmd, address);\n\tptl = pte_lockptr(mm, pmd);\n\n\tspin_lock(&mm->page_table_lock); /* probably unnecessary */\n\t/*\n\t * After this gup_fast can't run anymore. This also removes\n\t * any huge TLB entry from the CPU so we won't allow\n\t * huge and small TLB entries for the same virtual address\n\t * to avoid the risk of CPU bugs in that area.\n\t */\n\t_pmd = pmdp_clear_flush_notify(vma, address, pmd);\n\tspin_unlock(&mm->page_table_lock);\n\n\tspin_lock(ptl);\n\tisolated = __collapse_huge_page_isolate(vma, address, pte);\n\tspin_unlock(ptl);\n\n\tif (unlikely(!isolated)) {\n\t\tpte_unmap(pte);\n\t\tspin_lock(&mm->page_table_lock);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\tset_pmd_at(mm, address, pmd, _pmd);\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock(vma->anon_vma);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * All pages are isolated and locked so anon_vma rmap\n\t * can't run anymore.\n\t */\n\tanon_vma_unlock(vma->anon_vma);\n\n\t__collapse_huge_page_copy(pte, new_page, vma, address, ptl);\n\tpte_unmap(pte);\n\t__SetPageUptodate(new_page);\n\tpgtable = pmd_pgtable(_pmd);\n\tVM_BUG_ON(page_count(pgtable) != 1);\n\tVM_BUG_ON(page_mapcount(pgtable) != 0);\n\n\t_pmd = mk_pmd(new_page, vma->vm_page_prot);\n\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);\n\t_pmd = pmd_mkhuge(_pmd);\n\n\t/*\n\t * spin_lock() below is not the equivalent of smp_wmb(), so\n\t * this is needed to avoid the copy_huge_page writes to become\n\t * visible after the set_pmd_at() write.\n\t */\n\tsmp_wmb();\n\n\tspin_lock(&mm->page_table_lock);\n\tBUG_ON(!pmd_none(*pmd));\n\tpage_add_new_anon_rmap(new_page, vma, address);\n\tset_pmd_at(mm, address, pmd, _pmd);\n\tupdate_mmu_cache(vma, address, entry);\n\tprepare_pmd_huge_pte(pgtable, mm);\n\tmm->nr_ptes--;\n\tspin_unlock(&mm->page_table_lock);\n\n#ifndef CONFIG_NUMA\n\t*hpage = NULL;\n#endif\n\tkhugepaged_pages_collapsed++;\nout_up_write:\n\tup_write(&mm->mmap_sem);\n\treturn;\n\nout:\n\tmem_cgroup_uncharge_page(new_page);\n#ifdef CONFIG_NUMA\n\tput_page(new_page);\n#endif\n\tgoto out_up_write;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -66,6 +66,8 @@\n \n \t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n \tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n+\t\tgoto out;\n+\tif (is_vma_temporary_stack(vma))\n \t\tgoto out;\n \tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tgoto out;",
                "\tif (is_vma_temporary_stack(vma))"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-0999",
        "func_name": "torvalds/linux/khugepaged",
        "description": "mm/huge_memory.c in the Linux kernel before 2.6.38-rc5 does not prevent creation of a transparent huge page (THP) during the existence of a temporary stack for an exec system call, which allows local users to cause a denial of service (memory consumption) or possibly have unspecified other impact via a crafted application.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=a7d6e4ecdb7648478ddec76d30d87d03d6e22b31",
        "commit_title": "Transparent hugepages can only be created if rmap is fully",
        "commit_text": "functional. So we must prevent hugepages to be created while is_vma_temporary_stack() is true.  This also optmizes away some harmless but unnecessary setting of khugepaged_scan.address and it switches some BUG_ON to VM_BUG_ON.  ",
        "func_before": "static int khugepaged(void *none)\n{\n\tstruct mm_slot *mm_slot;\n\n\tset_freezable();\n\tset_user_nice(current, 19);\n\n\t/* serialize with start_khugepaged() */\n\tmutex_lock(&khugepaged_mutex);\n\n\tfor (;;) {\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tBUG_ON(khugepaged_thread != current);\n\t\tkhugepaged_loop();\n\t\tBUG_ON(khugepaged_thread != current);\n\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_enabled())\n\t\t\tbreak;\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = khugepaged_scan.mm_slot;\n\tkhugepaged_scan.mm_slot = NULL;\n\tif (mm_slot)\n\t\tcollect_mm_slot(mm_slot);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tkhugepaged_thread = NULL;\n\tmutex_unlock(&khugepaged_mutex);\n\n\treturn 0;\n}",
        "func": "static int khugepaged(void *none)\n{\n\tstruct mm_slot *mm_slot;\n\n\tset_freezable();\n\tset_user_nice(current, 19);\n\n\t/* serialize with start_khugepaged() */\n\tmutex_lock(&khugepaged_mutex);\n\n\tfor (;;) {\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\t\tkhugepaged_loop();\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_enabled())\n\t\t\tbreak;\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = khugepaged_scan.mm_slot;\n\tkhugepaged_scan.mm_slot = NULL;\n\tif (mm_slot)\n\t\tcollect_mm_slot(mm_slot);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tkhugepaged_thread = NULL;\n\tmutex_unlock(&khugepaged_mutex);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,9 +10,9 @@\n \n \tfor (;;) {\n \t\tmutex_unlock(&khugepaged_mutex);\n-\t\tBUG_ON(khugepaged_thread != current);\n+\t\tVM_BUG_ON(khugepaged_thread != current);\n \t\tkhugepaged_loop();\n-\t\tBUG_ON(khugepaged_thread != current);\n+\t\tVM_BUG_ON(khugepaged_thread != current);\n \n \t\tmutex_lock(&khugepaged_mutex);\n \t\tif (!khugepaged_enabled())",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tBUG_ON(khugepaged_thread != current);",
                "\t\tBUG_ON(khugepaged_thread != current);"
            ],
            "added_lines": [
                "\t\tVM_BUG_ON(khugepaged_thread != current);",
                "\t\tVM_BUG_ON(khugepaged_thread != current);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-0999",
        "func_name": "torvalds/linux/khugepaged_scan_mm_slot",
        "description": "mm/huge_memory.c in the Linux kernel before 2.6.38-rc5 does not prevent creation of a transparent huge page (THP) during the existence of a temporary stack for an exec system call, which allows local users to cause a denial of service (memory consumption) or possibly have unspecified other impact via a crafted application.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=a7d6e4ecdb7648478ddec76d30d87d03d6e22b31",
        "commit_title": "Transparent hugepages can only be created if rmap is fully",
        "commit_text": "functional. So we must prevent hugepages to be created while is_vma_temporary_stack() is true.  This also optmizes away some harmless but unnecessary setting of khugepaged_scan.address and it switches some BUG_ON to VM_BUG_ON.  ",
        "func_before": "static unsigned int khugepaged_scan_mm_slot(unsigned int pages,\n\t\t\t\t\t    struct page **hpage)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint progress = 0;\n\n\tVM_BUG_ON(!pages);\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_scan.mm_slot)\n\t\tmm_slot = khugepaged_scan.mm_slot;\n\telse {\n\t\tmm_slot = list_entry(khugepaged_scan.mm_head.next,\n\t\t\t\t     struct mm_slot, mm_node);\n\t\tkhugepaged_scan.address = 0;\n\t\tkhugepaged_scan.mm_slot = mm_slot;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmm = mm_slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, khugepaged_scan.address);\n\n\tprogress++;\n\tfor (; vma; vma = vma->vm_next) {\n\t\tunsigned long hstart, hend;\n\n\t\tcond_resched();\n\t\tif (unlikely(khugepaged_test_exit(mm))) {\n\t\t\tprogress++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((!(vma->vm_flags & VM_HUGEPAGE) &&\n\t\t     !khugepaged_always()) ||\n\t\t    (vma->vm_flags & VM_NOHUGEPAGE)) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file) {\n\t\t\tkhugepaged_scan.address = vma->vm_end;\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\t\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\t\thend = vma->vm_end & HPAGE_PMD_MASK;\n\t\tif (hstart >= hend) {\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (khugepaged_scan.address < hstart)\n\t\t\tkhugepaged_scan.address = hstart;\n\t\tif (khugepaged_scan.address > hend) {\n\t\t\tkhugepaged_scan.address = hend + HPAGE_PMD_SIZE;\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tBUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n\n\t\twhile (khugepaged_scan.address < hend) {\n\t\t\tint ret;\n\t\t\tcond_resched();\n\t\t\tif (unlikely(khugepaged_test_exit(mm)))\n\t\t\t\tgoto breakouterloop;\n\n\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||\n\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >\n\t\t\t\t  hend);\n\t\t\tret = khugepaged_scan_pmd(mm, vma,\n\t\t\t\t\t\t  khugepaged_scan.address,\n\t\t\t\t\t\t  hpage);\n\t\t\t/* move to next address */\n\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;\n\t\t\tprogress += HPAGE_PMD_NR;\n\t\t\tif (ret)\n\t\t\t\t/* we released mmap_sem so break loop */\n\t\t\t\tgoto breakouterloop_mmap_sem;\n\t\t\tif (progress >= pages)\n\t\t\t\tgoto breakouterloop;\n\t\t}\n\t}\nbreakouterloop:\n\tup_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */\nbreakouterloop_mmap_sem:\n\n\tspin_lock(&khugepaged_mm_lock);\n\tBUG_ON(khugepaged_scan.mm_slot != mm_slot);\n\t/*\n\t * Release the current mm_slot if this mm is about to die, or\n\t * if we scanned all vmas of this mm.\n\t */\n\tif (khugepaged_test_exit(mm) || !vma) {\n\t\t/*\n\t\t * Make sure that if mm_users is reaching zero while\n\t\t * khugepaged runs here, khugepaged_exit will find\n\t\t * mm_slot not pointing to the exiting mm.\n\t\t */\n\t\tif (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {\n\t\t\tkhugepaged_scan.mm_slot = list_entry(\n\t\t\t\tmm_slot->mm_node.next,\n\t\t\t\tstruct mm_slot, mm_node);\n\t\t\tkhugepaged_scan.address = 0;\n\t\t} else {\n\t\t\tkhugepaged_scan.mm_slot = NULL;\n\t\t\tkhugepaged_full_scans++;\n\t\t}\n\n\t\tcollect_mm_slot(mm_slot);\n\t}\n\n\treturn progress;\n}",
        "func": "static unsigned int khugepaged_scan_mm_slot(unsigned int pages,\n\t\t\t\t\t    struct page **hpage)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint progress = 0;\n\n\tVM_BUG_ON(!pages);\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_scan.mm_slot)\n\t\tmm_slot = khugepaged_scan.mm_slot;\n\telse {\n\t\tmm_slot = list_entry(khugepaged_scan.mm_head.next,\n\t\t\t\t     struct mm_slot, mm_node);\n\t\tkhugepaged_scan.address = 0;\n\t\tkhugepaged_scan.mm_slot = mm_slot;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmm = mm_slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, khugepaged_scan.address);\n\n\tprogress++;\n\tfor (; vma; vma = vma->vm_next) {\n\t\tunsigned long hstart, hend;\n\n\t\tcond_resched();\n\t\tif (unlikely(khugepaged_test_exit(mm))) {\n\t\t\tprogress++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((!(vma->vm_flags & VM_HUGEPAGE) &&\n\t\t     !khugepaged_always()) ||\n\t\t    (vma->vm_flags & VM_NOHUGEPAGE)) {\n\t\tskip:\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\t\tgoto skip;\n\t\tif (is_vma_temporary_stack(vma))\n\t\t\tgoto skip;\n\n\t\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\t\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\t\thend = vma->vm_end & HPAGE_PMD_MASK;\n\t\tif (hstart >= hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address > hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address < hstart)\n\t\t\tkhugepaged_scan.address = hstart;\n\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n\n\t\twhile (khugepaged_scan.address < hend) {\n\t\t\tint ret;\n\t\t\tcond_resched();\n\t\t\tif (unlikely(khugepaged_test_exit(mm)))\n\t\t\t\tgoto breakouterloop;\n\n\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||\n\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >\n\t\t\t\t  hend);\n\t\t\tret = khugepaged_scan_pmd(mm, vma,\n\t\t\t\t\t\t  khugepaged_scan.address,\n\t\t\t\t\t\t  hpage);\n\t\t\t/* move to next address */\n\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;\n\t\t\tprogress += HPAGE_PMD_NR;\n\t\t\tif (ret)\n\t\t\t\t/* we released mmap_sem so break loop */\n\t\t\t\tgoto breakouterloop_mmap_sem;\n\t\t\tif (progress >= pages)\n\t\t\t\tgoto breakouterloop;\n\t\t}\n\t}\nbreakouterloop:\n\tup_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */\nbreakouterloop_mmap_sem:\n\n\tspin_lock(&khugepaged_mm_lock);\n\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);\n\t/*\n\t * Release the current mm_slot if this mm is about to die, or\n\t * if we scanned all vmas of this mm.\n\t */\n\tif (khugepaged_test_exit(mm) || !vma) {\n\t\t/*\n\t\t * Make sure that if mm_users is reaching zero while\n\t\t * khugepaged runs here, khugepaged_exit will find\n\t\t * mm_slot not pointing to the exiting mm.\n\t\t */\n\t\tif (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {\n\t\t\tkhugepaged_scan.mm_slot = list_entry(\n\t\t\t\tmm_slot->mm_node.next,\n\t\t\t\tstruct mm_slot, mm_node);\n\t\t\tkhugepaged_scan.address = 0;\n\t\t} else {\n\t\t\tkhugepaged_scan.mm_slot = NULL;\n\t\t\tkhugepaged_full_scans++;\n\t\t}\n\n\t\tcollect_mm_slot(mm_slot);\n\t}\n\n\treturn progress;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,32 +39,27 @@\n \t\tif ((!(vma->vm_flags & VM_HUGEPAGE) &&\n \t\t     !khugepaged_always()) ||\n \t\t    (vma->vm_flags & VM_NOHUGEPAGE)) {\n+\t\tskip:\n \t\t\tprogress++;\n \t\t\tcontinue;\n \t\t}\n+\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n+\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n+\t\t\tgoto skip;\n+\t\tif (is_vma_temporary_stack(vma))\n+\t\t\tgoto skip;\n \n-\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n-\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file) {\n-\t\t\tkhugepaged_scan.address = vma->vm_end;\n-\t\t\tprogress++;\n-\t\t\tcontinue;\n-\t\t}\n \t\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n \n \t\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n \t\thend = vma->vm_end & HPAGE_PMD_MASK;\n-\t\tif (hstart >= hend) {\n-\t\t\tprogress++;\n-\t\t\tcontinue;\n-\t\t}\n+\t\tif (hstart >= hend)\n+\t\t\tgoto skip;\n+\t\tif (khugepaged_scan.address > hend)\n+\t\t\tgoto skip;\n \t\tif (khugepaged_scan.address < hstart)\n \t\t\tkhugepaged_scan.address = hstart;\n-\t\tif (khugepaged_scan.address > hend) {\n-\t\t\tkhugepaged_scan.address = hend + HPAGE_PMD_SIZE;\n-\t\t\tprogress++;\n-\t\t\tcontinue;\n-\t\t}\n-\t\tBUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n+\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n \n \t\twhile (khugepaged_scan.address < hend) {\n \t\t\tint ret;\n@@ -93,7 +88,7 @@\n breakouterloop_mmap_sem:\n \n \tspin_lock(&khugepaged_mm_lock);\n-\tBUG_ON(khugepaged_scan.mm_slot != mm_slot);\n+\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);\n \t/*\n \t * Release the current mm_slot if this mm is about to die, or\n \t * if we scanned all vmas of this mm.",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */",
                "\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file) {",
                "\t\t\tkhugepaged_scan.address = vma->vm_end;",
                "\t\t\tprogress++;",
                "\t\t\tcontinue;",
                "\t\t}",
                "\t\tif (hstart >= hend) {",
                "\t\t\tprogress++;",
                "\t\t\tcontinue;",
                "\t\t}",
                "\t\tif (khugepaged_scan.address > hend) {",
                "\t\t\tkhugepaged_scan.address = hend + HPAGE_PMD_SIZE;",
                "\t\t\tprogress++;",
                "\t\t\tcontinue;",
                "\t\t}",
                "\t\tBUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);",
                "\tBUG_ON(khugepaged_scan.mm_slot != mm_slot);"
            ],
            "added_lines": [
                "\t\tskip:",
                "\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */",
                "\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)",
                "\t\t\tgoto skip;",
                "\t\tif (is_vma_temporary_stack(vma))",
                "\t\t\tgoto skip;",
                "\t\tif (hstart >= hend)",
                "\t\t\tgoto skip;",
                "\t\tif (khugepaged_scan.address > hend)",
                "\t\t\tgoto skip;",
                "\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);",
                "\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);"
            ]
        }
    },
    {
        "cve_id": "CVE-2011-1082",
        "func_name": "torvalds/linux/eventpoll_init",
        "description": "fs/eventpoll.c in the Linux kernel before 2.6.38 places epoll file descriptors within other epoll data structures without properly checking for (1) closed loops or (2) deep chains, which allows local users to cause a denial of service (deadlock or stack memory consumption) via a crafted application that makes epoll_create and epoll_ctl system calls.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=22bacca48a1755f79b7e0f192ddb9fbb7fc6e64e",
        "commit_title": "In several places, an epoll fd can call another file's ->f_op->poll()",
        "commit_text": "method with ep->mtx held.  This is in general unsafe, because that other file could itself be an epoll fd that contains the original epoll fd.  The code defends against this possibility in its own ->poll() method using ep_call_nested, but there are several other unsafe calls to ->poll elsewhere that can be made to deadlock.  For example, the following simple program causes the call in ep_insert recursively call the original fd's ->poll, leading to deadlock:   #include <unistd.h>  #include <sys/epoll.h>   int main(void) {      int e1, e2, p[2];      struct epoll_event evt = {          .events = EPOLLIN      };       e1 = epoll_create(1);      e2 = epoll_create(2);      pipe(p);       epoll_ctl(e2, EPOLL_CTL_ADD, e1, &evt);      epoll_ctl(e1, EPOLL_CTL_ADD, p[0], &evt);      write(p[1], p, sizeof p);      epoll_ctl(e1, EPOLL_CTL_ADD, e2, &evt);       return 0;  }  On insertion, check whether the inserted file is itself a struct epoll, and if so, do a recursive walk to detect whether inserting this file would create a loop of epoll structures, which could lead to deadlock.  [nelhage@ksplice.com: Use epmutex to serialize concurrent inserts] Cc: <stable@kernel.org>\t\t[2.6.34+, possibly earlier] ",
        "func_before": "static int __init eventpoll_init(void)\n{\n\tstruct sysinfo si;\n\n\tsi_meminfo(&si);\n\t/*\n\t * Allows top 4% of lomem to be allocated for epoll watches (per user).\n\t */\n\tmax_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /\n\t\tEP_ITEM_COST;\n\tBUG_ON(max_user_watches < 0);\n\n\t/* Initialize the structure used to perform safe poll wait head wake ups */\n\tep_nested_calls_init(&poll_safewake_ncalls);\n\n\t/* Initialize the structure used to perform file's f_op->poll() calls */\n\tep_nested_calls_init(&poll_readywalk_ncalls);\n\n\t/* Allocates slab cache used to allocate \"struct epitem\" items */\n\tepi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\t/* Allocates slab cache used to allocate \"struct eppoll_entry\" */\n\tpwq_cache = kmem_cache_create(\"eventpoll_pwq\",\n\t\t\tsizeof(struct eppoll_entry), 0, SLAB_PANIC, NULL);\n\n\treturn 0;\n}",
        "func": "static int __init eventpoll_init(void)\n{\n\tstruct sysinfo si;\n\n\tsi_meminfo(&si);\n\t/*\n\t * Allows top 4% of lomem to be allocated for epoll watches (per user).\n\t */\n\tmax_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /\n\t\tEP_ITEM_COST;\n\tBUG_ON(max_user_watches < 0);\n\n\t/*\n\t * Initialize the structure used to perform epoll file descriptor\n\t * inclusion loops checks.\n\t */\n\tep_nested_calls_init(&poll_loop_ncalls);\n\n\t/* Initialize the structure used to perform safe poll wait head wake ups */\n\tep_nested_calls_init(&poll_safewake_ncalls);\n\n\t/* Initialize the structure used to perform file's f_op->poll() calls */\n\tep_nested_calls_init(&poll_readywalk_ncalls);\n\n\t/* Allocates slab cache used to allocate \"struct epitem\" items */\n\tepi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\t/* Allocates slab cache used to allocate \"struct eppoll_entry\" */\n\tpwq_cache = kmem_cache_create(\"eventpoll_pwq\",\n\t\t\tsizeof(struct eppoll_entry), 0, SLAB_PANIC, NULL);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,12 @@\n \tmax_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /\n \t\tEP_ITEM_COST;\n \tBUG_ON(max_user_watches < 0);\n+\n+\t/*\n+\t * Initialize the structure used to perform epoll file descriptor\n+\t * inclusion loops checks.\n+\t */\n+\tep_nested_calls_init(&poll_loop_ncalls);\n \n \t/* Initialize the structure used to perform safe poll wait head wake ups */\n \tep_nested_calls_init(&poll_safewake_ncalls);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/*",
                "\t * Initialize the structure used to perform epoll file descriptor",
                "\t * inclusion loops checks.",
                "\t */",
                "\tep_nested_calls_init(&poll_loop_ncalls);"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4251",
        "func_name": "torvalds/linux/sk_receive_skb",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.34 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service (memory consumption) by sending a large amount of network traffic, as demonstrated by netperf UDP tests.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=8eae939f1400326b06d0c9afe53d2a484a326871",
        "commit_title": "We got system OOM while running some UDP netperf testing on the loopback",
        "commit_text": "device. The case is multiple senders sent stream UDP packets to a single receiver via loopback on local host. Of course, the receiver is not able to handle all the packets in time. But we surprisingly found that these packets were not discarded due to the receiver's sk->sk_rcvbuf limit. Instead, they are kept queuing to sk->sk_backlog and finally ate up all the memory. We believe this is a secure hole that a none privileged user can crash the system.  The root cause for this problem is, when the receiver is doing __release_sock() (i.e. after userspace recv, kernel udp_recvmsg -> skb_free_datagram_locked -> release_sock), it moves skbs from backlog to sk_receive_queue with the softirq enabled. In the above case, multiple busy senders will almost make it an endless loop. The skbs in the backlog end up eat all the system memory.  The issue is not only for UDP. Any protocols using socket backlog is potentially affected. The patch adds limit for socket backlog so that the backlog size cannot be expanded endlessly.  Cc: David Miller <davem@davemloft.net> Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net> Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru Cc: \"Pekka Savola (ipv6)\" <pekkas@netcore.fi> Cc: Patrick McHardy <kaber@trash.net> Cc: Vlad Yasevich <vladislav.yasevich@hp.com> Cc: Sridhar Samudrala <sri@us.ibm.com> Cc: Jon Maloy <jon.maloy@ericsson.com> Cc: Allan Stephens <allan.stephens@windriver.com> Cc: Andrew Hendry <andrew.hendry@gmail.com> ",
        "func_before": "int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else\n\t\tsk_add_backlog(sk, skb);\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "func": "int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog_limited(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,8 +20,12 @@\n \t\trc = sk_backlog_rcv(sk, skb);\n \n \t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n-\t} else\n-\t\tsk_add_backlog(sk, skb);\n+\t} else if (sk_add_backlog_limited(sk, skb)) {\n+\t\tbh_unlock_sock(sk);\n+\t\tatomic_inc(&sk->sk_drops);\n+\t\tgoto discard_and_relse;\n+\t}\n+\n \tbh_unlock_sock(sk);\n out:\n \tsock_put(sk);",
        "diff_line_info": {
            "deleted_lines": [
                "\t} else",
                "\t\tsk_add_backlog(sk, skb);"
            ],
            "added_lines": [
                "\t} else if (sk_add_backlog_limited(sk, skb)) {",
                "\t\tbh_unlock_sock(sk);",
                "\t\tatomic_inc(&sk->sk_drops);",
                "\t\tgoto discard_and_relse;",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4251",
        "func_name": "torvalds/linux/__release_sock",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.34 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service (memory consumption) by sending a large amount of network traffic, as demonstrated by netperf UDP tests.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=8eae939f1400326b06d0c9afe53d2a484a326871",
        "commit_title": "We got system OOM while running some UDP netperf testing on the loopback",
        "commit_text": "device. The case is multiple senders sent stream UDP packets to a single receiver via loopback on local host. Of course, the receiver is not able to handle all the packets in time. But we surprisingly found that these packets were not discarded due to the receiver's sk->sk_rcvbuf limit. Instead, they are kept queuing to sk->sk_backlog and finally ate up all the memory. We believe this is a secure hole that a none privileged user can crash the system.  The root cause for this problem is, when the receiver is doing __release_sock() (i.e. after userspace recv, kernel udp_recvmsg -> skb_free_datagram_locked -> release_sock), it moves skbs from backlog to sk_receive_queue with the softirq enabled. In the above case, multiple busy senders will almost make it an endless loop. The skbs in the backlog end up eat all the system memory.  The issue is not only for UDP. Any protocols using socket backlog is potentially affected. The patch adds limit for socket backlog so that the backlog size cannot be expanded endlessly.  Cc: David Miller <davem@davemloft.net> Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net> Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru Cc: \"Pekka Savola (ipv6)\" <pekkas@netcore.fi> Cc: Patrick McHardy <kaber@trash.net> Cc: Vlad Yasevich <vladislav.yasevich@hp.com> Cc: Sridhar Samudrala <sri@us.ibm.com> Cc: Jon Maloy <jon.maloy@ericsson.com> Cc: Allan Stephens <allan.stephens@windriver.com> Cc: Andrew Hendry <andrew.hendry@gmail.com> ",
        "func_before": "static void __release_sock(struct sock *sk)\n{\n\tstruct sk_buff *skb = sk->sk_backlog.head;\n\n\tdo {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\t\tbh_unlock_sock(sk);\n\n\t\tdo {\n\t\t\tstruct sk_buff *next = skb->next;\n\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\t/*\n\t\t\t * We are in process context here with softirqs\n\t\t\t * disabled, use cond_resched_softirq() to preempt.\n\t\t\t * This is safe to do because we've taken the backlog\n\t\t\t * queue private:\n\t\t\t */\n\t\t\tcond_resched_softirq();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tbh_lock_sock(sk);\n\t} while ((skb = sk->sk_backlog.head) != NULL);\n}",
        "func": "static void __release_sock(struct sock *sk)\n{\n\tstruct sk_buff *skb = sk->sk_backlog.head;\n\n\tdo {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\t\tbh_unlock_sock(sk);\n\n\t\tdo {\n\t\t\tstruct sk_buff *next = skb->next;\n\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\t/*\n\t\t\t * We are in process context here with softirqs\n\t\t\t * disabled, use cond_resched_softirq() to preempt.\n\t\t\t * This is safe to do because we've taken the backlog\n\t\t\t * queue private:\n\t\t\t */\n\t\t\tcond_resched_softirq();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tbh_lock_sock(sk);\n\t} while ((skb = sk->sk_backlog.head) != NULL);\n\n\t/*\n\t * Doing the zeroing here guarantee we can not loop forever\n\t * while a wild producer attempts to flood us.\n\t */\n\tsk->sk_backlog.len = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,4 +25,10 @@\n \n \t\tbh_lock_sock(sk);\n \t} while ((skb = sk->sk_backlog.head) != NULL);\n+\n+\t/*\n+\t * Doing the zeroing here guarantee we can not loop forever\n+\t * while a wild producer attempts to flood us.\n+\t */\n+\tsk->sk_backlog.len = 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/*",
                "\t * Doing the zeroing here guarantee we can not loop forever",
                "\t * while a wild producer attempts to flood us.",
                "\t */",
                "\tsk->sk_backlog.len = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4251",
        "func_name": "torvalds/linux/sock_init_data",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.34 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service (memory consumption) by sending a large amount of network traffic, as demonstrated by netperf UDP tests.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=8eae939f1400326b06d0c9afe53d2a484a326871",
        "commit_title": "We got system OOM while running some UDP netperf testing on the loopback",
        "commit_text": "device. The case is multiple senders sent stream UDP packets to a single receiver via loopback on local host. Of course, the receiver is not able to handle all the packets in time. But we surprisingly found that these packets were not discarded due to the receiver's sk->sk_rcvbuf limit. Instead, they are kept queuing to sk->sk_backlog and finally ate up all the memory. We believe this is a secure hole that a none privileged user can crash the system.  The root cause for this problem is, when the receiver is doing __release_sock() (i.e. after userspace recv, kernel udp_recvmsg -> skb_free_datagram_locked -> release_sock), it moves skbs from backlog to sk_receive_queue with the softirq enabled. In the above case, multiple busy senders will almost make it an endless loop. The skbs in the backlog end up eat all the system memory.  The issue is not only for UDP. Any protocols using socket backlog is potentially affected. The patch adds limit for socket backlog so that the backlog size cannot be expanded endlessly.  Cc: David Miller <davem@davemloft.net> Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net> Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru Cc: \"Pekka Savola (ipv6)\" <pekkas@netcore.fi> Cc: Patrick McHardy <kaber@trash.net> Cc: Vlad Yasevich <vladislav.yasevich@hp.com> Cc: Sridhar Samudrala <sri@us.ibm.com> Cc: Jon Maloy <jon.maloy@ericsson.com> Cc: Allan Stephens <allan.stephens@windriver.com> Cc: Andrew Hendry <andrew.hendry@gmail.com> ",
        "func_before": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_sleep\t=\t&sock->wait;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_sleep\t=\tNULL;\n\n\trwlock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\n\tsk->sk_peercred.pid \t=\t0;\n\tsk->sk_peercred.uid\t=\t-1;\n\tsk->sk_peercred.gid\t=\t-1;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "func": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_sleep\t=\t&sock->wait;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_sleep\t=\tNULL;\n\n\trwlock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\n\tsk->sk_peercred.pid \t=\t0;\n\tsk->sk_peercred.uid\t=\t-1;\n\tsk->sk_peercred.gid\t=\t-1;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,7 @@\n \tsk->sk_allocation\t=\tGFP_KERNEL;\n \tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n \tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n+\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;\n \tsk->sk_state\t\t=\tTCP_CLOSE;\n \tsk_set_socket(sk, sock);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4251",
        "func_name": "torvalds/linux/sk_clone",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.34 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service (memory consumption) by sending a large amount of network traffic, as demonstrated by netperf UDP tests.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=8eae939f1400326b06d0c9afe53d2a484a326871",
        "commit_title": "We got system OOM while running some UDP netperf testing on the loopback",
        "commit_text": "device. The case is multiple senders sent stream UDP packets to a single receiver via loopback on local host. Of course, the receiver is not able to handle all the packets in time. But we surprisingly found that these packets were not discarded due to the receiver's sk->sk_rcvbuf limit. Instead, they are kept queuing to sk->sk_backlog and finally ate up all the memory. We believe this is a secure hole that a none privileged user can crash the system.  The root cause for this problem is, when the receiver is doing __release_sock() (i.e. after userspace recv, kernel udp_recvmsg -> skb_free_datagram_locked -> release_sock), it moves skbs from backlog to sk_receive_queue with the softirq enabled. In the above case, multiple busy senders will almost make it an endless loop. The skbs in the backlog end up eat all the system memory.  The issue is not only for UDP. Any protocols using socket backlog is potentially affected. The patch adds limit for socket backlog so that the backlog size cannot be expanded endlessly.  Cc: David Miller <davem@davemloft.net> Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net> Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru Cc: \"Pekka Savola (ipv6)\" <pekkas@netcore.fi> Cc: Patrick McHardy <kaber@trash.net> Cc: Vlad Yasevich <vladislav.yasevich@hp.com> Cc: Sridhar Samudrala <sri@us.ibm.com> Cc: Jon Maloy <jon.maloy@ericsson.com> Cc: Allan Stephens <allan.stephens@windriver.com> Cc: Andrew Hendry <andrew.hendry@gmail.com> ",
        "func_before": "struct sock *sk_clone(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n#ifdef CONFIG_NET_DMA\n\t\tskb_queue_head_init(&newsk->sk_async_wait_queue);\n#endif\n\n\t\trwlock_init(&newsk->sk_dst_lock);\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = newsk->sk_filter;\n\t\tif (filter != NULL)\n\t\t\tsk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(xfrm_sk_clone_policy(newsk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_priority = 0;\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_sleep\t = NULL;\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tpercpu_counter_inc(newsk->sk_prot->sockets_allocated);\n\n\t\tif (sock_flag(newsk, SOCK_TIMESTAMP) ||\n\t\t    sock_flag(newsk, SOCK_TIMESTAMPING_RX_SOFTWARE))\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
        "func": "struct sock *sk_clone(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n#ifdef CONFIG_NET_DMA\n\t\tskb_queue_head_init(&newsk->sk_async_wait_queue);\n#endif\n\n\t\trwlock_init(&newsk->sk_dst_lock);\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = newsk->sk_filter;\n\t\tif (filter != NULL)\n\t\t\tsk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(xfrm_sk_clone_policy(newsk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_priority = 0;\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_sleep\t = NULL;\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tpercpu_counter_inc(newsk->sk_prot->sockets_allocated);\n\n\t\tif (sock_flag(newsk, SOCK_TIMESTAMP) ||\n\t\t    sock_flag(newsk, SOCK_TIMESTAMPING_RX_SOFTWARE))\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,7 @@\n \t\tsock_lock_init(newsk);\n \t\tbh_lock_sock(newsk);\n \t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n+\t\tnewsk->sk_backlog.len = 0;\n \n \t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n \t\t/*",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tnewsk->sk_backlog.len = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/udp_queue_rcv_skb",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (sk->sk_filter) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "func": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (sk->sk_filter) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\n\tif (sk_rcvqueues_full(sk, skb))\n\t\tgoto drop;\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -81,6 +81,10 @@\n \t\t\tgoto drop;\n \t}\n \n+\n+\tif (sk_rcvqueues_full(sk, skb))\n+\t\tgoto drop;\n+\n \trc = 0;\n \n \tbh_lock_sock(sk);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (sk_rcvqueues_full(sk, skb))",
                "\t\tgoto drop;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/udp_queue_rcv_skb",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (sk->sk_filter) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "func": "int udp_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\t/*\n\t *\tCharge it to the socket, dropping if the queue is full.\n\t */\n\tif (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\tnf_reset(skb);\n\n\tif (up->encap_type) {\n\t\t/*\n\t\t * This is an encapsulation socket so pass the skb to\n\t\t * the socket's udp_encap_rcv() hook. Otherwise, just\n\t\t * fall through and pass this up the UDP socket.\n\t\t * up->encap_rcv() returns the following value:\n\t\t * =0 if skb was successfully passed to the encap\n\t\t *    handler or was discarded by it.\n\t\t * >0 if skb should be passed on to UDP.\n\t\t * <0 if skb should be resubmitted as proto -N\n\t\t */\n\n\t\t/* if we're overly short, let UDP handle it */\n\t\tif (skb->len > sizeof(struct udphdr) &&\n\t\t    up->encap_rcv != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = (*up->encap_rcv)(sk, skb);\n\t\t\tif (ret <= 0) {\n\t\t\t\tUDP_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\t\t UDP_MIB_INDATAGRAMS,\n\t\t\t\t\t\t is_udplite);\n\t\t\t\treturn -ret;\n\t\t\t}\n\t\t}\n\n\t\t/* FALLTHROUGH -- it's a UDP Packet */\n\t}\n\n\t/*\n\t * \tUDP-Lite specific tests, ignored on UDP sockets\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\t/*\n\t\t * MIB statistics other than incrementing the error count are\n\t\t * disabled for the following two types of errors: these depend\n\t\t * on the application settings, not on the functioning of the\n\t\t * protocol stack as such.\n\t\t *\n\t\t * RFC 3828 here recommends (sec 3.3): \"There should also be a\n\t\t * way ... to ... at least let the receiving application block\n\t\t * delivery of packets with coverage values less than a value\n\t\t * provided by the application.\"\n\t\t */\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE: partial coverage \"\n\t\t\t\t\"%d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\t/* The next case involves violating the min. coverage requested\n\t\t * by the receiver. This is subtle: if receiver wants x and x is\n\t\t * greater than the buffersize/MTU then receiver will complain\n\t\t * that it wants x while sender emits packets of smaller size y.\n\t\t * Therefore the above ...()->partial_cov statement is essential.\n\t\t */\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING\n\t\t\t\t\"UDPLITE: coverage %d too small, need min %d\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (sk->sk_filter) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\n\tif (sk_rcvqueues_full(sk, skb))\n\t\tgoto drop;\n\n\trc = 0;\n\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\trc = __udp_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tgoto drop;\n\t}\n\tbh_unlock_sock(sk);\n\n\treturn rc;\n\ndrop:\n\tUDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tatomic_inc(&sk->sk_drops);\n\tkfree_skb(skb);\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -81,6 +81,10 @@\n \t\t\tgoto drop;\n \t}\n \n+\n+\tif (sk_rcvqueues_full(sk, skb))\n+\t\tgoto drop;\n+\n \trc = 0;\n \n \tbh_lock_sock(sk);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (sk_rcvqueues_full(sk, skb))",
                "\t\tgoto drop;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/sk_add_backlog",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)\n{\n\tif (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))\n\t\treturn -ENOBUFS;\n\n\t__sk_add_backlog(sk, skb);\n\tsk->sk_backlog.len += skb->truesize;\n\treturn 0;\n}",
        "func": "static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)\n{\n\tif (sk_rcvqueues_full(sk, skb))\n\t\treturn -ENOBUFS;\n\n\t__sk_add_backlog(sk, skb);\n\tsk->sk_backlog.len += skb->truesize;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)\n {\n-\tif (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))\n+\tif (sk_rcvqueues_full(sk, skb))\n \t\treturn -ENOBUFS;\n \n \t__sk_add_backlog(sk, skb);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))"
            ],
            "added_lines": [
                "\tif (sk_rcvqueues_full(sk, skb))"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/sk_receive_skb",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "func": "int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,10 @@\n \n \tskb->dev = NULL;\n \n+\tif (sk_rcvqueues_full(sk, skb)) {\n+\t\tatomic_inc(&sk->sk_drops);\n+\t\tgoto discard_and_relse;\n+\t}\n \tif (nested)\n \t\tbh_lock_sock_nested(sk);\n \telse",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (sk_rcvqueues_full(sk, skb)) {",
                "\t\tatomic_inc(&sk->sk_drops);",
                "\t\tgoto discard_and_relse;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/sock_init_data",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_sleep\t=\t&sock->wait;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_sleep\t=\tNULL;\n\n\tspin_lock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\n\tsk->sk_peercred.pid \t=\t0;\n\tsk->sk_peercred.uid\t=\t-1;\n\tsk->sk_peercred.gid\t=\t-1;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "func": "void sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_sleep\t=\t&sock->wait;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_sleep\t=\tNULL;\n\n\tspin_lock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\n\tsk->sk_peercred.pid \t=\t0;\n\tsk->sk_peercred.uid\t=\t-1;\n\tsk->sk_peercred.gid\t=\t-1;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,6 @@\n \tsk->sk_allocation\t=\tGFP_KERNEL;\n \tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n \tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n-\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;\n \tsk->sk_state\t\t=\tTCP_CLOSE;\n \tsk_set_socket(sk, sock);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tsk->sk_backlog.limit\t=\tsk->sk_rcvbuf << 1;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2010-4805",
        "func_name": "torvalds/linux/sctp_init_sock",
        "description": "The socket implementation in net/core/sock.c in the Linux kernel before 2.6.35 does not properly manage a backlog of received packets, which allows remote attackers to cause a denial of service by sending a large amount of network traffic, related to the sk_add_backlog function and the sk_rmem_alloc socket field.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2010-4251.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=c377411f2494a931ff7facdbb3a6839b1266bcf6",
        "commit_title": "Current socket backlog limit is not enough to really stop DDOS attacks,",
        "commit_text": "because user thread spend many time to process a full backlog each round, and user might crazy spin on socket lock.  We should add backlog size and receive_queue size (aka rmem_alloc) to pace writers, and let user run without being slow down too much.  Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in stress situations.  Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp receiver can now process ~200.000 pps (instead of ~100 pps before the patch) on a 8 core machine.  ",
        "func_before": "SCTP_STATIC int sctp_init_sock(struct sock *sk)\n{\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_sock *sp;\n\n\tSCTP_DEBUG_PRINTK(\"sctp_init_sock(sk: %p)\\n\", sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = sctp_max_burst;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = sctp_max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = sctp_rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = sctp_rto_initial;\n\tsp->rtoinfo.srto_max     = sctp_rto_max;\n\tsp->rtoinfo.srto_min     = sctp_rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = sctp_max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = sctp_valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = sctp_hb_interval;\n\tsp->pathmaxrxt  = sctp_max_retrans_path;\n\tsp->pathmtu     = 0; // allow default discovery\n\tsp->sackdelay   = sctp_sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!ep)\n\t\treturn -ENOMEM;\n\n\tsp->ep = ep;\n\tsp->hmac = NULL;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\n\t/* Set socket backlog limit. */\n\tsk->sk_backlog.limit = sysctl_sctp_rmem[1];\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "func": "SCTP_STATIC int sctp_init_sock(struct sock *sk)\n{\n\tstruct sctp_endpoint *ep;\n\tstruct sctp_sock *sp;\n\n\tSCTP_DEBUG_PRINTK(\"sctp_init_sock(sk: %p)\\n\", sk);\n\n\tsp = sctp_sk(sk);\n\n\t/* Initialize the SCTP per socket area.  */\n\tswitch (sk->sk_type) {\n\tcase SOCK_SEQPACKET:\n\t\tsp->type = SCTP_SOCKET_UDP;\n\t\tbreak;\n\tcase SOCK_STREAM:\n\t\tsp->type = SCTP_SOCKET_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\t/* Initialize default send parameters. These parameters can be\n\t * modified with the SCTP_DEFAULT_SEND_PARAM socket option.\n\t */\n\tsp->default_stream = 0;\n\tsp->default_ppid = 0;\n\tsp->default_flags = 0;\n\tsp->default_context = 0;\n\tsp->default_timetolive = 0;\n\n\tsp->default_rcv_context = 0;\n\tsp->max_burst = sctp_max_burst;\n\n\t/* Initialize default setup parameters. These parameters\n\t * can be modified with the SCTP_INITMSG socket option or\n\t * overridden by the SCTP_INIT CMSG.\n\t */\n\tsp->initmsg.sinit_num_ostreams   = sctp_max_outstreams;\n\tsp->initmsg.sinit_max_instreams  = sctp_max_instreams;\n\tsp->initmsg.sinit_max_attempts   = sctp_max_retrans_init;\n\tsp->initmsg.sinit_max_init_timeo = sctp_rto_max;\n\n\t/* Initialize default RTO related parameters.  These parameters can\n\t * be modified for with the SCTP_RTOINFO socket option.\n\t */\n\tsp->rtoinfo.srto_initial = sctp_rto_initial;\n\tsp->rtoinfo.srto_max     = sctp_rto_max;\n\tsp->rtoinfo.srto_min     = sctp_rto_min;\n\n\t/* Initialize default association related parameters. These parameters\n\t * can be modified with the SCTP_ASSOCINFO socket option.\n\t */\n\tsp->assocparams.sasoc_asocmaxrxt = sctp_max_retrans_association;\n\tsp->assocparams.sasoc_number_peer_destinations = 0;\n\tsp->assocparams.sasoc_peer_rwnd = 0;\n\tsp->assocparams.sasoc_local_rwnd = 0;\n\tsp->assocparams.sasoc_cookie_life = sctp_valid_cookie_life;\n\n\t/* Initialize default event subscriptions. By default, all the\n\t * options are off.\n\t */\n\tmemset(&sp->subscribe, 0, sizeof(struct sctp_event_subscribe));\n\n\t/* Default Peer Address Parameters.  These defaults can\n\t * be modified via SCTP_PEER_ADDR_PARAMS\n\t */\n\tsp->hbinterval  = sctp_hb_interval;\n\tsp->pathmaxrxt  = sctp_max_retrans_path;\n\tsp->pathmtu     = 0; // allow default discovery\n\tsp->sackdelay   = sctp_sack_timeout;\n\tsp->sackfreq\t= 2;\n\tsp->param_flags = SPP_HB_ENABLE |\n\t\t\t  SPP_PMTUD_ENABLE |\n\t\t\t  SPP_SACKDELAY_ENABLE;\n\n\t/* If enabled no SCTP message fragmentation will be performed.\n\t * Configure through SCTP_DISABLE_FRAGMENTS socket option.\n\t */\n\tsp->disable_fragments = 0;\n\n\t/* Enable Nagle algorithm by default.  */\n\tsp->nodelay           = 0;\n\n\t/* Enable by default. */\n\tsp->v4mapped          = 1;\n\n\t/* Auto-close idle associations after the configured\n\t * number of seconds.  A value of 0 disables this\n\t * feature.  Configure through the SCTP_AUTOCLOSE socket option,\n\t * for UDP-style sockets only.\n\t */\n\tsp->autoclose         = 0;\n\n\t/* User specified fragmentation limit. */\n\tsp->user_frag         = 0;\n\n\tsp->adaptation_ind = 0;\n\n\tsp->pf = sctp_get_pf_specific(sk->sk_family);\n\n\t/* Control variables for partial data delivery. */\n\tatomic_set(&sp->pd_mode, 0);\n\tskb_queue_head_init(&sp->pd_lobby);\n\tsp->frag_interleave = 0;\n\n\t/* Create a per socket endpoint structure.  Even if we\n\t * change the data structure relationships, this may still\n\t * be useful for storing pre-connect address information.\n\t */\n\tep = sctp_endpoint_new(sk, GFP_KERNEL);\n\tif (!ep)\n\t\treturn -ENOMEM;\n\n\tsp->ep = ep;\n\tsp->hmac = NULL;\n\n\tSCTP_DBG_OBJCNT_INC(sock);\n\tpercpu_counter_inc(&sctp_sockets_allocated);\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\tlocal_bh_enable();\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -117,9 +117,6 @@\n \tSCTP_DBG_OBJCNT_INC(sock);\n \tpercpu_counter_inc(&sctp_sockets_allocated);\n \n-\t/* Set socket backlog limit. */\n-\tsk->sk_backlog.limit = sysctl_sctp_rmem[1];\n-\n \tlocal_bh_disable();\n \tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n \tlocal_bh_enable();",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* Set socket backlog limit. */",
                "\tsk->sk_backlog.limit = sysctl_sctp_rmem[1];",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-10047",
        "func_name": "ImageMagick/GetMagickModulePath",
        "description": "Memory leak in the NewXMLTree function in magick/xml-tree.c in ImageMagick before 6.9.4-7 allows remote attackers to cause a denial of service (memory consumption) via a crafted XML file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/fc6080f1321fd21e86ef916195cc110b05d9effb",
        "commit_title": "Coder path traversal is not authorized, bug report provided by Masaaki Chida",
        "commit_text": "",
        "func_before": "static MagickBooleanType GetMagickModulePath(const char *filename,\n  MagickModuleType module_type,char *path,ExceptionInfo *exception)\n{\n  char\n    *module_path;\n\n  assert(filename != (const char *) NULL);\n  (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",filename);\n  assert(path != (char *) NULL);\n  assert(exception != (ExceptionInfo *) NULL);\n  (void) CopyMagickString(path,filename,MaxTextExtent);\n  module_path=(char *) NULL;\n  switch (module_type)\n  {\n    case MagickImageCoderModule:\n    default:\n    {\n      (void) LogMagickEvent(ModuleEvent,GetMagickModule(),\n        \"Searching for coder module file \\\"%s\\\" ...\",filename);\n      module_path=GetEnvironmentValue(\"MAGICK_CODER_MODULE_PATH\");\n#if defined(MAGICKCORE_CODER_PATH)\n      if (module_path == (char *) NULL)\n        module_path=AcquireString(MAGICKCORE_CODER_PATH);\n#endif\n      break;\n    }\n    case MagickImageFilterModule:\n    {\n      (void) LogMagickEvent(ModuleEvent,GetMagickModule(),\n        \"Searching for filter module file \\\"%s\\\" ...\",filename);\n      module_path=GetEnvironmentValue(\"MAGICK_CODER_FILTER_PATH\");\n#if defined(MAGICKCORE_FILTER_PATH)\n      if (module_path == (char *) NULL)\n        module_path=AcquireString(MAGICKCORE_FILTER_PATH);\n#endif\n      break;\n    }\n  }\n  if (module_path != (char *) NULL)\n    {\n      register char\n        *p,\n        *q;\n\n      for (p=module_path-1; p != (char *) NULL; )\n      {\n        (void) CopyMagickString(path,p+1,MaxTextExtent);\n        q=strchr(path,DirectoryListSeparator);\n        if (q != (char *) NULL)\n          *q='\\0';\n        q=path+strlen(path)-1;\n        if ((q >= path) && (*q != *DirectorySeparator))\n          (void) ConcatenateMagickString(path,DirectorySeparator,MaxTextExtent);\n        (void) ConcatenateMagickString(path,filename,MaxTextExtent);\n        if (IsPathAccessible(path) != MagickFalse)\n          {\n            module_path=DestroyString(module_path);\n            return(MagickTrue);\n          }\n        p=strchr(p+1,DirectoryListSeparator);\n      }\n      module_path=DestroyString(module_path);\n    }\n#if defined(MAGICKCORE_INSTALLED_SUPPORT)\n  else\n#if defined(MAGICKCORE_CODER_PATH)\n    {\n      const char\n        *directory;\n\n      /*\n        Search hard coded paths.\n      */\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          directory=MAGICKCORE_CODER_PATH;\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          directory=MAGICKCORE_FILTER_PATH;\n          break;\n        }\n      }\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s\",directory,filename);\n      if (IsPathAccessible(path) == MagickFalse)\n        {\n          ThrowFileException(exception,ConfigureWarning,\n            \"UnableToOpenModuleFile\",path);\n          return(MagickFalse);\n        }\n      return(MagickTrue);\n    }\n#else\n#if defined(MAGICKCORE_WINDOWS_SUPPORT)\n    {\n      const char\n        *registery_key;\n\n      unsigned char\n        *key_value;\n\n      /*\n        Locate path via registry key.\n      */\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          registery_key=\"CoderModulesPath\";\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          registery_key=\"FilterModulesPath\";\n          break;\n        }\n      }\n      key_value=NTRegistryKeyLookup(registery_key);\n      if (key_value == (unsigned char *) NULL)\n        {\n          ThrowMagickException(exception,GetMagickModule(),ConfigureError,\n            \"RegistryKeyLookupFailed\",\"`%s'\",registery_key);\n          return(MagickFalse);\n        }\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",(char *) key_value,\n        DirectorySeparator,filename);\n      key_value=(unsigned char *) RelinquishMagickMemory(key_value);\n      if (IsPathAccessible(path) == MagickFalse)\n        {\n          ThrowFileException(exception,ConfigureWarning,\n            \"UnableToOpenModuleFile\",path);\n          return(MagickFalse);\n        }\n      return(MagickTrue);\n    }\n#endif\n#endif\n#if !defined(MAGICKCORE_CODER_PATH) && !defined(MAGICKCORE_WINDOWS_SUPPORT)\n# error MAGICKCORE_CODER_PATH or MAGICKCORE_WINDOWS_SUPPORT must be defined when MAGICKCORE_INSTALLED_SUPPORT is defined\n#endif\n#else\n  {\n    char\n      *home;\n\n    home=GetEnvironmentValue(\"MAGICK_HOME\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search MAGICK_HOME.\n        */\n#if !defined(MAGICKCORE_POSIX_SUPPORT)\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",home,\n          DirectorySeparator,filename);\n#else\n        const char\n          *directory;\n\n        switch (module_type)\n        {\n          case MagickImageCoderModule:\n          default:\n          {\n            directory=MAGICKCORE_CODER_RELATIVE_PATH;\n            break;\n          }\n          case MagickImageFilterModule:\n          {\n            directory=MAGICKCORE_FILTER_RELATIVE_PATH;\n            break;\n          }\n        }\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s/lib/%s/%s\",home,\n          directory,filename);\n#endif\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n  if (*GetClientPath() != '\\0')\n    {\n      /*\n        Search based on executable directory.\n      */\n#if !defined(MAGICKCORE_POSIX_SUPPORT)\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",GetClientPath(),\n        DirectorySeparator,filename);\n#else\n      char\n        prefix[MaxTextExtent];\n\n      const char\n        *directory;\n\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          directory=\"coders\";\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          directory=\"filters\";\n          break;\n        }\n      }\n      (void) CopyMagickString(prefix,GetClientPath(),MaxTextExtent);\n      ChopPathComponents(prefix,1);\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s/lib/%s/%s/%s\",prefix,\n        MAGICKCORE_MODULES_RELATIVE_PATH,directory,filename);\n#endif\n      if (IsPathAccessible(path) != MagickFalse)\n        return(MagickTrue);\n    }\n#if defined(MAGICKCORE_WINDOWS_SUPPORT)\n  {\n    /*\n      Search module path.\n    */\n    if ((NTGetModulePath(\"CORE_RL_magick_.dll\",path) != MagickFalse) ||\n        (NTGetModulePath(\"CORE_DB_magick_.dll\",path) != MagickFalse) ||\n        (NTGetModulePath(\"Magick.dll\",path) != MagickFalse))\n      {\n        (void) ConcatenateMagickString(path,DirectorySeparator,MaxTextExtent);\n        (void) ConcatenateMagickString(path,filename,MaxTextExtent);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n#endif\n  {\n    char\n      *home;\n\n    home=GetEnvironmentValue(\"XDG_CONFIG_HOME\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"LOCALAPPDATA\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"APPDATA\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"USERPROFILE\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search $XDG_CONFIG_HOME/ImageMagick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%sImageMagick%s%s\",\n          home,DirectorySeparator,DirectorySeparator,filename);\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n    home=GetEnvironmentValue(\"HOME\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search $HOME/.config/ImageMagick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\n          \"%s%s.config%sImageMagick%s%s\",home,DirectorySeparator,\n          DirectorySeparator,DirectorySeparator,filename);\n        if (IsPathAccessible(path) != MagickFalse)\n          {\n            home=DestroyString(home);\n            return(MagickTrue);\n          }\n        /*\n          Search $HOME/.magick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%s.magick%s%s\",home,\n          DirectorySeparator,DirectorySeparator,filename);\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n  /*\n    Search current directory.\n  */\n  if (IsPathAccessible(path) != MagickFalse)\n    return(MagickTrue);\n  if (exception->severity < ConfigureError)\n    ThrowFileException(exception,ConfigureWarning,\"UnableToOpenModuleFile\",\n      path);\n#endif\n  return(MagickFalse);\n}",
        "func": "static MagickBooleanType GetMagickModulePath(const char *filename,\n  MagickModuleType module_type,char *path,ExceptionInfo *exception)\n{\n  char\n    *module_path;\n\n  assert(filename != (const char *) NULL);\n  (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",filename);\n  assert(path != (char *) NULL);\n  assert(exception != (ExceptionInfo *) NULL);\n  (void) CopyMagickString(path,filename,MaxTextExtent);\n#if defined(MAGICKCORE_INSTALLED_SUPPORT)\n  if (strstr(path,\"../\") != (char *) NULL)\n    {\n      errno=EPERM;\n      (void) ThrowMagickException(exception,GetMagickModule(),PolicyError,\n        \"NotAuthorized\",\"`%s'\",path);\n      return(MagickFalse);\n    }\n#endif\n  module_path=(char *) NULL;\n  switch (module_type)\n  {\n    case MagickImageCoderModule:\n    default:\n    {\n      (void) LogMagickEvent(ModuleEvent,GetMagickModule(),\n        \"Searching for coder module file \\\"%s\\\" ...\",filename);\n      module_path=GetEnvironmentValue(\"MAGICK_CODER_MODULE_PATH\");\n#if defined(MAGICKCORE_CODER_PATH)\n      if (module_path == (char *) NULL)\n        module_path=AcquireString(MAGICKCORE_CODER_PATH);\n#endif\n      break;\n    }\n    case MagickImageFilterModule:\n    {\n      (void) LogMagickEvent(ModuleEvent,GetMagickModule(),\n        \"Searching for filter module file \\\"%s\\\" ...\",filename);\n      module_path=GetEnvironmentValue(\"MAGICK_CODER_FILTER_PATH\");\n#if defined(MAGICKCORE_FILTER_PATH)\n      if (module_path == (char *) NULL)\n        module_path=AcquireString(MAGICKCORE_FILTER_PATH);\n#endif\n      break;\n    }\n  }\n  if (module_path != (char *) NULL)\n    {\n      register char\n        *p,\n        *q;\n\n      for (p=module_path-1; p != (char *) NULL; )\n      {\n        (void) CopyMagickString(path,p+1,MaxTextExtent);\n        q=strchr(path,DirectoryListSeparator);\n        if (q != (char *) NULL)\n          *q='\\0';\n        q=path+strlen(path)-1;\n        if ((q >= path) && (*q != *DirectorySeparator))\n          (void) ConcatenateMagickString(path,DirectorySeparator,MaxTextExtent);\n        (void) ConcatenateMagickString(path,filename,MaxTextExtent);\n        if (IsPathAccessible(path) != MagickFalse)\n          {\n            module_path=DestroyString(module_path);\n            return(MagickTrue);\n          }\n        p=strchr(p+1,DirectoryListSeparator);\n      }\n      module_path=DestroyString(module_path);\n    }\n#if defined(MAGICKCORE_INSTALLED_SUPPORT)\n  else\n#if defined(MAGICKCORE_CODER_PATH)\n    {\n      const char\n        *directory;\n\n      /*\n        Search hard coded paths.\n      */\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          directory=MAGICKCORE_CODER_PATH;\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          directory=MAGICKCORE_FILTER_PATH;\n          break;\n        }\n      }\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s\",directory,filename);\n      if (IsPathAccessible(path) == MagickFalse)\n        {\n          ThrowFileException(exception,ConfigureWarning,\n            \"UnableToOpenModuleFile\",path);\n          return(MagickFalse);\n        }\n      return(MagickTrue);\n    }\n#else\n#if defined(MAGICKCORE_WINDOWS_SUPPORT)\n    {\n      const char\n        *registery_key;\n\n      unsigned char\n        *key_value;\n\n      /*\n        Locate path via registry key.\n      */\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          registery_key=\"CoderModulesPath\";\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          registery_key=\"FilterModulesPath\";\n          break;\n        }\n      }\n      key_value=NTRegistryKeyLookup(registery_key);\n      if (key_value == (unsigned char *) NULL)\n        {\n          ThrowMagickException(exception,GetMagickModule(),ConfigureError,\n            \"RegistryKeyLookupFailed\",\"`%s'\",registery_key);\n          return(MagickFalse);\n        }\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",(char *) key_value,\n        DirectorySeparator,filename);\n      key_value=(unsigned char *) RelinquishMagickMemory(key_value);\n      if (IsPathAccessible(path) == MagickFalse)\n        {\n          ThrowFileException(exception,ConfigureWarning,\n            \"UnableToOpenModuleFile\",path);\n          return(MagickFalse);\n        }\n      return(MagickTrue);\n    }\n#endif\n#endif\n#if !defined(MAGICKCORE_CODER_PATH) && !defined(MAGICKCORE_WINDOWS_SUPPORT)\n# error MAGICKCORE_CODER_PATH or MAGICKCORE_WINDOWS_SUPPORT must be defined when MAGICKCORE_INSTALLED_SUPPORT is defined\n#endif\n#else\n  {\n    char\n      *home;\n\n    home=GetEnvironmentValue(\"MAGICK_HOME\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search MAGICK_HOME.\n        */\n#if !defined(MAGICKCORE_POSIX_SUPPORT)\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",home,\n          DirectorySeparator,filename);\n#else\n        const char\n          *directory;\n\n        switch (module_type)\n        {\n          case MagickImageCoderModule:\n          default:\n          {\n            directory=MAGICKCORE_CODER_RELATIVE_PATH;\n            break;\n          }\n          case MagickImageFilterModule:\n          {\n            directory=MAGICKCORE_FILTER_RELATIVE_PATH;\n            break;\n          }\n        }\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s/lib/%s/%s\",home,\n          directory,filename);\n#endif\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n  if (*GetClientPath() != '\\0')\n    {\n      /*\n        Search based on executable directory.\n      */\n#if !defined(MAGICKCORE_POSIX_SUPPORT)\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s%s%s\",GetClientPath(),\n        DirectorySeparator,filename);\n#else\n      char\n        prefix[MaxTextExtent];\n\n      const char\n        *directory;\n\n      switch (module_type)\n      {\n        case MagickImageCoderModule:\n        default:\n        {\n          directory=\"coders\";\n          break;\n        }\n        case MagickImageFilterModule:\n        {\n          directory=\"filters\";\n          break;\n        }\n      }\n      (void) CopyMagickString(prefix,GetClientPath(),MaxTextExtent);\n      ChopPathComponents(prefix,1);\n      (void) FormatLocaleString(path,MaxTextExtent,\"%s/lib/%s/%s/%s\",prefix,\n        MAGICKCORE_MODULES_RELATIVE_PATH,directory,filename);\n#endif\n      if (IsPathAccessible(path) != MagickFalse)\n        return(MagickTrue);\n    }\n#if defined(MAGICKCORE_WINDOWS_SUPPORT)\n  {\n    /*\n      Search module path.\n    */\n    if ((NTGetModulePath(\"CORE_RL_magick_.dll\",path) != MagickFalse) ||\n        (NTGetModulePath(\"CORE_DB_magick_.dll\",path) != MagickFalse) ||\n        (NTGetModulePath(\"Magick.dll\",path) != MagickFalse))\n      {\n        (void) ConcatenateMagickString(path,DirectorySeparator,MaxTextExtent);\n        (void) ConcatenateMagickString(path,filename,MaxTextExtent);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n#endif\n  {\n    char\n      *home;\n\n    home=GetEnvironmentValue(\"XDG_CONFIG_HOME\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"LOCALAPPDATA\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"APPDATA\");\n    if (home == (char *) NULL)\n      home=GetEnvironmentValue(\"USERPROFILE\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search $XDG_CONFIG_HOME/ImageMagick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%sImageMagick%s%s\",\n          home,DirectorySeparator,DirectorySeparator,filename);\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n    home=GetEnvironmentValue(\"HOME\");\n    if (home != (char *) NULL)\n      {\n        /*\n          Search $HOME/.config/ImageMagick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\n          \"%s%s.config%sImageMagick%s%s\",home,DirectorySeparator,\n          DirectorySeparator,DirectorySeparator,filename);\n        if (IsPathAccessible(path) != MagickFalse)\n          {\n            home=DestroyString(home);\n            return(MagickTrue);\n          }\n        /*\n          Search $HOME/.magick.\n        */\n        (void) FormatLocaleString(path,MaxTextExtent,\"%s%s.magick%s%s\",home,\n          DirectorySeparator,DirectorySeparator,filename);\n        home=DestroyString(home);\n        if (IsPathAccessible(path) != MagickFalse)\n          return(MagickTrue);\n      }\n  }\n  /*\n    Search current directory.\n  */\n  if (IsPathAccessible(path) != MagickFalse)\n    return(MagickTrue);\n  if (exception->severity < ConfigureError)\n    ThrowFileException(exception,ConfigureWarning,\"UnableToOpenModuleFile\",\n      path);\n#endif\n  return(MagickFalse);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,15 @@\n   assert(path != (char *) NULL);\n   assert(exception != (ExceptionInfo *) NULL);\n   (void) CopyMagickString(path,filename,MaxTextExtent);\n+#if defined(MAGICKCORE_INSTALLED_SUPPORT)\n+  if (strstr(path,\"../\") != (char *) NULL)\n+    {\n+      errno=EPERM;\n+      (void) ThrowMagickException(exception,GetMagickModule(),PolicyError,\n+        \"NotAuthorized\",\"`%s'\",path);\n+      return(MagickFalse);\n+    }\n+#endif\n   module_path=(char *) NULL;\n   switch (module_type)\n   {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "#if defined(MAGICKCORE_INSTALLED_SUPPORT)",
                "  if (strstr(path,\"../\") != (char *) NULL)",
                "    {",
                "      errno=EPERM;",
                "      (void) ThrowMagickException(exception,GetMagickModule(),PolicyError,",
                "        \"NotAuthorized\",\"`%s'\",path);",
                "      return(MagickFalse);",
                "    }",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10047",
        "func_name": "ImageMagick/NewXMLTree",
        "description": "Memory leak in the NewXMLTree function in magick/xml-tree.c in ImageMagick before 6.9.4-7 allows remote attackers to cause a denial of service (memory consumption) via a crafted XML file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/fc6080f1321fd21e86ef916195cc110b05d9effb",
        "commit_title": "Coder path traversal is not authorized, bug report provided by Masaaki Chida",
        "commit_text": "",
        "func_before": "MagickExport XMLTreeInfo *NewXMLTree(const char *xml,ExceptionInfo *exception)\n{\n  char\n    **attribute,\n    **attributes,\n    *tag,\n    *utf8;\n\n  int\n    c,\n    terminal;\n\n  MagickBooleanType\n    status;\n\n  register char\n    *p;\n\n  register ssize_t\n    i;\n\n  size_t\n    ignore_depth,\n    length;\n\n  ssize_t\n    j,\n    l;\n\n  XMLTreeRoot\n    *root;\n\n  /*\n    Convert xml-string to UTF8.\n  */\n  if ((xml == (const char *) NULL) || (strlen(xml) == 0))\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      return((XMLTreeInfo *) NULL);\n    }\n  root=(XMLTreeRoot *) NewXMLTreeTag((char *) NULL);\n  length=strlen(xml);\n  utf8=ConvertUTF16ToUTF8(xml,&length);\n  if (utf8 == (char *) NULL)\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"UTF16 to UTF8 failed\");\n      return((XMLTreeInfo *) NULL);\n    }\n  terminal=utf8[length-1];\n  utf8[length-1]='\\0';\n  p=utf8;\n  while ((*p != '\\0') && (*p != '<'))\n    p++;\n  if (*p == '\\0')\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      utf8=DestroyString(utf8);\n      return((XMLTreeInfo *) NULL);\n    }\n  attribute=(char **) NULL;\n  l=0;\n  ignore_depth=0;\n  for (p++; ; p++)\n  {\n    attributes=(char **) sentinel;\n    tag=p;\n    c=(*p);\n    if ((isalpha((int) ((unsigned char) *p)) !=0) || (*p == '_') ||\n        (*p == ':') || (c < '\\0'))\n      {\n        /*\n          Tag.\n        */\n        if (root->node == (XMLTreeInfo *) NULL)\n          {\n            (void) ThrowMagickException(exception,GetMagickModule(),\n              OptionWarning,\"ParseError\",\"root tag missing\");\n            utf8=DestroyString(utf8);\n            return(&root->root);\n          }\n        p+=strcspn(p,XMLWhitespace \"/>\");\n        while (isspace((int) ((unsigned char) *p)) != 0)\n          *p++='\\0';\n        if (ignore_depth == 0)\n          {\n            if ((*p != '\\0') && (*p != '/') && (*p != '>'))\n              {\n                /*\n                  Find tag in default attributes list.\n                */\n                i=0;\n                while ((root->attributes[i] != (char **) NULL) &&\n                       (strcmp(root->attributes[i][0],tag) != 0))\n                  i++;\n                attribute=root->attributes[i];\n              }\n            for (l=0; (*p != '\\0') && (*p != '/') && (*p != '>'); l+=2)\n            {\n              /*\n                Attribute.\n              */\n              if (l == 0)\n                attributes=(char **) AcquireQuantumMemory(4,\n                  sizeof(*attributes));\n              else\n                attributes=(char **) ResizeQuantumMemory(attributes,\n                  (size_t) (l+4),sizeof(*attributes));\n              if (attributes == (char **) NULL)\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    ResourceLimitError,\"MemoryAllocationFailed\",\"`%s'\",\"\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n              attributes[l+2]=(char *) NULL;\n              attributes[l+1]=(char *) NULL;\n              attributes[l]=p;\n              p+=strcspn(p,XMLWhitespace \"=/>\");\n              if ((*p != '=') && (isspace((int) ((unsigned char) *p)) == 0))\n                attributes[l]=ConstantString(\"\");\n              else\n                {\n                  *p++='\\0';\n                  p+=strspn(p,XMLWhitespace \"=\");\n                  c=(*p);\n                  if ((c == '\"') || (c == '\\''))\n                    {\n                      /*\n                        Attributes value.\n                      */\n                      p++;\n                      attributes[l+1]=p;\n                      while ((*p != '\\0') && (*p != c))\n                        p++;\n                      if (*p != '\\0')\n                        *p++='\\0';\n                      else\n                        {\n                          attributes[l]=ConstantString(\"\");\n                          attributes[l+1]=ConstantString(\"\");\n                          (void) DestroyXMLTreeAttributes(attributes);\n                          (void) ThrowMagickException(exception,\n                            GetMagickModule(),OptionWarning,\"ParseError\",\n                            \"missing %c\",c);\n                          utf8=DestroyString(utf8);\n                          return(&root->root);\n                        }\n                      j=1;\n                      while ((attribute != (char **) NULL) &&\n                             (attribute[j] != (char *) NULL) &&\n                             (strcmp(attribute[j],attributes[l]) != 0))\n                        j+=3;\n                      attributes[l+1]=ParseEntities(attributes[l+1],\n                        root->entities,(attribute != (char **) NULL) &&\n                        (attribute[j] != (char *) NULL) ? *attribute[j+2] :\n                        ' ');\n                    }\n                  attributes[l]=ConstantString(attributes[l]);\n                }\n              while (isspace((int) ((unsigned char) *p)) != 0)\n                p++;\n            }\n          }\n        else\n          {\n            while((*p != '\\0') && (*p != '/') && (*p != '>'))\n              p++;\n          }\n        if (*p == '/')\n          {\n            /*\n              Self closing tag.\n            */\n            *p++='\\0';\n            if (((*p != '\\0') && (*p != '>')) ||\n                ((*p == '\\0') && (terminal != '>')))\n              {\n                if (l != 0)\n                  (void) DestroyXMLTreeAttributes(attributes);\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"missing >\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n            if ((ignore_depth == 0) && (IsSkipTag(tag) == MagickFalse))\n              {\n                ParseOpenTag(root,tag,attributes);\n                (void) ParseCloseTag(root,tag,exception);\n              }\n          }\n        else\n          {\n            c=(*p);\n            if ((*p == '>') || ((*p == '\\0') && (terminal == '>')))\n              {\n                *p='\\0';\n                if ((ignore_depth == 0) && (IsSkipTag(tag) == MagickFalse))\n                  ParseOpenTag(root,tag,attributes);\n                else\n                  ignore_depth++;\n                *p=c;\n              }\n            else\n              {\n                if (l != 0)\n                  (void) DestroyXMLTreeAttributes(attributes);\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"missing >\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n          }\n      }\n    else\n      if (*p == '/')\n        {\n          /*\n            Close tag.\n          */\n          tag=p+1;\n          p+=strcspn(tag,XMLWhitespace \">\")+1;\n          c=(*p);\n          if ((c == '\\0') && (terminal != '>'))\n            {\n              (void) ThrowMagickException(exception,GetMagickModule(),\n                OptionWarning,\"ParseError\",\"missing >\");\n              utf8=DestroyString(utf8);\n              return(&root->root);\n            }\n          *p='\\0';\n          if (ignore_depth == 0 && ParseCloseTag(root,tag,exception) !=\n              (XMLTreeInfo *) NULL)\n            {\n              utf8=DestroyString(utf8);\n              return(&root->root);\n            }\n          if (ignore_depth > 0)\n            ignore_depth--;\n          *p=c;\n          if (isspace((int) ((unsigned char) *p)) != 0)\n            p+=strspn(p,XMLWhitespace);\n        }\n      else\n        if (strncmp(p,\"!--\",3) == 0)\n          {\n            /*\n              Comment.\n            */\n            p=strstr(p+3,\"--\");\n            if ((p == (char *) NULL) || ((*(p+=2) != '>') && (*p != '\\0')) ||\n                ((*p == '\\0') && (terminal != '>')))\n              {\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"unclosed <!--\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n          }\n        else\n          if (strncmp(p,\"![CDATA[\",8) == 0)\n            {\n              /*\n                Cdata.\n              */\n              p=strstr(p,\"]]>\");\n              if (p != (char *) NULL)\n                {\n                  p+=2;\n                  if (ignore_depth == 0)\n                    ParseCharacterContent(root,tag+8,(size_t) (p-tag-10),'c');\n                }\n              else\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    OptionWarning,\"ParseError\",\"unclosed <![CDATA[\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n            }\n          else\n            if (strncmp(p,\"!DOCTYPE\",8) == 0)\n              {\n                /*\n                  DTD.\n                */\n                for (l=0; (*p != '\\0') && (((l == 0) && (*p != '>')) ||\n                     ((l != 0) && ((*p != ']') ||\n                     (*(p+strspn(p+1,XMLWhitespace)+1) != '>'))));\n                  l=(ssize_t) ((*p == '[') ? 1 : l))\n                p+=strcspn(p+1,\"[]>\")+1;\n                if ((*p == '\\0') && (terminal != '>'))\n                  {\n                    (void) ThrowMagickException(exception,GetMagickModule(),\n                      OptionWarning,\"ParseError\",\"unclosed <!DOCTYPE\");\n                    utf8=DestroyString(utf8);\n                    return(&root->root);\n                  }\n                if (l != 0)\n                  tag=strchr(tag,'[')+1;\n                if (l != 0)\n                  {\n                    status=ParseInternalDoctype(root,tag,(size_t) (p-tag),\n                      exception);\n                    if (status == MagickFalse)\n                      {\n                        utf8=DestroyString(utf8);\n                        return(&root->root);\n                      }\n                    p++;\n                  }\n              }\n            else\n              if (*p == '?')\n                {\n                  /*\n                    Processing instructions.\n                  */\n                  do\n                  {\n                    p=strchr(p,'?');\n                    if (p == (char *) NULL)\n                      break;\n                    p++;\n                  } while ((*p != '\\0') && (*p != '>'));\n                  if ((p == (char *) NULL) || ((*p == '\\0') &&\n                      (terminal != '>')))\n                    {\n                      (void) ThrowMagickException(exception,GetMagickModule(),\n                        OptionWarning,\"ParseError\",\"unclosed <?\");\n                      utf8=DestroyString(utf8);\n                      return(&root->root);\n                    }\n                  ParseProcessingInstructions(root,tag+1,(size_t) (p-tag-2));\n                }\n              else\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    OptionWarning,\"ParseError\",\"unexpected <\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n     if ((p == (char *) NULL) || (*p == '\\0'))\n       break;\n     *p++='\\0';\n     tag=p;\n     if ((*p != '\\0') && (*p != '<'))\n       {\n        /*\n          Tag character content.\n        */\n        while ((*p != '\\0') && (*p != '<'))\n          p++;\n        if (*p == '\\0')\n          break;\n        if (ignore_depth == 0)\n          ParseCharacterContent(root,tag,(size_t) (p-tag),'&');\n      }\n    else\n      if (*p == '\\0')\n        break;\n  }\n  utf8=DestroyString(utf8);\n  if (root->node == (XMLTreeInfo *) NULL)\n    return(&root->root);\n  if (root->node->tag == (char *) NULL)\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      return(&root->root);\n    }\n  (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n    \"ParseError\",\"unclosed tag: `%s'\",root->node->tag);\n  return(&root->root);\n}",
        "func": "MagickExport XMLTreeInfo *NewXMLTree(const char *xml,ExceptionInfo *exception)\n{\n  char\n    **attribute,\n    **attributes,\n    *tag,\n    *utf8;\n\n  int\n    c,\n    terminal;\n\n  MagickBooleanType\n    status;\n\n  register char\n    *p;\n\n  register ssize_t\n    i;\n\n  size_t\n    ignore_depth,\n    length;\n\n  ssize_t\n    j,\n    l;\n\n  XMLTreeRoot\n    *root;\n\n  /*\n    Convert xml-string to UTF8.\n  */\n  if ((xml == (const char *) NULL) || (strlen(xml) == 0))\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      return((XMLTreeInfo *) NULL);\n    }\n  root=(XMLTreeRoot *) NewXMLTreeTag((char *) NULL);\n  length=strlen(xml);\n  utf8=ConvertUTF16ToUTF8(xml,&length);\n  if (utf8 == (char *) NULL)\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"UTF16 to UTF8 failed\");\n      return((XMLTreeInfo *) NULL);\n    }\n  terminal=utf8[length-1];\n  utf8[length-1]='\\0';\n  p=utf8;\n  while ((*p != '\\0') && (*p != '<'))\n    p++;\n  if (*p == '\\0')\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      utf8=DestroyString(utf8);\n      return((XMLTreeInfo *) NULL);\n    }\n  attribute=(char **) NULL;\n  l=0;\n  ignore_depth=0;\n  for (p++; ; p++)\n  {\n    attributes=(char **) sentinel;\n    tag=p;\n    c=(*p);\n    if ((isalpha((int) ((unsigned char) *p)) !=0) || (*p == '_') ||\n        (*p == ':') || (c < '\\0'))\n      {\n        /*\n          Tag.\n        */\n        if (root->node == (XMLTreeInfo *) NULL)\n          {\n            (void) ThrowMagickException(exception,GetMagickModule(),\n              OptionWarning,\"ParseError\",\"root tag missing\");\n            utf8=DestroyString(utf8);\n            return(&root->root);\n          }\n        p+=strcspn(p,XMLWhitespace \"/>\");\n        while (isspace((int) ((unsigned char) *p)) != 0)\n          *p++='\\0';\n        if (ignore_depth == 0)\n          {\n            if ((*p != '\\0') && (*p != '/') && (*p != '>'))\n              {\n                /*\n                  Find tag in default attributes list.\n                */\n                i=0;\n                while ((root->attributes[i] != (char **) NULL) &&\n                       (strcmp(root->attributes[i][0],tag) != 0))\n                  i++;\n                attribute=root->attributes[i];\n              }\n            for (l=0; (*p != '\\0') && (*p != '/') && (*p != '>'); l+=2)\n            {\n              /*\n                Attribute.\n              */\n              if (l == 0)\n                attributes=(char **) AcquireQuantumMemory(4,\n                  sizeof(*attributes));\n              else\n                attributes=(char **) ResizeQuantumMemory(attributes,\n                  (size_t) (l+4),sizeof(*attributes));\n              if (attributes == (char **) NULL)\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    ResourceLimitError,\"MemoryAllocationFailed\",\"`%s'\",\"\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n              attributes[l+2]=(char *) NULL;\n              attributes[l+1]=(char *) NULL;\n              attributes[l]=p;\n              p+=strcspn(p,XMLWhitespace \"=/>\");\n              if ((*p != '=') && (isspace((int) ((unsigned char) *p)) == 0))\n                attributes[l]=ConstantString(\"\");\n              else\n                {\n                  *p++='\\0';\n                  p+=strspn(p,XMLWhitespace \"=\");\n                  c=(*p);\n                  if ((c == '\"') || (c == '\\''))\n                    {\n                      /*\n                        Attributes value.\n                      */\n                      p++;\n                      attributes[l+1]=p;\n                      while ((*p != '\\0') && (*p != c))\n                        p++;\n                      if (*p != '\\0')\n                        *p++='\\0';\n                      else\n                        {\n                          attributes[l]=ConstantString(\"\");\n                          attributes[l+1]=ConstantString(\"\");\n                          (void) DestroyXMLTreeAttributes(attributes);\n                          (void) ThrowMagickException(exception,\n                            GetMagickModule(),OptionWarning,\"ParseError\",\n                            \"missing %c\",c);\n                          utf8=DestroyString(utf8);\n                          return(&root->root);\n                        }\n                      j=1;\n                      while ((attribute != (char **) NULL) &&\n                             (attribute[j] != (char *) NULL) &&\n                             (strcmp(attribute[j],attributes[l]) != 0))\n                        j+=3;\n                      attributes[l+1]=ParseEntities(attributes[l+1],\n                        root->entities,(attribute != (char **) NULL) &&\n                        (attribute[j] != (char *) NULL) ? *attribute[j+2] :\n                        ' ');\n                    }\n                  attributes[l]=ConstantString(attributes[l]);\n                }\n              while (isspace((int) ((unsigned char) *p)) != 0)\n                p++;\n            }\n          }\n        else\n          {\n            while((*p != '\\0') && (*p != '/') && (*p != '>'))\n              p++;\n          }\n        if (*p == '/')\n          {\n            /*\n              Self closing tag.\n            */\n            *p++='\\0';\n            if (((*p != '\\0') && (*p != '>')) ||\n                ((*p == '\\0') && (terminal != '>')))\n              {\n                if (l != 0)\n                  (void) DestroyXMLTreeAttributes(attributes);\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"missing >\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n            if ((ignore_depth == 0) && (IsSkipTag(tag) == MagickFalse))\n              {\n                ParseOpenTag(root,tag,attributes);\n                (void) ParseCloseTag(root,tag,exception);\n              }\n          }\n        else\n          {\n            c=(*p);\n            if ((*p == '>') || ((*p == '\\0') && (terminal == '>')))\n              {\n                *p='\\0';\n                if ((ignore_depth == 0) && (IsSkipTag(tag) == MagickFalse))\n                  ParseOpenTag(root,tag,attributes);\n                else\n                  {\n                    ignore_depth++;\n                    (void) DestroyXMLTreeAttributes(attributes);\n                  }\n                *p=c;\n              }\n            else\n              {\n                if (l != 0)\n                  (void) DestroyXMLTreeAttributes(attributes);\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"missing >\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n          }\n      }\n    else\n      if (*p == '/')\n        {\n          /*\n            Close tag.\n          */\n          tag=p+1;\n          p+=strcspn(tag,XMLWhitespace \">\")+1;\n          c=(*p);\n          if ((c == '\\0') && (terminal != '>'))\n            {\n              (void) ThrowMagickException(exception,GetMagickModule(),\n                OptionWarning,\"ParseError\",\"missing >\");\n              utf8=DestroyString(utf8);\n              return(&root->root);\n            }\n          *p='\\0';\n          if (ignore_depth == 0 && ParseCloseTag(root,tag,exception) !=\n              (XMLTreeInfo *) NULL)\n            {\n              utf8=DestroyString(utf8);\n              return(&root->root);\n            }\n          if (ignore_depth > 0)\n            ignore_depth--;\n          *p=c;\n          if (isspace((int) ((unsigned char) *p)) != 0)\n            p+=strspn(p,XMLWhitespace);\n        }\n      else\n        if (strncmp(p,\"!--\",3) == 0)\n          {\n            /*\n              Comment.\n            */\n            p=strstr(p+3,\"--\");\n            if ((p == (char *) NULL) || ((*(p+=2) != '>') && (*p != '\\0')) ||\n                ((*p == '\\0') && (terminal != '>')))\n              {\n                (void) ThrowMagickException(exception,GetMagickModule(),\n                  OptionWarning,\"ParseError\",\"unclosed <!--\");\n                utf8=DestroyString(utf8);\n                return(&root->root);\n              }\n          }\n        else\n          if (strncmp(p,\"![CDATA[\",8) == 0)\n            {\n              /*\n                Cdata.\n              */\n              p=strstr(p,\"]]>\");\n              if (p != (char *) NULL)\n                {\n                  p+=2;\n                  if (ignore_depth == 0)\n                    ParseCharacterContent(root,tag+8,(size_t) (p-tag-10),'c');\n                }\n              else\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    OptionWarning,\"ParseError\",\"unclosed <![CDATA[\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n            }\n          else\n            if (strncmp(p,\"!DOCTYPE\",8) == 0)\n              {\n                /*\n                  DTD.\n                */\n                for (l=0; (*p != '\\0') && (((l == 0) && (*p != '>')) ||\n                     ((l != 0) && ((*p != ']') ||\n                     (*(p+strspn(p+1,XMLWhitespace)+1) != '>'))));\n                  l=(ssize_t) ((*p == '[') ? 1 : l))\n                p+=strcspn(p+1,\"[]>\")+1;\n                if ((*p == '\\0') && (terminal != '>'))\n                  {\n                    (void) ThrowMagickException(exception,GetMagickModule(),\n                      OptionWarning,\"ParseError\",\"unclosed <!DOCTYPE\");\n                    utf8=DestroyString(utf8);\n                    return(&root->root);\n                  }\n                if (l != 0)\n                  tag=strchr(tag,'[')+1;\n                if (l != 0)\n                  {\n                    status=ParseInternalDoctype(root,tag,(size_t) (p-tag),\n                      exception);\n                    if (status == MagickFalse)\n                      {\n                        utf8=DestroyString(utf8);\n                        return(&root->root);\n                      }\n                    p++;\n                  }\n              }\n            else\n              if (*p == '?')\n                {\n                  /*\n                    Processing instructions.\n                  */\n                  do\n                  {\n                    p=strchr(p,'?');\n                    if (p == (char *) NULL)\n                      break;\n                    p++;\n                  } while ((*p != '\\0') && (*p != '>'));\n                  if ((p == (char *) NULL) || ((*p == '\\0') &&\n                      (terminal != '>')))\n                    {\n                      (void) ThrowMagickException(exception,GetMagickModule(),\n                        OptionWarning,\"ParseError\",\"unclosed <?\");\n                      utf8=DestroyString(utf8);\n                      return(&root->root);\n                    }\n                  ParseProcessingInstructions(root,tag+1,(size_t) (p-tag-2));\n                }\n              else\n                {\n                  (void) ThrowMagickException(exception,GetMagickModule(),\n                    OptionWarning,\"ParseError\",\"unexpected <\");\n                  utf8=DestroyString(utf8);\n                  return(&root->root);\n                }\n     if ((p == (char *) NULL) || (*p == '\\0'))\n       break;\n     *p++='\\0';\n     tag=p;\n     if ((*p != '\\0') && (*p != '<'))\n       {\n        /*\n          Tag character content.\n        */\n        while ((*p != '\\0') && (*p != '<'))\n          p++;\n        if (*p == '\\0')\n          break;\n        if (ignore_depth == 0)\n          ParseCharacterContent(root,tag,(size_t) (p-tag),'&');\n      }\n    else\n      if (*p == '\\0')\n        break;\n  }\n  utf8=DestroyString(utf8);\n  if (root->node == (XMLTreeInfo *) NULL)\n    return(&root->root);\n  if (root->node->tag == (char *) NULL)\n    {\n      (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n        \"ParseError\",\"root tag missing\");\n      return(&root->root);\n    }\n  (void) ThrowMagickException(exception,GetMagickModule(),OptionWarning,\n    \"ParseError\",\"unclosed tag: `%s'\",root->node->tag);\n  return(&root->root);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -200,7 +200,10 @@\n                 if ((ignore_depth == 0) && (IsSkipTag(tag) == MagickFalse))\n                   ParseOpenTag(root,tag,attributes);\n                 else\n-                  ignore_depth++;\n+                  {\n+                    ignore_depth++;\n+                    (void) DestroyXMLTreeAttributes(attributes);\n+                  }\n                 *p=c;\n               }\n             else",
        "diff_line_info": {
            "deleted_lines": [
                "                  ignore_depth++;"
            ],
            "added_lines": [
                "                  {",
                "                    ignore_depth++;",
                "                    (void) DestroyXMLTreeAttributes(attributes);",
                "                  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-10058",
        "func_name": "ImageMagick/ReadPSDLayers",
        "description": "Memory leak in the ReadPSDLayers function in coders/psd.c in ImageMagick before 6.9.6-3 allows remote attackers to cause a denial of service (memory consumption) via a crafted image file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/4ec444f4eab88cf4bec664fafcf9cab50bc5ff6a",
        "commit_title": "Fixed memory leak.",
        "commit_text": "",
        "func_before": "ModuleExport MagickBooleanType ReadPSDLayers(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,\n  const MagickBooleanType skip_layers,ExceptionInfo *exception)\n{\n  char\n    type[4];\n\n  LayerInfo\n    *layer_info;\n\n  MagickSizeType\n    size;\n\n  MagickBooleanType\n    status;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count,\n    j,\n    number_layers;\n\n  size=GetPSDSize(psd_info,image);\n  if (size == 0)\n    {\n      /*\n        Skip layers & masks.\n      */\n      (void) ReadBlobLong(image);\n      count=ReadBlob(image,4,(unsigned char *) type);\n      ReversePSDString(image,type,4);\n      status=MagickFalse;\n      if ((count == 0) || (LocaleNCompare(type,\"8BIM\",4) != 0))\n        return(MagickTrue);\n      else\n        {\n          count=ReadBlob(image,4,(unsigned char *) type);\n          ReversePSDString(image,type,4);\n          if ((count != 0) && (LocaleNCompare(type,\"Lr16\",4) == 0))\n            size=GetPSDSize(psd_info,image);\n          else\n            return(MagickTrue);\n        }\n    }\n  status=MagickTrue;\n  if (size != 0)\n    {\n      layer_info=(LayerInfo *) NULL;\n      number_layers=(short) ReadBlobShort(image);\n\n      if (number_layers < 0)\n        {\n          /*\n            The first alpha channel in the merged result contains the\n            transparency data for the merged result.\n          */\n          number_layers=MagickAbsoluteValue(number_layers);\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  negative layer count corrected for\");\n          image->matte=MagickTrue;\n        }\n\n      /*\n        We only need to know if the image has an alpha channel\n      */\n      if (skip_layers != MagickFalse)\n        return(MagickTrue);\n\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image contains %.20g layers\",(double) number_layers);\n\n      if (number_layers == 0)\n        ThrowBinaryException(CorruptImageError,\"InvalidNumberOfLayers\",\n          image->filename);\n\n      layer_info=(LayerInfo *) AcquireQuantumMemory((size_t) number_layers,\n        sizeof(*layer_info));\n      if (layer_info == (LayerInfo *) NULL)\n        {\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  allocation of LayerInfo failed\");\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        }\n      (void) ResetMagickMemory(layer_info,0,(size_t) number_layers*\n        sizeof(*layer_info));\n\n      for (i=0; i < number_layers; i++)\n      {\n        ssize_t\n          x,\n          y;\n\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  reading layer #%.20g\",(double) i+1);\n        layer_info[i].page.y=ReadBlobSignedLong(image);\n        layer_info[i].page.x=ReadBlobSignedLong(image);\n        y=ReadBlobSignedLong(image);\n        x=ReadBlobSignedLong(image);\n        layer_info[i].page.width=(size_t) (x-layer_info[i].page.x);\n        layer_info[i].page.height=(size_t) (y-layer_info[i].page.y);\n        layer_info[i].channels=ReadBlobShort(image);\n        if (layer_info[i].channels > MaxPSDChannels)\n          {\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            ThrowBinaryException(CorruptImageError,\"MaximumChannelsExceeded\",\n              image->filename);\n          }\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    offset(%.20g,%.20g), size(%.20g,%.20g), channels=%.20g\",\n            (double) layer_info[i].page.x,(double) layer_info[i].page.y,\n            (double) layer_info[i].page.height,(double)\n            layer_info[i].page.width,(double) layer_info[i].channels);\n        for (j=0; j < (ssize_t) layer_info[i].channels; j++)\n        {\n          layer_info[i].channel_info[j].type=(short) ReadBlobShort(image);\n          layer_info[i].channel_info[j].size=(size_t) GetPSDSize(psd_info,\n            image);\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    channel[%.20g]: type=%.20g, size=%.20g\",(double) j,\n              (double) layer_info[i].channel_info[j].type,\n              (double) layer_info[i].channel_info[j].size);\n        }\n        count=ReadBlob(image,4,(unsigned char *) type);\n        ReversePSDString(image,type,4);\n        if ((count == 0) || (LocaleNCompare(type,\"8BIM\",4) != 0))\n          {\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  layer type was %.4s instead of 8BIM\", type);\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            ThrowBinaryException(CorruptImageError,\"ImproperImageHeader\",\n              image->filename);\n          }\n        (void) ReadBlob(image,4,(unsigned char *) layer_info[i].blendkey);\n        ReversePSDString(image,layer_info[i].blendkey,4);\n        layer_info[i].opacity=(Quantum) ScaleCharToQuantum((unsigned char)\n          ReadBlobByte(image));\n        layer_info[i].clipping=(unsigned char) ReadBlobByte(image);\n        layer_info[i].flags=(unsigned char) ReadBlobByte(image);\n        layer_info[i].visible=!(layer_info[i].flags & 0x02);\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"   blend=%.4s, opacity=%.20g, clipping=%s, flags=%d, visible=%s\",\n            layer_info[i].blendkey,(double) layer_info[i].opacity,\n            layer_info[i].clipping ? \"true\" : \"false\",layer_info[i].flags,\n            layer_info[i].visible ? \"true\" : \"false\");\n        (void) ReadBlobByte(image);  /* filler */\n\n        size=ReadBlobLong(image);\n        if (size != 0)\n          {\n            MagickSizeType\n              combined_length,\n              length;\n\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    layer contains additional info\");\n            length=ReadBlobLong(image);\n            combined_length=length+4;\n            if (length != 0)\n              {\n                /*\n                  Layer mask info.\n                */\n                layer_info[i].mask.page.y=ReadBlobSignedLong(image);\n                layer_info[i].mask.page.x=ReadBlobSignedLong(image);\n                layer_info[i].mask.page.height=(size_t) (ReadBlobLong(image)-\n                  layer_info[i].mask.page.y);\n                layer_info[i].mask.page.width=(size_t) (ReadBlobLong(image)-\n                  layer_info[i].mask.page.x);\n                layer_info[i].mask.background=(unsigned char) ReadBlobByte(\n                  image);\n                layer_info[i].mask.flags=(unsigned char) ReadBlobByte(image);\n                if (!(layer_info[i].mask.flags & 0x01))\n                  {\n                    layer_info[i].mask.page.y=layer_info[i].mask.page.y-\n                      layer_info[i].page.y;\n                    layer_info[i].mask.page.x=layer_info[i].mask.page.x-\n                      layer_info[i].page.x;\n                  }\n                if (image->debug != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      layer mask: offset(%.20g,%.20g), size(%.20g,%.20g), length=%.20g\",\n                    (double) layer_info[i].mask.page.x,(double)\n                    layer_info[i].mask.page.y,(double) layer_info[i].mask.page.width,\n                    (double) layer_info[i].mask.page.height,(double)\n                    ((MagickOffsetType) length)-18);\n                /*\n                  Skip over the rest of the layer mask information.\n                */\n                if (DiscardBlobBytes(image,(MagickSizeType) (length-18)) == MagickFalse)\n                  {\n                    layer_info=DestroyLayerInfo(layer_info,number_layers);\n                    ThrowBinaryException(CorruptImageError,\"UnexpectedEndOfFile\",\n                      image->filename);\n                  }\n              }\n            length=ReadBlobLong(image);\n            combined_length+=length+4;\n            if (length != 0)\n              {\n                /*\n                  Layer blending ranges info.\n                */\n                if (image->debug != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      layer blending ranges: length=%.20g\",(double)\n                    ((MagickOffsetType) length));\n                /*\n                  We read it, but don't use it...\n                */\n                for (j=0; j < (ssize_t) length; j+=8)\n                {\n                  size_t blend_source=ReadBlobLong(image);\n                  size_t blend_dest=ReadBlobLong(image);\n                  if (image->debug != MagickFalse)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"        source(%x), dest(%x)\",(unsigned int)\n                      blend_source,(unsigned int) blend_dest);\n                }\n              }\n            /*\n              Layer name.\n            */\n            length=(MagickSizeType) ReadBlobByte(image);\n            combined_length+=length+1;\n            if (length > 0)\n              (void) ReadBlob(image,(size_t) length++,layer_info[i].name);\n            layer_info[i].name[length]='\\0';\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"      layer name: %s\",layer_info[i].name);\n            if ((length % 4) != 0)\n              {\n                length=4-(length % 4);\n                combined_length+=length;\n                /* Skip over the padding of the layer name */\n                if (DiscardBlobBytes(image,length) == MagickFalse)\n                  {\n                    layer_info=DestroyLayerInfo(layer_info,number_layers);\n                    ThrowBinaryException(CorruptImageError,\n                      \"UnexpectedEndOfFile\",image->filename);\n                  }\n              }\n            length=(MagickSizeType) size-combined_length;\n            if (length > 0)\n              {\n                unsigned char\n                  *info;\n\n                layer_info[i].info=AcquireStringInfo((const size_t) length);\n                info=GetStringInfoDatum(layer_info[i].info);\n                (void) ReadBlob(image,(const size_t) length,info);\n              }\n          }\n      }\n\n      for (i=0; i < number_layers; i++)\n      {\n        if ((layer_info[i].page.width == 0) ||\n              (layer_info[i].page.height == 0))\n          {\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"      layer data is empty\");\n            continue;\n          }\n\n        /*\n          Allocate layered image.\n        */\n        layer_info[i].image=CloneImage(image,layer_info[i].page.width,\n          layer_info[i].page.height,MagickFalse,exception);\n        if (layer_info[i].image == (Image *) NULL)\n          {\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  allocation of image for layer %.20g failed\",(double) i);\n            ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n              image->filename);\n          }\n\n        if (layer_info[i].info != (StringInfo *) NULL)\n          {\n            (void) SetImageProfile(layer_info[i].image,\"psd:additional-info\",\n              layer_info[i].info);\n            layer_info[i].info=DestroyStringInfo(layer_info[i].info);\n          }\n      }\n\n      if (image_info->ping == MagickFalse)\n        {\n          for (i=0; i < number_layers; i++)\n          {\n            if (layer_info[i].image == (Image *) NULL)\n              {\n                for (j=0; j < layer_info[i].channels; j++)\n                {\n                  if (DiscardBlobBytes(image,(MagickSizeType)\n                      layer_info[i].channel_info[j].size) == MagickFalse)\n                    {\n                      layer_info=DestroyLayerInfo(layer_info,number_layers);\n                      ThrowBinaryException(CorruptImageError,\n                        \"UnexpectedEndOfFile\",image->filename);\n                    }\n                }\n                continue;\n              }\n\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  reading data for layer %.20g\",(double) i);\n            status=ReadPSDLayer(image,image_info,psd_info,&layer_info[i],\n              exception);\n            if (status == MagickFalse)\n              break;\n\n            status=SetImageProgress(image,LoadImagesTag,i,(MagickSizeType)\n              number_layers);\n            if (status == MagickFalse)\n              break;\n          }\n        }\n\n      if (status != MagickFalse)\n        {\n          for (i=0; i < number_layers; i++)\n          {\n            if (layer_info[i].image == (Image *) NULL)\n              {\n                for (j=i; j < number_layers - 1; j++)\n                  layer_info[j] = layer_info[j+1];\n                number_layers--;\n                i--;\n              }\n          }\n\n          if (number_layers > 0)\n            {\n              for (i=0; i < number_layers; i++)\n              {\n                if (i > 0)\n                  layer_info[i].image->previous=layer_info[i-1].image;\n                if (i < (number_layers-1))\n                  layer_info[i].image->next=layer_info[i+1].image;\n                layer_info[i].image->page=layer_info[i].page;\n              }\n              image->next=layer_info[0].image;\n              layer_info[0].image->previous=image;\n            }\n          layer_info=(LayerInfo *) RelinquishMagickMemory(layer_info);\n        }\n      else\n        layer_info=DestroyLayerInfo(layer_info,number_layers);\n    }\n\n  return(status);\n}",
        "func": "ModuleExport MagickBooleanType ReadPSDLayers(Image *image,\n  const ImageInfo *image_info,const PSDInfo *psd_info,\n  const MagickBooleanType skip_layers,ExceptionInfo *exception)\n{\n  char\n    type[4];\n\n  LayerInfo\n    *layer_info;\n\n  MagickSizeType\n    size;\n\n  MagickBooleanType\n    status;\n\n  register ssize_t\n    i;\n\n  ssize_t\n    count,\n    j,\n    number_layers;\n\n  size=GetPSDSize(psd_info,image);\n  if (size == 0)\n    {\n      /*\n        Skip layers & masks.\n      */\n      (void) ReadBlobLong(image);\n      count=ReadBlob(image,4,(unsigned char *) type);\n      ReversePSDString(image,type,4);\n      status=MagickFalse;\n      if ((count == 0) || (LocaleNCompare(type,\"8BIM\",4) != 0))\n        return(MagickTrue);\n      else\n        {\n          count=ReadBlob(image,4,(unsigned char *) type);\n          ReversePSDString(image,type,4);\n          if ((count != 0) && (LocaleNCompare(type,\"Lr16\",4) == 0))\n            size=GetPSDSize(psd_info,image);\n          else\n            return(MagickTrue);\n        }\n    }\n  status=MagickTrue;\n  if (size != 0)\n    {\n      layer_info=(LayerInfo *) NULL;\n      number_layers=(short) ReadBlobShort(image);\n\n      if (number_layers < 0)\n        {\n          /*\n            The first alpha channel in the merged result contains the\n            transparency data for the merged result.\n          */\n          number_layers=MagickAbsoluteValue(number_layers);\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  negative layer count corrected for\");\n          image->matte=MagickTrue;\n        }\n\n      /*\n        We only need to know if the image has an alpha channel\n      */\n      if (skip_layers != MagickFalse)\n        return(MagickTrue);\n\n      if (image->debug != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n          \"  image contains %.20g layers\",(double) number_layers);\n\n      if (number_layers == 0)\n        ThrowBinaryException(CorruptImageError,\"InvalidNumberOfLayers\",\n          image->filename);\n\n      layer_info=(LayerInfo *) AcquireQuantumMemory((size_t) number_layers,\n        sizeof(*layer_info));\n      if (layer_info == (LayerInfo *) NULL)\n        {\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"  allocation of LayerInfo failed\");\n          ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n            image->filename);\n        }\n      (void) ResetMagickMemory(layer_info,0,(size_t) number_layers*\n        sizeof(*layer_info));\n\n      for (i=0; i < number_layers; i++)\n      {\n        ssize_t\n          x,\n          y;\n\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"  reading layer #%.20g\",(double) i+1);\n        layer_info[i].page.y=ReadBlobSignedLong(image);\n        layer_info[i].page.x=ReadBlobSignedLong(image);\n        y=ReadBlobSignedLong(image);\n        x=ReadBlobSignedLong(image);\n        layer_info[i].page.width=(size_t) (x-layer_info[i].page.x);\n        layer_info[i].page.height=(size_t) (y-layer_info[i].page.y);\n        layer_info[i].channels=ReadBlobShort(image);\n        if (layer_info[i].channels > MaxPSDChannels)\n          {\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            ThrowBinaryException(CorruptImageError,\"MaximumChannelsExceeded\",\n              image->filename);\n          }\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    offset(%.20g,%.20g), size(%.20g,%.20g), channels=%.20g\",\n            (double) layer_info[i].page.x,(double) layer_info[i].page.y,\n            (double) layer_info[i].page.height,(double)\n            layer_info[i].page.width,(double) layer_info[i].channels);\n        for (j=0; j < (ssize_t) layer_info[i].channels; j++)\n        {\n          layer_info[i].channel_info[j].type=(short) ReadBlobShort(image);\n          layer_info[i].channel_info[j].size=(size_t) GetPSDSize(psd_info,\n            image);\n          if (image->debug != MagickFalse)\n            (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n              \"    channel[%.20g]: type=%.20g, size=%.20g\",(double) j,\n              (double) layer_info[i].channel_info[j].type,\n              (double) layer_info[i].channel_info[j].size);\n        }\n        count=ReadBlob(image,4,(unsigned char *) type);\n        ReversePSDString(image,type,4);\n        if ((count == 0) || (LocaleNCompare(type,\"8BIM\",4) != 0))\n          {\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  layer type was %.4s instead of 8BIM\", type);\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            ThrowBinaryException(CorruptImageError,\"ImproperImageHeader\",\n              image->filename);\n          }\n        (void) ReadBlob(image,4,(unsigned char *) layer_info[i].blendkey);\n        ReversePSDString(image,layer_info[i].blendkey,4);\n        layer_info[i].opacity=(Quantum) ScaleCharToQuantum((unsigned char)\n          ReadBlobByte(image));\n        layer_info[i].clipping=(unsigned char) ReadBlobByte(image);\n        layer_info[i].flags=(unsigned char) ReadBlobByte(image);\n        layer_info[i].visible=!(layer_info[i].flags & 0x02);\n        if (image->debug != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"   blend=%.4s, opacity=%.20g, clipping=%s, flags=%d, visible=%s\",\n            layer_info[i].blendkey,(double) layer_info[i].opacity,\n            layer_info[i].clipping ? \"true\" : \"false\",layer_info[i].flags,\n            layer_info[i].visible ? \"true\" : \"false\");\n        (void) ReadBlobByte(image);  /* filler */\n\n        size=ReadBlobLong(image);\n        if (size != 0)\n          {\n            MagickSizeType\n              combined_length,\n              length;\n\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    layer contains additional info\");\n            length=ReadBlobLong(image);\n            combined_length=length+4;\n            if (length != 0)\n              {\n                /*\n                  Layer mask info.\n                */\n                layer_info[i].mask.page.y=ReadBlobSignedLong(image);\n                layer_info[i].mask.page.x=ReadBlobSignedLong(image);\n                layer_info[i].mask.page.height=(size_t) (ReadBlobLong(image)-\n                  layer_info[i].mask.page.y);\n                layer_info[i].mask.page.width=(size_t) (ReadBlobLong(image)-\n                  layer_info[i].mask.page.x);\n                layer_info[i].mask.background=(unsigned char) ReadBlobByte(\n                  image);\n                layer_info[i].mask.flags=(unsigned char) ReadBlobByte(image);\n                if (!(layer_info[i].mask.flags & 0x01))\n                  {\n                    layer_info[i].mask.page.y=layer_info[i].mask.page.y-\n                      layer_info[i].page.y;\n                    layer_info[i].mask.page.x=layer_info[i].mask.page.x-\n                      layer_info[i].page.x;\n                  }\n                if (image->debug != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      layer mask: offset(%.20g,%.20g), size(%.20g,%.20g), length=%.20g\",\n                    (double) layer_info[i].mask.page.x,(double)\n                    layer_info[i].mask.page.y,(double) layer_info[i].mask.page.width,\n                    (double) layer_info[i].mask.page.height,(double)\n                    ((MagickOffsetType) length)-18);\n                /*\n                  Skip over the rest of the layer mask information.\n                */\n                if (DiscardBlobBytes(image,(MagickSizeType) (length-18)) == MagickFalse)\n                  {\n                    layer_info=DestroyLayerInfo(layer_info,number_layers);\n                    ThrowBinaryException(CorruptImageError,\"UnexpectedEndOfFile\",\n                      image->filename);\n                  }\n              }\n            length=ReadBlobLong(image);\n            combined_length+=length+4;\n            if (length != 0)\n              {\n                /*\n                  Layer blending ranges info.\n                */\n                if (image->debug != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"      layer blending ranges: length=%.20g\",(double)\n                    ((MagickOffsetType) length));\n                /*\n                  We read it, but don't use it...\n                */\n                for (j=0; j < (ssize_t) length; j+=8)\n                {\n                  size_t blend_source=ReadBlobLong(image);\n                  size_t blend_dest=ReadBlobLong(image);\n                  if (image->debug != MagickFalse)\n                    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                      \"        source(%x), dest(%x)\",(unsigned int)\n                      blend_source,(unsigned int) blend_dest);\n                }\n              }\n            /*\n              Layer name.\n            */\n            length=(MagickSizeType) ReadBlobByte(image);\n            combined_length+=length+1;\n            if (length > 0)\n              (void) ReadBlob(image,(size_t) length++,layer_info[i].name);\n            layer_info[i].name[length]='\\0';\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"      layer name: %s\",layer_info[i].name);\n            if ((length % 4) != 0)\n              {\n                length=4-(length % 4);\n                combined_length+=length;\n                /* Skip over the padding of the layer name */\n                if (DiscardBlobBytes(image,length) == MagickFalse)\n                  {\n                    layer_info=DestroyLayerInfo(layer_info,number_layers);\n                    ThrowBinaryException(CorruptImageError,\n                      \"UnexpectedEndOfFile\",image->filename);\n                  }\n              }\n            length=(MagickSizeType) size-combined_length;\n            if (length > 0)\n              {\n                unsigned char\n                  *info;\n\n                layer_info[i].info=AcquireStringInfo((const size_t) length);\n                info=GetStringInfoDatum(layer_info[i].info);\n                (void) ReadBlob(image,(const size_t) length,info);\n              }\n          }\n      }\n\n      for (i=0; i < number_layers; i++)\n      {\n        if ((layer_info[i].page.width == 0) ||\n              (layer_info[i].page.height == 0))\n          {\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"      layer data is empty\");\n            if (layer_info[i].info != (StringInfo *) NULL)\n              layer_info[i].info=DestroyStringInfo(layer_info[i].info);\n            continue;\n          }\n\n        /*\n          Allocate layered image.\n        */\n        layer_info[i].image=CloneImage(image,layer_info[i].page.width,\n          layer_info[i].page.height,MagickFalse,exception);\n        if (layer_info[i].image == (Image *) NULL)\n          {\n            layer_info=DestroyLayerInfo(layer_info,number_layers);\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  allocation of image for layer %.20g failed\",(double) i);\n            ThrowBinaryException(ResourceLimitError,\"MemoryAllocationFailed\",\n              image->filename);\n          }\n\n        if (layer_info[i].info != (StringInfo *) NULL)\n          {\n            (void) SetImageProfile(layer_info[i].image,\"psd:additional-info\",\n              layer_info[i].info);\n            layer_info[i].info=DestroyStringInfo(layer_info[i].info);\n          }\n      }\n\n      if (image_info->ping == MagickFalse)\n        {\n          for (i=0; i < number_layers; i++)\n          {\n            if (layer_info[i].image == (Image *) NULL)\n              {\n                for (j=0; j < layer_info[i].channels; j++)\n                {\n                  if (DiscardBlobBytes(image,(MagickSizeType)\n                      layer_info[i].channel_info[j].size) == MagickFalse)\n                    {\n                      layer_info=DestroyLayerInfo(layer_info,number_layers);\n                      ThrowBinaryException(CorruptImageError,\n                        \"UnexpectedEndOfFile\",image->filename);\n                    }\n                }\n                continue;\n              }\n\n            if (image->debug != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"  reading data for layer %.20g\",(double) i);\n            status=ReadPSDLayer(image,image_info,psd_info,&layer_info[i],\n              exception);\n            if (status == MagickFalse)\n              break;\n\n            status=SetImageProgress(image,LoadImagesTag,i,(MagickSizeType)\n              number_layers);\n            if (status == MagickFalse)\n              break;\n          }\n        }\n\n      if (status != MagickFalse)\n        {\n          for (i=0; i < number_layers; i++)\n          {\n            if (layer_info[i].image == (Image *) NULL)\n              {\n                for (j=i; j < number_layers - 1; j++)\n                  layer_info[j] = layer_info[j+1];\n                number_layers--;\n                i--;\n              }\n          }\n\n          if (number_layers > 0)\n            {\n              for (i=0; i < number_layers; i++)\n              {\n                if (i > 0)\n                  layer_info[i].image->previous=layer_info[i-1].image;\n                if (i < (number_layers-1))\n                  layer_info[i].image->next=layer_info[i+1].image;\n                layer_info[i].image->page=layer_info[i].page;\n              }\n              image->next=layer_info[0].image;\n              layer_info[0].image->previous=image;\n            }\n          layer_info=(LayerInfo *) RelinquishMagickMemory(layer_info);\n        }\n      else\n        layer_info=DestroyLayerInfo(layer_info,number_layers);\n    }\n\n  return(status);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -273,6 +273,8 @@\n             if (image->debug != MagickFalse)\n               (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                 \"      layer data is empty\");\n+            if (layer_info[i].info != (StringInfo *) NULL)\n+              layer_info[i].info=DestroyStringInfo(layer_info[i].info);\n             continue;\n           }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            if (layer_info[i].info != (StringInfo *) NULL)",
                "              layer_info[i].info=DestroyStringInfo(layer_info[i].info);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-7940",
        "func_name": "jsummers/imageworsener/iw_read_gif_file",
        "description": "The iw_read_gif_file function in imagew-gif.c in libimageworsener.a in ImageWorsener 1.3.0 allows remote attackers to consume an amount of available memory via a crafted file.",
        "git_url": "https://github.com/jsummers/imageworsener/commit/5fa486466630a9677b86c1d4cbfd9a5ff76cb33b",
        "commit_title": "Fixed a memory leak in the GIF decoder",
        "commit_text": " Fixes issue #18",
        "func_before": "IW_IMPL(int) iw_read_gif_file(struct iw_context *ctx, struct iw_iodescr *iodescr)\n{\n\tstruct iw_image img;\n\tstruct iwgifrcontext *rctx = NULL;\n\tint retval=0;\n\n\tiw_zeromem(&img,sizeof(struct iw_image));\n\trctx = iw_mallocz(ctx,sizeof(struct iwgifrcontext));\n\tif(!rctx) goto done;\n\n\trctx->ctx = ctx;\n\trctx->iodescr = iodescr;\n\trctx->img = &img;\n\n\t// Assume GIF images are sRGB.\n\tiw_make_srgb_csdescr_2(&rctx->csdescr);\n\n\trctx->page = iw_get_value(ctx,IW_VAL_PAGE_TO_READ);\n\tif(rctx->page<1) rctx->page = 1;\n\n\trctx->include_screen = iw_get_value(ctx,IW_VAL_INCLUDE_SCREEN);\n\n\tif(!iwgif_read_main(rctx))\n\t\tgoto done;\n\n\tiw_set_input_image(ctx, &img);\n\n\tiw_set_input_colorspace(ctx,&rctx->csdescr);\n\n\tretval = 1;\n\ndone:\n\tif(!retval) {\n\t\tiw_set_error(ctx,\"Failed to read GIF file\");\n\t}\n\n\tif(rctx) {\n\t\tif(rctx->row_pointers) iw_free(ctx,rctx->row_pointers);\n\t\tiw_free(ctx,rctx);\n\t}\n\n\treturn retval;\n}",
        "func": "IW_IMPL(int) iw_read_gif_file(struct iw_context *ctx, struct iw_iodescr *iodescr)\n{\n\tstruct iw_image img;\n\tstruct iwgifrcontext *rctx = NULL;\n\tint retval=0;\n\n\tiw_zeromem(&img,sizeof(struct iw_image));\n\trctx = iw_mallocz(ctx,sizeof(struct iwgifrcontext));\n\tif(!rctx) goto done;\n\n\trctx->ctx = ctx;\n\trctx->iodescr = iodescr;\n\trctx->img = &img;\n\n\t// Assume GIF images are sRGB.\n\tiw_make_srgb_csdescr_2(&rctx->csdescr);\n\n\trctx->page = iw_get_value(ctx,IW_VAL_PAGE_TO_READ);\n\tif(rctx->page<1) rctx->page = 1;\n\n\trctx->include_screen = iw_get_value(ctx,IW_VAL_INCLUDE_SCREEN);\n\n\tif(!iwgif_read_main(rctx))\n\t\tgoto done;\n\n\tiw_set_input_image(ctx, &img);\n\n\tiw_set_input_colorspace(ctx,&rctx->csdescr);\n\n\tretval = 1;\n\ndone:\n\tif(!retval) {\n\t\tiw_set_error(ctx,\"Failed to read GIF file\");\n\t\t// If we didn't call iw_set_input_image, 'img' still belongs to us,\n\t\t// so free its contents.\n\t\tiw_free(ctx, img.pixels);\n\t}\n\n\tif(rctx) {\n\t\tif(rctx->row_pointers) iw_free(ctx,rctx->row_pointers);\n\t\tiw_free(ctx,rctx);\n\t}\n\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -32,6 +32,9 @@\n done:\n \tif(!retval) {\n \t\tiw_set_error(ctx,\"Failed to read GIF file\");\n+\t\t// If we didn't call iw_set_input_image, 'img' still belongs to us,\n+\t\t// so free its contents.\n+\t\tiw_free(ctx, img.pixels);\n \t}\n \n \tif(rctx) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t// If we didn't call iw_set_input_image, 'img' still belongs to us,",
                "\t\t// so free its contents.",
                "\t\tiw_free(ctx, img.pixels);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11142",
        "func_name": "php/php-src/add_post_vars",
        "description": "In PHP before 5.6.31, 7.x before 7.0.17, and 7.1.x before 7.1.3, remote attackers could cause a CPU consumption denial of service attack by injecting long form variables, related to main/php_variables.c.",
        "git_url": "https://github.com/php/php-src/commit/0f8cf3b8497dc45c010c44ed9e96518e11e19fc3",
        "commit_title": "Fix bug #73807",
        "commit_text": "",
        "func_before": "static inline int add_post_vars(zval *arr, post_var_data_t *vars, zend_bool eof TSRMLS_DC)\n{\n\tuint64_t max_vars = PG(max_input_vars);\n\n\tvars->ptr = vars->str.c;\n\tvars->end = vars->str.c + vars->str.len;\n\twhile (add_post_var(arr, vars, eof TSRMLS_CC)) {\n\t\tif (++vars->cnt > max_vars) {\n\t\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING,\n\t\t\t\t\t\"Input variables exceeded %\" PRIu64 \". \"\n\t\t\t\t\t\"To increase the limit change max_input_vars in php.ini.\",\n\t\t\t\t\tmax_vars);\n\t\t\treturn FAILURE;\n\t\t}\n\t}\n\n\tif (!eof) {\n\t\tmemmove(vars->str.c, vars->ptr, vars->str.len = vars->end - vars->ptr);\n\t}\n\treturn SUCCESS;\n}",
        "func": "static inline int add_post_vars(zval *arr, post_var_data_t *vars, zend_bool eof TSRMLS_DC)\n{\n\tuint64_t max_vars = PG(max_input_vars);\n\n\tvars->ptr = vars->str.c;\n\tvars->end = vars->str.c + vars->str.len;\n\twhile (add_post_var(arr, vars, eof TSRMLS_CC)) {\n\t\tif (++vars->cnt > max_vars) {\n\t\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING,\n\t\t\t\t\t\"Input variables exceeded %\" PRIu64 \". \"\n\t\t\t\t\t\"To increase the limit change max_input_vars in php.ini.\",\n\t\t\t\t\tmax_vars);\n\t\t\treturn FAILURE;\n\t\t}\n\t}\n\n\tif (!eof && vars->str.c != vars->ptr) {\n\t\tmemmove(vars->str.c, vars->ptr, vars->str.len = vars->end - vars->ptr);\n\t}\n\treturn SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n \t\t}\n \t}\n \n-\tif (!eof) {\n+\tif (!eof && vars->str.c != vars->ptr) {\n \t\tmemmove(vars->str.c, vars->ptr, vars->str.len = vars->end - vars->ptr);\n \t}\n \treturn SUCCESS;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!eof) {"
            ],
            "added_lines": [
                "\tif (!eof && vars->str.c != vars->ptr) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11142",
        "func_name": "php/php-src/add_post_var",
        "description": "In PHP before 5.6.31, 7.x before 7.0.17, and 7.1.x before 7.1.3, remote attackers could cause a CPU consumption denial of service attack by injecting long form variables, related to main/php_variables.c.",
        "git_url": "https://github.com/php/php-src/commit/0f8cf3b8497dc45c010c44ed9e96518e11e19fc3",
        "commit_title": "Fix bug #73807",
        "commit_text": "",
        "func_before": "static zend_bool add_post_var(zval *arr, post_var_data_t *var, zend_bool eof TSRMLS_DC)\n{\n\tchar *ksep, *vsep, *val;\n\tsize_t klen, vlen;\n\t/* FIXME: string-size_t */\n\tunsigned int new_vlen;\n\n\tif (var->ptr >= var->end) {\n\t\treturn 0;\n\t}\n\n\tvsep = memchr(var->ptr, '&', var->end - var->ptr);\n\tif (!vsep) {\n\t\tif (!eof) {\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvsep = var->end;\n\t\t}\n\t}\n\n\tksep = memchr(var->ptr, '=', vsep - var->ptr);\n\tif (ksep) {\n\t\t*ksep = '\\0';\n\t\t/* \"foo=bar&\" or \"foo=&\" */\n\t\tklen = ksep - var->ptr;\n\t\tvlen = vsep - ++ksep;\n\t} else {\n\t\tksep = \"\";\n\t\t/* \"foo&\" */\n\t\tklen = vsep - var->ptr;\n\t\tvlen = 0;\n\t}\n\n\tphp_url_decode(var->ptr, klen);\n\n\tval = estrndup(ksep, vlen);\n\tif (vlen) {\n\t\tvlen = php_url_decode(val, vlen);\n\t}\n\n\tif (sapi_module.input_filter(PARSE_POST, var->ptr, &val, vlen, &new_vlen TSRMLS_CC)) {\n\t\tphp_register_variable_safe(var->ptr, val, new_vlen, arr TSRMLS_CC);\n\t}\n\tefree(val);\n\n\tvar->ptr = vsep + (vsep != var->end);\n\treturn 1;\n}",
        "func": "static zend_bool add_post_var(zval *arr, post_var_data_t *var, zend_bool eof TSRMLS_DC)\n{\n\tchar *start, *ksep, *vsep, *val;\n\tsize_t klen, vlen;\n\t/* FIXME: string-size_t */\n\tunsigned int new_vlen;\n\n\tif (var->ptr >= var->end) {\n\t\treturn 0;\n\t}\n\n\tstart = var->ptr + var->already_scanned;\n\tvsep = memchr(start, '&', var->end - start);\n\tif (!vsep) {\n\t\tif (!eof) {\n\t\t\tvar->already_scanned = var->end - var->ptr;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvsep = var->end;\n\t\t}\n\t}\n\n\tksep = memchr(var->ptr, '=', vsep - var->ptr);\n\tif (ksep) {\n\t\t*ksep = '\\0';\n\t\t/* \"foo=bar&\" or \"foo=&\" */\n\t\tklen = ksep - var->ptr;\n\t\tvlen = vsep - ++ksep;\n\t} else {\n\t\tksep = \"\";\n\t\t/* \"foo&\" */\n\t\tklen = vsep - var->ptr;\n\t\tvlen = 0;\n\t}\n\n\tphp_url_decode(var->ptr, klen);\n\n\tval = estrndup(ksep, vlen);\n\tif (vlen) {\n\t\tvlen = php_url_decode(val, vlen);\n\t}\n\n\tif (sapi_module.input_filter(PARSE_POST, var->ptr, &val, vlen, &new_vlen TSRMLS_CC)) {\n\t\tphp_register_variable_safe(var->ptr, val, new_vlen, arr TSRMLS_CC);\n\t}\n\tefree(val);\n\n\tvar->ptr = vsep + (vsep != var->end);\n\tvar->already_scanned = 0;\n\treturn 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static zend_bool add_post_var(zval *arr, post_var_data_t *var, zend_bool eof TSRMLS_DC)\n {\n-\tchar *ksep, *vsep, *val;\n+\tchar *start, *ksep, *vsep, *val;\n \tsize_t klen, vlen;\n \t/* FIXME: string-size_t */\n \tunsigned int new_vlen;\n@@ -9,9 +9,11 @@\n \t\treturn 0;\n \t}\n \n-\tvsep = memchr(var->ptr, '&', var->end - var->ptr);\n+\tstart = var->ptr + var->already_scanned;\n+\tvsep = memchr(start, '&', var->end - start);\n \tif (!vsep) {\n \t\tif (!eof) {\n+\t\t\tvar->already_scanned = var->end - var->ptr;\n \t\t\treturn 0;\n \t\t} else {\n \t\t\tvsep = var->end;\n@@ -44,5 +46,6 @@\n \tefree(val);\n \n \tvar->ptr = vsep + (vsep != var->end);\n+\tvar->already_scanned = 0;\n \treturn 1;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tchar *ksep, *vsep, *val;",
                "\tvsep = memchr(var->ptr, '&', var->end - var->ptr);"
            ],
            "added_lines": [
                "\tchar *start, *ksep, *vsep, *val;",
                "\tstart = var->ptr + var->already_scanned;",
                "\tvsep = memchr(start, '&', var->end - start);",
                "\t\t\tvar->already_scanned = var->end - var->ptr;",
                "\tvar->already_scanned = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-7540",
        "func_name": "xen-project/xen/guest_remove_page",
        "description": "An issue was discovered in Xen through 4.10.x allowing x86 PV guest OS users to cause a denial of service (host OS CPU hang) via non-preemptable L3/L4 pagetable freeing.",
        "git_url": "https://github.com/xen-project/xen/commit/d798a0952903db9d8ee0a580e03f214d2b49b7d7",
        "commit_title": "memory: don't implicitly unpin for decrease-reservation",
        "commit_text": " It very likely was a mistake (copy-and-paste from domain cleanup code) to implicitly unpin here: The caller should really unpin itself before (or after, if they so wish) requesting the page to be removed.  This is XSA-252. ",
        "func_before": "int guest_remove_page(struct domain *d, unsigned long gmfn)\n{\n    struct page_info *page;\n#ifdef CONFIG_X86\n    p2m_type_t p2mt;\n#endif\n    mfn_t mfn;\n    int rc;\n\n#ifdef CONFIG_X86\n    mfn = get_gfn_query(d, gmfn, &p2mt);\n    if ( unlikely(p2mt == p2m_invalid) || unlikely(p2mt == p2m_mmio_dm) )\n        return -ENOENT;\n\n    if ( unlikely(p2m_is_paging(p2mt)) )\n    {\n        rc = guest_physmap_remove_page(d, _gfn(gmfn), mfn, 0);\n        if ( rc )\n            goto out_put_gfn;\n\n        put_gfn(d, gmfn);\n\n        /* If the page hasn't yet been paged out, there is an\n         * actual page that needs to be released. */\n        if ( p2mt == p2m_ram_paging_out )\n        {\n            ASSERT(mfn_valid(mfn));\n            page = mfn_to_page(mfn_x(mfn));\n            if ( test_and_clear_bit(_PGC_allocated, &page->count_info) )\n                put_page(page);\n        }\n        p2m_mem_paging_drop_page(d, gmfn, p2mt);\n\n        return 0;\n    }\n    if ( p2mt == p2m_mmio_direct )\n    {\n        rc = clear_mmio_p2m_entry(d, gmfn, mfn, PAGE_ORDER_4K);\n        goto out_put_gfn;\n    }\n#else\n    mfn = gfn_to_mfn(d, _gfn(gmfn));\n#endif\n    if ( unlikely(!mfn_valid(mfn)) )\n    {\n        put_gfn(d, gmfn);\n        gdprintk(XENLOG_INFO, \"Domain %u page number %lx invalid\\n\",\n                d->domain_id, gmfn);\n\n        return -EINVAL;\n    }\n            \n#ifdef CONFIG_X86\n    if ( p2m_is_shared(p2mt) )\n    {\n        /*\n         * Unshare the page, bail out on error. We unshare because we\n         * might be the only one using this shared page, and we need to\n         * trigger proper cleanup. Once done, this is like any other page.\n         */\n        rc = mem_sharing_unshare_page(d, gmfn, 0);\n        if ( rc )\n        {\n            (void)mem_sharing_notify_enomem(d, gmfn, 0);\n            goto out_put_gfn;\n        }\n        /* Maybe the mfn changed */\n        mfn = get_gfn_query_unlocked(d, gmfn, &p2mt);\n        ASSERT(!p2m_is_shared(p2mt));\n    }\n#endif /* CONFIG_X86 */\n\n    page = mfn_to_page(mfn_x(mfn));\n    if ( unlikely(!get_page(page, d)) )\n    {\n        put_gfn(d, gmfn);\n        gdprintk(XENLOG_INFO, \"Bad page free for domain %u\\n\", d->domain_id);\n\n        return -ENXIO;\n    }\n\n    rc = guest_physmap_remove_page(d, _gfn(gmfn), mfn, 0);\n\n#ifdef _PGT_pinned\n    if ( !rc && test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n        put_page_and_type(page);\n#endif\n\n    /*\n     * With the lack of an IOMMU on some platforms, domains with DMA-capable\n     * device must retrieve the same pfn when the hypercall populate_physmap\n     * is called.\n     *\n     * For this purpose (and to match populate_physmap() behavior), the page\n     * is kept allocated.\n     */\n    if ( !rc && !is_domain_direct_mapped(d) &&\n         test_and_clear_bit(_PGC_allocated, &page->count_info) )\n        put_page(page);\n\n    put_page(page);\n out_put_gfn: __maybe_unused\n    put_gfn(d, gmfn);\n\n    /*\n     * Filter out -ENOENT return values that aren't a result of an empty p2m\n     * entry.\n     */\n    return rc != -ENOENT ? rc : -EINVAL;\n}",
        "func": "int guest_remove_page(struct domain *d, unsigned long gmfn)\n{\n    struct page_info *page;\n#ifdef CONFIG_X86\n    p2m_type_t p2mt;\n#endif\n    mfn_t mfn;\n    int rc;\n\n#ifdef CONFIG_X86\n    mfn = get_gfn_query(d, gmfn, &p2mt);\n    if ( unlikely(p2mt == p2m_invalid) || unlikely(p2mt == p2m_mmio_dm) )\n        return -ENOENT;\n\n    if ( unlikely(p2m_is_paging(p2mt)) )\n    {\n        rc = guest_physmap_remove_page(d, _gfn(gmfn), mfn, 0);\n        if ( rc )\n            goto out_put_gfn;\n\n        put_gfn(d, gmfn);\n\n        /* If the page hasn't yet been paged out, there is an\n         * actual page that needs to be released. */\n        if ( p2mt == p2m_ram_paging_out )\n        {\n            ASSERT(mfn_valid(mfn));\n            page = mfn_to_page(mfn_x(mfn));\n            if ( test_and_clear_bit(_PGC_allocated, &page->count_info) )\n                put_page(page);\n        }\n        p2m_mem_paging_drop_page(d, gmfn, p2mt);\n\n        return 0;\n    }\n    if ( p2mt == p2m_mmio_direct )\n    {\n        rc = clear_mmio_p2m_entry(d, gmfn, mfn, PAGE_ORDER_4K);\n        goto out_put_gfn;\n    }\n#else\n    mfn = gfn_to_mfn(d, _gfn(gmfn));\n#endif\n    if ( unlikely(!mfn_valid(mfn)) )\n    {\n        put_gfn(d, gmfn);\n        gdprintk(XENLOG_INFO, \"Domain %u page number %lx invalid\\n\",\n                d->domain_id, gmfn);\n\n        return -EINVAL;\n    }\n            \n#ifdef CONFIG_X86\n    if ( p2m_is_shared(p2mt) )\n    {\n        /*\n         * Unshare the page, bail out on error. We unshare because we\n         * might be the only one using this shared page, and we need to\n         * trigger proper cleanup. Once done, this is like any other page.\n         */\n        rc = mem_sharing_unshare_page(d, gmfn, 0);\n        if ( rc )\n        {\n            (void)mem_sharing_notify_enomem(d, gmfn, 0);\n            goto out_put_gfn;\n        }\n        /* Maybe the mfn changed */\n        mfn = get_gfn_query_unlocked(d, gmfn, &p2mt);\n        ASSERT(!p2m_is_shared(p2mt));\n    }\n#endif /* CONFIG_X86 */\n\n    page = mfn_to_page(mfn_x(mfn));\n    if ( unlikely(!get_page(page, d)) )\n    {\n        put_gfn(d, gmfn);\n        gdprintk(XENLOG_INFO, \"Bad page free for domain %u\\n\", d->domain_id);\n\n        return -ENXIO;\n    }\n\n    rc = guest_physmap_remove_page(d, _gfn(gmfn), mfn, 0);\n\n    /*\n     * With the lack of an IOMMU on some platforms, domains with DMA-capable\n     * device must retrieve the same pfn when the hypercall populate_physmap\n     * is called.\n     *\n     * For this purpose (and to match populate_physmap() behavior), the page\n     * is kept allocated.\n     */\n    if ( !rc && !is_domain_direct_mapped(d) &&\n         test_and_clear_bit(_PGC_allocated, &page->count_info) )\n        put_page(page);\n\n    put_page(page);\n out_put_gfn: __maybe_unused\n    put_gfn(d, gmfn);\n\n    /*\n     * Filter out -ENOENT return values that aren't a result of an empty p2m\n     * entry.\n     */\n    return rc != -ENOENT ? rc : -EINVAL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -81,11 +81,6 @@\n \n     rc = guest_physmap_remove_page(d, _gfn(gmfn), mfn, 0);\n \n-#ifdef _PGT_pinned\n-    if ( !rc && test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )\n-        put_page_and_type(page);\n-#endif\n-\n     /*\n      * With the lack of an IOMMU on some platforms, domains with DMA-capable\n      * device must retrieve the same pfn when the hypercall populate_physmap",
        "diff_line_info": {
            "deleted_lines": [
                "#ifdef _PGT_pinned",
                "    if ( !rc && test_and_clear_bit(_PGT_pinned, &page->u.inuse.type_info) )",
                "        put_page_and_type(page);",
                "#endif",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2018-1000115",
        "func_name": "memcached/settings_init",
        "description": "Memcached version 1.5.5 contains an Insufficient Control of Network Message Volume (Network Amplification, CWE-406) vulnerability in the UDP support of the memcached server that can result in denial of service via network flood (traffic amplification of 1:50,000 has been reported by reliable sources). This attack appear to be exploitable via network connectivity to port 11211 UDP. This vulnerability appears to have been fixed in 1.5.6 due to the disabling of the UDP protocol by default.",
        "git_url": "https://github.com/memcached/memcached/commit/dbb7a8af90054bf4ef51f5814ef7ceb17d83d974",
        "commit_title": "disable UDP port by default",
        "commit_text": " As reported, UDP amplification attacks have started to use insecure internet-exposed memcached instances. UDP used to be a lot more popular as a transport for memcached many years ago, but I'm not aware of many recent users.  Ten years ago, the TCP connection overhead from many clients was relatively high (dozens or hundreds per client server), but these days many clients are batched, or user fewer processes, or simply anre't worried about it.  While changing the default to listen on localhost only would also help, the true culprit is UDP. There are many more use cases for using memcached over the network than there are for using the UDP protocol.",
        "func_before": "static void settings_init(void) {\n    settings.use_cas = true;\n    settings.access = 0700;\n    settings.port = 11211;\n    settings.udpport = 11211;\n    /* By default this string should be NULL for getaddrinfo() */\n    settings.inter = NULL;\n    settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */\n    settings.maxconns = 1024;         /* to limit connections-related memory to about 5MB */\n    settings.verbose = 0;\n    settings.oldest_live = 0;\n    settings.oldest_cas = 0;          /* supplements accuracy of oldest_live */\n    settings.evict_to_free = 1;       /* push old items out of cache when memory runs out */\n    settings.socketpath = NULL;       /* by default, not using a unix socket */\n    settings.factor = 1.25;\n    settings.chunk_size = 48;         /* space for a modest key and value */\n    settings.num_threads = 4;         /* N workers */\n    settings.num_threads_per_udp = 0;\n    settings.prefix_delimiter = ':';\n    settings.detail_enabled = 0;\n    settings.reqs_per_event = 20;\n    settings.backlog = 1024;\n    settings.binding_protocol = negotiating_prot;\n    settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */\n    settings.slab_page_size = 1024 * 1024; /* chunks are split from 1MB pages. */\n    settings.slab_chunk_size_max = settings.slab_page_size / 2;\n    settings.sasl = false;\n    settings.maxconns_fast = true;\n    settings.lru_crawler = false;\n    settings.lru_crawler_sleep = 100;\n    settings.lru_crawler_tocrawl = 0;\n    settings.lru_maintainer_thread = false;\n    settings.lru_segmented = true;\n    settings.hot_lru_pct = 20;\n    settings.warm_lru_pct = 40;\n    settings.hot_max_factor = 0.2;\n    settings.warm_max_factor = 2.0;\n    settings.inline_ascii_response = false;\n    settings.temp_lru = false;\n    settings.temporary_ttl = 61;\n    settings.idle_timeout = 0; /* disabled */\n    settings.hashpower_init = 0;\n    settings.slab_reassign = true;\n    settings.slab_automove = 1;\n    settings.slab_automove_ratio = 0.8;\n    settings.slab_automove_window = 30;\n    settings.shutdown_command = false;\n    settings.tail_repair_time = TAIL_REPAIR_TIME_DEFAULT;\n    settings.flush_enabled = true;\n    settings.dump_enabled = true;\n    settings.crawls_persleep = 1000;\n    settings.logger_watcher_buf_size = LOGGER_WATCHER_BUF_SIZE;\n    settings.logger_buf_size = LOGGER_BUF_SIZE;\n    settings.drop_privileges = true;\n#ifdef MEMCACHED_DEBUG\n    settings.relaxed_privileges = false;\n#endif\n}",
        "func": "static void settings_init(void) {\n    settings.use_cas = true;\n    settings.access = 0700;\n    settings.port = 11211;\n    settings.udpport = 0;\n    /* By default this string should be NULL for getaddrinfo() */\n    settings.inter = NULL;\n    settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */\n    settings.maxconns = 1024;         /* to limit connections-related memory to about 5MB */\n    settings.verbose = 0;\n    settings.oldest_live = 0;\n    settings.oldest_cas = 0;          /* supplements accuracy of oldest_live */\n    settings.evict_to_free = 1;       /* push old items out of cache when memory runs out */\n    settings.socketpath = NULL;       /* by default, not using a unix socket */\n    settings.factor = 1.25;\n    settings.chunk_size = 48;         /* space for a modest key and value */\n    settings.num_threads = 4;         /* N workers */\n    settings.num_threads_per_udp = 0;\n    settings.prefix_delimiter = ':';\n    settings.detail_enabled = 0;\n    settings.reqs_per_event = 20;\n    settings.backlog = 1024;\n    settings.binding_protocol = negotiating_prot;\n    settings.item_size_max = 1024 * 1024; /* The famous 1MB upper limit. */\n    settings.slab_page_size = 1024 * 1024; /* chunks are split from 1MB pages. */\n    settings.slab_chunk_size_max = settings.slab_page_size / 2;\n    settings.sasl = false;\n    settings.maxconns_fast = true;\n    settings.lru_crawler = false;\n    settings.lru_crawler_sleep = 100;\n    settings.lru_crawler_tocrawl = 0;\n    settings.lru_maintainer_thread = false;\n    settings.lru_segmented = true;\n    settings.hot_lru_pct = 20;\n    settings.warm_lru_pct = 40;\n    settings.hot_max_factor = 0.2;\n    settings.warm_max_factor = 2.0;\n    settings.inline_ascii_response = false;\n    settings.temp_lru = false;\n    settings.temporary_ttl = 61;\n    settings.idle_timeout = 0; /* disabled */\n    settings.hashpower_init = 0;\n    settings.slab_reassign = true;\n    settings.slab_automove = 1;\n    settings.slab_automove_ratio = 0.8;\n    settings.slab_automove_window = 30;\n    settings.shutdown_command = false;\n    settings.tail_repair_time = TAIL_REPAIR_TIME_DEFAULT;\n    settings.flush_enabled = true;\n    settings.dump_enabled = true;\n    settings.crawls_persleep = 1000;\n    settings.logger_watcher_buf_size = LOGGER_WATCHER_BUF_SIZE;\n    settings.logger_buf_size = LOGGER_BUF_SIZE;\n    settings.drop_privileges = true;\n#ifdef MEMCACHED_DEBUG\n    settings.relaxed_privileges = false;\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n     settings.use_cas = true;\n     settings.access = 0700;\n     settings.port = 11211;\n-    settings.udpport = 11211;\n+    settings.udpport = 0;\n     /* By default this string should be NULL for getaddrinfo() */\n     settings.inter = NULL;\n     settings.maxbytes = 64 * 1024 * 1024; /* default is 64MB */",
        "diff_line_info": {
            "deleted_lines": [
                "    settings.udpport = 11211;"
            ],
            "added_lines": [
                "    settings.udpport = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/do_sched_cfs_period_timer",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)\n{\n\tu64 runtime, runtime_expires;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\truntime_expires = cfs_b->runtime_expires;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n\t\t\t\t\t\t runtime_expires);\n\t\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}",
        "func": "static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)\n{\n\tu64 runtime;\n\tint throttled;\n\n\t/* no need to continue the timer with no bandwidth constraint */\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tgoto out_deactivate;\n\n\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\tcfs_b->nr_periods += overrun;\n\n\t/*\n\t * idle depends on !throttled (for the case of a large deficit), and if\n\t * we're going inactive then everything else can be deferred\n\t */\n\tif (cfs_b->idle && !throttled)\n\t\tgoto out_deactivate;\n\n\t__refill_cfs_bandwidth_runtime(cfs_b);\n\n\tif (!throttled) {\n\t\t/* mark as potentially idle for the upcoming period */\n\t\tcfs_b->idle = 1;\n\t\treturn 0;\n\t}\n\n\t/* account preceding periods in which throttling occurred */\n\tcfs_b->nr_throttled += overrun;\n\n\t/*\n\t * This check is repeated as we are holding onto the new bandwidth while\n\t * we unthrottle. This can potentially race with an unthrottled group\n\t * trying to acquire new bandwidth from the global pool. This can result\n\t * in us over-using our runtime if it is all used during this loop, but\n\t * only by limited amounts in that extreme case.\n\t */\n\twhile (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {\n\t\truntime = cfs_b->runtime;\n\t\tcfs_b->distribute_running = 1;\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n\t\truntime = distribute_cfs_runtime(cfs_b, runtime);\n\t\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\n\t\tcfs_b->distribute_running = 0;\n\t\tthrottled = !list_empty(&cfs_b->throttled_cfs_rq);\n\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\t}\n\n\t/*\n\t * While we are ensured activity in the period following an\n\t * unthrottle, this also covers the case in which the new bandwidth is\n\t * insufficient to cover the existing bandwidth deficit.  (Forcing the\n\t * timer to remain active while there are any throttled entities.)\n\t */\n\tcfs_b->idle = 0;\n\n\treturn 0;\n\nout_deactivate:\n\treturn 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)\n {\n-\tu64 runtime, runtime_expires;\n+\tu64 runtime;\n \tint throttled;\n \n \t/* no need to continue the timer with no bandwidth constraint */\n@@ -28,8 +28,6 @@\n \t/* account preceding periods in which throttling occurred */\n \tcfs_b->nr_throttled += overrun;\n \n-\truntime_expires = cfs_b->runtime_expires;\n-\n \t/*\n \t * This check is repeated as we are holding onto the new bandwidth while\n \t * we unthrottle. This can potentially race with an unthrottled group\n@@ -42,8 +40,7 @@\n \t\tcfs_b->distribute_running = 1;\n \t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n \t\t/* we can't nest cfs_b->lock while distributing bandwidth */\n-\t\truntime = distribute_cfs_runtime(cfs_b, runtime,\n-\t\t\t\t\t\t runtime_expires);\n+\t\truntime = distribute_cfs_runtime(cfs_b, runtime);\n \t\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n \n \t\tcfs_b->distribute_running = 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\tu64 runtime, runtime_expires;",
                "\truntime_expires = cfs_b->runtime_expires;",
                "",
                "\t\truntime = distribute_cfs_runtime(cfs_b, runtime,",
                "\t\t\t\t\t\t runtime_expires);"
            ],
            "added_lines": [
                "\tu64 runtime;",
                "\t\truntime = distribute_cfs_runtime(cfs_b, runtime);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/assign_cfs_rq_runtime",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount, expires;\n\tint expires_seq;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\texpires_seq = cfs_b->expires_seq;\n\texpires = cfs_b->runtime_expires;\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\t/*\n\t * we may have advanced our local expiration to account for allowed\n\t * spread between our sched_clock and the one on which runtime was\n\t * issued.\n\t */\n\tif (cfs_rq->expires_seq != expires_seq) {\n\t\tcfs_rq->expires_seq = expires_seq;\n\t\tcfs_rq->runtime_expires = expires;\n\t}\n\n\treturn cfs_rq->runtime_remaining > 0;\n}",
        "func": "static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct task_group *tg = cfs_rq->tg;\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n\tu64 amount = 0, min_amount;\n\n\t/* note: this is a positive sum as runtime_remaining <= 0 */\n\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\tamount = min_amount;\n\telse {\n\t\tstart_cfs_bandwidth(cfs_b);\n\n\t\tif (cfs_b->runtime > 0) {\n\t\t\tamount = min(cfs_b->runtime, min_amount);\n\t\t\tcfs_b->runtime -= amount;\n\t\t\tcfs_b->idle = 0;\n\t\t}\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\tcfs_rq->runtime_remaining += amount;\n\n\treturn cfs_rq->runtime_remaining > 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,8 +2,7 @@\n {\n \tstruct task_group *tg = cfs_rq->tg;\n \tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);\n-\tu64 amount = 0, min_amount, expires;\n-\tint expires_seq;\n+\tu64 amount = 0, min_amount;\n \n \t/* note: this is a positive sum as runtime_remaining <= 0 */\n \tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;\n@@ -20,20 +19,9 @@\n \t\t\tcfs_b->idle = 0;\n \t\t}\n \t}\n-\texpires_seq = cfs_b->expires_seq;\n-\texpires = cfs_b->runtime_expires;\n \traw_spin_unlock(&cfs_b->lock);\n \n \tcfs_rq->runtime_remaining += amount;\n-\t/*\n-\t * we may have advanced our local expiration to account for allowed\n-\t * spread between our sched_clock and the one on which runtime was\n-\t * issued.\n-\t */\n-\tif (cfs_rq->expires_seq != expires_seq) {\n-\t\tcfs_rq->expires_seq = expires_seq;\n-\t\tcfs_rq->runtime_expires = expires;\n-\t}\n \n \treturn cfs_rq->runtime_remaining > 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tu64 amount = 0, min_amount, expires;",
                "\tint expires_seq;",
                "\texpires_seq = cfs_b->expires_seq;",
                "\texpires = cfs_b->runtime_expires;",
                "\t/*",
                "\t * we may have advanced our local expiration to account for allowed",
                "\t * spread between our sched_clock and the one on which runtime was",
                "\t * issued.",
                "\t */",
                "\tif (cfs_rq->expires_seq != expires_seq) {",
                "\t\tcfs_rq->expires_seq = expires_seq;",
                "\t\tcfs_rq->runtime_expires = expires;",
                "\t}"
            ],
            "added_lines": [
                "\tu64 amount = 0, min_amount;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/__return_cfs_rq_runtime",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF &&\n\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}",
        "func": "static void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq)\n{\n\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);\n\ts64 slack_runtime = cfs_rq->runtime_remaining - min_cfs_rq_runtime;\n\n\tif (slack_runtime <= 0)\n\t\treturn;\n\n\traw_spin_lock(&cfs_b->lock);\n\tif (cfs_b->quota != RUNTIME_INF) {\n\t\tcfs_b->runtime += slack_runtime;\n\n\t\t/* we are under rq->lock, defer unthrottling using a timer */\n\t\tif (cfs_b->runtime > sched_cfs_bandwidth_slice() &&\n\t\t    !list_empty(&cfs_b->throttled_cfs_rq))\n\t\t\tstart_cfs_slack_bandwidth(cfs_b);\n\t}\n\traw_spin_unlock(&cfs_b->lock);\n\n\t/* even if it's not valid for return we don't want to try again */\n\tcfs_rq->runtime_remaining -= slack_runtime;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,8 +7,7 @@\n \t\treturn;\n \n \traw_spin_lock(&cfs_b->lock);\n-\tif (cfs_b->quota != RUNTIME_INF &&\n-\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {\n+\tif (cfs_b->quota != RUNTIME_INF) {\n \t\tcfs_b->runtime += slack_runtime;\n \n \t\t/* we are under rq->lock, defer unthrottling using a timer */",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (cfs_b->quota != RUNTIME_INF &&",
                "\t    cfs_rq->runtime_expires == cfs_b->runtime_expires) {"
            ],
            "added_lines": [
                "\tif (cfs_b->quota != RUNTIME_INF) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/do_sched_cfs_slack_timer",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tunsigned long flags;\n\tu64 expires;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tcfs_b->slack_started = false;\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\texpires = cfs_b->runtime_expires;\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n\n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tif (expires == cfs_b->runtime_expires)\n\t\tlsub_positive(&cfs_b->runtime, runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n}",
        "func": "static void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b)\n{\n\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n\tunsigned long flags;\n\n\t/* confirm we're still not at a refresh boundary */\n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tcfs_b->slack_started = false;\n\tif (cfs_b->distribute_running) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\treturn;\n\t}\n\n\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {\n\t\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\t\treturn;\n\t}\n\n\tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n\t\truntime = cfs_b->runtime;\n\n\tif (runtime)\n\t\tcfs_b->distribute_running = 1;\n\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n\n\tif (!runtime)\n\t\treturn;\n\n\truntime = distribute_cfs_runtime(cfs_b, runtime);\n\n\traw_spin_lock_irqsave(&cfs_b->lock, flags);\n\tlsub_positive(&cfs_b->runtime, runtime);\n\tcfs_b->distribute_running = 0;\n\traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,6 @@\n {\n \tu64 runtime = 0, slice = sched_cfs_bandwidth_slice();\n \tunsigned long flags;\n-\tu64 expires;\n \n \t/* confirm we're still not at a refresh boundary */\n \traw_spin_lock_irqsave(&cfs_b->lock, flags);\n@@ -20,7 +19,6 @@\n \tif (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)\n \t\truntime = cfs_b->runtime;\n \n-\texpires = cfs_b->runtime_expires;\n \tif (runtime)\n \t\tcfs_b->distribute_running = 1;\n \n@@ -29,11 +27,10 @@\n \tif (!runtime)\n \t\treturn;\n \n-\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);\n+\truntime = distribute_cfs_runtime(cfs_b, runtime);\n \n \traw_spin_lock_irqsave(&cfs_b->lock, flags);\n-\tif (expires == cfs_b->runtime_expires)\n-\t\tlsub_positive(&cfs_b->runtime, runtime);\n+\tlsub_positive(&cfs_b->runtime, runtime);\n \tcfs_b->distribute_running = 0;\n \traw_spin_unlock_irqrestore(&cfs_b->lock, flags);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tu64 expires;",
                "\texpires = cfs_b->runtime_expires;",
                "\truntime = distribute_cfs_runtime(cfs_b, runtime, expires);",
                "\tif (expires == cfs_b->runtime_expires)",
                "\t\tlsub_positive(&cfs_b->runtime, runtime);"
            ],
            "added_lines": [
                "\truntime = distribute_cfs_runtime(cfs_b, runtime);",
                "\tlsub_positive(&cfs_b->runtime, runtime);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/__refill_cfs_bandwidth_runtime",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n}",
        "func": "void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b)\n{\n\tu64 now;\n\n\tif (cfs_b->quota == RUNTIME_INF)\n\t\treturn;\n\n\tnow = sched_clock_cpu(smp_processor_id());\n\tcfs_b->runtime = cfs_b->quota;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,4 @@\n \n \tnow = sched_clock_cpu(smp_processor_id());\n \tcfs_b->runtime = cfs_b->quota;\n-\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);\n-\tcfs_b->expires_seq++;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tcfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);",
                "\tcfs_b->expires_seq++;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/distribute_cfs_runtime",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n\t\tu64 remaining, u64 expires)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock_irqsave(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\t\tcfs_rq->runtime_expires = expires;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}",
        "func": "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining)\n{\n\tstruct cfs_rq *cfs_rq;\n\tu64 runtime;\n\tu64 starting_runtime = remaining;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,\n\t\t\t\tthrottled_list) {\n\t\tstruct rq *rq = rq_of(cfs_rq);\n\t\tstruct rq_flags rf;\n\n\t\trq_lock_irqsave(rq, &rf);\n\t\tif (!cfs_rq_throttled(cfs_rq))\n\t\t\tgoto next;\n\n\t\truntime = -cfs_rq->runtime_remaining + 1;\n\t\tif (runtime > remaining)\n\t\t\truntime = remaining;\n\t\tremaining -= runtime;\n\n\t\tcfs_rq->runtime_remaining += runtime;\n\n\t\t/* we check whether we're throttled above */\n\t\tif (cfs_rq->runtime_remaining > 0)\n\t\t\tunthrottle_cfs_rq(cfs_rq);\n\nnext:\n\t\trq_unlock_irqrestore(rq, &rf);\n\n\t\tif (!remaining)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn starting_runtime - remaining;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,4 @@\n-static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,\n-\t\tu64 remaining, u64 expires)\n+static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining)\n {\n \tstruct cfs_rq *cfs_rq;\n \tu64 runtime;\n@@ -21,7 +20,6 @@\n \t\tremaining -= runtime;\n \n \t\tcfs_rq->runtime_remaining += runtime;\n-\t\tcfs_rq->runtime_expires = expires;\n \n \t\t/* we check whether we're throttled above */\n \t\tif (cfs_rq->runtime_remaining > 0)",
        "diff_line_info": {
            "deleted_lines": [
                "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,",
                "\t\tu64 remaining, u64 expires)",
                "\t\tcfs_rq->runtime_expires = expires;"
            ],
            "added_lines": [
                "static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining)"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/start_cfs_bandwidth",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n\tcfs_b->expires_seq++;\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}",
        "func": "void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b)\n{\n\tu64 overrun;\n\n\tlockdep_assert_held(&cfs_b->lock);\n\n\tif (cfs_b->period_active)\n\t\treturn;\n\n\tcfs_b->period_active = 1;\n\toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n\thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,5 @@\n \n \tcfs_b->period_active = 1;\n \toverrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);\n-\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);\n-\tcfs_b->expires_seq++;\n \thrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tcfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);",
                "\tcfs_b->expires_seq++;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2019-19922",
        "func_name": "torvalds/linux/__account_cfs_rq_runtime",
        "description": "kernel/sched/fair.c in the Linux kernel before 5.3.9, when cpu.cfs_quota_us is used (e.g., with Kubernetes), allows attackers to cause a denial of service against non-cpu-bound applications by generating a workload that triggers unwanted slice expiration, aka CID-de53fd7aedb1. (In other words, although this slice expiration would typically be seen with benign workloads, it is possible that an attacker could calculate how many stray requests are required to force an entire Kubernetes cluster into a low-performance state caused by slice expiration, and ensure that a DDoS attack sent that number of stray requests. An attack does not affect the stability of the kernel; it only causes mismanagement of application execution.)",
        "git_url": "https://github.com/torvalds/linux/commit/de53fd7aedb100f03e5d2231cfce0e4993282425",
        "commit_title": "sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices",
        "commit_text": " It has been observed, that highly-threaded, non-cpu-bound applications running under cpu.cfs_quota_us constraints can hit a high percentage of periods throttled while simultaneously not consuming the allocated amount of quota. This use case is typical of user-interactive non-cpu bound applications, such as those running in kubernetes or mesos when run on multiple cpu cores.  This has been root caused to cpu-local run queue being allocated per cpu bandwidth slices, and then not fully using that slice within the period. At which point the slice and quota expires. This expiration of unused slice results in applications not being able to utilize the quota for which they are allocated.  The non-expiration of per-cpu slices was recently fixed by 'commit 512ac999d275 (\"sched/fair: Fix bandwidth timer clock drift condition\")'. Prior to that it appears that this had been broken since at least 'commit 51f2176d74ac (\"sched/fair: Fix unlocked reads of some cfs_b->quota/period\")' which was introduced in v3.16-rc1 in 2014. That added the following conditional which resulted in slices never being expired.  if (cfs_rq->runtime_expires != cfs_b->runtime_expires) { \t/* extend local deadline, drift is bounded above by 2 ticks */ \tcfs_rq->runtime_expires += TICK_NSEC;  Because this was broken for nearly 5 years, and has recently been fixed and is now being noticed by many users running kubernetes (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion that the mechanisms around expiring runtime should be removed altogether.  This allows quota already allocated to per-cpu run-queues to live longer than the period boundary. This allows threads on runqueues that do not use much CPU to continue to use their remaining slice over a longer period of time than cpu.cfs_period_us. However, this helps prevent the above condition of hitting throttling while also not fully utilizing your cpu quota.  This theoretically allows a machine to use slightly more than its allotted quota in some periods. This overflow would be bounded by the remaining quota left on each per-cpu runqueueu. This is typically no more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will change nothing, as they should theoretically fully utilize all of their quota in each period. For user-interactive tasks as described above this provides a much better user/application experience as their cpu utilization will more closely match the amount they requested when they hit throttling. This means that cpu limits no longer strictly apply per period for non-cpu bound applications, but that they are still accurate over longer timeframes.  This greatly improves performance of high-thread-count, non-cpu bound applications with low cfs_quota_us allocation on high-core-count machines. In the case of an artificial testcase (10ms/100ms of quota on 80 CPU machine), this commit resulted in almost 30x performance improvement, while still maintaining correct cpu quota restrictions. That testcase is available at https://github.com/indeedeng/fibtest.  Cc: Ingo Molnar <mingo@redhat.com> Cc: John Hammond <jhammond@indeed.com> Cc: Jonathan Corbet <corbet@lwn.net> Cc: Kyle Anderson <kwa@yelp.com> Cc: Gabriel Munos <gmunoz@netflix.com> Cc: Peter Oskolkov <posk@posk.io> Cc: Cong Wang <xiyou.wangcong@gmail.com> Cc: Brendan Gregg <bgregg@netflix.com> Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com",
        "func_before": "static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\texpire_cfs_rq_runtime(cfs_rq);\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}",
        "func": "static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)\n{\n\t/* dock delta_exec before expiring quota (as it could span periods) */\n\tcfs_rq->runtime_remaining -= delta_exec;\n\n\tif (likely(cfs_rq->runtime_remaining > 0))\n\t\treturn;\n\n\t/*\n\t * if we're unable to extend our runtime we resched so that the active\n\t * hierarchy can be throttled\n\t */\n\tif (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))\n\t\tresched_curr(rq_of(cfs_rq));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,6 @@\n {\n \t/* dock delta_exec before expiring quota (as it could span periods) */\n \tcfs_rq->runtime_remaining -= delta_exec;\n-\texpire_cfs_rq_runtime(cfs_rq);\n \n \tif (likely(cfs_rq->runtime_remaining > 0))\n \t\treturn;",
        "diff_line_info": {
            "deleted_lines": [
                "\texpire_cfs_rq_runtime(cfs_rq);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2019-20176",
        "func_name": "jedisct1/pure-ftpd/listdir",
        "description": "In Pure-FTPd 1.0.49, a stack exhaustion issue was discovered in the listdir function in ls.c.",
        "git_url": "https://github.com/jedisct1/pure-ftpd/commit/aea56f4bcb9948d456f3fae4d044fd3fa2e19706",
        "commit_title": "listdir(): reuse a single buffer to store every file name to display",
        "commit_text": " Allocating a new buffer for each entry is useless.  And as these buffers are allocated on the stack, on systems with a small stack size, with many entries, the limit can easily be reached, causing a stack exhaustion and aborting the user session.  Reported by Antonio Morales from the GitHub Security Lab team, thanks!",
        "func_before": "static void listdir(unsigned int depth, int f, void * const tls_fd,\n                    const char *name)\n{\n    PureFileInfo *dir;\n    char *names;\n    PureFileInfo *s;\n    PureFileInfo *r;\n    int d;\n\n    if (depth >= max_ls_depth || matches >= max_ls_files) {\n        return;\n    }\n    if ((dir = sreaddir(&names)) == NULL) {\n        addreply(226, MSG_CANT_READ_FILE, name);\n        return;\n    }\n    s = dir;\n    while (s->name_offset != (size_t) -1) {\n        d = 0;\n        if (FI_NAME(s)[0] != '.') {\n            d = listfile(s, NULL);\n        } else if (opt_a) {\n            if (FI_NAME(s)[1] == 0 ||\n                (FI_NAME(s)[1] == '.' && FI_NAME(s)[2] == 0)) {\n                listfile(s, NULL);\n            } else {\n                d = listfile(s, NULL);\n            }\n        }\n        if (!d) {\n            s->name_offset = (size_t) -1;\n        }\n        s++;\n    }\n    outputfiles(f, tls_fd);\n    r = dir;\n    while (opt_R && r != s) {\n        if (r->name_offset != (size_t) -1 && !chdir(FI_NAME(r))) {\n            char *alloca_subdir;\n            const size_t sizeof_subdir = PATH_MAX + 1U;\n\n            if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {\n                goto toomany;\n            }\n            if (SNCHECK(snprintf(alloca_subdir, sizeof_subdir, \"%s/%s\",\n                                 name, FI_NAME(r)), sizeof_subdir)) {\n                goto nolist;\n            }\n            wrstr(f, tls_fd, \"\\r\\n\\r\\n\");\n            wrstr(f, tls_fd, alloca_subdir);\n            wrstr(f, tls_fd, \":\\r\\n\\r\\n\");\n            listdir(depth + 1U, f, tls_fd, alloca_subdir);\n            nolist:\n            ALLOCA_FREE(alloca_subdir);\n            if (matches >= max_ls_files) {\n                goto toomany;\n            }\n            if (chdir(\"..\")) {    /* defensive in the extreme... */\n                if (chdir(wd) || chdir(name)) {    /* someone rmdir()'d it? */\n                    die(421, LOG_ERR, \"chdir: %s\", strerror(errno));\n                }\n            }\n        }\n        r++;\n    }\n    toomany:\n    free(names);\n    free(dir);\n    names = NULL;\n}",
        "func": "static void listdir(unsigned int depth, int f, void * const tls_fd,\n                    const char *name)\n{\n    PureFileInfo *dir;\n    char *names;\n    PureFileInfo *s;\n    PureFileInfo *r;\n    char *alloca_subdir;\n    size_t sizeof_subdir;\n    int d;\n\n    if (depth >= max_ls_depth || matches >= max_ls_files) {\n        return;\n    }\n    if ((dir = sreaddir(&names)) == NULL) {\n        addreply(226, MSG_CANT_READ_FILE, name);\n        return;\n    }\n    s = dir;\n    while (s->name_offset != (size_t) -1) {\n        d = 0;\n        if (FI_NAME(s)[0] != '.') {\n            d = listfile(s, NULL);\n        } else if (opt_a) {\n            if (FI_NAME(s)[1] == 0 ||\n                (FI_NAME(s)[1] == '.' && FI_NAME(s)[2] == 0)) {\n                listfile(s, NULL);\n            } else {\n                d = listfile(s, NULL);\n            }\n        }\n        if (!d) {\n            s->name_offset = (size_t) -1;\n        }\n        s++;\n    }\n    outputfiles(f, tls_fd);\n    r = dir;\n    sizeof_subdir = PATH_MAX + 1U;\n    if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {\n        goto toomany;\n    }\n    while (opt_R && r != s) {\n        if (r->name_offset != (size_t) -1 && !chdir(FI_NAME(r))) {\n            if (SNCHECK(snprintf(alloca_subdir, sizeof_subdir, \"%s/%s\",\n                                 name, FI_NAME(r)), sizeof_subdir)) {\n                goto nolist;\n            }\n            wrstr(f, tls_fd, \"\\r\\n\\r\\n\");\n            wrstr(f, tls_fd, alloca_subdir);\n            wrstr(f, tls_fd, \":\\r\\n\\r\\n\");\n            listdir(depth + 1U, f, tls_fd, alloca_subdir);\n\n            nolist:\n            if (matches >= max_ls_files) {\n                goto toomany;\n            }\n            if (chdir(\"..\")) {    /* defensive in the extreme... */\n                if (chdir(wd) || chdir(name)) {    /* someone rmdir()'d it? */\n                    die(421, LOG_ERR, \"chdir: %s\", strerror(errno));\n                }\n            }\n        }\n        r++;\n    }\n    toomany:\n    ALLOCA_FREE(alloca_subdir);\n    free(names);\n    free(dir);\n    names = NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,8 @@\n     char *names;\n     PureFileInfo *s;\n     PureFileInfo *r;\n+    char *alloca_subdir;\n+    size_t sizeof_subdir;\n     int d;\n \n     if (depth >= max_ls_depth || matches >= max_ls_files) {\n@@ -34,14 +36,12 @@\n     }\n     outputfiles(f, tls_fd);\n     r = dir;\n+    sizeof_subdir = PATH_MAX + 1U;\n+    if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {\n+        goto toomany;\n+    }\n     while (opt_R && r != s) {\n         if (r->name_offset != (size_t) -1 && !chdir(FI_NAME(r))) {\n-            char *alloca_subdir;\n-            const size_t sizeof_subdir = PATH_MAX + 1U;\n-\n-            if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {\n-                goto toomany;\n-            }\n             if (SNCHECK(snprintf(alloca_subdir, sizeof_subdir, \"%s/%s\",\n                                  name, FI_NAME(r)), sizeof_subdir)) {\n                 goto nolist;\n@@ -50,8 +50,8 @@\n             wrstr(f, tls_fd, alloca_subdir);\n             wrstr(f, tls_fd, \":\\r\\n\\r\\n\");\n             listdir(depth + 1U, f, tls_fd, alloca_subdir);\n+\n             nolist:\n-            ALLOCA_FREE(alloca_subdir);\n             if (matches >= max_ls_files) {\n                 goto toomany;\n             }\n@@ -64,6 +64,7 @@\n         r++;\n     }\n     toomany:\n+    ALLOCA_FREE(alloca_subdir);\n     free(names);\n     free(dir);\n     names = NULL;",
        "diff_line_info": {
            "deleted_lines": [
                "            char *alloca_subdir;",
                "            const size_t sizeof_subdir = PATH_MAX + 1U;",
                "",
                "            if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {",
                "                goto toomany;",
                "            }",
                "            ALLOCA_FREE(alloca_subdir);"
            ],
            "added_lines": [
                "    char *alloca_subdir;",
                "    size_t sizeof_subdir;",
                "    sizeof_subdir = PATH_MAX + 1U;",
                "    if ((alloca_subdir = ALLOCA(sizeof_subdir)) == NULL) {",
                "        goto toomany;",
                "    }",
                "",
                "    ALLOCA_FREE(alloca_subdir);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5390",
        "func_name": "kernel/git/netdev/net/tcp_data_queue_ofo",
        "description": "Linux kernel versions 4.9+ can be forced to make very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet which can lead to a denial of service.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?h=1a4f14bab1868b443f0dd3c55b689a478f82e72e",
        "commit_title": "Eric Dumazet says:",
        "commit_text": " ==================== Juha-Matti Tilli reported that malicious peers could inject tiny packets in out_of_order_queue, forcing very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet.  With tcp_rmem[2] default of 6MB, the ooo queue could contain ~7000 nodes.  This patch series makes sure we cut cpu cycles enough to render the attack not critical.  We might in the future go further, like disconnecting or black-holing proven malicious flows. ====================  ",
        "func_before": "static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\tbool fragstolen;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\tSOCK_DEBUG(sk, \"out of order segment: rcv_next %X seq %X - %X\\n\",\n\t\t   tp->rcv_nxt, seq, end_seq);\n\n\tp = &tp->out_of_order_queue.rb_node;\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t/* Initial out of order segment, build 1 SACK. */\n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = seq;\n\t\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\t}\n\t\trb_link_node(&skb->rbnode, NULL, p);\n\t\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\t\ttp->ooo_last_skb = skb;\n\t\tgoto end;\n\t}\n\n\t/* In the typical case, we are adding an skb to the end of the list.\n\t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\n\t */\n\tif (tcp_try_coalesce(sk, tp->ooo_last_skb,\n\t\t\t     skb, &fragstolen)) {\ncoalesce_done:\n\t\ttcp_grow_window(sk, skb);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\tskb = NULL;\n\t\tgoto add_sack;\n\t}\n\t/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */\n\tif (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {\n\t\tparent = &tp->ooo_last_skb->rbnode;\n\t\tp = &parent->rb_right;\n\t\tgoto insert;\n\t}\n\n\t/* Find place to insert this segment. Handle overlaps on the way. */\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t\t/* All the bits are present. Drop. */\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\t__kfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\t\tgoto add_sack;\n\t\t\t}\n\t\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t\t/* Partial overlap. */\n\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);\n\t\t\t} else {\n\t\t\t\t/* skb's seq == skb1's seq and skb covers skb1.\n\t\t\t\t * Replace skb1 with skb.\n\t\t\t\t */\n\t\t\t\trb_replace_node(&skb1->rbnode, &skb->rbnode,\n\t\t\t\t\t\t&tp->out_of_order_queue);\n\t\t\t\ttcp_dsack_extend(sk,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\t__kfree_skb(skb1);\n\t\t\t\tgoto merge_right;\n\t\t\t}\n\t\t} else if (tcp_try_coalesce(sk, skb1,\n\t\t\t\t\t    skb, &fragstolen)) {\n\t\t\tgoto coalesce_done;\n\t\t}\n\t\tp = &parent->rb_right;\n\t}\ninsert:\n\t/* Insert segment into RB tree. */\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\nmerge_right:\n\t/* Remove other segments covered by skb. */\n\twhile ((skb1 = skb_rb_next(skb)) != NULL) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\trb_erase(&skb1->rbnode, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop(sk, skb1);\n\t}\n\t/* If there is no skb after us, we are the last_skb ! */\n\tif (!skb1)\n\t\ttp->ooo_last_skb = skb;\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\ttcp_grow_window(sk, skb);\n\t\tskb_condense(skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n}",
        "func": "static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node **p, *parent;\n\tstruct sk_buff *skb1;\n\tu32 seq, end_seq;\n\tbool fragstolen;\n\n\ttcp_ecn_check_ce(sk, skb);\n\n\tif (unlikely(tcp_try_rmem_schedule(sk, skb, skb->truesize))) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);\n\t\ttcp_drop(sk, skb);\n\t\treturn;\n\t}\n\n\t/* Disable header prediction. */\n\ttp->pred_flags = 0;\n\tinet_csk_schedule_ack(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);\n\tseq = TCP_SKB_CB(skb)->seq;\n\tend_seq = TCP_SKB_CB(skb)->end_seq;\n\tSOCK_DEBUG(sk, \"out of order segment: rcv_next %X seq %X - %X\\n\",\n\t\t   tp->rcv_nxt, seq, end_seq);\n\n\tp = &tp->out_of_order_queue.rb_node;\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue)) {\n\t\t/* Initial out of order segment, build 1 SACK. */\n\t\tif (tcp_is_sack(tp)) {\n\t\t\ttp->rx_opt.num_sacks = 1;\n\t\t\ttp->selective_acks[0].start_seq = seq;\n\t\t\ttp->selective_acks[0].end_seq = end_seq;\n\t\t}\n\t\trb_link_node(&skb->rbnode, NULL, p);\n\t\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\t\ttp->ooo_last_skb = skb;\n\t\tgoto end;\n\t}\n\n\t/* In the typical case, we are adding an skb to the end of the list.\n\t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\n\t */\n\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,\n\t\t\t\t skb, &fragstolen)) {\ncoalesce_done:\n\t\ttcp_grow_window(sk, skb);\n\t\tkfree_skb_partial(skb, fragstolen);\n\t\tskb = NULL;\n\t\tgoto add_sack;\n\t}\n\t/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */\n\tif (!before(seq, TCP_SKB_CB(tp->ooo_last_skb)->end_seq)) {\n\t\tparent = &tp->ooo_last_skb->rbnode;\n\t\tp = &parent->rb_right;\n\t\tgoto insert;\n\t}\n\n\t/* Find place to insert this segment. Handle overlaps on the way. */\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tskb1 = rb_to_skb(parent);\n\t\tif (before(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\tp = &parent->rb_left;\n\t\t\tcontinue;\n\t\t}\n\t\tif (before(seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\t\t/* All the bits are present. Drop. */\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop(sk, skb);\n\t\t\t\tskb = NULL;\n\t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n\t\t\t\tgoto add_sack;\n\t\t\t}\n\t\t\tif (after(seq, TCP_SKB_CB(skb1)->seq)) {\n\t\t\t\t/* Partial overlap. */\n\t\t\t\ttcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)->end_seq);\n\t\t\t} else {\n\t\t\t\t/* skb's seq == skb1's seq and skb covers skb1.\n\t\t\t\t * Replace skb1 with skb.\n\t\t\t\t */\n\t\t\t\trb_replace_node(&skb1->rbnode, &skb->rbnode,\n\t\t\t\t\t\t&tp->out_of_order_queue);\n\t\t\t\ttcp_dsack_extend(sk,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n\t\t\t\ttcp_drop(sk, skb1);\n\t\t\t\tgoto merge_right;\n\t\t\t}\n\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,\n\t\t\t\t\t\tskb, &fragstolen)) {\n\t\t\tgoto coalesce_done;\n\t\t}\n\t\tp = &parent->rb_right;\n\t}\ninsert:\n\t/* Insert segment into RB tree. */\n\trb_link_node(&skb->rbnode, parent, p);\n\trb_insert_color(&skb->rbnode, &tp->out_of_order_queue);\n\nmerge_right:\n\t/* Remove other segments covered by skb. */\n\twhile ((skb1 = skb_rb_next(skb)) != NULL) {\n\t\tif (!after(end_seq, TCP_SKB_CB(skb1)->seq))\n\t\t\tbreak;\n\t\tif (before(end_seq, TCP_SKB_CB(skb1)->end_seq)) {\n\t\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t\t end_seq);\n\t\t\tbreak;\n\t\t}\n\t\trb_erase(&skb1->rbnode, &tp->out_of_order_queue);\n\t\ttcp_dsack_extend(sk, TCP_SKB_CB(skb1)->seq,\n\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);\n\t\ttcp_drop(sk, skb1);\n\t}\n\t/* If there is no skb after us, we are the last_skb ! */\n\tif (!skb1)\n\t\ttp->ooo_last_skb = skb;\n\nadd_sack:\n\tif (tcp_is_sack(tp))\n\t\ttcp_sack_new_ofo_skb(sk, seq, end_seq);\nend:\n\tif (skb) {\n\t\ttcp_grow_window(sk, skb);\n\t\tskb_condense(skb);\n\t\tskb_set_owner_r(skb, sk);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,8 +41,8 @@\n \t/* In the typical case, we are adding an skb to the end of the list.\n \t * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.\n \t */\n-\tif (tcp_try_coalesce(sk, tp->ooo_last_skb,\n-\t\t\t     skb, &fragstolen)) {\n+\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,\n+\t\t\t\t skb, &fragstolen)) {\n coalesce_done:\n \t\ttcp_grow_window(sk, skb);\n \t\tkfree_skb_partial(skb, fragstolen);\n@@ -70,7 +70,7 @@\n \t\t\t\t/* All the bits are present. Drop. */\n \t\t\t\tNET_INC_STATS(sock_net(sk),\n \t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n-\t\t\t\t__kfree_skb(skb);\n+\t\t\t\ttcp_drop(sk, skb);\n \t\t\t\tskb = NULL;\n \t\t\t\ttcp_dsack_set(sk, seq, end_seq);\n \t\t\t\tgoto add_sack;\n@@ -89,11 +89,11 @@\n \t\t\t\t\t\t TCP_SKB_CB(skb1)->end_seq);\n \t\t\t\tNET_INC_STATS(sock_net(sk),\n \t\t\t\t\t      LINUX_MIB_TCPOFOMERGE);\n-\t\t\t\t__kfree_skb(skb1);\n+\t\t\t\ttcp_drop(sk, skb1);\n \t\t\t\tgoto merge_right;\n \t\t\t}\n-\t\t} else if (tcp_try_coalesce(sk, skb1,\n-\t\t\t\t\t    skb, &fragstolen)) {\n+\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,\n+\t\t\t\t\t\tskb, &fragstolen)) {\n \t\t\tgoto coalesce_done;\n \t\t}\n \t\tp = &parent->rb_right;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (tcp_try_coalesce(sk, tp->ooo_last_skb,",
                "\t\t\t     skb, &fragstolen)) {",
                "\t\t\t\t__kfree_skb(skb);",
                "\t\t\t\t__kfree_skb(skb1);",
                "\t\t} else if (tcp_try_coalesce(sk, skb1,",
                "\t\t\t\t\t    skb, &fragstolen)) {"
            ],
            "added_lines": [
                "\tif (tcp_ooo_try_coalesce(sk, tp->ooo_last_skb,",
                "\t\t\t\t skb, &fragstolen)) {",
                "\t\t\t\ttcp_drop(sk, skb);",
                "\t\t\t\ttcp_drop(sk, skb1);",
                "\t\t} else if (tcp_ooo_try_coalesce(sk, skb1,",
                "\t\t\t\t\t\tskb, &fragstolen)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5390",
        "func_name": "kernel/git/netdev/net/tcp_prune_queue",
        "description": "Linux kernel versions 4.9+ can be forced to make very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet which can lead to a denial of service.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?h=1a4f14bab1868b443f0dd3c55b689a478f82e72e",
        "commit_title": "Eric Dumazet says:",
        "commit_text": " ==================== Juha-Matti Tilli reported that malicious peers could inject tiny packets in out_of_order_queue, forcing very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet.  With tcp_rmem[2] default of 6MB, the ooo queue could contain ~7000 nodes.  This patch series makes sure we cut cpu cycles enough to render the attack not critical.  We might in the future go further, like disconnecting or black-holing proven malicious flows. ====================  ",
        "func_before": "static int tcp_prune_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tSOCK_DEBUG(sk, \"prune_queue: c=%x\\n\", tp->copied_seq);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue, NULL,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\tsk_mem_reclaim(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* Collapsing did not help, destructive actions follow.\n\t * This must not ever occur. */\n\n\ttcp_prune_ofo_queue(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* If we are really being abused, tell the caller to silently\n\t * drop receive data on the floor.  It will get retransmitted\n\t * and hopefully then we'll have sufficient space.\n\t */\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t/* Massive buffer overcommit. */\n\ttp->pred_flags = 0;\n\treturn -1;\n}",
        "func": "static int tcp_prune_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tSOCK_DEBUG(sk, \"prune_queue: c=%x\\n\", tp->copied_seq);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\ttcp_clamp_window(sk);\n\telse if (tcp_under_memory_pressure(sk))\n\t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\ttcp_collapse_ofo_queue(sk);\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\ttcp_collapse(sk, &sk->sk_receive_queue, NULL,\n\t\t\t     skb_peek(&sk->sk_receive_queue),\n\t\t\t     NULL,\n\t\t\t     tp->copied_seq, tp->rcv_nxt);\n\tsk_mem_reclaim(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* Collapsing did not help, destructive actions follow.\n\t * This must not ever occur. */\n\n\ttcp_prune_ofo_queue(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n\t\treturn 0;\n\n\t/* If we are really being abused, tell the caller to silently\n\t * drop receive data on the floor.  It will get retransmitted\n\t * and hopefully then we'll have sufficient space.\n\t */\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_RCVPRUNED);\n\n\t/* Massive buffer overcommit. */\n\ttp->pred_flags = 0;\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,9 @@\n \t\ttcp_clamp_window(sk);\n \telse if (tcp_under_memory_pressure(sk))\n \t\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, 4U * tp->advmss);\n+\n+\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)\n+\t\treturn 0;\n \n \ttcp_collapse_ofo_queue(sk);\n \tif (!skb_queue_empty(&sk->sk_receive_queue))",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)",
                "\t\treturn 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5390",
        "func_name": "kernel/git/netdev/net/tcp_collapse_ofo_queue",
        "description": "Linux kernel versions 4.9+ can be forced to make very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet which can lead to a denial of service.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?h=1a4f14bab1868b443f0dd3c55b689a478f82e72e",
        "commit_title": "Eric Dumazet says:",
        "commit_text": " ==================== Juha-Matti Tilli reported that malicious peers could inject tiny packets in out_of_order_queue, forcing very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet.  With tcp_rmem[2] default of 6MB, the ooo queue could contain ~7000 nodes.  This patch series makes sure we cut cpu cycles enough to render the attack not critical.  We might in the future go further, like disconnecting or black-holing proven malicious flows. ====================  ",
        "func_before": "static void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb, *head;\n\tu32 start, end;\n\n\tskb = skb_rb_first(&tp->out_of_order_queue);\nnew_range:\n\tif (!skb) {\n\t\ttp->ooo_last_skb = skb_rb_last(&tp->out_of_order_queue);\n\t\treturn;\n\t}\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\n\tfor (head = skb;;) {\n\t\tskb = skb_rb_next(skb);\n\n\t\t/* Range is terminated when we see a gap or when\n\t\t * we are at the queue end.\n\t\t */\n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n\t\t\t\t     head, skb, start, end);\n\t\t\tgoto new_range;\n\t\t}\n\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t}\n}",
        "func": "static void tcp_collapse_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 range_truesize, sum_tiny = 0;\n\tstruct sk_buff *skb, *head;\n\tu32 start, end;\n\n\tskb = skb_rb_first(&tp->out_of_order_queue);\nnew_range:\n\tif (!skb) {\n\t\ttp->ooo_last_skb = skb_rb_last(&tp->out_of_order_queue);\n\t\treturn;\n\t}\n\tstart = TCP_SKB_CB(skb)->seq;\n\tend = TCP_SKB_CB(skb)->end_seq;\n\trange_truesize = skb->truesize;\n\n\tfor (head = skb;;) {\n\t\tskb = skb_rb_next(skb);\n\n\t\t/* Range is terminated when we see a gap or when\n\t\t * we are at the queue end.\n\t\t */\n\t\tif (!skb ||\n\t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n\t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n\t\t\t/* Do not attempt collapsing tiny skbs */\n\t\t\tif (range_truesize != head->truesize ||\n\t\t\t    end - start >= SKB_WITH_OVERHEAD(SK_MEM_QUANTUM)) {\n\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n\t\t\t\t\t     head, skb, start, end);\n\t\t\t} else {\n\t\t\t\tsum_tiny += range_truesize;\n\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)\n\t\t\t\t\treturn;\n\t\t\t}\n\t\t\tgoto new_range;\n\t\t}\n\n\t\trange_truesize += skb->truesize;\n\t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n\t\t\tstart = TCP_SKB_CB(skb)->seq;\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, end))\n\t\t\tend = TCP_SKB_CB(skb)->end_seq;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n static void tcp_collapse_ofo_queue(struct sock *sk)\n {\n \tstruct tcp_sock *tp = tcp_sk(sk);\n+\tu32 range_truesize, sum_tiny = 0;\n \tstruct sk_buff *skb, *head;\n \tu32 start, end;\n \n@@ -12,6 +13,7 @@\n \t}\n \tstart = TCP_SKB_CB(skb)->seq;\n \tend = TCP_SKB_CB(skb)->end_seq;\n+\trange_truesize = skb->truesize;\n \n \tfor (head = skb;;) {\n \t\tskb = skb_rb_next(skb);\n@@ -22,11 +24,20 @@\n \t\tif (!skb ||\n \t\t    after(TCP_SKB_CB(skb)->seq, end) ||\n \t\t    before(TCP_SKB_CB(skb)->end_seq, start)) {\n-\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n-\t\t\t\t     head, skb, start, end);\n+\t\t\t/* Do not attempt collapsing tiny skbs */\n+\t\t\tif (range_truesize != head->truesize ||\n+\t\t\t    end - start >= SKB_WITH_OVERHEAD(SK_MEM_QUANTUM)) {\n+\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,\n+\t\t\t\t\t     head, skb, start, end);\n+\t\t\t} else {\n+\t\t\t\tsum_tiny += range_truesize;\n+\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)\n+\t\t\t\t\treturn;\n+\t\t\t}\n \t\t\tgoto new_range;\n \t\t}\n \n+\t\trange_truesize += skb->truesize;\n \t\tif (unlikely(before(TCP_SKB_CB(skb)->seq, start)))\n \t\t\tstart = TCP_SKB_CB(skb)->seq;\n \t\tif (after(TCP_SKB_CB(skb)->end_seq, end))",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,",
                "\t\t\t\t     head, skb, start, end);"
            ],
            "added_lines": [
                "\tu32 range_truesize, sum_tiny = 0;",
                "\trange_truesize = skb->truesize;",
                "\t\t\t/* Do not attempt collapsing tiny skbs */",
                "\t\t\tif (range_truesize != head->truesize ||",
                "\t\t\t    end - start >= SKB_WITH_OVERHEAD(SK_MEM_QUANTUM)) {",
                "\t\t\t\ttcp_collapse(sk, NULL, &tp->out_of_order_queue,",
                "\t\t\t\t\t     head, skb, start, end);",
                "\t\t\t} else {",
                "\t\t\t\tsum_tiny += range_truesize;",
                "\t\t\t\tif (sum_tiny > sk->sk_rcvbuf >> 3)",
                "\t\t\t\t\treturn;",
                "\t\t\t}",
                "\t\trange_truesize += skb->truesize;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5390",
        "func_name": "kernel/git/netdev/net/tcp_prune_ofo_queue",
        "description": "Linux kernel versions 4.9+ can be forced to make very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet which can lead to a denial of service.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/davem/net.git/commit/?h=1a4f14bab1868b443f0dd3c55b689a478f82e72e",
        "commit_title": "Eric Dumazet says:",
        "commit_text": " ==================== Juha-Matti Tilli reported that malicious peers could inject tiny packets in out_of_order_queue, forcing very expensive calls to tcp_collapse_ofo_queue() and tcp_prune_ofo_queue() for every incoming packet.  With tcp_rmem[2] default of 6MB, the ooo queue could contain ~7000 nodes.  This patch series makes sure we cut cpu cycles enough to render the attack not critical.  We might in the future go further, like disconnecting or black-holing proven malicious flows. ====================  ",
        "func_before": "static bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\tnode = &tp->ooo_last_skb->rbnode;\n\tdo {\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\ttcp_drop(sk, rb_to_skb(node));\n\t\tsk_mem_reclaim(sk);\n\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t    !tcp_under_memory_pressure(sk))\n\t\t\tbreak;\n\t\tnode = prev;\n\t} while (node);\n\ttp->ooo_last_skb = rb_to_skb(prev);\n\n\t/* Reset SACK state.  A conforming SACK implementation will\n\t * do the same at a timeout based retransmit.  When a connection\n\t * is in a sad state like this, we care only about integrity\n\t * of the connection not performance.\n\t */\n\tif (tp->rx_opt.sack_ok)\n\t\ttcp_sack_reset(&tp->rx_opt);\n\treturn true;\n}",
        "func": "static bool tcp_prune_ofo_queue(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct rb_node *node, *prev;\n\tint goal;\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n\t\treturn false;\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n\tgoal = sk->sk_rcvbuf >> 3;\n\tnode = &tp->ooo_last_skb->rbnode;\n\tdo {\n\t\tprev = rb_prev(node);\n\t\trb_erase(node, &tp->out_of_order_queue);\n\t\tgoal -= rb_to_skb(node)->truesize;\n\t\ttcp_drop(sk, rb_to_skb(node));\n\t\tif (!prev || goal <= 0) {\n\t\t\tsk_mem_reclaim(sk);\n\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t\t\t    !tcp_under_memory_pressure(sk))\n\t\t\t\tbreak;\n\t\t\tgoal = sk->sk_rcvbuf >> 3;\n\t\t}\n\t\tnode = prev;\n\t} while (node);\n\ttp->ooo_last_skb = rb_to_skb(prev);\n\n\t/* Reset SACK state.  A conforming SACK implementation will\n\t * do the same at a timeout based retransmit.  When a connection\n\t * is in a sad state like this, we care only about integrity\n\t * of the connection not performance.\n\t */\n\tif (tp->rx_opt.sack_ok)\n\t\ttcp_sack_reset(&tp->rx_opt);\n\treturn true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,20 +2,26 @@\n {\n \tstruct tcp_sock *tp = tcp_sk(sk);\n \tstruct rb_node *node, *prev;\n+\tint goal;\n \n \tif (RB_EMPTY_ROOT(&tp->out_of_order_queue))\n \t\treturn false;\n \n \tNET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);\n+\tgoal = sk->sk_rcvbuf >> 3;\n \tnode = &tp->ooo_last_skb->rbnode;\n \tdo {\n \t\tprev = rb_prev(node);\n \t\trb_erase(node, &tp->out_of_order_queue);\n+\t\tgoal -= rb_to_skb(node)->truesize;\n \t\ttcp_drop(sk, rb_to_skb(node));\n-\t\tsk_mem_reclaim(sk);\n-\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n-\t\t    !tcp_under_memory_pressure(sk))\n-\t\t\tbreak;\n+\t\tif (!prev || goal <= 0) {\n+\t\t\tsk_mem_reclaim(sk);\n+\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n+\t\t\t    !tcp_under_memory_pressure(sk))\n+\t\t\t\tbreak;\n+\t\t\tgoal = sk->sk_rcvbuf >> 3;\n+\t\t}\n \t\tnode = prev;\n \t} while (node);\n \ttp->ooo_last_skb = rb_to_skb(prev);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tsk_mem_reclaim(sk);",
                "\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&",
                "\t\t    !tcp_under_memory_pressure(sk))",
                "\t\t\tbreak;"
            ],
            "added_lines": [
                "\tint goal;",
                "\tgoal = sk->sk_rcvbuf >> 3;",
                "\t\tgoal -= rb_to_skb(node)->truesize;",
                "\t\tif (!prev || goal <= 0) {",
                "\t\t\tsk_mem_reclaim(sk);",
                "\t\t\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&",
                "\t\t\t    !tcp_under_memory_pressure(sk))",
                "\t\t\t\tbreak;",
                "\t\t\tgoal = sk->sk_rcvbuf >> 3;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-15853",
        "func_name": "xkbcommon/libxkbcommon/ExprResolveBoolean",
        "description": "Endless recursion exists in xkbcomp/expr.c in xkbcommon and libxkbcommon before 0.8.1, which could be used by local attackers to crash xkbcommon users by supplying a crafted keymap file that triggers boolean negation.",
        "git_url": "https://github.com/xkbcommon/libxkbcommon/commit/1f9d1248c07cda8aaff762429c0dce146de8632a",
        "commit_title": "xkbcomp: fix stack overflow when evaluating boolean negation",
        "commit_text": " The expression evaluator would go into an infinite recursion when evaluating something like this as a boolean: `!True`. Instead of recursing to just `True` and negating, it recursed to `!True` itself again.  Bug inherited from xkbcomp.  Caught with the afl fuzzer. ",
        "func_before": "bool\nExprResolveBoolean(struct xkb_context *ctx, const ExprDef *expr,\n                   bool *set_rtrn)\n{\n    bool ok = false;\n    const char *ident;\n\n    switch (expr->expr.op) {\n    case EXPR_VALUE:\n        if (expr->expr.value_type != EXPR_TYPE_BOOLEAN) {\n            log_err(ctx,\n                    \"Found constant of type %s where boolean was expected\\n\",\n                    expr_value_type_to_string(expr->expr.value_type));\n            return false;\n        }\n        *set_rtrn = expr->boolean.set;\n        return true;\n\n    case EXPR_IDENT:\n        ident = xkb_atom_text(ctx, expr->ident.ident);\n        if (ident) {\n            if (istreq(ident, \"true\") ||\n                istreq(ident, \"yes\") ||\n                istreq(ident, \"on\")) {\n                *set_rtrn = true;\n                return true;\n            }\n            else if (istreq(ident, \"false\") ||\n                     istreq(ident, \"no\") ||\n                     istreq(ident, \"off\")) {\n                *set_rtrn = false;\n                return true;\n            }\n        }\n        log_err(ctx, \"Identifier \\\"%s\\\" of type boolean is unknown\\n\", ident);\n        return false;\n\n    case EXPR_FIELD_REF:\n        log_err(ctx, \"Default \\\"%s.%s\\\" of type boolean is unknown\\n\",\n                xkb_atom_text(ctx, expr->field_ref.element),\n                xkb_atom_text(ctx, expr->field_ref.field));\n        return false;\n\n    case EXPR_INVERT:\n    case EXPR_NOT:\n        ok = ExprResolveBoolean(ctx, expr, set_rtrn);\n        if (ok)\n            *set_rtrn = !*set_rtrn;\n        return ok;\n    case EXPR_ADD:\n    case EXPR_SUBTRACT:\n    case EXPR_MULTIPLY:\n    case EXPR_DIVIDE:\n    case EXPR_ASSIGN:\n    case EXPR_NEGATE:\n    case EXPR_UNARY_PLUS:\n        log_err(ctx, \"%s of boolean values not permitted\\n\",\n                expr_op_type_to_string(expr->expr.op));\n        break;\n\n    default:\n        log_wsgo(ctx, \"Unknown operator %d in ResolveBoolean\\n\",\n                 expr->expr.op);\n        break;\n    }\n\n    return false;\n}",
        "func": "bool\nExprResolveBoolean(struct xkb_context *ctx, const ExprDef *expr,\n                   bool *set_rtrn)\n{\n    bool ok = false;\n    const char *ident;\n\n    switch (expr->expr.op) {\n    case EXPR_VALUE:\n        if (expr->expr.value_type != EXPR_TYPE_BOOLEAN) {\n            log_err(ctx,\n                    \"Found constant of type %s where boolean was expected\\n\",\n                    expr_value_type_to_string(expr->expr.value_type));\n            return false;\n        }\n        *set_rtrn = expr->boolean.set;\n        return true;\n\n    case EXPR_IDENT:\n        ident = xkb_atom_text(ctx, expr->ident.ident);\n        if (ident) {\n            if (istreq(ident, \"true\") ||\n                istreq(ident, \"yes\") ||\n                istreq(ident, \"on\")) {\n                *set_rtrn = true;\n                return true;\n            }\n            else if (istreq(ident, \"false\") ||\n                     istreq(ident, \"no\") ||\n                     istreq(ident, \"off\")) {\n                *set_rtrn = false;\n                return true;\n            }\n        }\n        log_err(ctx, \"Identifier \\\"%s\\\" of type boolean is unknown\\n\", ident);\n        return false;\n\n    case EXPR_FIELD_REF:\n        log_err(ctx, \"Default \\\"%s.%s\\\" of type boolean is unknown\\n\",\n                xkb_atom_text(ctx, expr->field_ref.element),\n                xkb_atom_text(ctx, expr->field_ref.field));\n        return false;\n\n    case EXPR_INVERT:\n    case EXPR_NOT:\n        ok = ExprResolveBoolean(ctx, expr->unary.child, set_rtrn);\n        if (ok)\n            *set_rtrn = !*set_rtrn;\n        return ok;\n    case EXPR_ADD:\n    case EXPR_SUBTRACT:\n    case EXPR_MULTIPLY:\n    case EXPR_DIVIDE:\n    case EXPR_ASSIGN:\n    case EXPR_NEGATE:\n    case EXPR_UNARY_PLUS:\n        log_err(ctx, \"%s of boolean values not permitted\\n\",\n                expr_op_type_to_string(expr->expr.op));\n        break;\n\n    default:\n        log_wsgo(ctx, \"Unknown operator %d in ResolveBoolean\\n\",\n                 expr->expr.op);\n        break;\n    }\n\n    return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,7 +43,7 @@\n \n     case EXPR_INVERT:\n     case EXPR_NOT:\n-        ok = ExprResolveBoolean(ctx, expr, set_rtrn);\n+        ok = ExprResolveBoolean(ctx, expr->unary.child, set_rtrn);\n         if (ok)\n             *set_rtrn = !*set_rtrn;\n         return ok;",
        "diff_line_info": {
            "deleted_lines": [
                "        ok = ExprResolveBoolean(ctx, expr, set_rtrn);"
            ],
            "added_lines": [
                "        ok = ExprResolveBoolean(ctx, expr->unary.child, set_rtrn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/OverridableHttpConfigParams",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/015c9bc58d5ebf733060e6b23f54bfb143155f9d",
        "commit_title": "Disables the support for multi-range request by default",
        "commit_text": "",
        "func_before": "OverridableHttpConfigParams()\n    : maintain_pristine_host_hdr(1),\n      chunking_enabled(1),\n      negative_caching_enabled(0),\n      negative_revalidating_enabled(0),\n      cache_when_to_revalidate(0),\n      keep_alive_enabled_in(1),\n      keep_alive_enabled_out(1),\n      keep_alive_post_out(1),\n      server_session_sharing_match(TS_SERVER_SESSION_SHARING_MATCH_BOTH),\n      auth_server_session_private(1),\n      fwd_proxy_auth_to_parent(0),\n      uncacheable_requests_bypass_parent(1),\n      attach_server_session_to_client(0),\n      forward_connect_method(0),\n      insert_age_in_response(1),\n      anonymize_remove_from(0),\n      anonymize_remove_referer(0),\n      anonymize_remove_user_agent(0),\n      anonymize_remove_cookie(0),\n      anonymize_remove_client_ip(0),\n      anonymize_insert_client_ip(1),\n      proxy_response_server_enabled(1),\n      proxy_response_hsts_include_subdomains(0),\n      insert_squid_x_forwarded_for(1),\n      insert_forwarded(HttpForwarded::OptionBitSet()),\n      send_http11_requests(1),\n      cache_http(1),\n      cache_ignore_client_no_cache(1),\n      cache_ignore_client_cc_max_age(0),\n      cache_ims_on_client_no_cache(1),\n      cache_ignore_server_no_cache(0),\n      cache_responses_to_cookies(1),\n      cache_ignore_auth(0),\n      cache_urls_that_look_dynamic(1),\n      cache_required_headers(2),\n      cache_range_lookup(1),\n      cache_range_write(0),\n      allow_multi_range(1),\n      cache_enable_default_vary_headers(0),\n      ignore_accept_mismatch(0),\n      ignore_accept_language_mismatch(0),\n      ignore_accept_encoding_mismatch(0),\n      ignore_accept_charset_mismatch(0),\n      insert_request_via_string(1),\n      insert_response_via_string(0),\n      doc_in_cache_skip_dns(1),\n      flow_control_enabled(0),\n      normalize_ae(0),\n      srv_enabled(0),\n      parent_failures_update_hostdb(0),\n      cache_open_write_fail_action(0),\n      post_check_content_length_enabled(1),\n      ssl_client_verify_server(0),\n      redirect_use_orig_cache_key(0),\n      number_of_redirections(0),\n      proxy_response_hsts_max_age(-1),\n      negative_caching_lifetime(1800),\n      negative_revalidating_lifetime(1800),\n      sock_recv_buffer_size_out(0),\n      sock_send_buffer_size_out(0),\n      sock_option_flag_out(0),\n      sock_packet_mark_out(0),\n      sock_packet_tos_out(0),\n      server_tcp_init_cwnd(0),\n      request_hdr_max_size(131072),\n      response_hdr_max_size(131072),\n      cache_heuristic_min_lifetime(3600),\n      cache_heuristic_max_lifetime(86400),\n      cache_guaranteed_min_lifetime(0),\n      cache_guaranteed_max_lifetime(31536000),\n      cache_max_stale_age(604800),\n      keep_alive_no_activity_timeout_in(120),\n      keep_alive_no_activity_timeout_out(120),\n      transaction_no_activity_timeout_in(30),\n      transaction_no_activity_timeout_out(30),\n      transaction_active_timeout_out(0),\n      transaction_active_timeout_in(900),\n      websocket_active_timeout(3600),\n      websocket_inactive_timeout(600),\n      origin_max_connections(0),\n      origin_max_connections_queue(0),\n      connect_attempts_max_retries(0),\n      connect_attempts_max_retries_dead_server(3),\n      connect_attempts_rr_retries(3),\n      connect_attempts_timeout(30),\n      post_connect_attempts_timeout(1800),\n      parent_connect_attempts(4),\n      parent_retry_time(300),\n      parent_fail_threshold(10),\n      per_parent_connect_attempts(2),\n      parent_connect_timeout(30),\n      down_server_timeout(300),\n      client_abort_threshold(10),\n      max_cache_open_read_retries(-1),\n      cache_open_read_retry_time(10),\n      cache_generation_number(-1),\n      max_cache_open_write_retries(1),\n      background_fill_active_timeout(60),\n      http_chunking_size(4096),\n      flow_high_water_mark(0),\n      flow_low_water_mark(0),\n      default_buffer_size_index(8),\n      default_buffer_water_mark(32768),\n      slow_log_threshold(0),\n      body_factory_template_base(NULL),\n      body_factory_template_base_len(0),\n      proxy_response_server_string(NULL),\n      proxy_response_server_string_len(0),\n      global_user_agent_header(NULL),\n      global_user_agent_header_size(0),\n      cache_heuristic_lm_factor(0.10),\n      background_fill_threshold(0.5),\n      client_cert_filename(NULL),\n      client_cert_filepath(NULL),\n      cache_vary_default_text(NULL),\n      cache_vary_default_images(NULL),\n      cache_vary_default_other(NULL)\n  {\n  }",
        "func": "OverridableHttpConfigParams()\n    : maintain_pristine_host_hdr(1),\n      chunking_enabled(1),\n      negative_caching_enabled(0),\n      negative_revalidating_enabled(0),\n      cache_when_to_revalidate(0),\n      keep_alive_enabled_in(1),\n      keep_alive_enabled_out(1),\n      keep_alive_post_out(1),\n      server_session_sharing_match(TS_SERVER_SESSION_SHARING_MATCH_BOTH),\n      auth_server_session_private(1),\n      fwd_proxy_auth_to_parent(0),\n      uncacheable_requests_bypass_parent(1),\n      attach_server_session_to_client(0),\n      forward_connect_method(0),\n      insert_age_in_response(1),\n      anonymize_remove_from(0),\n      anonymize_remove_referer(0),\n      anonymize_remove_user_agent(0),\n      anonymize_remove_cookie(0),\n      anonymize_remove_client_ip(0),\n      anonymize_insert_client_ip(1),\n      proxy_response_server_enabled(1),\n      proxy_response_hsts_include_subdomains(0),\n      insert_squid_x_forwarded_for(1),\n      insert_forwarded(HttpForwarded::OptionBitSet()),\n      send_http11_requests(1),\n      cache_http(1),\n      cache_ignore_client_no_cache(1),\n      cache_ignore_client_cc_max_age(0),\n      cache_ims_on_client_no_cache(1),\n      cache_ignore_server_no_cache(0),\n      cache_responses_to_cookies(1),\n      cache_ignore_auth(0),\n      cache_urls_that_look_dynamic(1),\n      cache_required_headers(2),\n      cache_range_lookup(1),\n      cache_range_write(0),\n      allow_multi_range(0),\n      cache_enable_default_vary_headers(0),\n      ignore_accept_mismatch(0),\n      ignore_accept_language_mismatch(0),\n      ignore_accept_encoding_mismatch(0),\n      ignore_accept_charset_mismatch(0),\n      insert_request_via_string(1),\n      insert_response_via_string(0),\n      doc_in_cache_skip_dns(1),\n      flow_control_enabled(0),\n      normalize_ae(0),\n      srv_enabled(0),\n      parent_failures_update_hostdb(0),\n      cache_open_write_fail_action(0),\n      post_check_content_length_enabled(1),\n      ssl_client_verify_server(0),\n      redirect_use_orig_cache_key(0),\n      number_of_redirections(0),\n      proxy_response_hsts_max_age(-1),\n      negative_caching_lifetime(1800),\n      negative_revalidating_lifetime(1800),\n      sock_recv_buffer_size_out(0),\n      sock_send_buffer_size_out(0),\n      sock_option_flag_out(0),\n      sock_packet_mark_out(0),\n      sock_packet_tos_out(0),\n      server_tcp_init_cwnd(0),\n      request_hdr_max_size(131072),\n      response_hdr_max_size(131072),\n      cache_heuristic_min_lifetime(3600),\n      cache_heuristic_max_lifetime(86400),\n      cache_guaranteed_min_lifetime(0),\n      cache_guaranteed_max_lifetime(31536000),\n      cache_max_stale_age(604800),\n      keep_alive_no_activity_timeout_in(120),\n      keep_alive_no_activity_timeout_out(120),\n      transaction_no_activity_timeout_in(30),\n      transaction_no_activity_timeout_out(30),\n      transaction_active_timeout_out(0),\n      transaction_active_timeout_in(900),\n      websocket_active_timeout(3600),\n      websocket_inactive_timeout(600),\n      origin_max_connections(0),\n      origin_max_connections_queue(0),\n      connect_attempts_max_retries(0),\n      connect_attempts_max_retries_dead_server(3),\n      connect_attempts_rr_retries(3),\n      connect_attempts_timeout(30),\n      post_connect_attempts_timeout(1800),\n      parent_connect_attempts(4),\n      parent_retry_time(300),\n      parent_fail_threshold(10),\n      per_parent_connect_attempts(2),\n      parent_connect_timeout(30),\n      down_server_timeout(300),\n      client_abort_threshold(10),\n      max_cache_open_read_retries(-1),\n      cache_open_read_retry_time(10),\n      cache_generation_number(-1),\n      max_cache_open_write_retries(1),\n      background_fill_active_timeout(60),\n      http_chunking_size(4096),\n      flow_high_water_mark(0),\n      flow_low_water_mark(0),\n      default_buffer_size_index(8),\n      default_buffer_water_mark(32768),\n      slow_log_threshold(0),\n      body_factory_template_base(NULL),\n      body_factory_template_base_len(0),\n      proxy_response_server_string(NULL),\n      proxy_response_server_string_len(0),\n      global_user_agent_header(NULL),\n      global_user_agent_header_size(0),\n      cache_heuristic_lm_factor(0.10),\n      background_fill_threshold(0.5),\n      client_cert_filename(NULL),\n      client_cert_filepath(NULL),\n      cache_vary_default_text(NULL),\n      cache_vary_default_images(NULL),\n      cache_vary_default_other(NULL)\n  {\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,7 +36,7 @@\n       cache_required_headers(2),\n       cache_range_lookup(1),\n       cache_range_write(0),\n-      allow_multi_range(1),\n+      allow_multi_range(0),\n       cache_enable_default_vary_headers(0),\n       ignore_accept_mismatch(0),\n       ignore_accept_language_mismatch(0),",
        "diff_line_info": {
            "deleted_lines": [
                "      allow_multi_range(1),"
            ],
            "added_lines": [
                "      allow_multi_range(0),"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/OverridableHttpConfigParams",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "OverridableHttpConfigParams()\n    : maintain_pristine_host_hdr(1),\n      chunking_enabled(1),\n      negative_caching_enabled(0),\n      negative_revalidating_enabled(0),\n      cache_when_to_revalidate(0),\n      keep_alive_enabled_in(1),\n      keep_alive_enabled_out(1),\n      keep_alive_post_out(1),\n      server_session_sharing_match(TS_SERVER_SESSION_SHARING_MATCH_BOTH),\n      auth_server_session_private(1),\n      fwd_proxy_auth_to_parent(0),\n      uncacheable_requests_bypass_parent(1),\n      attach_server_session_to_client(0),\n      forward_connect_method(0),\n      insert_age_in_response(1),\n      anonymize_remove_from(0),\n      anonymize_remove_referer(0),\n      anonymize_remove_user_agent(0),\n      anonymize_remove_cookie(0),\n      anonymize_remove_client_ip(0),\n      anonymize_insert_client_ip(1),\n      proxy_response_server_enabled(1),\n      proxy_response_hsts_include_subdomains(0),\n      insert_squid_x_forwarded_for(1),\n      insert_forwarded(HttpForwarded::OptionBitSet()),\n      send_http11_requests(1),\n      cache_http(1),\n      cache_ignore_client_no_cache(1),\n      cache_ignore_client_cc_max_age(0),\n      cache_ims_on_client_no_cache(1),\n      cache_ignore_server_no_cache(0),\n      cache_responses_to_cookies(1),\n      cache_ignore_auth(0),\n      cache_urls_that_look_dynamic(1),\n      cache_required_headers(2),\n      cache_range_lookup(1),\n      cache_range_write(0),\n      cache_enable_default_vary_headers(0),\n      ignore_accept_mismatch(0),\n      ignore_accept_language_mismatch(0),\n      ignore_accept_encoding_mismatch(0),\n      ignore_accept_charset_mismatch(0),\n      insert_request_via_string(1),\n      insert_response_via_string(0),\n      doc_in_cache_skip_dns(1),\n      flow_control_enabled(0),\n      normalize_ae(0),\n      srv_enabled(0),\n      parent_failures_update_hostdb(0),\n      cache_open_write_fail_action(0),\n      post_check_content_length_enabled(1),\n      ssl_client_verify_server(0),\n      redirect_use_orig_cache_key(0),\n      number_of_redirections(0),\n      proxy_response_hsts_max_age(-1),\n      negative_caching_lifetime(1800),\n      negative_revalidating_lifetime(1800),\n      sock_recv_buffer_size_out(0),\n      sock_send_buffer_size_out(0),\n      sock_option_flag_out(0),\n      sock_packet_mark_out(0),\n      sock_packet_tos_out(0),\n      server_tcp_init_cwnd(0),\n      request_hdr_max_size(131072),\n      response_hdr_max_size(131072),\n      cache_heuristic_min_lifetime(3600),\n      cache_heuristic_max_lifetime(86400),\n      cache_guaranteed_min_lifetime(0),\n      cache_guaranteed_max_lifetime(31536000),\n      cache_max_stale_age(604800),\n      keep_alive_no_activity_timeout_in(120),\n      keep_alive_no_activity_timeout_out(120),\n      transaction_no_activity_timeout_in(30),\n      transaction_no_activity_timeout_out(30),\n      transaction_active_timeout_out(0),\n      transaction_active_timeout_in(900),\n      websocket_active_timeout(3600),\n      websocket_inactive_timeout(600),\n      origin_max_connections(0),\n      origin_max_connections_queue(0),\n      connect_attempts_max_retries(0),\n      connect_attempts_max_retries_dead_server(3),\n      connect_attempts_rr_retries(3),\n      connect_attempts_timeout(30),\n      post_connect_attempts_timeout(1800),\n      parent_connect_attempts(4),\n      parent_retry_time(300),\n      parent_fail_threshold(10),\n      per_parent_connect_attempts(2),\n      parent_connect_timeout(30),\n      down_server_timeout(300),\n      client_abort_threshold(10),\n      max_cache_open_read_retries(-1),\n      cache_open_read_retry_time(10),\n      cache_generation_number(-1),\n      max_cache_open_write_retries(1),\n      background_fill_active_timeout(60),\n      http_chunking_size(4096),\n      flow_high_water_mark(0),\n      flow_low_water_mark(0),\n      default_buffer_size_index(8),\n      default_buffer_water_mark(32768),\n      slow_log_threshold(0),\n      body_factory_template_base(NULL),\n      body_factory_template_base_len(0),\n      proxy_response_server_string(NULL),\n      proxy_response_server_string_len(0),\n      global_user_agent_header(NULL),\n      global_user_agent_header_size(0),\n      cache_heuristic_lm_factor(0.10),\n      background_fill_threshold(0.5),\n      client_cert_filename(NULL),\n      client_cert_filepath(NULL),\n      cache_vary_default_text(NULL),\n      cache_vary_default_images(NULL),\n      cache_vary_default_other(NULL)\n  {\n  }",
        "func": "OverridableHttpConfigParams()\n    : maintain_pristine_host_hdr(1),\n      chunking_enabled(1),\n      negative_caching_enabled(0),\n      negative_revalidating_enabled(0),\n      cache_when_to_revalidate(0),\n      keep_alive_enabled_in(1),\n      keep_alive_enabled_out(1),\n      keep_alive_post_out(1),\n      server_session_sharing_match(TS_SERVER_SESSION_SHARING_MATCH_BOTH),\n      auth_server_session_private(1),\n      fwd_proxy_auth_to_parent(0),\n      uncacheable_requests_bypass_parent(1),\n      attach_server_session_to_client(0),\n      forward_connect_method(0),\n      insert_age_in_response(1),\n      anonymize_remove_from(0),\n      anonymize_remove_referer(0),\n      anonymize_remove_user_agent(0),\n      anonymize_remove_cookie(0),\n      anonymize_remove_client_ip(0),\n      anonymize_insert_client_ip(1),\n      proxy_response_server_enabled(1),\n      proxy_response_hsts_include_subdomains(0),\n      insert_squid_x_forwarded_for(1),\n      insert_forwarded(HttpForwarded::OptionBitSet()),\n      send_http11_requests(1),\n      cache_http(1),\n      cache_ignore_client_no_cache(1),\n      cache_ignore_client_cc_max_age(0),\n      cache_ims_on_client_no_cache(1),\n      cache_ignore_server_no_cache(0),\n      cache_responses_to_cookies(1),\n      cache_ignore_auth(0),\n      cache_urls_that_look_dynamic(1),\n      cache_required_headers(2),\n      cache_range_lookup(1),\n      cache_range_write(0),\n      allow_multi_range(1),\n      cache_enable_default_vary_headers(0),\n      ignore_accept_mismatch(0),\n      ignore_accept_language_mismatch(0),\n      ignore_accept_encoding_mismatch(0),\n      ignore_accept_charset_mismatch(0),\n      insert_request_via_string(1),\n      insert_response_via_string(0),\n      doc_in_cache_skip_dns(1),\n      flow_control_enabled(0),\n      normalize_ae(0),\n      srv_enabled(0),\n      parent_failures_update_hostdb(0),\n      cache_open_write_fail_action(0),\n      post_check_content_length_enabled(1),\n      ssl_client_verify_server(0),\n      redirect_use_orig_cache_key(0),\n      number_of_redirections(0),\n      proxy_response_hsts_max_age(-1),\n      negative_caching_lifetime(1800),\n      negative_revalidating_lifetime(1800),\n      sock_recv_buffer_size_out(0),\n      sock_send_buffer_size_out(0),\n      sock_option_flag_out(0),\n      sock_packet_mark_out(0),\n      sock_packet_tos_out(0),\n      server_tcp_init_cwnd(0),\n      request_hdr_max_size(131072),\n      response_hdr_max_size(131072),\n      cache_heuristic_min_lifetime(3600),\n      cache_heuristic_max_lifetime(86400),\n      cache_guaranteed_min_lifetime(0),\n      cache_guaranteed_max_lifetime(31536000),\n      cache_max_stale_age(604800),\n      keep_alive_no_activity_timeout_in(120),\n      keep_alive_no_activity_timeout_out(120),\n      transaction_no_activity_timeout_in(30),\n      transaction_no_activity_timeout_out(30),\n      transaction_active_timeout_out(0),\n      transaction_active_timeout_in(900),\n      websocket_active_timeout(3600),\n      websocket_inactive_timeout(600),\n      origin_max_connections(0),\n      origin_max_connections_queue(0),\n      connect_attempts_max_retries(0),\n      connect_attempts_max_retries_dead_server(3),\n      connect_attempts_rr_retries(3),\n      connect_attempts_timeout(30),\n      post_connect_attempts_timeout(1800),\n      parent_connect_attempts(4),\n      parent_retry_time(300),\n      parent_fail_threshold(10),\n      per_parent_connect_attempts(2),\n      parent_connect_timeout(30),\n      down_server_timeout(300),\n      client_abort_threshold(10),\n      max_cache_open_read_retries(-1),\n      cache_open_read_retry_time(10),\n      cache_generation_number(-1),\n      max_cache_open_write_retries(1),\n      background_fill_active_timeout(60),\n      http_chunking_size(4096),\n      flow_high_water_mark(0),\n      flow_low_water_mark(0),\n      default_buffer_size_index(8),\n      default_buffer_water_mark(32768),\n      slow_log_threshold(0),\n      body_factory_template_base(NULL),\n      body_factory_template_base_len(0),\n      proxy_response_server_string(NULL),\n      proxy_response_server_string_len(0),\n      global_user_agent_header(NULL),\n      global_user_agent_header_size(0),\n      cache_heuristic_lm_factor(0.10),\n      background_fill_threshold(0.5),\n      client_cert_filename(NULL),\n      client_cert_filepath(NULL),\n      cache_vary_default_text(NULL),\n      cache_vary_default_images(NULL),\n      cache_vary_default_other(NULL)\n  {\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,6 +36,7 @@\n       cache_required_headers(2),\n       cache_range_lookup(1),\n       cache_range_write(0),\n+      allow_multi_range(1),\n       cache_enable_default_vary_headers(0),\n       ignore_accept_mismatch(0),\n       ignore_accept_language_mismatch(0),",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      allow_multi_range(1),"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/HttpConfig::reconfigure",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "void\nHttpConfig::reconfigure()\n{\n#define INT_TO_BOOL(i) ((i) ? 1 : 0);\n\n  HttpConfigParams *params;\n\n  params = new HttpConfigParams;\n\n  params->inbound_ip4 = m_master.inbound_ip4;\n  params->inbound_ip6 = m_master.inbound_ip6;\n\n  params->outbound_ip4 = m_master.outbound_ip4;\n  params->outbound_ip6 = m_master.outbound_ip6;\n\n  params->proxy_hostname                           = ats_strdup(m_master.proxy_hostname);\n  params->proxy_hostname_len                       = (params->proxy_hostname) ? strlen(params->proxy_hostname) : 0;\n  params->no_dns_forward_to_parent                 = INT_TO_BOOL(m_master.no_dns_forward_to_parent);\n  params->oride.uncacheable_requests_bypass_parent = INT_TO_BOOL(m_master.oride.uncacheable_requests_bypass_parent);\n  params->no_origin_server_dns                     = INT_TO_BOOL(m_master.no_origin_server_dns);\n  params->use_client_target_addr                   = m_master.use_client_target_addr;\n  params->use_client_source_port                   = INT_TO_BOOL(m_master.use_client_source_port);\n  params->oride.maintain_pristine_host_hdr         = INT_TO_BOOL(m_master.oride.maintain_pristine_host_hdr);\n\n  params->disable_ssl_parenting        = INT_TO_BOOL(m_master.disable_ssl_parenting);\n  params->oride.forward_connect_method = INT_TO_BOOL(m_master.oride.forward_connect_method);\n\n  params->server_max_connections             = m_master.server_max_connections;\n  params->max_websocket_connections          = m_master.max_websocket_connections;\n  params->oride.server_tcp_init_cwnd         = m_master.oride.server_tcp_init_cwnd;\n  params->oride.origin_max_connections       = m_master.oride.origin_max_connections;\n  params->oride.origin_max_connections_queue = m_master.oride.origin_max_connections_queue;\n  // if origin_max_connections_queue is set without max_connections, it is meaningless, so we'll warn\n  if (params->oride.origin_max_connections_queue > 0 &&\n      !(params->oride.origin_max_connections || params->origin_min_keep_alive_connections)) {\n    Warning(\"origin_max_connections_queue is set, but neither origin_max_connections nor origin_min_keep_alive_connections are \"\n            \"set, please correct your records.config\");\n  }\n  params->origin_min_keep_alive_connections     = m_master.origin_min_keep_alive_connections;\n  params->oride.attach_server_session_to_client = m_master.oride.attach_server_session_to_client;\n\n  if (params->oride.origin_max_connections && params->oride.origin_max_connections < params->origin_min_keep_alive_connections) {\n    Warning(\"origin_max_connections < origin_min_keep_alive_connections, setting min=max , please correct your records.config\");\n    params->origin_min_keep_alive_connections = params->oride.origin_max_connections;\n  }\n\n  params->oride.insert_request_via_string   = m_master.oride.insert_request_via_string;\n  params->oride.insert_response_via_string  = m_master.oride.insert_response_via_string;\n  params->proxy_request_via_string          = ats_strdup(m_master.proxy_request_via_string);\n  params->proxy_request_via_string_len      = (params->proxy_request_via_string) ? strlen(params->proxy_request_via_string) : 0;\n  params->proxy_response_via_string         = ats_strdup(m_master.proxy_response_via_string);\n  params->proxy_response_via_string_len     = (params->proxy_response_via_string) ? strlen(params->proxy_response_via_string) : 0;\n  params->oride.proxy_response_hsts_max_age = m_master.oride.proxy_response_hsts_max_age;\n  params->oride.proxy_response_hsts_include_subdomains = m_master.oride.proxy_response_hsts_include_subdomains;\n\n  params->oride.keep_alive_enabled_in       = INT_TO_BOOL(m_master.oride.keep_alive_enabled_in);\n  params->oride.keep_alive_enabled_out      = INT_TO_BOOL(m_master.oride.keep_alive_enabled_out);\n  params->oride.chunking_enabled            = INT_TO_BOOL(m_master.oride.chunking_enabled);\n  params->oride.auth_server_session_private = INT_TO_BOOL(m_master.oride.auth_server_session_private);\n\n  params->oride.http_chunking_size = m_master.oride.http_chunking_size;\n\n  params->oride.post_check_content_length_enabled = INT_TO_BOOL(m_master.oride.post_check_content_length_enabled);\n\n  params->oride.flow_control_enabled = INT_TO_BOOL(m_master.oride.flow_control_enabled);\n  params->oride.flow_high_water_mark = m_master.oride.flow_high_water_mark;\n  params->oride.flow_low_water_mark  = m_master.oride.flow_low_water_mark;\n  // If not set (zero) then make values the same.\n  if (params->oride.flow_low_water_mark <= 0) {\n    params->oride.flow_low_water_mark = params->oride.flow_high_water_mark;\n  }\n  if (params->oride.flow_high_water_mark <= 0) {\n    params->oride.flow_high_water_mark = params->oride.flow_low_water_mark;\n  }\n  if (params->oride.flow_high_water_mark < params->oride.flow_low_water_mark) {\n    Warning(\"Flow control low water mark is greater than high water mark, flow control disabled\");\n    params->oride.flow_control_enabled = 0;\n    // zero means \"hardwired default\" when actually used.\n    params->oride.flow_high_water_mark = params->oride.flow_low_water_mark = 0;\n  }\n\n  params->oride.server_session_sharing_match = m_master.oride.server_session_sharing_match;\n  params->server_session_sharing_pool        = m_master.server_session_sharing_pool;\n  params->oride.keep_alive_post_out          = m_master.oride.keep_alive_post_out;\n\n  params->oride.keep_alive_no_activity_timeout_in   = m_master.oride.keep_alive_no_activity_timeout_in;\n  params->oride.keep_alive_no_activity_timeout_out  = m_master.oride.keep_alive_no_activity_timeout_out;\n  params->oride.transaction_no_activity_timeout_in  = m_master.oride.transaction_no_activity_timeout_in;\n  params->oride.transaction_no_activity_timeout_out = m_master.oride.transaction_no_activity_timeout_out;\n  params->oride.transaction_active_timeout_in       = m_master.oride.transaction_active_timeout_in;\n  params->oride.transaction_active_timeout_out      = m_master.oride.transaction_active_timeout_out;\n  params->oride.websocket_active_timeout            = m_master.oride.websocket_active_timeout;\n  params->oride.websocket_inactive_timeout          = m_master.oride.websocket_inactive_timeout;\n  params->accept_no_activity_timeout                = m_master.accept_no_activity_timeout;\n  params->oride.background_fill_active_timeout      = m_master.oride.background_fill_active_timeout;\n  params->oride.background_fill_threshold           = m_master.oride.background_fill_threshold;\n\n  params->oride.connect_attempts_max_retries             = m_master.oride.connect_attempts_max_retries;\n  params->oride.connect_attempts_max_retries_dead_server = m_master.oride.connect_attempts_max_retries_dead_server;\n  if (m_master.oride.connect_attempts_rr_retries > params->oride.connect_attempts_max_retries) {\n    Warning(\"connect_attempts_rr_retries (%\" PRIu64 \") is greater than \"\n            \"connect_attempts_max_retries (%\" PRIu64 \"), this means requests \"\n            \"will never redispatch to another server\",\n            m_master.oride.connect_attempts_rr_retries, params->oride.connect_attempts_max_retries);\n  }\n  params->oride.connect_attempts_rr_retries   = m_master.oride.connect_attempts_rr_retries;\n  params->oride.connect_attempts_timeout      = m_master.oride.connect_attempts_timeout;\n  params->oride.post_connect_attempts_timeout = m_master.oride.post_connect_attempts_timeout;\n  params->oride.parent_connect_attempts       = m_master.oride.parent_connect_attempts;\n  params->oride.parent_retry_time             = m_master.oride.parent_retry_time;\n  params->oride.parent_fail_threshold         = m_master.oride.parent_fail_threshold;\n  params->oride.per_parent_connect_attempts   = m_master.oride.per_parent_connect_attempts;\n  params->oride.parent_connect_timeout        = m_master.oride.parent_connect_timeout;\n  params->oride.parent_failures_update_hostdb = m_master.oride.parent_failures_update_hostdb;\n\n  params->oride.sock_recv_buffer_size_out = m_master.oride.sock_recv_buffer_size_out;\n  params->oride.sock_send_buffer_size_out = m_master.oride.sock_send_buffer_size_out;\n  params->oride.sock_packet_mark_out      = m_master.oride.sock_packet_mark_out;\n  params->oride.sock_packet_tos_out       = m_master.oride.sock_packet_tos_out;\n  params->oride.sock_option_flag_out      = m_master.oride.sock_option_flag_out;\n\n  // Clear the TCP Fast Open option if it is not supported on this host.\n  if ((params->oride.sock_option_flag_out & NetVCOptions::SOCK_OPT_TCP_FAST_OPEN) && !SocketManager::fastopen_supported()) {\n    Status(\"disabling unsupported TCP Fast Open flag on proxy.config.net.sock_option_flag_out\");\n    params->oride.sock_option_flag_out &= ~NetVCOptions::SOCK_OPT_TCP_FAST_OPEN;\n  }\n\n  params->oride.fwd_proxy_auth_to_parent = INT_TO_BOOL(m_master.oride.fwd_proxy_auth_to_parent);\n\n  params->oride.anonymize_remove_from       = INT_TO_BOOL(m_master.oride.anonymize_remove_from);\n  params->oride.anonymize_remove_referer    = INT_TO_BOOL(m_master.oride.anonymize_remove_referer);\n  params->oride.anonymize_remove_user_agent = INT_TO_BOOL(m_master.oride.anonymize_remove_user_agent);\n  params->oride.anonymize_remove_cookie     = INT_TO_BOOL(m_master.oride.anonymize_remove_cookie);\n  params->oride.anonymize_remove_client_ip  = INT_TO_BOOL(m_master.oride.anonymize_remove_client_ip);\n  params->oride.anonymize_insert_client_ip  = INT_TO_BOOL(m_master.oride.anonymize_insert_client_ip);\n  params->anonymize_other_header_list       = ats_strdup(m_master.anonymize_other_header_list);\n\n  params->oride.global_user_agent_header = ats_strdup(m_master.oride.global_user_agent_header);\n  params->oride.global_user_agent_header_size =\n    params->oride.global_user_agent_header ? strlen(params->oride.global_user_agent_header) : 0;\n\n  params->oride.proxy_response_server_string = ats_strdup(m_master.oride.proxy_response_server_string);\n  params->oride.proxy_response_server_string_len =\n    params->oride.proxy_response_server_string ? strlen(params->oride.proxy_response_server_string) : 0;\n  params->oride.proxy_response_server_enabled = m_master.oride.proxy_response_server_enabled;\n\n  params->oride.insert_squid_x_forwarded_for = INT_TO_BOOL(m_master.oride.insert_squid_x_forwarded_for);\n  params->oride.insert_forwarded             = m_master.oride.insert_forwarded;\n  params->oride.insert_age_in_response       = INT_TO_BOOL(m_master.oride.insert_age_in_response);\n  params->enable_http_stats                  = INT_TO_BOOL(m_master.enable_http_stats);\n  params->oride.normalize_ae                 = m_master.oride.normalize_ae;\n\n  params->oride.cache_heuristic_min_lifetime = m_master.oride.cache_heuristic_min_lifetime;\n  params->oride.cache_heuristic_max_lifetime = m_master.oride.cache_heuristic_max_lifetime;\n  params->oride.cache_heuristic_lm_factor    = std::min(std::max(m_master.oride.cache_heuristic_lm_factor, 0.0f), 1.0f);\n\n  params->oride.cache_guaranteed_min_lifetime = m_master.oride.cache_guaranteed_min_lifetime;\n  params->oride.cache_guaranteed_max_lifetime = m_master.oride.cache_guaranteed_max_lifetime;\n\n  params->oride.cache_max_stale_age = m_master.oride.cache_max_stale_age;\n\n  params->oride.cache_vary_default_text   = ats_strdup(m_master.oride.cache_vary_default_text);\n  params->oride.cache_vary_default_images = ats_strdup(m_master.oride.cache_vary_default_images);\n  params->oride.cache_vary_default_other  = ats_strdup(m_master.oride.cache_vary_default_other);\n\n  params->oride.srv_enabled = m_master.oride.srv_enabled;\n\n  // open read failure retries\n  params->oride.max_cache_open_read_retries = m_master.oride.max_cache_open_read_retries;\n  params->oride.cache_open_read_retry_time  = m_master.oride.cache_open_read_retry_time;\n  params->oride.cache_generation_number     = m_master.oride.cache_generation_number;\n\n  // open write failure retries\n  params->oride.max_cache_open_write_retries = m_master.oride.max_cache_open_write_retries;\n\n  params->oride.cache_http                        = INT_TO_BOOL(m_master.oride.cache_http);\n  params->oride.cache_ignore_client_no_cache      = INT_TO_BOOL(m_master.oride.cache_ignore_client_no_cache);\n  params->oride.cache_ignore_client_cc_max_age    = INT_TO_BOOL(m_master.oride.cache_ignore_client_cc_max_age);\n  params->oride.cache_ims_on_client_no_cache      = INT_TO_BOOL(m_master.oride.cache_ims_on_client_no_cache);\n  params->oride.cache_ignore_server_no_cache      = INT_TO_BOOL(m_master.oride.cache_ignore_server_no_cache);\n  params->oride.cache_responses_to_cookies        = m_master.oride.cache_responses_to_cookies;\n  params->oride.cache_ignore_auth                 = INT_TO_BOOL(m_master.oride.cache_ignore_auth);\n  params->oride.cache_urls_that_look_dynamic      = INT_TO_BOOL(m_master.oride.cache_urls_that_look_dynamic);\n  params->oride.cache_enable_default_vary_headers = INT_TO_BOOL(m_master.oride.cache_enable_default_vary_headers);\n  params->cache_post_method                       = INT_TO_BOOL(m_master.cache_post_method);\n\n  params->oride.ignore_accept_mismatch          = m_master.oride.ignore_accept_mismatch;\n  params->oride.ignore_accept_language_mismatch = m_master.oride.ignore_accept_language_mismatch;\n  params->oride.ignore_accept_encoding_mismatch = m_master.oride.ignore_accept_encoding_mismatch;\n  params->oride.ignore_accept_charset_mismatch  = m_master.oride.ignore_accept_charset_mismatch;\n\n  params->send_100_continue_response = INT_TO_BOOL(m_master.send_100_continue_response);\n  params->disallow_post_100_continue = INT_TO_BOOL(m_master.disallow_post_100_continue);\n  params->parser_allow_non_http      = INT_TO_BOOL(m_master.parser_allow_non_http);\n  params->keepalive_internal_vc      = INT_TO_BOOL(m_master.keepalive_internal_vc);\n\n  params->oride.cache_open_write_fail_action = m_master.oride.cache_open_write_fail_action;\n\n  params->oride.cache_when_to_revalidate = m_master.oride.cache_when_to_revalidate;\n  params->max_post_size                  = m_master.max_post_size;\n\n  params->oride.cache_required_headers = m_master.oride.cache_required_headers;\n  params->oride.cache_range_lookup     = INT_TO_BOOL(m_master.oride.cache_range_lookup);\n  params->oride.cache_range_write      = INT_TO_BOOL(m_master.oride.cache_range_write);\n\n  params->connect_ports_string = ats_strdup(m_master.connect_ports_string);\n  params->connect_ports        = parse_ports_list(params->connect_ports_string);\n\n  params->oride.request_hdr_max_size  = m_master.oride.request_hdr_max_size;\n  params->oride.response_hdr_max_size = m_master.oride.response_hdr_max_size;\n\n  params->push_method_enabled = INT_TO_BOOL(m_master.push_method_enabled);\n\n  params->reverse_proxy_enabled            = INT_TO_BOOL(m_master.reverse_proxy_enabled);\n  params->url_remap_required               = INT_TO_BOOL(m_master.url_remap_required);\n  params->errors_log_error_pages           = INT_TO_BOOL(m_master.errors_log_error_pages);\n  params->oride.slow_log_threshold         = m_master.oride.slow_log_threshold;\n  params->record_cop_page                  = INT_TO_BOOL(m_master.record_cop_page);\n  params->oride.ssl_client_verify_server   = m_master.oride.ssl_client_verify_server;\n  params->oride.send_http11_requests       = m_master.oride.send_http11_requests;\n  params->oride.doc_in_cache_skip_dns      = INT_TO_BOOL(m_master.oride.doc_in_cache_skip_dns);\n  params->oride.default_buffer_size_index  = m_master.oride.default_buffer_size_index;\n  params->oride.default_buffer_water_mark  = m_master.oride.default_buffer_water_mark;\n  params->enable_http_info                 = INT_TO_BOOL(m_master.enable_http_info);\n  params->oride.body_factory_template_base = ats_strdup(m_master.oride.body_factory_template_base);\n  params->oride.body_factory_template_base_len =\n    params->oride.body_factory_template_base ? strlen(params->oride.body_factory_template_base) : 0;\n  params->body_factory_response_max_size = m_master.body_factory_response_max_size;\n  params->reverse_proxy_no_host_redirect = ats_strdup(m_master.reverse_proxy_no_host_redirect);\n  params->reverse_proxy_no_host_redirect_len =\n    params->reverse_proxy_no_host_redirect ? strlen(params->reverse_proxy_no_host_redirect) : 0;\n\n  params->referer_filter_enabled  = INT_TO_BOOL(m_master.referer_filter_enabled);\n  params->referer_format_redirect = INT_TO_BOOL(m_master.referer_format_redirect);\n\n  params->strict_uri_parsing = INT_TO_BOOL(m_master.strict_uri_parsing);\n\n  params->oride.down_server_timeout    = m_master.oride.down_server_timeout;\n  params->oride.client_abort_threshold = m_master.oride.client_abort_threshold;\n\n  params->oride.negative_caching_enabled       = INT_TO_BOOL(m_master.oride.negative_caching_enabled);\n  params->oride.negative_caching_lifetime      = m_master.oride.negative_caching_lifetime;\n  params->oride.negative_revalidating_enabled  = INT_TO_BOOL(m_master.oride.negative_revalidating_enabled);\n  params->oride.negative_revalidating_lifetime = m_master.oride.negative_revalidating_lifetime;\n\n  params->oride.redirect_use_orig_cache_key = INT_TO_BOOL(m_master.oride.redirect_use_orig_cache_key);\n  params->redirection_host_no_port          = INT_TO_BOOL(m_master.redirection_host_no_port);\n  params->oride.number_of_redirections      = m_master.oride.number_of_redirections;\n  params->post_copy_size                    = m_master.post_copy_size;\n  params->oride.client_cert_filename        = ats_strdup(m_master.oride.client_cert_filename);\n  params->oride.client_cert_filepath        = ats_strdup(m_master.oride.client_cert_filepath);\n\n  // Local Manager\n  params->synthetic_port = m_master.synthetic_port;\n\n  m_id = configProcessor.set(m_id, params);\n\n#undef INT_TO_BOOL\n}",
        "func": "void\nHttpConfig::reconfigure()\n{\n#define INT_TO_BOOL(i) ((i) ? 1 : 0);\n\n  HttpConfigParams *params;\n\n  params = new HttpConfigParams;\n\n  params->inbound_ip4 = m_master.inbound_ip4;\n  params->inbound_ip6 = m_master.inbound_ip6;\n\n  params->outbound_ip4 = m_master.outbound_ip4;\n  params->outbound_ip6 = m_master.outbound_ip6;\n\n  params->proxy_hostname                           = ats_strdup(m_master.proxy_hostname);\n  params->proxy_hostname_len                       = (params->proxy_hostname) ? strlen(params->proxy_hostname) : 0;\n  params->no_dns_forward_to_parent                 = INT_TO_BOOL(m_master.no_dns_forward_to_parent);\n  params->oride.uncacheable_requests_bypass_parent = INT_TO_BOOL(m_master.oride.uncacheable_requests_bypass_parent);\n  params->no_origin_server_dns                     = INT_TO_BOOL(m_master.no_origin_server_dns);\n  params->use_client_target_addr                   = m_master.use_client_target_addr;\n  params->use_client_source_port                   = INT_TO_BOOL(m_master.use_client_source_port);\n  params->oride.maintain_pristine_host_hdr         = INT_TO_BOOL(m_master.oride.maintain_pristine_host_hdr);\n\n  params->disable_ssl_parenting        = INT_TO_BOOL(m_master.disable_ssl_parenting);\n  params->oride.forward_connect_method = INT_TO_BOOL(m_master.oride.forward_connect_method);\n\n  params->server_max_connections             = m_master.server_max_connections;\n  params->max_websocket_connections          = m_master.max_websocket_connections;\n  params->oride.server_tcp_init_cwnd         = m_master.oride.server_tcp_init_cwnd;\n  params->oride.origin_max_connections       = m_master.oride.origin_max_connections;\n  params->oride.origin_max_connections_queue = m_master.oride.origin_max_connections_queue;\n  // if origin_max_connections_queue is set without max_connections, it is meaningless, so we'll warn\n  if (params->oride.origin_max_connections_queue > 0 &&\n      !(params->oride.origin_max_connections || params->origin_min_keep_alive_connections)) {\n    Warning(\"origin_max_connections_queue is set, but neither origin_max_connections nor origin_min_keep_alive_connections are \"\n            \"set, please correct your records.config\");\n  }\n  params->origin_min_keep_alive_connections     = m_master.origin_min_keep_alive_connections;\n  params->oride.attach_server_session_to_client = m_master.oride.attach_server_session_to_client;\n\n  if (params->oride.origin_max_connections && params->oride.origin_max_connections < params->origin_min_keep_alive_connections) {\n    Warning(\"origin_max_connections < origin_min_keep_alive_connections, setting min=max , please correct your records.config\");\n    params->origin_min_keep_alive_connections = params->oride.origin_max_connections;\n  }\n\n  params->oride.insert_request_via_string   = m_master.oride.insert_request_via_string;\n  params->oride.insert_response_via_string  = m_master.oride.insert_response_via_string;\n  params->proxy_request_via_string          = ats_strdup(m_master.proxy_request_via_string);\n  params->proxy_request_via_string_len      = (params->proxy_request_via_string) ? strlen(params->proxy_request_via_string) : 0;\n  params->proxy_response_via_string         = ats_strdup(m_master.proxy_response_via_string);\n  params->proxy_response_via_string_len     = (params->proxy_response_via_string) ? strlen(params->proxy_response_via_string) : 0;\n  params->oride.proxy_response_hsts_max_age = m_master.oride.proxy_response_hsts_max_age;\n  params->oride.proxy_response_hsts_include_subdomains = m_master.oride.proxy_response_hsts_include_subdomains;\n\n  params->oride.keep_alive_enabled_in       = INT_TO_BOOL(m_master.oride.keep_alive_enabled_in);\n  params->oride.keep_alive_enabled_out      = INT_TO_BOOL(m_master.oride.keep_alive_enabled_out);\n  params->oride.chunking_enabled            = INT_TO_BOOL(m_master.oride.chunking_enabled);\n  params->oride.auth_server_session_private = INT_TO_BOOL(m_master.oride.auth_server_session_private);\n\n  params->oride.http_chunking_size = m_master.oride.http_chunking_size;\n\n  params->oride.post_check_content_length_enabled = INT_TO_BOOL(m_master.oride.post_check_content_length_enabled);\n\n  params->oride.flow_control_enabled = INT_TO_BOOL(m_master.oride.flow_control_enabled);\n  params->oride.flow_high_water_mark = m_master.oride.flow_high_water_mark;\n  params->oride.flow_low_water_mark  = m_master.oride.flow_low_water_mark;\n  // If not set (zero) then make values the same.\n  if (params->oride.flow_low_water_mark <= 0) {\n    params->oride.flow_low_water_mark = params->oride.flow_high_water_mark;\n  }\n  if (params->oride.flow_high_water_mark <= 0) {\n    params->oride.flow_high_water_mark = params->oride.flow_low_water_mark;\n  }\n  if (params->oride.flow_high_water_mark < params->oride.flow_low_water_mark) {\n    Warning(\"Flow control low water mark is greater than high water mark, flow control disabled\");\n    params->oride.flow_control_enabled = 0;\n    // zero means \"hardwired default\" when actually used.\n    params->oride.flow_high_water_mark = params->oride.flow_low_water_mark = 0;\n  }\n\n  params->oride.server_session_sharing_match = m_master.oride.server_session_sharing_match;\n  params->server_session_sharing_pool        = m_master.server_session_sharing_pool;\n  params->oride.keep_alive_post_out          = m_master.oride.keep_alive_post_out;\n\n  params->oride.keep_alive_no_activity_timeout_in   = m_master.oride.keep_alive_no_activity_timeout_in;\n  params->oride.keep_alive_no_activity_timeout_out  = m_master.oride.keep_alive_no_activity_timeout_out;\n  params->oride.transaction_no_activity_timeout_in  = m_master.oride.transaction_no_activity_timeout_in;\n  params->oride.transaction_no_activity_timeout_out = m_master.oride.transaction_no_activity_timeout_out;\n  params->oride.transaction_active_timeout_in       = m_master.oride.transaction_active_timeout_in;\n  params->oride.transaction_active_timeout_out      = m_master.oride.transaction_active_timeout_out;\n  params->oride.websocket_active_timeout            = m_master.oride.websocket_active_timeout;\n  params->oride.websocket_inactive_timeout          = m_master.oride.websocket_inactive_timeout;\n  params->accept_no_activity_timeout                = m_master.accept_no_activity_timeout;\n  params->oride.background_fill_active_timeout      = m_master.oride.background_fill_active_timeout;\n  params->oride.background_fill_threshold           = m_master.oride.background_fill_threshold;\n\n  params->oride.connect_attempts_max_retries             = m_master.oride.connect_attempts_max_retries;\n  params->oride.connect_attempts_max_retries_dead_server = m_master.oride.connect_attempts_max_retries_dead_server;\n  if (m_master.oride.connect_attempts_rr_retries > params->oride.connect_attempts_max_retries) {\n    Warning(\"connect_attempts_rr_retries (%\" PRIu64 \") is greater than \"\n            \"connect_attempts_max_retries (%\" PRIu64 \"), this means requests \"\n            \"will never redispatch to another server\",\n            m_master.oride.connect_attempts_rr_retries, params->oride.connect_attempts_max_retries);\n  }\n  params->oride.connect_attempts_rr_retries   = m_master.oride.connect_attempts_rr_retries;\n  params->oride.connect_attempts_timeout      = m_master.oride.connect_attempts_timeout;\n  params->oride.post_connect_attempts_timeout = m_master.oride.post_connect_attempts_timeout;\n  params->oride.parent_connect_attempts       = m_master.oride.parent_connect_attempts;\n  params->oride.parent_retry_time             = m_master.oride.parent_retry_time;\n  params->oride.parent_fail_threshold         = m_master.oride.parent_fail_threshold;\n  params->oride.per_parent_connect_attempts   = m_master.oride.per_parent_connect_attempts;\n  params->oride.parent_connect_timeout        = m_master.oride.parent_connect_timeout;\n  params->oride.parent_failures_update_hostdb = m_master.oride.parent_failures_update_hostdb;\n\n  params->oride.sock_recv_buffer_size_out = m_master.oride.sock_recv_buffer_size_out;\n  params->oride.sock_send_buffer_size_out = m_master.oride.sock_send_buffer_size_out;\n  params->oride.sock_packet_mark_out      = m_master.oride.sock_packet_mark_out;\n  params->oride.sock_packet_tos_out       = m_master.oride.sock_packet_tos_out;\n  params->oride.sock_option_flag_out      = m_master.oride.sock_option_flag_out;\n\n  // Clear the TCP Fast Open option if it is not supported on this host.\n  if ((params->oride.sock_option_flag_out & NetVCOptions::SOCK_OPT_TCP_FAST_OPEN) && !SocketManager::fastopen_supported()) {\n    Status(\"disabling unsupported TCP Fast Open flag on proxy.config.net.sock_option_flag_out\");\n    params->oride.sock_option_flag_out &= ~NetVCOptions::SOCK_OPT_TCP_FAST_OPEN;\n  }\n\n  params->oride.fwd_proxy_auth_to_parent = INT_TO_BOOL(m_master.oride.fwd_proxy_auth_to_parent);\n\n  params->oride.anonymize_remove_from       = INT_TO_BOOL(m_master.oride.anonymize_remove_from);\n  params->oride.anonymize_remove_referer    = INT_TO_BOOL(m_master.oride.anonymize_remove_referer);\n  params->oride.anonymize_remove_user_agent = INT_TO_BOOL(m_master.oride.anonymize_remove_user_agent);\n  params->oride.anonymize_remove_cookie     = INT_TO_BOOL(m_master.oride.anonymize_remove_cookie);\n  params->oride.anonymize_remove_client_ip  = INT_TO_BOOL(m_master.oride.anonymize_remove_client_ip);\n  params->oride.anonymize_insert_client_ip  = INT_TO_BOOL(m_master.oride.anonymize_insert_client_ip);\n  params->anonymize_other_header_list       = ats_strdup(m_master.anonymize_other_header_list);\n\n  params->oride.global_user_agent_header = ats_strdup(m_master.oride.global_user_agent_header);\n  params->oride.global_user_agent_header_size =\n    params->oride.global_user_agent_header ? strlen(params->oride.global_user_agent_header) : 0;\n\n  params->oride.proxy_response_server_string = ats_strdup(m_master.oride.proxy_response_server_string);\n  params->oride.proxy_response_server_string_len =\n    params->oride.proxy_response_server_string ? strlen(params->oride.proxy_response_server_string) : 0;\n  params->oride.proxy_response_server_enabled = m_master.oride.proxy_response_server_enabled;\n\n  params->oride.insert_squid_x_forwarded_for = INT_TO_BOOL(m_master.oride.insert_squid_x_forwarded_for);\n  params->oride.insert_forwarded             = m_master.oride.insert_forwarded;\n  params->oride.insert_age_in_response       = INT_TO_BOOL(m_master.oride.insert_age_in_response);\n  params->enable_http_stats                  = INT_TO_BOOL(m_master.enable_http_stats);\n  params->oride.normalize_ae                 = m_master.oride.normalize_ae;\n\n  params->oride.cache_heuristic_min_lifetime = m_master.oride.cache_heuristic_min_lifetime;\n  params->oride.cache_heuristic_max_lifetime = m_master.oride.cache_heuristic_max_lifetime;\n  params->oride.cache_heuristic_lm_factor    = std::min(std::max(m_master.oride.cache_heuristic_lm_factor, 0.0f), 1.0f);\n\n  params->oride.cache_guaranteed_min_lifetime = m_master.oride.cache_guaranteed_min_lifetime;\n  params->oride.cache_guaranteed_max_lifetime = m_master.oride.cache_guaranteed_max_lifetime;\n\n  params->oride.cache_max_stale_age = m_master.oride.cache_max_stale_age;\n\n  params->oride.cache_vary_default_text   = ats_strdup(m_master.oride.cache_vary_default_text);\n  params->oride.cache_vary_default_images = ats_strdup(m_master.oride.cache_vary_default_images);\n  params->oride.cache_vary_default_other  = ats_strdup(m_master.oride.cache_vary_default_other);\n\n  params->oride.srv_enabled = m_master.oride.srv_enabled;\n\n  // open read failure retries\n  params->oride.max_cache_open_read_retries = m_master.oride.max_cache_open_read_retries;\n  params->oride.cache_open_read_retry_time  = m_master.oride.cache_open_read_retry_time;\n  params->oride.cache_generation_number     = m_master.oride.cache_generation_number;\n\n  // open write failure retries\n  params->oride.max_cache_open_write_retries = m_master.oride.max_cache_open_write_retries;\n\n  params->oride.cache_http                        = INT_TO_BOOL(m_master.oride.cache_http);\n  params->oride.cache_ignore_client_no_cache      = INT_TO_BOOL(m_master.oride.cache_ignore_client_no_cache);\n  params->oride.cache_ignore_client_cc_max_age    = INT_TO_BOOL(m_master.oride.cache_ignore_client_cc_max_age);\n  params->oride.cache_ims_on_client_no_cache      = INT_TO_BOOL(m_master.oride.cache_ims_on_client_no_cache);\n  params->oride.cache_ignore_server_no_cache      = INT_TO_BOOL(m_master.oride.cache_ignore_server_no_cache);\n  params->oride.cache_responses_to_cookies        = m_master.oride.cache_responses_to_cookies;\n  params->oride.cache_ignore_auth                 = INT_TO_BOOL(m_master.oride.cache_ignore_auth);\n  params->oride.cache_urls_that_look_dynamic      = INT_TO_BOOL(m_master.oride.cache_urls_that_look_dynamic);\n  params->oride.cache_enable_default_vary_headers = INT_TO_BOOL(m_master.oride.cache_enable_default_vary_headers);\n  params->cache_post_method                       = INT_TO_BOOL(m_master.cache_post_method);\n\n  params->oride.ignore_accept_mismatch          = m_master.oride.ignore_accept_mismatch;\n  params->oride.ignore_accept_language_mismatch = m_master.oride.ignore_accept_language_mismatch;\n  params->oride.ignore_accept_encoding_mismatch = m_master.oride.ignore_accept_encoding_mismatch;\n  params->oride.ignore_accept_charset_mismatch  = m_master.oride.ignore_accept_charset_mismatch;\n\n  params->send_100_continue_response = INT_TO_BOOL(m_master.send_100_continue_response);\n  params->disallow_post_100_continue = INT_TO_BOOL(m_master.disallow_post_100_continue);\n  params->parser_allow_non_http      = INT_TO_BOOL(m_master.parser_allow_non_http);\n  params->keepalive_internal_vc      = INT_TO_BOOL(m_master.keepalive_internal_vc);\n\n  params->oride.cache_open_write_fail_action = m_master.oride.cache_open_write_fail_action;\n\n  params->oride.cache_when_to_revalidate = m_master.oride.cache_when_to_revalidate;\n  params->max_post_size                  = m_master.max_post_size;\n\n  params->oride.cache_required_headers = m_master.oride.cache_required_headers;\n  params->oride.cache_range_lookup     = INT_TO_BOOL(m_master.oride.cache_range_lookup);\n  params->oride.cache_range_write      = INT_TO_BOOL(m_master.oride.cache_range_write);\n  params->oride.allow_multi_range      = m_master.oride.allow_multi_range;\n\n  params->connect_ports_string = ats_strdup(m_master.connect_ports_string);\n  params->connect_ports        = parse_ports_list(params->connect_ports_string);\n\n  params->oride.request_hdr_max_size  = m_master.oride.request_hdr_max_size;\n  params->oride.response_hdr_max_size = m_master.oride.response_hdr_max_size;\n\n  params->push_method_enabled = INT_TO_BOOL(m_master.push_method_enabled);\n\n  params->reverse_proxy_enabled            = INT_TO_BOOL(m_master.reverse_proxy_enabled);\n  params->url_remap_required               = INT_TO_BOOL(m_master.url_remap_required);\n  params->errors_log_error_pages           = INT_TO_BOOL(m_master.errors_log_error_pages);\n  params->oride.slow_log_threshold         = m_master.oride.slow_log_threshold;\n  params->record_cop_page                  = INT_TO_BOOL(m_master.record_cop_page);\n  params->oride.ssl_client_verify_server   = m_master.oride.ssl_client_verify_server;\n  params->oride.send_http11_requests       = m_master.oride.send_http11_requests;\n  params->oride.doc_in_cache_skip_dns      = INT_TO_BOOL(m_master.oride.doc_in_cache_skip_dns);\n  params->oride.default_buffer_size_index  = m_master.oride.default_buffer_size_index;\n  params->oride.default_buffer_water_mark  = m_master.oride.default_buffer_water_mark;\n  params->enable_http_info                 = INT_TO_BOOL(m_master.enable_http_info);\n  params->oride.body_factory_template_base = ats_strdup(m_master.oride.body_factory_template_base);\n  params->oride.body_factory_template_base_len =\n    params->oride.body_factory_template_base ? strlen(params->oride.body_factory_template_base) : 0;\n  params->body_factory_response_max_size = m_master.body_factory_response_max_size;\n  params->reverse_proxy_no_host_redirect = ats_strdup(m_master.reverse_proxy_no_host_redirect);\n  params->reverse_proxy_no_host_redirect_len =\n    params->reverse_proxy_no_host_redirect ? strlen(params->reverse_proxy_no_host_redirect) : 0;\n\n  params->referer_filter_enabled  = INT_TO_BOOL(m_master.referer_filter_enabled);\n  params->referer_format_redirect = INT_TO_BOOL(m_master.referer_format_redirect);\n\n  params->strict_uri_parsing = INT_TO_BOOL(m_master.strict_uri_parsing);\n\n  params->oride.down_server_timeout    = m_master.oride.down_server_timeout;\n  params->oride.client_abort_threshold = m_master.oride.client_abort_threshold;\n\n  params->oride.negative_caching_enabled       = INT_TO_BOOL(m_master.oride.negative_caching_enabled);\n  params->oride.negative_caching_lifetime      = m_master.oride.negative_caching_lifetime;\n  params->oride.negative_revalidating_enabled  = INT_TO_BOOL(m_master.oride.negative_revalidating_enabled);\n  params->oride.negative_revalidating_lifetime = m_master.oride.negative_revalidating_lifetime;\n\n  params->oride.redirect_use_orig_cache_key = INT_TO_BOOL(m_master.oride.redirect_use_orig_cache_key);\n  params->redirection_host_no_port          = INT_TO_BOOL(m_master.redirection_host_no_port);\n  params->oride.number_of_redirections      = m_master.oride.number_of_redirections;\n  params->post_copy_size                    = m_master.post_copy_size;\n  params->oride.client_cert_filename        = ats_strdup(m_master.oride.client_cert_filename);\n  params->oride.client_cert_filepath        = ats_strdup(m_master.oride.client_cert_filepath);\n\n  // Local Manager\n  params->synthetic_port = m_master.synthetic_port;\n\n  m_id = configProcessor.set(m_id, params);\n\n#undef INT_TO_BOOL\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -202,6 +202,7 @@\n   params->oride.cache_required_headers = m_master.oride.cache_required_headers;\n   params->oride.cache_range_lookup     = INT_TO_BOOL(m_master.oride.cache_range_lookup);\n   params->oride.cache_range_write      = INT_TO_BOOL(m_master.oride.cache_range_write);\n+  params->oride.allow_multi_range      = m_master.oride.allow_multi_range;\n \n   params->connect_ports_string = ats_strdup(m_master.connect_ports_string);\n   params->connect_ports        = parse_ports_list(params->connect_ports_string);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  params->oride.allow_multi_range      = m_master.oride.allow_multi_range;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/HttpConfig::startup",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "void\nHttpConfig::startup()\n{\n  http_rsb = RecAllocateRawStatBlock((int)http_stat_count);\n  register_stat_callbacks();\n\n  HttpConfigParams &c = m_master;\n\n  http_config_cont = new HttpConfigCont;\n\n  HttpEstablishStaticConfigStringAlloc(c.proxy_hostname, \"proxy.config.proxy_name\");\n  c.proxy_hostname_len = -1;\n\n  if (c.proxy_hostname == nullptr) {\n    c.proxy_hostname    = (char *)ats_malloc(sizeof(char));\n    c.proxy_hostname[0] = '\\0';\n  }\n\n  RecHttpLoadIp(\"proxy.local.incoming_ip_to_bind\", c.inbound_ip4, c.inbound_ip6);\n  RecHttpLoadIp(\"proxy.local.outgoing_ip_to_bind\", c.outbound_ip4, c.outbound_ip6);\n\n  HttpEstablishStaticConfigLongLong(c.server_max_connections, \"proxy.config.http.server_max_connections\");\n  HttpEstablishStaticConfigLongLong(c.max_websocket_connections, \"proxy.config.http.websocket.max_number_of_connections\");\n  HttpEstablishStaticConfigLongLong(c.oride.server_tcp_init_cwnd, \"proxy.config.http.server_tcp_init_cwnd\");\n  HttpEstablishStaticConfigLongLong(c.oride.origin_max_connections, \"proxy.config.http.origin_max_connections\");\n  HttpEstablishStaticConfigLongLong(c.oride.origin_max_connections_queue, \"proxy.config.http.origin_max_connections_queue\");\n  HttpEstablishStaticConfigLongLong(c.origin_min_keep_alive_connections, \"proxy.config.http.origin_min_keep_alive_connections\");\n  HttpEstablishStaticConfigByte(c.oride.attach_server_session_to_client, \"proxy.config.http.attach_server_session_to_client\");\n\n  HttpEstablishStaticConfigByte(c.disable_ssl_parenting, \"proxy.local.http.parent_proxy.disable_connect_tunneling\");\n  HttpEstablishStaticConfigByte(c.oride.forward_connect_method, \"proxy.config.http.forward_connect_method\");\n\n  HttpEstablishStaticConfigByte(c.no_dns_forward_to_parent, \"proxy.config.http.no_dns_just_forward_to_parent\");\n  HttpEstablishStaticConfigByte(c.oride.uncacheable_requests_bypass_parent, \"proxy.config.http.uncacheable_requests_bypass_parent\");\n  HttpEstablishStaticConfigByte(c.oride.doc_in_cache_skip_dns, \"proxy.config.http.doc_in_cache_skip_dns\");\n\n  HttpEstablishStaticConfigByte(c.no_origin_server_dns, \"proxy.config.http.no_origin_server_dns\");\n  HttpEstablishStaticConfigByte(c.use_client_target_addr, \"proxy.config.http.use_client_target_addr\");\n  HttpEstablishStaticConfigByte(c.use_client_source_port, \"proxy.config.http.use_client_source_port\");\n  HttpEstablishStaticConfigByte(c.oride.maintain_pristine_host_hdr, \"proxy.config.url_remap.pristine_host_hdr\");\n\n  HttpEstablishStaticConfigByte(c.oride.insert_request_via_string, \"proxy.config.http.insert_request_via_str\");\n  HttpEstablishStaticConfigByte(c.oride.insert_response_via_string, \"proxy.config.http.insert_response_via_str\");\n  HttpEstablishStaticConfigLongLong(c.oride.proxy_response_hsts_max_age, \"proxy.config.ssl.hsts_max_age\");\n  HttpEstablishStaticConfigByte(c.oride.proxy_response_hsts_include_subdomains, \"proxy.config.ssl.hsts_include_subdomains\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.client_cert_filename, \"proxy.config.ssl.client.cert.filename\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.client_cert_filepath, \"proxy.config.ssl.client.cert.path\");\n\n  HttpEstablishStaticConfigStringAlloc(c.proxy_request_via_string, \"proxy.config.http.request_via_str\");\n  c.proxy_request_via_string_len = -1;\n  HttpEstablishStaticConfigStringAlloc(c.proxy_response_via_string, \"proxy.config.http.response_via_str\");\n  c.proxy_response_via_string_len = -1;\n\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_enabled_in, \"proxy.config.http.keep_alive_enabled_in\");\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_enabled_out, \"proxy.config.http.keep_alive_enabled_out\");\n  HttpEstablishStaticConfigByte(c.oride.chunking_enabled, \"proxy.config.http.chunking_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.http_chunking_size, \"proxy.config.http.chunking.size\");\n  HttpEstablishStaticConfigByte(c.oride.flow_control_enabled, \"proxy.config.http.flow_control.enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.flow_high_water_mark, \"proxy.config.http.flow_control.high_water\");\n  HttpEstablishStaticConfigLongLong(c.oride.flow_low_water_mark, \"proxy.config.http.flow_control.low_water\");\n  HttpEstablishStaticConfigByte(c.oride.post_check_content_length_enabled, \"proxy.config.http.post.check.content_length.enabled\");\n  HttpEstablishStaticConfigByte(c.strict_uri_parsing, \"proxy.config.http.strict_uri_parsing\");\n\n  // [amc] This is a bit of a mess, need to figure out to make this cleaner.\n  RecRegisterConfigUpdateCb(\"proxy.config.http.server_session_sharing.match\", &http_server_session_sharing_cb, &c);\n  http_config_enum_read(\"proxy.config.http.server_session_sharing.match\", SessionSharingMatchStrings,\n                        c.oride.server_session_sharing_match);\n  http_config_enum_read(\"proxy.config.http.server_session_sharing.pool\", SessionSharingPoolStrings, c.server_session_sharing_pool);\n\n  RecRegisterConfigUpdateCb(\"proxy.config.http.insert_forwarded\", &http_insert_forwarded_cb, &c);\n  {\n    char str[512];\n\n    if (REC_ERR_OKAY == RecGetRecordString(\"proxy.config.http.insert_forwarded\", str, sizeof(str))) {\n      ts::LocalBufferWriter<1024> error;\n      HttpForwarded::OptionBitSet bs = HttpForwarded::optStrToBitset(ts::string_view(str), error);\n      if (!error.size()) {\n        c.oride.insert_forwarded = bs;\n      } else {\n        Error(\"HTTP %.*s\", static_cast<int>(error.size()), error.data());\n      }\n    }\n  }\n\n  HttpEstablishStaticConfigByte(c.oride.auth_server_session_private, \"proxy.config.http.auth_server_session_private\");\n\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_post_out, \"proxy.config.http.keep_alive_post_out\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.keep_alive_no_activity_timeout_in,\n                                    \"proxy.config.http.keep_alive_no_activity_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.keep_alive_no_activity_timeout_out,\n                                    \"proxy.config.http.keep_alive_no_activity_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_no_activity_timeout_in,\n                                    \"proxy.config.http.transaction_no_activity_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_no_activity_timeout_out,\n                                    \"proxy.config.http.transaction_no_activity_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.websocket_active_timeout, \"proxy.config.websocket.active_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.websocket_inactive_timeout, \"proxy.config.websocket.no_activity_timeout\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_active_timeout_in, \"proxy.config.http.transaction_active_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_active_timeout_out, \"proxy.config.http.transaction_active_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.accept_no_activity_timeout, \"proxy.config.http.accept_no_activity_timeout\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.background_fill_active_timeout, \"proxy.config.http.background_fill_active_timeout\");\n  HttpEstablishStaticConfigFloat(c.oride.background_fill_threshold, \"proxy.config.http.background_fill_completed_threshold\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_max_retries, \"proxy.config.http.connect_attempts_max_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_max_retries_dead_server,\n                                    \"proxy.config.http.connect_attempts_max_retries_dead_server\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_rr_retries, \"proxy.config.http.connect_attempts_rr_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_timeout, \"proxy.config.http.connect_attempts_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.post_connect_attempts_timeout, \"proxy.config.http.post_connect_attempts_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_connect_attempts, \"proxy.config.http.parent_proxy.total_connect_attempts\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_retry_time, \"proxy.config.http.parent_proxy.retry_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_fail_threshold, \"proxy.config.http.parent_proxy.fail_threshold\");\n  HttpEstablishStaticConfigLongLong(c.oride.per_parent_connect_attempts,\n                                    \"proxy.config.http.parent_proxy.per_parent_connect_attempts\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_connect_timeout, \"proxy.config.http.parent_proxy.connect_attempts_timeout\");\n  HttpEstablishStaticConfigByte(c.oride.parent_failures_update_hostdb, \"proxy.config.http.parent_proxy.mark_down_hostdb\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.sock_recv_buffer_size_out, \"proxy.config.net.sock_recv_buffer_size_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_send_buffer_size_out, \"proxy.config.net.sock_send_buffer_size_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_option_flag_out, \"proxy.config.net.sock_option_flag_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_packet_mark_out, \"proxy.config.net.sock_packet_mark_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_packet_tos_out, \"proxy.config.net.sock_packet_tos_out\");\n\n  HttpEstablishStaticConfigByte(c.oride.fwd_proxy_auth_to_parent, \"proxy.config.http.forward.proxy_auth_to_parent\");\n\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_from, \"proxy.config.http.anonymize_remove_from\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_referer, \"proxy.config.http.anonymize_remove_referer\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_user_agent, \"proxy.config.http.anonymize_remove_user_agent\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_cookie, \"proxy.config.http.anonymize_remove_cookie\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_client_ip, \"proxy.config.http.anonymize_remove_client_ip\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_insert_client_ip, \"proxy.config.http.insert_client_ip\");\n  HttpEstablishStaticConfigStringAlloc(c.anonymize_other_header_list, \"proxy.config.http.anonymize_other_header_list\");\n\n  HttpEstablishStaticConfigStringAlloc(c.oride.global_user_agent_header, \"proxy.config.http.global_user_agent_header\");\n  c.oride.global_user_agent_header_size = c.oride.global_user_agent_header ? strlen(c.oride.global_user_agent_header) : 0;\n\n  HttpEstablishStaticConfigByte(c.oride.proxy_response_server_enabled, \"proxy.config.http.response_server_enabled\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.proxy_response_server_string, \"proxy.config.http.response_server_str\");\n  c.oride.proxy_response_server_string_len =\n    c.oride.proxy_response_server_string ? strlen(c.oride.proxy_response_server_string) : 0;\n\n  HttpEstablishStaticConfigByte(c.oride.insert_squid_x_forwarded_for, \"proxy.config.http.insert_squid_x_forwarded_for\");\n\n  HttpEstablishStaticConfigByte(c.oride.insert_age_in_response, \"proxy.config.http.insert_age_in_response\");\n  HttpEstablishStaticConfigByte(c.enable_http_stats, \"proxy.config.http.enable_http_stats\");\n  HttpEstablishStaticConfigByte(c.oride.normalize_ae, \"proxy.config.http.normalize_ae\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_heuristic_min_lifetime, \"proxy.config.http.cache.heuristic_min_lifetime\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_heuristic_max_lifetime, \"proxy.config.http.cache.heuristic_max_lifetime\");\n  HttpEstablishStaticConfigFloat(c.oride.cache_heuristic_lm_factor, \"proxy.config.http.cache.heuristic_lm_factor\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_guaranteed_min_lifetime, \"proxy.config.http.cache.guaranteed_min_lifetime\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_guaranteed_max_lifetime, \"proxy.config.http.cache.guaranteed_max_lifetime\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_max_stale_age, \"proxy.config.http.cache.max_stale_age\");\n  HttpEstablishStaticConfigByte(c.oride.srv_enabled, \"proxy.config.srv_enabled\");\n\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_text, \"proxy.config.http.cache.vary_default_text\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_images, \"proxy.config.http.cache.vary_default_images\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_other, \"proxy.config.http.cache.vary_default_other\");\n\n  // open read failure retries\n  HttpEstablishStaticConfigLongLong(c.oride.max_cache_open_read_retries, \"proxy.config.http.cache.max_open_read_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_open_read_retry_time, \"proxy.config.http.cache.open_read_retry_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_generation_number, \"proxy.config.http.cache.generation\");\n\n  // open write failure retries\n  HttpEstablishStaticConfigLongLong(c.oride.max_cache_open_write_retries, \"proxy.config.http.cache.max_open_write_retries\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_http, \"proxy.config.http.cache.http\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_client_no_cache, \"proxy.config.http.cache.ignore_client_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_client_cc_max_age, \"proxy.config.http.cache.ignore_client_cc_max_age\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ims_on_client_no_cache, \"proxy.config.http.cache.ims_on_client_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_server_no_cache, \"proxy.config.http.cache.ignore_server_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_responses_to_cookies, \"proxy.config.http.cache.cache_responses_to_cookies\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_auth, \"proxy.config.http.cache.ignore_authentication\");\n  HttpEstablishStaticConfigByte(c.oride.cache_urls_that_look_dynamic, \"proxy.config.http.cache.cache_urls_that_look_dynamic\");\n  HttpEstablishStaticConfigByte(c.oride.cache_enable_default_vary_headers, \"proxy.config.http.cache.enable_default_vary_headers\");\n  HttpEstablishStaticConfigByte(c.cache_post_method, \"proxy.config.http.cache.post_method\");\n\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_mismatch, \"proxy.config.http.cache.ignore_accept_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_language_mismatch, \"proxy.config.http.cache.ignore_accept_language_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_encoding_mismatch, \"proxy.config.http.cache.ignore_accept_encoding_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_charset_mismatch, \"proxy.config.http.cache.ignore_accept_charset_mismatch\");\n\n  HttpEstablishStaticConfigByte(c.send_100_continue_response, \"proxy.config.http.send_100_continue_response\");\n  HttpEstablishStaticConfigByte(c.disallow_post_100_continue, \"proxy.config.http.disallow_post_100_continue\");\n  HttpEstablishStaticConfigByte(c.parser_allow_non_http, \"proxy.config.http.parse.allow_non_http\");\n\n  HttpEstablishStaticConfigByte(c.keepalive_internal_vc, \"proxy.config.http.keepalive_internal_vc\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_open_write_fail_action, \"proxy.config.http.cache.open_write_fail_action\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_when_to_revalidate, \"proxy.config.http.cache.when_to_revalidate\");\n  HttpEstablishStaticConfigByte(c.oride.cache_required_headers, \"proxy.config.http.cache.required_headers\");\n  HttpEstablishStaticConfigByte(c.oride.cache_range_lookup, \"proxy.config.http.cache.range.lookup\");\n  HttpEstablishStaticConfigByte(c.oride.cache_range_write, \"proxy.config.http.cache.range.write\");\n\n  HttpEstablishStaticConfigStringAlloc(c.connect_ports_string, \"proxy.config.http.connect_ports\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.request_hdr_max_size, \"proxy.config.http.request_header_max_size\");\n  HttpEstablishStaticConfigLongLong(c.oride.response_hdr_max_size, \"proxy.config.http.response_header_max_size\");\n\n  HttpEstablishStaticConfigByte(c.push_method_enabled, \"proxy.config.http.push_method_enabled\");\n\n  HttpEstablishStaticConfigByte(c.reverse_proxy_enabled, \"proxy.config.reverse_proxy.enabled\");\n  HttpEstablishStaticConfigByte(c.url_remap_required, \"proxy.config.url_remap.remap_required\");\n\n  HttpEstablishStaticConfigStringAlloc(c.reverse_proxy_no_host_redirect, \"proxy.config.header.parse.no_host_url_redirect\");\n  c.reverse_proxy_no_host_redirect_len = -1;\n  HttpEstablishStaticConfigStringAlloc(c.oride.body_factory_template_base, \"proxy.config.body_factory.template_base\");\n  c.oride.body_factory_template_base_len = c.oride.body_factory_template_base ? strlen(c.oride.body_factory_template_base) : 0;\n  HttpEstablishStaticConfigLongLong(c.body_factory_response_max_size, \"proxy.config.body_factory.response_max_size\");\n  HttpEstablishStaticConfigByte(c.errors_log_error_pages, \"proxy.config.http.errors.log_error_pages\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.slow_log_threshold, \"proxy.config.http.slow.log.threshold\");\n  HttpEstablishStaticConfigByte(c.oride.ssl_client_verify_server, \"proxy.config.ssl.client.verify.server\");\n\n  HttpEstablishStaticConfigByte(c.record_cop_page, \"proxy.config.http.record_heartbeat\");\n\n  HttpEstablishStaticConfigByte(c.oride.send_http11_requests, \"proxy.config.http.send_http11_requests\");\n\n  // HTTP Referer Filtering\n  HttpEstablishStaticConfigByte(c.referer_filter_enabled, \"proxy.config.http.referer_filter\");\n  HttpEstablishStaticConfigByte(c.referer_format_redirect, \"proxy.config.http.referer_format_redirect\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.down_server_timeout, \"proxy.config.http.down_server.cache_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.client_abort_threshold, \"proxy.config.http.down_server.abort_threshold\");\n\n  // Negative caching and revalidation\n  HttpEstablishStaticConfigByte(c.oride.negative_caching_enabled, \"proxy.config.http.negative_caching_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.negative_caching_lifetime, \"proxy.config.http.negative_caching_lifetime\");\n  HttpEstablishStaticConfigByte(c.oride.negative_revalidating_enabled, \"proxy.config.http.negative_revalidating_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.negative_revalidating_lifetime, \"proxy.config.http.negative_revalidating_lifetime\");\n\n  // Buffer size and watermark\n  HttpEstablishStaticConfigLongLong(c.oride.default_buffer_size_index, \"proxy.config.http.default_buffer_size\");\n  HttpEstablishStaticConfigLongLong(c.oride.default_buffer_water_mark, \"proxy.config.http.default_buffer_water_mark\");\n\n  // Stat Page Info\n  HttpEstablishStaticConfigByte(c.enable_http_info, \"proxy.config.http.enable_http_info\");\n\n  HttpEstablishStaticConfigLongLong(c.max_post_size, \"proxy.config.http.max_post_size\");\n\n  //##############################################################################\n  //#\n  //# Redirection\n  //#\n  //# 1. number_of_redirections: The maximum number of redirections YTS permits. 0 == disabled\n  //# 2. redirect_use_orig_cache_key: if set to 1, use original request cache key.\n  //# 3. post_copy_size: The maximum POST data size YTS permits to copy\n  //# 4. redirection_host_no_port: do not include default port in host header during redirection\n  //#\n  //##############################################################################\n  HttpEstablishStaticConfigByte(c.oride.redirect_use_orig_cache_key, \"proxy.config.http.redirect_use_orig_cache_key\");\n  HttpEstablishStaticConfigByte(c.redirection_host_no_port, \"proxy.config.http.redirect_host_no_port\");\n  HttpEstablishStaticConfigLongLong(c.oride.number_of_redirections, \"proxy.config.http.number_of_redirections\");\n  HttpEstablishStaticConfigLongLong(c.post_copy_size, \"proxy.config.http.post_copy_size\");\n\n  // Local Manager\n  HttpEstablishStaticConfigLongLong(c.synthetic_port, \"proxy.config.admin.synthetic_port\");\n\n  http_config_cont->handleEvent(EVENT_NONE, nullptr);\n\n  return;\n}",
        "func": "void\nHttpConfig::startup()\n{\n  http_rsb = RecAllocateRawStatBlock((int)http_stat_count);\n  register_stat_callbacks();\n\n  HttpConfigParams &c = m_master;\n\n  http_config_cont = new HttpConfigCont;\n\n  HttpEstablishStaticConfigStringAlloc(c.proxy_hostname, \"proxy.config.proxy_name\");\n  c.proxy_hostname_len = -1;\n\n  if (c.proxy_hostname == nullptr) {\n    c.proxy_hostname    = (char *)ats_malloc(sizeof(char));\n    c.proxy_hostname[0] = '\\0';\n  }\n\n  RecHttpLoadIp(\"proxy.local.incoming_ip_to_bind\", c.inbound_ip4, c.inbound_ip6);\n  RecHttpLoadIp(\"proxy.local.outgoing_ip_to_bind\", c.outbound_ip4, c.outbound_ip6);\n\n  HttpEstablishStaticConfigLongLong(c.server_max_connections, \"proxy.config.http.server_max_connections\");\n  HttpEstablishStaticConfigLongLong(c.max_websocket_connections, \"proxy.config.http.websocket.max_number_of_connections\");\n  HttpEstablishStaticConfigLongLong(c.oride.server_tcp_init_cwnd, \"proxy.config.http.server_tcp_init_cwnd\");\n  HttpEstablishStaticConfigLongLong(c.oride.origin_max_connections, \"proxy.config.http.origin_max_connections\");\n  HttpEstablishStaticConfigLongLong(c.oride.origin_max_connections_queue, \"proxy.config.http.origin_max_connections_queue\");\n  HttpEstablishStaticConfigLongLong(c.origin_min_keep_alive_connections, \"proxy.config.http.origin_min_keep_alive_connections\");\n  HttpEstablishStaticConfigByte(c.oride.attach_server_session_to_client, \"proxy.config.http.attach_server_session_to_client\");\n\n  HttpEstablishStaticConfigByte(c.disable_ssl_parenting, \"proxy.local.http.parent_proxy.disable_connect_tunneling\");\n  HttpEstablishStaticConfigByte(c.oride.forward_connect_method, \"proxy.config.http.forward_connect_method\");\n\n  HttpEstablishStaticConfigByte(c.no_dns_forward_to_parent, \"proxy.config.http.no_dns_just_forward_to_parent\");\n  HttpEstablishStaticConfigByte(c.oride.uncacheable_requests_bypass_parent, \"proxy.config.http.uncacheable_requests_bypass_parent\");\n  HttpEstablishStaticConfigByte(c.oride.doc_in_cache_skip_dns, \"proxy.config.http.doc_in_cache_skip_dns\");\n\n  HttpEstablishStaticConfigByte(c.no_origin_server_dns, \"proxy.config.http.no_origin_server_dns\");\n  HttpEstablishStaticConfigByte(c.use_client_target_addr, \"proxy.config.http.use_client_target_addr\");\n  HttpEstablishStaticConfigByte(c.use_client_source_port, \"proxy.config.http.use_client_source_port\");\n  HttpEstablishStaticConfigByte(c.oride.maintain_pristine_host_hdr, \"proxy.config.url_remap.pristine_host_hdr\");\n\n  HttpEstablishStaticConfigByte(c.oride.insert_request_via_string, \"proxy.config.http.insert_request_via_str\");\n  HttpEstablishStaticConfigByte(c.oride.insert_response_via_string, \"proxy.config.http.insert_response_via_str\");\n  HttpEstablishStaticConfigLongLong(c.oride.proxy_response_hsts_max_age, \"proxy.config.ssl.hsts_max_age\");\n  HttpEstablishStaticConfigByte(c.oride.proxy_response_hsts_include_subdomains, \"proxy.config.ssl.hsts_include_subdomains\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.client_cert_filename, \"proxy.config.ssl.client.cert.filename\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.client_cert_filepath, \"proxy.config.ssl.client.cert.path\");\n\n  HttpEstablishStaticConfigStringAlloc(c.proxy_request_via_string, \"proxy.config.http.request_via_str\");\n  c.proxy_request_via_string_len = -1;\n  HttpEstablishStaticConfigStringAlloc(c.proxy_response_via_string, \"proxy.config.http.response_via_str\");\n  c.proxy_response_via_string_len = -1;\n\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_enabled_in, \"proxy.config.http.keep_alive_enabled_in\");\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_enabled_out, \"proxy.config.http.keep_alive_enabled_out\");\n  HttpEstablishStaticConfigByte(c.oride.chunking_enabled, \"proxy.config.http.chunking_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.http_chunking_size, \"proxy.config.http.chunking.size\");\n  HttpEstablishStaticConfigByte(c.oride.flow_control_enabled, \"proxy.config.http.flow_control.enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.flow_high_water_mark, \"proxy.config.http.flow_control.high_water\");\n  HttpEstablishStaticConfigLongLong(c.oride.flow_low_water_mark, \"proxy.config.http.flow_control.low_water\");\n  HttpEstablishStaticConfigByte(c.oride.post_check_content_length_enabled, \"proxy.config.http.post.check.content_length.enabled\");\n  HttpEstablishStaticConfigByte(c.strict_uri_parsing, \"proxy.config.http.strict_uri_parsing\");\n\n  // [amc] This is a bit of a mess, need to figure out to make this cleaner.\n  RecRegisterConfigUpdateCb(\"proxy.config.http.server_session_sharing.match\", &http_server_session_sharing_cb, &c);\n  http_config_enum_read(\"proxy.config.http.server_session_sharing.match\", SessionSharingMatchStrings,\n                        c.oride.server_session_sharing_match);\n  http_config_enum_read(\"proxy.config.http.server_session_sharing.pool\", SessionSharingPoolStrings, c.server_session_sharing_pool);\n\n  RecRegisterConfigUpdateCb(\"proxy.config.http.insert_forwarded\", &http_insert_forwarded_cb, &c);\n  {\n    char str[512];\n\n    if (REC_ERR_OKAY == RecGetRecordString(\"proxy.config.http.insert_forwarded\", str, sizeof(str))) {\n      ts::LocalBufferWriter<1024> error;\n      HttpForwarded::OptionBitSet bs = HttpForwarded::optStrToBitset(ts::string_view(str), error);\n      if (!error.size()) {\n        c.oride.insert_forwarded = bs;\n      } else {\n        Error(\"HTTP %.*s\", static_cast<int>(error.size()), error.data());\n      }\n    }\n  }\n\n  HttpEstablishStaticConfigByte(c.oride.auth_server_session_private, \"proxy.config.http.auth_server_session_private\");\n\n  HttpEstablishStaticConfigByte(c.oride.keep_alive_post_out, \"proxy.config.http.keep_alive_post_out\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.keep_alive_no_activity_timeout_in,\n                                    \"proxy.config.http.keep_alive_no_activity_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.keep_alive_no_activity_timeout_out,\n                                    \"proxy.config.http.keep_alive_no_activity_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_no_activity_timeout_in,\n                                    \"proxy.config.http.transaction_no_activity_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_no_activity_timeout_out,\n                                    \"proxy.config.http.transaction_no_activity_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.websocket_active_timeout, \"proxy.config.websocket.active_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.websocket_inactive_timeout, \"proxy.config.websocket.no_activity_timeout\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_active_timeout_in, \"proxy.config.http.transaction_active_timeout_in\");\n  HttpEstablishStaticConfigLongLong(c.oride.transaction_active_timeout_out, \"proxy.config.http.transaction_active_timeout_out\");\n  HttpEstablishStaticConfigLongLong(c.accept_no_activity_timeout, \"proxy.config.http.accept_no_activity_timeout\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.background_fill_active_timeout, \"proxy.config.http.background_fill_active_timeout\");\n  HttpEstablishStaticConfigFloat(c.oride.background_fill_threshold, \"proxy.config.http.background_fill_completed_threshold\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_max_retries, \"proxy.config.http.connect_attempts_max_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_max_retries_dead_server,\n                                    \"proxy.config.http.connect_attempts_max_retries_dead_server\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_rr_retries, \"proxy.config.http.connect_attempts_rr_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.connect_attempts_timeout, \"proxy.config.http.connect_attempts_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.post_connect_attempts_timeout, \"proxy.config.http.post_connect_attempts_timeout\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_connect_attempts, \"proxy.config.http.parent_proxy.total_connect_attempts\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_retry_time, \"proxy.config.http.parent_proxy.retry_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_fail_threshold, \"proxy.config.http.parent_proxy.fail_threshold\");\n  HttpEstablishStaticConfigLongLong(c.oride.per_parent_connect_attempts,\n                                    \"proxy.config.http.parent_proxy.per_parent_connect_attempts\");\n  HttpEstablishStaticConfigLongLong(c.oride.parent_connect_timeout, \"proxy.config.http.parent_proxy.connect_attempts_timeout\");\n  HttpEstablishStaticConfigByte(c.oride.parent_failures_update_hostdb, \"proxy.config.http.parent_proxy.mark_down_hostdb\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.sock_recv_buffer_size_out, \"proxy.config.net.sock_recv_buffer_size_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_send_buffer_size_out, \"proxy.config.net.sock_send_buffer_size_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_option_flag_out, \"proxy.config.net.sock_option_flag_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_packet_mark_out, \"proxy.config.net.sock_packet_mark_out\");\n  HttpEstablishStaticConfigLongLong(c.oride.sock_packet_tos_out, \"proxy.config.net.sock_packet_tos_out\");\n\n  HttpEstablishStaticConfigByte(c.oride.fwd_proxy_auth_to_parent, \"proxy.config.http.forward.proxy_auth_to_parent\");\n\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_from, \"proxy.config.http.anonymize_remove_from\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_referer, \"proxy.config.http.anonymize_remove_referer\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_user_agent, \"proxy.config.http.anonymize_remove_user_agent\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_cookie, \"proxy.config.http.anonymize_remove_cookie\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_remove_client_ip, \"proxy.config.http.anonymize_remove_client_ip\");\n  HttpEstablishStaticConfigByte(c.oride.anonymize_insert_client_ip, \"proxy.config.http.insert_client_ip\");\n  HttpEstablishStaticConfigStringAlloc(c.anonymize_other_header_list, \"proxy.config.http.anonymize_other_header_list\");\n\n  HttpEstablishStaticConfigStringAlloc(c.oride.global_user_agent_header, \"proxy.config.http.global_user_agent_header\");\n  c.oride.global_user_agent_header_size = c.oride.global_user_agent_header ? strlen(c.oride.global_user_agent_header) : 0;\n\n  HttpEstablishStaticConfigByte(c.oride.proxy_response_server_enabled, \"proxy.config.http.response_server_enabled\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.proxy_response_server_string, \"proxy.config.http.response_server_str\");\n  c.oride.proxy_response_server_string_len =\n    c.oride.proxy_response_server_string ? strlen(c.oride.proxy_response_server_string) : 0;\n\n  HttpEstablishStaticConfigByte(c.oride.insert_squid_x_forwarded_for, \"proxy.config.http.insert_squid_x_forwarded_for\");\n\n  HttpEstablishStaticConfigByte(c.oride.insert_age_in_response, \"proxy.config.http.insert_age_in_response\");\n  HttpEstablishStaticConfigByte(c.enable_http_stats, \"proxy.config.http.enable_http_stats\");\n  HttpEstablishStaticConfigByte(c.oride.normalize_ae, \"proxy.config.http.normalize_ae\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_heuristic_min_lifetime, \"proxy.config.http.cache.heuristic_min_lifetime\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_heuristic_max_lifetime, \"proxy.config.http.cache.heuristic_max_lifetime\");\n  HttpEstablishStaticConfigFloat(c.oride.cache_heuristic_lm_factor, \"proxy.config.http.cache.heuristic_lm_factor\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_guaranteed_min_lifetime, \"proxy.config.http.cache.guaranteed_min_lifetime\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_guaranteed_max_lifetime, \"proxy.config.http.cache.guaranteed_max_lifetime\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.cache_max_stale_age, \"proxy.config.http.cache.max_stale_age\");\n  HttpEstablishStaticConfigByte(c.oride.srv_enabled, \"proxy.config.srv_enabled\");\n\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_text, \"proxy.config.http.cache.vary_default_text\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_images, \"proxy.config.http.cache.vary_default_images\");\n  HttpEstablishStaticConfigStringAlloc(c.oride.cache_vary_default_other, \"proxy.config.http.cache.vary_default_other\");\n\n  // open read failure retries\n  HttpEstablishStaticConfigLongLong(c.oride.max_cache_open_read_retries, \"proxy.config.http.cache.max_open_read_retries\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_open_read_retry_time, \"proxy.config.http.cache.open_read_retry_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.cache_generation_number, \"proxy.config.http.cache.generation\");\n\n  // open write failure retries\n  HttpEstablishStaticConfigLongLong(c.oride.max_cache_open_write_retries, \"proxy.config.http.cache.max_open_write_retries\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_http, \"proxy.config.http.cache.http\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_client_no_cache, \"proxy.config.http.cache.ignore_client_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_client_cc_max_age, \"proxy.config.http.cache.ignore_client_cc_max_age\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ims_on_client_no_cache, \"proxy.config.http.cache.ims_on_client_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_server_no_cache, \"proxy.config.http.cache.ignore_server_no_cache\");\n  HttpEstablishStaticConfigByte(c.oride.cache_responses_to_cookies, \"proxy.config.http.cache.cache_responses_to_cookies\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_ignore_auth, \"proxy.config.http.cache.ignore_authentication\");\n  HttpEstablishStaticConfigByte(c.oride.cache_urls_that_look_dynamic, \"proxy.config.http.cache.cache_urls_that_look_dynamic\");\n  HttpEstablishStaticConfigByte(c.oride.cache_enable_default_vary_headers, \"proxy.config.http.cache.enable_default_vary_headers\");\n  HttpEstablishStaticConfigByte(c.cache_post_method, \"proxy.config.http.cache.post_method\");\n\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_mismatch, \"proxy.config.http.cache.ignore_accept_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_language_mismatch, \"proxy.config.http.cache.ignore_accept_language_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_encoding_mismatch, \"proxy.config.http.cache.ignore_accept_encoding_mismatch\");\n  HttpEstablishStaticConfigByte(c.oride.ignore_accept_charset_mismatch, \"proxy.config.http.cache.ignore_accept_charset_mismatch\");\n\n  HttpEstablishStaticConfigByte(c.send_100_continue_response, \"proxy.config.http.send_100_continue_response\");\n  HttpEstablishStaticConfigByte(c.disallow_post_100_continue, \"proxy.config.http.disallow_post_100_continue\");\n  HttpEstablishStaticConfigByte(c.parser_allow_non_http, \"proxy.config.http.parse.allow_non_http\");\n\n  HttpEstablishStaticConfigByte(c.keepalive_internal_vc, \"proxy.config.http.keepalive_internal_vc\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_open_write_fail_action, \"proxy.config.http.cache.open_write_fail_action\");\n\n  HttpEstablishStaticConfigByte(c.oride.cache_when_to_revalidate, \"proxy.config.http.cache.when_to_revalidate\");\n  HttpEstablishStaticConfigByte(c.oride.cache_required_headers, \"proxy.config.http.cache.required_headers\");\n  HttpEstablishStaticConfigByte(c.oride.cache_range_lookup, \"proxy.config.http.cache.range.lookup\");\n  HttpEstablishStaticConfigByte(c.oride.cache_range_write, \"proxy.config.http.cache.range.write\");\n\n  HttpEstablishStaticConfigStringAlloc(c.connect_ports_string, \"proxy.config.http.connect_ports\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.request_hdr_max_size, \"proxy.config.http.request_header_max_size\");\n  HttpEstablishStaticConfigLongLong(c.oride.response_hdr_max_size, \"proxy.config.http.response_header_max_size\");\n\n  HttpEstablishStaticConfigByte(c.push_method_enabled, \"proxy.config.http.push_method_enabled\");\n\n  HttpEstablishStaticConfigByte(c.reverse_proxy_enabled, \"proxy.config.reverse_proxy.enabled\");\n  HttpEstablishStaticConfigByte(c.url_remap_required, \"proxy.config.url_remap.remap_required\");\n\n  HttpEstablishStaticConfigStringAlloc(c.reverse_proxy_no_host_redirect, \"proxy.config.header.parse.no_host_url_redirect\");\n  c.reverse_proxy_no_host_redirect_len = -1;\n  HttpEstablishStaticConfigStringAlloc(c.oride.body_factory_template_base, \"proxy.config.body_factory.template_base\");\n  c.oride.body_factory_template_base_len = c.oride.body_factory_template_base ? strlen(c.oride.body_factory_template_base) : 0;\n  HttpEstablishStaticConfigLongLong(c.body_factory_response_max_size, \"proxy.config.body_factory.response_max_size\");\n  HttpEstablishStaticConfigByte(c.errors_log_error_pages, \"proxy.config.http.errors.log_error_pages\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.slow_log_threshold, \"proxy.config.http.slow.log.threshold\");\n  HttpEstablishStaticConfigByte(c.oride.ssl_client_verify_server, \"proxy.config.ssl.client.verify.server\");\n\n  HttpEstablishStaticConfigByte(c.record_cop_page, \"proxy.config.http.record_heartbeat\");\n\n  HttpEstablishStaticConfigByte(c.oride.send_http11_requests, \"proxy.config.http.send_http11_requests\");\n  HttpEstablishStaticConfigByte(c.oride.allow_multi_range, \"proxy.config.http.allow_multi_range\");\n\n  // HTTP Referer Filtering\n  HttpEstablishStaticConfigByte(c.referer_filter_enabled, \"proxy.config.http.referer_filter\");\n  HttpEstablishStaticConfigByte(c.referer_format_redirect, \"proxy.config.http.referer_format_redirect\");\n\n  HttpEstablishStaticConfigLongLong(c.oride.down_server_timeout, \"proxy.config.http.down_server.cache_time\");\n  HttpEstablishStaticConfigLongLong(c.oride.client_abort_threshold, \"proxy.config.http.down_server.abort_threshold\");\n\n  // Negative caching and revalidation\n  HttpEstablishStaticConfigByte(c.oride.negative_caching_enabled, \"proxy.config.http.negative_caching_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.negative_caching_lifetime, \"proxy.config.http.negative_caching_lifetime\");\n  HttpEstablishStaticConfigByte(c.oride.negative_revalidating_enabled, \"proxy.config.http.negative_revalidating_enabled\");\n  HttpEstablishStaticConfigLongLong(c.oride.negative_revalidating_lifetime, \"proxy.config.http.negative_revalidating_lifetime\");\n\n  // Buffer size and watermark\n  HttpEstablishStaticConfigLongLong(c.oride.default_buffer_size_index, \"proxy.config.http.default_buffer_size\");\n  HttpEstablishStaticConfigLongLong(c.oride.default_buffer_water_mark, \"proxy.config.http.default_buffer_water_mark\");\n\n  // Stat Page Info\n  HttpEstablishStaticConfigByte(c.enable_http_info, \"proxy.config.http.enable_http_info\");\n\n  HttpEstablishStaticConfigLongLong(c.max_post_size, \"proxy.config.http.max_post_size\");\n\n  //##############################################################################\n  //#\n  //# Redirection\n  //#\n  //# 1. number_of_redirections: The maximum number of redirections YTS permits. 0 == disabled\n  //# 2. redirect_use_orig_cache_key: if set to 1, use original request cache key.\n  //# 3. post_copy_size: The maximum POST data size YTS permits to copy\n  //# 4. redirection_host_no_port: do not include default port in host header during redirection\n  //#\n  //##############################################################################\n  HttpEstablishStaticConfigByte(c.oride.redirect_use_orig_cache_key, \"proxy.config.http.redirect_use_orig_cache_key\");\n  HttpEstablishStaticConfigByte(c.redirection_host_no_port, \"proxy.config.http.redirect_host_no_port\");\n  HttpEstablishStaticConfigLongLong(c.oride.number_of_redirections, \"proxy.config.http.number_of_redirections\");\n  HttpEstablishStaticConfigLongLong(c.post_copy_size, \"proxy.config.http.post_copy_size\");\n\n  // Local Manager\n  HttpEstablishStaticConfigLongLong(c.synthetic_port, \"proxy.config.admin.synthetic_port\");\n\n  http_config_cont->handleEvent(EVENT_NONE, nullptr);\n\n  return;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -224,6 +224,7 @@\n   HttpEstablishStaticConfigByte(c.record_cop_page, \"proxy.config.http.record_heartbeat\");\n \n   HttpEstablishStaticConfigByte(c.oride.send_http11_requests, \"proxy.config.http.send_http11_requests\");\n+  HttpEstablishStaticConfigByte(c.oride.allow_multi_range, \"proxy.config.http.allow_multi_range\");\n \n   // HTTP Referer Filtering\n   HttpEstablishStaticConfigByte(c.referer_filter_enabled, \"proxy.config.http.referer_filter\");",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  HttpEstablishStaticConfigByte(c.oride.allow_multi_range, \"proxy.config.http.allow_multi_range\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/HttpSM::do_range_setup_if_necessary",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "void\nHttpSM::do_range_setup_if_necessary()\n{\n  MIMEField *field;\n  INKVConnInternal *range_trans;\n  int field_content_type_len = -1;\n  const char *content_type;\n\n  ink_assert(t_state.cache_info.object_read != nullptr);\n\n  field = t_state.hdr_info.client_request.field_find(MIME_FIELD_RANGE, MIME_LEN_RANGE);\n  ink_assert(field != nullptr);\n\n  t_state.range_setup = HttpTransact::RANGE_NONE;\n\n  if (t_state.method == HTTP_WKSIDX_GET && t_state.hdr_info.client_request.version_get() == HTTPVersion(1, 1)) {\n    do_range_parse(field);\n\n    if (t_state.range_setup == HttpTransact::RANGE_REQUESTED) {\n      if (!t_state.range_in_cache) {\n        Debug(\"http_range\", \"range can't be satisfied from cache, force origin request\");\n        t_state.cache_lookup_result = HttpTransact::CACHE_LOOKUP_MISS;\n        return;\n      }\n\n      // if only one range entry and pread is capable, no need transform range\n      if (t_state.num_range_fields == 1 && cache_sm.cache_read_vc->is_pread_capable()) {\n        t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;\n      } else if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {\n        Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");\n        content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,\n                                                                                 &field_content_type_len);\n        // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)\n        range_trans = transformProcessor.range_transform(mutex.get(), t_state.ranges, t_state.num_range_fields,\n                                                         &t_state.hdr_info.transform_response, content_type, field_content_type_len,\n                                                         t_state.cache_info.object_read->object_size_get());\n        api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);\n      }\n    }\n  }\n}",
        "func": "void\nHttpSM::do_range_setup_if_necessary()\n{\n  MIMEField *field;\n  INKVConnInternal *range_trans;\n  int field_content_type_len = -1;\n  const char *content_type;\n\n  ink_assert(t_state.cache_info.object_read != nullptr);\n\n  field = t_state.hdr_info.client_request.field_find(MIME_FIELD_RANGE, MIME_LEN_RANGE);\n  ink_assert(field != nullptr);\n\n  t_state.range_setup = HttpTransact::RANGE_NONE;\n\n  if (t_state.method == HTTP_WKSIDX_GET && t_state.hdr_info.client_request.version_get() == HTTPVersion(1, 1)) {\n    do_range_parse(field);\n\n    if (t_state.range_setup == HttpTransact::RANGE_REQUESTED) {\n      bool do_transform = false;\n\n      if (!t_state.range_in_cache) {\n        Debug(\"http_range\", \"range can't be satisfied from cache, force origin request\");\n        t_state.cache_lookup_result = HttpTransact::CACHE_LOOKUP_MISS;\n        return;\n      }\n\n      if (t_state.num_range_fields > 1) {\n        if (0 == t_state.txn_conf->allow_multi_range) {\n          t_state.range_setup = HttpTransact::RANGE_NONE;                                 // No Range required (not allowed)\n          t_state.hdr_info.client_request.field_delete(MIME_FIELD_RANGE, MIME_LEN_RANGE); // ... and nuke the Range header too\n          t_state.num_range_fields = 0;\n        } else if (1 == t_state.txn_conf->allow_multi_range) {\n          do_transform = true;\n        } else {\n          t_state.num_range_fields = 0;\n          t_state.range_setup      = HttpTransact::RANGE_NOT_SATISFIABLE;\n        }\n      } else {\n        if (cache_sm.cache_read_vc->is_pread_capable()) {\n          // If only one range entry and pread is capable, no need transform range\n          t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;\n        } else {\n          do_transform = true;\n        }\n      }\n\n      // We have to do the transform on (allowed) multi-range request, *or* if the VC is not pread capable\n      if (do_transform) {\n        if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {\n          Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");\n          content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,\n                                                                                   &field_content_type_len);\n          // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)\n          range_trans = transformProcessor.range_transform(\n            mutex.get(), t_state.ranges, t_state.num_range_fields, &t_state.hdr_info.transform_response, content_type,\n            field_content_type_len, t_state.cache_info.object_read->object_size_get());\n          api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);\n        } else {\n          // ToDo: Do we do something here? The theory is that multiple transforms do not behave well with\n          // the range transform needed here.\n        }\n      }\n    }\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,24 +17,49 @@\n     do_range_parse(field);\n \n     if (t_state.range_setup == HttpTransact::RANGE_REQUESTED) {\n+      bool do_transform = false;\n+\n       if (!t_state.range_in_cache) {\n         Debug(\"http_range\", \"range can't be satisfied from cache, force origin request\");\n         t_state.cache_lookup_result = HttpTransact::CACHE_LOOKUP_MISS;\n         return;\n       }\n \n-      // if only one range entry and pread is capable, no need transform range\n-      if (t_state.num_range_fields == 1 && cache_sm.cache_read_vc->is_pread_capable()) {\n-        t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;\n-      } else if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {\n-        Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");\n-        content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,\n-                                                                                 &field_content_type_len);\n-        // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)\n-        range_trans = transformProcessor.range_transform(mutex.get(), t_state.ranges, t_state.num_range_fields,\n-                                                         &t_state.hdr_info.transform_response, content_type, field_content_type_len,\n-                                                         t_state.cache_info.object_read->object_size_get());\n-        api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);\n+      if (t_state.num_range_fields > 1) {\n+        if (0 == t_state.txn_conf->allow_multi_range) {\n+          t_state.range_setup = HttpTransact::RANGE_NONE;                                 // No Range required (not allowed)\n+          t_state.hdr_info.client_request.field_delete(MIME_FIELD_RANGE, MIME_LEN_RANGE); // ... and nuke the Range header too\n+          t_state.num_range_fields = 0;\n+        } else if (1 == t_state.txn_conf->allow_multi_range) {\n+          do_transform = true;\n+        } else {\n+          t_state.num_range_fields = 0;\n+          t_state.range_setup      = HttpTransact::RANGE_NOT_SATISFIABLE;\n+        }\n+      } else {\n+        if (cache_sm.cache_read_vc->is_pread_capable()) {\n+          // If only one range entry and pread is capable, no need transform range\n+          t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;\n+        } else {\n+          do_transform = true;\n+        }\n+      }\n+\n+      // We have to do the transform on (allowed) multi-range request, *or* if the VC is not pread capable\n+      if (do_transform) {\n+        if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {\n+          Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");\n+          content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,\n+                                                                                   &field_content_type_len);\n+          // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)\n+          range_trans = transformProcessor.range_transform(\n+            mutex.get(), t_state.ranges, t_state.num_range_fields, &t_state.hdr_info.transform_response, content_type,\n+            field_content_type_len, t_state.cache_info.object_read->object_size_get());\n+          api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);\n+        } else {\n+          // ToDo: Do we do something here? The theory is that multiple transforms do not behave well with\n+          // the range transform needed here.\n+        }\n       }\n     }\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "      // if only one range entry and pread is capable, no need transform range",
                "      if (t_state.num_range_fields == 1 && cache_sm.cache_read_vc->is_pread_capable()) {",
                "        t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;",
                "      } else if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {",
                "        Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");",
                "        content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,",
                "                                                                                 &field_content_type_len);",
                "        // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)",
                "        range_trans = transformProcessor.range_transform(mutex.get(), t_state.ranges, t_state.num_range_fields,",
                "                                                         &t_state.hdr_info.transform_response, content_type, field_content_type_len,",
                "                                                         t_state.cache_info.object_read->object_size_get());",
                "        api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);"
            ],
            "added_lines": [
                "      bool do_transform = false;",
                "",
                "      if (t_state.num_range_fields > 1) {",
                "        if (0 == t_state.txn_conf->allow_multi_range) {",
                "          t_state.range_setup = HttpTransact::RANGE_NONE;                                 // No Range required (not allowed)",
                "          t_state.hdr_info.client_request.field_delete(MIME_FIELD_RANGE, MIME_LEN_RANGE); // ... and nuke the Range header too",
                "          t_state.num_range_fields = 0;",
                "        } else if (1 == t_state.txn_conf->allow_multi_range) {",
                "          do_transform = true;",
                "        } else {",
                "          t_state.num_range_fields = 0;",
                "          t_state.range_setup      = HttpTransact::RANGE_NOT_SATISFIABLE;",
                "        }",
                "      } else {",
                "        if (cache_sm.cache_read_vc->is_pread_capable()) {",
                "          // If only one range entry and pread is capable, no need transform range",
                "          t_state.range_setup = HttpTransact::RANGE_NOT_TRANSFORM_REQUESTED;",
                "        } else {",
                "          do_transform = true;",
                "        }",
                "      }",
                "",
                "      // We have to do the transform on (allowed) multi-range request, *or* if the VC is not pread capable",
                "      if (do_transform) {",
                "        if (api_hooks.get(TS_HTTP_RESPONSE_TRANSFORM_HOOK) == nullptr) {",
                "          Debug(\"http_trans\", \"Unable to accelerate range request, fallback to transform\");",
                "          content_type = t_state.cache_info.object_read->response_get()->value_get(MIME_FIELD_CONTENT_TYPE, MIME_LEN_CONTENT_TYPE,",
                "                                                                                   &field_content_type_len);",
                "          // create a Range: transform processor for requests of type Range: bytes=1-2,4-5,10-100 (eg. multiple ranges)",
                "          range_trans = transformProcessor.range_transform(",
                "            mutex.get(), t_state.ranges, t_state.num_range_fields, &t_state.hdr_info.transform_response, content_type,",
                "            field_content_type_len, t_state.cache_info.object_read->object_size_get());",
                "          api_hooks.append(TS_HTTP_RESPONSE_TRANSFORM_HOOK, range_trans);",
                "        } else {",
                "          // ToDo: Do we do something here? The theory is that multiple transforms do not behave well with",
                "          // the range transform needed here.",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/TSHttpTxnConfigFind",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "TSReturnCode\nTSHttpTxnConfigFind(const char *name, int length, TSOverridableConfigKey *conf, TSRecordDataType *type)\n{\n  sdk_assert(sdk_sanity_check_null_ptr((void *)name) == TS_SUCCESS);\n  sdk_assert(sdk_sanity_check_null_ptr((void *)conf) == TS_SUCCESS);\n\n  TSOverridableConfigKey cnf = TS_CONFIG_NULL;\n  TSRecordDataType typ       = TS_RECORDDATATYPE_INT;\n\n  if (length == -1) {\n    length = strlen(name);\n  }\n  // Lots of string comparisons here, but we avoid quite a few by checking lengths\n  switch (length) {\n  case 24:\n    if (!strncmp(name, \"proxy.config.srv_enabled\", length)) {\n      cnf = TS_CONFIG_SRV_ENABLED;\n    }\n    break;\n  case 28:\n    if (!strncmp(name, \"proxy.config.http.cache.http\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_HTTP;\n    }\n    break;\n\n  case 29:\n    if (!strncmp(name, \"proxy.config.ssl.hsts_max_age\", length)) {\n      cnf = TS_CONFIG_SSL_HSTS_MAX_AGE;\n    }\n    break;\n\n  case 30:\n    if (!strncmp(name, \"proxy.config.http.normalize_ae\", length)) {\n      cnf = TS_CONFIG_HTTP_NORMALIZE_AE;\n    }\n    break;\n\n  case 31:\n    if (!strncmp(name, \"proxy.config.http.chunking.size\", length)) {\n      cnf = TS_CONFIG_HTTP_CHUNKING_SIZE;\n    }\n    break;\n\n  case 33:\n    if (!strncmp(name, \"proxy.config.ssl.client.cert.path\", length)) {\n      cnf = TS_CONFIG_SSL_CERT_FILEPATH;\n      typ = TS_RECORDDATATYPE_STRING;\n    }\n    break;\n\n  case 34:\n    if (!strncmp(name, \"proxy.config.http.chunking_enabled\", length)) {\n      cnf = TS_CONFIG_HTTP_CHUNKING_ENABLED;\n    } else if (!strncmp(name, \"proxy.config.http.cache.generation\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_GENERATION;\n    } else if (!strncmp(name, \"proxy.config.http.insert_client_ip\", length)) {\n      cnf = TS_CONFIG_HTTP_ANONYMIZE_INSERT_CLIENT_IP;\n    } else if (!strncmp(name, \"proxy.config.http.insert_forwarded\", length)) {\n      cnf = TS_CONFIG_HTTP_INSERT_FORWARDED;\n      typ = TS_RECORDDATATYPE_STRING;\n    }\n    break;\n\n  case 35:\n    if (!strncmp(name, \"proxy.config.http.cache.range.write\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_RANGE_WRITE;\n    }\n    break;\n\n  case 36:\n    switch (name[length - 1]) {\n    case 'p':\n      if (!strncmp(name, \"proxy.config.http.cache.range.lookup\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_RANGE_LOOKUP;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.net.sock_packet_tos_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_PACKET_TOS_OUT;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.slow.log.threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_SLOW_LOG_THRESHOLD;\n      }\n      break;\n    }\n    break;\n\n  case 37:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.max_stale_age\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_STALE_AGE;\n      } else if (!strncmp(name, \"proxy.config.http.default_buffer_size\", length)) {\n        cnf = TS_CONFIG_HTTP_DEFAULT_BUFFER_SIZE;\n      } else if (!strncmp(name, \"proxy.config.ssl.client.cert.filename\", length)) {\n        cnf = TS_CONFIG_SSL_CERT_FILENAME;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.response_server_str\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_SERVER_STR;\n        typ = TS_RECORDDATATYPE_STRING;\n      } else if (!strncmp(name, \"proxy.config.ssl.client.verify.server\", length)) {\n        cnf = TS_CONFIG_SSL_CLIENT_VERIFY_SERVER;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_post_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_POST_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_option_flag_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_OPTION_FLAG_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_packet_mark_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_PACKET_MARK_OUT;\n      } else if (!strncmp(name, \"proxy.config.websocket.active_timeout\", length)) {\n        cnf = TS_CONFIG_WEBSOCKET_ACTIVE_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 38:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.server_tcp_init_cwnd\", length)) {\n        cnf = TS_CONFIG_HTTP_SERVER_TCP_INIT_CWND;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_ENABLED;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.send_http11_requests\", length)) {\n        cnf = TS_CONFIG_HTTP_SEND_HTTP11_REQUESTS;\n      }\n      break;\n    }\n    break;\n\n  case 39:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.body_factory.template_base\", length)) {\n        cnf = TS_CONFIG_BODY_FACTORY_TEMPLATE_BASE;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    case 'm':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_from\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_FROM;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_enabled_in\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_IN;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.doc_in_cache_skip_dns\", length)) {\n        cnf = TS_CONFIG_HTTP_DOC_IN_CACHE_SKIP_DNS;\n      }\n      break;\n    }\n    break;\n\n  case 40:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.forward_connect_method\", length)) {\n        cnf = TS_CONFIG_HTTP_FORWARD_CONNECT_METHOD;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.down_server.cache_time\", length)) {\n        cnf = TS_CONFIG_HTTP_DOWN_SERVER_CACHE_TIME;\n      } else if (!strncmp(name, \"proxy.config.http.insert_age_in_response\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_AGE_IN_RESPONSE;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.url_remap.pristine_host_hdr\", length)) {\n        cnf = TS_CONFIG_URL_REMAP_PRISTINE_HOST_HDR;\n      } else if (!strncmp(name, \"proxy.config.http.insert_request_via_str\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_REQUEST_VIA_STR;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.low_water\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_LOW_WATER_MARK;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.origin_max_connections\", length)) {\n        cnf = TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS;\n      } else if (!strncmp(name, \"proxy.config.http.cache.required_headers\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_REQUIRED_HEADERS;\n      } else if (!strncmp(name, \"proxy.config.ssl.hsts_include_subdomains\", length)) {\n        cnf = TS_CONFIG_SSL_HSTS_INCLUDE_SUBDOMAINS;\n      } else if (!strncmp(name, \"proxy.config.http.number_of_redirections\", length)) {\n        cnf = TS_CONFIG_HTTP_NUMBER_OF_REDIRECTIONS;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_enabled_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_OUT;\n      }\n      break;\n    }\n    break;\n\n  case 41:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.response_server_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_SERVER_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_cookie\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_COOKIE;\n      } else if (!strncmp(name, \"proxy.config.http.request_header_max_size\", length)) {\n        cnf = TS_CONFIG_HTTP_REQUEST_HEADER_MAX_SIZE;\n      } else if (!strncmp(name, \"proxy.config.http.parent_proxy.retry_time\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_RETRY_TIME;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.insert_response_via_str\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_RESPONSE_VIA_STR;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.high_water\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_HIGH_WATER_MARK;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.cache.vary_default_text\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_TEXT;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    }\n    break;\n\n  case 42:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.negative_caching_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_CACHING_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.when_to_revalidate\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_WHEN_TO_REVALIDATE;\n      } else if (!strncmp(name, \"proxy.config.http.response_header_max_size\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_HEADER_MAX_SIZE;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_referer\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_REFERER;\n      } else if (!strncmp(name, \"proxy.config.http.global_user_agent_header\", length)) {\n        cnf = TS_CONFIG_HTTP_GLOBAL_USER_AGENT_HEADER;\n        typ = TS_RECORDDATATYPE_STRING;\n      } else if (!strncmp(name, \"proxy.config.http.cache.vary_default_other\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_OTHER;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.net.sock_recv_buffer_size_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_RECV_BUFFER_SIZE_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_send_buffer_size_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_SEND_BUFFER_SIZE_OUT;\n      } else if (!strncmp(name, \"proxy.config.http.connect_attempts_timeout\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_TIMEOUT;\n      } else if (!strncmp(name, \"proxy.config.websocket.no_activity_timeout\", length)) {\n        cnf = TS_CONFIG_WEBSOCKET_NO_ACTIVITY_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 43:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.negative_caching_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_CACHING_LIFETIME;\n      }\n      break;\n    case 'k':\n      if (!strncmp(name, \"proxy.config.http.default_buffer_water_mark\", length)) {\n        cnf = TS_CONFIG_HTTP_DEFAULT_BUFFER_WATER_MARK;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.cache.heuristic_lm_factor\", length)) {\n        typ = TS_RECORDDATATYPE_FLOAT;\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_LM_FACTOR;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.cache.vary_default_images\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_IMAGES;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    }\n    break;\n\n  case 44:\n    switch (name[length - 1]) {\n    case 'p':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_client_ip\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_CLIENT_IP;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.open_read_retry_time\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_OPEN_READ_RETRY_TIME;\n      }\n      break;\n    }\n    break;\n\n  case 45:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.down_server.abort_threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_DOWN_SERVER_ABORT_THRESHOLD;\n      } else if (!strncmp(name, \"proxy.config.http.parent_proxy.fail_threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_FAIL_THRESHOLD;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_authentication\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_AUTHENTICATION;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_user_agent\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_USER_AGENT;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.connect_attempts_rr_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_RR_RETRIES;\n      } else if (!strncmp(name, \"proxy.config.http.cache.max_open_read_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_OPEN_READ_RETRIES;\n      }\n      break;\n    case 'e':\n      if (0 == strncmp(name, \"proxy.config.http.auth_server_session_private\", length)) {\n        cnf = TS_CONFIG_HTTP_AUTH_SERVER_SESSION_PRIVATE;\n      }\n      break;\n    case 'y':\n      if (!strncmp(name, \"proxy.config.http.redirect_use_orig_cache_key\", length)) {\n        cnf = TS_CONFIG_HTTP_REDIRECT_USE_ORIG_CACHE_KEY;\n      }\n      break;\n    }\n    break;\n\n  case 46:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_client_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ims_on_client_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IMS_ON_CLIENT_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ignore_server_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_SERVER_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.heuristic_min_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_MIN_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.cache.heuristic_max_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_MAX_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.origin_max_connections_queue\", length)) {\n        cnf = TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS_QUEUE;\n      }\n      break;\n    case 'h':\n      if (0 == strncmp(name, \"proxy.config.http.server_session_sharing.match\", length)) {\n        cnf = TS_CONFIG_HTTP_SERVER_SESSION_SHARING_MATCH;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_mismatch\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_MISMATCH;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.cache.open_write_fail_action\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_OPEN_WRITE_FAIL_ACTION;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.insert_squid_x_forwarded_for\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_SQUID_X_FORWARDED_FOR;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.connect_attempts_max_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES;\n      } else if (!strncmp(name, \"proxy.config.http.cache.max_open_write_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_OPEN_WRITE_RETRIES;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.forward.proxy_auth_to_parent\", length)) {\n        cnf = TS_CONFIG_HTTP_FORWARD_PROXY_AUTH_TO_PARENT;\n      }\n      break;\n    }\n    break;\n\n  case 47:\n    switch (name[length - 1]) {\n    case 'b':\n      if (!strncmp(name, \"proxy.config.http.parent_proxy.mark_down_hostdb\", length)) {\n        cnf = TS_CONFIG_PARENT_FAILURES_UPDATE_HOSTDB;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.negative_revalidating_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.guaranteed_min_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_GUARANTEED_MIN_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.cache.guaranteed_max_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_GUARANTEED_MAX_LIFETIME;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.transaction_active_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_IN;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.post_connect_attempts_timeout\", length)) {\n        cnf = TS_CONFIG_HTTP_POST_CONNECT_ATTEMPTS_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 48:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_client_cc_max_age\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_CC_MAX_AGE;\n      } else if (!strncmp(name, \"proxy.config.http.negative_revalidating_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_LIFETIME;\n      }\n      break;\n    case 't':\n      switch (name[length - 4]) {\n      case '_':\n        if (!strncmp(name, \"proxy.config.http.transaction_active_timeout_out\", length)) {\n          cnf = TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_OUT;\n        }\n        break;\n      case 'e':\n        if (!strncmp(name, \"proxy.config.http.background_fill_active_timeout\", length)) {\n          cnf = TS_CONFIG_HTTP_BACKGROUND_FILL_ACTIVE_TIMEOUT;\n        }\n        break;\n      }\n      break;\n    }\n    break;\n\n  case 49:\n    if (!strncmp(name, \"proxy.config.http.attach_server_session_to_client\", length)) {\n      cnf = TS_CONFIG_HTTP_ATTACH_SERVER_SESSION_TO_CLIENT;\n    }\n    break;\n\n  case 50:\n    if (!strncmp(name, \"proxy.config.http.cache.cache_responses_to_cookies\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_CACHE_RESPONSES_TO_COOKIES;\n    }\n    break;\n\n  case 51:\n    switch (name[length - 1]) {\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_no_activity_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_IN;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.post.check.content_length.enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_POST_CHECK_CONTENT_LENGTH_ENABLED;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.cache.enable_default_vary_headers\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_ENABLE_DEFAULT_VARY_HEADER;\n      }\n      break;\n    }\n    break;\n\n  case 52:\n    switch (name[length - 1]) {\n    case 'c':\n      if (!strncmp(name, \"proxy.config.http.cache.cache_urls_that_look_dynamic\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_CACHE_URLS_THAT_LOOK_DYNAMIC;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.transaction_no_activity_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_IN;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_no_activity_timeout_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_OUT;\n      } else if (!strncmp(name, \"proxy.config.http.uncacheable_requests_bypass_parent\", length)) {\n        cnf = TS_CONFIG_HTTP_UNCACHEABLE_REQUESTS_BYPASS_PARENT;\n      }\n      break;\n    }\n    break;\n\n  case 53:\n    switch (name[length - 1]) {\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.transaction_no_activity_timeout_out\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_OUT;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.background_fill_completed_threshold\", length)) {\n        typ = TS_RECORDDATATYPE_FLOAT;\n        cnf = TS_CONFIG_HTTP_BACKGROUND_FILL_COMPLETED_THRESHOLD;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.parent_proxy.total_connect_attempts\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_TOTAL_CONNECT_ATTEMPTS;\n      }\n      break;\n    }\n    break;\n\n  case 54:\n    if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_charset_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_CHARSET_MISMATCH;\n    }\n    break;\n\n  case 55:\n    if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_language_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_LANGUAGE_MISMATCH;\n    } else if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_encoding_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_ENCODING_MISMATCH;\n    } else if (!strncmp(name, \"proxy.config.http.parent_proxy.connect_attempts_timeout\", length)) {\n      cnf = TS_CONFIG_HTTP_PARENT_CONNECT_ATTEMPT_TIMEOUT;\n    }\n    break;\n\n  case 58:\n    if (!strncmp(name, \"proxy.config.http.connect_attempts_max_retries_dead_server\", length)) {\n      cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES_DEAD_SERVER;\n    } else if (!strncmp(name, \"proxy.config.http.parent_proxy.per_parent_connect_attempts\", length)) {\n      cnf = TS_CONFIG_HTTP_PER_PARENT_CONNECT_ATTEMPTS;\n    }\n    break;\n  }\n\n  *conf = cnf;\n  if (type) {\n    *type = typ;\n  }\n\n  return ((cnf != TS_CONFIG_NULL) ? TS_SUCCESS : TS_ERROR);\n}",
        "func": "TSReturnCode\nTSHttpTxnConfigFind(const char *name, int length, TSOverridableConfigKey *conf, TSRecordDataType *type)\n{\n  sdk_assert(sdk_sanity_check_null_ptr((void *)name) == TS_SUCCESS);\n  sdk_assert(sdk_sanity_check_null_ptr((void *)conf) == TS_SUCCESS);\n\n  TSOverridableConfigKey cnf = TS_CONFIG_NULL;\n  TSRecordDataType typ       = TS_RECORDDATATYPE_INT;\n\n  if (length == -1) {\n    length = strlen(name);\n  }\n  // Lots of string comparisons here, but we avoid quite a few by checking lengths\n  switch (length) {\n  case 24:\n    if (!strncmp(name, \"proxy.config.srv_enabled\", length)) {\n      cnf = TS_CONFIG_SRV_ENABLED;\n    }\n    break;\n  case 28:\n    if (!strncmp(name, \"proxy.config.http.cache.http\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_HTTP;\n    }\n    break;\n\n  case 29:\n    if (!strncmp(name, \"proxy.config.ssl.hsts_max_age\", length)) {\n      cnf = TS_CONFIG_SSL_HSTS_MAX_AGE;\n    }\n    break;\n\n  case 30:\n    if (!strncmp(name, \"proxy.config.http.normalize_ae\", length)) {\n      cnf = TS_CONFIG_HTTP_NORMALIZE_AE;\n    }\n    break;\n\n  case 31:\n    if (!strncmp(name, \"proxy.config.http.chunking.size\", length)) {\n      cnf = TS_CONFIG_HTTP_CHUNKING_SIZE;\n    }\n    break;\n\n  case 33:\n    if (!strncmp(name, \"proxy.config.ssl.client.cert.path\", length)) {\n      cnf = TS_CONFIG_SSL_CERT_FILEPATH;\n      typ = TS_RECORDDATATYPE_STRING;\n    }\n    break;\n\n  case 34:\n    if (!strncmp(name, \"proxy.config.http.chunking_enabled\", length)) {\n      cnf = TS_CONFIG_HTTP_CHUNKING_ENABLED;\n    } else if (!strncmp(name, \"proxy.config.http.cache.generation\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_GENERATION;\n    } else if (!strncmp(name, \"proxy.config.http.insert_client_ip\", length)) {\n      cnf = TS_CONFIG_HTTP_ANONYMIZE_INSERT_CLIENT_IP;\n    } else if (!strncmp(name, \"proxy.config.http.insert_forwarded\", length)) {\n      cnf = TS_CONFIG_HTTP_INSERT_FORWARDED;\n      typ = TS_RECORDDATATYPE_STRING;\n    }\n    break;\n\n  case 35:\n    if (!strncmp(name, \"proxy.config.http.cache.range.write\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_RANGE_WRITE;\n    } else if (!strncmp(name, \"proxy.config.http.allow_multi_range\", length)) {\n      cnf = TS_CONFIG_HTTP_ALLOW_MULTI_RANGE;\n    }\n    break;\n\n  case 36:\n    switch (name[length - 1]) {\n    case 'p':\n      if (!strncmp(name, \"proxy.config.http.cache.range.lookup\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_RANGE_LOOKUP;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.net.sock_packet_tos_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_PACKET_TOS_OUT;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.slow.log.threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_SLOW_LOG_THRESHOLD;\n      }\n      break;\n    }\n    break;\n\n  case 37:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.max_stale_age\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_STALE_AGE;\n      } else if (!strncmp(name, \"proxy.config.http.default_buffer_size\", length)) {\n        cnf = TS_CONFIG_HTTP_DEFAULT_BUFFER_SIZE;\n      } else if (!strncmp(name, \"proxy.config.ssl.client.cert.filename\", length)) {\n        cnf = TS_CONFIG_SSL_CERT_FILENAME;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.response_server_str\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_SERVER_STR;\n        typ = TS_RECORDDATATYPE_STRING;\n      } else if (!strncmp(name, \"proxy.config.ssl.client.verify.server\", length)) {\n        cnf = TS_CONFIG_SSL_CLIENT_VERIFY_SERVER;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_post_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_POST_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_option_flag_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_OPTION_FLAG_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_packet_mark_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_PACKET_MARK_OUT;\n      } else if (!strncmp(name, \"proxy.config.websocket.active_timeout\", length)) {\n        cnf = TS_CONFIG_WEBSOCKET_ACTIVE_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 38:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.server_tcp_init_cwnd\", length)) {\n        cnf = TS_CONFIG_HTTP_SERVER_TCP_INIT_CWND;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_ENABLED;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.send_http11_requests\", length)) {\n        cnf = TS_CONFIG_HTTP_SEND_HTTP11_REQUESTS;\n      }\n      break;\n    }\n    break;\n\n  case 39:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.body_factory.template_base\", length)) {\n        cnf = TS_CONFIG_BODY_FACTORY_TEMPLATE_BASE;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    case 'm':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_from\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_FROM;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_enabled_in\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_IN;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.doc_in_cache_skip_dns\", length)) {\n        cnf = TS_CONFIG_HTTP_DOC_IN_CACHE_SKIP_DNS;\n      }\n      break;\n    }\n    break;\n\n  case 40:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.forward_connect_method\", length)) {\n        cnf = TS_CONFIG_HTTP_FORWARD_CONNECT_METHOD;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.down_server.cache_time\", length)) {\n        cnf = TS_CONFIG_HTTP_DOWN_SERVER_CACHE_TIME;\n      } else if (!strncmp(name, \"proxy.config.http.insert_age_in_response\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_AGE_IN_RESPONSE;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.url_remap.pristine_host_hdr\", length)) {\n        cnf = TS_CONFIG_URL_REMAP_PRISTINE_HOST_HDR;\n      } else if (!strncmp(name, \"proxy.config.http.insert_request_via_str\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_REQUEST_VIA_STR;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.low_water\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_LOW_WATER_MARK;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.origin_max_connections\", length)) {\n        cnf = TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS;\n      } else if (!strncmp(name, \"proxy.config.http.cache.required_headers\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_REQUIRED_HEADERS;\n      } else if (!strncmp(name, \"proxy.config.ssl.hsts_include_subdomains\", length)) {\n        cnf = TS_CONFIG_SSL_HSTS_INCLUDE_SUBDOMAINS;\n      } else if (!strncmp(name, \"proxy.config.http.number_of_redirections\", length)) {\n        cnf = TS_CONFIG_HTTP_NUMBER_OF_REDIRECTIONS;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_enabled_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_OUT;\n      }\n      break;\n    }\n    break;\n\n  case 41:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.response_server_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_SERVER_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_cookie\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_COOKIE;\n      } else if (!strncmp(name, \"proxy.config.http.request_header_max_size\", length)) {\n        cnf = TS_CONFIG_HTTP_REQUEST_HEADER_MAX_SIZE;\n      } else if (!strncmp(name, \"proxy.config.http.parent_proxy.retry_time\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_RETRY_TIME;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.insert_response_via_str\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_RESPONSE_VIA_STR;\n      } else if (!strncmp(name, \"proxy.config.http.flow_control.high_water\", length)) {\n        cnf = TS_CONFIG_HTTP_FLOW_CONTROL_HIGH_WATER_MARK;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.cache.vary_default_text\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_TEXT;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    }\n    break;\n\n  case 42:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.negative_caching_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_CACHING_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.when_to_revalidate\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_WHEN_TO_REVALIDATE;\n      } else if (!strncmp(name, \"proxy.config.http.response_header_max_size\", length)) {\n        cnf = TS_CONFIG_HTTP_RESPONSE_HEADER_MAX_SIZE;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_referer\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_REFERER;\n      } else if (!strncmp(name, \"proxy.config.http.global_user_agent_header\", length)) {\n        cnf = TS_CONFIG_HTTP_GLOBAL_USER_AGENT_HEADER;\n        typ = TS_RECORDDATATYPE_STRING;\n      } else if (!strncmp(name, \"proxy.config.http.cache.vary_default_other\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_OTHER;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.net.sock_recv_buffer_size_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_RECV_BUFFER_SIZE_OUT;\n      } else if (!strncmp(name, \"proxy.config.net.sock_send_buffer_size_out\", length)) {\n        cnf = TS_CONFIG_NET_SOCK_SEND_BUFFER_SIZE_OUT;\n      } else if (!strncmp(name, \"proxy.config.http.connect_attempts_timeout\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_TIMEOUT;\n      } else if (!strncmp(name, \"proxy.config.websocket.no_activity_timeout\", length)) {\n        cnf = TS_CONFIG_WEBSOCKET_NO_ACTIVITY_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 43:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.negative_caching_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_CACHING_LIFETIME;\n      }\n      break;\n    case 'k':\n      if (!strncmp(name, \"proxy.config.http.default_buffer_water_mark\", length)) {\n        cnf = TS_CONFIG_HTTP_DEFAULT_BUFFER_WATER_MARK;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.cache.heuristic_lm_factor\", length)) {\n        typ = TS_RECORDDATATYPE_FLOAT;\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_LM_FACTOR;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.cache.vary_default_images\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_IMAGES;\n        typ = TS_RECORDDATATYPE_STRING;\n      }\n      break;\n    }\n    break;\n\n  case 44:\n    switch (name[length - 1]) {\n    case 'p':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_client_ip\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_CLIENT_IP;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.open_read_retry_time\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_OPEN_READ_RETRY_TIME;\n      }\n      break;\n    }\n    break;\n\n  case 45:\n    switch (name[length - 1]) {\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.down_server.abort_threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_DOWN_SERVER_ABORT_THRESHOLD;\n      } else if (!strncmp(name, \"proxy.config.http.parent_proxy.fail_threshold\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_FAIL_THRESHOLD;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_authentication\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_AUTHENTICATION;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.anonymize_remove_user_agent\", length)) {\n        cnf = TS_CONFIG_HTTP_ANONYMIZE_REMOVE_USER_AGENT;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.connect_attempts_rr_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_RR_RETRIES;\n      } else if (!strncmp(name, \"proxy.config.http.cache.max_open_read_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_OPEN_READ_RETRIES;\n      }\n      break;\n    case 'e':\n      if (0 == strncmp(name, \"proxy.config.http.auth_server_session_private\", length)) {\n        cnf = TS_CONFIG_HTTP_AUTH_SERVER_SESSION_PRIVATE;\n      }\n      break;\n    case 'y':\n      if (!strncmp(name, \"proxy.config.http.redirect_use_orig_cache_key\", length)) {\n        cnf = TS_CONFIG_HTTP_REDIRECT_USE_ORIG_CACHE_KEY;\n      }\n      break;\n    }\n    break;\n\n  case 46:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_client_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ims_on_client_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IMS_ON_CLIENT_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ignore_server_no_cache\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_SERVER_NO_CACHE;\n      } else if (!strncmp(name, \"proxy.config.http.cache.heuristic_min_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_MIN_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.cache.heuristic_max_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_HEURISTIC_MAX_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.origin_max_connections_queue\", length)) {\n        cnf = TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS_QUEUE;\n      }\n      break;\n    case 'h':\n      if (0 == strncmp(name, \"proxy.config.http.server_session_sharing.match\", length)) {\n        cnf = TS_CONFIG_HTTP_SERVER_SESSION_SHARING_MATCH;\n      } else if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_mismatch\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_MISMATCH;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.cache.open_write_fail_action\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_OPEN_WRITE_FAIL_ACTION;\n      }\n      break;\n    case 'r':\n      if (!strncmp(name, \"proxy.config.http.insert_squid_x_forwarded_for\", length)) {\n        cnf = TS_CONFIG_HTTP_INSERT_SQUID_X_FORWARDED_FOR;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.connect_attempts_max_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES;\n      } else if (!strncmp(name, \"proxy.config.http.cache.max_open_write_retries\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_MAX_OPEN_WRITE_RETRIES;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.forward.proxy_auth_to_parent\", length)) {\n        cnf = TS_CONFIG_HTTP_FORWARD_PROXY_AUTH_TO_PARENT;\n      }\n      break;\n    }\n    break;\n\n  case 47:\n    switch (name[length - 1]) {\n    case 'b':\n      if (!strncmp(name, \"proxy.config.http.parent_proxy.mark_down_hostdb\", length)) {\n        cnf = TS_CONFIG_PARENT_FAILURES_UPDATE_HOSTDB;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.negative_revalidating_enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_ENABLED;\n      }\n      break;\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.guaranteed_min_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_GUARANTEED_MIN_LIFETIME;\n      } else if (!strncmp(name, \"proxy.config.http.cache.guaranteed_max_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_GUARANTEED_MAX_LIFETIME;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.transaction_active_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_IN;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.post_connect_attempts_timeout\", length)) {\n        cnf = TS_CONFIG_HTTP_POST_CONNECT_ATTEMPTS_TIMEOUT;\n      }\n      break;\n    }\n    break;\n\n  case 48:\n    switch (name[length - 1]) {\n    case 'e':\n      if (!strncmp(name, \"proxy.config.http.cache.ignore_client_cc_max_age\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_CC_MAX_AGE;\n      } else if (!strncmp(name, \"proxy.config.http.negative_revalidating_lifetime\", length)) {\n        cnf = TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_LIFETIME;\n      }\n      break;\n    case 't':\n      switch (name[length - 4]) {\n      case '_':\n        if (!strncmp(name, \"proxy.config.http.transaction_active_timeout_out\", length)) {\n          cnf = TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_OUT;\n        }\n        break;\n      case 'e':\n        if (!strncmp(name, \"proxy.config.http.background_fill_active_timeout\", length)) {\n          cnf = TS_CONFIG_HTTP_BACKGROUND_FILL_ACTIVE_TIMEOUT;\n        }\n        break;\n      }\n      break;\n    }\n    break;\n\n  case 49:\n    if (!strncmp(name, \"proxy.config.http.attach_server_session_to_client\", length)) {\n      cnf = TS_CONFIG_HTTP_ATTACH_SERVER_SESSION_TO_CLIENT;\n    }\n    break;\n\n  case 50:\n    if (!strncmp(name, \"proxy.config.http.cache.cache_responses_to_cookies\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_CACHE_RESPONSES_TO_COOKIES;\n    }\n    break;\n\n  case 51:\n    switch (name[length - 1]) {\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_no_activity_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_IN;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.post.check.content_length.enabled\", length)) {\n        cnf = TS_CONFIG_HTTP_POST_CHECK_CONTENT_LENGTH_ENABLED;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.cache.enable_default_vary_headers\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_ENABLE_DEFAULT_VARY_HEADER;\n      }\n      break;\n    }\n    break;\n\n  case 52:\n    switch (name[length - 1]) {\n    case 'c':\n      if (!strncmp(name, \"proxy.config.http.cache.cache_urls_that_look_dynamic\", length)) {\n        cnf = TS_CONFIG_HTTP_CACHE_CACHE_URLS_THAT_LOOK_DYNAMIC;\n      }\n      break;\n    case 'n':\n      if (!strncmp(name, \"proxy.config.http.transaction_no_activity_timeout_in\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_IN;\n      }\n      break;\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.keep_alive_no_activity_timeout_out\", length)) {\n        cnf = TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_OUT;\n      } else if (!strncmp(name, \"proxy.config.http.uncacheable_requests_bypass_parent\", length)) {\n        cnf = TS_CONFIG_HTTP_UNCACHEABLE_REQUESTS_BYPASS_PARENT;\n      }\n      break;\n    }\n    break;\n\n  case 53:\n    switch (name[length - 1]) {\n    case 't':\n      if (!strncmp(name, \"proxy.config.http.transaction_no_activity_timeout_out\", length)) {\n        cnf = TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_OUT;\n      }\n      break;\n    case 'd':\n      if (!strncmp(name, \"proxy.config.http.background_fill_completed_threshold\", length)) {\n        typ = TS_RECORDDATATYPE_FLOAT;\n        cnf = TS_CONFIG_HTTP_BACKGROUND_FILL_COMPLETED_THRESHOLD;\n      }\n      break;\n    case 's':\n      if (!strncmp(name, \"proxy.config.http.parent_proxy.total_connect_attempts\", length)) {\n        cnf = TS_CONFIG_HTTP_PARENT_PROXY_TOTAL_CONNECT_ATTEMPTS;\n      }\n      break;\n    }\n    break;\n\n  case 54:\n    if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_charset_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_CHARSET_MISMATCH;\n    }\n    break;\n\n  case 55:\n    if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_language_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_LANGUAGE_MISMATCH;\n    } else if (!strncmp(name, \"proxy.config.http.cache.ignore_accept_encoding_mismatch\", length)) {\n      cnf = TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_ENCODING_MISMATCH;\n    } else if (!strncmp(name, \"proxy.config.http.parent_proxy.connect_attempts_timeout\", length)) {\n      cnf = TS_CONFIG_HTTP_PARENT_CONNECT_ATTEMPT_TIMEOUT;\n    }\n    break;\n\n  case 58:\n    if (!strncmp(name, \"proxy.config.http.connect_attempts_max_retries_dead_server\", length)) {\n      cnf = TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES_DEAD_SERVER;\n    } else if (!strncmp(name, \"proxy.config.http.parent_proxy.per_parent_connect_attempts\", length)) {\n      cnf = TS_CONFIG_HTTP_PER_PARENT_CONNECT_ATTEMPTS;\n    }\n    break;\n  }\n\n  *conf = cnf;\n  if (type) {\n    *type = typ;\n  }\n\n  return ((cnf != TS_CONFIG_NULL) ? TS_SUCCESS : TS_ERROR);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -64,6 +64,8 @@\n   case 35:\n     if (!strncmp(name, \"proxy.config.http.cache.range.write\", length)) {\n       cnf = TS_CONFIG_HTTP_CACHE_RANGE_WRITE;\n+    } else if (!strncmp(name, \"proxy.config.http.allow_multi_range\", length)) {\n+      cnf = TS_CONFIG_HTTP_ALLOW_MULTI_RANGE;\n     }\n     break;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    } else if (!strncmp(name, \"proxy.config.http.allow_multi_range\", length)) {",
                "      cnf = TS_CONFIG_HTTP_ALLOW_MULTI_RANGE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8005",
        "func_name": "apache/trafficserver/_conf_to_memberp",
        "description": "When there are multiple ranges in a range request, Apache Traffic Server (ATS) will read the entire object from cache. This can cause performance problems with large objects in cache. This affects versions 6.0.0 to 6.2.2 and 7.0.0 to 7.1.3. To resolve this issue users running 6.x users should upgrade to 6.2.3 or later versions and 7.x users should upgrade to 7.1.4 or later versions.",
        "git_url": "https://github.com/apache/trafficserver/commit/6d248026b04d69e5c5049709c17ea671328ea4ea",
        "commit_title": "Adds a new configuration proxy.config.http.allow_multi_range",
        "commit_text": " This is needed to prevent potential abuse with well formed multi- range requests.",
        "func_before": "static void *\n_conf_to_memberp(TSOverridableConfigKey conf, OverridableHttpConfigParams *overridableHttpConfig, std::type_info const *&typep)\n{\n  void *ret = nullptr;\n  typep     = &typeid(void);\n\n  switch (conf) {\n  case TS_CONFIG_URL_REMAP_PRISTINE_HOST_HDR:\n    ret = _memberp_to_generic(&overridableHttpConfig->maintain_pristine_host_hdr, typep);\n    break;\n  case TS_CONFIG_HTTP_CHUNKING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->chunking_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_CACHING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_caching_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_CACHING_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_caching_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_WHEN_TO_REVALIDATE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_when_to_revalidate, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_enabled_in, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_enabled_out, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_POST_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_post_out, typep);\n    break;\n  case TS_CONFIG_HTTP_SERVER_SESSION_SHARING_MATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->server_session_sharing_match, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_RECV_BUFFER_SIZE_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_recv_buffer_size_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_SEND_BUFFER_SIZE_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_send_buffer_size_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_OPTION_FLAG_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_option_flag_out, typep);\n    break;\n  case TS_CONFIG_HTTP_FORWARD_PROXY_AUTH_TO_PARENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->fwd_proxy_auth_to_parent, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_FROM:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_from, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_REFERER:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_referer, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_USER_AGENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_user_agent, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_COOKIE:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_cookie, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_CLIENT_IP:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_client_ip, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_INSERT_CLIENT_IP:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_insert_client_ip, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_SERVER_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_server_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_SQUID_X_FORWARDED_FOR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_squid_x_forwarded_for, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_FORWARDED:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_forwarded, typep);\n    break;\n  case TS_CONFIG_HTTP_SERVER_TCP_INIT_CWND:\n    ret = _memberp_to_generic(&overridableHttpConfig->server_tcp_init_cwnd, typep);\n    break;\n  case TS_CONFIG_HTTP_SEND_HTTP11_REQUESTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->send_http11_requests, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HTTP:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_http, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_client_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_CC_MAX_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_client_cc_max_age, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IMS_ON_CLIENT_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ims_on_client_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_SERVER_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_server_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_CACHE_RESPONSES_TO_COOKIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_responses_to_cookies, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_AUTHENTICATION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_auth, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_CACHE_URLS_THAT_LOOK_DYNAMIC:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_urls_that_look_dynamic, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_REQUIRED_HEADERS:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_required_headers, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_REQUEST_VIA_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_request_via_string, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_RESPONSE_VIA_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_response_via_string, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_MIN_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_min_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_MAX_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_max_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GUARANTEED_MIN_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_guaranteed_min_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GUARANTEED_MAX_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_guaranteed_max_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_STALE_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_max_stale_age, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_no_activity_timeout_in, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_no_activity_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_no_activity_timeout_in, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_no_activity_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_active_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS:\n    ret = _memberp_to_generic(&overridableHttpConfig->origin_max_connections, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_max_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES_DEAD_SERVER:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_max_retries_dead_server, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_RR_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_rr_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_POST_CONNECT_ATTEMPTS_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->post_connect_attempts_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_DOWN_SERVER_CACHE_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->down_server_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_DOWN_SERVER_ABORT_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_abort_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_DOC_IN_CACHE_SKIP_DNS:\n    ret = _memberp_to_generic(&overridableHttpConfig->doc_in_cache_skip_dns, typep);\n    break;\n  case TS_CONFIG_HTTP_BACKGROUND_FILL_ACTIVE_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->background_fill_active_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_SERVER_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_server_string, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_LM_FACTOR:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_lm_factor, typep);\n    break;\n  case TS_CONFIG_HTTP_BACKGROUND_FILL_COMPLETED_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->background_fill_threshold, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_PACKET_MARK_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_packet_mark_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_PACKET_TOS_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_packet_tos_out, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_AGE_IN_RESPONSE:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_age_in_response, typep);\n    break;\n  case TS_CONFIG_HTTP_CHUNKING_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->http_chunking_size, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_control_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_LOW_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_low_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_HIGH_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_high_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_RANGE_LOOKUP:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_range_lookup, typep);\n    break;\n  case TS_CONFIG_HTTP_NORMALIZE_AE:\n    ret = _memberp_to_generic(&overridableHttpConfig->normalize_ae, typep);\n    break;\n  case TS_CONFIG_HTTP_DEFAULT_BUFFER_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->default_buffer_size_index, typep);\n    break;\n  case TS_CONFIG_HTTP_DEFAULT_BUFFER_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->default_buffer_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_REQUEST_HEADER_MAX_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->request_hdr_max_size, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_HEADER_MAX_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->response_hdr_max_size, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_revalidating_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_revalidating_lifetime, typep);\n    break;\n  case TS_CONFIG_SSL_HSTS_MAX_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_hsts_max_age, typep);\n    break;\n  case TS_CONFIG_SSL_HSTS_INCLUDE_SUBDOMAINS:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_hsts_include_subdomains, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_OPEN_READ_RETRY_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_open_read_retry_time, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_OPEN_READ_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->max_cache_open_read_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_RANGE_WRITE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_range_write, typep);\n    break;\n  case TS_CONFIG_HTTP_POST_CHECK_CONTENT_LENGTH_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->post_check_content_length_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_GLOBAL_USER_AGENT_HEADER:\n    ret = _memberp_to_generic(&overridableHttpConfig->global_user_agent_header, typep);\n    break;\n  case TS_CONFIG_HTTP_AUTH_SERVER_SESSION_PRIVATE:\n    ret = _memberp_to_generic(&overridableHttpConfig->auth_server_session_private, typep);\n    break;\n  case TS_CONFIG_HTTP_SLOW_LOG_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->slow_log_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GENERATION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_generation_number, typep);\n    break;\n  case TS_CONFIG_BODY_FACTORY_TEMPLATE_BASE:\n    ret = _memberp_to_generic(&overridableHttpConfig->body_factory_template_base, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_OPEN_WRITE_FAIL_ACTION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_open_write_fail_action, typep);\n    break;\n  case TS_CONFIG_HTTP_NUMBER_OF_REDIRECTIONS:\n    ret = _memberp_to_generic(&overridableHttpConfig->number_of_redirections, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_OPEN_WRITE_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->max_cache_open_write_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_REDIRECT_USE_ORIG_CACHE_KEY:\n    ret = _memberp_to_generic(&overridableHttpConfig->redirect_use_orig_cache_key, typep);\n    break;\n  case TS_CONFIG_HTTP_ATTACH_SERVER_SESSION_TO_CLIENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->attach_server_session_to_client, typep);\n    break;\n  case TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS_QUEUE:\n    ret = _memberp_to_generic(&overridableHttpConfig->origin_max_connections_queue, typep);\n    break;\n  case TS_CONFIG_WEBSOCKET_NO_ACTIVITY_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->websocket_inactive_timeout, typep);\n    break;\n  case TS_CONFIG_WEBSOCKET_ACTIVE_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->websocket_active_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_UNCACHEABLE_REQUESTS_BYPASS_PARENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->uncacheable_requests_bypass_parent, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_TOTAL_CONNECT_ATTEMPTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_connect_attempts, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_active_timeout_in, typep);\n    break;\n  case TS_CONFIG_SRV_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->srv_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_FORWARD_CONNECT_METHOD:\n    ret = _memberp_to_generic(&overridableHttpConfig->forward_connect_method, typep);\n    break;\n  case TS_CONFIG_SSL_CERT_FILENAME:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_cert_filename, typep);\n    break;\n  case TS_CONFIG_SSL_CERT_FILEPATH:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_cert_filepath, typep);\n    break;\n  case TS_CONFIG_PARENT_FAILURES_UPDATE_HOSTDB:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_failures_update_hostdb, typep);\n    break;\n  case TS_CONFIG_SSL_CLIENT_VERIFY_SERVER:\n    ret = _memberp_to_generic(&overridableHttpConfig->ssl_client_verify_server, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_ENABLE_DEFAULT_VARY_HEADER:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_enable_default_vary_headers, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_TEXT:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_text, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_IMAGES:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_images, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_OTHER:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_other, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_LANGUAGE_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_language_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_ENCODING_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_encoding_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_CHARSET_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_charset_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_FAIL_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_fail_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_RETRY_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_retry_time, typep);\n    break;\n  case TS_CONFIG_HTTP_PER_PARENT_CONNECT_ATTEMPTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->per_parent_connect_attempts, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_CONNECT_ATTEMPT_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_connect_timeout, typep);\n    break;\n  // This helps avoiding compiler warnings, yet detect unhandled enum members.\n  case TS_CONFIG_NULL:\n  case TS_CONFIG_LAST_ENTRY:\n    break;\n  }\n\n  return ret;\n}",
        "func": "static void *\n_conf_to_memberp(TSOverridableConfigKey conf, OverridableHttpConfigParams *overridableHttpConfig, std::type_info const *&typep)\n{\n  void *ret = nullptr;\n  typep     = &typeid(void);\n\n  switch (conf) {\n  case TS_CONFIG_URL_REMAP_PRISTINE_HOST_HDR:\n    ret = _memberp_to_generic(&overridableHttpConfig->maintain_pristine_host_hdr, typep);\n    break;\n  case TS_CONFIG_HTTP_CHUNKING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->chunking_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_CACHING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_caching_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_CACHING_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_caching_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_WHEN_TO_REVALIDATE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_when_to_revalidate, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_enabled_in, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_ENABLED_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_enabled_out, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_POST_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_post_out, typep);\n    break;\n  case TS_CONFIG_HTTP_SERVER_SESSION_SHARING_MATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->server_session_sharing_match, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_RECV_BUFFER_SIZE_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_recv_buffer_size_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_SEND_BUFFER_SIZE_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_send_buffer_size_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_OPTION_FLAG_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_option_flag_out, typep);\n    break;\n  case TS_CONFIG_HTTP_FORWARD_PROXY_AUTH_TO_PARENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->fwd_proxy_auth_to_parent, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_FROM:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_from, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_REFERER:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_referer, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_USER_AGENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_user_agent, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_COOKIE:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_cookie, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_REMOVE_CLIENT_IP:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_remove_client_ip, typep);\n    break;\n  case TS_CONFIG_HTTP_ANONYMIZE_INSERT_CLIENT_IP:\n    ret = _memberp_to_generic(&overridableHttpConfig->anonymize_insert_client_ip, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_SERVER_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_server_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_SQUID_X_FORWARDED_FOR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_squid_x_forwarded_for, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_FORWARDED:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_forwarded, typep);\n    break;\n  case TS_CONFIG_HTTP_SERVER_TCP_INIT_CWND:\n    ret = _memberp_to_generic(&overridableHttpConfig->server_tcp_init_cwnd, typep);\n    break;\n  case TS_CONFIG_HTTP_SEND_HTTP11_REQUESTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->send_http11_requests, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HTTP:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_http, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_client_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_CLIENT_CC_MAX_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_client_cc_max_age, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IMS_ON_CLIENT_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ims_on_client_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_SERVER_NO_CACHE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_server_no_cache, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_CACHE_RESPONSES_TO_COOKIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_responses_to_cookies, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_AUTHENTICATION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_ignore_auth, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_CACHE_URLS_THAT_LOOK_DYNAMIC:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_urls_that_look_dynamic, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_REQUIRED_HEADERS:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_required_headers, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_REQUEST_VIA_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_request_via_string, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_RESPONSE_VIA_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_response_via_string, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_MIN_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_min_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_MAX_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_max_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GUARANTEED_MIN_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_guaranteed_min_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GUARANTEED_MAX_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_guaranteed_max_lifetime, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_STALE_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_max_stale_age, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_no_activity_timeout_in, typep);\n    break;\n  case TS_CONFIG_HTTP_KEEP_ALIVE_NO_ACTIVITY_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->keep_alive_no_activity_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_no_activity_timeout_in, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_NO_ACTIVITY_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_no_activity_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_active_timeout_out, typep);\n    break;\n  case TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS:\n    ret = _memberp_to_generic(&overridableHttpConfig->origin_max_connections, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_max_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_MAX_RETRIES_DEAD_SERVER:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_max_retries_dead_server, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_RR_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_rr_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CONNECT_ATTEMPTS_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->connect_attempts_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_POST_CONNECT_ATTEMPTS_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->post_connect_attempts_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_DOWN_SERVER_CACHE_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->down_server_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_DOWN_SERVER_ABORT_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_abort_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_DOC_IN_CACHE_SKIP_DNS:\n    ret = _memberp_to_generic(&overridableHttpConfig->doc_in_cache_skip_dns, typep);\n    break;\n  case TS_CONFIG_HTTP_BACKGROUND_FILL_ACTIVE_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->background_fill_active_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_SERVER_STR:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_server_string, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_HEURISTIC_LM_FACTOR:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_heuristic_lm_factor, typep);\n    break;\n  case TS_CONFIG_HTTP_BACKGROUND_FILL_COMPLETED_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->background_fill_threshold, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_PACKET_MARK_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_packet_mark_out, typep);\n    break;\n  case TS_CONFIG_NET_SOCK_PACKET_TOS_OUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->sock_packet_tos_out, typep);\n    break;\n  case TS_CONFIG_HTTP_INSERT_AGE_IN_RESPONSE:\n    ret = _memberp_to_generic(&overridableHttpConfig->insert_age_in_response, typep);\n    break;\n  case TS_CONFIG_HTTP_CHUNKING_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->http_chunking_size, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_control_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_LOW_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_low_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_FLOW_CONTROL_HIGH_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->flow_high_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_RANGE_LOOKUP:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_range_lookup, typep);\n    break;\n  case TS_CONFIG_HTTP_NORMALIZE_AE:\n    ret = _memberp_to_generic(&overridableHttpConfig->normalize_ae, typep);\n    break;\n  case TS_CONFIG_HTTP_DEFAULT_BUFFER_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->default_buffer_size_index, typep);\n    break;\n  case TS_CONFIG_HTTP_DEFAULT_BUFFER_WATER_MARK:\n    ret = _memberp_to_generic(&overridableHttpConfig->default_buffer_water_mark, typep);\n    break;\n  case TS_CONFIG_HTTP_REQUEST_HEADER_MAX_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->request_hdr_max_size, typep);\n    break;\n  case TS_CONFIG_HTTP_RESPONSE_HEADER_MAX_SIZE:\n    ret = _memberp_to_generic(&overridableHttpConfig->response_hdr_max_size, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_revalidating_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_NEGATIVE_REVALIDATING_LIFETIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->negative_revalidating_lifetime, typep);\n    break;\n  case TS_CONFIG_SSL_HSTS_MAX_AGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_hsts_max_age, typep);\n    break;\n  case TS_CONFIG_SSL_HSTS_INCLUDE_SUBDOMAINS:\n    ret = _memberp_to_generic(&overridableHttpConfig->proxy_response_hsts_include_subdomains, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_OPEN_READ_RETRY_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_open_read_retry_time, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_OPEN_READ_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->max_cache_open_read_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_RANGE_WRITE:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_range_write, typep);\n    break;\n  case TS_CONFIG_HTTP_POST_CHECK_CONTENT_LENGTH_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->post_check_content_length_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_GLOBAL_USER_AGENT_HEADER:\n    ret = _memberp_to_generic(&overridableHttpConfig->global_user_agent_header, typep);\n    break;\n  case TS_CONFIG_HTTP_AUTH_SERVER_SESSION_PRIVATE:\n    ret = _memberp_to_generic(&overridableHttpConfig->auth_server_session_private, typep);\n    break;\n  case TS_CONFIG_HTTP_SLOW_LOG_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->slow_log_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_GENERATION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_generation_number, typep);\n    break;\n  case TS_CONFIG_BODY_FACTORY_TEMPLATE_BASE:\n    ret = _memberp_to_generic(&overridableHttpConfig->body_factory_template_base, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_OPEN_WRITE_FAIL_ACTION:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_open_write_fail_action, typep);\n    break;\n  case TS_CONFIG_HTTP_NUMBER_OF_REDIRECTIONS:\n    ret = _memberp_to_generic(&overridableHttpConfig->number_of_redirections, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_MAX_OPEN_WRITE_RETRIES:\n    ret = _memberp_to_generic(&overridableHttpConfig->max_cache_open_write_retries, typep);\n    break;\n  case TS_CONFIG_HTTP_REDIRECT_USE_ORIG_CACHE_KEY:\n    ret = _memberp_to_generic(&overridableHttpConfig->redirect_use_orig_cache_key, typep);\n    break;\n  case TS_CONFIG_HTTP_ATTACH_SERVER_SESSION_TO_CLIENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->attach_server_session_to_client, typep);\n    break;\n  case TS_CONFIG_HTTP_ORIGIN_MAX_CONNECTIONS_QUEUE:\n    ret = _memberp_to_generic(&overridableHttpConfig->origin_max_connections_queue, typep);\n    break;\n  case TS_CONFIG_WEBSOCKET_NO_ACTIVITY_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->websocket_inactive_timeout, typep);\n    break;\n  case TS_CONFIG_WEBSOCKET_ACTIVE_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->websocket_active_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_UNCACHEABLE_REQUESTS_BYPASS_PARENT:\n    ret = _memberp_to_generic(&overridableHttpConfig->uncacheable_requests_bypass_parent, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_TOTAL_CONNECT_ATTEMPTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_connect_attempts, typep);\n    break;\n  case TS_CONFIG_HTTP_TRANSACTION_ACTIVE_TIMEOUT_IN:\n    ret = _memberp_to_generic(&overridableHttpConfig->transaction_active_timeout_in, typep);\n    break;\n  case TS_CONFIG_SRV_ENABLED:\n    ret = _memberp_to_generic(&overridableHttpConfig->srv_enabled, typep);\n    break;\n  case TS_CONFIG_HTTP_FORWARD_CONNECT_METHOD:\n    ret = _memberp_to_generic(&overridableHttpConfig->forward_connect_method, typep);\n    break;\n  case TS_CONFIG_SSL_CERT_FILENAME:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_cert_filename, typep);\n    break;\n  case TS_CONFIG_SSL_CERT_FILEPATH:\n    ret = _memberp_to_generic(&overridableHttpConfig->client_cert_filepath, typep);\n    break;\n  case TS_CONFIG_PARENT_FAILURES_UPDATE_HOSTDB:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_failures_update_hostdb, typep);\n    break;\n  case TS_CONFIG_SSL_CLIENT_VERIFY_SERVER:\n    ret = _memberp_to_generic(&overridableHttpConfig->ssl_client_verify_server, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_ENABLE_DEFAULT_VARY_HEADER:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_enable_default_vary_headers, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_TEXT:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_text, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_IMAGES:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_images, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_VARY_DEFAULT_OTHER:\n    ret = _memberp_to_generic(&overridableHttpConfig->cache_vary_default_other, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_LANGUAGE_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_language_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_ENCODING_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_encoding_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_CACHE_IGNORE_ACCEPT_CHARSET_MISMATCH:\n    ret = _memberp_to_generic(&overridableHttpConfig->ignore_accept_charset_mismatch, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_FAIL_THRESHOLD:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_fail_threshold, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_PROXY_RETRY_TIME:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_retry_time, typep);\n    break;\n  case TS_CONFIG_HTTP_PER_PARENT_CONNECT_ATTEMPTS:\n    ret = _memberp_to_generic(&overridableHttpConfig->per_parent_connect_attempts, typep);\n    break;\n  case TS_CONFIG_HTTP_PARENT_CONNECT_ATTEMPT_TIMEOUT:\n    ret = _memberp_to_generic(&overridableHttpConfig->parent_connect_timeout, typep);\n    break;\n  case TS_CONFIG_HTTP_ALLOW_MULTI_RANGE:\n    ret = _memberp_to_generic(&overridableHttpConfig->allow_multi_range, typep);\n    break;\n  // This helps avoiding compiler warnings, yet detect unhandled enum members.\n  case TS_CONFIG_NULL:\n  case TS_CONFIG_LAST_ENTRY:\n    break;\n  }\n\n  return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -344,6 +344,9 @@\n   case TS_CONFIG_HTTP_PARENT_CONNECT_ATTEMPT_TIMEOUT:\n     ret = _memberp_to_generic(&overridableHttpConfig->parent_connect_timeout, typep);\n     break;\n+  case TS_CONFIG_HTTP_ALLOW_MULTI_RANGE:\n+    ret = _memberp_to_generic(&overridableHttpConfig->allow_multi_range, typep);\n+    break;\n   // This helps avoiding compiler warnings, yet detect unhandled enum members.\n   case TS_CONFIG_NULL:\n   case TS_CONFIG_LAST_ENTRY:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  case TS_CONFIG_HTTP_ALLOW_MULTI_RANGE:",
                "    ret = _memberp_to_generic(&overridableHttpConfig->allow_multi_range, typep);",
                "    break;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-17581",
        "func_name": "Exiv2/exiv2/CiffDirectory::readDirectory",
        "description": "CiffDirectory::readDirectory() at crwimage_int.cpp in Exiv2 0.26 has excessive stack consumption due to a recursive function, leading to Denial of service.",
        "git_url": "https://github.com/Exiv2/exiv2/commit/b3d077dcaefb6747fff8204490f33eba5a144edb",
        "commit_title": "Fix #460 by adding more checks in CiffDirectory::readDirectory",
        "commit_text": "",
        "func_before": "void CiffDirectory::readDirectory(const byte* pData,\n                                      uint32_t    size,\n                                      ByteOrder   byteOrder)\n    {\n        uint32_t o = getULong(pData + size - 4, byteOrder);\n        if (size < 2 || o > size-2) throw Error(kerNotACrwImage);\n        uint16_t count = getUShort(pData + o, byteOrder);\n#ifdef DEBUG\n        std::cout << \"Directory at offset \" << std::dec << o\n                  <<\", \" << count << \" entries \\n\";\n#endif\n        o += 2;\n        for (uint16_t i = 0; i < count; ++i) {\n            if (o + 10 > size) throw Error(kerNotACrwImage);\n            uint16_t tag = getUShort(pData + o, byteOrder);\n            CiffComponent::AutoPtr m;\n            switch (CiffComponent::typeId(tag)) {\n            case directory: m = CiffComponent::AutoPtr(new CiffDirectory); break;\n            default: m = CiffComponent::AutoPtr(new CiffEntry); break;\n            }\n            m->setDir(this->tag());\n            m->read(pData, size, o, byteOrder);\n            add(m);\n            o += 10;\n        }\n    }",
        "func": "void CiffDirectory::readDirectory(const byte* pData,\n                                      uint32_t    size,\n                                      ByteOrder   byteOrder)\n    {\n        if (size < 4)\n            throw Error(kerCorruptedMetadata);\n        uint32_t o = getULong(pData + size - 4, byteOrder);\n        if ( o+2 > size )\n            throw Error(kerCorruptedMetadata);\n        uint16_t count = getUShort(pData + o, byteOrder);\n#ifdef DEBUG\n        std::cout << \"Directory at offset \" << std::dec << o\n                  <<\", \" << count << \" entries \\n\";\n#endif\n        o += 2;\n        if ( (o + (count * 10)) > size )\n            throw Error(kerCorruptedMetadata);\n\n        for (uint16_t i = 0; i < count; ++i) {\n            uint16_t tag = getUShort(pData + o, byteOrder);\n            CiffComponent::AutoPtr m;\n            switch (CiffComponent::typeId(tag)) {\n            case directory: m = CiffComponent::AutoPtr(new CiffDirectory); break;\n            default: m = CiffComponent::AutoPtr(new CiffEntry); break;\n            }\n            m->setDir(this->tag());\n            m->read(pData, size, o, byteOrder);\n            add(m);\n            o += 10;\n        }\n    }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,16 +2,21 @@\n                                       uint32_t    size,\n                                       ByteOrder   byteOrder)\n     {\n+        if (size < 4)\n+            throw Error(kerCorruptedMetadata);\n         uint32_t o = getULong(pData + size - 4, byteOrder);\n-        if (size < 2 || o > size-2) throw Error(kerNotACrwImage);\n+        if ( o+2 > size )\n+            throw Error(kerCorruptedMetadata);\n         uint16_t count = getUShort(pData + o, byteOrder);\n #ifdef DEBUG\n         std::cout << \"Directory at offset \" << std::dec << o\n                   <<\", \" << count << \" entries \\n\";\n #endif\n         o += 2;\n+        if ( (o + (count * 10)) > size )\n+            throw Error(kerCorruptedMetadata);\n+\n         for (uint16_t i = 0; i < count; ++i) {\n-            if (o + 10 > size) throw Error(kerNotACrwImage);\n             uint16_t tag = getUShort(pData + o, byteOrder);\n             CiffComponent::AutoPtr m;\n             switch (CiffComponent::typeId(tag)) {",
        "diff_line_info": {
            "deleted_lines": [
                "        if (size < 2 || o > size-2) throw Error(kerNotACrwImage);",
                "            if (o + 10 > size) throw Error(kerNotACrwImage);"
            ],
            "added_lines": [
                "        if (size < 4)",
                "            throw Error(kerCorruptedMetadata);",
                "        if ( o+2 > size )",
                "            throw Error(kerCorruptedMetadata);",
                "        if ( (o + (count * 10)) > size )",
                "            throw Error(kerCorruptedMetadata);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19837",
        "func_name": "sass/libsass/Parser::parse_value",
        "description": "In LibSass prior to 3.5.5, Sass::Eval::operator()(Sass::Binary_Expression*) inside eval.cpp allows attackers to cause a denial-of-service resulting from stack consumption via a crafted sass file, because of certain incorrect parsing of '%' as a modulo operator in parser.cpp.",
        "git_url": "https://github.com/sass/libsass/commit/210fdff7a65370c2ae24e022a2b35da8c423cc5f",
        "commit_title": "Remove legacy workaround for parsing modulo operator",
        "commit_text": " Fixes #2659",
        "func_before": "Expression_Obj Parser::parse_value()\n  {\n    lex< css_comments >(false);\n    if (lex< ampersand >())\n    {\n      if (match< ampersand >()) {\n        warning(\"In Sass, \\\"&&\\\" means two copies of the parent selector. You probably want to use \\\"and\\\" instead.\", pstate);\n      }\n      return SASS_MEMORY_NEW(Parent_Selector, pstate); }\n\n    if (lex< kwd_important >())\n    { return SASS_MEMORY_NEW(String_Constant, pstate, \"!important\"); }\n\n    // parse `10%4px` into separated items and not a schema\n    if (lex< sequence < percentage, lookahead < number > > >())\n    { return lexed_percentage(lexed); }\n\n    if (lex< sequence < number, lookahead< sequence < op, number > > > >())\n    { return lexed_number(lexed); }\n\n    // string may be interpolated\n    if (lex< sequence < quoted_string, lookahead < exactly <'-'> > > >())\n    { return parse_string(); }\n\n    if (const char* stop = peek< value_schema >())\n    { return parse_value_schema(stop); }\n\n    // string may be interpolated\n    if (lex< quoted_string >())\n    { return parse_string(); }\n\n    if (lex< kwd_true >())\n    { return SASS_MEMORY_NEW(Boolean, pstate, true); }\n\n    if (lex< kwd_false >())\n    { return SASS_MEMORY_NEW(Boolean, pstate, false); }\n\n    if (lex< kwd_null >())\n    { return SASS_MEMORY_NEW(Null, pstate); }\n\n    if (lex< identifier >()) {\n      return color_or_string(lexed);\n    }\n\n    if (lex< percentage >())\n    { return lexed_percentage(lexed); }\n\n    // match hex number first because 0x000 looks like a number followed by an identifier\n    if (lex< sequence < alternatives< hex, hex0 >, negate < exactly<'-'> > > >())\n    { return lexed_hex_color(lexed); }\n\n    if (lex< hexa >())\n    {\n      std::string s = lexed.to_string();\n\n      deprecated(\n        \"The value \\\"\"+s+\"\\\" is currently parsed as a string, but it will be parsed as a color in\",\n        \"future versions of Sass. Use \\\"unquote('\"+s+\"')\\\" to continue parsing it as a string.\",\n        true, pstate\n      );\n\n      return SASS_MEMORY_NEW(String_Quoted, pstate, lexed);\n    }\n\n    if (lex< sequence < exactly <'#'>, identifier > >())\n    { return SASS_MEMORY_NEW(String_Quoted, pstate, lexed); }\n\n    // also handle the 10em- foo special case\n    // alternatives < exactly < '.' >, .. > -- `1.5em-.75em` is split into a list, not a binary expression\n    if (lex< sequence< dimension, optional< sequence< exactly<'-'>, lookahead< alternatives < space > > > > > >())\n    { return lexed_dimension(lexed); }\n\n    if (lex< sequence< static_component, one_plus< strict_identifier > > >())\n    { return SASS_MEMORY_NEW(String_Constant, pstate, lexed); }\n\n    if (lex< number >())\n    { return lexed_number(lexed); }\n\n    if (lex< variable >())\n    { return SASS_MEMORY_NEW(Variable, pstate, Util::normalize_underscores(lexed)); }\n\n    // Special case handling for `%` proceeding an interpolant.\n    if (lex< sequence< exactly<'%'>, optional< percentage > > >())\n    { return SASS_MEMORY_NEW(String_Constant, pstate, lexed); }\n\n    css_error(\"Invalid CSS\", \" after \", \": expected expression (e.g. 1px, bold), was \");\n\n    // unreachable statement\n    return 0;\n  }",
        "func": "Expression_Obj Parser::parse_value()\n  {\n    lex< css_comments >(false);\n    if (lex< ampersand >())\n    {\n      if (match< ampersand >()) {\n        warning(\"In Sass, \\\"&&\\\" means two copies of the parent selector. You probably want to use \\\"and\\\" instead.\", pstate);\n      }\n      return SASS_MEMORY_NEW(Parent_Selector, pstate); }\n\n    if (lex< kwd_important >())\n    { return SASS_MEMORY_NEW(String_Constant, pstate, \"!important\"); }\n\n    // parse `10%4px` into separated items and not a schema\n    if (lex< sequence < percentage, lookahead < number > > >())\n    { return lexed_percentage(lexed); }\n\n    if (lex< sequence < number, lookahead< sequence < op, number > > > >())\n    { return lexed_number(lexed); }\n\n    // string may be interpolated\n    if (lex< sequence < quoted_string, lookahead < exactly <'-'> > > >())\n    { return parse_string(); }\n\n    if (const char* stop = peek< value_schema >())\n    { return parse_value_schema(stop); }\n\n    // string may be interpolated\n    if (lex< quoted_string >())\n    { return parse_string(); }\n\n    if (lex< kwd_true >())\n    { return SASS_MEMORY_NEW(Boolean, pstate, true); }\n\n    if (lex< kwd_false >())\n    { return SASS_MEMORY_NEW(Boolean, pstate, false); }\n\n    if (lex< kwd_null >())\n    { return SASS_MEMORY_NEW(Null, pstate); }\n\n    if (lex< identifier >()) {\n      return color_or_string(lexed);\n    }\n\n    if (lex< percentage >())\n    { return lexed_percentage(lexed); }\n\n    // match hex number first because 0x000 looks like a number followed by an identifier\n    if (lex< sequence < alternatives< hex, hex0 >, negate < exactly<'-'> > > >())\n    { return lexed_hex_color(lexed); }\n\n    if (lex< hexa >())\n    {\n      std::string s = lexed.to_string();\n\n      deprecated(\n        \"The value \\\"\"+s+\"\\\" is currently parsed as a string, but it will be parsed as a color in\",\n        \"future versions of Sass. Use \\\"unquote('\"+s+\"')\\\" to continue parsing it as a string.\",\n        true, pstate\n      );\n\n      return SASS_MEMORY_NEW(String_Quoted, pstate, lexed);\n    }\n\n    if (lex< sequence < exactly <'#'>, identifier > >())\n    { return SASS_MEMORY_NEW(String_Quoted, pstate, lexed); }\n\n    // also handle the 10em- foo special case\n    // alternatives < exactly < '.' >, .. > -- `1.5em-.75em` is split into a list, not a binary expression\n    if (lex< sequence< dimension, optional< sequence< exactly<'-'>, lookahead< alternatives < space > > > > > >())\n    { return lexed_dimension(lexed); }\n\n    if (lex< sequence< static_component, one_plus< strict_identifier > > >())\n    { return SASS_MEMORY_NEW(String_Constant, pstate, lexed); }\n\n    if (lex< number >())\n    { return lexed_number(lexed); }\n\n    if (lex< variable >())\n    { return SASS_MEMORY_NEW(Variable, pstate, Util::normalize_underscores(lexed)); }\n\n    css_error(\"Invalid CSS\", \" after \", \": expected expression (e.g. 1px, bold), was \");\n\n    // unreachable statement\n    return 0;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -79,10 +79,6 @@\n     if (lex< variable >())\n     { return SASS_MEMORY_NEW(Variable, pstate, Util::normalize_underscores(lexed)); }\n \n-    // Special case handling for `%` proceeding an interpolant.\n-    if (lex< sequence< exactly<'%'>, optional< percentage > > >())\n-    { return SASS_MEMORY_NEW(String_Constant, pstate, lexed); }\n-\n     css_error(\"Invalid CSS\", \" after \", \": expected expression (e.g. 1px, bold), was \");\n \n     // unreachable statement",
        "diff_line_info": {
            "deleted_lines": [
                "    // Special case handling for `%` proceeding an interpolant.",
                "    if (lex< sequence< exactly<'%'>, optional< percentage > > >())",
                "    { return SASS_MEMORY_NEW(String_Constant, pstate, lexed); }",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2018-20169",
        "func_name": "torvalds/linux/__usb_get_extra_descriptor",
        "description": "An issue was discovered in the Linux kernel before 4.19.9. The USB subsystem mishandles size checks during the reading of an extra descriptor, related to __usb_get_extra_descriptor in drivers/usb/core/usb.c.",
        "git_url": "https://github.com/torvalds/linux/commit/704620afc70cf47abb9d6a1a57f3825d2bca49cf",
        "commit_title": "USB: check usb_get_extra_descriptor for proper size",
        "commit_text": " When reading an extra descriptor, we need to properly check the minimum and maximum size allowed, to prevent from invalid data being sent by a device.  Co-developed-by: Linus Torvalds <torvalds@linux-foundation.org> Cc: stable <stable@kernel.org>",
        "func_before": "int __usb_get_extra_descriptor(char *buffer, unsigned size,\n\t\t\t       unsigned char type, void **ptr)\n{\n\tstruct usb_descriptor_header *header;\n\n\twhile (size >= sizeof(struct usb_descriptor_header)) {\n\t\theader = (struct usb_descriptor_header *)buffer;\n\n\t\tif (header->bLength < 2) {\n\t\t\tprintk(KERN_ERR\n\t\t\t\t\"%s: bogus descriptor, type %d length %d\\n\",\n\t\t\t\tusbcore_name,\n\t\t\t\theader->bDescriptorType,\n\t\t\t\theader->bLength);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (header->bDescriptorType == type) {\n\t\t\t*ptr = header;\n\t\t\treturn 0;\n\t\t}\n\n\t\tbuffer += header->bLength;\n\t\tsize -= header->bLength;\n\t}\n\treturn -1;\n}",
        "func": "int __usb_get_extra_descriptor(char *buffer, unsigned size,\n\t\t\t       unsigned char type, void **ptr, size_t minsize)\n{\n\tstruct usb_descriptor_header *header;\n\n\twhile (size >= sizeof(struct usb_descriptor_header)) {\n\t\theader = (struct usb_descriptor_header *)buffer;\n\n\t\tif (header->bLength < 2 || header->bLength > size) {\n\t\t\tprintk(KERN_ERR\n\t\t\t\t\"%s: bogus descriptor, type %d length %d\\n\",\n\t\t\t\tusbcore_name,\n\t\t\t\theader->bDescriptorType,\n\t\t\t\theader->bLength);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (header->bDescriptorType == type && header->bLength >= minsize) {\n\t\t\t*ptr = header;\n\t\t\treturn 0;\n\t\t}\n\n\t\tbuffer += header->bLength;\n\t\tsize -= header->bLength;\n\t}\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,12 @@\n int __usb_get_extra_descriptor(char *buffer, unsigned size,\n-\t\t\t       unsigned char type, void **ptr)\n+\t\t\t       unsigned char type, void **ptr, size_t minsize)\n {\n \tstruct usb_descriptor_header *header;\n \n \twhile (size >= sizeof(struct usb_descriptor_header)) {\n \t\theader = (struct usb_descriptor_header *)buffer;\n \n-\t\tif (header->bLength < 2) {\n+\t\tif (header->bLength < 2 || header->bLength > size) {\n \t\t\tprintk(KERN_ERR\n \t\t\t\t\"%s: bogus descriptor, type %d length %d\\n\",\n \t\t\t\tusbcore_name,\n@@ -15,7 +15,7 @@\n \t\t\treturn -1;\n \t\t}\n \n-\t\tif (header->bDescriptorType == type) {\n+\t\tif (header->bDescriptorType == type && header->bLength >= minsize) {\n \t\t\t*ptr = header;\n \t\t\treturn 0;\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t       unsigned char type, void **ptr)",
                "\t\tif (header->bLength < 2) {",
                "\t\tif (header->bDescriptorType == type) {"
            ],
            "added_lines": [
                "\t\t\t       unsigned char type, void **ptr, size_t minsize)",
                "\t\tif (header->bLength < 2 || header->bLength > size) {",
                "\t\tif (header->bDescriptorType == type && header->bLength >= minsize) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20169",
        "func_name": "torvalds/linux/usb_enumerate_device_otg",
        "description": "An issue was discovered in the Linux kernel before 4.19.9. The USB subsystem mishandles size checks during the reading of an extra descriptor, related to __usb_get_extra_descriptor in drivers/usb/core/usb.c.",
        "git_url": "https://github.com/torvalds/linux/commit/704620afc70cf47abb9d6a1a57f3825d2bca49cf",
        "commit_title": "USB: check usb_get_extra_descriptor for proper size",
        "commit_text": " When reading an extra descriptor, we need to properly check the minimum and maximum size allowed, to prevent from invalid data being sent by a device.  Co-developed-by: Linus Torvalds <torvalds@linux-foundation.org> Cc: stable <stable@kernel.org>",
        "func_before": "static int usb_enumerate_device_otg(struct usb_device *udev)\n{\n\tint err = 0;\n\n#ifdef\tCONFIG_USB_OTG\n\t/*\n\t * OTG-aware devices on OTG-capable root hubs may be able to use SRP,\n\t * to wake us after we've powered off VBUS; and HNP, switching roles\n\t * \"host\" to \"peripheral\".  The OTG descriptor helps figure this out.\n\t */\n\tif (!udev->bus->is_b_host\n\t\t\t&& udev->config\n\t\t\t&& udev->parent == udev->bus->root_hub) {\n\t\tstruct usb_otg_descriptor\t*desc = NULL;\n\t\tstruct usb_bus\t\t\t*bus = udev->bus;\n\t\tunsigned\t\t\tport1 = udev->portnum;\n\n\t\t/* descriptor may appear anywhere in config */\n\t\terr = __usb_get_extra_descriptor(udev->rawdescriptors[0],\n\t\t\t\tle16_to_cpu(udev->config[0].desc.wTotalLength),\n\t\t\t\tUSB_DT_OTG, (void **) &desc);\n\t\tif (err || !(desc->bmAttributes & USB_OTG_HNP))\n\t\t\treturn 0;\n\n\t\tdev_info(&udev->dev, \"Dual-Role OTG device on %sHNP port\\n\",\n\t\t\t\t\t(port1 == bus->otg_port) ? \"\" : \"non-\");\n\n\t\t/* enable HNP before suspend, it's simpler */\n\t\tif (port1 == bus->otg_port) {\n\t\t\tbus->b_hnp_enable = 1;\n\t\t\terr = usb_control_msg(udev,\n\t\t\t\tusb_sndctrlpipe(udev, 0),\n\t\t\t\tUSB_REQ_SET_FEATURE, 0,\n\t\t\t\tUSB_DEVICE_B_HNP_ENABLE,\n\t\t\t\t0, NULL, 0,\n\t\t\t\tUSB_CTRL_SET_TIMEOUT);\n\t\t\tif (err < 0) {\n\t\t\t\t/*\n\t\t\t\t * OTG MESSAGE: report errors here,\n\t\t\t\t * customize to match your product.\n\t\t\t\t */\n\t\t\t\tdev_err(&udev->dev, \"can't set HNP mode: %d\\n\",\n\t\t\t\t\t\t\t\t\terr);\n\t\t\t\tbus->b_hnp_enable = 0;\n\t\t\t}\n\t\t} else if (desc->bLength == sizeof\n\t\t\t\t(struct usb_otg_descriptor)) {\n\t\t\t/* Set a_alt_hnp_support for legacy otg device */\n\t\t\terr = usb_control_msg(udev,\n\t\t\t\tusb_sndctrlpipe(udev, 0),\n\t\t\t\tUSB_REQ_SET_FEATURE, 0,\n\t\t\t\tUSB_DEVICE_A_ALT_HNP_SUPPORT,\n\t\t\t\t0, NULL, 0,\n\t\t\t\tUSB_CTRL_SET_TIMEOUT);\n\t\t\tif (err < 0)\n\t\t\t\tdev_err(&udev->dev,\n\t\t\t\t\t\"set a_alt_hnp_support failed: %d\\n\",\n\t\t\t\t\terr);\n\t\t}\n\t}\n#endif\n\treturn err;\n}",
        "func": "static int usb_enumerate_device_otg(struct usb_device *udev)\n{\n\tint err = 0;\n\n#ifdef\tCONFIG_USB_OTG\n\t/*\n\t * OTG-aware devices on OTG-capable root hubs may be able to use SRP,\n\t * to wake us after we've powered off VBUS; and HNP, switching roles\n\t * \"host\" to \"peripheral\".  The OTG descriptor helps figure this out.\n\t */\n\tif (!udev->bus->is_b_host\n\t\t\t&& udev->config\n\t\t\t&& udev->parent == udev->bus->root_hub) {\n\t\tstruct usb_otg_descriptor\t*desc = NULL;\n\t\tstruct usb_bus\t\t\t*bus = udev->bus;\n\t\tunsigned\t\t\tport1 = udev->portnum;\n\n\t\t/* descriptor may appear anywhere in config */\n\t\terr = __usb_get_extra_descriptor(udev->rawdescriptors[0],\n\t\t\t\tle16_to_cpu(udev->config[0].desc.wTotalLength),\n\t\t\t\tUSB_DT_OTG, (void **) &desc, sizeof(*desc));\n\t\tif (err || !(desc->bmAttributes & USB_OTG_HNP))\n\t\t\treturn 0;\n\n\t\tdev_info(&udev->dev, \"Dual-Role OTG device on %sHNP port\\n\",\n\t\t\t\t\t(port1 == bus->otg_port) ? \"\" : \"non-\");\n\n\t\t/* enable HNP before suspend, it's simpler */\n\t\tif (port1 == bus->otg_port) {\n\t\t\tbus->b_hnp_enable = 1;\n\t\t\terr = usb_control_msg(udev,\n\t\t\t\tusb_sndctrlpipe(udev, 0),\n\t\t\t\tUSB_REQ_SET_FEATURE, 0,\n\t\t\t\tUSB_DEVICE_B_HNP_ENABLE,\n\t\t\t\t0, NULL, 0,\n\t\t\t\tUSB_CTRL_SET_TIMEOUT);\n\t\t\tif (err < 0) {\n\t\t\t\t/*\n\t\t\t\t * OTG MESSAGE: report errors here,\n\t\t\t\t * customize to match your product.\n\t\t\t\t */\n\t\t\t\tdev_err(&udev->dev, \"can't set HNP mode: %d\\n\",\n\t\t\t\t\t\t\t\t\terr);\n\t\t\t\tbus->b_hnp_enable = 0;\n\t\t\t}\n\t\t} else if (desc->bLength == sizeof\n\t\t\t\t(struct usb_otg_descriptor)) {\n\t\t\t/* Set a_alt_hnp_support for legacy otg device */\n\t\t\terr = usb_control_msg(udev,\n\t\t\t\tusb_sndctrlpipe(udev, 0),\n\t\t\t\tUSB_REQ_SET_FEATURE, 0,\n\t\t\t\tUSB_DEVICE_A_ALT_HNP_SUPPORT,\n\t\t\t\t0, NULL, 0,\n\t\t\t\tUSB_CTRL_SET_TIMEOUT);\n\t\t\tif (err < 0)\n\t\t\t\tdev_err(&udev->dev,\n\t\t\t\t\t\"set a_alt_hnp_support failed: %d\\n\",\n\t\t\t\t\terr);\n\t\t}\n\t}\n#endif\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,7 +18,7 @@\n \t\t/* descriptor may appear anywhere in config */\n \t\terr = __usb_get_extra_descriptor(udev->rawdescriptors[0],\n \t\t\t\tle16_to_cpu(udev->config[0].desc.wTotalLength),\n-\t\t\t\tUSB_DT_OTG, (void **) &desc);\n+\t\t\t\tUSB_DT_OTG, (void **) &desc, sizeof(*desc));\n \t\tif (err || !(desc->bmAttributes & USB_OTG_HNP))\n \t\t\treturn 0;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tUSB_DT_OTG, (void **) &desc);"
            ],
            "added_lines": [
                "\t\t\t\tUSB_DT_OTG, (void **) &desc, sizeof(*desc));"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20169",
        "func_name": "torvalds/linux/hwahc_security_create",
        "description": "An issue was discovered in the Linux kernel before 4.19.9. The USB subsystem mishandles size checks during the reading of an extra descriptor, related to __usb_get_extra_descriptor in drivers/usb/core/usb.c.",
        "git_url": "https://github.com/torvalds/linux/commit/704620afc70cf47abb9d6a1a57f3825d2bca49cf",
        "commit_title": "USB: check usb_get_extra_descriptor for proper size",
        "commit_text": " When reading an extra descriptor, we need to properly check the minimum and maximum size allowed, to prevent from invalid data being sent by a device.  Co-developed-by: Linus Torvalds <torvalds@linux-foundation.org> Cc: stable <stable@kernel.org>",
        "func_before": "static int hwahc_security_create(struct hwahc *hwahc)\n{\n\tint result;\n\tstruct wusbhc *wusbhc = &hwahc->wusbhc;\n\tstruct usb_device *usb_dev = hwahc->wa.usb_dev;\n\tstruct device *dev = &usb_dev->dev;\n\tstruct usb_security_descriptor *secd;\n\tstruct usb_encryption_descriptor *etd;\n\tvoid *itr, *top;\n\tsize_t itr_size, needed, bytes;\n\tu8 index;\n\tchar buf[64];\n\n\t/* Find the host's security descriptors in the config descr bundle */\n\tindex = (usb_dev->actconfig - usb_dev->config) /\n\t\tsizeof(usb_dev->config[0]);\n\titr = usb_dev->rawdescriptors[index];\n\titr_size = le16_to_cpu(usb_dev->actconfig->desc.wTotalLength);\n\ttop = itr + itr_size;\n\tresult = __usb_get_extra_descriptor(usb_dev->rawdescriptors[index],\n\t\t\tle16_to_cpu(usb_dev->actconfig->desc.wTotalLength),\n\t\t\tUSB_DT_SECURITY, (void **) &secd);\n\tif (result == -1) {\n\t\tdev_warn(dev, \"BUG? WUSB host has no security descriptors\\n\");\n\t\treturn 0;\n\t}\n\tneeded = sizeof(*secd);\n\tif (top - (void *)secd < needed) {\n\t\tdev_err(dev, \"BUG? Not enough data to process security \"\n\t\t\t\"descriptor header (%zu bytes left vs %zu needed)\\n\",\n\t\t\ttop - (void *) secd, needed);\n\t\treturn 0;\n\t}\n\tneeded = le16_to_cpu(secd->wTotalLength);\n\tif (top - (void *)secd < needed) {\n\t\tdev_err(dev, \"BUG? Not enough data to process security \"\n\t\t\t\"descriptors (%zu bytes left vs %zu needed)\\n\",\n\t\t\ttop - (void *) secd, needed);\n\t\treturn 0;\n\t}\n\t/* Walk over the sec descriptors and store CCM1's on wusbhc */\n\titr = (void *) secd + sizeof(*secd);\n\ttop = (void *) secd + le16_to_cpu(secd->wTotalLength);\n\tindex = 0;\n\tbytes = 0;\n\twhile (itr < top) {\n\t\tetd = itr;\n\t\tif (top - itr < sizeof(*etd)) {\n\t\t\tdev_err(dev, \"BUG: bad host security descriptor; \"\n\t\t\t\t\"not enough data (%zu vs %zu left)\\n\",\n\t\t\t\ttop - itr, sizeof(*etd));\n\t\t\tbreak;\n\t\t}\n\t\tif (etd->bLength < sizeof(*etd)) {\n\t\t\tdev_err(dev, \"BUG: bad host encryption descriptor; \"\n\t\t\t\t\"descriptor is too short \"\n\t\t\t\t\"(%zu vs %zu needed)\\n\",\n\t\t\t\t(size_t)etd->bLength, sizeof(*etd));\n\t\t\tbreak;\n\t\t}\n\t\titr += etd->bLength;\n\t\tbytes += snprintf(buf + bytes, sizeof(buf) - bytes,\n\t\t\t\t  \"%s (0x%02x) \",\n\t\t\t\t  wusb_et_name(etd->bEncryptionType),\n\t\t\t\t  etd->bEncryptionValue);\n\t\twusbhc->ccm1_etd = etd;\n\t}\n\tdev_info(dev, \"supported encryption types: %s\\n\", buf);\n\tif (wusbhc->ccm1_etd == NULL) {\n\t\tdev_err(dev, \"E: host doesn't support CCM-1 crypto\\n\");\n\t\treturn 0;\n\t}\n\t/* Pretty print what we support */\n\treturn 0;\n}",
        "func": "static int hwahc_security_create(struct hwahc *hwahc)\n{\n\tint result;\n\tstruct wusbhc *wusbhc = &hwahc->wusbhc;\n\tstruct usb_device *usb_dev = hwahc->wa.usb_dev;\n\tstruct device *dev = &usb_dev->dev;\n\tstruct usb_security_descriptor *secd;\n\tstruct usb_encryption_descriptor *etd;\n\tvoid *itr, *top;\n\tsize_t itr_size, needed, bytes;\n\tu8 index;\n\tchar buf[64];\n\n\t/* Find the host's security descriptors in the config descr bundle */\n\tindex = (usb_dev->actconfig - usb_dev->config) /\n\t\tsizeof(usb_dev->config[0]);\n\titr = usb_dev->rawdescriptors[index];\n\titr_size = le16_to_cpu(usb_dev->actconfig->desc.wTotalLength);\n\ttop = itr + itr_size;\n\tresult = __usb_get_extra_descriptor(usb_dev->rawdescriptors[index],\n\t\t\tle16_to_cpu(usb_dev->actconfig->desc.wTotalLength),\n\t\t\tUSB_DT_SECURITY, (void **) &secd, sizeof(*secd));\n\tif (result == -1) {\n\t\tdev_warn(dev, \"BUG? WUSB host has no security descriptors\\n\");\n\t\treturn 0;\n\t}\n\tneeded = sizeof(*secd);\n\tif (top - (void *)secd < needed) {\n\t\tdev_err(dev, \"BUG? Not enough data to process security \"\n\t\t\t\"descriptor header (%zu bytes left vs %zu needed)\\n\",\n\t\t\ttop - (void *) secd, needed);\n\t\treturn 0;\n\t}\n\tneeded = le16_to_cpu(secd->wTotalLength);\n\tif (top - (void *)secd < needed) {\n\t\tdev_err(dev, \"BUG? Not enough data to process security \"\n\t\t\t\"descriptors (%zu bytes left vs %zu needed)\\n\",\n\t\t\ttop - (void *) secd, needed);\n\t\treturn 0;\n\t}\n\t/* Walk over the sec descriptors and store CCM1's on wusbhc */\n\titr = (void *) secd + sizeof(*secd);\n\ttop = (void *) secd + le16_to_cpu(secd->wTotalLength);\n\tindex = 0;\n\tbytes = 0;\n\twhile (itr < top) {\n\t\tetd = itr;\n\t\tif (top - itr < sizeof(*etd)) {\n\t\t\tdev_err(dev, \"BUG: bad host security descriptor; \"\n\t\t\t\t\"not enough data (%zu vs %zu left)\\n\",\n\t\t\t\ttop - itr, sizeof(*etd));\n\t\t\tbreak;\n\t\t}\n\t\tif (etd->bLength < sizeof(*etd)) {\n\t\t\tdev_err(dev, \"BUG: bad host encryption descriptor; \"\n\t\t\t\t\"descriptor is too short \"\n\t\t\t\t\"(%zu vs %zu needed)\\n\",\n\t\t\t\t(size_t)etd->bLength, sizeof(*etd));\n\t\t\tbreak;\n\t\t}\n\t\titr += etd->bLength;\n\t\tbytes += snprintf(buf + bytes, sizeof(buf) - bytes,\n\t\t\t\t  \"%s (0x%02x) \",\n\t\t\t\t  wusb_et_name(etd->bEncryptionType),\n\t\t\t\t  etd->bEncryptionValue);\n\t\twusbhc->ccm1_etd = etd;\n\t}\n\tdev_info(dev, \"supported encryption types: %s\\n\", buf);\n\tif (wusbhc->ccm1_etd == NULL) {\n\t\tdev_err(dev, \"E: host doesn't support CCM-1 crypto\\n\");\n\t\treturn 0;\n\t}\n\t/* Pretty print what we support */\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,7 +19,7 @@\n \ttop = itr + itr_size;\n \tresult = __usb_get_extra_descriptor(usb_dev->rawdescriptors[index],\n \t\t\tle16_to_cpu(usb_dev->actconfig->desc.wTotalLength),\n-\t\t\tUSB_DT_SECURITY, (void **) &secd);\n+\t\t\tUSB_DT_SECURITY, (void **) &secd, sizeof(*secd));\n \tif (result == -1) {\n \t\tdev_warn(dev, \"BUG? WUSB host has no security descriptors\\n\");\n \t\treturn 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tUSB_DT_SECURITY, (void **) &secd);"
            ],
            "added_lines": [
                "\t\t\tUSB_DT_SECURITY, (void **) &secd, sizeof(*secd));"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-20186",
        "func_name": "axiomatic-systems/Bento4/AP4_Sample::ReadData",
        "description": "An issue was discovered in Bento4 1.5.1-627. AP4_Sample::ReadData in Core/Ap4Sample.cpp allows attackers to trigger an attempted excessive memory allocation, related to AP4_DataBuffer::SetDataSize and AP4_DataBuffer::ReallocateBuffer in Core/Ap4DataBuffer.cpp.",
        "git_url": "https://github.com/axiomatic-systems/Bento4/commit/f7ba0dca622837bfab063e500f6013640e8e76e4",
        "commit_title": "fix #342",
        "commit_text": "",
        "func_before": "AP4_Result\nAP4_Sample::ReadData(AP4_DataBuffer& data, AP4_Size size, AP4_Size offset)\n{\n    // check that we have a stream\n    if (m_DataStream == NULL) return AP4_FAILURE;\n\n    // shortcut\n    if (size == 0) return AP4_SUCCESS;\n\n    // check the size\n    if (m_Size < size+offset) return AP4_FAILURE;\n\n    // set the buffer size\n    AP4_Result result = data.SetDataSize(size);\n    if (AP4_FAILED(result)) return result;\n\n    // get the data from the stream\n    result = m_DataStream->Seek(m_Offset+offset);\n    if (AP4_FAILED(result)) return result;\n    return m_DataStream->Read(data.UseData(), size);\n}",
        "func": "AP4_Result\nAP4_Sample::ReadData(AP4_DataBuffer& data, AP4_Size size, AP4_Size offset)\n{\n    // check that we have a stream\n    if (m_DataStream == NULL) return AP4_FAILURE;\n\n    // shortcut\n    if (size == 0) return AP4_SUCCESS;\n\n    // check the size\n    if (m_Size < size+offset) return AP4_FAILURE;\n\n    // check if there's enough data in the stream\n    AP4_LargeSize stream_size = 0;\n    AP4_Result result = m_DataStream->GetSize(stream_size);\n    if (AP4_SUCCEEDED(result)) {\n        if (size + offset > stream_size) {\n            return AP4_ERROR_OUT_OF_RANGE;\n        }\n    }\n    \n    // set the buffer size\n    result = data.SetDataSize(size);\n    if (AP4_FAILED(result)) return result;\n\n    // get the data from the stream\n    result = m_DataStream->Seek(m_Offset+offset);\n    if (AP4_FAILED(result)) return result;\n    return m_DataStream->Read(data.UseData(), size);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,17 @@\n     // check the size\n     if (m_Size < size+offset) return AP4_FAILURE;\n \n+    // check if there's enough data in the stream\n+    AP4_LargeSize stream_size = 0;\n+    AP4_Result result = m_DataStream->GetSize(stream_size);\n+    if (AP4_SUCCEEDED(result)) {\n+        if (size + offset > stream_size) {\n+            return AP4_ERROR_OUT_OF_RANGE;\n+        }\n+    }\n+    \n     // set the buffer size\n-    AP4_Result result = data.SetDataSize(size);\n+    result = data.SetDataSize(size);\n     if (AP4_FAILED(result)) return result;\n \n     // get the data from the stream",
        "diff_line_info": {
            "deleted_lines": [
                "    AP4_Result result = data.SetDataSize(size);"
            ],
            "added_lines": [
                "    // check if there's enough data in the stream",
                "    AP4_LargeSize stream_size = 0;",
                "    AP4_Result result = m_DataStream->GetSize(stream_size);",
                "    if (AP4_SUCCEEDED(result)) {",
                "        if (size + offset > stream_size) {",
                "            return AP4_ERROR_OUT_OF_RANGE;",
                "        }",
                "    }",
                "    ",
                "    result = data.SetDataSize(size);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-29499",
        "func_name": "GNOME/glib/gvs_variable_sized_array_get_frame_offsets",
        "description": "A flaw was found in GLib. GVariant deserialization fails to validate that the input conforms to the expected format, leading to denial of service.",
        "git_url": "https://github.com/GNOME/glib/commit/5f4485c4ff57fdefb1661531788def7ca5a47328",
        "commit_title": "gvariant-serialiser: Check offset table entry size is minimal",
        "commit_text": " The entries in an offset table (which is used for variable sized arrays and tuples containing variable sized members) are sized so that they can address every byte in the overall variant.  The specification requires that for a variant to be in normal form, its offset table entries must be the minimum width such that they can address every byte in the variant.  That minimality requirement was not checked in `g_variant_is_normal_form()`, leading to two different byte arrays being interpreted as the normal form of a given variant tree. That kind of confusion could potentially be exploited, and is certainly a bug.  Fix it by adding the necessary checks on offset table entry width, and unit tests.  Spotted by William Manley.  ",
        "func_before": "static struct Offsets\ngvs_variable_sized_array_get_frame_offsets (GVariantSerialised value)\n{\n  struct Offsets out = { 0, };\n  gsize offsets_array_size;\n  gsize last_end;\n\n  if (value.size == 0)\n    {\n      out.is_normal = TRUE;\n      return out;\n    }\n\n  out.offset_size = gvs_get_offset_size (value.size);\n  last_end = gvs_read_unaligned_le (value.data + value.size - out.offset_size,\n                                    out.offset_size);\n\n  if (last_end > value.size)\n    return out;  /* offsets not normal */\n\n  offsets_array_size = value.size - last_end;\n\n  if (offsets_array_size % out.offset_size)\n    return out;  /* offsets not normal */\n\n  out.data_size = last_end;\n  out.array = value.data + last_end;\n  out.length = offsets_array_size / out.offset_size;\n  out.is_normal = TRUE;\n\n  return out;\n}",
        "func": "static struct Offsets\ngvs_variable_sized_array_get_frame_offsets (GVariantSerialised value)\n{\n  struct Offsets out = { 0, };\n  gsize offsets_array_size;\n  gsize last_end;\n\n  if (value.size == 0)\n    {\n      out.is_normal = TRUE;\n      return out;\n    }\n\n  out.offset_size = gvs_get_offset_size (value.size);\n  last_end = gvs_read_unaligned_le (value.data + value.size - out.offset_size,\n                                    out.offset_size);\n\n  if (last_end > value.size)\n    return out;  /* offsets not normal */\n\n  offsets_array_size = value.size - last_end;\n\n  if (offsets_array_size % out.offset_size)\n    return out;  /* offsets not normal */\n\n  out.data_size = last_end;\n  out.array = value.data + last_end;\n  out.length = offsets_array_size / out.offset_size;\n\n  if (out.length > 0 && gvs_calculate_total_size (last_end, out.length) != value.size)\n    return out;  /* offset size not minimal */\n\n  out.is_normal = TRUE;\n\n  return out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,6 +26,10 @@\n   out.data_size = last_end;\n   out.array = value.data + last_end;\n   out.length = offsets_array_size / out.offset_size;\n+\n+  if (out.length > 0 && gvs_calculate_total_size (last_end, out.length) != value.size)\n+    return out;  /* offset size not minimal */\n+\n   out.is_normal = TRUE;\n \n   return out;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  if (out.length > 0 && gvs_calculate_total_size (last_end, out.length) != value.size)",
                "    return out;  /* offset size not minimal */",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-29499",
        "func_name": "GNOME/glib/gvs_tuple_is_normal",
        "description": "A flaw was found in GLib. GVariant deserialization fails to validate that the input conforms to the expected format, leading to denial of service.",
        "git_url": "https://github.com/GNOME/glib/commit/5f4485c4ff57fdefb1661531788def7ca5a47328",
        "commit_title": "gvariant-serialiser: Check offset table entry size is minimal",
        "commit_text": " The entries in an offset table (which is used for variable sized arrays and tuples containing variable sized members) are sized so that they can address every byte in the overall variant.  The specification requires that for a variant to be in normal form, its offset table entries must be the minimum width such that they can address every byte in the variant.  That minimality requirement was not checked in `g_variant_is_normal_form()`, leading to two different byte arrays being interpreted as the normal form of a given variant tree. That kind of confusion could potentially be exploited, and is certainly a bug.  Fix it by adding the necessary checks on offset table entry width, and unit tests.  Spotted by William Manley.  ",
        "func_before": "static gboolean\ngvs_tuple_is_normal (GVariantSerialised value)\n{\n  guint offset_size;\n  gsize offset_ptr;\n  gsize length;\n  gsize offset;\n  gsize i;\n\n  /* as per the comment in gvs_tuple_get_child() */\n  if G_UNLIKELY (value.data == NULL && value.size != 0)\n    return FALSE;\n\n  offset_size = gvs_get_offset_size (value.size);\n  length = g_variant_type_info_n_members (value.type_info);\n  offset_ptr = value.size;\n  offset = 0;\n\n  for (i = 0; i < length; i++)\n    {\n      const GVariantMemberInfo *member_info;\n      GVariantSerialised child = { 0, };\n      gsize fixed_size;\n      guint alignment;\n      gsize end;\n\n      member_info = g_variant_type_info_member_info (value.type_info, i);\n      child.type_info = member_info->type_info;\n      child.depth = value.depth + 1;\n\n      g_variant_type_info_query (child.type_info, &alignment, &fixed_size);\n\n      while (offset & alignment)\n        {\n          if (offset > value.size || value.data[offset] != '\\0')\n            return FALSE;\n          offset++;\n        }\n\n      child.data = value.data + offset;\n\n      switch (member_info->ending_type)\n        {\n        case G_VARIANT_MEMBER_ENDING_FIXED:\n          end = offset + fixed_size;\n          break;\n\n        case G_VARIANT_MEMBER_ENDING_LAST:\n          end = offset_ptr;\n          break;\n\n        case G_VARIANT_MEMBER_ENDING_OFFSET:\n          if (offset_ptr < offset_size)\n            return FALSE;\n\n          offset_ptr -= offset_size;\n\n          if (offset_ptr < offset)\n            return FALSE;\n\n          end = gvs_read_unaligned_le (value.data + offset_ptr, offset_size);\n          break;\n\n        default:\n          g_assert_not_reached ();\n        }\n\n      if (end < offset || end > offset_ptr)\n        return FALSE;\n\n      child.size = end - offset;\n\n      if (child.size == 0)\n        child.data = NULL;\n\n      if (!g_variant_serialised_is_normal (child))\n        return FALSE;\n\n      offset = end;\n    }\n\n  /* All element bounds have been checked above. */\n  value.ordered_offsets_up_to = G_MAXSIZE;\n  value.checked_offsets_up_to = G_MAXSIZE;\n\n  {\n    gsize fixed_size;\n    guint alignment;\n\n    g_variant_type_info_query (value.type_info, &alignment, &fixed_size);\n\n    if (fixed_size)\n      {\n        g_assert (fixed_size == value.size);\n        g_assert (offset_ptr == value.size);\n\n        if (i == 0)\n          {\n            if (value.data[offset++] != '\\0')\n              return FALSE;\n          }\n        else\n          {\n            while (offset & alignment)\n              if (value.data[offset++] != '\\0')\n                return FALSE;\n          }\n\n        g_assert (offset == value.size);\n      }\n  }\n\n  return offset_ptr == offset;\n}",
        "func": "static gboolean\ngvs_tuple_is_normal (GVariantSerialised value)\n{\n  guint offset_size;\n  gsize offset_ptr;\n  gsize length;\n  gsize offset;\n  gsize i;\n  gsize offset_table_size;\n\n  /* as per the comment in gvs_tuple_get_child() */\n  if G_UNLIKELY (value.data == NULL && value.size != 0)\n    return FALSE;\n\n  offset_size = gvs_get_offset_size (value.size);\n  length = g_variant_type_info_n_members (value.type_info);\n  offset_ptr = value.size;\n  offset = 0;\n\n  for (i = 0; i < length; i++)\n    {\n      const GVariantMemberInfo *member_info;\n      GVariantSerialised child = { 0, };\n      gsize fixed_size;\n      guint alignment;\n      gsize end;\n\n      member_info = g_variant_type_info_member_info (value.type_info, i);\n      child.type_info = member_info->type_info;\n      child.depth = value.depth + 1;\n\n      g_variant_type_info_query (child.type_info, &alignment, &fixed_size);\n\n      while (offset & alignment)\n        {\n          if (offset > value.size || value.data[offset] != '\\0')\n            return FALSE;\n          offset++;\n        }\n\n      child.data = value.data + offset;\n\n      switch (member_info->ending_type)\n        {\n        case G_VARIANT_MEMBER_ENDING_FIXED:\n          end = offset + fixed_size;\n          break;\n\n        case G_VARIANT_MEMBER_ENDING_LAST:\n          end = offset_ptr;\n          break;\n\n        case G_VARIANT_MEMBER_ENDING_OFFSET:\n          if (offset_ptr < offset_size)\n            return FALSE;\n\n          offset_ptr -= offset_size;\n\n          if (offset_ptr < offset)\n            return FALSE;\n\n          end = gvs_read_unaligned_le (value.data + offset_ptr, offset_size);\n          break;\n\n        default:\n          g_assert_not_reached ();\n        }\n\n      if (end < offset || end > offset_ptr)\n        return FALSE;\n\n      child.size = end - offset;\n\n      if (child.size == 0)\n        child.data = NULL;\n\n      if (!g_variant_serialised_is_normal (child))\n        return FALSE;\n\n      offset = end;\n    }\n\n  /* All element bounds have been checked above. */\n  value.ordered_offsets_up_to = G_MAXSIZE;\n  value.checked_offsets_up_to = G_MAXSIZE;\n\n  {\n    gsize fixed_size;\n    guint alignment;\n\n    g_variant_type_info_query (value.type_info, &alignment, &fixed_size);\n\n    if (fixed_size)\n      {\n        g_assert (fixed_size == value.size);\n        g_assert (offset_ptr == value.size);\n\n        if (i == 0)\n          {\n            if (value.data[offset++] != '\\0')\n              return FALSE;\n          }\n        else\n          {\n            while (offset & alignment)\n              if (value.data[offset++] != '\\0')\n                return FALSE;\n          }\n\n        g_assert (offset == value.size);\n      }\n  }\n\n  /* @offset_ptr has been counting backwards from the end of the variant, to\n   * find the beginning of the offset table. @offset has been counting forwards\n   * from the beginning of the variant to find the end of the data. They should\n   * have met in the middle. */\n  if (offset_ptr != offset)\n    return FALSE;\n\n  offset_table_size = value.size - offset_ptr;\n  if (value.size > 0 &&\n      gvs_calculate_total_size (offset, offset_table_size / offset_size) != value.size)\n    return FALSE;  /* offset size not minimal */\n\n  return TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n   gsize length;\n   gsize offset;\n   gsize i;\n+  gsize offset_table_size;\n \n   /* as per the comment in gvs_tuple_get_child() */\n   if G_UNLIKELY (value.data == NULL && value.size != 0)\n@@ -110,5 +111,17 @@\n       }\n   }\n \n-  return offset_ptr == offset;\n+  /* @offset_ptr has been counting backwards from the end of the variant, to\n+   * find the beginning of the offset table. @offset has been counting forwards\n+   * from the beginning of the variant to find the end of the data. They should\n+   * have met in the middle. */\n+  if (offset_ptr != offset)\n+    return FALSE;\n+\n+  offset_table_size = value.size - offset_ptr;\n+  if (value.size > 0 &&\n+      gvs_calculate_total_size (offset, offset_table_size / offset_size) != value.size)\n+    return FALSE;  /* offset size not minimal */\n+\n+  return TRUE;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  return offset_ptr == offset;"
            ],
            "added_lines": [
                "  gsize offset_table_size;",
                "  /* @offset_ptr has been counting backwards from the end of the variant, to",
                "   * find the beginning of the offset table. @offset has been counting forwards",
                "   * from the beginning of the variant to find the end of the data. They should",
                "   * have met in the middle. */",
                "  if (offset_ptr != offset)",
                "    return FALSE;",
                "",
                "  offset_table_size = value.size - offset_ptr;",
                "  if (value.size > 0 &&",
                "      gvs_calculate_total_size (offset, offset_table_size / offset_size) != value.size)",
                "    return FALSE;  /* offset size not minimal */",
                "",
                "  return TRUE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-32611",
        "func_name": "GNOME/glib/g_variant_deep_copy",
        "description": "A flaw was found in GLib. GVariant deserialization is vulnerable to a slowdown issue where a crafted GVariant can cause excessive processing, leading to denial of service.",
        "git_url": "https://github.com/GNOME/glib/commit/a70a16b28bf78403dc86ae798c291ba167573d4a",
        "commit_title": "gvariant: Allow g_variant_byteswap() to operate on tree-form variants",
        "commit_text": " This avoids needing to always serialise a variant before byteswapping it. With variants in non-normal forms, serialisation can result in a large increase in size of the variant, and a lot of allocations for leaf `GVariant`s. This can lead to a denial of service attack.  Avoid that by changing byteswapping so that it happens on the tree form of the variant if the input is in non-normal form. If the input is in normal form (either serialised or in tree form), continue using the existing code as byteswapping an already-serialised normal variant is about 3 faster than byteswapping on the equivalent tree form.  The existing unit tests cover byteswapping well, but need some adaptation so that they operate on tree form variants too.  I considered dropping the serialised byteswapping code and doing all byteswapping on tree-form variants, as that would make maintenance simpler (avoiding having two parallel implementations of byteswapping). However, most inputs to `g_variant_byteswap()` are likely to be serialised variants (coming from a byte array of input from some foreign source) and most of them are going to be in normal form (as corruption and malicious action are rare). So getting rid of the serialised byteswapping code would impose quite a performance penalty on the common case.  ",
        "func_before": "static GVariant *\ng_variant_deep_copy (GVariant *value)\n{\n  switch (g_variant_classify (value))\n    {\n    case G_VARIANT_CLASS_MAYBE:\n    case G_VARIANT_CLASS_TUPLE:\n    case G_VARIANT_CLASS_DICT_ENTRY:\n    case G_VARIANT_CLASS_VARIANT:\n      {\n        GVariantBuilder builder;\n        gsize i, n_children;\n\n        g_variant_builder_init (&builder, g_variant_get_type (value));\n\n        for (i = 0, n_children = g_variant_n_children (value); i < n_children; i++)\n          {\n            GVariant *child = g_variant_get_child_value (value, i);\n            g_variant_builder_add_value (&builder, g_variant_deep_copy (child));\n            g_variant_unref (child);\n          }\n\n        return g_variant_builder_end (&builder);\n      }\n\n    case G_VARIANT_CLASS_ARRAY:\n      {\n        GVariantBuilder builder;\n        gsize i, n_children;\n        GVariant *first_invalid_child_deep_copy = NULL;\n\n        /* Arrays are in theory treated the same as maybes, tuples, dict entries\n         * and variants, and could be another case in the above block of code.\n         *\n         * However, they have the property that when dealing with non-normal\n         * data (which is the only time g_variant_deep_copy() is currently\n         * called) in a variable-sized array, the code above can easily end up\n         * creating many default child values in order to return an array which\n         * is of the right length and type, but without containing non-normal\n         * data. This can happen if the offset table for the array is malformed.\n         *\n         * In this case, the code above would end up allocating the same default\n         * value for each one of the child indexes beyond the first malformed\n         * entry in the offset table. This can end up being a lot of identical\n         * allocations of default values, particularly if the non-normal array\n         * is crafted maliciously.\n         *\n         * Avoid that problem by returning a new reference to the same default\n         * value for every child after the first invalid one. This results in\n         * returning an equivalent array, in normal form and trusted  but with\n         * significantly fewer memory allocations.\n         *\n         * See https://gitlab.gnome.org/GNOME/glib/-/issues/2540 */\n\n        g_variant_builder_init (&builder, g_variant_get_type (value));\n\n        for (i = 0, n_children = g_variant_n_children (value); i < n_children; i++)\n          {\n            /* Try maybe_get_child_value() first; if it returns NULL, this child\n             * is non-normal. get_child_value() would have constructed and\n             * returned a default value in that case. */\n            GVariant *child = g_variant_maybe_get_child_value (value, i);\n\n            if (child != NULL)\n              {\n                /* Non-normal children may not always be contiguous, as they may\n                 * be non-normal for reasons other than invalid offset table\n                 * entries. As they are all the same type, they will all have\n                 * the same default value though, so keep that around. */\n                g_variant_builder_add_value (&builder, g_variant_deep_copy (child));\n              }\n            else if (child == NULL && first_invalid_child_deep_copy != NULL)\n              {\n                g_variant_builder_add_value (&builder, first_invalid_child_deep_copy);\n              }\n            else if (child == NULL)\n              {\n                child = g_variant_get_child_value (value, i);\n                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child));\n                g_variant_builder_add_value (&builder, first_invalid_child_deep_copy);\n              }\n\n            g_clear_pointer (&child, g_variant_unref);\n          }\n\n        g_clear_pointer (&first_invalid_child_deep_copy, g_variant_unref);\n\n        return g_variant_builder_end (&builder);\n      }\n\n    case G_VARIANT_CLASS_BOOLEAN:\n      return g_variant_new_boolean (g_variant_get_boolean (value));\n\n    case G_VARIANT_CLASS_BYTE:\n      return g_variant_new_byte (g_variant_get_byte (value));\n\n    case G_VARIANT_CLASS_INT16:\n      return g_variant_new_int16 (g_variant_get_int16 (value));\n\n    case G_VARIANT_CLASS_UINT16:\n      return g_variant_new_uint16 (g_variant_get_uint16 (value));\n\n    case G_VARIANT_CLASS_INT32:\n      return g_variant_new_int32 (g_variant_get_int32 (value));\n\n    case G_VARIANT_CLASS_UINT32:\n      return g_variant_new_uint32 (g_variant_get_uint32 (value));\n\n    case G_VARIANT_CLASS_INT64:\n      return g_variant_new_int64 (g_variant_get_int64 (value));\n\n    case G_VARIANT_CLASS_UINT64:\n      return g_variant_new_uint64 (g_variant_get_uint64 (value));\n\n    case G_VARIANT_CLASS_HANDLE:\n      return g_variant_new_handle (g_variant_get_handle (value));\n\n    case G_VARIANT_CLASS_DOUBLE:\n      return g_variant_new_double (g_variant_get_double (value));\n\n    case G_VARIANT_CLASS_STRING:\n      return g_variant_new_string (g_variant_get_string (value, NULL));\n\n    case G_VARIANT_CLASS_OBJECT_PATH:\n      return g_variant_new_object_path (g_variant_get_string (value, NULL));\n\n    case G_VARIANT_CLASS_SIGNATURE:\n      return g_variant_new_signature (g_variant_get_string (value, NULL));\n    }\n\n  g_assert_not_reached ();\n}",
        "func": "static GVariant *\ng_variant_deep_copy (GVariant *value,\n                     gboolean  byteswap)\n{\n  switch (g_variant_classify (value))\n    {\n    case G_VARIANT_CLASS_MAYBE:\n    case G_VARIANT_CLASS_TUPLE:\n    case G_VARIANT_CLASS_DICT_ENTRY:\n    case G_VARIANT_CLASS_VARIANT:\n      {\n        GVariantBuilder builder;\n        gsize i, n_children;\n\n        g_variant_builder_init (&builder, g_variant_get_type (value));\n\n        for (i = 0, n_children = g_variant_n_children (value); i < n_children; i++)\n          {\n            GVariant *child = g_variant_get_child_value (value, i);\n            g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));\n            g_variant_unref (child);\n          }\n\n        return g_variant_builder_end (&builder);\n      }\n\n    case G_VARIANT_CLASS_ARRAY:\n      {\n        GVariantBuilder builder;\n        gsize i, n_children;\n        GVariant *first_invalid_child_deep_copy = NULL;\n\n        /* Arrays are in theory treated the same as maybes, tuples, dict entries\n         * and variants, and could be another case in the above block of code.\n         *\n         * However, they have the property that when dealing with non-normal\n         * data (which is the only time g_variant_deep_copy() is currently\n         * called) in a variable-sized array, the code above can easily end up\n         * creating many default child values in order to return an array which\n         * is of the right length and type, but without containing non-normal\n         * data. This can happen if the offset table for the array is malformed.\n         *\n         * In this case, the code above would end up allocating the same default\n         * value for each one of the child indexes beyond the first malformed\n         * entry in the offset table. This can end up being a lot of identical\n         * allocations of default values, particularly if the non-normal array\n         * is crafted maliciously.\n         *\n         * Avoid that problem by returning a new reference to the same default\n         * value for every child after the first invalid one. This results in\n         * returning an equivalent array, in normal form and trusted  but with\n         * significantly fewer memory allocations.\n         *\n         * See https://gitlab.gnome.org/GNOME/glib/-/issues/2540 */\n\n        g_variant_builder_init (&builder, g_variant_get_type (value));\n\n        for (i = 0, n_children = g_variant_n_children (value); i < n_children; i++)\n          {\n            /* Try maybe_get_child_value() first; if it returns NULL, this child\n             * is non-normal. get_child_value() would have constructed and\n             * returned a default value in that case. */\n            GVariant *child = g_variant_maybe_get_child_value (value, i);\n\n            if (child != NULL)\n              {\n                /* Non-normal children may not always be contiguous, as they may\n                 * be non-normal for reasons other than invalid offset table\n                 * entries. As they are all the same type, they will all have\n                 * the same default value though, so keep that around. */\n                g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));\n              }\n            else if (child == NULL && first_invalid_child_deep_copy != NULL)\n              {\n                g_variant_builder_add_value (&builder, first_invalid_child_deep_copy);\n              }\n            else if (child == NULL)\n              {\n                child = g_variant_get_child_value (value, i);\n                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child, byteswap));\n                g_variant_builder_add_value (&builder, first_invalid_child_deep_copy);\n              }\n\n            g_clear_pointer (&child, g_variant_unref);\n          }\n\n        g_clear_pointer (&first_invalid_child_deep_copy, g_variant_unref);\n\n        return g_variant_builder_end (&builder);\n      }\n\n    case G_VARIANT_CLASS_BOOLEAN:\n      return g_variant_new_boolean (g_variant_get_boolean (value));\n\n    case G_VARIANT_CLASS_BYTE:\n      return g_variant_new_byte (g_variant_get_byte (value));\n\n    case G_VARIANT_CLASS_INT16:\n      if (byteswap)\n        return g_variant_new_int16 (GUINT16_SWAP_LE_BE (g_variant_get_int16 (value)));\n      else\n        return g_variant_new_int16 (g_variant_get_int16 (value));\n\n    case G_VARIANT_CLASS_UINT16:\n      if (byteswap)\n        return g_variant_new_uint16 (GUINT16_SWAP_LE_BE (g_variant_get_uint16 (value)));\n      else\n        return g_variant_new_uint16 (g_variant_get_uint16 (value));\n\n    case G_VARIANT_CLASS_INT32:\n      if (byteswap)\n        return g_variant_new_int32 (GUINT32_SWAP_LE_BE (g_variant_get_int32 (value)));\n      else\n        return g_variant_new_int32 (g_variant_get_int32 (value));\n\n    case G_VARIANT_CLASS_UINT32:\n      if (byteswap)\n        return g_variant_new_uint32 (GUINT32_SWAP_LE_BE (g_variant_get_uint32 (value)));\n      else\n        return g_variant_new_uint32 (g_variant_get_uint32 (value));\n\n    case G_VARIANT_CLASS_INT64:\n      if (byteswap)\n        return g_variant_new_int64 (GUINT64_SWAP_LE_BE (g_variant_get_int64 (value)));\n      else\n        return g_variant_new_int64 (g_variant_get_int64 (value));\n\n    case G_VARIANT_CLASS_UINT64:\n      if (byteswap)\n        return g_variant_new_uint64 (GUINT64_SWAP_LE_BE (g_variant_get_uint64 (value)));\n      else\n        return g_variant_new_uint64 (g_variant_get_uint64 (value));\n\n    case G_VARIANT_CLASS_HANDLE:\n      if (byteswap)\n        return g_variant_new_handle (GUINT32_SWAP_LE_BE (g_variant_get_handle (value)));\n      else\n        return g_variant_new_handle (g_variant_get_handle (value));\n\n    case G_VARIANT_CLASS_DOUBLE:\n      if (byteswap)\n        {\n          /* We have to convert the double to a uint64 here using a union,\n           * because a cast will round it numerically. */\n          union\n            {\n              guint64 u64;\n              gdouble dbl;\n            } u1, u2;\n          u1.dbl = g_variant_get_double (value);\n          u2.u64 = GUINT64_SWAP_LE_BE (u1.u64);\n          return g_variant_new_double (u2.dbl);\n        }\n      else\n        return g_variant_new_double (g_variant_get_double (value));\n\n    case G_VARIANT_CLASS_STRING:\n      return g_variant_new_string (g_variant_get_string (value, NULL));\n\n    case G_VARIANT_CLASS_OBJECT_PATH:\n      return g_variant_new_object_path (g_variant_get_string (value, NULL));\n\n    case G_VARIANT_CLASS_SIGNATURE:\n      return g_variant_new_signature (g_variant_get_string (value, NULL));\n    }\n\n  g_assert_not_reached ();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n static GVariant *\n-g_variant_deep_copy (GVariant *value)\n+g_variant_deep_copy (GVariant *value,\n+                     gboolean  byteswap)\n {\n   switch (g_variant_classify (value))\n     {\n@@ -16,7 +17,7 @@\n         for (i = 0, n_children = g_variant_n_children (value); i < n_children; i++)\n           {\n             GVariant *child = g_variant_get_child_value (value, i);\n-            g_variant_builder_add_value (&builder, g_variant_deep_copy (child));\n+            g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));\n             g_variant_unref (child);\n           }\n \n@@ -67,7 +68,7 @@\n                  * be non-normal for reasons other than invalid offset table\n                  * entries. As they are all the same type, they will all have\n                  * the same default value though, so keep that around. */\n-                g_variant_builder_add_value (&builder, g_variant_deep_copy (child));\n+                g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));\n               }\n             else if (child == NULL && first_invalid_child_deep_copy != NULL)\n               {\n@@ -76,7 +77,7 @@\n             else if (child == NULL)\n               {\n                 child = g_variant_get_child_value (value, i);\n-                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child));\n+                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child, byteswap));\n                 g_variant_builder_add_value (&builder, first_invalid_child_deep_copy);\n               }\n \n@@ -95,28 +96,63 @@\n       return g_variant_new_byte (g_variant_get_byte (value));\n \n     case G_VARIANT_CLASS_INT16:\n-      return g_variant_new_int16 (g_variant_get_int16 (value));\n+      if (byteswap)\n+        return g_variant_new_int16 (GUINT16_SWAP_LE_BE (g_variant_get_int16 (value)));\n+      else\n+        return g_variant_new_int16 (g_variant_get_int16 (value));\n \n     case G_VARIANT_CLASS_UINT16:\n-      return g_variant_new_uint16 (g_variant_get_uint16 (value));\n+      if (byteswap)\n+        return g_variant_new_uint16 (GUINT16_SWAP_LE_BE (g_variant_get_uint16 (value)));\n+      else\n+        return g_variant_new_uint16 (g_variant_get_uint16 (value));\n \n     case G_VARIANT_CLASS_INT32:\n-      return g_variant_new_int32 (g_variant_get_int32 (value));\n+      if (byteswap)\n+        return g_variant_new_int32 (GUINT32_SWAP_LE_BE (g_variant_get_int32 (value)));\n+      else\n+        return g_variant_new_int32 (g_variant_get_int32 (value));\n \n     case G_VARIANT_CLASS_UINT32:\n-      return g_variant_new_uint32 (g_variant_get_uint32 (value));\n+      if (byteswap)\n+        return g_variant_new_uint32 (GUINT32_SWAP_LE_BE (g_variant_get_uint32 (value)));\n+      else\n+        return g_variant_new_uint32 (g_variant_get_uint32 (value));\n \n     case G_VARIANT_CLASS_INT64:\n-      return g_variant_new_int64 (g_variant_get_int64 (value));\n+      if (byteswap)\n+        return g_variant_new_int64 (GUINT64_SWAP_LE_BE (g_variant_get_int64 (value)));\n+      else\n+        return g_variant_new_int64 (g_variant_get_int64 (value));\n \n     case G_VARIANT_CLASS_UINT64:\n-      return g_variant_new_uint64 (g_variant_get_uint64 (value));\n+      if (byteswap)\n+        return g_variant_new_uint64 (GUINT64_SWAP_LE_BE (g_variant_get_uint64 (value)));\n+      else\n+        return g_variant_new_uint64 (g_variant_get_uint64 (value));\n \n     case G_VARIANT_CLASS_HANDLE:\n-      return g_variant_new_handle (g_variant_get_handle (value));\n+      if (byteswap)\n+        return g_variant_new_handle (GUINT32_SWAP_LE_BE (g_variant_get_handle (value)));\n+      else\n+        return g_variant_new_handle (g_variant_get_handle (value));\n \n     case G_VARIANT_CLASS_DOUBLE:\n-      return g_variant_new_double (g_variant_get_double (value));\n+      if (byteswap)\n+        {\n+          /* We have to convert the double to a uint64 here using a union,\n+           * because a cast will round it numerically. */\n+          union\n+            {\n+              guint64 u64;\n+              gdouble dbl;\n+            } u1, u2;\n+          u1.dbl = g_variant_get_double (value);\n+          u2.u64 = GUINT64_SWAP_LE_BE (u1.u64);\n+          return g_variant_new_double (u2.dbl);\n+        }\n+      else\n+        return g_variant_new_double (g_variant_get_double (value));\n \n     case G_VARIANT_CLASS_STRING:\n       return g_variant_new_string (g_variant_get_string (value, NULL));",
        "diff_line_info": {
            "deleted_lines": [
                "g_variant_deep_copy (GVariant *value)",
                "            g_variant_builder_add_value (&builder, g_variant_deep_copy (child));",
                "                g_variant_builder_add_value (&builder, g_variant_deep_copy (child));",
                "                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child));",
                "      return g_variant_new_int16 (g_variant_get_int16 (value));",
                "      return g_variant_new_uint16 (g_variant_get_uint16 (value));",
                "      return g_variant_new_int32 (g_variant_get_int32 (value));",
                "      return g_variant_new_uint32 (g_variant_get_uint32 (value));",
                "      return g_variant_new_int64 (g_variant_get_int64 (value));",
                "      return g_variant_new_uint64 (g_variant_get_uint64 (value));",
                "      return g_variant_new_handle (g_variant_get_handle (value));",
                "      return g_variant_new_double (g_variant_get_double (value));"
            ],
            "added_lines": [
                "g_variant_deep_copy (GVariant *value,",
                "                     gboolean  byteswap)",
                "            g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));",
                "                g_variant_builder_add_value (&builder, g_variant_deep_copy (child, byteswap));",
                "                first_invalid_child_deep_copy = g_variant_ref_sink (g_variant_deep_copy (child, byteswap));",
                "      if (byteswap)",
                "        return g_variant_new_int16 (GUINT16_SWAP_LE_BE (g_variant_get_int16 (value)));",
                "      else",
                "        return g_variant_new_int16 (g_variant_get_int16 (value));",
                "      if (byteswap)",
                "        return g_variant_new_uint16 (GUINT16_SWAP_LE_BE (g_variant_get_uint16 (value)));",
                "      else",
                "        return g_variant_new_uint16 (g_variant_get_uint16 (value));",
                "      if (byteswap)",
                "        return g_variant_new_int32 (GUINT32_SWAP_LE_BE (g_variant_get_int32 (value)));",
                "      else",
                "        return g_variant_new_int32 (g_variant_get_int32 (value));",
                "      if (byteswap)",
                "        return g_variant_new_uint32 (GUINT32_SWAP_LE_BE (g_variant_get_uint32 (value)));",
                "      else",
                "        return g_variant_new_uint32 (g_variant_get_uint32 (value));",
                "      if (byteswap)",
                "        return g_variant_new_int64 (GUINT64_SWAP_LE_BE (g_variant_get_int64 (value)));",
                "      else",
                "        return g_variant_new_int64 (g_variant_get_int64 (value));",
                "      if (byteswap)",
                "        return g_variant_new_uint64 (GUINT64_SWAP_LE_BE (g_variant_get_uint64 (value)));",
                "      else",
                "        return g_variant_new_uint64 (g_variant_get_uint64 (value));",
                "      if (byteswap)",
                "        return g_variant_new_handle (GUINT32_SWAP_LE_BE (g_variant_get_handle (value)));",
                "      else",
                "        return g_variant_new_handle (g_variant_get_handle (value));",
                "      if (byteswap)",
                "        {",
                "          /* We have to convert the double to a uint64 here using a union,",
                "           * because a cast will round it numerically. */",
                "          union",
                "            {",
                "              guint64 u64;",
                "              gdouble dbl;",
                "            } u1, u2;",
                "          u1.dbl = g_variant_get_double (value);",
                "          u2.u64 = GUINT64_SWAP_LE_BE (u1.u64);",
                "          return g_variant_new_double (u2.dbl);",
                "        }",
                "      else",
                "        return g_variant_new_double (g_variant_get_double (value));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-32611",
        "func_name": "GNOME/glib/g_variant_byteswap",
        "description": "A flaw was found in GLib. GVariant deserialization is vulnerable to a slowdown issue where a crafted GVariant can cause excessive processing, leading to denial of service.",
        "git_url": "https://github.com/GNOME/glib/commit/a70a16b28bf78403dc86ae798c291ba167573d4a",
        "commit_title": "gvariant: Allow g_variant_byteswap() to operate on tree-form variants",
        "commit_text": " This avoids needing to always serialise a variant before byteswapping it. With variants in non-normal forms, serialisation can result in a large increase in size of the variant, and a lot of allocations for leaf `GVariant`s. This can lead to a denial of service attack.  Avoid that by changing byteswapping so that it happens on the tree form of the variant if the input is in non-normal form. If the input is in normal form (either serialised or in tree form), continue using the existing code as byteswapping an already-serialised normal variant is about 3 faster than byteswapping on the equivalent tree form.  The existing unit tests cover byteswapping well, but need some adaptation so that they operate on tree form variants too.  I considered dropping the serialised byteswapping code and doing all byteswapping on tree-form variants, as that would make maintenance simpler (avoiding having two parallel implementations of byteswapping). However, most inputs to `g_variant_byteswap()` are likely to be serialised variants (coming from a byte array of input from some foreign source) and most of them are going to be in normal form (as corruption and malicious action are rare). So getting rid of the serialised byteswapping code would impose quite a performance penalty on the common case.  ",
        "func_before": "GVariant *\ng_variant_byteswap (GVariant *value)\n{\n  GVariantTypeInfo *type_info;\n  guint alignment;\n  GVariant *new;\n\n  type_info = g_variant_get_type_info (value);\n\n  g_variant_type_info_query (type_info, &alignment, NULL);\n\n  if (alignment)\n    /* (potentially) contains multi-byte numeric data */\n    {\n      GVariantSerialised serialised = { 0, };\n      GVariant *trusted;\n      GBytes *bytes;\n\n      trusted = g_variant_get_normal_form (value);\n      serialised.type_info = g_variant_get_type_info (trusted);\n      serialised.size = g_variant_get_size (trusted);\n      serialised.data = g_malloc (serialised.size);\n      serialised.depth = g_variant_get_depth (trusted);\n      serialised.ordered_offsets_up_to = G_MAXSIZE;  /* operating on the normal form */\n      serialised.checked_offsets_up_to = G_MAXSIZE;\n      g_variant_store (trusted, serialised.data);\n      g_variant_unref (trusted);\n\n      g_variant_serialised_byteswap (serialised);\n\n      bytes = g_bytes_new_take (serialised.data, serialised.size);\n      new = g_variant_ref_sink (g_variant_new_from_bytes (g_variant_get_type (value), bytes, TRUE));\n      g_bytes_unref (bytes);\n    }\n  else\n    /* contains no multi-byte data */\n    new = g_variant_get_normal_form (value);\n\n  g_assert (g_variant_is_trusted (new));\n\n  return g_steal_pointer (&new);\n}",
        "func": "GVariant *\ng_variant_byteswap (GVariant *value)\n{\n  GVariantTypeInfo *type_info;\n  guint alignment;\n  GVariant *new;\n\n  type_info = g_variant_get_type_info (value);\n\n  g_variant_type_info_query (type_info, &alignment, NULL);\n\n  if (alignment && g_variant_is_normal_form (value))\n    {\n      /* (potentially) contains multi-byte numeric data, but is also already in\n       * normal form so we can use a faster byteswapping codepath on the\n       * serialised data */\n      GVariantSerialised serialised = { 0, };\n      GBytes *bytes;\n\n      serialised.type_info = g_variant_get_type_info (value);\n      serialised.size = g_variant_get_size (value);\n      serialised.data = g_malloc (serialised.size);\n      serialised.depth = g_variant_get_depth (value);\n      serialised.ordered_offsets_up_to = G_MAXSIZE;  /* operating on the normal form */\n      serialised.checked_offsets_up_to = G_MAXSIZE;\n      g_variant_store (value, serialised.data);\n\n      g_variant_serialised_byteswap (serialised);\n\n      bytes = g_bytes_new_take (serialised.data, serialised.size);\n      new = g_variant_ref_sink (g_variant_new_from_bytes (g_variant_get_type (value), bytes, TRUE));\n      g_bytes_unref (bytes);\n    }\n  else if (alignment)\n    /* (potentially) contains multi-byte numeric data */\n    new = g_variant_ref_sink (g_variant_deep_copy (value, TRUE));\n  else\n    /* contains no multi-byte data */\n    new = g_variant_get_normal_form (value);\n\n  g_assert (g_variant_is_trusted (new));\n\n  return g_steal_pointer (&new);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,22 +9,21 @@\n \n   g_variant_type_info_query (type_info, &alignment, NULL);\n \n-  if (alignment)\n-    /* (potentially) contains multi-byte numeric data */\n+  if (alignment && g_variant_is_normal_form (value))\n     {\n+      /* (potentially) contains multi-byte numeric data, but is also already in\n+       * normal form so we can use a faster byteswapping codepath on the\n+       * serialised data */\n       GVariantSerialised serialised = { 0, };\n-      GVariant *trusted;\n       GBytes *bytes;\n \n-      trusted = g_variant_get_normal_form (value);\n-      serialised.type_info = g_variant_get_type_info (trusted);\n-      serialised.size = g_variant_get_size (trusted);\n+      serialised.type_info = g_variant_get_type_info (value);\n+      serialised.size = g_variant_get_size (value);\n       serialised.data = g_malloc (serialised.size);\n-      serialised.depth = g_variant_get_depth (trusted);\n+      serialised.depth = g_variant_get_depth (value);\n       serialised.ordered_offsets_up_to = G_MAXSIZE;  /* operating on the normal form */\n       serialised.checked_offsets_up_to = G_MAXSIZE;\n-      g_variant_store (trusted, serialised.data);\n-      g_variant_unref (trusted);\n+      g_variant_store (value, serialised.data);\n \n       g_variant_serialised_byteswap (serialised);\n \n@@ -32,6 +31,9 @@\n       new = g_variant_ref_sink (g_variant_new_from_bytes (g_variant_get_type (value), bytes, TRUE));\n       g_bytes_unref (bytes);\n     }\n+  else if (alignment)\n+    /* (potentially) contains multi-byte numeric data */\n+    new = g_variant_ref_sink (g_variant_deep_copy (value, TRUE));\n   else\n     /* contains no multi-byte data */\n     new = g_variant_get_normal_form (value);",
        "diff_line_info": {
            "deleted_lines": [
                "  if (alignment)",
                "    /* (potentially) contains multi-byte numeric data */",
                "      GVariant *trusted;",
                "      trusted = g_variant_get_normal_form (value);",
                "      serialised.type_info = g_variant_get_type_info (trusted);",
                "      serialised.size = g_variant_get_size (trusted);",
                "      serialised.depth = g_variant_get_depth (trusted);",
                "      g_variant_store (trusted, serialised.data);",
                "      g_variant_unref (trusted);"
            ],
            "added_lines": [
                "  if (alignment && g_variant_is_normal_form (value))",
                "      /* (potentially) contains multi-byte numeric data, but is also already in",
                "       * normal form so we can use a faster byteswapping codepath on the",
                "       * serialised data */",
                "      serialised.type_info = g_variant_get_type_info (value);",
                "      serialised.size = g_variant_get_size (value);",
                "      serialised.depth = g_variant_get_depth (value);",
                "      g_variant_store (value, serialised.data);",
                "  else if (alignment)",
                "    /* (potentially) contains multi-byte numeric data */",
                "    new = g_variant_ref_sink (g_variant_deep_copy (value, TRUE));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-32611",
        "func_name": "GNOME/glib/g_variant_get_normal_form",
        "description": "A flaw was found in GLib. GVariant deserialization is vulnerable to a slowdown issue where a crafted GVariant can cause excessive processing, leading to denial of service.",
        "git_url": "https://github.com/GNOME/glib/commit/a70a16b28bf78403dc86ae798c291ba167573d4a",
        "commit_title": "gvariant: Allow g_variant_byteswap() to operate on tree-form variants",
        "commit_text": " This avoids needing to always serialise a variant before byteswapping it. With variants in non-normal forms, serialisation can result in a large increase in size of the variant, and a lot of allocations for leaf `GVariant`s. This can lead to a denial of service attack.  Avoid that by changing byteswapping so that it happens on the tree form of the variant if the input is in non-normal form. If the input is in normal form (either serialised or in tree form), continue using the existing code as byteswapping an already-serialised normal variant is about 3 faster than byteswapping on the equivalent tree form.  The existing unit tests cover byteswapping well, but need some adaptation so that they operate on tree form variants too.  I considered dropping the serialised byteswapping code and doing all byteswapping on tree-form variants, as that would make maintenance simpler (avoiding having two parallel implementations of byteswapping). However, most inputs to `g_variant_byteswap()` are likely to be serialised variants (coming from a byte array of input from some foreign source) and most of them are going to be in normal form (as corruption and malicious action are rare). So getting rid of the serialised byteswapping code would impose quite a performance penalty on the common case.  ",
        "func_before": "GVariant *\ng_variant_get_normal_form (GVariant *value)\n{\n  GVariant *trusted;\n\n  if (g_variant_is_normal_form (value))\n    return g_variant_ref (value);\n\n  trusted = g_variant_deep_copy (value);\n  g_assert (g_variant_is_trusted (trusted));\n\n  return g_variant_ref_sink (trusted);\n}",
        "func": "GVariant *\ng_variant_get_normal_form (GVariant *value)\n{\n  GVariant *trusted;\n\n  if (g_variant_is_normal_form (value))\n    return g_variant_ref (value);\n\n  trusted = g_variant_deep_copy (value, FALSE);\n  g_assert (g_variant_is_trusted (trusted));\n\n  return g_variant_ref_sink (trusted);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n   if (g_variant_is_normal_form (value))\n     return g_variant_ref (value);\n \n-  trusted = g_variant_deep_copy (value);\n+  trusted = g_variant_deep_copy (value, FALSE);\n   g_assert (g_variant_is_trusted (trusted));\n \n   return g_variant_ref_sink (trusted);",
        "diff_line_info": {
            "deleted_lines": [
                "  trusted = g_variant_deep_copy (value);"
            ],
            "added_lines": [
                "  trusted = g_variant_deep_copy (value, FALSE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/decode",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "static bool\n  decode(const Node &node, YamlSNIConfig::Item &item)\n  {\n    for (const auto &elem : node) {\n      if (std::none_of(valid_sni_config_keys.begin(), valid_sni_config_keys.end(),\n                       [&elem](const std::string &s) { return s == elem.first.as<std::string>(); })) {\n        Warning(\"unsupported key '%s' in SNI config\", elem.first.as<std::string>().c_str());\n      }\n    }\n\n    if (node[TS_fqdn]) {\n      item.fqdn = node[TS_fqdn].as<std::string>();\n    } else {\n      return false; // servername must be present\n    }\n    if (node[TS_disable_h2]) {\n      item.offer_h2 = false;\n    }\n    if (node[TS_http2]) {\n      item.offer_h2 = node[TS_http2].as<bool>();\n    }\n    if (node[TS_http2_buffer_water_mark]) {\n      item.http2_buffer_water_mark = node[TS_http2_buffer_water_mark].as<int>();\n    }\n\n    // enum\n    if (node[TS_verify_client]) {\n      auto value = node[TS_verify_client].as<std::string>();\n      int level  = LEVEL_DESCRIPTOR.get(value);\n      if (level < 0) {\n        throw YAML::ParserException(node[TS_verify_client].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_client_level = static_cast<uint8_t>(level);\n    }\n\n    if (node[TS_verify_client_ca_certs]) {\n#if !TS_HAS_VERIFY_CERT_STORE\n      // TS was compiled with an older version of the OpenSSL interface, that doesn't have\n      // SSL_set1_verify_cert_store().  We need this macro in order to set the CA certs for verifying clients\n      // after the client sends the SNI server name.\n      //\n      throw YAML::ParserException(node[TS_verify_client_ca_certs].Mark(),\n                                  std::string(TS_verify_client_ca_certs) + \" requires features from OpenSSL 1.0.2 or later\");\n#else\n      std::string file, dir;\n      auto const &n = node[TS_verify_client_ca_certs];\n\n      if (n.IsMap()) {\n        for (const auto &elem : n) {\n          std::string key = elem.first.as<std::string>();\n          if (\"file\" == key) {\n            if (!file.empty()) {\n              throw YAML::ParserException(elem.first.Mark(), \"duplicate key \\\"file\\\"\");\n            }\n            file = elem.second.as<std::string>();\n\n          } else if (\"dir\" == key) {\n            if (!dir.empty()) {\n              throw YAML::ParserException(elem.first.Mark(), \"duplicate key \\\"dir\\\"\");\n            }\n            dir = elem.second.as<std::string>();\n\n          } else {\n            throw YAML::ParserException(elem.first.Mark(), \"unsupported key \" + elem.first.as<std::string>());\n          }\n        }\n      } else {\n        // Value should be string scalar with file.\n        //\n        file = n.as<std::string>();\n      }\n      ink_assert(!(file.empty() && dir.empty()));\n\n      if (!file.empty() && (file[0] != '/')) {\n        file = RecConfigReadConfigDir() + '/' + file;\n      }\n      if (!dir.empty() && (dir[0] != '/')) {\n        dir = RecConfigReadConfigDir() + '/' + dir;\n      }\n      item.verify_client_ca_file = file;\n      item.verify_client_ca_dir  = dir;\n#endif\n    }\n\n    if (node[TS_host_sni_policy]) {\n      auto value           = node[TS_host_sni_policy].as<std::string>();\n      int policy           = POLICY_DESCRIPTOR.get(value);\n      item.host_sni_policy = static_cast<uint8_t>(policy);\n    }\n\n    YamlSNIConfig::TunnelPreWarm t_prewarm = YamlSNIConfig::TunnelPreWarm::UNSET;\n    uint32_t t_min                         = item.tunnel_prewarm_min;\n    int32_t t_max                          = item.tunnel_prewarm_max;\n    double t_rate                          = item.tunnel_prewarm_rate;\n    uint32_t t_connect_timeout             = item.tunnel_prewarm_connect_timeout;\n    uint32_t t_inactive_timeout            = item.tunnel_prewarm_inactive_timeout;\n    bool t_srv                             = item.tunnel_prewarm_srv;\n\n    if (node[TS_tunnel_prewarm]) {\n      auto is_prewarm_enabled = node[TS_tunnel_prewarm].as<bool>();\n      if (is_prewarm_enabled) {\n        t_prewarm = YamlSNIConfig::TunnelPreWarm::ENABLED;\n      } else {\n        t_prewarm = YamlSNIConfig::TunnelPreWarm::DISABLED;\n      }\n    }\n    if (node[TS_tunnel_prewarm_min]) {\n      t_min = node[TS_tunnel_prewarm_min].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_max]) {\n      t_max = node[TS_tunnel_prewarm_max].as<int32_t>();\n    }\n    if (node[TS_tunnel_prewarm_rate]) {\n      t_rate = node[TS_tunnel_prewarm_rate].as<double>();\n    }\n    if (node[TS_tunnel_prewarm_connect_timeout]) {\n      t_connect_timeout = node[TS_tunnel_prewarm_connect_timeout].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_inactive_timeout]) {\n      t_inactive_timeout = node[TS_tunnel_prewarm_inactive_timeout].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_srv]) {\n      t_srv = node[TS_tunnel_prewarm_srv].as<bool>();\n    }\n\n    if (node[TS_tunnel_route]) {\n      item.tunnel_destination = node[TS_tunnel_route].as<std::string>();\n      item.tunnel_type        = SNIRoutingType::BLIND;\n    } else if (node[TS_forward_route]) {\n      item.tunnel_destination              = node[TS_forward_route].as<std::string>();\n      item.tunnel_type                     = SNIRoutingType::FORWARD;\n      item.tunnel_prewarm                  = t_prewarm;\n      item.tunnel_prewarm_min              = t_min;\n      item.tunnel_prewarm_max              = t_max;\n      item.tunnel_prewarm_rate             = t_rate;\n      item.tunnel_prewarm_connect_timeout  = t_connect_timeout;\n      item.tunnel_prewarm_inactive_timeout = t_inactive_timeout;\n      item.tunnel_prewarm_srv              = t_srv;\n    } else if (node[TS_partial_blind_route]) {\n      item.tunnel_destination              = node[TS_partial_blind_route].as<std::string>();\n      item.tunnel_type                     = SNIRoutingType::PARTIAL_BLIND;\n      item.tunnel_prewarm                  = t_prewarm;\n      item.tunnel_prewarm_min              = t_min;\n      item.tunnel_prewarm_max              = t_max;\n      item.tunnel_prewarm_rate             = t_rate;\n      item.tunnel_prewarm_connect_timeout  = t_connect_timeout;\n      item.tunnel_prewarm_inactive_timeout = t_inactive_timeout;\n      item.tunnel_prewarm_srv              = t_srv;\n\n      if (node[TS_tunnel_alpn]) {\n        load_tunnel_alpn(item.tunnel_alpn, node[TS_tunnel_alpn]);\n      }\n    }\n\n    if (node[TS_verify_server_policy]) {\n      auto value = node[TS_verify_server_policy].as<std::string>();\n      int policy = POLICY_DESCRIPTOR.get(value);\n      if (policy < 0) {\n        throw YAML::ParserException(node[TS_verify_server_policy].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_server_policy = static_cast<YamlSNIConfig::Policy>(policy);\n    }\n\n    if (node[TS_verify_server_properties]) {\n      auto value     = node[TS_verify_server_properties].as<std::string>();\n      int properties = PROPERTIES_DESCRIPTOR.get(value);\n      if (properties < 0) {\n        throw YAML::ParserException(node[TS_verify_server_properties].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_server_properties = static_cast<YamlSNIConfig::Property>(properties);\n    }\n\n    if (node[TS_client_cert]) {\n      item.client_cert = node[TS_client_cert].as<std::string>();\n    }\n    if (node[TS_client_key]) {\n      item.client_key = node[TS_client_key].as<std::string>();\n    }\n    if (node[TS_client_sni_policy]) {\n      item.client_sni_policy = node[TS_client_sni_policy].as<std::string>();\n    }\n\n    if (node[TS_ip_allow]) {\n      item.ip_allow = node[TS_ip_allow].as<std::string>();\n    }\n    if (node[TS_valid_tls_versions_in]) {\n      for (unsigned int i = 0; i < node[TS_valid_tls_versions_in].size(); i++) {\n        auto value   = node[TS_valid_tls_versions_in][i].as<std::string>();\n        int protocol = TLS_PROTOCOLS_DESCRIPTOR.get(value);\n        item.EnableProtocol(static_cast<YamlSNIConfig::TLSProtocol>(protocol));\n      }\n    }\n    return true;\n  }",
        "func": "static bool\n  decode(const Node &node, YamlSNIConfig::Item &item)\n  {\n    for (const auto &elem : node) {\n      if (std::none_of(valid_sni_config_keys.begin(), valid_sni_config_keys.end(),\n                       [&elem](const std::string &s) { return s == elem.first.as<std::string>(); })) {\n        Warning(\"unsupported key '%s' in SNI config\", elem.first.as<std::string>().c_str());\n      }\n    }\n\n    if (node[TS_fqdn]) {\n      item.fqdn = node[TS_fqdn].as<std::string>();\n    } else {\n      return false; // servername must be present\n    }\n    if (node[TS_disable_h2]) {\n      item.offer_h2 = false;\n    }\n    if (node[TS_http2]) {\n      item.offer_h2 = node[TS_http2].as<bool>();\n    }\n    if (node[TS_http2_buffer_water_mark]) {\n      item.http2_buffer_water_mark = node[TS_http2_buffer_water_mark].as<int>();\n    }\n    if (node[TS_http2_max_settings_frames_per_minute]) {\n      item.http2_max_settings_frames_per_minute = node[TS_http2_max_settings_frames_per_minute].as<int>();\n    }\n    if (node[TS_http2_max_ping_frames_per_minute]) {\n      item.http2_max_ping_frames_per_minute = node[TS_http2_max_ping_frames_per_minute].as<int>();\n    }\n    if (node[TS_http2_max_priority_frames_per_minute]) {\n      item.http2_max_priority_frames_per_minute = node[TS_http2_max_priority_frames_per_minute].as<int>();\n    }\n    if (node[TS_http2_max_rst_stream_frames_per_minute]) {\n      item.http2_max_rst_stream_frames_per_minute = node[TS_http2_max_rst_stream_frames_per_minute].as<int>();\n    }\n\n    // enum\n    if (node[TS_verify_client]) {\n      auto value = node[TS_verify_client].as<std::string>();\n      int level  = LEVEL_DESCRIPTOR.get(value);\n      if (level < 0) {\n        throw YAML::ParserException(node[TS_verify_client].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_client_level = static_cast<uint8_t>(level);\n    }\n\n    if (node[TS_verify_client_ca_certs]) {\n#if !TS_HAS_VERIFY_CERT_STORE\n      // TS was compiled with an older version of the OpenSSL interface, that doesn't have\n      // SSL_set1_verify_cert_store().  We need this macro in order to set the CA certs for verifying clients\n      // after the client sends the SNI server name.\n      //\n      throw YAML::ParserException(node[TS_verify_client_ca_certs].Mark(),\n                                  std::string(TS_verify_client_ca_certs) + \" requires features from OpenSSL 1.0.2 or later\");\n#else\n      std::string file, dir;\n      auto const &n = node[TS_verify_client_ca_certs];\n\n      if (n.IsMap()) {\n        for (const auto &elem : n) {\n          std::string key = elem.first.as<std::string>();\n          if (\"file\" == key) {\n            if (!file.empty()) {\n              throw YAML::ParserException(elem.first.Mark(), \"duplicate key \\\"file\\\"\");\n            }\n            file = elem.second.as<std::string>();\n\n          } else if (\"dir\" == key) {\n            if (!dir.empty()) {\n              throw YAML::ParserException(elem.first.Mark(), \"duplicate key \\\"dir\\\"\");\n            }\n            dir = elem.second.as<std::string>();\n\n          } else {\n            throw YAML::ParserException(elem.first.Mark(), \"unsupported key \" + elem.first.as<std::string>());\n          }\n        }\n      } else {\n        // Value should be string scalar with file.\n        //\n        file = n.as<std::string>();\n      }\n      ink_assert(!(file.empty() && dir.empty()));\n\n      if (!file.empty() && (file[0] != '/')) {\n        file = RecConfigReadConfigDir() + '/' + file;\n      }\n      if (!dir.empty() && (dir[0] != '/')) {\n        dir = RecConfigReadConfigDir() + '/' + dir;\n      }\n      item.verify_client_ca_file = file;\n      item.verify_client_ca_dir  = dir;\n#endif\n    }\n\n    if (node[TS_host_sni_policy]) {\n      auto value           = node[TS_host_sni_policy].as<std::string>();\n      int policy           = POLICY_DESCRIPTOR.get(value);\n      item.host_sni_policy = static_cast<uint8_t>(policy);\n    }\n\n    YamlSNIConfig::TunnelPreWarm t_prewarm = YamlSNIConfig::TunnelPreWarm::UNSET;\n    uint32_t t_min                         = item.tunnel_prewarm_min;\n    int32_t t_max                          = item.tunnel_prewarm_max;\n    double t_rate                          = item.tunnel_prewarm_rate;\n    uint32_t t_connect_timeout             = item.tunnel_prewarm_connect_timeout;\n    uint32_t t_inactive_timeout            = item.tunnel_prewarm_inactive_timeout;\n    bool t_srv                             = item.tunnel_prewarm_srv;\n\n    if (node[TS_tunnel_prewarm]) {\n      auto is_prewarm_enabled = node[TS_tunnel_prewarm].as<bool>();\n      if (is_prewarm_enabled) {\n        t_prewarm = YamlSNIConfig::TunnelPreWarm::ENABLED;\n      } else {\n        t_prewarm = YamlSNIConfig::TunnelPreWarm::DISABLED;\n      }\n    }\n    if (node[TS_tunnel_prewarm_min]) {\n      t_min = node[TS_tunnel_prewarm_min].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_max]) {\n      t_max = node[TS_tunnel_prewarm_max].as<int32_t>();\n    }\n    if (node[TS_tunnel_prewarm_rate]) {\n      t_rate = node[TS_tunnel_prewarm_rate].as<double>();\n    }\n    if (node[TS_tunnel_prewarm_connect_timeout]) {\n      t_connect_timeout = node[TS_tunnel_prewarm_connect_timeout].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_inactive_timeout]) {\n      t_inactive_timeout = node[TS_tunnel_prewarm_inactive_timeout].as<uint32_t>();\n    }\n    if (node[TS_tunnel_prewarm_srv]) {\n      t_srv = node[TS_tunnel_prewarm_srv].as<bool>();\n    }\n\n    if (node[TS_tunnel_route]) {\n      item.tunnel_destination = node[TS_tunnel_route].as<std::string>();\n      item.tunnel_type        = SNIRoutingType::BLIND;\n    } else if (node[TS_forward_route]) {\n      item.tunnel_destination              = node[TS_forward_route].as<std::string>();\n      item.tunnel_type                     = SNIRoutingType::FORWARD;\n      item.tunnel_prewarm                  = t_prewarm;\n      item.tunnel_prewarm_min              = t_min;\n      item.tunnel_prewarm_max              = t_max;\n      item.tunnel_prewarm_rate             = t_rate;\n      item.tunnel_prewarm_connect_timeout  = t_connect_timeout;\n      item.tunnel_prewarm_inactive_timeout = t_inactive_timeout;\n      item.tunnel_prewarm_srv              = t_srv;\n    } else if (node[TS_partial_blind_route]) {\n      item.tunnel_destination              = node[TS_partial_blind_route].as<std::string>();\n      item.tunnel_type                     = SNIRoutingType::PARTIAL_BLIND;\n      item.tunnel_prewarm                  = t_prewarm;\n      item.tunnel_prewarm_min              = t_min;\n      item.tunnel_prewarm_max              = t_max;\n      item.tunnel_prewarm_rate             = t_rate;\n      item.tunnel_prewarm_connect_timeout  = t_connect_timeout;\n      item.tunnel_prewarm_inactive_timeout = t_inactive_timeout;\n      item.tunnel_prewarm_srv              = t_srv;\n\n      if (node[TS_tunnel_alpn]) {\n        load_tunnel_alpn(item.tunnel_alpn, node[TS_tunnel_alpn]);\n      }\n    }\n\n    if (node[TS_verify_server_policy]) {\n      auto value = node[TS_verify_server_policy].as<std::string>();\n      int policy = POLICY_DESCRIPTOR.get(value);\n      if (policy < 0) {\n        throw YAML::ParserException(node[TS_verify_server_policy].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_server_policy = static_cast<YamlSNIConfig::Policy>(policy);\n    }\n\n    if (node[TS_verify_server_properties]) {\n      auto value     = node[TS_verify_server_properties].as<std::string>();\n      int properties = PROPERTIES_DESCRIPTOR.get(value);\n      if (properties < 0) {\n        throw YAML::ParserException(node[TS_verify_server_properties].Mark(), \"unknown value \\\"\" + value + \"\\\"\");\n      }\n      item.verify_server_properties = static_cast<YamlSNIConfig::Property>(properties);\n    }\n\n    if (node[TS_client_cert]) {\n      item.client_cert = node[TS_client_cert].as<std::string>();\n    }\n    if (node[TS_client_key]) {\n      item.client_key = node[TS_client_key].as<std::string>();\n    }\n    if (node[TS_client_sni_policy]) {\n      item.client_sni_policy = node[TS_client_sni_policy].as<std::string>();\n    }\n\n    if (node[TS_ip_allow]) {\n      item.ip_allow = node[TS_ip_allow].as<std::string>();\n    }\n    if (node[TS_valid_tls_versions_in]) {\n      for (unsigned int i = 0; i < node[TS_valid_tls_versions_in].size(); i++) {\n        auto value   = node[TS_valid_tls_versions_in][i].as<std::string>();\n        int protocol = TLS_PROTOCOLS_DESCRIPTOR.get(value);\n        item.EnableProtocol(static_cast<YamlSNIConfig::TLSProtocol>(protocol));\n      }\n    }\n    return true;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,18 @@\n     }\n     if (node[TS_http2_buffer_water_mark]) {\n       item.http2_buffer_water_mark = node[TS_http2_buffer_water_mark].as<int>();\n+    }\n+    if (node[TS_http2_max_settings_frames_per_minute]) {\n+      item.http2_max_settings_frames_per_minute = node[TS_http2_max_settings_frames_per_minute].as<int>();\n+    }\n+    if (node[TS_http2_max_ping_frames_per_minute]) {\n+      item.http2_max_ping_frames_per_minute = node[TS_http2_max_ping_frames_per_minute].as<int>();\n+    }\n+    if (node[TS_http2_max_priority_frames_per_minute]) {\n+      item.http2_max_priority_frames_per_minute = node[TS_http2_max_priority_frames_per_minute].as<int>();\n+    }\n+    if (node[TS_http2_max_rst_stream_frames_per_minute]) {\n+      item.http2_max_rst_stream_frames_per_minute = node[TS_http2_max_rst_stream_frames_per_minute].as<int>();\n     }\n \n     // enum",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    }",
                "    if (node[TS_http2_max_settings_frames_per_minute]) {",
                "      item.http2_max_settings_frames_per_minute = node[TS_http2_max_settings_frames_per_minute].as<int>();",
                "    }",
                "    if (node[TS_http2_max_ping_frames_per_minute]) {",
                "      item.http2_max_ping_frames_per_minute = node[TS_http2_max_ping_frames_per_minute].as<int>();",
                "    }",
                "    if (node[TS_http2_max_priority_frames_per_minute]) {",
                "      item.http2_max_priority_frames_per_minute = node[TS_http2_max_priority_frames_per_minute].as<int>();",
                "    }",
                "    if (node[TS_http2_max_rst_stream_frames_per_minute]) {",
                "      item.http2_max_rst_stream_frames_per_minute = node[TS_http2_max_rst_stream_frames_per_minute].as<int>();"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/Http2ConnectionState::init",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "void\nHttp2ConnectionState::init(Http2CommonSession *ssn)\n{\n  session = ssn;\n\n  if (Http2::initial_window_size < HTTP2_INITIAL_WINDOW_SIZE) {\n    // There is no HTTP/2 specified way to shrink the connection window size\n    // other than to receive data and not send WINDOW_UPDATE frames for a\n    // while.\n    this->_server_rwnd             = HTTP2_INITIAL_WINDOW_SIZE;\n    this->server_rwnd_is_shrinking = true;\n  } else {\n    this->_server_rwnd             = Http2::initial_window_size;\n    this->server_rwnd_is_shrinking = false;\n  }\n\n  local_hpack_handle  = new HpackHandle(HTTP2_HEADER_TABLE_SIZE);\n  remote_hpack_handle = new HpackHandle(HTTP2_HEADER_TABLE_SIZE);\n  if (Http2::stream_priority_enabled) {\n    dependency_tree = new DependencyTree(Http2::max_concurrent_streams_in);\n  }\n\n  _cop = ActivityCop<Http2Stream>(this->mutex, &stream_list, 1);\n  _cop.start();\n}",
        "func": "void\nHttp2ConnectionState::init(Http2CommonSession *ssn)\n{\n  session = ssn;\n\n  if (Http2::initial_window_size < HTTP2_INITIAL_WINDOW_SIZE) {\n    // There is no HTTP/2 specified way to shrink the connection window size\n    // other than to receive data and not send WINDOW_UPDATE frames for a\n    // while.\n    this->_server_rwnd             = HTTP2_INITIAL_WINDOW_SIZE;\n    this->server_rwnd_is_shrinking = true;\n  } else {\n    this->_server_rwnd             = Http2::initial_window_size;\n    this->server_rwnd_is_shrinking = false;\n  }\n\n  local_hpack_handle  = new HpackHandle(HTTP2_HEADER_TABLE_SIZE);\n  remote_hpack_handle = new HpackHandle(HTTP2_HEADER_TABLE_SIZE);\n  if (Http2::stream_priority_enabled) {\n    dependency_tree = new DependencyTree(Http2::max_concurrent_streams_in);\n  }\n\n  configured_max_settings_frames_per_minute   = Http2::max_settings_frames_per_minute;\n  configured_max_ping_frames_per_minute       = Http2::max_ping_frames_per_minute;\n  configured_max_priority_frames_per_minute   = Http2::max_priority_frames_per_minute;\n  configured_max_rst_stream_frames_per_minute = Http2::max_rst_stream_frames_per_minute;\n  if (auto snis = dynamic_cast<TLSSNISupport *>(session->get_netvc()); snis) {\n    if (snis->hints_from_sni.http2_max_settings_frames_per_minute.has_value()) {\n      configured_max_settings_frames_per_minute = snis->hints_from_sni.http2_max_settings_frames_per_minute.value();\n    }\n    if (snis->hints_from_sni.http2_max_ping_frames_per_minute.has_value()) {\n      configured_max_ping_frames_per_minute = snis->hints_from_sni.http2_max_ping_frames_per_minute.value();\n    }\n    if (snis->hints_from_sni.http2_max_priority_frames_per_minute.has_value()) {\n      configured_max_priority_frames_per_minute = snis->hints_from_sni.http2_max_priority_frames_per_minute.value();\n    }\n    if (snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.has_value()) {\n      configured_max_rst_stream_frames_per_minute = snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.value();\n    }\n  }\n\n  _cop = ActivityCop<Http2Stream>(this->mutex, &stream_list, 1);\n  _cop.start();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,25 @@\n     dependency_tree = new DependencyTree(Http2::max_concurrent_streams_in);\n   }\n \n+  configured_max_settings_frames_per_minute   = Http2::max_settings_frames_per_minute;\n+  configured_max_ping_frames_per_minute       = Http2::max_ping_frames_per_minute;\n+  configured_max_priority_frames_per_minute   = Http2::max_priority_frames_per_minute;\n+  configured_max_rst_stream_frames_per_minute = Http2::max_rst_stream_frames_per_minute;\n+  if (auto snis = dynamic_cast<TLSSNISupport *>(session->get_netvc()); snis) {\n+    if (snis->hints_from_sni.http2_max_settings_frames_per_minute.has_value()) {\n+      configured_max_settings_frames_per_minute = snis->hints_from_sni.http2_max_settings_frames_per_minute.value();\n+    }\n+    if (snis->hints_from_sni.http2_max_ping_frames_per_minute.has_value()) {\n+      configured_max_ping_frames_per_minute = snis->hints_from_sni.http2_max_ping_frames_per_minute.value();\n+    }\n+    if (snis->hints_from_sni.http2_max_priority_frames_per_minute.has_value()) {\n+      configured_max_priority_frames_per_minute = snis->hints_from_sni.http2_max_priority_frames_per_minute.value();\n+    }\n+    if (snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.has_value()) {\n+      configured_max_rst_stream_frames_per_minute = snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.value();\n+    }\n+  }\n+\n   _cop = ActivityCop<Http2Stream>(this->mutex, &stream_list, 1);\n   _cop.start();\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  configured_max_settings_frames_per_minute   = Http2::max_settings_frames_per_minute;",
                "  configured_max_ping_frames_per_minute       = Http2::max_ping_frames_per_minute;",
                "  configured_max_priority_frames_per_minute   = Http2::max_priority_frames_per_minute;",
                "  configured_max_rst_stream_frames_per_minute = Http2::max_rst_stream_frames_per_minute;",
                "  if (auto snis = dynamic_cast<TLSSNISupport *>(session->get_netvc()); snis) {",
                "    if (snis->hints_from_sni.http2_max_settings_frames_per_minute.has_value()) {",
                "      configured_max_settings_frames_per_minute = snis->hints_from_sni.http2_max_settings_frames_per_minute.value();",
                "    }",
                "    if (snis->hints_from_sni.http2_max_ping_frames_per_minute.has_value()) {",
                "      configured_max_ping_frames_per_minute = snis->hints_from_sni.http2_max_ping_frames_per_minute.value();",
                "    }",
                "    if (snis->hints_from_sni.http2_max_priority_frames_per_minute.has_value()) {",
                "      configured_max_priority_frames_per_minute = snis->hints_from_sni.http2_max_priority_frames_per_minute.value();",
                "    }",
                "    if (snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.has_value()) {",
                "      configured_max_rst_stream_frames_per_minute = snis->hints_from_sni.http2_max_rst_stream_frames_per_minute.value();",
                "    }",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/rcv_rst_stream_frame",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "static Http2Error\nrcv_rst_stream_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  Http2RstStream rst_stream;\n  char buf[HTTP2_RST_STREAM_LEN];\n  char *end;\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, frame.header().streamid, \"Received RST_STREAM frame\");\n\n  // RST_STREAM frames MUST be associated with a stream.  If a RST_STREAM\n  // frame is received with a stream identifier of 0x0, the recipient MUST\n  // treat this as a connection error (Section 5.4.1) of type\n  // PROTOCOL_ERROR.\n  if (stream_id == 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset access stream with invalid id\");\n  }\n\n  Http2Stream *stream = cstate.find_stream(stream_id);\n  if (stream == nullptr) {\n    if (cstate.is_valid_streamid(stream_id)) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n    } else {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                        \"reset frame bad id stream not found\");\n    }\n  }\n\n  // A RST_STREAM frame with a length other than 4 octets MUST be treated\n  // as a connection error (Section 5.4.1) of type FRAME_SIZE_ERROR.\n  if (frame.header().length != HTTP2_RST_STREAM_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"reset frame wrong length\");\n  }\n\n  if (stream == nullptr || !stream->change_state(frame.header().type, frame.header().flags)) {\n    // If a RST_STREAM frame identifying an idle stream is received, the\n    // recipient MUST treat this as a connection error of type PROTOCOL_ERROR.\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset missing stream or bad stream state\");\n  }\n\n  end = frame.reader()->memcpy(buf, sizeof(buf), 0);\n\n  if (!http2_parse_rst_stream(make_iovec(buf, end - buf), rst_stream)) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset failed to parse\");\n  }\n\n  if (stream != nullptr) {\n    Http2StreamDebug(cstate.session, stream_id, \"RST_STREAM: Error Code: %u\", rst_stream.error_code);\n\n    stream->set_rx_error_code({ProxyErrorClass::TXN, static_cast<uint32_t>(rst_stream.error_code)});\n    stream->initiating_close();\n  }\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "func": "static Http2Error\nrcv_rst_stream_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  Http2RstStream rst_stream;\n  char buf[HTTP2_RST_STREAM_LEN];\n  char *end;\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, frame.header().streamid, \"Received RST_STREAM frame\");\n\n  // RST_STREAM frames MUST be associated with a stream.  If a RST_STREAM\n  // frame is received with a stream identifier of 0x0, the recipient MUST\n  // treat this as a connection error (Section 5.4.1) of type\n  // PROTOCOL_ERROR.\n  if (stream_id == 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset access stream with invalid id\");\n  }\n\n  Http2Stream *stream = cstate.find_stream(stream_id);\n  if (stream == nullptr) {\n    if (cstate.is_valid_streamid(stream_id)) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n    } else {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                        \"reset frame bad id stream not found\");\n    }\n  }\n\n  // A RST_STREAM frame with a length other than 4 octets MUST be treated\n  // as a connection error (Section 5.4.1) of type FRAME_SIZE_ERROR.\n  if (frame.header().length != HTTP2_RST_STREAM_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"reset frame wrong length\");\n  }\n\n  // Update RST_STREAM frame count per minute\n  cstate.increment_received_rst_stream_frame_count();\n  // Close this connection if its RST_STREAM frame count exceeds a limit\n  if (cstate.configured_max_rst_stream_frames_per_minute != 0 &&\n      cstate.get_received_rst_stream_frame_count() > cstate.configured_max_rst_stream_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent RST_STREAM frames: %u frames within a last minute\",\n                     cstate.get_received_settings_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"reset too frequent RST_STREAM frames\");\n  }\n\n  if (stream == nullptr || !stream->change_state(frame.header().type, frame.header().flags)) {\n    // If a RST_STREAM frame identifying an idle stream is received, the\n    // recipient MUST treat this as a connection error of type PROTOCOL_ERROR.\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset missing stream or bad stream state\");\n  }\n\n  end = frame.reader()->memcpy(buf, sizeof(buf), 0);\n\n  if (!http2_parse_rst_stream(make_iovec(buf, end - buf), rst_stream)) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"reset failed to parse\");\n  }\n\n  if (stream != nullptr) {\n    Http2StreamDebug(cstate.session, stream_id, \"RST_STREAM: Error Code: %u\", rst_stream.error_code);\n\n    stream->set_rx_error_code({ProxyErrorClass::TXN, static_cast<uint32_t>(rst_stream.error_code)});\n    stream->initiating_close();\n  }\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -34,6 +34,18 @@\n                       \"reset frame wrong length\");\n   }\n \n+  // Update RST_STREAM frame count per minute\n+  cstate.increment_received_rst_stream_frame_count();\n+  // Close this connection if its RST_STREAM frame count exceeds a limit\n+  if (cstate.configured_max_rst_stream_frames_per_minute != 0 &&\n+      cstate.get_received_rst_stream_frame_count() > cstate.configured_max_rst_stream_frames_per_minute) {\n+    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n+    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent RST_STREAM frames: %u frames within a last minute\",\n+                     cstate.get_received_settings_frame_count());\n+    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n+                      \"reset too frequent RST_STREAM frames\");\n+  }\n+\n   if (stream == nullptr || !stream->change_state(frame.header().type, frame.header().flags)) {\n     // If a RST_STREAM frame identifying an idle stream is received, the\n     // recipient MUST treat this as a connection error of type PROTOCOL_ERROR.",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  // Update RST_STREAM frame count per minute",
                "  cstate.increment_received_rst_stream_frame_count();",
                "  // Close this connection if its RST_STREAM frame count exceeds a limit",
                "  if (cstate.configured_max_rst_stream_frames_per_minute != 0 &&",
                "      cstate.get_received_rst_stream_frame_count() > cstate.configured_max_rst_stream_frames_per_minute) {",
                "    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());",
                "    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent RST_STREAM frames: %u frames within a last minute\",",
                "                     cstate.get_received_settings_frame_count());",
                "    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,",
                "                      \"reset too frequent RST_STREAM frames\");",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/rcv_settings_frame",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "static Http2Error\nrcv_settings_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  Http2SettingsParameter param;\n  char buf[HTTP2_SETTINGS_PARAMETER_LEN];\n  unsigned nbytes               = 0;\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received SETTINGS frame\");\n\n  if (cstate.get_zombie_event()) {\n    Warning(\"Setting frame for zombied session %\" PRId64, cstate.session->get_connection_id());\n  }\n\n  // Update SETTIGNS frame count per minute\n  cstate.increment_received_settings_frame_count();\n  // Close this connection if its SETTINGS frame count exceeds a limit\n  if (cstate.get_received_settings_frame_count() > Http2::max_settings_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent SETTINGS frames: %u frames within a last minute\",\n                     cstate.get_received_settings_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv settings too frequent SETTINGS frames\");\n  }\n\n  // [RFC 7540] 6.5. The stream identifier for a SETTINGS frame MUST be zero.\n  // If an endpoint receives a SETTINGS frame whose stream identifier field is\n  // anything other than 0x0, the endpoint MUST respond with a connection\n  // error (Section 5.4.1) of type PROTOCOL_ERROR.\n  if (stream_id != 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"recv settings stream not 0\");\n  }\n\n  // [RFC 7540] 6.5. Receipt of a SETTINGS frame with the ACK flag set and a\n  // length field value other than 0 MUST be treated as a connection\n  // error of type FRAME_SIZE_ERROR.\n  if (frame.header().flags & HTTP2_FLAGS_SETTINGS_ACK) {\n    if (frame.header().length == 0) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n    } else {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                        \"recv settings ACK header length not 0\");\n    }\n  }\n\n  // A SETTINGS frame with a length other than a multiple of 6 octets MUST\n  // be treated as a connection error (Section 5.4.1) of type\n  // FRAME_SIZE_ERROR.\n  if (frame.header().length % 6 != 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"recv settings header wrong length\");\n  }\n\n  uint32_t n_settings = 0;\n  while (nbytes < frame.header().length) {\n    if (n_settings >= Http2::max_settings_per_frame) {\n      HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED, this_ethread());\n      Http2StreamDebug(cstate.session, stream_id, \"Observed too many settings in a frame\");\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                        \"recv settings too many settings in a frame\");\n    }\n\n    unsigned read_bytes = read_rcv_buffer(buf, sizeof(buf), nbytes, frame);\n\n    if (!http2_parse_settings_parameter(make_iovec(buf, read_bytes), param)) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                        \"recv settings parse failed\");\n    }\n\n    if (!http2_settings_parameter_is_valid(param)) {\n      if (param.id == HTTP2_SETTINGS_INITIAL_WINDOW_SIZE) {\n        return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FLOW_CONTROL_ERROR,\n                          \"recv settings bad initial window size\");\n      } else {\n        return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                          \"recv settings bad param\");\n      }\n    }\n\n    Http2StreamDebug(cstate.session, stream_id, \"   %s : %u\", Http2DebugNames::get_settings_param_name(param.id), param.value);\n\n    // [RFC 7540] 6.9.2. When the value of SETTINGS_INITIAL_WINDOW_SIZE\n    // changes, a receiver MUST adjust the size of all stream flow control\n    // windows that it maintains by the difference between the new value and\n    // the old value.\n    if (param.id == HTTP2_SETTINGS_INITIAL_WINDOW_SIZE) {\n      cstate.update_initial_rwnd(param.value);\n    }\n\n    cstate.client_settings.set(static_cast<Http2SettingsIdentifier>(param.id), param.value);\n    ++n_settings;\n  }\n\n  // Update settings count per minute\n  cstate.increment_received_settings_count(n_settings);\n  // Close this connection if its settings count received exceeds a limit\n  if (cstate.get_received_settings_count() > Http2::max_settings_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent setting changes: %u settings within a last minute\",\n                     cstate.get_received_settings_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv settings too frequent setting changes\");\n  }\n\n  // [RFC 7540] 6.5. Once all values have been applied, the recipient MUST\n  // immediately emit a SETTINGS frame with the ACK flag set.\n  Http2SettingsFrame ack_frame(0, HTTP2_FLAGS_SETTINGS_ACK);\n  cstate.session->xmit(ack_frame);\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "func": "static Http2Error\nrcv_settings_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  Http2SettingsParameter param;\n  char buf[HTTP2_SETTINGS_PARAMETER_LEN];\n  unsigned nbytes               = 0;\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received SETTINGS frame\");\n\n  if (cstate.get_zombie_event()) {\n    Warning(\"Setting frame for zombied session %\" PRId64, cstate.session->get_connection_id());\n  }\n\n  // Update SETTIGNS frame count per minute\n  cstate.increment_received_settings_frame_count();\n  // Close this connection if its SETTINGS frame count exceeds a limit\n  if (cstate.configured_max_settings_frames_per_minute != 0 &&\n      cstate.get_received_settings_frame_count() > cstate.configured_max_settings_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent SETTINGS frames: %u frames within a last minute\",\n                     cstate.get_received_settings_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv settings too frequent SETTINGS frames\");\n  }\n\n  // [RFC 7540] 6.5. The stream identifier for a SETTINGS frame MUST be zero.\n  // If an endpoint receives a SETTINGS frame whose stream identifier field is\n  // anything other than 0x0, the endpoint MUST respond with a connection\n  // error (Section 5.4.1) of type PROTOCOL_ERROR.\n  if (stream_id != 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"recv settings stream not 0\");\n  }\n\n  // [RFC 7540] 6.5. Receipt of a SETTINGS frame with the ACK flag set and a\n  // length field value other than 0 MUST be treated as a connection\n  // error of type FRAME_SIZE_ERROR.\n  if (frame.header().flags & HTTP2_FLAGS_SETTINGS_ACK) {\n    if (frame.header().length == 0) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n    } else {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                        \"recv settings ACK header length not 0\");\n    }\n  }\n\n  // A SETTINGS frame with a length other than a multiple of 6 octets MUST\n  // be treated as a connection error (Section 5.4.1) of type\n  // FRAME_SIZE_ERROR.\n  if (frame.header().length % 6 != 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"recv settings header wrong length\");\n  }\n\n  uint32_t n_settings = 0;\n  while (nbytes < frame.header().length) {\n    if (n_settings >= Http2::max_settings_per_frame) {\n      HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED, this_ethread());\n      Http2StreamDebug(cstate.session, stream_id, \"Observed too many settings in a frame\");\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                        \"recv settings too many settings in a frame\");\n    }\n\n    unsigned read_bytes = read_rcv_buffer(buf, sizeof(buf), nbytes, frame);\n\n    if (!http2_parse_settings_parameter(make_iovec(buf, read_bytes), param)) {\n      return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                        \"recv settings parse failed\");\n    }\n\n    if (!http2_settings_parameter_is_valid(param)) {\n      if (param.id == HTTP2_SETTINGS_INITIAL_WINDOW_SIZE) {\n        return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FLOW_CONTROL_ERROR,\n                          \"recv settings bad initial window size\");\n      } else {\n        return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                          \"recv settings bad param\");\n      }\n    }\n\n    Http2StreamDebug(cstate.session, stream_id, \"   %s : %u\", Http2DebugNames::get_settings_param_name(param.id), param.value);\n\n    // [RFC 7540] 6.9.2. When the value of SETTINGS_INITIAL_WINDOW_SIZE\n    // changes, a receiver MUST adjust the size of all stream flow control\n    // windows that it maintains by the difference between the new value and\n    // the old value.\n    if (param.id == HTTP2_SETTINGS_INITIAL_WINDOW_SIZE) {\n      cstate.update_initial_rwnd(param.value);\n    }\n\n    cstate.client_settings.set(static_cast<Http2SettingsIdentifier>(param.id), param.value);\n    ++n_settings;\n  }\n\n  // Update settings count per minute\n  cstate.increment_received_settings_count(n_settings);\n  // Close this connection if its settings count received exceeds a limit\n  if (cstate.get_received_settings_count() > Http2::max_settings_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent setting changes: %u settings within a last minute\",\n                     cstate.get_received_settings_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv settings too frequent setting changes\");\n  }\n\n  // [RFC 7540] 6.5. Once all values have been applied, the recipient MUST\n  // immediately emit a SETTINGS frame with the ACK flag set.\n  Http2SettingsFrame ack_frame(0, HTTP2_FLAGS_SETTINGS_ACK);\n  cstate.session->xmit(ack_frame);\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,8 @@\n   // Update SETTIGNS frame count per minute\n   cstate.increment_received_settings_frame_count();\n   // Close this connection if its SETTINGS frame count exceeds a limit\n-  if (cstate.get_received_settings_frame_count() > Http2::max_settings_frames_per_minute) {\n+  if (cstate.configured_max_settings_frames_per_minute != 0 &&\n+      cstate.get_received_settings_frame_count() > cstate.configured_max_settings_frames_per_minute) {\n     HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n     Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent SETTINGS frames: %u frames within a last minute\",\n                      cstate.get_received_settings_frame_count());",
        "diff_line_info": {
            "deleted_lines": [
                "  if (cstate.get_received_settings_frame_count() > Http2::max_settings_frames_per_minute) {"
            ],
            "added_lines": [
                "  if (cstate.configured_max_settings_frames_per_minute != 0 &&",
                "      cstate.get_received_settings_frame_count() > cstate.configured_max_settings_frames_per_minute) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/rcv_ping_frame",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "static Http2Error\nrcv_ping_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  uint8_t opaque_data[HTTP2_PING_LEN];\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received PING frame\");\n\n  cstate.schedule_zombie_event();\n\n  //  If a PING frame is received with a stream identifier field value other\n  //  than 0x0, the recipient MUST respond with a connection error of type\n  //  PROTOCOL_ERROR.\n  if (stream_id != 0x0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR, \"ping id not 0\");\n  }\n\n  // Receipt of a PING frame with a length field value other than 8 MUST\n  // be treated as a connection error (Section 5.4.1) of type FRAME_SIZE_ERROR.\n  if (frame.header().length != HTTP2_PING_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"ping bad length\");\n  }\n\n  // Update PING frame count per minute\n  cstate.increment_received_ping_frame_count();\n  // Close this connection if its ping count received exceeds a limit\n  if (cstate.get_received_ping_frame_count() > Http2::max_ping_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent PING frames: %u PING frames within a last minute\",\n                     cstate.get_received_ping_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv ping too frequent PING frame\");\n  }\n\n  // An endpoint MUST NOT respond to PING frames containing this flag.\n  if (frame.header().flags & HTTP2_FLAGS_PING_ACK) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n  }\n\n  frame.reader()->memcpy(opaque_data, HTTP2_PING_LEN, 0);\n\n  // ACK (0x1): An endpoint MUST set this flag in PING responses.\n  cstate.send_ping_frame(stream_id, HTTP2_FLAGS_PING_ACK, opaque_data);\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "func": "static Http2Error\nrcv_ping_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  uint8_t opaque_data[HTTP2_PING_LEN];\n  const Http2StreamId stream_id = frame.header().streamid;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received PING frame\");\n\n  cstate.schedule_zombie_event();\n\n  //  If a PING frame is received with a stream identifier field value other\n  //  than 0x0, the recipient MUST respond with a connection error of type\n  //  PROTOCOL_ERROR.\n  if (stream_id != 0x0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR, \"ping id not 0\");\n  }\n\n  // Receipt of a PING frame with a length field value other than 8 MUST\n  // be treated as a connection error (Section 5.4.1) of type FRAME_SIZE_ERROR.\n  if (frame.header().length != HTTP2_PING_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"ping bad length\");\n  }\n\n  // Update PING frame count per minute\n  cstate.increment_received_ping_frame_count();\n  // Close this connection if its ping count received exceeds a limit\n  if (cstate.configured_max_ping_frames_per_minute != 0 &&\n      cstate.get_received_ping_frame_count() > cstate.configured_max_ping_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent PING frames: %u PING frames within a last minute\",\n                     cstate.get_received_ping_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv ping too frequent PING frame\");\n  }\n\n  // An endpoint MUST NOT respond to PING frames containing this flag.\n  if (frame.header().flags & HTTP2_FLAGS_PING_ACK) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n  }\n\n  frame.reader()->memcpy(opaque_data, HTTP2_PING_LEN, 0);\n\n  // ACK (0x1): An endpoint MUST set this flag in PING responses.\n  cstate.send_ping_frame(stream_id, HTTP2_FLAGS_PING_ACK, opaque_data);\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,7 +25,8 @@\n   // Update PING frame count per minute\n   cstate.increment_received_ping_frame_count();\n   // Close this connection if its ping count received exceeds a limit\n-  if (cstate.get_received_ping_frame_count() > Http2::max_ping_frames_per_minute) {\n+  if (cstate.configured_max_ping_frames_per_minute != 0 &&\n+      cstate.get_received_ping_frame_count() > cstate.configured_max_ping_frames_per_minute) {\n     HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n     Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent PING frames: %u PING frames within a last minute\",\n                      cstate.get_received_ping_frame_count());",
        "diff_line_info": {
            "deleted_lines": [
                "  if (cstate.get_received_ping_frame_count() > Http2::max_ping_frames_per_minute) {"
            ],
            "added_lines": [
                "  if (cstate.configured_max_ping_frames_per_minute != 0 &&",
                "      cstate.get_received_ping_frame_count() > cstate.configured_max_ping_frames_per_minute) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/rcv_priority_frame",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "static Http2Error\nrcv_priority_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  const Http2StreamId stream_id = frame.header().streamid;\n  const uint32_t payload_length = frame.header().length;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received PRIORITY frame\");\n\n  if (cstate.get_zombie_event()) {\n    Warning(\"Priority frame for zombied session %\" PRId64, cstate.session->get_connection_id());\n  }\n\n  // If a PRIORITY frame is received with a stream identifier of 0x0, the\n  // recipient MUST respond with a connection error of type PROTOCOL_ERROR.\n  if (stream_id == 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"priority 0 stream_id\");\n  }\n\n  // A PRIORITY frame with a length other than 5 octets MUST be treated as\n  // a stream error (Section 5.4.2) of type FRAME_SIZE_ERROR.\n  if (payload_length != HTTP2_PRIORITY_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_STREAM, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"priority bad length\");\n  }\n\n  uint8_t buf[HTTP2_PRIORITY_LEN] = {0};\n  frame.reader()->memcpy(buf, HTTP2_PRIORITY_LEN, 0);\n\n  Http2Priority priority;\n  if (!http2_parse_priority_parameter(make_iovec(buf, HTTP2_PRIORITY_LEN), priority)) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"priority parse error\");\n  }\n\n  //  A stream cannot depend on itself.  An endpoint MUST treat this as a stream error of type PROTOCOL_ERROR.\n  if (stream_id == priority.stream_dependency) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_STREAM, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"PRIORITY frame depends on itself\");\n  }\n\n  if (!Http2::stream_priority_enabled) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n  }\n\n  // Update PRIORITY frame count per minute\n  cstate.increment_received_priority_frame_count();\n  // Close this connection if its priority frame count received exceeds a limit\n  if (Http2::max_priority_frames_per_minute != 0 &&\n      cstate.get_received_priority_frame_count() > Http2::max_priority_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent priority changes: %u priority changes within a last minute\",\n                     cstate.get_received_priority_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv priority too frequent priority changes\");\n  }\n\n  Http2StreamDebug(cstate.session, stream_id, \"PRIORITY - dep: %d, weight: %d, excl: %d, tree size: %d\", priority.stream_dependency,\n                   priority.weight, priority.exclusive_flag, cstate.dependency_tree->size());\n\n  Http2DependencyTree::Node *node = cstate.dependency_tree->find(stream_id);\n\n  if (node != nullptr) {\n    // [RFC 7540] 5.3.3 Reprioritization\n    Http2StreamDebug(cstate.session, stream_id, \"Reprioritize\");\n    cstate.dependency_tree->reprioritize(node, priority.stream_dependency, priority.exclusive_flag);\n    if (is_debug_tag_set(\"http2_priority\")) {\n      std::stringstream output;\n      cstate.dependency_tree->dump_tree(output);\n      Debug(\"http2_priority\", \"[%\" PRId64 \"] reprioritize %s\", cstate.session->get_connection_id(), output.str().c_str());\n    }\n  } else {\n    // PRIORITY frame is received before HEADERS frame.\n\n    // Restrict number of inactive node in dependency tree smaller than max_concurrent_streams.\n    // Current number of inactive node is size of tree minus active node count.\n    if (Http2::max_concurrent_streams_in > cstate.dependency_tree->size() - cstate.get_client_stream_count() + 1) {\n      cstate.dependency_tree->add(priority.stream_dependency, stream_id, priority.weight, priority.exclusive_flag, nullptr);\n    }\n  }\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "func": "static Http2Error\nrcv_priority_frame(Http2ConnectionState &cstate, const Http2Frame &frame)\n{\n  const Http2StreamId stream_id = frame.header().streamid;\n  const uint32_t payload_length = frame.header().length;\n\n  Http2StreamDebug(cstate.session, stream_id, \"Received PRIORITY frame\");\n\n  if (cstate.get_zombie_event()) {\n    Warning(\"Priority frame for zombied session %\" PRId64, cstate.session->get_connection_id());\n  }\n\n  // If a PRIORITY frame is received with a stream identifier of 0x0, the\n  // recipient MUST respond with a connection error of type PROTOCOL_ERROR.\n  if (stream_id == 0) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"priority 0 stream_id\");\n  }\n\n  // A PRIORITY frame with a length other than 5 octets MUST be treated as\n  // a stream error (Section 5.4.2) of type FRAME_SIZE_ERROR.\n  if (payload_length != HTTP2_PRIORITY_LEN) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_STREAM, Http2ErrorCode::HTTP2_ERROR_FRAME_SIZE_ERROR,\n                      \"priority bad length\");\n  }\n\n  uint8_t buf[HTTP2_PRIORITY_LEN] = {0};\n  frame.reader()->memcpy(buf, HTTP2_PRIORITY_LEN, 0);\n\n  Http2Priority priority;\n  if (!http2_parse_priority_parameter(make_iovec(buf, HTTP2_PRIORITY_LEN), priority)) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"priority parse error\");\n  }\n\n  //  A stream cannot depend on itself.  An endpoint MUST treat this as a stream error of type PROTOCOL_ERROR.\n  if (stream_id == priority.stream_dependency) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_STREAM, Http2ErrorCode::HTTP2_ERROR_PROTOCOL_ERROR,\n                      \"PRIORITY frame depends on itself\");\n  }\n\n  if (!Http2::stream_priority_enabled) {\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n  }\n\n  // Update PRIORITY frame count per minute\n  cstate.increment_received_priority_frame_count();\n  // Close this connection if its priority frame count received exceeds a limit\n  if (cstate.configured_max_priority_frames_per_minute != 0 &&\n      cstate.get_received_priority_frame_count() > cstate.configured_max_priority_frames_per_minute) {\n    HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n    Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent priority changes: %u priority changes within a last minute\",\n                     cstate.get_received_priority_frame_count());\n    return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_CONNECTION, Http2ErrorCode::HTTP2_ERROR_ENHANCE_YOUR_CALM,\n                      \"recv priority too frequent priority changes\");\n  }\n\n  Http2StreamDebug(cstate.session, stream_id, \"PRIORITY - dep: %d, weight: %d, excl: %d, tree size: %d\", priority.stream_dependency,\n                   priority.weight, priority.exclusive_flag, cstate.dependency_tree->size());\n\n  Http2DependencyTree::Node *node = cstate.dependency_tree->find(stream_id);\n\n  if (node != nullptr) {\n    // [RFC 7540] 5.3.3 Reprioritization\n    Http2StreamDebug(cstate.session, stream_id, \"Reprioritize\");\n    cstate.dependency_tree->reprioritize(node, priority.stream_dependency, priority.exclusive_flag);\n    if (is_debug_tag_set(\"http2_priority\")) {\n      std::stringstream output;\n      cstate.dependency_tree->dump_tree(output);\n      Debug(\"http2_priority\", \"[%\" PRId64 \"] reprioritize %s\", cstate.session->get_connection_id(), output.str().c_str());\n    }\n  } else {\n    // PRIORITY frame is received before HEADERS frame.\n\n    // Restrict number of inactive node in dependency tree smaller than max_concurrent_streams.\n    // Current number of inactive node is size of tree minus active node count.\n    if (Http2::max_concurrent_streams_in > cstate.dependency_tree->size() - cstate.get_client_stream_count() + 1) {\n      cstate.dependency_tree->add(priority.stream_dependency, stream_id, priority.weight, priority.exclusive_flag, nullptr);\n    }\n  }\n\n  return Http2Error(Http2ErrorClass::HTTP2_ERROR_CLASS_NONE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -46,8 +46,8 @@\n   // Update PRIORITY frame count per minute\n   cstate.increment_received_priority_frame_count();\n   // Close this connection if its priority frame count received exceeds a limit\n-  if (Http2::max_priority_frames_per_minute != 0 &&\n-      cstate.get_received_priority_frame_count() > Http2::max_priority_frames_per_minute) {\n+  if (cstate.configured_max_priority_frames_per_minute != 0 &&\n+      cstate.get_received_priority_frame_count() > cstate.configured_max_priority_frames_per_minute) {\n     HTTP2_INCREMENT_THREAD_DYN_STAT(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED, this_ethread());\n     Http2StreamDebug(cstate.session, stream_id, \"Observed too frequent priority changes: %u priority changes within a last minute\",\n                      cstate.get_received_priority_frame_count());",
        "diff_line_info": {
            "deleted_lines": [
                "  if (Http2::max_priority_frames_per_minute != 0 &&",
                "      cstate.get_received_priority_frame_count() > Http2::max_priority_frames_per_minute) {"
            ],
            "added_lines": [
                "  if (cstate.configured_max_priority_frames_per_minute != 0 &&",
                "      cstate.get_received_priority_frame_count() > cstate.configured_max_priority_frames_per_minute) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/SNIConfigParams::load_sni_config",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "int\nSNIConfigParams::load_sni_config()\n{\n  for (auto &item : yaml_sni.items) {\n    auto ai = sni_action_list.emplace(sni_action_list.end());\n    ai->set_glob_name(item.fqdn);\n    Debug(\"ssl\", \"name: %s\", item.fqdn.data());\n\n    // set SNI based actions to be called in the ssl_servername_only callback\n    if (item.offer_h2.has_value()) {\n      ai->actions.push_back(std::make_unique<ControlH2>(item.offer_h2.value()));\n    }\n    if (item.verify_client_level != 255) {\n      ai->actions.push_back(\n        std::make_unique<VerifyClient>(item.verify_client_level, item.verify_client_ca_file, item.verify_client_ca_dir));\n    }\n    if (item.host_sni_policy != 255) {\n      ai->actions.push_back(std::make_unique<HostSniPolicy>(item.host_sni_policy));\n    }\n    if (!item.protocol_unset) {\n      ai->actions.push_back(std::make_unique<TLSValidProtocols>(item.protocol_mask));\n    }\n    if (item.tunnel_destination.length() > 0) {\n      ai->actions.push_back(\n        std::make_unique<TunnelDestination>(item.tunnel_destination, item.tunnel_type, item.tunnel_prewarm, item.tunnel_alpn));\n    }\n    if (!item.client_sni_policy.empty()) {\n      ai->actions.push_back(std::make_unique<OutboundSNIPolicy>(item.client_sni_policy));\n    }\n    if (item.http2_buffer_water_mark.has_value()) {\n      ai->actions.push_back(std::make_unique<HTTP2BufferWaterMark>(item.http2_buffer_water_mark.value()));\n    }\n\n    ai->actions.push_back(std::make_unique<SNI_IpAllow>(item.ip_allow, item.fqdn));\n\n    // set the next hop properties\n    auto nps = next_hop_list.emplace(next_hop_list.end());\n\n    SSLConfig::scoped_config params;\n    // Load if we have at least specified the client certificate\n    if (!item.client_cert.empty()) {\n      nps->prop.client_cert_file = Layout::get()->relative_to(params->clientCertPathOnly, item.client_cert.data());\n      if (!item.client_key.empty()) {\n        nps->prop.client_key_file = Layout::get()->relative_to(params->clientKeyPathOnly, item.client_key.data());\n      }\n\n      auto ctx = params->getCTX(nps->prop.client_cert_file, nps->prop.client_key_file, params->clientCACertFilename,\n                                params->clientCACertPath);\n      if (ctx.get() == nullptr) {\n        return 1;\n      }\n    }\n\n    nps->set_glob_name(item.fqdn);\n    nps->prop.verify_server_policy     = item.verify_server_policy;\n    nps->prop.verify_server_properties = item.verify_server_properties;\n  } // end for\n\n  return 0;\n}",
        "func": "int\nSNIConfigParams::load_sni_config()\n{\n  for (auto &item : yaml_sni.items) {\n    auto ai = sni_action_list.emplace(sni_action_list.end());\n    ai->set_glob_name(item.fqdn);\n    Debug(\"ssl\", \"name: %s\", item.fqdn.data());\n\n    // set SNI based actions to be called in the ssl_servername_only callback\n    if (item.offer_h2.has_value()) {\n      ai->actions.push_back(std::make_unique<ControlH2>(item.offer_h2.value()));\n    }\n    if (item.verify_client_level != 255) {\n      ai->actions.push_back(\n        std::make_unique<VerifyClient>(item.verify_client_level, item.verify_client_ca_file, item.verify_client_ca_dir));\n    }\n    if (item.host_sni_policy != 255) {\n      ai->actions.push_back(std::make_unique<HostSniPolicy>(item.host_sni_policy));\n    }\n    if (!item.protocol_unset) {\n      ai->actions.push_back(std::make_unique<TLSValidProtocols>(item.protocol_mask));\n    }\n    if (item.tunnel_destination.length() > 0) {\n      ai->actions.push_back(\n        std::make_unique<TunnelDestination>(item.tunnel_destination, item.tunnel_type, item.tunnel_prewarm, item.tunnel_alpn));\n    }\n    if (!item.client_sni_policy.empty()) {\n      ai->actions.push_back(std::make_unique<OutboundSNIPolicy>(item.client_sni_policy));\n    }\n    if (item.http2_buffer_water_mark.has_value()) {\n      ai->actions.push_back(std::make_unique<HTTP2BufferWaterMark>(item.http2_buffer_water_mark.value()));\n    }\n    if (item.http2_max_settings_frames_per_minute.has_value()) {\n      ai->actions.push_back(std::make_unique<HTTP2MaxSettingsFramesPerMinute>(item.http2_max_settings_frames_per_minute.value()));\n    }\n    if (item.http2_max_ping_frames_per_minute.has_value()) {\n      ai->actions.push_back(std::make_unique<HTTP2MaxPingFramesPerMinute>(item.http2_max_ping_frames_per_minute.value()));\n    }\n    if (item.http2_max_priority_frames_per_minute.has_value()) {\n      ai->actions.push_back(std::make_unique<HTTP2MaxPriorityFramesPerMinute>(item.http2_max_priority_frames_per_minute.value()));\n    }\n    if (item.http2_max_rst_stream_frames_per_minute.has_value()) {\n      ai->actions.push_back(\n        std::make_unique<HTTP2MaxRstStreamFramesPerMinute>(item.http2_max_rst_stream_frames_per_minute.value()));\n    }\n\n    ai->actions.push_back(std::make_unique<SNI_IpAllow>(item.ip_allow, item.fqdn));\n\n    // set the next hop properties\n    auto nps = next_hop_list.emplace(next_hop_list.end());\n\n    SSLConfig::scoped_config params;\n    // Load if we have at least specified the client certificate\n    if (!item.client_cert.empty()) {\n      nps->prop.client_cert_file = Layout::get()->relative_to(params->clientCertPathOnly, item.client_cert.data());\n      if (!item.client_key.empty()) {\n        nps->prop.client_key_file = Layout::get()->relative_to(params->clientKeyPathOnly, item.client_key.data());\n      }\n\n      auto ctx = params->getCTX(nps->prop.client_cert_file, nps->prop.client_key_file, params->clientCACertFilename,\n                                params->clientCACertPath);\n      if (ctx.get() == nullptr) {\n        return 1;\n      }\n    }\n\n    nps->set_glob_name(item.fqdn);\n    nps->prop.verify_server_policy     = item.verify_server_policy;\n    nps->prop.verify_server_properties = item.verify_server_properties;\n  } // end for\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -30,6 +30,19 @@\n     if (item.http2_buffer_water_mark.has_value()) {\n       ai->actions.push_back(std::make_unique<HTTP2BufferWaterMark>(item.http2_buffer_water_mark.value()));\n     }\n+    if (item.http2_max_settings_frames_per_minute.has_value()) {\n+      ai->actions.push_back(std::make_unique<HTTP2MaxSettingsFramesPerMinute>(item.http2_max_settings_frames_per_minute.value()));\n+    }\n+    if (item.http2_max_ping_frames_per_minute.has_value()) {\n+      ai->actions.push_back(std::make_unique<HTTP2MaxPingFramesPerMinute>(item.http2_max_ping_frames_per_minute.value()));\n+    }\n+    if (item.http2_max_priority_frames_per_minute.has_value()) {\n+      ai->actions.push_back(std::make_unique<HTTP2MaxPriorityFramesPerMinute>(item.http2_max_priority_frames_per_minute.value()));\n+    }\n+    if (item.http2_max_rst_stream_frames_per_minute.has_value()) {\n+      ai->actions.push_back(\n+        std::make_unique<HTTP2MaxRstStreamFramesPerMinute>(item.http2_max_rst_stream_frames_per_minute.value()));\n+    }\n \n     ai->actions.push_back(std::make_unique<SNI_IpAllow>(item.ip_allow, item.fqdn));\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (item.http2_max_settings_frames_per_minute.has_value()) {",
                "      ai->actions.push_back(std::make_unique<HTTP2MaxSettingsFramesPerMinute>(item.http2_max_settings_frames_per_minute.value()));",
                "    }",
                "    if (item.http2_max_ping_frames_per_minute.has_value()) {",
                "      ai->actions.push_back(std::make_unique<HTTP2MaxPingFramesPerMinute>(item.http2_max_ping_frames_per_minute.value()));",
                "    }",
                "    if (item.http2_max_priority_frames_per_minute.has_value()) {",
                "      ai->actions.push_back(std::make_unique<HTTP2MaxPriorityFramesPerMinute>(item.http2_max_priority_frames_per_minute.value()));",
                "    }",
                "    if (item.http2_max_rst_stream_frames_per_minute.has_value()) {",
                "      ai->actions.push_back(",
                "        std::make_unique<HTTP2MaxRstStreamFramesPerMinute>(item.http2_max_rst_stream_frames_per_minute.value()));",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "apache/trafficserver/Http2::init",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/apache/trafficserver/commit/74b91ae4a5379126d6db9d31f419448b4388a579",
        "commit_title": "Add an HTTP/2 related rate limiting",
        "commit_text": "",
        "func_before": "void\nHttp2::init()\n{\n  REC_EstablishStaticConfigInt32U(max_concurrent_streams_in, \"proxy.config.http2.max_concurrent_streams_in\");\n  REC_EstablishStaticConfigInt32U(min_concurrent_streams_in, \"proxy.config.http2.min_concurrent_streams_in\");\n  REC_EstablishStaticConfigInt32U(max_active_streams_in, \"proxy.config.http2.max_active_streams_in\");\n  REC_EstablishStaticConfigInt32U(stream_priority_enabled, \"proxy.config.http2.stream_priority_enabled\");\n  REC_EstablishStaticConfigInt32U(initial_window_size, \"proxy.config.http2.initial_window_size_in\");\n  REC_EstablishStaticConfigInt32U(max_frame_size, \"proxy.config.http2.max_frame_size\");\n  REC_EstablishStaticConfigInt32U(header_table_size, \"proxy.config.http2.header_table_size\");\n  REC_EstablishStaticConfigInt32U(max_header_list_size, \"proxy.config.http2.max_header_list_size\");\n  REC_EstablishStaticConfigInt32U(accept_no_activity_timeout, \"proxy.config.http2.accept_no_activity_timeout\");\n  REC_EstablishStaticConfigInt32U(no_activity_timeout_in, \"proxy.config.http2.no_activity_timeout_in\");\n  REC_EstablishStaticConfigInt32U(active_timeout_in, \"proxy.config.http2.active_timeout_in\");\n  REC_EstablishStaticConfigInt32U(push_diary_size, \"proxy.config.http2.push_diary_size\");\n  REC_EstablishStaticConfigInt32U(zombie_timeout_in, \"proxy.config.http2.zombie_debug_timeout_in\");\n  REC_EstablishStaticConfigFloat(stream_error_rate_threshold, \"proxy.config.http2.stream_error_rate_threshold\");\n  REC_EstablishStaticConfigInt32U(stream_error_sampling_threshold, \"proxy.config.http2.stream_error_sampling_threshold\");\n  REC_EstablishStaticConfigInt32U(max_settings_per_frame, \"proxy.config.http2.max_settings_per_frame\");\n  REC_EstablishStaticConfigInt32U(max_settings_per_minute, \"proxy.config.http2.max_settings_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_settings_frames_per_minute, \"proxy.config.http2.max_settings_frames_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_ping_frames_per_minute, \"proxy.config.http2.max_ping_frames_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_priority_frames_per_minute, \"proxy.config.http2.max_priority_frames_per_minute\");\n  REC_EstablishStaticConfigFloat(min_avg_window_update, \"proxy.config.http2.min_avg_window_update\");\n  REC_EstablishStaticConfigInt32U(con_slow_log_threshold, \"proxy.config.http2.connection.slow.log.threshold\");\n  REC_EstablishStaticConfigInt32U(stream_slow_log_threshold, \"proxy.config.http2.stream.slow.log.threshold\");\n  REC_EstablishStaticConfigInt32U(header_table_size_limit, \"proxy.config.http2.header_table_size_limit\");\n  REC_EstablishStaticConfigInt32U(write_buffer_block_size, \"proxy.config.http2.write_buffer_block_size\");\n  REC_EstablishStaticConfigFloat(write_size_threshold, \"proxy.config.http2.write_size_threshold\");\n  REC_EstablishStaticConfigInt32U(write_time_threshold, \"proxy.config.http2.write_time_threshold\");\n  REC_EstablishStaticConfigInt32U(buffer_water_mark, \"proxy.config.http2.default_buffer_water_mark\");\n\n  // If any settings is broken, ATS should not start\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_CONCURRENT_STREAMS, max_concurrent_streams_in}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_CONCURRENT_STREAMS, min_concurrent_streams_in}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_INITIAL_WINDOW_SIZE, initial_window_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_FRAME_SIZE, max_frame_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_HEADER_TABLE_SIZE, header_table_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_HEADER_LIST_SIZE, max_header_list_size}));\n\n#define HTTP2_CLEAR_DYN_STAT(x)          \\\n  do {                                   \\\n    RecSetRawStatSum(http2_rsb, x, 0);   \\\n    RecSetRawStatCount(http2_rsb, x, 0); \\\n  } while (0);\n\n  // Setup statistics\n  http2_rsb = RecAllocateRawStatBlock(static_cast<int>(HTTP2_N_STATS));\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_CLIENT_CONNECTION_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_CLIENT_SESSION_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_CLIENT_SESSION_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_CLIENT_STREAM_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_CLIENT_STREAM_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_CLIENT_STREAM_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_CLIENT_STREAM_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_CLIENT_STREAM_COUNT), RecRawStatSyncCount);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_TRANSACTIONS_TIME_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_TRANSACTIONS_TIME), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_CLIENT_CONNECTION_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_CLIENT_CONNECTION_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CONNECTION_ERRORS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CONNECTION_ERRORS_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_STREAM_ERRORS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_STREAM_ERRORS_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_DEFAULT_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_DEFAULT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_OTHER_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_OTHER), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_EOS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_EOS), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_ACTIVE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_ACTIVE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_INACTIVE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_INACTIVE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_ERROR_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_ERROR), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_HIGH_ERROR_RATE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_HIGH_ERROR_RATE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_IN_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_IN), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_OUT_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_OUT), RecRawStatSyncSum);\n\n  http2_init();\n}",
        "func": "void\nHttp2::init()\n{\n  REC_EstablishStaticConfigInt32U(max_concurrent_streams_in, \"proxy.config.http2.max_concurrent_streams_in\");\n  REC_EstablishStaticConfigInt32U(min_concurrent_streams_in, \"proxy.config.http2.min_concurrent_streams_in\");\n  REC_EstablishStaticConfigInt32U(max_active_streams_in, \"proxy.config.http2.max_active_streams_in\");\n  REC_EstablishStaticConfigInt32U(stream_priority_enabled, \"proxy.config.http2.stream_priority_enabled\");\n  REC_EstablishStaticConfigInt32U(initial_window_size, \"proxy.config.http2.initial_window_size_in\");\n  REC_EstablishStaticConfigInt32U(max_frame_size, \"proxy.config.http2.max_frame_size\");\n  REC_EstablishStaticConfigInt32U(header_table_size, \"proxy.config.http2.header_table_size\");\n  REC_EstablishStaticConfigInt32U(max_header_list_size, \"proxy.config.http2.max_header_list_size\");\n  REC_EstablishStaticConfigInt32U(accept_no_activity_timeout, \"proxy.config.http2.accept_no_activity_timeout\");\n  REC_EstablishStaticConfigInt32U(no_activity_timeout_in, \"proxy.config.http2.no_activity_timeout_in\");\n  REC_EstablishStaticConfigInt32U(active_timeout_in, \"proxy.config.http2.active_timeout_in\");\n  REC_EstablishStaticConfigInt32U(push_diary_size, \"proxy.config.http2.push_diary_size\");\n  REC_EstablishStaticConfigInt32U(zombie_timeout_in, \"proxy.config.http2.zombie_debug_timeout_in\");\n  REC_EstablishStaticConfigFloat(stream_error_rate_threshold, \"proxy.config.http2.stream_error_rate_threshold\");\n  REC_EstablishStaticConfigInt32U(stream_error_sampling_threshold, \"proxy.config.http2.stream_error_sampling_threshold\");\n  REC_EstablishStaticConfigInt32U(max_settings_per_frame, \"proxy.config.http2.max_settings_per_frame\");\n  REC_EstablishStaticConfigInt32U(max_settings_per_minute, \"proxy.config.http2.max_settings_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_settings_frames_per_minute, \"proxy.config.http2.max_settings_frames_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_ping_frames_per_minute, \"proxy.config.http2.max_ping_frames_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_priority_frames_per_minute, \"proxy.config.http2.max_priority_frames_per_minute\");\n  REC_EstablishStaticConfigInt32U(max_rst_stream_frames_per_minute, \"proxy.config.http2.max_rst_stream_frames_per_minute\");\n  REC_EstablishStaticConfigFloat(min_avg_window_update, \"proxy.config.http2.min_avg_window_update\");\n  REC_EstablishStaticConfigInt32U(con_slow_log_threshold, \"proxy.config.http2.connection.slow.log.threshold\");\n  REC_EstablishStaticConfigInt32U(stream_slow_log_threshold, \"proxy.config.http2.stream.slow.log.threshold\");\n  REC_EstablishStaticConfigInt32U(header_table_size_limit, \"proxy.config.http2.header_table_size_limit\");\n  REC_EstablishStaticConfigInt32U(write_buffer_block_size, \"proxy.config.http2.write_buffer_block_size\");\n  REC_EstablishStaticConfigFloat(write_size_threshold, \"proxy.config.http2.write_size_threshold\");\n  REC_EstablishStaticConfigInt32U(write_time_threshold, \"proxy.config.http2.write_time_threshold\");\n  REC_EstablishStaticConfigInt32U(buffer_water_mark, \"proxy.config.http2.default_buffer_water_mark\");\n\n  // If any settings is broken, ATS should not start\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_CONCURRENT_STREAMS, max_concurrent_streams_in}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_CONCURRENT_STREAMS, min_concurrent_streams_in}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_INITIAL_WINDOW_SIZE, initial_window_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_FRAME_SIZE, max_frame_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_HEADER_TABLE_SIZE, header_table_size}));\n  ink_release_assert(http2_settings_parameter_is_valid({HTTP2_SETTINGS_MAX_HEADER_LIST_SIZE, max_header_list_size}));\n\n#define HTTP2_CLEAR_DYN_STAT(x)          \\\n  do {                                   \\\n    RecSetRawStatSum(http2_rsb, x, 0);   \\\n    RecSetRawStatCount(http2_rsb, x, 0); \\\n  } while (0);\n\n  // Setup statistics\n  http2_rsb = RecAllocateRawStatBlock(static_cast<int>(HTTP2_N_STATS));\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_CLIENT_CONNECTION_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_CLIENT_SESSION_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_CLIENT_SESSION_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_ACTIVE_CLIENT_CONNECTION_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CURRENT_CLIENT_STREAM_NAME, RECD_INT, RECP_NON_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CURRENT_CLIENT_STREAM_COUNT), RecRawStatSyncSum);\n  HTTP2_CLEAR_DYN_STAT(HTTP2_STAT_CURRENT_CLIENT_STREAM_COUNT);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_CLIENT_STREAM_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_CLIENT_STREAM_COUNT), RecRawStatSyncCount);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_TRANSACTIONS_TIME_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_TRANSACTIONS_TIME), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_TOTAL_CLIENT_CONNECTION_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_TOTAL_CLIENT_CONNECTION_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_CONNECTION_ERRORS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_CONNECTION_ERRORS_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_STREAM_ERRORS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_STREAM_ERRORS_COUNT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_DEFAULT_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_DEFAULT), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_OTHER_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_OTHER), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_EOS_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_EOS), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_ACTIVE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_ACTIVE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_INACTIVE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_INACTIVE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_ERROR_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_ERROR), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_SESSION_DIE_HIGH_ERROR_RATE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_SESSION_DIE_HIGH_ERROR_RATE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_PER_FRAME_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_SETTINGS_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_IN_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_IN), RecRawStatSyncSum);\n  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_OUT_NAME, RECD_INT, RECP_PERSISTENT,\n                     static_cast<int>(HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_OUT), RecRawStatSyncSum);\n\n  http2_init();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,7 @@\n   REC_EstablishStaticConfigInt32U(max_settings_frames_per_minute, \"proxy.config.http2.max_settings_frames_per_minute\");\n   REC_EstablishStaticConfigInt32U(max_ping_frames_per_minute, \"proxy.config.http2.max_ping_frames_per_minute\");\n   REC_EstablishStaticConfigInt32U(max_priority_frames_per_minute, \"proxy.config.http2.max_priority_frames_per_minute\");\n+  REC_EstablishStaticConfigInt32U(max_rst_stream_frames_per_minute, \"proxy.config.http2.max_rst_stream_frames_per_minute\");\n   REC_EstablishStaticConfigFloat(min_avg_window_update, \"proxy.config.http2.min_avg_window_update\");\n   REC_EstablishStaticConfigInt32U(con_slow_log_threshold, \"proxy.config.http2.connection.slow.log.threshold\");\n   REC_EstablishStaticConfigInt32U(stream_slow_log_threshold, \"proxy.config.http2.stream.slow.log.threshold\");\n@@ -89,6 +90,8 @@\n                      static_cast<int>(HTTP2_STAT_MAX_PING_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n   RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n                      static_cast<int>(HTTP2_STAT_MAX_PRIORITY_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n+  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,\n+                     static_cast<int>(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);\n   RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE_NAME, RECD_INT, RECP_PERSISTENT,\n                      static_cast<int>(HTTP2_STAT_INSUFFICIENT_AVG_WINDOW_UPDATE), RecRawStatSyncSum);\n   RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_CONCURRENT_STREAMS_EXCEEDED_IN_NAME, RECD_INT, RECP_PERSISTENT,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  REC_EstablishStaticConfigInt32U(max_rst_stream_frames_per_minute, \"proxy.config.http2.max_rst_stream_frames_per_minute\");",
                "  RecRegisterRawStat(http2_rsb, RECT_PROCESS, HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED_NAME, RECD_INT, RECP_PERSISTENT,",
                "                     static_cast<int>(HTTP2_STAT_MAX_RST_STREAM_FRAMES_PER_MINUTE_EXCEEDED), RecRawStatSyncSum);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nodejs/node/nghttp2_session_add_goaway",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nodejs/node/commit/d14b7acc3d972b4becb2b3a27a868935eaea36b1",
        "commit_title": "deps: update nghttp2 to 1.57.0",
        "commit_text": "",
        "func_before": "int nghttp2_session_add_goaway(nghttp2_session *session, int32_t last_stream_id,\n                               uint32_t error_code, const uint8_t *opaque_data,\n                               size_t opaque_data_len, uint8_t aux_flags) {\n  int rv;\n  nghttp2_outbound_item *item;\n  nghttp2_frame *frame;\n  uint8_t *opaque_data_copy = NULL;\n  nghttp2_goaway_aux_data *aux_data;\n  nghttp2_mem *mem;\n\n  mem = &session->mem;\n\n  if (nghttp2_session_is_my_stream_id(session, last_stream_id)) {\n    return NGHTTP2_ERR_INVALID_ARGUMENT;\n  }\n\n  if (opaque_data_len) {\n    if (opaque_data_len + 8 > NGHTTP2_MAX_PAYLOADLEN) {\n      return NGHTTP2_ERR_INVALID_ARGUMENT;\n    }\n    opaque_data_copy = nghttp2_mem_malloc(mem, opaque_data_len);\n    if (opaque_data_copy == NULL) {\n      return NGHTTP2_ERR_NOMEM;\n    }\n    memcpy(opaque_data_copy, opaque_data, opaque_data_len);\n  }\n\n  item = nghttp2_mem_malloc(mem, sizeof(nghttp2_outbound_item));\n  if (item == NULL) {\n    nghttp2_mem_free(mem, opaque_data_copy);\n    return NGHTTP2_ERR_NOMEM;\n  }\n\n  nghttp2_outbound_item_init(item);\n\n  frame = &item->frame;\n\n  /* last_stream_id must not be increased from the value previously\n     sent */\n  last_stream_id = nghttp2_min(last_stream_id, session->local_last_stream_id);\n\n  nghttp2_frame_goaway_init(&frame->goaway, last_stream_id, error_code,\n                            opaque_data_copy, opaque_data_len);\n\n  aux_data = &item->aux_data.goaway;\n  aux_data->flags = aux_flags;\n\n  rv = nghttp2_session_add_item(session, item);\n  if (rv != 0) {\n    nghttp2_frame_goaway_free(&frame->goaway, mem);\n    nghttp2_mem_free(mem, item);\n    return rv;\n  }\n  return 0;\n}",
        "func": "int nghttp2_session_add_goaway(nghttp2_session *session, int32_t last_stream_id,\n                               uint32_t error_code, const uint8_t *opaque_data,\n                               size_t opaque_data_len, uint8_t aux_flags) {\n  int rv;\n  nghttp2_outbound_item *item;\n  nghttp2_frame *frame;\n  uint8_t *opaque_data_copy = NULL;\n  nghttp2_goaway_aux_data *aux_data;\n  nghttp2_mem *mem;\n\n  mem = &session->mem;\n\n  if (nghttp2_session_is_my_stream_id(session, last_stream_id)) {\n    return NGHTTP2_ERR_INVALID_ARGUMENT;\n  }\n\n  if (opaque_data_len) {\n    if (opaque_data_len + 8 > NGHTTP2_MAX_PAYLOADLEN) {\n      return NGHTTP2_ERR_INVALID_ARGUMENT;\n    }\n    opaque_data_copy = nghttp2_mem_malloc(mem, opaque_data_len);\n    if (opaque_data_copy == NULL) {\n      return NGHTTP2_ERR_NOMEM;\n    }\n    memcpy(opaque_data_copy, opaque_data, opaque_data_len);\n  }\n\n  item = nghttp2_mem_malloc(mem, sizeof(nghttp2_outbound_item));\n  if (item == NULL) {\n    nghttp2_mem_free(mem, opaque_data_copy);\n    return NGHTTP2_ERR_NOMEM;\n  }\n\n  nghttp2_outbound_item_init(item);\n\n  frame = &item->frame;\n\n  /* last_stream_id must not be increased from the value previously\n     sent */\n  last_stream_id = nghttp2_min(last_stream_id, session->local_last_stream_id);\n\n  nghttp2_frame_goaway_init(&frame->goaway, last_stream_id, error_code,\n                            opaque_data_copy, opaque_data_len);\n\n  aux_data = &item->aux_data.goaway;\n  aux_data->flags = aux_flags;\n\n  rv = nghttp2_session_add_item(session, item);\n  if (rv != 0) {\n    nghttp2_frame_goaway_free(&frame->goaway, mem);\n    nghttp2_mem_free(mem, item);\n    return rv;\n  }\n\n  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,5 +51,8 @@\n     nghttp2_mem_free(mem, item);\n     return rv;\n   }\n+\n+  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;\n+\n   return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nodejs/node/session_new",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nodejs/node/commit/d14b7acc3d972b4becb2b3a27a868935eaea36b1",
        "commit_title": "deps: update nghttp2 to 1.57.0",
        "commit_text": "",
        "func_before": "static int session_new(nghttp2_session **session_ptr,\n                       const nghttp2_session_callbacks *callbacks,\n                       void *user_data, int server,\n                       const nghttp2_option *option, nghttp2_mem *mem) {\n  int rv;\n  size_t nbuffer;\n  size_t max_deflate_dynamic_table_size =\n      NGHTTP2_HD_DEFAULT_MAX_DEFLATE_BUFFER_SIZE;\n  size_t i;\n\n  if (mem == NULL) {\n    mem = nghttp2_mem_default();\n  }\n\n  *session_ptr = nghttp2_mem_calloc(mem, 1, sizeof(nghttp2_session));\n  if (*session_ptr == NULL) {\n    rv = NGHTTP2_ERR_NOMEM;\n    goto fail_session;\n  }\n\n  (*session_ptr)->mem = *mem;\n  mem = &(*session_ptr)->mem;\n\n  /* next_stream_id is initialized in either\n     nghttp2_session_client_new2 or nghttp2_session_server_new2 */\n\n  nghttp2_stream_init(&(*session_ptr)->root, 0, NGHTTP2_STREAM_FLAG_NONE,\n                      NGHTTP2_STREAM_IDLE, NGHTTP2_DEFAULT_WEIGHT, 0, 0, NULL,\n                      mem);\n\n  (*session_ptr)->remote_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n  (*session_ptr)->recv_window_size = 0;\n  (*session_ptr)->consumed_size = 0;\n  (*session_ptr)->recv_reduction = 0;\n  (*session_ptr)->local_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n\n  (*session_ptr)->goaway_flags = NGHTTP2_GOAWAY_NONE;\n  (*session_ptr)->local_last_stream_id = (1u << 31) - 1;\n  (*session_ptr)->remote_last_stream_id = (1u << 31) - 1;\n\n  (*session_ptr)->pending_local_max_concurrent_stream =\n      NGHTTP2_DEFAULT_MAX_CONCURRENT_STREAMS;\n  (*session_ptr)->pending_enable_push = 1;\n  (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n\n  if (server) {\n    (*session_ptr)->server = 1;\n  }\n\n  init_settings(&(*session_ptr)->remote_settings);\n  init_settings(&(*session_ptr)->local_settings);\n\n  (*session_ptr)->max_incoming_reserved_streams =\n      NGHTTP2_MAX_INCOMING_RESERVED_STREAMS;\n\n  /* Limit max outgoing concurrent streams to sensible value */\n  (*session_ptr)->remote_settings.max_concurrent_streams = 100;\n\n  (*session_ptr)->max_send_header_block_length = NGHTTP2_MAX_HEADERSLEN;\n  (*session_ptr)->max_outbound_ack = NGHTTP2_DEFAULT_MAX_OBQ_FLOOD_ITEM;\n  (*session_ptr)->max_settings = NGHTTP2_DEFAULT_MAX_SETTINGS;\n\n  if (option) {\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_WINDOW_UPDATE) &&\n        option->no_auto_window_update) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_WINDOW_UPDATE;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_PEER_MAX_CONCURRENT_STREAMS) {\n\n      (*session_ptr)->remote_settings.max_concurrent_streams =\n          option->peer_max_concurrent_streams;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_RESERVED_REMOTE_STREAMS) {\n\n      (*session_ptr)->max_incoming_reserved_streams =\n          option->max_reserved_remote_streams;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_RECV_CLIENT_MAGIC) &&\n        option->no_recv_client_magic) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_HTTP_MESSAGING) &&\n        option->no_http_messaging) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_HTTP_MESSAGING;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_USER_RECV_EXT_TYPES) {\n      memcpy((*session_ptr)->user_recv_ext_types, option->user_recv_ext_types,\n             sizeof((*session_ptr)->user_recv_ext_types));\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_BUILTIN_RECV_EXT_TYPES) {\n      (*session_ptr)->builtin_recv_ext_types = option->builtin_recv_ext_types;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_PING_ACK) &&\n        option->no_auto_ping_ack) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_PING_ACK;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_SEND_HEADER_BLOCK_LENGTH) {\n      (*session_ptr)->max_send_header_block_length =\n          option->max_send_header_block_length;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_DEFLATE_DYNAMIC_TABLE_SIZE) {\n      max_deflate_dynamic_table_size = option->max_deflate_dynamic_table_size;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_CLOSED_STREAMS) &&\n        option->no_closed_streams) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_CLOSED_STREAMS;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_OUTBOUND_ACK) {\n      (*session_ptr)->max_outbound_ack = option->max_outbound_ack;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_MAX_SETTINGS) &&\n        option->max_settings) {\n      (*session_ptr)->max_settings = option->max_settings;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_SERVER_FALLBACK_RFC7540_PRIORITIES) &&\n        option->server_fallback_rfc7540_priorities) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_SERVER_FALLBACK_RFC7540_PRIORITIES;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION) &&\n        option->no_rfc9113_leading_and_trailing_ws_validation) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n    }\n  }\n\n  rv = nghttp2_hd_deflate_init2(&(*session_ptr)->hd_deflater,\n                                max_deflate_dynamic_table_size, mem);\n  if (rv != 0) {\n    goto fail_hd_deflater;\n  }\n  rv = nghttp2_hd_inflate_init(&(*session_ptr)->hd_inflater, mem);\n  if (rv != 0) {\n    goto fail_hd_inflater;\n  }\n\n  nbuffer = ((*session_ptr)->max_send_header_block_length +\n             NGHTTP2_FRAMEBUF_CHUNKLEN - 1) /\n            NGHTTP2_FRAMEBUF_CHUNKLEN;\n\n  if (nbuffer == 0) {\n    nbuffer = 1;\n  }\n\n  /* 1 for Pad Field. */\n  rv = nghttp2_bufs_init3(&(*session_ptr)->aob.framebufs,\n                          NGHTTP2_FRAMEBUF_CHUNKLEN, nbuffer, 1,\n                          NGHTTP2_FRAME_HDLEN + 1, mem);\n  if (rv != 0) {\n    goto fail_aob_framebuf;\n  }\n\n  nghttp2_map_init(&(*session_ptr)->streams, mem);\n\n  active_outbound_item_reset(&(*session_ptr)->aob, mem);\n\n  (*session_ptr)->callbacks = *callbacks;\n  (*session_ptr)->user_data = user_data;\n\n  session_inbound_frame_reset(*session_ptr);\n\n  if (nghttp2_enable_strict_preface) {\n    nghttp2_inbound_frame *iframe = &(*session_ptr)->iframe;\n\n    if (server && ((*session_ptr)->opt_flags &\n                   NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC) == 0) {\n      iframe->state = NGHTTP2_IB_READ_CLIENT_MAGIC;\n      iframe->payloadleft = NGHTTP2_CLIENT_MAGIC_LEN;\n    } else {\n      iframe->state = NGHTTP2_IB_READ_FIRST_SETTINGS;\n    }\n\n    if (!server) {\n      (*session_ptr)->aob.state = NGHTTP2_OB_SEND_CLIENT_MAGIC;\n      nghttp2_bufs_add(&(*session_ptr)->aob.framebufs, NGHTTP2_CLIENT_MAGIC,\n                       NGHTTP2_CLIENT_MAGIC_LEN);\n    }\n  }\n\n  for (i = 0; i < NGHTTP2_EXTPRI_URGENCY_LEVELS; ++i) {\n    nghttp2_pq_init(&(*session_ptr)->sched[i].ob_data, stream_less, mem);\n  }\n\n  return 0;\n\nfail_aob_framebuf:\n  nghttp2_hd_inflate_free(&(*session_ptr)->hd_inflater);\nfail_hd_inflater:\n  nghttp2_hd_deflate_free(&(*session_ptr)->hd_deflater);\nfail_hd_deflater:\n  nghttp2_mem_free(mem, *session_ptr);\nfail_session:\n  return rv;\n}",
        "func": "static int session_new(nghttp2_session **session_ptr,\n                       const nghttp2_session_callbacks *callbacks,\n                       void *user_data, int server,\n                       const nghttp2_option *option, nghttp2_mem *mem) {\n  int rv;\n  size_t nbuffer;\n  size_t max_deflate_dynamic_table_size =\n      NGHTTP2_HD_DEFAULT_MAX_DEFLATE_BUFFER_SIZE;\n  size_t i;\n\n  if (mem == NULL) {\n    mem = nghttp2_mem_default();\n  }\n\n  *session_ptr = nghttp2_mem_calloc(mem, 1, sizeof(nghttp2_session));\n  if (*session_ptr == NULL) {\n    rv = NGHTTP2_ERR_NOMEM;\n    goto fail_session;\n  }\n\n  (*session_ptr)->mem = *mem;\n  mem = &(*session_ptr)->mem;\n\n  /* next_stream_id is initialized in either\n     nghttp2_session_client_new2 or nghttp2_session_server_new2 */\n\n  nghttp2_stream_init(&(*session_ptr)->root, 0, NGHTTP2_STREAM_FLAG_NONE,\n                      NGHTTP2_STREAM_IDLE, NGHTTP2_DEFAULT_WEIGHT, 0, 0, NULL,\n                      mem);\n\n  (*session_ptr)->remote_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n  (*session_ptr)->recv_window_size = 0;\n  (*session_ptr)->consumed_size = 0;\n  (*session_ptr)->recv_reduction = 0;\n  (*session_ptr)->local_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n\n  (*session_ptr)->goaway_flags = NGHTTP2_GOAWAY_NONE;\n  (*session_ptr)->local_last_stream_id = (1u << 31) - 1;\n  (*session_ptr)->remote_last_stream_id = (1u << 31) - 1;\n\n  (*session_ptr)->pending_local_max_concurrent_stream =\n      NGHTTP2_DEFAULT_MAX_CONCURRENT_STREAMS;\n  (*session_ptr)->pending_enable_push = 1;\n  (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n\n  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,\n                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);\n\n  if (server) {\n    (*session_ptr)->server = 1;\n  }\n\n  init_settings(&(*session_ptr)->remote_settings);\n  init_settings(&(*session_ptr)->local_settings);\n\n  (*session_ptr)->max_incoming_reserved_streams =\n      NGHTTP2_MAX_INCOMING_RESERVED_STREAMS;\n\n  /* Limit max outgoing concurrent streams to sensible value */\n  (*session_ptr)->remote_settings.max_concurrent_streams = 100;\n\n  (*session_ptr)->max_send_header_block_length = NGHTTP2_MAX_HEADERSLEN;\n  (*session_ptr)->max_outbound_ack = NGHTTP2_DEFAULT_MAX_OBQ_FLOOD_ITEM;\n  (*session_ptr)->max_settings = NGHTTP2_DEFAULT_MAX_SETTINGS;\n\n  if (option) {\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_WINDOW_UPDATE) &&\n        option->no_auto_window_update) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_WINDOW_UPDATE;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_PEER_MAX_CONCURRENT_STREAMS) {\n\n      (*session_ptr)->remote_settings.max_concurrent_streams =\n          option->peer_max_concurrent_streams;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_RESERVED_REMOTE_STREAMS) {\n\n      (*session_ptr)->max_incoming_reserved_streams =\n          option->max_reserved_remote_streams;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_RECV_CLIENT_MAGIC) &&\n        option->no_recv_client_magic) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_HTTP_MESSAGING) &&\n        option->no_http_messaging) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_HTTP_MESSAGING;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_USER_RECV_EXT_TYPES) {\n      memcpy((*session_ptr)->user_recv_ext_types, option->user_recv_ext_types,\n             sizeof((*session_ptr)->user_recv_ext_types));\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_BUILTIN_RECV_EXT_TYPES) {\n      (*session_ptr)->builtin_recv_ext_types = option->builtin_recv_ext_types;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_PING_ACK) &&\n        option->no_auto_ping_ack) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_PING_ACK;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_SEND_HEADER_BLOCK_LENGTH) {\n      (*session_ptr)->max_send_header_block_length =\n          option->max_send_header_block_length;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_DEFLATE_DYNAMIC_TABLE_SIZE) {\n      max_deflate_dynamic_table_size = option->max_deflate_dynamic_table_size;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_CLOSED_STREAMS) &&\n        option->no_closed_streams) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_CLOSED_STREAMS;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_OUTBOUND_ACK) {\n      (*session_ptr)->max_outbound_ack = option->max_outbound_ack;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_MAX_SETTINGS) &&\n        option->max_settings) {\n      (*session_ptr)->max_settings = option->max_settings;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_SERVER_FALLBACK_RFC7540_PRIORITIES) &&\n        option->server_fallback_rfc7540_priorities) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_SERVER_FALLBACK_RFC7540_PRIORITIES;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION) &&\n        option->no_rfc9113_leading_and_trailing_ws_validation) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {\n      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n                           option->stream_reset_burst,\n                           option->stream_reset_rate);\n    }\n  }\n\n  rv = nghttp2_hd_deflate_init2(&(*session_ptr)->hd_deflater,\n                                max_deflate_dynamic_table_size, mem);\n  if (rv != 0) {\n    goto fail_hd_deflater;\n  }\n  rv = nghttp2_hd_inflate_init(&(*session_ptr)->hd_inflater, mem);\n  if (rv != 0) {\n    goto fail_hd_inflater;\n  }\n\n  nbuffer = ((*session_ptr)->max_send_header_block_length +\n             NGHTTP2_FRAMEBUF_CHUNKLEN - 1) /\n            NGHTTP2_FRAMEBUF_CHUNKLEN;\n\n  if (nbuffer == 0) {\n    nbuffer = 1;\n  }\n\n  /* 1 for Pad Field. */\n  rv = nghttp2_bufs_init3(&(*session_ptr)->aob.framebufs,\n                          NGHTTP2_FRAMEBUF_CHUNKLEN, nbuffer, 1,\n                          NGHTTP2_FRAME_HDLEN + 1, mem);\n  if (rv != 0) {\n    goto fail_aob_framebuf;\n  }\n\n  nghttp2_map_init(&(*session_ptr)->streams, mem);\n\n  active_outbound_item_reset(&(*session_ptr)->aob, mem);\n\n  (*session_ptr)->callbacks = *callbacks;\n  (*session_ptr)->user_data = user_data;\n\n  session_inbound_frame_reset(*session_ptr);\n\n  if (nghttp2_enable_strict_preface) {\n    nghttp2_inbound_frame *iframe = &(*session_ptr)->iframe;\n\n    if (server && ((*session_ptr)->opt_flags &\n                   NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC) == 0) {\n      iframe->state = NGHTTP2_IB_READ_CLIENT_MAGIC;\n      iframe->payloadleft = NGHTTP2_CLIENT_MAGIC_LEN;\n    } else {\n      iframe->state = NGHTTP2_IB_READ_FIRST_SETTINGS;\n    }\n\n    if (!server) {\n      (*session_ptr)->aob.state = NGHTTP2_OB_SEND_CLIENT_MAGIC;\n      nghttp2_bufs_add(&(*session_ptr)->aob.framebufs, NGHTTP2_CLIENT_MAGIC,\n                       NGHTTP2_CLIENT_MAGIC_LEN);\n    }\n  }\n\n  for (i = 0; i < NGHTTP2_EXTPRI_URGENCY_LEVELS; ++i) {\n    nghttp2_pq_init(&(*session_ptr)->sched[i].ob_data, stream_less, mem);\n  }\n\n  return 0;\n\nfail_aob_framebuf:\n  nghttp2_hd_inflate_free(&(*session_ptr)->hd_inflater);\nfail_hd_inflater:\n  nghttp2_hd_deflate_free(&(*session_ptr)->hd_deflater);\nfail_hd_deflater:\n  nghttp2_mem_free(mem, *session_ptr);\nfail_session:\n  return rv;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,6 +43,10 @@\n   (*session_ptr)->pending_enable_push = 1;\n   (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n \n+  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n+                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,\n+                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);\n+\n   if (server) {\n     (*session_ptr)->server = 1;\n   }\n@@ -140,6 +144,12 @@\n         option->no_rfc9113_leading_and_trailing_ws_validation) {\n       (*session_ptr)->opt_flags |=\n           NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n+    }\n+\n+    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {\n+      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n+                           option->stream_reset_burst,\n+                           option->stream_reset_rate);\n     }\n   }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,",
                "                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,",
                "                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);",
                "",
                "    }",
                "",
                "    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {",
                "      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,",
                "                           option->stream_reset_burst,",
                "                           option->stream_reset_rate);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nodejs/node/nghttp2_session_on_rst_stream_received",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nodejs/node/commit/d14b7acc3d972b4becb2b3a27a868935eaea36b1",
        "commit_title": "deps: update nghttp2 to 1.57.0",
        "commit_text": "",
        "func_before": "int nghttp2_session_on_rst_stream_received(nghttp2_session *session,\n                                           nghttp2_frame *frame) {\n  int rv;\n  nghttp2_stream *stream;\n  if (frame->hd.stream_id == 0) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream_id == 0\");\n  }\n\n  if (session_detect_idle_stream(session, frame->hd.stream_id)) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream in idle\");\n  }\n\n  stream = nghttp2_session_get_stream(session, frame->hd.stream_id);\n  if (stream) {\n    /* We may use stream->shut_flags for strict error checking. */\n    nghttp2_stream_shutdown(stream, NGHTTP2_SHUT_RD);\n  }\n\n  rv = session_call_on_frame_received(session, frame);\n  if (rv != 0) {\n    return rv;\n  }\n  rv = nghttp2_session_close_stream(session, frame->hd.stream_id,\n                                    frame->rst_stream.error_code);\n  if (nghttp2_is_fatal(rv)) {\n    return rv;\n  }\n  return 0;\n}",
        "func": "int nghttp2_session_on_rst_stream_received(nghttp2_session *session,\n                                           nghttp2_frame *frame) {\n  int rv;\n  nghttp2_stream *stream;\n  if (frame->hd.stream_id == 0) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream_id == 0\");\n  }\n\n  if (session_detect_idle_stream(session, frame->hd.stream_id)) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream in idle\");\n  }\n\n  stream = nghttp2_session_get_stream(session, frame->hd.stream_id);\n  if (stream) {\n    /* We may use stream->shut_flags for strict error checking. */\n    nghttp2_stream_shutdown(stream, NGHTTP2_SHUT_RD);\n  }\n\n  rv = session_call_on_frame_received(session, frame);\n  if (rv != 0) {\n    return rv;\n  }\n  rv = nghttp2_session_close_stream(session, frame->hd.stream_id,\n                                    frame->rst_stream.error_code);\n  if (nghttp2_is_fatal(rv)) {\n    return rv;\n  }\n\n  return session_update_stream_reset_ratelim(session);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,5 +27,6 @@\n   if (nghttp2_is_fatal(rv)) {\n     return rv;\n   }\n-  return 0;\n+\n+  return session_update_stream_reset_ratelim(session);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  return 0;"
            ],
            "added_lines": [
                "",
                "  return session_update_stream_reset_ratelim(session);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "alibaba/tengine/ngx_http_v2_read_handler",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/alibaba/tengine/commit/9330174ba9ba2c1e186a9ffa10ae962ccb9280f1",
        "commit_title": "HTTP/2: per-iteration stream handling limit.",
        "commit_text": "",
        "func_before": "static void\nngx_http_v2_read_handler(ngx_event_t *rev)\n{\n    u_char                    *p, *end;\n    size_t                     available;\n    ssize_t                    n;\n    ngx_connection_t          *c;\n    ngx_http_v2_main_conf_t   *h2mcf;\n    ngx_http_v2_connection_t  *h2c;\n\n    c = rev->data;\n    h2c = c->data;\n\n    if (rev->timedout) {\n        ngx_log_error(NGX_LOG_INFO, c->log, NGX_ETIMEDOUT, \"client timed out\");\n        ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n        return;\n    }\n\n    ngx_log_debug0(NGX_LOG_DEBUG_HTTP, c->log, 0, \"http2 read handler\");\n\n    h2c->blocked = 1;\n\n    if (c->close) {\n        c->close = 0;\n\n        if (c->error) {\n            ngx_http_v2_finalize_connection(h2c, 0);\n            return;\n        }\n\n        if (!h2c->processing && !h2c->pushing) {\n            ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_NO_ERROR);\n            return;\n        }\n\n        if (!h2c->goaway) {\n            h2c->goaway = 1;\n\n            if (ngx_http_v2_send_goaway(h2c, NGX_HTTP_V2_NO_ERROR)\n                == NGX_ERROR)\n            {\n                ngx_http_v2_finalize_connection(h2c, 0);\n                return;\n            }\n\n            if (ngx_http_v2_send_output_queue(h2c) == NGX_ERROR) {\n                ngx_http_v2_finalize_connection(h2c, 0);\n                return;\n            }\n        }\n\n        h2c->blocked = 0;\n\n        return;\n    }\n\n    h2mcf = ngx_http_get_module_main_conf(h2c->http_connection->conf_ctx,\n                                          ngx_http_v2_module);\n\n    available = h2mcf->recv_buffer_size - 2 * NGX_HTTP_V2_STATE_BUFFER_SIZE;\n\n    do {\n        p = h2mcf->recv_buffer;\n\n        ngx_memcpy(p, h2c->state.buffer, NGX_HTTP_V2_STATE_BUFFER_SIZE);\n        end = p + h2c->state.buffer_used;\n\n        n = c->recv(c, end, available);\n\n        if (n == NGX_AGAIN) {\n            break;\n        }\n\n        if (n == 0\n            && (h2c->state.incomplete || h2c->processing || h2c->pushing))\n        {\n            ngx_log_error(NGX_LOG_INFO, c->log, 0,\n                          \"client prematurely closed connection\");\n        }\n\n        if (n == 0 || n == NGX_ERROR) {\n            c->error = 1;\n            ngx_http_v2_finalize_connection(h2c, 0);\n            return;\n        }\n\n        end += n;\n\n        h2c->state.buffer_used = 0;\n        h2c->state.incomplete = 0;\n\n        do {\n            p = h2c->state.handler(h2c, p, end);\n\n            if (p == NULL) {\n                return;\n            }\n\n        } while (p != end);\n\n        h2c->total_bytes += n;\n\n        if (h2c->total_bytes / 8 > h2c->payload_bytes + 1048576) {\n            ngx_log_error(NGX_LOG_INFO, c->log, 0, \"http2 flood detected\");\n            ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_NO_ERROR);\n            return;\n        }\n\n    } while (rev->ready);\n\n    if (ngx_handle_read_event(rev, 0) != NGX_OK) {\n        ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n        return;\n    }\n\n    if (h2c->last_out && ngx_http_v2_send_output_queue(h2c) == NGX_ERROR) {\n        ngx_http_v2_finalize_connection(h2c, 0);\n        return;\n    }\n\n    h2c->blocked = 0;\n\n    ngx_http_v2_handle_connection(h2c);\n}",
        "func": "static void\nngx_http_v2_read_handler(ngx_event_t *rev)\n{\n    u_char                    *p, *end;\n    size_t                     available;\n    ssize_t                    n;\n    ngx_connection_t          *c;\n    ngx_http_v2_main_conf_t   *h2mcf;\n    ngx_http_v2_connection_t  *h2c;\n\n    c = rev->data;\n    h2c = c->data;\n\n    if (rev->timedout) {\n        ngx_log_error(NGX_LOG_INFO, c->log, NGX_ETIMEDOUT, \"client timed out\");\n        ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n        return;\n    }\n\n    ngx_log_debug0(NGX_LOG_DEBUG_HTTP, c->log, 0, \"http2 read handler\");\n\n    h2c->blocked = 1;\n    h2c->new_streams = 0;\n\n    if (c->close) {\n        c->close = 0;\n\n        if (c->error) {\n            ngx_http_v2_finalize_connection(h2c, 0);\n            return;\n        }\n\n        if (!h2c->processing && !h2c->pushing) {\n            ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_NO_ERROR);\n            return;\n        }\n\n        if (!h2c->goaway) {\n            h2c->goaway = 1;\n\n            if (ngx_http_v2_send_goaway(h2c, NGX_HTTP_V2_NO_ERROR)\n                == NGX_ERROR)\n            {\n                ngx_http_v2_finalize_connection(h2c, 0);\n                return;\n            }\n\n            if (ngx_http_v2_send_output_queue(h2c) == NGX_ERROR) {\n                ngx_http_v2_finalize_connection(h2c, 0);\n                return;\n            }\n        }\n\n        h2c->blocked = 0;\n\n        return;\n    }\n\n    h2mcf = ngx_http_get_module_main_conf(h2c->http_connection->conf_ctx,\n                                          ngx_http_v2_module);\n\n    available = h2mcf->recv_buffer_size - 2 * NGX_HTTP_V2_STATE_BUFFER_SIZE;\n\n    do {\n        p = h2mcf->recv_buffer;\n\n        ngx_memcpy(p, h2c->state.buffer, NGX_HTTP_V2_STATE_BUFFER_SIZE);\n        end = p + h2c->state.buffer_used;\n\n        n = c->recv(c, end, available);\n\n        if (n == NGX_AGAIN) {\n            break;\n        }\n\n        if (n == 0\n            && (h2c->state.incomplete || h2c->processing || h2c->pushing))\n        {\n            ngx_log_error(NGX_LOG_INFO, c->log, 0,\n                          \"client prematurely closed connection\");\n        }\n\n        if (n == 0 || n == NGX_ERROR) {\n            c->error = 1;\n            ngx_http_v2_finalize_connection(h2c, 0);\n            return;\n        }\n\n        end += n;\n\n        h2c->state.buffer_used = 0;\n        h2c->state.incomplete = 0;\n\n        do {\n            p = h2c->state.handler(h2c, p, end);\n\n            if (p == NULL) {\n                return;\n            }\n\n        } while (p != end);\n\n        h2c->total_bytes += n;\n\n        if (h2c->total_bytes / 8 > h2c->payload_bytes + 1048576) {\n            ngx_log_error(NGX_LOG_INFO, c->log, 0, \"http2 flood detected\");\n            ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_NO_ERROR);\n            return;\n        }\n\n    } while (rev->ready);\n\n    if (ngx_handle_read_event(rev, 0) != NGX_OK) {\n        ngx_http_v2_finalize_connection(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n        return;\n    }\n\n    if (h2c->last_out && ngx_http_v2_send_output_queue(h2c) == NGX_ERROR) {\n        ngx_http_v2_finalize_connection(h2c, 0);\n        return;\n    }\n\n    h2c->blocked = 0;\n\n    ngx_http_v2_handle_connection(h2c);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,7 @@\n     ngx_log_debug0(NGX_LOG_DEBUG_HTTP, c->log, 0, \"http2 read handler\");\n \n     h2c->blocked = 1;\n+    h2c->new_streams = 0;\n \n     if (c->close) {\n         c->close = 0;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    h2c->new_streams = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "alibaba/tengine/ngx_http_v2_state_headers",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/alibaba/tengine/commit/9330174ba9ba2c1e186a9ffa10ae962ccb9280f1",
        "commit_title": "HTTP/2: per-iteration stream handling limit.",
        "commit_text": "",
        "func_before": "static u_char *\nngx_http_v2_state_headers(ngx_http_v2_connection_t *h2c, u_char *pos,\n    u_char *end)\n{\n    size_t                     size;\n    ngx_uint_t                 padded, priority, depend, dependency, excl,\n                               weight;\n    ngx_uint_t                 status;\n    ngx_http_v2_node_t        *node;\n    ngx_http_v2_stream_t      *stream;\n    ngx_http_v2_srv_conf_t    *h2scf;\n    ngx_http_core_srv_conf_t  *cscf;\n    ngx_http_core_loc_conf_t  *clcf;\n\n    padded = h2c->state.flags & NGX_HTTP_V2_PADDED_FLAG;\n    priority = h2c->state.flags & NGX_HTTP_V2_PRIORITY_FLAG;\n\n    size = 0;\n\n    if (padded) {\n        size++;\n    }\n\n    if (priority) {\n        size += sizeof(uint32_t) + 1;\n    }\n\n    if (h2c->state.length < size) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with incorrect length %uz\",\n                      h2c->state.length);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_SIZE_ERROR);\n    }\n\n    if (h2c->state.length == size) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with empty header block\");\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_SIZE_ERROR);\n    }\n\n    if (h2c->goaway) {\n        ngx_log_debug0(NGX_LOG_DEBUG_HTTP, h2c->connection->log, 0,\n                       \"skipping http2 HEADERS frame\");\n        return ngx_http_v2_state_skip(h2c, pos, end);\n    }\n\n    if ((size_t) (end - pos) < size) {\n        return ngx_http_v2_state_save(h2c, pos, end,\n                                      ngx_http_v2_state_headers);\n    }\n\n    h2c->state.length -= size;\n\n    if (padded) {\n        h2c->state.padding = *pos++;\n\n        if (h2c->state.padding > h2c->state.length) {\n            ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                          \"client sent padded HEADERS frame \"\n                          \"with incorrect length: %uz, padding: %uz\",\n                          h2c->state.length, h2c->state.padding);\n\n            return ngx_http_v2_connection_error(h2c,\n                                                NGX_HTTP_V2_PROTOCOL_ERROR);\n        }\n\n        h2c->state.length -= h2c->state.padding;\n    }\n\n    depend = 0;\n    excl = 0;\n    weight = NGX_HTTP_V2_DEFAULT_WEIGHT;\n\n    if (priority) {\n        dependency = ngx_http_v2_parse_uint32(pos);\n\n        depend = dependency & 0x7fffffff;\n        excl = dependency >> 31;\n        weight = pos[4] + 1;\n\n        pos += sizeof(uint32_t) + 1;\n    }\n\n    ngx_log_debug4(NGX_LOG_DEBUG_HTTP, h2c->connection->log, 0,\n                   \"http2 HEADERS frame sid:%ui \"\n                   \"depends on %ui excl:%ui weight:%ui\",\n                   h2c->state.sid, depend, excl, weight);\n\n    if (h2c->state.sid % 2 == 0 || h2c->state.sid <= h2c->last_sid) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with incorrect identifier \"\n                      \"%ui, the last was %ui\", h2c->state.sid, h2c->last_sid);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n    }\n\n    if (depend == h2c->state.sid) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame for stream %ui \"\n                      \"with incorrect dependency\", h2c->state.sid);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n    }\n\n    h2c->last_sid = h2c->state.sid;\n\n    h2c->state.pool = ngx_create_pool(1024, h2c->connection->log);\n    if (h2c->state.pool == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    cscf = ngx_http_get_module_srv_conf(h2c->http_connection->conf_ctx,\n                                        ngx_http_core_module);\n\n    h2c->state.header_limit = cscf->large_client_header_buffers.size\n                              * cscf->large_client_header_buffers.num;\n\n    h2scf = ngx_http_get_module_srv_conf(h2c->http_connection->conf_ctx,\n                                         ngx_http_v2_module);\n\n    if (h2c->processing >= h2scf->concurrent_streams) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"concurrent streams exceeded %ui\", h2c->processing);\n\n        status = NGX_HTTP_V2_REFUSED_STREAM;\n        goto rst_stream;\n    }\n\n    if (!h2c->settings_ack\n        && !(h2c->state.flags & NGX_HTTP_V2_END_STREAM_FLAG)\n        && h2scf->preread_size < NGX_HTTP_V2_DEFAULT_WINDOW)\n    {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent stream with data \"\n                      \"before settings were acknowledged\");\n\n        status = NGX_HTTP_V2_REFUSED_STREAM;\n        goto rst_stream;\n    }\n\n    node = ngx_http_v2_get_node_by_id(h2c, h2c->state.sid, 1);\n\n    if (node == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    if (node->parent) {\n        ngx_queue_remove(&node->reuse);\n        h2c->closed_nodes--;\n    }\n\n    stream = ngx_http_v2_create_stream(h2c, 0);\n    if (stream == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    h2c->state.stream = stream;\n\n    stream->pool = h2c->state.pool;\n    h2c->state.keep_pool = 1;\n\n    stream->request->request_length = h2c->state.length;\n\n    stream->in_closed = h2c->state.flags & NGX_HTTP_V2_END_STREAM_FLAG;\n    stream->node = node;\n\n    node->stream = stream;\n\n    if (priority || node->parent == NULL) {\n        node->weight = weight;\n        ngx_http_v2_set_dependency(h2c, node, depend, excl);\n    }\n\n    clcf = ngx_http_get_module_loc_conf(h2c->http_connection->conf_ctx,\n                                        ngx_http_core_module);\n\n    if (clcf->keepalive_timeout == 0\n        || h2c->connection->requests >= clcf->keepalive_requests\n        || ngx_current_msec - h2c->connection->start_time\n           > clcf->keepalive_time)\n    {\n        h2c->goaway = 1;\n\n        if (ngx_http_v2_send_goaway(h2c, NGX_HTTP_V2_NO_ERROR) == NGX_ERROR) {\n            return ngx_http_v2_connection_error(h2c,\n                                                NGX_HTTP_V2_INTERNAL_ERROR);\n        }\n    }\n\n    return ngx_http_v2_state_header_block(h2c, pos, end);\n\nrst_stream:\n\n    if (ngx_http_v2_send_rst_stream(h2c, h2c->state.sid, status) != NGX_OK) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    return ngx_http_v2_state_header_block(h2c, pos, end);\n}",
        "func": "static u_char *\nngx_http_v2_state_headers(ngx_http_v2_connection_t *h2c, u_char *pos,\n    u_char *end)\n{\n    size_t                     size;\n    ngx_uint_t                 padded, priority, depend, dependency, excl,\n                               weight;\n    ngx_uint_t                 status;\n    ngx_http_v2_node_t        *node;\n    ngx_http_v2_stream_t      *stream;\n    ngx_http_v2_srv_conf_t    *h2scf;\n    ngx_http_core_srv_conf_t  *cscf;\n    ngx_http_core_loc_conf_t  *clcf;\n\n    padded = h2c->state.flags & NGX_HTTP_V2_PADDED_FLAG;\n    priority = h2c->state.flags & NGX_HTTP_V2_PRIORITY_FLAG;\n\n    size = 0;\n\n    if (padded) {\n        size++;\n    }\n\n    if (priority) {\n        size += sizeof(uint32_t) + 1;\n    }\n\n    if (h2c->state.length < size) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with incorrect length %uz\",\n                      h2c->state.length);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_SIZE_ERROR);\n    }\n\n    if (h2c->state.length == size) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with empty header block\");\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_SIZE_ERROR);\n    }\n\n    if (h2c->goaway) {\n        ngx_log_debug0(NGX_LOG_DEBUG_HTTP, h2c->connection->log, 0,\n                       \"skipping http2 HEADERS frame\");\n        return ngx_http_v2_state_skip(h2c, pos, end);\n    }\n\n    if ((size_t) (end - pos) < size) {\n        return ngx_http_v2_state_save(h2c, pos, end,\n                                      ngx_http_v2_state_headers);\n    }\n\n    h2c->state.length -= size;\n\n    if (padded) {\n        h2c->state.padding = *pos++;\n\n        if (h2c->state.padding > h2c->state.length) {\n            ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                          \"client sent padded HEADERS frame \"\n                          \"with incorrect length: %uz, padding: %uz\",\n                          h2c->state.length, h2c->state.padding);\n\n            return ngx_http_v2_connection_error(h2c,\n                                                NGX_HTTP_V2_PROTOCOL_ERROR);\n        }\n\n        h2c->state.length -= h2c->state.padding;\n    }\n\n    depend = 0;\n    excl = 0;\n    weight = NGX_HTTP_V2_DEFAULT_WEIGHT;\n\n    if (priority) {\n        dependency = ngx_http_v2_parse_uint32(pos);\n\n        depend = dependency & 0x7fffffff;\n        excl = dependency >> 31;\n        weight = pos[4] + 1;\n\n        pos += sizeof(uint32_t) + 1;\n    }\n\n    ngx_log_debug4(NGX_LOG_DEBUG_HTTP, h2c->connection->log, 0,\n                   \"http2 HEADERS frame sid:%ui \"\n                   \"depends on %ui excl:%ui weight:%ui\",\n                   h2c->state.sid, depend, excl, weight);\n\n    if (h2c->state.sid % 2 == 0 || h2c->state.sid <= h2c->last_sid) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame with incorrect identifier \"\n                      \"%ui, the last was %ui\", h2c->state.sid, h2c->last_sid);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n    }\n\n    if (depend == h2c->state.sid) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent HEADERS frame for stream %ui \"\n                      \"with incorrect dependency\", h2c->state.sid);\n\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_PROTOCOL_ERROR);\n    }\n\n    h2c->last_sid = h2c->state.sid;\n\n    h2c->state.pool = ngx_create_pool(1024, h2c->connection->log);\n    if (h2c->state.pool == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    cscf = ngx_http_get_module_srv_conf(h2c->http_connection->conf_ctx,\n                                        ngx_http_core_module);\n\n    h2c->state.header_limit = cscf->large_client_header_buffers.size\n                              * cscf->large_client_header_buffers.num;\n\n    h2scf = ngx_http_get_module_srv_conf(h2c->http_connection->conf_ctx,\n                                         ngx_http_v2_module);\n\n    if (h2c->processing >= h2scf->concurrent_streams) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"concurrent streams exceeded %ui\", h2c->processing);\n\n        status = NGX_HTTP_V2_REFUSED_STREAM;\n        goto rst_stream;\n    }\n\n    if (h2c->new_streams++ >= 2 * h2scf->concurrent_streams) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent too many streams at once\");\n\n        status = NGX_HTTP_V2_REFUSED_STREAM;\n        goto rst_stream;\n    }\n\n    if (!h2c->settings_ack\n        && !(h2c->state.flags & NGX_HTTP_V2_END_STREAM_FLAG)\n        && h2scf->preread_size < NGX_HTTP_V2_DEFAULT_WINDOW)\n    {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent stream with data \"\n                      \"before settings were acknowledged\");\n\n        status = NGX_HTTP_V2_REFUSED_STREAM;\n        goto rst_stream;\n    }\n\n    node = ngx_http_v2_get_node_by_id(h2c, h2c->state.sid, 1);\n\n    if (node == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    if (node->parent) {\n        ngx_queue_remove(&node->reuse);\n        h2c->closed_nodes--;\n    }\n\n    stream = ngx_http_v2_create_stream(h2c, 0);\n    if (stream == NULL) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    h2c->state.stream = stream;\n\n    stream->pool = h2c->state.pool;\n    h2c->state.keep_pool = 1;\n\n    stream->request->request_length = h2c->state.length;\n\n    stream->in_closed = h2c->state.flags & NGX_HTTP_V2_END_STREAM_FLAG;\n    stream->node = node;\n\n    node->stream = stream;\n\n    if (priority || node->parent == NULL) {\n        node->weight = weight;\n        ngx_http_v2_set_dependency(h2c, node, depend, excl);\n    }\n\n    clcf = ngx_http_get_module_loc_conf(h2c->http_connection->conf_ctx,\n                                        ngx_http_core_module);\n\n    if (clcf->keepalive_timeout == 0\n        || h2c->connection->requests >= clcf->keepalive_requests\n        || ngx_current_msec - h2c->connection->start_time\n           > clcf->keepalive_time)\n    {\n        h2c->goaway = 1;\n\n        if (ngx_http_v2_send_goaway(h2c, NGX_HTTP_V2_NO_ERROR) == NGX_ERROR) {\n            return ngx_http_v2_connection_error(h2c,\n                                                NGX_HTTP_V2_INTERNAL_ERROR);\n        }\n    }\n\n    return ngx_http_v2_state_header_block(h2c, pos, end);\n\nrst_stream:\n\n    if (h2c->refused_streams++ > ngx_max(h2scf->concurrent_streams, 100)) {\n        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n                      \"client sent too many refused streams\");\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_NO_ERROR);\n    }\n\n    if (ngx_http_v2_send_rst_stream(h2c, h2c->state.sid, status) != NGX_OK) {\n        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n    }\n\n    return ngx_http_v2_state_header_block(h2c, pos, end);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -128,6 +128,14 @@\n         goto rst_stream;\n     }\n \n+    if (h2c->new_streams++ >= 2 * h2scf->concurrent_streams) {\n+        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n+                      \"client sent too many streams at once\");\n+\n+        status = NGX_HTTP_V2_REFUSED_STREAM;\n+        goto rst_stream;\n+    }\n+\n     if (!h2c->settings_ack\n         && !(h2c->state.flags & NGX_HTTP_V2_END_STREAM_FLAG)\n         && h2scf->preread_size < NGX_HTTP_V2_DEFAULT_WINDOW)\n@@ -193,6 +201,12 @@\n \n rst_stream:\n \n+    if (h2c->refused_streams++ > ngx_max(h2scf->concurrent_streams, 100)) {\n+        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,\n+                      \"client sent too many refused streams\");\n+        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_NO_ERROR);\n+    }\n+\n     if (ngx_http_v2_send_rst_stream(h2c, h2c->state.sid, status) != NGX_OK) {\n         return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_INTERNAL_ERROR);\n     }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (h2c->new_streams++ >= 2 * h2scf->concurrent_streams) {",
                "        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,",
                "                      \"client sent too many streams at once\");",
                "",
                "        status = NGX_HTTP_V2_REFUSED_STREAM;",
                "        goto rst_stream;",
                "    }",
                "",
                "    if (h2c->refused_streams++ > ngx_max(h2scf->concurrent_streams, 100)) {",
                "        ngx_log_error(NGX_LOG_INFO, h2c->connection->log, 0,",
                "                      \"client sent too many refused streams\");",
                "        return ngx_http_v2_connection_error(h2c, NGX_HTTP_V2_NO_ERROR);",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/h2o_config_init",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "void h2o_config_init(h2o_globalconf_t *config)\n{\n    memset(config, 0, sizeof(*config));\n    config->hosts = h2o_mem_alloc(sizeof(config->hosts[0]));\n    config->hosts[0] = NULL;\n    h2o_linklist_init_anchor(&config->configurators);\n    config->server_name = h2o_iovec_init(H2O_STRLIT(\"h2o/\" H2O_VERSION));\n    config->max_request_entity_size = H2O_DEFAULT_MAX_REQUEST_ENTITY_SIZE;\n    config->max_delegations = H2O_DEFAULT_MAX_DELEGATIONS;\n    config->max_reprocesses = H2O_DEFAULT_MAX_REPROCESSES;\n    config->handshake_timeout = H2O_DEFAULT_HANDSHAKE_TIMEOUT;\n    config->http1.req_timeout = H2O_DEFAULT_HTTP1_REQ_TIMEOUT;\n    config->http1.req_io_timeout = H2O_DEFAULT_HTTP1_REQ_IO_TIMEOUT;\n    config->http1.upgrade_to_http2 = H2O_DEFAULT_HTTP1_UPGRADE_TO_HTTP2;\n    config->http2.idle_timeout = H2O_DEFAULT_HTTP2_IDLE_TIMEOUT;\n    config->http2.graceful_shutdown_timeout = H2O_DEFAULT_HTTP2_GRACEFUL_SHUTDOWN_TIMEOUT;\n    config->proxy.io_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.connect_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.first_byte_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.emit_x_forwarded_headers = 1;\n    config->proxy.emit_via_header = 1;\n    config->proxy.emit_missing_date_header = 1;\n    config->proxy.zerocopy = H2O_PROXY_ZEROCOPY_ENABLED;\n    config->http2.max_concurrent_requests_per_connection = H2O_HTTP2_SETTINGS_HOST_MAX_CONCURRENT_STREAMS;\n    config->http2.max_concurrent_streaming_requests_per_connection = H2O_HTTP2_SETTINGS_HOST_MAX_CONCURRENT_STREAMING_REQUESTS;\n    config->http2.max_streams_for_priority = 16;\n    config->http2.active_stream_window_size = H2O_DEFAULT_HTTP2_ACTIVE_STREAM_WINDOW_SIZE;\n    config->http2.latency_optimization.min_rtt = 50; // milliseconds\n    config->http2.latency_optimization.max_additional_delay = 10;\n    config->http2.latency_optimization.max_cwnd = 65535;\n    config->http3.idle_timeout = quicly_spec_context.transport_params.max_idle_timeout;\n    config->http3.active_stream_window_size = H2O_DEFAULT_HTTP3_ACTIVE_STREAM_WINDOW_SIZE;\n    config->http3.allow_delayed_ack = 1;\n    config->http3.use_gso = 1;\n    config->send_informational_mode = H2O_SEND_INFORMATIONAL_MODE_EXCEPT_H1;\n    config->mimemap = h2o_mimemap_create();\n    h2o_socketpool_init_global(&config->proxy.global_socketpool, SIZE_MAX);\n\n    h2o_configurator__init_core(config);\n\n    config->fallback_host = create_hostconf(config);\n    config->fallback_host->authority.port = 65535;\n    config->fallback_host->authority.host = h2o_strdup(NULL, H2O_STRLIT(\"*\"));\n    config->fallback_host->authority.hostport = h2o_strdup(NULL, H2O_STRLIT(\"*\"));\n}",
        "func": "void h2o_config_init(h2o_globalconf_t *config)\n{\n    memset(config, 0, sizeof(*config));\n    config->hosts = h2o_mem_alloc(sizeof(config->hosts[0]));\n    config->hosts[0] = NULL;\n    h2o_linklist_init_anchor(&config->configurators);\n    config->server_name = h2o_iovec_init(H2O_STRLIT(\"h2o/\" H2O_VERSION));\n    config->max_request_entity_size = H2O_DEFAULT_MAX_REQUEST_ENTITY_SIZE;\n    config->max_delegations = H2O_DEFAULT_MAX_DELEGATIONS;\n    config->max_reprocesses = H2O_DEFAULT_MAX_REPROCESSES;\n    config->handshake_timeout = H2O_DEFAULT_HANDSHAKE_TIMEOUT;\n    config->http1.req_timeout = H2O_DEFAULT_HTTP1_REQ_TIMEOUT;\n    config->http1.req_io_timeout = H2O_DEFAULT_HTTP1_REQ_IO_TIMEOUT;\n    config->http1.upgrade_to_http2 = H2O_DEFAULT_HTTP1_UPGRADE_TO_HTTP2;\n    config->http2.idle_timeout = H2O_DEFAULT_HTTP2_IDLE_TIMEOUT;\n    config->http2.graceful_shutdown_timeout = H2O_DEFAULT_HTTP2_GRACEFUL_SHUTDOWN_TIMEOUT;\n    config->proxy.io_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.connect_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.first_byte_timeout = H2O_DEFAULT_PROXY_IO_TIMEOUT;\n    config->proxy.emit_x_forwarded_headers = 1;\n    config->proxy.emit_via_header = 1;\n    config->proxy.emit_missing_date_header = 1;\n    config->proxy.zerocopy = H2O_PROXY_ZEROCOPY_ENABLED;\n    config->http2.max_concurrent_requests_per_connection = H2O_HTTP2_SETTINGS_HOST_MAX_CONCURRENT_STREAMS;\n    config->http2.max_concurrent_streaming_requests_per_connection = H2O_HTTP2_SETTINGS_HOST_MAX_CONCURRENT_STREAMING_REQUESTS;\n    config->http2.max_streams_for_priority = 16;\n    config->http2.active_stream_window_size = H2O_DEFAULT_HTTP2_ACTIVE_STREAM_WINDOW_SIZE;\n    config->http2.latency_optimization.min_rtt = 50; // milliseconds\n    config->http2.latency_optimization.max_additional_delay = 10;\n    config->http2.latency_optimization.max_cwnd = 65535;\n    config->http2.dos_delay = 100; /* 100ms processing delay when observing suspicious behavior */\n    config->http3.idle_timeout = quicly_spec_context.transport_params.max_idle_timeout;\n    config->http3.active_stream_window_size = H2O_DEFAULT_HTTP3_ACTIVE_STREAM_WINDOW_SIZE;\n    config->http3.allow_delayed_ack = 1;\n    config->http3.use_gso = 1;\n    config->send_informational_mode = H2O_SEND_INFORMATIONAL_MODE_EXCEPT_H1;\n    config->mimemap = h2o_mimemap_create();\n    h2o_socketpool_init_global(&config->proxy.global_socketpool, SIZE_MAX);\n\n    h2o_configurator__init_core(config);\n\n    config->fallback_host = create_hostconf(config);\n    config->fallback_host->authority.port = 65535;\n    config->fallback_host->authority.host = h2o_strdup(NULL, H2O_STRLIT(\"*\"));\n    config->fallback_host->authority.hostport = h2o_strdup(NULL, H2O_STRLIT(\"*\"));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,7 @@\n     config->http2.latency_optimization.min_rtt = 50; // milliseconds\n     config->http2.latency_optimization.max_additional_delay = 10;\n     config->http2.latency_optimization.max_cwnd = 65535;\n+    config->http2.dos_delay = 100; /* 100ms processing delay when observing suspicious behavior */\n     config->http3.idle_timeout = quicly_spec_context.transport_params.max_idle_timeout;\n     config->http3.active_stream_window_size = H2O_DEFAULT_HTTP3_ACTIVE_STREAM_WINDOW_SIZE;\n     config->http3.allow_delayed_ack = 1;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    config->http2.dos_delay = 100; /* 100ms processing delay when observing suspicious behavior */"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/h2o_configurator__init_core",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "void h2o_configurator__init_core(h2o_globalconf_t *conf)\n{\n    /* check if already initialized */\n    if (h2o_configurator_get_command(conf, \"files\") != NULL)\n        return;\n\n    { /* `hosts` and `paths` */\n        h2o_configurator_t *c = h2o_configurator_create(conf, sizeof(*c));\n        h2o_configurator_define_command(\n            c, \"hosts\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING | H2O_CONFIGURATOR_FLAG_DEFERRED,\n            on_config_hosts);\n        h2o_configurator_define_command(\n            c, \"paths\", H2O_CONFIGURATOR_FLAG_HOST | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING | H2O_CONFIGURATOR_FLAG_DEFERRED,\n            on_config_paths);\n        h2o_configurator_define_command(c, \"strict-match\", H2O_CONFIGURATOR_FLAG_HOST | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_strict_match);\n    };\n\n    { /* setup global configurators */\n        struct st_core_configurator_t *c = (void *)h2o_configurator_create(conf, sizeof(*c));\n        c->super.enter = on_core_enter;\n        c->super.exit = on_core_exit;\n        c->vars = c->_vars_stack;\n        c->vars->http2.reprioritize_blocking_assets = 1; /* defaults to ON */\n        c->vars->http2.push_preload = 1;                 /* defaults to ON */\n        c->vars->error_log.emit_request_errors = 1;      /* defaults to ON */\n        h2o_configurator_define_command(&c->super, \"limit-request-body\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_limit_request_body);\n        h2o_configurator_define_command(&c->super, \"max-delegations\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_max_delegations);\n        h2o_configurator_define_command(&c->super, \"max-reprocesses\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_max_reprocesses);\n        h2o_configurator_define_command(&c->super, \"handshake-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_handshake_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-request-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_request_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-request-io-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_request_io_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-upgrade-to-http2\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_upgrade_to_http2);\n        h2o_configurator_define_command(&c->super, \"http2-idle-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_idle_timeout);\n        h2o_configurator_define_command(&c->super, \"http2-graceful-shutdown-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_graceful_shutdown_timeout);\n        h2o_configurator_define_command(&c->super, \"http2-max-concurrent-requests-per-connection\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_max_concurrent_requests_per_connection);\n        h2o_configurator_define_command(&c->super, \"http2-max-concurrent-streaming-requests-per-connection\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_max_concurrent_streaming_requests_per_connection);\n        h2o_configurator_define_command(&c->super, \"http2-input-window-size\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_input_window_size);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-min-rtt\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_min_rtt);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-max-additional-delay\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_max_additional_delay);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-max-cwnd\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_max_cwnd);\n        h2o_configurator_define_command(&c->super, \"http2-reprioritize-blocking-assets\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_reprioritize_blocking_assets);\n        h2o_configurator_define_command(&c->super, \"http2-push-preload\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_push_preload);\n        h2o_configurator_define_command(&c->super, \"http2-allow-cross-origin-push\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_PATH |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_allow_cross_origin_push);\n        h2o_configurator_define_command(&c->super, \"http2-casper\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST,\n                                        on_config_http2_casper);\n        h2o_configurator_define_command(&c->super, \"http3-idle-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_idle_timeout);\n        h2o_configurator_define_command(&c->super, \"http3-graceful-shutdown-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_graceful_shutdown_timeout);\n        h2o_configurator_define_command(&c->super, \"http3-input-window-size\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_input_window_size);\n        h2o_configurator_define_command(&c->super, \"http3-ack-frequency\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_ack_frequency);\n        h2o_configurator_define_command(&c->super, \"http3-allow-delayed-ack\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_allow_delayed_ack);\n        h2o_configurator_define_command(&c->super, \"http3-gso\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_gso);\n        h2o_configurator_define_command(&c->super, \"file.mime.settypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING,\n                                        on_config_mime_settypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.addtypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING,\n                                        on_config_mime_addtypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.removetypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SEQUENCE,\n                                        on_config_mime_removetypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.setdefaulttype\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_mime_setdefaulttype);\n        h2o_configurator_define_command(&c->super, \"file.custom-handler\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_SEMI_DEFERRED,\n                                        on_config_custom_handler);\n        h2o_configurator_define_command(&c->super, \"setenv\",\n                                        H2O_CONFIGURATOR_FLAG_ALL_LEVELS | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING, on_config_setenv);\n        h2o_configurator_define_command(&c->super, \"unsetenv\", H2O_CONFIGURATOR_FLAG_ALL_LEVELS, on_config_unsetenv);\n        h2o_configurator_define_command(&c->super, \"server-name\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR, on_config_server_name);\n        h2o_configurator_define_command(&c->super, \"send-server-name\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR |\n                                            H2O_CONFIGURATOR_FLAG_DEFERRED,\n                                        on_config_send_server_name);\n        h2o_configurator_define_command(&c->super, \"error-log.emit-request-errors\",\n                                        H2O_CONFIGURATOR_FLAG_ALL_LEVELS | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_error_log_emit_request_errors);\n        h2o_configurator_define_command(&c->super, \"send-informational\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_send_informational);\n        h2o_configurator_define_command(&c->super, \"stash\", H2O_CONFIGURATOR_FLAG_ALL_LEVELS, on_config_stash);\n        h2o_configurator_define_command(&c->super, \"usdt-selective-tracing\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_usdt_selective_tracing);\n    }\n}",
        "func": "void h2o_configurator__init_core(h2o_globalconf_t *conf)\n{\n    /* check if already initialized */\n    if (h2o_configurator_get_command(conf, \"files\") != NULL)\n        return;\n\n    { /* `hosts` and `paths` */\n        h2o_configurator_t *c = h2o_configurator_create(conf, sizeof(*c));\n        h2o_configurator_define_command(\n            c, \"hosts\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING | H2O_CONFIGURATOR_FLAG_DEFERRED,\n            on_config_hosts);\n        h2o_configurator_define_command(\n            c, \"paths\", H2O_CONFIGURATOR_FLAG_HOST | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING | H2O_CONFIGURATOR_FLAG_DEFERRED,\n            on_config_paths);\n        h2o_configurator_define_command(c, \"strict-match\", H2O_CONFIGURATOR_FLAG_HOST | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_strict_match);\n    };\n\n    { /* setup global configurators */\n        struct st_core_configurator_t *c = (void *)h2o_configurator_create(conf, sizeof(*c));\n        c->super.enter = on_core_enter;\n        c->super.exit = on_core_exit;\n        c->vars = c->_vars_stack;\n        c->vars->http2.reprioritize_blocking_assets = 1; /* defaults to ON */\n        c->vars->http2.push_preload = 1;                 /* defaults to ON */\n        c->vars->error_log.emit_request_errors = 1;      /* defaults to ON */\n        h2o_configurator_define_command(&c->super, \"limit-request-body\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_limit_request_body);\n        h2o_configurator_define_command(&c->super, \"max-delegations\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_max_delegations);\n        h2o_configurator_define_command(&c->super, \"max-reprocesses\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_max_reprocesses);\n        h2o_configurator_define_command(&c->super, \"handshake-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_handshake_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-request-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_request_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-request-io-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_request_io_timeout);\n        h2o_configurator_define_command(&c->super, \"http1-upgrade-to-http2\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http1_upgrade_to_http2);\n        h2o_configurator_define_command(&c->super, \"http2-idle-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_idle_timeout);\n        h2o_configurator_define_command(&c->super, \"http2-graceful-shutdown-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_graceful_shutdown_timeout);\n        h2o_configurator_define_command(&c->super, \"http2-max-concurrent-requests-per-connection\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_max_concurrent_requests_per_connection);\n        h2o_configurator_define_command(&c->super, \"http2-max-concurrent-streaming-requests-per-connection\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_max_concurrent_streaming_requests_per_connection);\n        h2o_configurator_define_command(&c->super, \"http2-input-window-size\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_input_window_size);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-min-rtt\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_min_rtt);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-max-additional-delay\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_max_additional_delay);\n        h2o_configurator_define_command(&c->super, \"http2-latency-optimization-max-cwnd\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_latency_optimization_max_cwnd);\n        h2o_configurator_define_command(&c->super, \"http2-reprioritize-blocking-assets\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_reprioritize_blocking_assets);\n        h2o_configurator_define_command(&c->super, \"http2-push-preload\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_push_preload);\n        h2o_configurator_define_command(&c->super, \"http2-allow-cross-origin-push\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_PATH |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_allow_cross_origin_push);\n        h2o_configurator_define_command(&c->super, \"http2-casper\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST,\n                                        on_config_http2_casper);\n        h2o_configurator_define_command(&c->super, \"http2-dos-delay\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http2_dos_delay);\n        h2o_configurator_define_command(&c->super, \"http3-idle-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_idle_timeout);\n        h2o_configurator_define_command(&c->super, \"http3-graceful-shutdown-timeout\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_graceful_shutdown_timeout);\n        h2o_configurator_define_command(&c->super, \"http3-input-window-size\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_input_window_size);\n        h2o_configurator_define_command(&c->super, \"http3-ack-frequency\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_ack_frequency);\n        h2o_configurator_define_command(&c->super, \"http3-allow-delayed-ack\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_allow_delayed_ack);\n        h2o_configurator_define_command(&c->super, \"http3-gso\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_http3_gso);\n        h2o_configurator_define_command(&c->super, \"file.mime.settypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING,\n                                        on_config_mime_settypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.addtypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING,\n                                        on_config_mime_addtypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.removetypes\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SEQUENCE,\n                                        on_config_mime_removetypes);\n        h2o_configurator_define_command(&c->super, \"file.mime.setdefaulttype\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_mime_setdefaulttype);\n        h2o_configurator_define_command(&c->super, \"file.custom-handler\",\n                                        (H2O_CONFIGURATOR_FLAG_ALL_LEVELS & ~H2O_CONFIGURATOR_FLAG_EXTENSION) |\n                                            H2O_CONFIGURATOR_FLAG_SEMI_DEFERRED,\n                                        on_config_custom_handler);\n        h2o_configurator_define_command(&c->super, \"setenv\",\n                                        H2O_CONFIGURATOR_FLAG_ALL_LEVELS | H2O_CONFIGURATOR_FLAG_EXPECT_MAPPING, on_config_setenv);\n        h2o_configurator_define_command(&c->super, \"unsetenv\", H2O_CONFIGURATOR_FLAG_ALL_LEVELS, on_config_unsetenv);\n        h2o_configurator_define_command(&c->super, \"server-name\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR, on_config_server_name);\n        h2o_configurator_define_command(&c->super, \"send-server-name\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR |\n                                            H2O_CONFIGURATOR_FLAG_DEFERRED,\n                                        on_config_send_server_name);\n        h2o_configurator_define_command(&c->super, \"error-log.emit-request-errors\",\n                                        H2O_CONFIGURATOR_FLAG_ALL_LEVELS | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_error_log_emit_request_errors);\n        h2o_configurator_define_command(&c->super, \"send-informational\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_send_informational);\n        h2o_configurator_define_command(&c->super, \"stash\", H2O_CONFIGURATOR_FLAG_ALL_LEVELS, on_config_stash);\n        h2o_configurator_define_command(&c->super, \"usdt-selective-tracing\",\n                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                        on_config_usdt_selective_tracing);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -83,6 +83,9 @@\n                                         on_config_http2_allow_cross_origin_push);\n         h2o_configurator_define_command(&c->super, \"http2-casper\", H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_HOST,\n                                         on_config_http2_casper);\n+        h2o_configurator_define_command(&c->super, \"http2-dos-delay\",\n+                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n+                                        on_config_http2_dos_delay);\n         h2o_configurator_define_command(&c->super, \"http3-idle-timeout\",\n                                         H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,\n                                         on_config_http3_idle_timeout);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        h2o_configurator_define_command(&c->super, \"http2-dos-delay\",",
                "                                        H2O_CONFIGURATOR_FLAG_GLOBAL | H2O_CONFIGURATOR_FLAG_EXPECT_SCALAR,",
                "                                        on_config_http2_dos_delay);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/handle_rst_stream_frame",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "static int handle_rst_stream_frame(h2o_http2_conn_t *conn, h2o_http2_frame_t *frame, const char **err_desc)\n{\n    h2o_http2_rst_stream_payload_t payload;\n    h2o_http2_stream_t *stream;\n    int ret;\n\n    if ((ret = h2o_http2_decode_rst_stream_payload(&payload, frame, err_desc)) != 0)\n        return ret;\n    if (is_idle_stream_id(conn, frame->stream_id)) {\n        *err_desc = \"unexpected stream id in RST_STREAM frame\";\n        return H2O_HTTP2_ERROR_PROTOCOL;\n    }\n\n    stream = h2o_http2_conn_get_stream(conn, frame->stream_id);\n    if (stream != NULL) {\n        /* reset the stream */\n        h2o_http2_stream_reset(conn, stream);\n    }\n    /* TODO log */\n\n    return 0;\n}",
        "func": "static int handle_rst_stream_frame(h2o_http2_conn_t *conn, h2o_http2_frame_t *frame, const char **err_desc)\n{\n    h2o_http2_rst_stream_payload_t payload;\n    h2o_http2_stream_t *stream;\n    int ret;\n\n    if ((ret = h2o_http2_decode_rst_stream_payload(&payload, frame, err_desc)) != 0)\n        return ret;\n    if (is_idle_stream_id(conn, frame->stream_id)) {\n        *err_desc = \"unexpected stream id in RST_STREAM frame\";\n        return H2O_HTTP2_ERROR_PROTOCOL;\n    }\n\n    if ((stream = h2o_http2_conn_get_stream(conn, frame->stream_id)) == NULL)\n        return 0;\n\n    /* reset the stream */\n    stream->reset_by_peer = 1;\n    h2o_http2_stream_reset(conn, stream);\n\n    /* setup process delay if we've just ran out of reset budget */\n    if (conn->dos_mitigation.reset_budget == 0 && conn->super.ctx->globalconf->http2.dos_delay != 0 &&\n        !h2o_timer_is_linked(&conn->dos_mitigation.process_delay))\n        h2o_timer_link(conn->super.ctx->loop, conn->super.ctx->globalconf->http2.dos_delay,\n                       &conn->dos_mitigation.process_delay);\n\n    /* TODO log */\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,11 +11,19 @@\n         return H2O_HTTP2_ERROR_PROTOCOL;\n     }\n \n-    stream = h2o_http2_conn_get_stream(conn, frame->stream_id);\n-    if (stream != NULL) {\n-        /* reset the stream */\n-        h2o_http2_stream_reset(conn, stream);\n-    }\n+    if ((stream = h2o_http2_conn_get_stream(conn, frame->stream_id)) == NULL)\n+        return 0;\n+\n+    /* reset the stream */\n+    stream->reset_by_peer = 1;\n+    h2o_http2_stream_reset(conn, stream);\n+\n+    /* setup process delay if we've just ran out of reset budget */\n+    if (conn->dos_mitigation.reset_budget == 0 && conn->super.ctx->globalconf->http2.dos_delay != 0 &&\n+        !h2o_timer_is_linked(&conn->dos_mitigation.process_delay))\n+        h2o_timer_link(conn->super.ctx->loop, conn->super.ctx->globalconf->http2.dos_delay,\n+                       &conn->dos_mitigation.process_delay);\n+\n     /* TODO log */\n \n     return 0;",
        "diff_line_info": {
            "deleted_lines": [
                "    stream = h2o_http2_conn_get_stream(conn, frame->stream_id);",
                "    if (stream != NULL) {",
                "        /* reset the stream */",
                "        h2o_http2_stream_reset(conn, stream);",
                "    }"
            ],
            "added_lines": [
                "    if ((stream = h2o_http2_conn_get_stream(conn, frame->stream_id)) == NULL)",
                "        return 0;",
                "",
                "    /* reset the stream */",
                "    stream->reset_by_peer = 1;",
                "    h2o_http2_stream_reset(conn, stream);",
                "",
                "    /* setup process delay if we've just ran out of reset budget */",
                "    if (conn->dos_mitigation.reset_budget == 0 && conn->super.ctx->globalconf->http2.dos_delay != 0 &&",
                "        !h2o_timer_is_linked(&conn->dos_mitigation.process_delay))",
                "        h2o_timer_link(conn->super.ctx->loop, conn->super.ctx->globalconf->http2.dos_delay,",
                "                       &conn->dos_mitigation.process_delay);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/h2o_http2_conn_unregister_stream",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "void h2o_http2_conn_unregister_stream(h2o_http2_conn_t *conn, h2o_http2_stream_t *stream)\n{\n    h2o_http2_conn_preserve_stream_scheduler(conn, stream);\n\n    khiter_t iter = kh_get(h2o_http2_stream_t, conn->streams, stream->stream_id);\n    assert(iter != kh_end(conn->streams));\n    kh_del(h2o_http2_stream_t, conn->streams, iter);\n\n    if (stream->req_body.state != H2O_HTTP2_REQ_BODY_NONE && stream->req_body.state < H2O_HTTP2_REQ_BODY_CLOSE_DELIVERED) {\n        stream->req.proceed_req = NULL;\n        set_req_body_state(conn, stream, H2O_HTTP2_REQ_BODY_CLOSE_DELIVERED);\n    }\n\n    if (stream->blocked_by_server)\n        h2o_http2_stream_set_blocked_by_server(conn, stream, 0);\n\n    switch (stream->state) {\n    case H2O_HTTP2_STREAM_STATE_RECV_BODY:\n        if (h2o_linklist_is_linked(&stream->_link))\n            h2o_linklist_unlink(&stream->_link);\n    /* fallthru */\n    case H2O_HTTP2_STREAM_STATE_IDLE:\n    case H2O_HTTP2_STREAM_STATE_RECV_HEADERS:\n        assert(!h2o_linklist_is_linked(&stream->_link));\n        break;\n    case H2O_HTTP2_STREAM_STATE_REQ_PENDING:\n        assert(h2o_linklist_is_linked(&stream->_link));\n        h2o_linklist_unlink(&stream->_link);\n        break;\n    case H2O_HTTP2_STREAM_STATE_SEND_HEADERS:\n    case H2O_HTTP2_STREAM_STATE_SEND_BODY:\n    case H2O_HTTP2_STREAM_STATE_SEND_BODY_IS_FINAL:\n    case H2O_HTTP2_STREAM_STATE_END_STREAM:\n        if (h2o_linklist_is_linked(&stream->_link))\n            h2o_linklist_unlink(&stream->_link);\n        break;\n    }\n    if (stream->state != H2O_HTTP2_STREAM_STATE_END_STREAM)\n        h2o_http2_stream_set_state(conn, stream, H2O_HTTP2_STREAM_STATE_END_STREAM);\n\n    if (conn->state < H2O_HTTP2_CONN_STATE_IS_CLOSING) {\n        run_pending_requests(conn);\n        update_idle_timeout(conn);\n    }\n}",
        "func": "void h2o_http2_conn_unregister_stream(h2o_http2_conn_t *conn, h2o_http2_stream_t *stream)\n{\n    h2o_http2_conn_preserve_stream_scheduler(conn, stream);\n\n    khiter_t iter = kh_get(h2o_http2_stream_t, conn->streams, stream->stream_id);\n    assert(iter != kh_end(conn->streams));\n    kh_del(h2o_http2_stream_t, conn->streams, iter);\n\n    if (stream->req_body.state != H2O_HTTP2_REQ_BODY_NONE && stream->req_body.state < H2O_HTTP2_REQ_BODY_CLOSE_DELIVERED) {\n        stream->req.proceed_req = NULL;\n        set_req_body_state(conn, stream, H2O_HTTP2_REQ_BODY_CLOSE_DELIVERED);\n    }\n\n    if (stream->blocked_by_server)\n        h2o_http2_stream_set_blocked_by_server(conn, stream, 0);\n\n    /* Decrement reset_budget if the stream was reset by peer, otherwise increment. By doing so, we penalize connections that\n     * generate resets for >50% of requests. */\n    if (stream->reset_by_peer) {\n        if (conn->dos_mitigation.reset_budget > 0)\n            --conn->dos_mitigation.reset_budget;\n    } else {\n        if (conn->dos_mitigation.reset_budget < conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection)\n            ++conn->dos_mitigation.reset_budget;\n    }\n\n    switch (stream->state) {\n    case H2O_HTTP2_STREAM_STATE_RECV_BODY:\n        if (h2o_linklist_is_linked(&stream->_link))\n            h2o_linklist_unlink(&stream->_link);\n    /* fallthru */\n    case H2O_HTTP2_STREAM_STATE_IDLE:\n    case H2O_HTTP2_STREAM_STATE_RECV_HEADERS:\n        assert(!h2o_linklist_is_linked(&stream->_link));\n        break;\n    case H2O_HTTP2_STREAM_STATE_REQ_PENDING:\n        assert(h2o_linklist_is_linked(&stream->_link));\n        h2o_linklist_unlink(&stream->_link);\n        break;\n    case H2O_HTTP2_STREAM_STATE_SEND_HEADERS:\n    case H2O_HTTP2_STREAM_STATE_SEND_BODY:\n    case H2O_HTTP2_STREAM_STATE_SEND_BODY_IS_FINAL:\n    case H2O_HTTP2_STREAM_STATE_END_STREAM:\n        if (h2o_linklist_is_linked(&stream->_link))\n            h2o_linklist_unlink(&stream->_link);\n        break;\n    }\n    if (stream->state != H2O_HTTP2_STREAM_STATE_END_STREAM)\n        h2o_http2_stream_set_state(conn, stream, H2O_HTTP2_STREAM_STATE_END_STREAM);\n\n    if (conn->state < H2O_HTTP2_CONN_STATE_IS_CLOSING) {\n        run_pending_requests(conn);\n        update_idle_timeout(conn);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,16 @@\n \n     if (stream->blocked_by_server)\n         h2o_http2_stream_set_blocked_by_server(conn, stream, 0);\n+\n+    /* Decrement reset_budget if the stream was reset by peer, otherwise increment. By doing so, we penalize connections that\n+     * generate resets for >50% of requests. */\n+    if (stream->reset_by_peer) {\n+        if (conn->dos_mitigation.reset_budget > 0)\n+            --conn->dos_mitigation.reset_budget;\n+    } else {\n+        if (conn->dos_mitigation.reset_budget < conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection)\n+            ++conn->dos_mitigation.reset_budget;\n+    }\n \n     switch (stream->state) {\n     case H2O_HTTP2_STREAM_STATE_RECV_BODY:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Decrement reset_budget if the stream was reset by peer, otherwise increment. By doing so, we penalize connections that",
                "     * generate resets for >50% of requests. */",
                "    if (stream->reset_by_peer) {",
                "        if (conn->dos_mitigation.reset_budget > 0)",
                "            --conn->dos_mitigation.reset_budget;",
                "    } else {",
                "        if (conn->dos_mitigation.reset_budget < conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection)",
                "            ++conn->dos_mitigation.reset_budget;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/create_conn",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "static h2o_http2_conn_t *create_conn(h2o_context_t *ctx, h2o_hostconf_t **hosts, h2o_socket_t *sock, struct timeval connected_at)\n{\n    static const h2o_conn_callbacks_t callbacks = {\n        .get_sockname = get_sockname,\n        .get_peername = get_peername,\n        .get_ptls = get_ptls,\n        .skip_tracing = skip_tracing,\n        .get_req_id = get_req_id,\n        .push_path = push_path,\n        .get_debug_state = h2o_http2_get_debug_state,\n        .close_idle_connection = close_idle_connection,\n        .foreach_request = foreach_request,\n        .request_shutdown = initiate_graceful_shutdown,\n        .get_rtt = get_rtt,\n        .log_ = {{\n            .transport =\n                {\n                    .cc_name = log_tcp_congestion_controller,\n                    .delivery_rate = log_tcp_delivery_rate,\n                },\n            .ssl =\n                {\n                    .protocol_version = log_ssl_protocol_version,\n                    .session_reused = log_ssl_session_reused,\n                    .cipher = log_ssl_cipher,\n                    .cipher_bits = log_ssl_cipher_bits,\n                    .session_id = log_ssl_session_id,\n                    .server_name = log_ssl_server_name,\n                    .negotiated_protocol = log_ssl_negotiated_protocol,\n                    .ech_config_id = log_ssl_ech_config_id,\n                    .ech_kem = log_ssl_ech_kem,\n                    .ech_cipher = log_ssl_ech_cipher,\n                    .ech_cipher_bits = log_ssl_ech_cipher_bits,\n                    .backend = log_ssl_backend,\n                },\n            .http2 =\n                {\n                    .stream_id = log_stream_id,\n                    .priority_received = log_priority_received,\n                    .priority_received_exclusive = log_priority_received_exclusive,\n                    .priority_received_parent = log_priority_received_parent,\n                    .priority_received_weight = log_priority_received_weight,\n                    .priority_actual = log_priority_actual,\n                    .priority_actual_parent = log_priority_actual_parent,\n                    .priority_actual_weight = log_priority_actual_weight,\n                },\n        }},\n    };\n\n    h2o_http2_conn_t *conn = (void *)h2o_create_connection(sizeof(*conn), ctx, hosts, connected_at, &callbacks);\n\n    memset((char *)conn + sizeof(conn->super), 0, sizeof(*conn) - sizeof(conn->super));\n    conn->sock = sock;\n    conn->peer_settings = H2O_HTTP2_SETTINGS_DEFAULT;\n    conn->streams = kh_init(h2o_http2_stream_t);\n    h2o_http2_scheduler_init(&conn->scheduler);\n    conn->state = H2O_HTTP2_CONN_STATE_OPEN;\n    conn->_read_expect = expect_preface;\n    conn->_input_header_table.hpack_capacity = conn->_input_header_table.hpack_max_capacity =\n        H2O_HTTP2_SETTINGS_DEFAULT.header_table_size;\n    h2o_http2_window_init(&conn->_input_window, H2O_HTTP2_SETTINGS_HOST_CONNECTION_WINDOW_SIZE);\n    conn->_output_header_table.hpack_capacity = H2O_HTTP2_SETTINGS_DEFAULT.header_table_size;\n    h2o_linklist_init_anchor(&conn->_pending_reqs);\n    h2o_buffer_init(&conn->_write.buf, &h2o_http2_wbuf_buffer_prototype);\n    h2o_linklist_init_anchor(&conn->_write.streams_to_proceed);\n    conn->_write.timeout_entry.cb = emit_writereq;\n    h2o_http2_window_init(&conn->_write.window, conn->peer_settings.initial_window_size);\n    h2o_linklist_init_anchor(&conn->early_data.blocked_streams);\n    conn->is_chromium_dependency_tree = 1; /* initially assume the client is Chromium until proven otherwise */\n    conn->received_any_request = 0;\n\n    return conn;\n}",
        "func": "static h2o_http2_conn_t *create_conn(h2o_context_t *ctx, h2o_hostconf_t **hosts, h2o_socket_t *sock, struct timeval connected_at)\n{\n    static const h2o_conn_callbacks_t callbacks = {\n        .get_sockname = get_sockname,\n        .get_peername = get_peername,\n        .get_ptls = get_ptls,\n        .skip_tracing = skip_tracing,\n        .get_req_id = get_req_id,\n        .push_path = push_path,\n        .get_debug_state = h2o_http2_get_debug_state,\n        .close_idle_connection = close_idle_connection,\n        .foreach_request = foreach_request,\n        .request_shutdown = initiate_graceful_shutdown,\n        .get_rtt = get_rtt,\n        .log_ = {{\n            .transport =\n                {\n                    .cc_name = log_tcp_congestion_controller,\n                    .delivery_rate = log_tcp_delivery_rate,\n                },\n            .ssl =\n                {\n                    .protocol_version = log_ssl_protocol_version,\n                    .session_reused = log_ssl_session_reused,\n                    .cipher = log_ssl_cipher,\n                    .cipher_bits = log_ssl_cipher_bits,\n                    .session_id = log_ssl_session_id,\n                    .server_name = log_ssl_server_name,\n                    .negotiated_protocol = log_ssl_negotiated_protocol,\n                    .ech_config_id = log_ssl_ech_config_id,\n                    .ech_kem = log_ssl_ech_kem,\n                    .ech_cipher = log_ssl_ech_cipher,\n                    .ech_cipher_bits = log_ssl_ech_cipher_bits,\n                    .backend = log_ssl_backend,\n                },\n            .http2 =\n                {\n                    .stream_id = log_stream_id,\n                    .priority_received = log_priority_received,\n                    .priority_received_exclusive = log_priority_received_exclusive,\n                    .priority_received_parent = log_priority_received_parent,\n                    .priority_received_weight = log_priority_received_weight,\n                    .priority_actual = log_priority_actual,\n                    .priority_actual_parent = log_priority_actual_parent,\n                    .priority_actual_weight = log_priority_actual_weight,\n                },\n        }},\n    };\n\n    h2o_http2_conn_t *conn = (void *)h2o_create_connection(sizeof(*conn), ctx, hosts, connected_at, &callbacks);\n\n    memset((char *)conn + sizeof(conn->super), 0, sizeof(*conn) - sizeof(conn->super));\n    conn->sock = sock;\n    conn->peer_settings = H2O_HTTP2_SETTINGS_DEFAULT;\n    conn->streams = kh_init(h2o_http2_stream_t);\n    h2o_http2_scheduler_init(&conn->scheduler);\n    conn->state = H2O_HTTP2_CONN_STATE_OPEN;\n    conn->_read_expect = expect_preface;\n    conn->_input_header_table.hpack_capacity = conn->_input_header_table.hpack_max_capacity =\n        H2O_HTTP2_SETTINGS_DEFAULT.header_table_size;\n    h2o_http2_window_init(&conn->_input_window, H2O_HTTP2_SETTINGS_HOST_CONNECTION_WINDOW_SIZE);\n    conn->_output_header_table.hpack_capacity = H2O_HTTP2_SETTINGS_DEFAULT.header_table_size;\n    h2o_linklist_init_anchor(&conn->_pending_reqs);\n    h2o_buffer_init(&conn->_write.buf, &h2o_http2_wbuf_buffer_prototype);\n    h2o_linklist_init_anchor(&conn->_write.streams_to_proceed);\n    conn->_write.timeout_entry.cb = emit_writereq;\n    h2o_http2_window_init(&conn->_write.window, conn->peer_settings.initial_window_size);\n    h2o_linklist_init_anchor(&conn->early_data.blocked_streams);\n    conn->is_chromium_dependency_tree = 1; /* initially assume the client is Chromium until proven otherwise */\n    conn->received_any_request = 0;\n    conn->dos_mitigation.process_delay.cb = on_dos_process_delay;\n    conn->dos_mitigation.reset_budget = conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection;\n\n    return conn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -68,6 +68,8 @@\n     h2o_linklist_init_anchor(&conn->early_data.blocked_streams);\n     conn->is_chromium_dependency_tree = 1; /* initially assume the client is Chromium until proven otherwise */\n     conn->received_any_request = 0;\n+    conn->dos_mitigation.process_delay.cb = on_dos_process_delay;\n+    conn->dos_mitigation.reset_budget = conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection;\n \n     return conn;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    conn->dos_mitigation.process_delay.cb = on_dos_process_delay;",
                "    conn->dos_mitigation.reset_budget = conn->super.ctx->globalconf->http2.max_concurrent_requests_per_connection;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "h2o/run_pending_requests",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/h2o/h2o/commit/94fbc54b6c9309912fe3d53e7b63408bbe9a1b0d",
        "commit_title": "[http2] delay processing requests upon observing suspicious behavior",
        "commit_text": "",
        "func_before": "static void run_pending_requests(h2o_http2_conn_t *conn)\n{\n    h2o_linklist_t *link, *lnext;\n    int ran_one_request;\n\n    do {\n        ran_one_request = 0;\n\n        for (link = conn->_pending_reqs.next; link != &conn->_pending_reqs && can_run_requests(conn); link = lnext) {\n            /* fetch and detach a pending stream */\n            h2o_http2_stream_t *stream = H2O_STRUCT_FROM_MEMBER(h2o_http2_stream_t, _link, link);\n\n            lnext = link->next;\n\n            /* handle no more than specified number of streaming requests at a time */\n            if (stream->req.proceed_req != NULL &&\n                conn->num_streams._req_streaming_in_progress - conn->num_streams.tunnel >=\n                    conn->super.ctx->globalconf->http2.max_concurrent_streaming_requests_per_connection)\n                continue;\n\n            /* handle it */\n            h2o_linklist_unlink(&stream->_link);\n            ran_one_request = 1;\n            process_request(conn, stream);\n        }\n\n    } while (ran_one_request && !h2o_linklist_is_empty(&conn->_pending_reqs));\n}",
        "func": "static void run_pending_requests(h2o_http2_conn_t *conn)\n{\n    if (h2o_timer_is_linked(&conn->dos_mitigation.process_delay))\n        return;\n\n    h2o_linklist_t *link, *lnext;\n    int ran_one_request;\n\n    do {\n        ran_one_request = 0;\n\n        for (link = conn->_pending_reqs.next; link != &conn->_pending_reqs && can_run_requests(conn); link = lnext) {\n            /* fetch and detach a pending stream */\n            h2o_http2_stream_t *stream = H2O_STRUCT_FROM_MEMBER(h2o_http2_stream_t, _link, link);\n\n            lnext = link->next;\n\n            /* handle no more than specified number of streaming requests at a time */\n            if (stream->req.proceed_req != NULL &&\n                conn->num_streams._req_streaming_in_progress - conn->num_streams.tunnel >=\n                    conn->super.ctx->globalconf->http2.max_concurrent_streaming_requests_per_connection)\n                continue;\n\n            /* handle it */\n            h2o_linklist_unlink(&stream->_link);\n            ran_one_request = 1;\n            process_request(conn, stream);\n        }\n\n    } while (ran_one_request && !h2o_linklist_is_empty(&conn->_pending_reqs));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,8 @@\n static void run_pending_requests(h2o_http2_conn_t *conn)\n {\n+    if (h2o_timer_is_linked(&conn->dos_mitigation.process_delay))\n+        return;\n+\n     h2o_linklist_t *link, *lnext;\n     int ran_one_request;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (h2o_timer_is_linked(&conn->dos_mitigation.process_delay))",
                "        return;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/State",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "State()\n          : codec_saw_local_complete_(false), codec_encode_complete_(false),\n            on_reset_stream_called_(false), is_zombie_stream_(false), successful_upgrade_(false),\n            is_internally_destroyed_(false), is_internally_created_(false), is_tunneling_(false),\n            decorated_propagate_(true) {}",
        "func": "State()\n          : codec_saw_local_complete_(false), codec_encode_complete_(false),\n            on_reset_stream_called_(false), is_zombie_stream_(false), successful_upgrade_(false),\n            is_internally_destroyed_(false), is_internally_created_(false), is_tunneling_(false),\n            decorated_propagate_(true), deferred_to_next_io_iteration_(false) {}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,4 +2,4 @@\n           : codec_saw_local_complete_(false), codec_encode_complete_(false),\n             on_reset_stream_called_(false), is_zombie_stream_(false), successful_upgrade_(false),\n             is_internally_destroyed_(false), is_internally_created_(false), is_tunneling_(false),\n-            decorated_propagate_(true) {}\n+            decorated_propagate_(true), deferred_to_next_io_iteration_(false) {}",
        "diff_line_info": {
            "deleted_lines": [
                "            decorated_propagate_(true) {}"
            ],
            "added_lines": [
                "            decorated_propagate_(true), deferred_to_next_io_iteration_(false) {}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ActiveStream::decodeHeaders",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "void ConnectionManagerImpl::ActiveStream::decodeHeaders(RequestHeaderMapSharedPtr&& headers,\n                                                        bool end_stream) {\n  ENVOY_STREAM_LOG(debug, \"request headers complete (end_stream={}):\\n{}\", *this, end_stream,\n                   *headers);\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  request_headers_ = std::move(headers);\n  filter_manager_.requestHeadersInitialized();\n  if (request_header_timer_ != nullptr) {\n    request_header_timer_->disableTimer();\n    request_header_timer_.reset();\n  }\n\n  // Both shouldDrainConnectionUponCompletion() and is_head_request_ affect local replies: set them\n  // as early as possible.\n  const Protocol protocol = connection_manager_.codec_->protocol();\n  if (Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.http1_connection_close_header_in_redirect\")) {\n    if (HeaderUtility::shouldCloseConnection(protocol, *request_headers_)) {\n      // Only mark the connection to be closed if the request indicates so. The connection might\n      // already be marked so before this step, in which case if shouldCloseConnection() returns\n      // false, the stream info value shouldn't be overridden.\n      filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(true);\n    }\n  } else {\n    filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(\n        HeaderUtility::shouldCloseConnection(protocol, *request_headers_));\n  }\n\n  filter_manager_.streamInfo().protocol(protocol);\n\n  // We end the decode here to mark that the downstream stream is complete.\n  maybeEndDecode(end_stream);\n\n  if (!validateHeaders()) {\n    ENVOY_STREAM_LOG(debug, \"request headers validation failed\\n{}\", *this, *request_headers_);\n    return;\n  }\n\n  // We need to snap snapped_route_config_ here as it's used in mutateRequestHeaders later.\n  if (connection_manager_.config_.isRoutable()) {\n    if (connection_manager_.config_.routeConfigProvider() != nullptr) {\n      snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->configCast();\n    } else if (connection_manager_.config_.scopedRouteConfigProvider() != nullptr &&\n               connection_manager_.config_.scopeKeyBuilder().has_value()) {\n      snapped_scoped_routes_config_ =\n          connection_manager_.config_.scopedRouteConfigProvider()->config<Router::ScopedConfig>();\n      snapScopedRouteConfig();\n    }\n  } else {\n    snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->configCast();\n  }\n\n  // Drop new requests when overloaded as soon as we have decoded the headers.\n  const bool drop_request_due_to_overload =\n      (connection_manager_.accept_new_http_stream_ != nullptr &&\n       connection_manager_.accept_new_http_stream_->shouldShedLoad()) ||\n      connection_manager_.random_generator_.bernoulli(\n          connection_manager_.overload_stop_accepting_requests_ref_.value());\n\n  if (drop_request_due_to_overload) {\n    // In this one special case, do not create the filter chain. If there is a risk of memory\n    // overload it is more important to avoid unnecessary allocation than to create the filters.\n    filter_manager_.skipFilterChainCreation();\n    connection_manager_.stats_.named_.downstream_rq_overload_close_.inc();\n    sendLocalReply(Http::Code::ServiceUnavailable, \"envoy overloaded\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().Overload);\n    return;\n  }\n\n  if (!connection_manager_.config_.proxy100Continue() && request_headers_->Expect() &&\n      // The Expect field-value is case-insensitive.\n      // https://tools.ietf.org/html/rfc7231#section-5.1.1\n      absl::EqualsIgnoreCase((request_headers_->Expect()->value().getStringView()),\n                             Headers::get().ExpectValues._100Continue)) {\n    // Note in the case Envoy is handling 100-Continue complexity, it skips the filter chain\n    // and sends the 100-Continue directly to the encoder.\n    chargeStats(continueHeader());\n    response_encoder_->encode1xxHeaders(continueHeader());\n    // Remove the Expect header so it won't be handled again upstream.\n    request_headers_->removeExpect();\n  }\n\n  connection_manager_.user_agent_.initializeFromHeaders(*request_headers_,\n                                                        connection_manager_.stats_.prefixStatName(),\n                                                        connection_manager_.stats_.scope_);\n\n  if (!request_headers_->Host()) {\n    // Require host header. For HTTP/1.1 Host has already been translated to :authority.\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().MissingHost);\n    return;\n  }\n\n  // Apply header sanity checks.\n  absl::optional<std::reference_wrapper<const absl::string_view>> error =\n      HeaderUtility::requestHeadersValid(*request_headers_);\n  if (error != absl::nullopt) {\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt, error.value().get());\n    if (!response_encoder_->streamErrorOnInvalidHttpMessage()) {\n      connection_manager_.handleCodecError(error.value().get());\n    }\n    return;\n  }\n\n  // Check for the existence of the :path header for non-CONNECT requests, or present-but-empty\n  // :path header for CONNECT requests. We expect the codec to have broken the path into pieces if\n  // applicable. NOTE: Currently the HTTP/1.1 codec only does this when the allow_absolute_url flag\n  // is enabled on the HCM.\n  if ((!HeaderUtility::isConnect(*request_headers_) || request_headers_->Path()) &&\n      request_headers_->getPathValue().empty()) {\n    sendLocalReply(Code::NotFound, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().MissingPath);\n    return;\n  }\n\n  // Rewrites the host of CONNECT-UDP requests.\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.enable_connect_udp_support\") &&\n      HeaderUtility::isConnectUdpRequest(*request_headers_) &&\n      !HeaderUtility::rewriteAuthorityForConnectUdp(*request_headers_)) {\n    sendLocalReply(Code::NotFound, \"The path is incorrect for CONNECT-UDP\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().InvalidPath);\n    return;\n  }\n\n  // Currently we only support relative paths at the application layer.\n  if (!request_headers_->getPathValue().empty() && request_headers_->getPathValue()[0] != '/') {\n    connection_manager_.stats_.named_.downstream_rq_non_relative_path_.inc();\n    sendLocalReply(Code::NotFound, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().AbsolutePath);\n    return;\n  }\n\n#ifndef ENVOY_ENABLE_UHV\n  // In UHV mode path normalization is done in the UHV\n  // Path sanitization should happen before any path access other than the above sanity check.\n  const auto action =\n      ConnectionManagerUtility::maybeNormalizePath(*request_headers_, connection_manager_.config_);\n  // gRPC requests are rejected if Envoy is configured to redirect post-normalization. This is\n  // because gRPC clients do not support redirect.\n  if (action == ConnectionManagerUtility::NormalizePathAction::Reject ||\n      (action == ConnectionManagerUtility::NormalizePathAction::Redirect &&\n       Grpc::Common::hasGrpcContentType(*request_headers_))) {\n    connection_manager_.stats_.named_.downstream_rq_failed_path_normalization_.inc();\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  } else if (action == ConnectionManagerUtility::NormalizePathAction::Redirect) {\n    connection_manager_.stats_.named_.downstream_rq_redirected_with_normalized_path_.inc();\n    sendLocalReply(\n        Code::TemporaryRedirect, \"\",\n        [new_path = request_headers_->Path()->value().getStringView()](\n            Http::ResponseHeaderMap& response_headers) -> void {\n          response_headers.addReferenceKey(Http::Headers::get().Location, new_path);\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  }\n\n  ASSERT(action == ConnectionManagerUtility::NormalizePathAction::Continue);\n#endif\n  auto optional_port = ConnectionManagerUtility::maybeNormalizeHost(\n      *request_headers_, connection_manager_.config_, localPort());\n  if (optional_port.has_value() &&\n      requestWasConnect(request_headers_, connection_manager_.codec_->protocol())) {\n    filter_manager_.streamInfo().filterState()->setData(\n        Router::OriginalConnectPort::key(),\n        std::make_unique<Router::OriginalConnectPort>(optional_port.value()),\n        StreamInfo::FilterState::StateType::ReadOnly, StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (!state_.is_internally_created_) { // Only sanitize headers on first pass.\n    // Modify the downstream remote address depending on configuration and headers.\n    const auto mutate_result = ConnectionManagerUtility::mutateRequestHeaders(\n        *request_headers_, connection_manager_.read_callbacks_->connection(),\n        connection_manager_.config_, *snapped_route_config_, connection_manager_.local_info_,\n        filter_manager_.streamInfo());\n\n    // IP detection failed, reject the request.\n    if (mutate_result.reject_request.has_value()) {\n      const auto& reject_request_params = mutate_result.reject_request.value();\n      connection_manager_.stats_.named_.downstream_rq_rejected_via_ip_detection_.inc();\n      sendLocalReply(reject_request_params.response_code, reject_request_params.body, nullptr,\n                     absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().OriginalIPDetectionFailed);\n      return;\n    }\n\n    filter_manager_.setDownstreamRemoteAddress(mutate_result.final_remote_address);\n  }\n  ASSERT(filter_manager_.streamInfo().downstreamAddressProvider().remoteAddress() != nullptr);\n\n  ASSERT(!cached_route_);\n  refreshCachedRoute();\n\n  if (!state_.is_internally_created_) { // Only mutate tracing headers on first pass.\n    filter_manager_.streamInfo().setTraceReason(\n        ConnectionManagerUtility::mutateTracingRequestHeader(\n            *request_headers_, connection_manager_.runtime_, connection_manager_.config_,\n            cached_route_.value().get()));\n  }\n\n  filter_manager_.streamInfo().setRequestHeaders(*request_headers_);\n\n  const bool upgrade_rejected = filter_manager_.createFilterChain() == false;\n\n  if (connection_manager_.config_.flushAccessLogOnNewRequest()) {\n    filter_manager_.log(AccessLog::AccessLogType::DownstreamStart);\n  }\n\n  // TODO if there are no filters when starting a filter iteration, the connection manager\n  // should return 404. The current returns no response if there is no router filter.\n  if (hasCachedRoute()) {\n    // Do not allow upgrades if the route does not support it.\n    if (upgrade_rejected) {\n      // While downstream servers should not send upgrade payload without the upgrade being\n      // accepted, err on the side of caution and refuse to process any further requests on this\n      // connection, to avoid a class of HTTP/1.1 smuggling bugs where Upgrade or CONNECT payload\n      // contains a smuggled HTTP request.\n      filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(true);\n      connection_manager_.stats_.named_.downstream_rq_ws_on_non_ws_route_.inc();\n      sendLocalReply(Code::Forbidden, \"\", nullptr, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().UpgradeFailed);\n      return;\n    }\n    // Allow non websocket requests to go through websocket enabled routes.\n  }\n\n  // Check if tracing is enabled.\n  if (connection_manager_tracing_config_.has_value()) {\n    traceRequest();\n  }\n\n  filter_manager_.decodeHeaders(*request_headers_, end_stream);\n\n  // Reset it here for both global and overridden cases.\n  resetIdleTimer();\n}",
        "func": "void ConnectionManagerImpl::ActiveStream::decodeHeaders(RequestHeaderMapSharedPtr&& headers,\n                                                        bool end_stream) {\n  ENVOY_STREAM_LOG(debug, \"request headers complete (end_stream={}):\\n{}\", *this, end_stream,\n                   *headers);\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  request_headers_ = std::move(headers);\n  filter_manager_.requestHeadersInitialized();\n  if (request_header_timer_ != nullptr) {\n    request_header_timer_->disableTimer();\n    request_header_timer_.reset();\n  }\n\n  // Both shouldDrainConnectionUponCompletion() and is_head_request_ affect local replies: set them\n  // as early as possible.\n  const Protocol protocol = connection_manager_.codec_->protocol();\n  if (Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.http1_connection_close_header_in_redirect\")) {\n    if (HeaderUtility::shouldCloseConnection(protocol, *request_headers_)) {\n      // Only mark the connection to be closed if the request indicates so. The connection might\n      // already be marked so before this step, in which case if shouldCloseConnection() returns\n      // false, the stream info value shouldn't be overridden.\n      filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(true);\n    }\n  } else {\n    filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(\n        HeaderUtility::shouldCloseConnection(protocol, *request_headers_));\n  }\n\n  filter_manager_.streamInfo().protocol(protocol);\n\n  // We end the decode here to mark that the downstream stream is complete.\n  maybeEndDecode(end_stream);\n\n  if (!validateHeaders()) {\n    ENVOY_STREAM_LOG(debug, \"request headers validation failed\\n{}\", *this, *request_headers_);\n    return;\n  }\n\n  // We need to snap snapped_route_config_ here as it's used in mutateRequestHeaders later.\n  if (connection_manager_.config_.isRoutable()) {\n    if (connection_manager_.config_.routeConfigProvider() != nullptr) {\n      snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->configCast();\n    } else if (connection_manager_.config_.scopedRouteConfigProvider() != nullptr &&\n               connection_manager_.config_.scopeKeyBuilder().has_value()) {\n      snapped_scoped_routes_config_ =\n          connection_manager_.config_.scopedRouteConfigProvider()->config<Router::ScopedConfig>();\n      snapScopedRouteConfig();\n    }\n  } else {\n    snapped_route_config_ = connection_manager_.config_.routeConfigProvider()->configCast();\n  }\n\n  // Drop new requests when overloaded as soon as we have decoded the headers.\n  const bool drop_request_due_to_overload =\n      (connection_manager_.accept_new_http_stream_ != nullptr &&\n       connection_manager_.accept_new_http_stream_->shouldShedLoad()) ||\n      connection_manager_.random_generator_.bernoulli(\n          connection_manager_.overload_stop_accepting_requests_ref_.value());\n\n  if (drop_request_due_to_overload) {\n    // In this one special case, do not create the filter chain. If there is a risk of memory\n    // overload it is more important to avoid unnecessary allocation than to create the filters.\n    filter_manager_.skipFilterChainCreation();\n    connection_manager_.stats_.named_.downstream_rq_overload_close_.inc();\n    sendLocalReply(Http::Code::ServiceUnavailable, \"envoy overloaded\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().Overload);\n    return;\n  }\n\n  if (!connection_manager_.config_.proxy100Continue() && request_headers_->Expect() &&\n      // The Expect field-value is case-insensitive.\n      // https://tools.ietf.org/html/rfc7231#section-5.1.1\n      absl::EqualsIgnoreCase((request_headers_->Expect()->value().getStringView()),\n                             Headers::get().ExpectValues._100Continue)) {\n    // Note in the case Envoy is handling 100-Continue complexity, it skips the filter chain\n    // and sends the 100-Continue directly to the encoder.\n    chargeStats(continueHeader());\n    response_encoder_->encode1xxHeaders(continueHeader());\n    // Remove the Expect header so it won't be handled again upstream.\n    request_headers_->removeExpect();\n  }\n\n  connection_manager_.user_agent_.initializeFromHeaders(*request_headers_,\n                                                        connection_manager_.stats_.prefixStatName(),\n                                                        connection_manager_.stats_.scope_);\n\n  if (!request_headers_->Host()) {\n    // Require host header. For HTTP/1.1 Host has already been translated to :authority.\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().MissingHost);\n    return;\n  }\n\n  // Apply header sanity checks.\n  absl::optional<std::reference_wrapper<const absl::string_view>> error =\n      HeaderUtility::requestHeadersValid(*request_headers_);\n  if (error != absl::nullopt) {\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt, error.value().get());\n    if (!response_encoder_->streamErrorOnInvalidHttpMessage()) {\n      connection_manager_.handleCodecError(error.value().get());\n    }\n    return;\n  }\n\n  // Check for the existence of the :path header for non-CONNECT requests, or present-but-empty\n  // :path header for CONNECT requests. We expect the codec to have broken the path into pieces if\n  // applicable. NOTE: Currently the HTTP/1.1 codec only does this when the allow_absolute_url flag\n  // is enabled on the HCM.\n  if ((!HeaderUtility::isConnect(*request_headers_) || request_headers_->Path()) &&\n      request_headers_->getPathValue().empty()) {\n    sendLocalReply(Code::NotFound, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().MissingPath);\n    return;\n  }\n\n  // Rewrites the host of CONNECT-UDP requests.\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.enable_connect_udp_support\") &&\n      HeaderUtility::isConnectUdpRequest(*request_headers_) &&\n      !HeaderUtility::rewriteAuthorityForConnectUdp(*request_headers_)) {\n    sendLocalReply(Code::NotFound, \"The path is incorrect for CONNECT-UDP\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().InvalidPath);\n    return;\n  }\n\n  // Currently we only support relative paths at the application layer.\n  if (!request_headers_->getPathValue().empty() && request_headers_->getPathValue()[0] != '/') {\n    connection_manager_.stats_.named_.downstream_rq_non_relative_path_.inc();\n    sendLocalReply(Code::NotFound, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().AbsolutePath);\n    return;\n  }\n\n#ifndef ENVOY_ENABLE_UHV\n  // In UHV mode path normalization is done in the UHV\n  // Path sanitization should happen before any path access other than the above sanity check.\n  const auto action =\n      ConnectionManagerUtility::maybeNormalizePath(*request_headers_, connection_manager_.config_);\n  // gRPC requests are rejected if Envoy is configured to redirect post-normalization. This is\n  // because gRPC clients do not support redirect.\n  if (action == ConnectionManagerUtility::NormalizePathAction::Reject ||\n      (action == ConnectionManagerUtility::NormalizePathAction::Redirect &&\n       Grpc::Common::hasGrpcContentType(*request_headers_))) {\n    connection_manager_.stats_.named_.downstream_rq_failed_path_normalization_.inc();\n    sendLocalReply(Code::BadRequest, \"\", nullptr, absl::nullopt,\n                   StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  } else if (action == ConnectionManagerUtility::NormalizePathAction::Redirect) {\n    connection_manager_.stats_.named_.downstream_rq_redirected_with_normalized_path_.inc();\n    sendLocalReply(\n        Code::TemporaryRedirect, \"\",\n        [new_path = request_headers_->Path()->value().getStringView()](\n            Http::ResponseHeaderMap& response_headers) -> void {\n          response_headers.addReferenceKey(Http::Headers::get().Location, new_path);\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().PathNormalizationFailed);\n    return;\n  }\n\n  ASSERT(action == ConnectionManagerUtility::NormalizePathAction::Continue);\n#endif\n  auto optional_port = ConnectionManagerUtility::maybeNormalizeHost(\n      *request_headers_, connection_manager_.config_, localPort());\n  if (optional_port.has_value() &&\n      requestWasConnect(request_headers_, connection_manager_.codec_->protocol())) {\n    filter_manager_.streamInfo().filterState()->setData(\n        Router::OriginalConnectPort::key(),\n        std::make_unique<Router::OriginalConnectPort>(optional_port.value()),\n        StreamInfo::FilterState::StateType::ReadOnly, StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (!state_.is_internally_created_) { // Only sanitize headers on first pass.\n    // Modify the downstream remote address depending on configuration and headers.\n    const auto mutate_result = ConnectionManagerUtility::mutateRequestHeaders(\n        *request_headers_, connection_manager_.read_callbacks_->connection(),\n        connection_manager_.config_, *snapped_route_config_, connection_manager_.local_info_,\n        filter_manager_.streamInfo());\n\n    // IP detection failed, reject the request.\n    if (mutate_result.reject_request.has_value()) {\n      const auto& reject_request_params = mutate_result.reject_request.value();\n      connection_manager_.stats_.named_.downstream_rq_rejected_via_ip_detection_.inc();\n      sendLocalReply(reject_request_params.response_code, reject_request_params.body, nullptr,\n                     absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().OriginalIPDetectionFailed);\n      return;\n    }\n\n    filter_manager_.setDownstreamRemoteAddress(mutate_result.final_remote_address);\n  }\n  ASSERT(filter_manager_.streamInfo().downstreamAddressProvider().remoteAddress() != nullptr);\n\n  ASSERT(!cached_route_);\n  refreshCachedRoute();\n\n  if (!state_.is_internally_created_) { // Only mutate tracing headers on first pass.\n    filter_manager_.streamInfo().setTraceReason(\n        ConnectionManagerUtility::mutateTracingRequestHeader(\n            *request_headers_, connection_manager_.runtime_, connection_manager_.config_,\n            cached_route_.value().get()));\n  }\n\n  filter_manager_.streamInfo().setRequestHeaders(*request_headers_);\n\n  const bool upgrade_rejected = filter_manager_.createFilterChain() == false;\n\n  if (connection_manager_.config_.flushAccessLogOnNewRequest()) {\n    filter_manager_.log(AccessLog::AccessLogType::DownstreamStart);\n  }\n\n  // TODO if there are no filters when starting a filter iteration, the connection manager\n  // should return 404. The current returns no response if there is no router filter.\n  if (hasCachedRoute()) {\n    // Do not allow upgrades if the route does not support it.\n    if (upgrade_rejected) {\n      // While downstream servers should not send upgrade payload without the upgrade being\n      // accepted, err on the side of caution and refuse to process any further requests on this\n      // connection, to avoid a class of HTTP/1.1 smuggling bugs where Upgrade or CONNECT payload\n      // contains a smuggled HTTP request.\n      filter_manager_.streamInfo().setShouldDrainConnectionUponCompletion(true);\n      connection_manager_.stats_.named_.downstream_rq_ws_on_non_ws_route_.inc();\n      sendLocalReply(Code::Forbidden, \"\", nullptr, absl::nullopt,\n                     StreamInfo::ResponseCodeDetails::get().UpgradeFailed);\n      return;\n    }\n    // Allow non websocket requests to go through websocket enabled routes.\n  }\n\n  // Check if tracing is enabled.\n  if (connection_manager_tracing_config_.has_value()) {\n    traceRequest();\n  }\n\n  if (!connection_manager_.shouldDeferRequestProxyingToNextIoCycle()) {\n    filter_manager_.decodeHeaders(*request_headers_, end_stream);\n  } else {\n    state_.deferred_to_next_io_iteration_ = true;\n    state_.deferred_end_stream_ = end_stream;\n  }\n\n  // Reset it here for both global and overridden cases.\n  resetIdleTimer();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -231,7 +231,12 @@\n     traceRequest();\n   }\n \n-  filter_manager_.decodeHeaders(*request_headers_, end_stream);\n+  if (!connection_manager_.shouldDeferRequestProxyingToNextIoCycle()) {\n+    filter_manager_.decodeHeaders(*request_headers_, end_stream);\n+  } else {\n+    state_.deferred_to_next_io_iteration_ = true;\n+    state_.deferred_end_stream_ = end_stream;\n+  }\n \n   // Reset it here for both global and overridden cases.\n   resetIdleTimer();",
        "diff_line_info": {
            "deleted_lines": [
                "  filter_manager_.decodeHeaders(*request_headers_, end_stream);"
            ],
            "added_lines": [
                "  if (!connection_manager_.shouldDeferRequestProxyingToNextIoCycle()) {",
                "    filter_manager_.decodeHeaders(*request_headers_, end_stream);",
                "  } else {",
                "    state_.deferred_to_next_io_iteration_ = true;",
                "    state_.deferred_end_stream_ = end_stream;",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::onData",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "Network::FilterStatus ConnectionManagerImpl::onData(Buffer::Instance& data, bool) {\n  if (!codec_) {\n    // Http3 codec should have been instantiated by now.\n    createCodec(data);\n  }\n\n  bool redispatch;\n  do {\n    redispatch = false;\n\n    const Status status = codec_->dispatch(data);\n\n    if (isBufferFloodError(status) || isInboundFramesWithEmptyPayloadError(status)) {\n      handleCodecError(status.message());\n      return Network::FilterStatus::StopIteration;\n    } else if (isCodecProtocolError(status)) {\n      stats_.named_.downstream_cx_protocol_error_.inc();\n      handleCodecError(status.message());\n      return Network::FilterStatus::StopIteration;\n    } else if (isEnvoyOverloadError(status)) {\n      // The other codecs aren't wired to send this status.\n      ASSERT(codec_->protocol() < Protocol::Http2,\n             \"Expected only HTTP1.1 and below to send overload error.\");\n      stats_.named_.downstream_rq_overload_close_.inc();\n      handleCodecOverloadError(status.message());\n      return Network::FilterStatus::StopIteration;\n    }\n    ASSERT(status.ok());\n\n    // Processing incoming data may release outbound data so check for closure here as well.\n    checkForDeferredClose(false);\n\n    // The HTTP/1 codec will pause dispatch after a single message is complete. We want to\n    // either redispatch if there are no streams and we have more data. If we have a single\n    // complete non-WebSocket stream but have not responded yet we will pause socket reads\n    // to apply back pressure.\n    if (codec_->protocol() < Protocol::Http2) {\n      if (read_callbacks_->connection().state() == Network::Connection::State::Open &&\n          data.length() > 0 && streams_.empty()) {\n        redispatch = true;\n      }\n    }\n  } while (redispatch);\n\n  if (!read_callbacks_->connection().streamInfo().protocol()) {\n    read_callbacks_->connection().streamInfo().protocol(codec_->protocol());\n  }\n\n  return Network::FilterStatus::StopIteration;\n}",
        "func": "Network::FilterStatus ConnectionManagerImpl::onData(Buffer::Instance& data, bool) {\n  requests_during_dispatch_count_ = 0;\n  if (!codec_) {\n    // Http3 codec should have been instantiated by now.\n    createCodec(data);\n  }\n\n  bool redispatch;\n  do {\n    redispatch = false;\n\n    const Status status = codec_->dispatch(data);\n\n    if (isBufferFloodError(status) || isInboundFramesWithEmptyPayloadError(status)) {\n      handleCodecError(status.message());\n      return Network::FilterStatus::StopIteration;\n    } else if (isCodecProtocolError(status)) {\n      stats_.named_.downstream_cx_protocol_error_.inc();\n      handleCodecError(status.message());\n      return Network::FilterStatus::StopIteration;\n    } else if (isEnvoyOverloadError(status)) {\n      // The other codecs aren't wired to send this status.\n      ASSERT(codec_->protocol() < Protocol::Http2,\n             \"Expected only HTTP1.1 and below to send overload error.\");\n      stats_.named_.downstream_rq_overload_close_.inc();\n      handleCodecOverloadError(status.message());\n      return Network::FilterStatus::StopIteration;\n    }\n    ASSERT(status.ok());\n\n    // Processing incoming data may release outbound data so check for closure here as well.\n    checkForDeferredClose(false);\n\n    // The HTTP/1 codec will pause dispatch after a single message is complete. We want to\n    // either redispatch if there are no streams and we have more data. If we have a single\n    // complete non-WebSocket stream but have not responded yet we will pause socket reads\n    // to apply back pressure.\n    if (codec_->protocol() < Protocol::Http2) {\n      if (read_callbacks_->connection().state() == Network::Connection::State::Open &&\n          data.length() > 0 && streams_.empty()) {\n        redispatch = true;\n      }\n    }\n  } while (redispatch);\n\n  if (!read_callbacks_->connection().streamInfo().protocol()) {\n    read_callbacks_->connection().streamInfo().protocol(codec_->protocol());\n  }\n\n  return Network::FilterStatus::StopIteration;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n Network::FilterStatus ConnectionManagerImpl::onData(Buffer::Instance& data, bool) {\n+  requests_during_dispatch_count_ = 0;\n   if (!codec_) {\n     // Http3 codec should have been instantiated by now.\n     createCodec(data);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  requests_during_dispatch_count_ = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ActiveStream::decodeData",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "void ConnectionManagerImpl::ActiveStream::decodeData(Buffer::Instance& data, bool end_stream) {\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  maybeEndDecode(end_stream);\n  filter_manager_.streamInfo().addBytesReceived(data.length());\n\n  filter_manager_.decodeData(data, end_stream);\n}",
        "func": "void ConnectionManagerImpl::ActiveStream::decodeData(Buffer::Instance& data, bool end_stream) {\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  maybeEndDecode(end_stream);\n  filter_manager_.streamInfo().addBytesReceived(data.length());\n  if (!state_.deferred_to_next_io_iteration_) {\n    filter_manager_.decodeData(data, end_stream);\n  } else {\n    if (!deferred_data_) {\n      deferred_data_ = std::make_unique<Buffer::OwnedImpl>();\n    }\n    deferred_data_->move(data);\n    state_.deferred_end_stream_ = end_stream;\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,13 @@\n                                connection_manager_.read_callbacks_->connection().dispatcher());\n   maybeEndDecode(end_stream);\n   filter_manager_.streamInfo().addBytesReceived(data.length());\n-\n-  filter_manager_.decodeData(data, end_stream);\n+  if (!state_.deferred_to_next_io_iteration_) {\n+    filter_manager_.decodeData(data, end_stream);\n+  } else {\n+    if (!deferred_data_) {\n+      deferred_data_ = std::make_unique<Buffer::OwnedImpl>();\n+    }\n+    deferred_data_->move(data);\n+    state_.deferred_end_stream_ = end_stream;\n+  }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "  filter_manager_.decodeData(data, end_stream);"
            ],
            "added_lines": [
                "  if (!state_.deferred_to_next_io_iteration_) {",
                "    filter_manager_.decodeData(data, end_stream);",
                "  } else {",
                "    if (!deferred_data_) {",
                "      deferred_data_ = std::make_unique<Buffer::OwnedImpl>();",
                "    }",
                "    deferred_data_->move(data);",
                "    state_.deferred_end_stream_ = end_stream;",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ActiveStream::decodeTrailers",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "void ConnectionManagerImpl::ActiveStream::decodeTrailers(RequestTrailerMapPtr&& trailers) {\n  ENVOY_STREAM_LOG(debug, \"request trailers complete:\\n{}\", *this, *trailers);\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  resetIdleTimer();\n\n  ASSERT(!request_trailers_);\n  request_trailers_ = std::move(trailers);\n  if (!validateTrailers()) {\n    ENVOY_STREAM_LOG(debug, \"request trailers validation failed:\\n{}\", *this, *request_trailers_);\n    return;\n  }\n  maybeEndDecode(true);\n  filter_manager_.decodeTrailers(*request_trailers_);\n}",
        "func": "void ConnectionManagerImpl::ActiveStream::decodeTrailers(RequestTrailerMapPtr&& trailers) {\n  ENVOY_STREAM_LOG(debug, \"request trailers complete:\\n{}\", *this, *trailers);\n  ScopeTrackerScopeState scope(this,\n                               connection_manager_.read_callbacks_->connection().dispatcher());\n  resetIdleTimer();\n\n  ASSERT(!request_trailers_);\n  request_trailers_ = std::move(trailers);\n  if (!validateTrailers()) {\n    ENVOY_STREAM_LOG(debug, \"request trailers validation failed:\\n{}\", *this, *request_trailers_);\n    return;\n  }\n  maybeEndDecode(true);\n  if (!state_.deferred_to_next_io_iteration_) {\n    filter_manager_.decodeTrailers(*request_trailers_);\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,5 +11,7 @@\n     return;\n   }\n   maybeEndDecode(true);\n-  filter_manager_.decodeTrailers(*request_trailers_);\n+  if (!state_.deferred_to_next_io_iteration_) {\n+    filter_manager_.decodeTrailers(*request_trailers_);\n+  }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  filter_manager_.decodeTrailers(*request_trailers_);"
            ],
            "added_lines": [
                "  if (!state_.deferred_to_next_io_iteration_) {",
                "    filter_manager_.decodeTrailers(*request_trailers_);",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::ConnectionManagerImpl",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "ConnectionManagerImpl::ConnectionManagerImpl(ConnectionManagerConfig& config,\n                                             const Network::DrainDecision& drain_close,\n                                             Random::RandomGenerator& random_generator,\n                                             Http::Context& http_context, Runtime::Loader& runtime,\n                                             const LocalInfo::LocalInfo& local_info,\n                                             Upstream::ClusterManager& cluster_manager,\n                                             Server::OverloadManager& overload_manager,\n                                             TimeSource& time_source)\n    : config_(config), stats_(config_.stats()),\n      conn_length_(new Stats::HistogramCompletableTimespanImpl(\n          stats_.named_.downstream_cx_length_ms_, time_source)),\n      drain_close_(drain_close), user_agent_(http_context.userAgentContext()),\n      random_generator_(random_generator), runtime_(runtime), local_info_(local_info),\n      cluster_manager_(cluster_manager), listener_stats_(config_.listenerStats()),\n      overload_manager_(overload_manager),\n      overload_state_(overload_manager.getThreadLocalOverloadState()),\n      accept_new_http_stream_(overload_manager.getLoadShedPoint(\n          \"envoy.load_shed_points.http_connection_manager_decode_headers\")),\n      overload_stop_accepting_requests_ref_(\n          overload_state_.getState(Server::OverloadActionNames::get().StopAcceptingRequests)),\n      overload_disable_keepalive_ref_(\n          overload_state_.getState(Server::OverloadActionNames::get().DisableHttpKeepAlive)),\n      time_source_(time_source), proxy_name_(StreamInfo::ProxyStatusUtils::makeProxyName(\n                                     /*node_id=*/local_info_.node().id(),\n                                     /*server_name=*/config_.serverName(),\n                                     /*proxy_status_config=*/config_.proxyStatusConfig())),\n      refresh_rtt_after_request_(\n          Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.refresh_rtt_after_request\")) {\n  ENVOY_LOG_ONCE_IF(\n      trace, accept_new_http_stream_ == nullptr,\n      \"LoadShedPoint envoy.load_shed_points.http_connection_manager_decode_headers is not \"\n      \"found. Is it configured?\");\n}",
        "func": "ConnectionManagerImpl::ConnectionManagerImpl(ConnectionManagerConfig& config,\n                                             const Network::DrainDecision& drain_close,\n                                             Random::RandomGenerator& random_generator,\n                                             Http::Context& http_context, Runtime::Loader& runtime,\n                                             const LocalInfo::LocalInfo& local_info,\n                                             Upstream::ClusterManager& cluster_manager,\n                                             Server::OverloadManager& overload_manager,\n                                             TimeSource& time_source)\n    : config_(config), stats_(config_.stats()),\n      conn_length_(new Stats::HistogramCompletableTimespanImpl(\n          stats_.named_.downstream_cx_length_ms_, time_source)),\n      drain_close_(drain_close), user_agent_(http_context.userAgentContext()),\n      random_generator_(random_generator), runtime_(runtime), local_info_(local_info),\n      cluster_manager_(cluster_manager), listener_stats_(config_.listenerStats()),\n      overload_manager_(overload_manager),\n      overload_state_(overload_manager.getThreadLocalOverloadState()),\n      accept_new_http_stream_(overload_manager.getLoadShedPoint(\n          \"envoy.load_shed_points.http_connection_manager_decode_headers\")),\n      overload_stop_accepting_requests_ref_(\n          overload_state_.getState(Server::OverloadActionNames::get().StopAcceptingRequests)),\n      overload_disable_keepalive_ref_(\n          overload_state_.getState(Server::OverloadActionNames::get().DisableHttpKeepAlive)),\n      time_source_(time_source), proxy_name_(StreamInfo::ProxyStatusUtils::makeProxyName(\n                                     /*node_id=*/local_info_.node().id(),\n                                     /*server_name=*/config_.serverName(),\n                                     /*proxy_status_config=*/config_.proxyStatusConfig())),\n      max_requests_during_dispatch_(\n          runtime_.snapshot().getInteger(ConnectionManagerImpl::MaxRequestsPerIoCycle, UINT32_MAX)),\n      refresh_rtt_after_request_(\n          Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.refresh_rtt_after_request\")) {\n  ENVOY_LOG_ONCE_IF(\n      trace, accept_new_http_stream_ == nullptr,\n      \"LoadShedPoint envoy.load_shed_points.http_connection_manager_decode_headers is not \"\n      \"found. Is it configured?\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,6 +24,8 @@\n                                      /*node_id=*/local_info_.node().id(),\n                                      /*server_name=*/config_.serverName(),\n                                      /*proxy_status_config=*/config_.proxyStatusConfig())),\n+      max_requests_during_dispatch_(\n+          runtime_.snapshot().getInteger(ConnectionManagerImpl::MaxRequestsPerIoCycle, UINT32_MAX)),\n       refresh_rtt_after_request_(\n           Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.refresh_rtt_after_request\")) {\n   ENVOY_LOG_ONCE_IF(",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      max_requests_during_dispatch_(",
                "          runtime_.snapshot().getInteger(ConnectionManagerImpl::MaxRequestsPerIoCycle, UINT32_MAX)),"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::initializeReadFilterCallbacks",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cf687ac66b60f14a71e2a7e552943f138922a71d",
        "commit_title": "Limit the number of HTTP requests processed from a connection in I/O cycle",
        "commit_text": " ",
        "func_before": "void ConnectionManagerImpl::initializeReadFilterCallbacks(Network::ReadFilterCallbacks& callbacks) {\n  read_callbacks_ = &callbacks;\n  dispatcher_ = &callbacks.connection().dispatcher();\n\n  stats_.named_.downstream_cx_total_.inc();\n  stats_.named_.downstream_cx_active_.inc();\n  if (read_callbacks_->connection().ssl()) {\n    stats_.named_.downstream_cx_ssl_total_.inc();\n    stats_.named_.downstream_cx_ssl_active_.inc();\n  }\n\n  read_callbacks_->connection().addConnectionCallbacks(*this);\n\n  if (config_.addProxyProtocolConnectionState() &&\n      !read_callbacks_->connection()\n           .streamInfo()\n           .filterState()\n           ->hasData<Network::ProxyProtocolFilterState>(Network::ProxyProtocolFilterState::key())) {\n    read_callbacks_->connection().streamInfo().filterState()->setData(\n        Network::ProxyProtocolFilterState::key(),\n        std::make_unique<Network::ProxyProtocolFilterState>(Network::ProxyProtocolData{\n            read_callbacks_->connection().connectionInfoProvider().remoteAddress(),\n            read_callbacks_->connection().connectionInfoProvider().localAddress()}),\n        StreamInfo::FilterState::StateType::ReadOnly,\n        StreamInfo::FilterState::LifeSpan::Connection);\n  }\n\n  if (config_.idleTimeout()) {\n    connection_idle_timer_ =\n        dispatcher_->createScaledTimer(Event::ScaledTimerType::HttpDownstreamIdleConnectionTimeout,\n                                       [this]() -> void { onIdleTimeout(); });\n    connection_idle_timer_->enableTimer(config_.idleTimeout().value());\n  }\n\n  if (config_.maxConnectionDuration()) {\n    connection_duration_timer_ =\n        dispatcher_->createTimer([this]() -> void { onConnectionDurationTimeout(); });\n    connection_duration_timer_->enableTimer(config_.maxConnectionDuration().value());\n  }\n\n  read_callbacks_->connection().setDelayedCloseTimeout(config_.delayedCloseTimeout());\n\n  read_callbacks_->connection().setConnectionStats(\n      {stats_.named_.downstream_cx_rx_bytes_total_, stats_.named_.downstream_cx_rx_bytes_buffered_,\n       stats_.named_.downstream_cx_tx_bytes_total_, stats_.named_.downstream_cx_tx_bytes_buffered_,\n       nullptr, &stats_.named_.downstream_cx_delayed_close_timeout_});\n}",
        "func": "void ConnectionManagerImpl::initializeReadFilterCallbacks(Network::ReadFilterCallbacks& callbacks) {\n  read_callbacks_ = &callbacks;\n  dispatcher_ = &callbacks.connection().dispatcher();\n  if (max_requests_during_dispatch_ != UINT32_MAX) {\n    deferred_request_processing_callback_ =\n        dispatcher_->createSchedulableCallback([this]() -> void { onDeferredRequestProcessing(); });\n  }\n\n  stats_.named_.downstream_cx_total_.inc();\n  stats_.named_.downstream_cx_active_.inc();\n  if (read_callbacks_->connection().ssl()) {\n    stats_.named_.downstream_cx_ssl_total_.inc();\n    stats_.named_.downstream_cx_ssl_active_.inc();\n  }\n\n  read_callbacks_->connection().addConnectionCallbacks(*this);\n\n  if (config_.addProxyProtocolConnectionState() &&\n      !read_callbacks_->connection()\n           .streamInfo()\n           .filterState()\n           ->hasData<Network::ProxyProtocolFilterState>(Network::ProxyProtocolFilterState::key())) {\n    read_callbacks_->connection().streamInfo().filterState()->setData(\n        Network::ProxyProtocolFilterState::key(),\n        std::make_unique<Network::ProxyProtocolFilterState>(Network::ProxyProtocolData{\n            read_callbacks_->connection().connectionInfoProvider().remoteAddress(),\n            read_callbacks_->connection().connectionInfoProvider().localAddress()}),\n        StreamInfo::FilterState::StateType::ReadOnly,\n        StreamInfo::FilterState::LifeSpan::Connection);\n  }\n\n  if (config_.idleTimeout()) {\n    connection_idle_timer_ =\n        dispatcher_->createScaledTimer(Event::ScaledTimerType::HttpDownstreamIdleConnectionTimeout,\n                                       [this]() -> void { onIdleTimeout(); });\n    connection_idle_timer_->enableTimer(config_.idleTimeout().value());\n  }\n\n  if (config_.maxConnectionDuration()) {\n    connection_duration_timer_ =\n        dispatcher_->createTimer([this]() -> void { onConnectionDurationTimeout(); });\n    connection_duration_timer_->enableTimer(config_.maxConnectionDuration().value());\n  }\n\n  read_callbacks_->connection().setDelayedCloseTimeout(config_.delayedCloseTimeout());\n\n  read_callbacks_->connection().setConnectionStats(\n      {stats_.named_.downstream_cx_rx_bytes_total_, stats_.named_.downstream_cx_rx_bytes_buffered_,\n       stats_.named_.downstream_cx_tx_bytes_total_, stats_.named_.downstream_cx_tx_bytes_buffered_,\n       nullptr, &stats_.named_.downstream_cx_delayed_close_timeout_});\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,10 @@\n void ConnectionManagerImpl::initializeReadFilterCallbacks(Network::ReadFilterCallbacks& callbacks) {\n   read_callbacks_ = &callbacks;\n   dispatcher_ = &callbacks.connection().dispatcher();\n+  if (max_requests_during_dispatch_ != UINT32_MAX) {\n+    deferred_request_processing_callback_ =\n+        dispatcher_->createSchedulableCallback([this]() -> void { onDeferredRequestProcessing(); });\n+  }\n \n   stats_.named_.downstream_cx_total_.inc();\n   stats_.named_.downstream_cx_active_.inc();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (max_requests_during_dispatch_ != UINT32_MAX) {",
                "    deferred_request_processing_callback_ =",
                "        dispatcher_->createSchedulableCallback([this]() -> void { onDeferredRequestProcessing(); });",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/HTTPSession::setupCodec",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void HTTPSession::setupCodec() {\n  if (!codec_->supportsParallelRequests()) {\n    // until we support upstream pipelining\n    maxConcurrentIncomingStreams_ = 1;\n    maxConcurrentOutgoingStreamsRemote_ = isDownstream() ? 0 : 1;\n  }\n\n  // If a secondary authentication manager is configured for this session, set\n  // the SETTINGS_HTTP_CERT_AUTH to indicate support for HTTP-layer certificate\n  // authentication.\n  uint32_t certAuthSettingVal = 0;\n  if (secondAuthManager_) {\n    certAuthSettingVal = getCertAuthSettingVal();\n  }\n  HTTPSettings* settings = codec_->getEgressSettings();\n  if (settings) {\n    settings->setSetting(SettingsId::MAX_CONCURRENT_STREAMS,\n                         maxConcurrentIncomingStreams_);\n    if (certAuthSettingVal != 0) {\n      settings->setSetting(SettingsId::SETTINGS_HTTP_CERT_AUTH,\n                           certAuthSettingVal);\n    }\n  }\n  codec_->generateConnectionPreface(writeBuf_);\n\n  if (codec_->supportsSessionFlowControl() && !connFlowControl_) {\n    connFlowControl_ = new FlowControlFilter(*this, writeBuf_, codec_.call());\n    codec_.addFilters(std::unique_ptr<FlowControlFilter>(connFlowControl_));\n    // if we really support switching from spdy <-> h2, we need to update\n    // existing flow control filter\n  }\n  if (codec_->supportsParallelRequests() && !controlMessageRateLimitFilter_ &&\n      sock_) {\n    controlMessageRateLimitFilter_ = new ControlMessageRateLimitFilter(\n        &getEventBase()->timer(), sessionStats_);\n    codec_.addFilters(std::unique_ptr<ControlMessageRateLimitFilter>(\n        controlMessageRateLimitFilter_));\n  }\n\n  codec_.setCallback(this);\n}",
        "func": "void HTTPSession::setupCodec() {\n  if (!codec_->supportsParallelRequests()) {\n    // until we support upstream pipelining\n    maxConcurrentIncomingStreams_ = 1;\n    maxConcurrentOutgoingStreamsRemote_ = isDownstream() ? 0 : 1;\n  }\n\n  // If a secondary authentication manager is configured for this session, set\n  // the SETTINGS_HTTP_CERT_AUTH to indicate support for HTTP-layer certificate\n  // authentication.\n  uint32_t certAuthSettingVal = 0;\n  if (secondAuthManager_) {\n    certAuthSettingVal = getCertAuthSettingVal();\n  }\n  HTTPSettings* settings = codec_->getEgressSettings();\n  if (settings) {\n    settings->setSetting(SettingsId::MAX_CONCURRENT_STREAMS,\n                         maxConcurrentIncomingStreams_);\n    if (certAuthSettingVal != 0) {\n      settings->setSetting(SettingsId::SETTINGS_HTTP_CERT_AUTH,\n                           certAuthSettingVal);\n    }\n  }\n  codec_->generateConnectionPreface(writeBuf_);\n\n  if (codec_->supportsSessionFlowControl() && !connFlowControl_) {\n    connFlowControl_ = new FlowControlFilter(*this, writeBuf_, codec_.call());\n    codec_.addFilters(std::unique_ptr<FlowControlFilter>(connFlowControl_));\n    // if we really support switching from spdy <-> h2, we need to update\n    // existing flow control filter\n  }\n  if (codec_->supportsParallelRequests() && !controlMessageRateLimitFilter_ &&\n      sock_ &&\n      codec_->getTransportDirection() == TransportDirection::DOWNSTREAM) {\n    controlMessageRateLimitFilter_ = new ControlMessageRateLimitFilter(\n        &getEventBase()->timer(), sessionStats_);\n    codec_.addFilters(std::unique_ptr<ControlMessageRateLimitFilter>(\n        controlMessageRateLimitFilter_));\n  }\n\n  codec_.setCallback(this);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -30,7 +30,8 @@\n     // existing flow control filter\n   }\n   if (codec_->supportsParallelRequests() && !controlMessageRateLimitFilter_ &&\n-      sock_) {\n+      sock_ &&\n+      codec_->getTransportDirection() == TransportDirection::DOWNSTREAM) {\n     controlMessageRateLimitFilter_ = new ControlMessageRateLimitFilter(\n         &getEventBase()->timer(), sessionStats_);\n     codec_.addFilters(std::unique_ptr<ControlMessageRateLimitFilter>(",
        "diff_line_info": {
            "deleted_lines": [
                "      sock_) {"
            ],
            "added_lines": [
                "      sock_ &&",
                "      codec_->getTransportDirection() == TransportDirection::DOWNSTREAM) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/TLHTTPSessionStats::TLHTTPSessionStats",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "TLHTTPSessionStats::TLHTTPSessionStats(const std::string& prefix)\n    : txnsOpen(prefix + \"_transactions_open\"),\n      pendingBufferedReadBytes(prefix + \"_pending_buffered_read_bytes\"),\n      pendingBufferedWriteBytes(prefix + \"_pending_buffered_write_bytes\"),\n      txnsOpened(prefix + \"_txn_opened\", facebook::fb303::SUM),\n      txnsFromSessionReuse(prefix + \"_txn_session_reuse\", facebook::fb303::SUM),\n      txnsTransactionStalled(prefix + \"_txn_transaction_stall\",\n                             facebook::fb303::SUM),\n      txnsSessionStalled(prefix + \"_txn_session_stall\", facebook::fb303::SUM),\n      egressContentLengthMismatches(\n          prefix + \"_egress_content_length_mismatches\", facebook::fb303::SUM),\n      sessionPeriodicPingProbeTimeout(\n          prefix + \"_session_periodic_ping_probe_timeout\",\n          facebook::fb303::SUM),\n      presendIoSplit(prefix + \"_presend_io_split\", facebook::fb303::SUM),\n      presendExceedLimit(prefix + \"_presend_exceed_limit\",\n                         facebook::fb303::SUM),\n      ttlbaTracked(prefix + \"_ttlba_tracked\", facebook::fb303::SUM),\n      ttlbaReceived(prefix + \"_ttlba_received\", facebook::fb303::SUM),\n      ttlbaTimeout(prefix + \"_ttlba_timeout\", facebook::fb303::SUM),\n      ttlbaNotFound(prefix + \"_ttlba_not_found\", facebook::fb303::SUM),\n      ttlbaExceedLimit(prefix + \"_ttlba_exceed_limit\", facebook::fb303::SUM),\n      ttbtxTracked(prefix + \"_ttbtx_tracked\", facebook::fb303::SUM),\n      ttbtxReceived(prefix + \"_ttbtx_received\", facebook::fb303::SUM),\n      ttbtxTimeout(prefix + \"_ttbtx_timeout\", facebook::fb303::SUM),\n      ttbtxNotFound(prefix + \"_ttbtx_not_found\", facebook::fb303::SUM),\n      ttbtxExceedLimit(prefix + \"_ttbtx_exceed_limit\", facebook::fb303::SUM),\n      ctrlMsgsRateLimited(prefix + \"_ctrl_msgs_rate_limited\",\n                          facebook::fb303::SUM),\n      txnsPerSession(prefix + \"_txn_per_session\",\n                     1,\n                     0,\n                     999,\n                     facebook::fb303::AVG,\n                     50,\n                     95,\n                     99),\n      sessionIdleTime(prefix + \"_session_idle_time\",\n                      1,\n                      0,\n                      150,\n                      facebook::fb303::AVG,\n                      50,\n                      75,\n                      95,\n                      99),\n      ctrlMsgsInInterval(\n          prefix + \"_ctrl_msgs_in_interval\",\n          1 /* bucketWidth */,\n          0 /* min */,\n          500 /* max, keep in sync with kDefaultMaxControlMsgsPerInterval */,\n          facebook::fb303::AVG,\n          50,\n          99,\n          100) {\n}",
        "func": "TLHTTPSessionStats::TLHTTPSessionStats(const std::string& prefix)\n    : txnsOpen(prefix + \"_transactions_open\"),\n      pendingBufferedReadBytes(prefix + \"_pending_buffered_read_bytes\"),\n      pendingBufferedWriteBytes(prefix + \"_pending_buffered_write_bytes\"),\n      txnsOpened(prefix + \"_txn_opened\", facebook::fb303::SUM),\n      txnsFromSessionReuse(prefix + \"_txn_session_reuse\", facebook::fb303::SUM),\n      txnsTransactionStalled(prefix + \"_txn_transaction_stall\",\n                             facebook::fb303::SUM),\n      txnsSessionStalled(prefix + \"_txn_session_stall\", facebook::fb303::SUM),\n      egressContentLengthMismatches(\n          prefix + \"_egress_content_length_mismatches\", facebook::fb303::SUM),\n      sessionPeriodicPingProbeTimeout(\n          prefix + \"_session_periodic_ping_probe_timeout\",\n          facebook::fb303::SUM),\n      presendIoSplit(prefix + \"_presend_io_split\", facebook::fb303::SUM),\n      presendExceedLimit(prefix + \"_presend_exceed_limit\",\n                         facebook::fb303::SUM),\n      ttlbaTracked(prefix + \"_ttlba_tracked\", facebook::fb303::SUM),\n      ttlbaReceived(prefix + \"_ttlba_received\", facebook::fb303::SUM),\n      ttlbaTimeout(prefix + \"_ttlba_timeout\", facebook::fb303::SUM),\n      ttlbaNotFound(prefix + \"_ttlba_not_found\", facebook::fb303::SUM),\n      ttlbaExceedLimit(prefix + \"_ttlba_exceed_limit\", facebook::fb303::SUM),\n      ttbtxTracked(prefix + \"_ttbtx_tracked\", facebook::fb303::SUM),\n      ttbtxReceived(prefix + \"_ttbtx_received\", facebook::fb303::SUM),\n      ttbtxTimeout(prefix + \"_ttbtx_timeout\", facebook::fb303::SUM),\n      ttbtxNotFound(prefix + \"_ttbtx_not_found\", facebook::fb303::SUM),\n      ttbtxExceedLimit(prefix + \"_ttbtx_exceed_limit\", facebook::fb303::SUM),\n      ctrlMsgsRateLimited(prefix + \"_ctrl_msgs_rate_limited\",\n                          facebook::fb303::SUM),\n      headersRateLimited(prefix + \"_headers_rate_limited\",\n                         facebook::fb303::SUM),\n      txnsPerSession(prefix + \"_txn_per_session\",\n                     1,\n                     0,\n                     999,\n                     facebook::fb303::AVG,\n                     50,\n                     95,\n                     99),\n      sessionIdleTime(prefix + \"_session_idle_time\",\n                      1,\n                      0,\n                      150,\n                      facebook::fb303::AVG,\n                      50,\n                      75,\n                      95,\n                      99),\n      ctrlMsgsInInterval(\n          prefix + \"_ctrl_msgs_in_interval\",\n          1 /* bucketWidth */,\n          0 /* min */,\n          500 /* max, keep in sync with kDefaultMaxControlMsgsPerInterval */,\n          facebook::fb303::AVG,\n          50,\n          99,\n          100),\n      headersInInterval(\n          prefix + \"_headers_in_interval\",\n          1 /* bucketWidth */,\n          0 /* min */,\n          500 /* max, keep in sync with kDefaultMaxHeadersMsgsPerInterval */,\n          facebook::fb303::AVG,\n          50,\n          99,\n          100) {\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,8 @@\n       ttbtxExceedLimit(prefix + \"_ttbtx_exceed_limit\", facebook::fb303::SUM),\n       ctrlMsgsRateLimited(prefix + \"_ctrl_msgs_rate_limited\",\n                           facebook::fb303::SUM),\n+      headersRateLimited(prefix + \"_headers_rate_limited\",\n+                         facebook::fb303::SUM),\n       txnsPerSession(prefix + \"_txn_per_session\",\n                      1,\n                      0,\n@@ -52,5 +54,14 @@\n           facebook::fb303::AVG,\n           50,\n           99,\n+          100),\n+      headersInInterval(\n+          prefix + \"_headers_in_interval\",\n+          1 /* bucketWidth */,\n+          0 /* min */,\n+          500 /* max, keep in sync with kDefaultMaxHeadersMsgsPerInterval */,\n+          facebook::fb303::AVG,\n+          50,\n+          99,\n           100) {\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "      headersRateLimited(prefix + \"_headers_rate_limited\",",
                "                         facebook::fb303::SUM),",
                "          100),",
                "      headersInInterval(",
                "          prefix + \"_headers_in_interval\",",
                "          1 /* bucketWidth */,",
                "          0 /* min */,",
                "          500 /* max, keep in sync with kDefaultMaxHeadersMsgsPerInterval */,",
                "          facebook::fb303::AVG,",
                "          50,",
                "          99,"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/HTTPSessionBase::setControlMessageRateLimitParams",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void HTTPSessionBase::setControlMessageRateLimitParams(\n    uint32_t maxControlMsgsPerInterval,\n    uint32_t maxDirectErrorHandlingPerInterval,\n    std::chrono::milliseconds controlMsgIntervalDuration,\n    std::chrono::milliseconds directErrorHandlingIntervalDuration) {\n  if (maxControlMsgsPerInterval < kMaxControlMsgsPerIntervalLowerBound) {\n    XLOG_EVERY_MS(WARNING, 60000)\n        << \"Invalid maxControlMsgsPerInterval: \" << maxControlMsgsPerInterval;\n    maxControlMsgsPerInterval = kMaxControlMsgsPerIntervalLowerBound;\n  }\n\n  if (maxDirectErrorHandlingPerInterval <\n      kMaxDirectErrorHandlingPerIntervalLowerBound) {\n    XLOG_EVERY_MS(WARNING, 60000)\n        << \"Invalid maxDirectErrorHandlingPerInterval: \"\n        << maxDirectErrorHandlingPerInterval;\n    maxDirectErrorHandlingPerInterval =\n        kMaxDirectErrorHandlingPerIntervalLowerBound;\n  }\n\n  if (controlMessageRateLimitFilter_) {\n    controlMessageRateLimitFilter_->setParams(\n        maxControlMsgsPerInterval,\n        maxDirectErrorHandlingPerInterval,\n        controlMsgIntervalDuration,\n        directErrorHandlingIntervalDuration);\n  }\n}",
        "func": "void HTTPSessionBase::setControlMessageRateLimitParams(\n    uint32_t maxControlMsgsPerInterval,\n    uint32_t maxDirectErrorHandlingPerInterval,\n    uint32_t maxHeadersPerInterval,\n    std::chrono::milliseconds controlMsgIntervalDuration,\n    std::chrono::milliseconds directErrorHandlingIntervalDuration,\n    std::chrono::milliseconds headersIntervalDuration) {\n\n  if (maxControlMsgsPerInterval < kMaxControlMsgsPerIntervalLowerBound) {\n    XLOG_EVERY_MS(WARNING, 60000)\n        << \"Invalid maxControlMsgsPerInterval: \" << maxControlMsgsPerInterval;\n    maxControlMsgsPerInterval = kMaxControlMsgsPerIntervalLowerBound;\n  }\n\n  if (maxDirectErrorHandlingPerInterval <\n      kMaxDirectErrorHandlingPerIntervalLowerBound) {\n    XLOG_EVERY_MS(WARNING, 60000)\n        << \"Invalid maxDirectErrorHandlingPerInterval: \"\n        << maxDirectErrorHandlingPerInterval;\n    maxDirectErrorHandlingPerInterval =\n        kMaxDirectErrorHandlingPerIntervalLowerBound;\n  }\n\n  if (maxHeadersPerInterval < kMaxHeadersPerIntervalLowerBound) {\n    XLOG_EVERY_MS(WARNING, 60000)\n        << \"Invalid maxHeadersPerInterval: \" << maxHeadersPerInterval;\n    maxHeadersPerInterval = kMaxHeadersPerIntervalLowerBound;\n  }\n\n  if (controlMessageRateLimitFilter_) {\n    controlMessageRateLimitFilter_->setParams(\n        maxControlMsgsPerInterval,\n        maxDirectErrorHandlingPerInterval,\n        maxHeadersPerInterval,\n        controlMsgIntervalDuration,\n        directErrorHandlingIntervalDuration,\n        headersIntervalDuration);\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,11 @@\n void HTTPSessionBase::setControlMessageRateLimitParams(\n     uint32_t maxControlMsgsPerInterval,\n     uint32_t maxDirectErrorHandlingPerInterval,\n+    uint32_t maxHeadersPerInterval,\n     std::chrono::milliseconds controlMsgIntervalDuration,\n-    std::chrono::milliseconds directErrorHandlingIntervalDuration) {\n+    std::chrono::milliseconds directErrorHandlingIntervalDuration,\n+    std::chrono::milliseconds headersIntervalDuration) {\n+\n   if (maxControlMsgsPerInterval < kMaxControlMsgsPerIntervalLowerBound) {\n     XLOG_EVERY_MS(WARNING, 60000)\n         << \"Invalid maxControlMsgsPerInterval: \" << maxControlMsgsPerInterval;\n@@ -18,11 +21,19 @@\n         kMaxDirectErrorHandlingPerIntervalLowerBound;\n   }\n \n+  if (maxHeadersPerInterval < kMaxHeadersPerIntervalLowerBound) {\n+    XLOG_EVERY_MS(WARNING, 60000)\n+        << \"Invalid maxHeadersPerInterval: \" << maxHeadersPerInterval;\n+    maxHeadersPerInterval = kMaxHeadersPerIntervalLowerBound;\n+  }\n+\n   if (controlMessageRateLimitFilter_) {\n     controlMessageRateLimitFilter_->setParams(\n         maxControlMsgsPerInterval,\n         maxDirectErrorHandlingPerInterval,\n+        maxHeadersPerInterval,\n         controlMsgIntervalDuration,\n-        directErrorHandlingIntervalDuration);\n+        directErrorHandlingIntervalDuration,\n+        headersIntervalDuration);\n   }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    std::chrono::milliseconds directErrorHandlingIntervalDuration) {",
                "        directErrorHandlingIntervalDuration);"
            ],
            "added_lines": [
                "    uint32_t maxHeadersPerInterval,",
                "    std::chrono::milliseconds directErrorHandlingIntervalDuration,",
                "    std::chrono::milliseconds headersIntervalDuration) {",
                "",
                "  if (maxHeadersPerInterval < kMaxHeadersPerIntervalLowerBound) {",
                "    XLOG_EVERY_MS(WARNING, 60000)",
                "        << \"Invalid maxHeadersPerInterval: \" << maxHeadersPerInterval;",
                "    maxHeadersPerInterval = kMaxHeadersPerIntervalLowerBound;",
                "  }",
                "",
                "        maxHeadersPerInterval,",
                "        directErrorHandlingIntervalDuration,",
                "        headersIntervalDuration);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/ControlMessageRateLimitFilter",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "explicit ControlMessageRateLimitFilter(folly::HHWheelTimer* timer,\n                                         HTTPSessionStats* httpSessionStats)\n      : resetControlMessages_(numControlMsgsInCurrentInterval_,\n                              httpSessionStats),\n        timer_(timer),\n        httpSessionStats_(httpSessionStats) {\n  }",
        "func": "explicit ControlMessageRateLimitFilter(folly::HHWheelTimer* timer,\n                                         HTTPSessionStats* httpSessionStats)\n      : resetControlMessages_(numControlMsgsInCurrentInterval_,\n                              RateLimitTarget::CONTROL_MSGS,\n                              httpSessionStats),\n        resetDirectErrors_(numDirectErrorHandlingInCurrentInterval_,\n                           RateLimitTarget::DIRECT_ERROR_HANDLING,\n                           httpSessionStats),\n        resetHeaders_(numHeadersInCurrentInterval_,\n                      RateLimitTarget::HEADERS,\n                      httpSessionStats),\n        timer_(timer),\n        httpSessionStats_(httpSessionStats) {\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,14 @@\n explicit ControlMessageRateLimitFilter(folly::HHWheelTimer* timer,\n                                          HTTPSessionStats* httpSessionStats)\n       : resetControlMessages_(numControlMsgsInCurrentInterval_,\n+                              RateLimitTarget::CONTROL_MSGS,\n                               httpSessionStats),\n+        resetDirectErrors_(numDirectErrorHandlingInCurrentInterval_,\n+                           RateLimitTarget::DIRECT_ERROR_HANDLING,\n+                           httpSessionStats),\n+        resetHeaders_(numHeadersInCurrentInterval_,\n+                      RateLimitTarget::HEADERS,\n+                      httpSessionStats),\n         timer_(timer),\n         httpSessionStats_(httpSessionStats) {\n   }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                              RateLimitTarget::CONTROL_MSGS,",
                "        resetDirectErrors_(numDirectErrorHandlingInCurrentInterval_,",
                "                           RateLimitTarget::DIRECT_ERROR_HANDLING,",
                "                           httpSessionStats),",
                "        resetHeaders_(numHeadersInCurrentInterval_,",
                "                      RateLimitTarget::HEADERS,",
                "                      httpSessionStats),"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/timeoutExpired",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void timeoutExpired() noexcept override {\n      if (counter > 0 && httpSessionStats) {\n        httpSessionStats->recordControlMsgsInInterval(counter);\n      }\n      counter = 0;\n    }",
        "func": "void timeoutExpired() noexcept override {\n      if (counter > 0 && httpSessionStats) {\n        switch (rateLimitTarget) {\n          case RateLimitTarget::CONTROL_MSGS:\n            httpSessionStats->recordControlMsgsInInterval(counter);\n            break;\n          case RateLimitTarget::DIRECT_ERROR_HANDLING:\n            // No stats for this one\n            break;\n          case RateLimitTarget::HEADERS:\n            httpSessionStats->recordHeadersInInterval(counter);\n            break;\n        }\n      }\n      counter = 0;\n    }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,16 @@\n void timeoutExpired() noexcept override {\n       if (counter > 0 && httpSessionStats) {\n-        httpSessionStats->recordControlMsgsInInterval(counter);\n+        switch (rateLimitTarget) {\n+          case RateLimitTarget::CONTROL_MSGS:\n+            httpSessionStats->recordControlMsgsInInterval(counter);\n+            break;\n+          case RateLimitTarget::DIRECT_ERROR_HANDLING:\n+            // No stats for this one\n+            break;\n+          case RateLimitTarget::HEADERS:\n+            httpSessionStats->recordHeadersInInterval(counter);\n+            break;\n+        }\n       }\n       counter = 0;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "        httpSessionStats->recordControlMsgsInInterval(counter);"
            ],
            "added_lines": [
                "        switch (rateLimitTarget) {",
                "          case RateLimitTarget::CONTROL_MSGS:",
                "            httpSessionStats->recordControlMsgsInInterval(counter);",
                "            break;",
                "          case RateLimitTarget::DIRECT_ERROR_HANDLING:",
                "            // No stats for this one",
                "            break;",
                "          case RateLimitTarget::HEADERS:",
                "            httpSessionStats->recordHeadersInInterval(counter);",
                "            break;",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/ResetCounterTimeout",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "explicit ResetCounterTimeout(uint32_t& counterIn,\n                                 HTTPSessionStats* httpSessionStatsIn = nullptr)\n        : counter(counterIn), httpSessionStats(httpSessionStatsIn) {\n    }",
        "func": "explicit ResetCounterTimeout(uint32_t& counterIn,\n                                 RateLimitTarget rateLimitTargetIn,\n                                 HTTPSessionStats* httpSessionStatsIn = nullptr)\n        : counter(counterIn),\n          rateLimitTarget(rateLimitTargetIn),\n          httpSessionStats(httpSessionStatsIn) {\n    }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,7 @@\n explicit ResetCounterTimeout(uint32_t& counterIn,\n+                                 RateLimitTarget rateLimitTargetIn,\n                                  HTTPSessionStats* httpSessionStatsIn = nullptr)\n-        : counter(counterIn), httpSessionStats(httpSessionStatsIn) {\n+        : counter(counterIn),\n+          rateLimitTarget(rateLimitTargetIn),\n+          httpSessionStats(httpSessionStatsIn) {\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "        : counter(counterIn), httpSessionStats(httpSessionStatsIn) {"
            ],
            "added_lines": [
                "                                 RateLimitTarget rateLimitTargetIn,",
                "        : counter(counterIn),",
                "          rateLimitTarget(rateLimitTargetIn),",
                "          httpSessionStats(httpSessionStatsIn) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/detachThreadLocals",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void detachThreadLocals() {\n    resetControlMessages_.cancelTimeout();\n    resetDirectErrors_.cancelTimeout();\n    timer_ = nullptr;\n    // Free pass when switching threads\n    numControlMsgsInCurrentInterval_ = 0;\n    numDirectErrorHandlingInCurrentInterval_ = 0;\n  }",
        "func": "void detachThreadLocals() {\n    resetControlMessages_.cancelTimeout();\n    resetDirectErrors_.cancelTimeout();\n    resetHeaders_.cancelTimeout();\n    timer_ = nullptr;\n    // Free pass when switching threads\n    numControlMsgsInCurrentInterval_ = 0;\n    numDirectErrorHandlingInCurrentInterval_ = 0;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,7 @@\n void detachThreadLocals() {\n     resetControlMessages_.cancelTimeout();\n     resetDirectErrors_.cancelTimeout();\n+    resetHeaders_.cancelTimeout();\n     timer_ = nullptr;\n     // Free pass when switching threads\n     numControlMsgsInCurrentInterval_ = 0;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    resetHeaders_.cancelTimeout();"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/setSessionStats",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void setSessionStats(HTTPSessionStats* httpSessionStats) {\n    httpSessionStats_ = httpSessionStats;\n    resetControlMessages_.httpSessionStats = httpSessionStats;\n  }",
        "func": "void setSessionStats(HTTPSessionStats* httpSessionStats) {\n    httpSessionStats_ = httpSessionStats;\n    resetControlMessages_.httpSessionStats = httpSessionStats;\n    resetHeaders_.httpSessionStats = httpSessionStats;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,5 @@\n void setSessionStats(HTTPSessionStats* httpSessionStats) {\n     httpSessionStats_ = httpSessionStats;\n     resetControlMessages_.httpSessionStats = httpSessionStats;\n+    resetHeaders_.httpSessionStats = httpSessionStats;\n   }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    resetHeaders_.httpSessionStats = httpSessionStats;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "facebook/proxygen/setParams",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/facebook/proxygen/commit/0a00c43d64c58a85e6250120f2377302675b0fe3",
        "commit_title": "Re-sync with internal repository",
        "commit_text": " The internal and external repositories are out of sync. This Pull Request attempts to brings them back in sync by patching the GitHub repository. Please carefully review this patch. You must disable ShipIt for your project in order to merge this pull request. DO NOT IMPORT this pull request. Instead, merge it directly on GitHub using the MERGE BUTTON. Re-enable ShipIt after merging.",
        "func_before": "void setParams(\n      uint32_t maxControlMsgsPerInterval,\n      uint32_t maxDirectErrorHandlingPerInterval,\n      std::chrono::milliseconds controlMsgIntervalDuration,\n      std::chrono::milliseconds directErrorHandlingIntervalDuration) {\n    maxControlMsgsPerInterval_ = maxControlMsgsPerInterval;\n    maxDirectErrorHandlingPerInterval_ = maxDirectErrorHandlingPerInterval;\n    controlMsgIntervalDuration_ = controlMsgIntervalDuration;\n    directErrorHandlingIntervalDuration_ = directErrorHandlingIntervalDuration;\n  }",
        "func": "void setParams(uint32_t maxControlMsgsPerInterval,\n                 uint32_t maxDirectErrorHandlingPerInterval,\n                 uint32_t maxHeadersPerInterval,\n                 std::chrono::milliseconds controlMsgIntervalDuration,\n                 std::chrono::milliseconds directErrorHandlingIntervalDuration,\n                 std::chrono::milliseconds headersIntervalDuration) {\n    maxControlMsgsPerInterval_ = maxControlMsgsPerInterval;\n    maxDirectErrorHandlingPerInterval_ = maxDirectErrorHandlingPerInterval;\n    controlMsgIntervalDuration_ = controlMsgIntervalDuration;\n    directErrorHandlingIntervalDuration_ = directErrorHandlingIntervalDuration;\n    maxHeadersPerInterval_ = maxHeadersPerInterval;\n    headersIntervalDuration_ = headersIntervalDuration;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,13 @@\n-void setParams(\n-      uint32_t maxControlMsgsPerInterval,\n-      uint32_t maxDirectErrorHandlingPerInterval,\n-      std::chrono::milliseconds controlMsgIntervalDuration,\n-      std::chrono::milliseconds directErrorHandlingIntervalDuration) {\n+void setParams(uint32_t maxControlMsgsPerInterval,\n+                 uint32_t maxDirectErrorHandlingPerInterval,\n+                 uint32_t maxHeadersPerInterval,\n+                 std::chrono::milliseconds controlMsgIntervalDuration,\n+                 std::chrono::milliseconds directErrorHandlingIntervalDuration,\n+                 std::chrono::milliseconds headersIntervalDuration) {\n     maxControlMsgsPerInterval_ = maxControlMsgsPerInterval;\n     maxDirectErrorHandlingPerInterval_ = maxDirectErrorHandlingPerInterval;\n     controlMsgIntervalDuration_ = controlMsgIntervalDuration;\n     directErrorHandlingIntervalDuration_ = directErrorHandlingIntervalDuration;\n+    maxHeadersPerInterval_ = maxHeadersPerInterval;\n+    headersIntervalDuration_ = headersIntervalDuration;\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "void setParams(",
                "      uint32_t maxControlMsgsPerInterval,",
                "      uint32_t maxDirectErrorHandlingPerInterval,",
                "      std::chrono::milliseconds controlMsgIntervalDuration,",
                "      std::chrono::milliseconds directErrorHandlingIntervalDuration) {"
            ],
            "added_lines": [
                "void setParams(uint32_t maxControlMsgsPerInterval,",
                "                 uint32_t maxDirectErrorHandlingPerInterval,",
                "                 uint32_t maxHeadersPerInterval,",
                "                 std::chrono::milliseconds controlMsgIntervalDuration,",
                "                 std::chrono::milliseconds directErrorHandlingIntervalDuration,",
                "                 std::chrono::milliseconds headersIntervalDuration) {",
                "    maxHeadersPerInterval_ = maxHeadersPerInterval;",
                "    headersIntervalDuration_ = headersIntervalDuration;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nghttp2/nghttp2_session_add_goaway",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nghttp2/nghttp2/commit/72b4af6143681f528f1d237b21a9a7aee1738832",
        "commit_title": "Rework session management",
        "commit_text": "",
        "func_before": "int nghttp2_session_add_goaway(nghttp2_session *session, int32_t last_stream_id,\n                               uint32_t error_code, const uint8_t *opaque_data,\n                               size_t opaque_data_len, uint8_t aux_flags) {\n  int rv;\n  nghttp2_outbound_item *item;\n  nghttp2_frame *frame;\n  uint8_t *opaque_data_copy = NULL;\n  nghttp2_goaway_aux_data *aux_data;\n  nghttp2_mem *mem;\n\n  mem = &session->mem;\n\n  if (nghttp2_session_is_my_stream_id(session, last_stream_id)) {\n    return NGHTTP2_ERR_INVALID_ARGUMENT;\n  }\n\n  if (opaque_data_len) {\n    if (opaque_data_len + 8 > NGHTTP2_MAX_PAYLOADLEN) {\n      return NGHTTP2_ERR_INVALID_ARGUMENT;\n    }\n    opaque_data_copy = nghttp2_mem_malloc(mem, opaque_data_len);\n    if (opaque_data_copy == NULL) {\n      return NGHTTP2_ERR_NOMEM;\n    }\n    memcpy(opaque_data_copy, opaque_data, opaque_data_len);\n  }\n\n  item = nghttp2_mem_malloc(mem, sizeof(nghttp2_outbound_item));\n  if (item == NULL) {\n    nghttp2_mem_free(mem, opaque_data_copy);\n    return NGHTTP2_ERR_NOMEM;\n  }\n\n  nghttp2_outbound_item_init(item);\n\n  frame = &item->frame;\n\n  /* last_stream_id must not be increased from the value previously\n     sent */\n  last_stream_id = nghttp2_min(last_stream_id, session->local_last_stream_id);\n\n  nghttp2_frame_goaway_init(&frame->goaway, last_stream_id, error_code,\n                            opaque_data_copy, opaque_data_len);\n\n  aux_data = &item->aux_data.goaway;\n  aux_data->flags = aux_flags;\n\n  rv = nghttp2_session_add_item(session, item);\n  if (rv != 0) {\n    nghttp2_frame_goaway_free(&frame->goaway, mem);\n    nghttp2_mem_free(mem, item);\n    return rv;\n  }\n  return 0;\n}",
        "func": "int nghttp2_session_add_goaway(nghttp2_session *session, int32_t last_stream_id,\n                               uint32_t error_code, const uint8_t *opaque_data,\n                               size_t opaque_data_len, uint8_t aux_flags) {\n  int rv;\n  nghttp2_outbound_item *item;\n  nghttp2_frame *frame;\n  uint8_t *opaque_data_copy = NULL;\n  nghttp2_goaway_aux_data *aux_data;\n  nghttp2_mem *mem;\n\n  mem = &session->mem;\n\n  if (nghttp2_session_is_my_stream_id(session, last_stream_id)) {\n    return NGHTTP2_ERR_INVALID_ARGUMENT;\n  }\n\n  if (opaque_data_len) {\n    if (opaque_data_len + 8 > NGHTTP2_MAX_PAYLOADLEN) {\n      return NGHTTP2_ERR_INVALID_ARGUMENT;\n    }\n    opaque_data_copy = nghttp2_mem_malloc(mem, opaque_data_len);\n    if (opaque_data_copy == NULL) {\n      return NGHTTP2_ERR_NOMEM;\n    }\n    memcpy(opaque_data_copy, opaque_data, opaque_data_len);\n  }\n\n  item = nghttp2_mem_malloc(mem, sizeof(nghttp2_outbound_item));\n  if (item == NULL) {\n    nghttp2_mem_free(mem, opaque_data_copy);\n    return NGHTTP2_ERR_NOMEM;\n  }\n\n  nghttp2_outbound_item_init(item);\n\n  frame = &item->frame;\n\n  /* last_stream_id must not be increased from the value previously\n     sent */\n  last_stream_id = nghttp2_min(last_stream_id, session->local_last_stream_id);\n\n  nghttp2_frame_goaway_init(&frame->goaway, last_stream_id, error_code,\n                            opaque_data_copy, opaque_data_len);\n\n  aux_data = &item->aux_data.goaway;\n  aux_data->flags = aux_flags;\n\n  rv = nghttp2_session_add_item(session, item);\n  if (rv != 0) {\n    nghttp2_frame_goaway_free(&frame->goaway, mem);\n    nghttp2_mem_free(mem, item);\n    return rv;\n  }\n\n  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,5 +51,8 @@\n     nghttp2_mem_free(mem, item);\n     return rv;\n   }\n+\n+  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;\n+\n   return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  session->goaway_flags |= NGHTTP2_GOAWAY_SUBMITTED;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nghttp2/session_new",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nghttp2/nghttp2/commit/72b4af6143681f528f1d237b21a9a7aee1738832",
        "commit_title": "Rework session management",
        "commit_text": "",
        "func_before": "static int session_new(nghttp2_session **session_ptr,\n                       const nghttp2_session_callbacks *callbacks,\n                       void *user_data, int server,\n                       const nghttp2_option *option, nghttp2_mem *mem) {\n  int rv;\n  size_t nbuffer;\n  size_t max_deflate_dynamic_table_size =\n      NGHTTP2_HD_DEFAULT_MAX_DEFLATE_BUFFER_SIZE;\n  size_t i;\n\n  if (mem == NULL) {\n    mem = nghttp2_mem_default();\n  }\n\n  *session_ptr = nghttp2_mem_calloc(mem, 1, sizeof(nghttp2_session));\n  if (*session_ptr == NULL) {\n    rv = NGHTTP2_ERR_NOMEM;\n    goto fail_session;\n  }\n\n  (*session_ptr)->mem = *mem;\n  mem = &(*session_ptr)->mem;\n\n  /* next_stream_id is initialized in either\n     nghttp2_session_client_new2 or nghttp2_session_server_new2 */\n\n  nghttp2_stream_init(&(*session_ptr)->root, 0, NGHTTP2_STREAM_FLAG_NONE,\n                      NGHTTP2_STREAM_IDLE, NGHTTP2_DEFAULT_WEIGHT, 0, 0, NULL,\n                      mem);\n\n  (*session_ptr)->remote_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n  (*session_ptr)->recv_window_size = 0;\n  (*session_ptr)->consumed_size = 0;\n  (*session_ptr)->recv_reduction = 0;\n  (*session_ptr)->local_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n\n  (*session_ptr)->goaway_flags = NGHTTP2_GOAWAY_NONE;\n  (*session_ptr)->local_last_stream_id = (1u << 31) - 1;\n  (*session_ptr)->remote_last_stream_id = (1u << 31) - 1;\n\n  (*session_ptr)->pending_local_max_concurrent_stream =\n      NGHTTP2_DEFAULT_MAX_CONCURRENT_STREAMS;\n  (*session_ptr)->pending_enable_push = 1;\n  (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n\n  if (server) {\n    (*session_ptr)->server = 1;\n  }\n\n  init_settings(&(*session_ptr)->remote_settings);\n  init_settings(&(*session_ptr)->local_settings);\n\n  (*session_ptr)->max_incoming_reserved_streams =\n      NGHTTP2_MAX_INCOMING_RESERVED_STREAMS;\n\n  /* Limit max outgoing concurrent streams to sensible value */\n  (*session_ptr)->remote_settings.max_concurrent_streams = 100;\n\n  (*session_ptr)->max_send_header_block_length = NGHTTP2_MAX_HEADERSLEN;\n  (*session_ptr)->max_outbound_ack = NGHTTP2_DEFAULT_MAX_OBQ_FLOOD_ITEM;\n  (*session_ptr)->max_settings = NGHTTP2_DEFAULT_MAX_SETTINGS;\n\n  if (option) {\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_WINDOW_UPDATE) &&\n        option->no_auto_window_update) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_WINDOW_UPDATE;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_PEER_MAX_CONCURRENT_STREAMS) {\n\n      (*session_ptr)->remote_settings.max_concurrent_streams =\n          option->peer_max_concurrent_streams;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_RESERVED_REMOTE_STREAMS) {\n\n      (*session_ptr)->max_incoming_reserved_streams =\n          option->max_reserved_remote_streams;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_RECV_CLIENT_MAGIC) &&\n        option->no_recv_client_magic) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_HTTP_MESSAGING) &&\n        option->no_http_messaging) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_HTTP_MESSAGING;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_USER_RECV_EXT_TYPES) {\n      memcpy((*session_ptr)->user_recv_ext_types, option->user_recv_ext_types,\n             sizeof((*session_ptr)->user_recv_ext_types));\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_BUILTIN_RECV_EXT_TYPES) {\n      (*session_ptr)->builtin_recv_ext_types = option->builtin_recv_ext_types;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_PING_ACK) &&\n        option->no_auto_ping_ack) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_PING_ACK;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_SEND_HEADER_BLOCK_LENGTH) {\n      (*session_ptr)->max_send_header_block_length =\n          option->max_send_header_block_length;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_DEFLATE_DYNAMIC_TABLE_SIZE) {\n      max_deflate_dynamic_table_size = option->max_deflate_dynamic_table_size;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_CLOSED_STREAMS) &&\n        option->no_closed_streams) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_CLOSED_STREAMS;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_OUTBOUND_ACK) {\n      (*session_ptr)->max_outbound_ack = option->max_outbound_ack;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_MAX_SETTINGS) &&\n        option->max_settings) {\n      (*session_ptr)->max_settings = option->max_settings;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_SERVER_FALLBACK_RFC7540_PRIORITIES) &&\n        option->server_fallback_rfc7540_priorities) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_SERVER_FALLBACK_RFC7540_PRIORITIES;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION) &&\n        option->no_rfc9113_leading_and_trailing_ws_validation) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n    }\n  }\n\n  rv = nghttp2_hd_deflate_init2(&(*session_ptr)->hd_deflater,\n                                max_deflate_dynamic_table_size, mem);\n  if (rv != 0) {\n    goto fail_hd_deflater;\n  }\n  rv = nghttp2_hd_inflate_init(&(*session_ptr)->hd_inflater, mem);\n  if (rv != 0) {\n    goto fail_hd_inflater;\n  }\n\n  nbuffer = ((*session_ptr)->max_send_header_block_length +\n             NGHTTP2_FRAMEBUF_CHUNKLEN - 1) /\n            NGHTTP2_FRAMEBUF_CHUNKLEN;\n\n  if (nbuffer == 0) {\n    nbuffer = 1;\n  }\n\n  /* 1 for Pad Field. */\n  rv = nghttp2_bufs_init3(&(*session_ptr)->aob.framebufs,\n                          NGHTTP2_FRAMEBUF_CHUNKLEN, nbuffer, 1,\n                          NGHTTP2_FRAME_HDLEN + 1, mem);\n  if (rv != 0) {\n    goto fail_aob_framebuf;\n  }\n\n  nghttp2_map_init(&(*session_ptr)->streams, mem);\n\n  active_outbound_item_reset(&(*session_ptr)->aob, mem);\n\n  (*session_ptr)->callbacks = *callbacks;\n  (*session_ptr)->user_data = user_data;\n\n  session_inbound_frame_reset(*session_ptr);\n\n  if (nghttp2_enable_strict_preface) {\n    nghttp2_inbound_frame *iframe = &(*session_ptr)->iframe;\n\n    if (server && ((*session_ptr)->opt_flags &\n                   NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC) == 0) {\n      iframe->state = NGHTTP2_IB_READ_CLIENT_MAGIC;\n      iframe->payloadleft = NGHTTP2_CLIENT_MAGIC_LEN;\n    } else {\n      iframe->state = NGHTTP2_IB_READ_FIRST_SETTINGS;\n    }\n\n    if (!server) {\n      (*session_ptr)->aob.state = NGHTTP2_OB_SEND_CLIENT_MAGIC;\n      nghttp2_bufs_add(&(*session_ptr)->aob.framebufs, NGHTTP2_CLIENT_MAGIC,\n                       NGHTTP2_CLIENT_MAGIC_LEN);\n    }\n  }\n\n  for (i = 0; i < NGHTTP2_EXTPRI_URGENCY_LEVELS; ++i) {\n    nghttp2_pq_init(&(*session_ptr)->sched[i].ob_data, stream_less, mem);\n  }\n\n  return 0;\n\nfail_aob_framebuf:\n  nghttp2_hd_inflate_free(&(*session_ptr)->hd_inflater);\nfail_hd_inflater:\n  nghttp2_hd_deflate_free(&(*session_ptr)->hd_deflater);\nfail_hd_deflater:\n  nghttp2_mem_free(mem, *session_ptr);\nfail_session:\n  return rv;\n}",
        "func": "static int session_new(nghttp2_session **session_ptr,\n                       const nghttp2_session_callbacks *callbacks,\n                       void *user_data, int server,\n                       const nghttp2_option *option, nghttp2_mem *mem) {\n  int rv;\n  size_t nbuffer;\n  size_t max_deflate_dynamic_table_size =\n      NGHTTP2_HD_DEFAULT_MAX_DEFLATE_BUFFER_SIZE;\n  size_t i;\n\n  if (mem == NULL) {\n    mem = nghttp2_mem_default();\n  }\n\n  *session_ptr = nghttp2_mem_calloc(mem, 1, sizeof(nghttp2_session));\n  if (*session_ptr == NULL) {\n    rv = NGHTTP2_ERR_NOMEM;\n    goto fail_session;\n  }\n\n  (*session_ptr)->mem = *mem;\n  mem = &(*session_ptr)->mem;\n\n  /* next_stream_id is initialized in either\n     nghttp2_session_client_new2 or nghttp2_session_server_new2 */\n\n  nghttp2_stream_init(&(*session_ptr)->root, 0, NGHTTP2_STREAM_FLAG_NONE,\n                      NGHTTP2_STREAM_IDLE, NGHTTP2_DEFAULT_WEIGHT, 0, 0, NULL,\n                      mem);\n\n  (*session_ptr)->remote_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n  (*session_ptr)->recv_window_size = 0;\n  (*session_ptr)->consumed_size = 0;\n  (*session_ptr)->recv_reduction = 0;\n  (*session_ptr)->local_window_size = NGHTTP2_INITIAL_CONNECTION_WINDOW_SIZE;\n\n  (*session_ptr)->goaway_flags = NGHTTP2_GOAWAY_NONE;\n  (*session_ptr)->local_last_stream_id = (1u << 31) - 1;\n  (*session_ptr)->remote_last_stream_id = (1u << 31) - 1;\n\n  (*session_ptr)->pending_local_max_concurrent_stream =\n      NGHTTP2_DEFAULT_MAX_CONCURRENT_STREAMS;\n  (*session_ptr)->pending_enable_push = 1;\n  (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n\n  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,\n                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);\n\n  if (server) {\n    (*session_ptr)->server = 1;\n  }\n\n  init_settings(&(*session_ptr)->remote_settings);\n  init_settings(&(*session_ptr)->local_settings);\n\n  (*session_ptr)->max_incoming_reserved_streams =\n      NGHTTP2_MAX_INCOMING_RESERVED_STREAMS;\n\n  /* Limit max outgoing concurrent streams to sensible value */\n  (*session_ptr)->remote_settings.max_concurrent_streams = 100;\n\n  (*session_ptr)->max_send_header_block_length = NGHTTP2_MAX_HEADERSLEN;\n  (*session_ptr)->max_outbound_ack = NGHTTP2_DEFAULT_MAX_OBQ_FLOOD_ITEM;\n  (*session_ptr)->max_settings = NGHTTP2_DEFAULT_MAX_SETTINGS;\n\n  if (option) {\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_WINDOW_UPDATE) &&\n        option->no_auto_window_update) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_WINDOW_UPDATE;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_PEER_MAX_CONCURRENT_STREAMS) {\n\n      (*session_ptr)->remote_settings.max_concurrent_streams =\n          option->peer_max_concurrent_streams;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_RESERVED_REMOTE_STREAMS) {\n\n      (*session_ptr)->max_incoming_reserved_streams =\n          option->max_reserved_remote_streams;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_RECV_CLIENT_MAGIC) &&\n        option->no_recv_client_magic) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_HTTP_MESSAGING) &&\n        option->no_http_messaging) {\n\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_HTTP_MESSAGING;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_USER_RECV_EXT_TYPES) {\n      memcpy((*session_ptr)->user_recv_ext_types, option->user_recv_ext_types,\n             sizeof((*session_ptr)->user_recv_ext_types));\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_BUILTIN_RECV_EXT_TYPES) {\n      (*session_ptr)->builtin_recv_ext_types = option->builtin_recv_ext_types;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_AUTO_PING_ACK) &&\n        option->no_auto_ping_ack) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_AUTO_PING_ACK;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_SEND_HEADER_BLOCK_LENGTH) {\n      (*session_ptr)->max_send_header_block_length =\n          option->max_send_header_block_length;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_DEFLATE_DYNAMIC_TABLE_SIZE) {\n      max_deflate_dynamic_table_size = option->max_deflate_dynamic_table_size;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_NO_CLOSED_STREAMS) &&\n        option->no_closed_streams) {\n      (*session_ptr)->opt_flags |= NGHTTP2_OPTMASK_NO_CLOSED_STREAMS;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_MAX_OUTBOUND_ACK) {\n      (*session_ptr)->max_outbound_ack = option->max_outbound_ack;\n    }\n\n    if ((option->opt_set_mask & NGHTTP2_OPT_MAX_SETTINGS) &&\n        option->max_settings) {\n      (*session_ptr)->max_settings = option->max_settings;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_SERVER_FALLBACK_RFC7540_PRIORITIES) &&\n        option->server_fallback_rfc7540_priorities) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_SERVER_FALLBACK_RFC7540_PRIORITIES;\n    }\n\n    if ((option->opt_set_mask &\n         NGHTTP2_OPT_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION) &&\n        option->no_rfc9113_leading_and_trailing_ws_validation) {\n      (*session_ptr)->opt_flags |=\n          NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n    }\n\n    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {\n      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n                           option->stream_reset_burst,\n                           option->stream_reset_rate);\n    }\n  }\n\n  rv = nghttp2_hd_deflate_init2(&(*session_ptr)->hd_deflater,\n                                max_deflate_dynamic_table_size, mem);\n  if (rv != 0) {\n    goto fail_hd_deflater;\n  }\n  rv = nghttp2_hd_inflate_init(&(*session_ptr)->hd_inflater, mem);\n  if (rv != 0) {\n    goto fail_hd_inflater;\n  }\n\n  nbuffer = ((*session_ptr)->max_send_header_block_length +\n             NGHTTP2_FRAMEBUF_CHUNKLEN - 1) /\n            NGHTTP2_FRAMEBUF_CHUNKLEN;\n\n  if (nbuffer == 0) {\n    nbuffer = 1;\n  }\n\n  /* 1 for Pad Field. */\n  rv = nghttp2_bufs_init3(&(*session_ptr)->aob.framebufs,\n                          NGHTTP2_FRAMEBUF_CHUNKLEN, nbuffer, 1,\n                          NGHTTP2_FRAME_HDLEN + 1, mem);\n  if (rv != 0) {\n    goto fail_aob_framebuf;\n  }\n\n  nghttp2_map_init(&(*session_ptr)->streams, mem);\n\n  active_outbound_item_reset(&(*session_ptr)->aob, mem);\n\n  (*session_ptr)->callbacks = *callbacks;\n  (*session_ptr)->user_data = user_data;\n\n  session_inbound_frame_reset(*session_ptr);\n\n  if (nghttp2_enable_strict_preface) {\n    nghttp2_inbound_frame *iframe = &(*session_ptr)->iframe;\n\n    if (server && ((*session_ptr)->opt_flags &\n                   NGHTTP2_OPTMASK_NO_RECV_CLIENT_MAGIC) == 0) {\n      iframe->state = NGHTTP2_IB_READ_CLIENT_MAGIC;\n      iframe->payloadleft = NGHTTP2_CLIENT_MAGIC_LEN;\n    } else {\n      iframe->state = NGHTTP2_IB_READ_FIRST_SETTINGS;\n    }\n\n    if (!server) {\n      (*session_ptr)->aob.state = NGHTTP2_OB_SEND_CLIENT_MAGIC;\n      nghttp2_bufs_add(&(*session_ptr)->aob.framebufs, NGHTTP2_CLIENT_MAGIC,\n                       NGHTTP2_CLIENT_MAGIC_LEN);\n    }\n  }\n\n  for (i = 0; i < NGHTTP2_EXTPRI_URGENCY_LEVELS; ++i) {\n    nghttp2_pq_init(&(*session_ptr)->sched[i].ob_data, stream_less, mem);\n  }\n\n  return 0;\n\nfail_aob_framebuf:\n  nghttp2_hd_inflate_free(&(*session_ptr)->hd_inflater);\nfail_hd_inflater:\n  nghttp2_hd_deflate_free(&(*session_ptr)->hd_deflater);\nfail_hd_deflater:\n  nghttp2_mem_free(mem, *session_ptr);\nfail_session:\n  return rv;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,6 +43,10 @@\n   (*session_ptr)->pending_enable_push = 1;\n   (*session_ptr)->pending_no_rfc7540_priorities = UINT8_MAX;\n \n+  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n+                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,\n+                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);\n+\n   if (server) {\n     (*session_ptr)->server = 1;\n   }\n@@ -140,6 +144,12 @@\n         option->no_rfc9113_leading_and_trailing_ws_validation) {\n       (*session_ptr)->opt_flags |=\n           NGHTTP2_OPTMASK_NO_RFC9113_LEADING_AND_TRAILING_WS_VALIDATION;\n+    }\n+\n+    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {\n+      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,\n+                           option->stream_reset_burst,\n+                           option->stream_reset_rate);\n     }\n   }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,",
                "                       NGHTTP2_DEFAULT_STREAM_RESET_BURST,",
                "                       NGHTTP2_DEFAULT_STREAM_RESET_RATE);",
                "",
                "    }",
                "",
                "    if (option->opt_set_mask & NGHTTP2_OPT_STREAM_RESET_RATE_LIMIT) {",
                "      nghttp2_ratelim_init(&(*session_ptr)->stream_reset_ratelim,",
                "                           option->stream_reset_burst,",
                "                           option->stream_reset_rate);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "nghttp2/nghttp2_session_on_rst_stream_received",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/nghttp2/nghttp2/commit/72b4af6143681f528f1d237b21a9a7aee1738832",
        "commit_title": "Rework session management",
        "commit_text": "",
        "func_before": "int nghttp2_session_on_rst_stream_received(nghttp2_session *session,\n                                           nghttp2_frame *frame) {\n  int rv;\n  nghttp2_stream *stream;\n  if (frame->hd.stream_id == 0) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream_id == 0\");\n  }\n\n  if (session_detect_idle_stream(session, frame->hd.stream_id)) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream in idle\");\n  }\n\n  stream = nghttp2_session_get_stream(session, frame->hd.stream_id);\n  if (stream) {\n    /* We may use stream->shut_flags for strict error checking. */\n    nghttp2_stream_shutdown(stream, NGHTTP2_SHUT_RD);\n  }\n\n  rv = session_call_on_frame_received(session, frame);\n  if (rv != 0) {\n    return rv;\n  }\n  rv = nghttp2_session_close_stream(session, frame->hd.stream_id,\n                                    frame->rst_stream.error_code);\n  if (nghttp2_is_fatal(rv)) {\n    return rv;\n  }\n  return 0;\n}",
        "func": "int nghttp2_session_on_rst_stream_received(nghttp2_session *session,\n                                           nghttp2_frame *frame) {\n  int rv;\n  nghttp2_stream *stream;\n  if (frame->hd.stream_id == 0) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream_id == 0\");\n  }\n\n  if (session_detect_idle_stream(session, frame->hd.stream_id)) {\n    return session_handle_invalid_connection(session, frame, NGHTTP2_ERR_PROTO,\n                                             \"RST_STREAM: stream in idle\");\n  }\n\n  stream = nghttp2_session_get_stream(session, frame->hd.stream_id);\n  if (stream) {\n    /* We may use stream->shut_flags for strict error checking. */\n    nghttp2_stream_shutdown(stream, NGHTTP2_SHUT_RD);\n  }\n\n  rv = session_call_on_frame_received(session, frame);\n  if (rv != 0) {\n    return rv;\n  }\n  rv = nghttp2_session_close_stream(session, frame->hd.stream_id,\n                                    frame->rst_stream.error_code);\n  if (nghttp2_is_fatal(rv)) {\n    return rv;\n  }\n\n  return session_update_stream_reset_ratelim(session);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,5 +27,6 @@\n   if (nghttp2_is_fatal(rv)) {\n     return rv;\n   }\n-  return 0;\n+\n+  return session_update_stream_reset_ratelim(session);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  return 0;"
            ],
            "added_lines": [
                "",
                "  return session_update_stream_reset_ratelim(session);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-44487",
        "func_name": "envoyproxy/envoy/ConnectionManagerImpl::doDeferredStreamDestroy",
        "description": "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October 2023.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/6f8a69e5023eba51bfaeec6d551a5b9840e8c6d1",
        "commit_title": "Close HTTP connections that prematurely reset streams",
        "commit_text": " ",
        "func_before": "void ConnectionManagerImpl::doDeferredStreamDestroy(ActiveStream& stream) {\n  if (stream.max_stream_duration_timer_ != nullptr) {\n    stream.max_stream_duration_timer_->disableTimer();\n    stream.max_stream_duration_timer_ = nullptr;\n  }\n  if (stream.stream_idle_timer_ != nullptr) {\n    stream.stream_idle_timer_->disableTimer();\n    stream.stream_idle_timer_ = nullptr;\n  }\n  stream.filter_manager_.disarmRequestTimeout();\n  if (stream.request_header_timer_ != nullptr) {\n    stream.request_header_timer_->disableTimer();\n    stream.request_header_timer_ = nullptr;\n  }\n  if (stream.access_log_flush_timer_ != nullptr) {\n    stream.access_log_flush_timer_->disableTimer();\n    stream.access_log_flush_timer_ = nullptr;\n  }\n\n  if (stream.expand_agnostic_stream_lifetime_) {\n    // Only destroy the active stream if the underlying codec has notified us of\n    // completion or we've internal redirect the stream.\n    if (!stream.canDestroyStream()) {\n      // Track that this stream is not expecting any additional calls apart from\n      // codec notification.\n      stream.state_.is_zombie_stream_ = true;\n      return;\n    }\n\n    if (stream.response_encoder_ != nullptr) {\n      stream.response_encoder_->getStream().registerCodecEventCallbacks(nullptr);\n    }\n  }\n\n  stream.completeRequest();\n\n  // If refresh rtt after request is required explicitly, then try to get rtt again set it into\n  // connection info.\n  if (refresh_rtt_after_request_) {\n    // Set roundtrip time in connectionInfoSetter before OnStreamComplete\n    absl::optional<std::chrono::milliseconds> t = read_callbacks_->connection().lastRoundTripTime();\n    if (t.has_value()) {\n      read_callbacks_->connection().connectionInfoSetter().setRoundTripTime(t.value());\n    }\n  }\n\n  stream.filter_manager_.onStreamComplete();\n\n  // For HTTP/3, skip access logging here and add deferred logging info\n  // to stream info for QuicStatsGatherer to use later.\n  if (codec_ && codec_->protocol() == Protocol::Http3 &&\n      // There was a downstream reset, log immediately.\n      !stream.filter_manager_.sawDownstreamReset() &&\n      // On recreate stream, log immediately.\n      stream.response_encoder_ != nullptr &&\n      Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.quic_defer_logging_to_ack_listener\")) {\n    stream.deferHeadersAndTrailers();\n  } else {\n    // For HTTP/1 and HTTP/2, log here as usual.\n    stream.filter_manager_.log(AccessLog::AccessLogType::DownstreamEnd);\n  }\n\n  stream.filter_manager_.destroyFilters();\n\n  dispatcher_->deferredDelete(stream.removeFromList(streams_));\n\n  // The response_encoder should never be dangling (unless we're destroying a\n  // stream we are recreating) as the codec level stream will either outlive the\n  // ActiveStream, or be alive in deferred deletion queue at this point.\n  if (stream.response_encoder_) {\n    stream.response_encoder_->getStream().removeCallbacks(stream);\n  }\n\n  if (connection_idle_timer_ && streams_.empty()) {\n    connection_idle_timer_->enableTimer(config_.idleTimeout().value());\n  }\n}",
        "func": "void ConnectionManagerImpl::doDeferredStreamDestroy(ActiveStream& stream) {\n  if (!stream.state_.is_internally_destroyed_) {\n    ++closed_non_internally_destroyed_requests_;\n    if (isPrematureRstStream(stream)) {\n      ++number_premature_stream_resets_;\n    }\n  }\n  if (stream.max_stream_duration_timer_ != nullptr) {\n    stream.max_stream_duration_timer_->disableTimer();\n    stream.max_stream_duration_timer_ = nullptr;\n  }\n  if (stream.stream_idle_timer_ != nullptr) {\n    stream.stream_idle_timer_->disableTimer();\n    stream.stream_idle_timer_ = nullptr;\n  }\n  stream.filter_manager_.disarmRequestTimeout();\n  if (stream.request_header_timer_ != nullptr) {\n    stream.request_header_timer_->disableTimer();\n    stream.request_header_timer_ = nullptr;\n  }\n  if (stream.access_log_flush_timer_ != nullptr) {\n    stream.access_log_flush_timer_->disableTimer();\n    stream.access_log_flush_timer_ = nullptr;\n  }\n\n  if (stream.expand_agnostic_stream_lifetime_) {\n    // Only destroy the active stream if the underlying codec has notified us of\n    // completion or we've internal redirect the stream.\n    if (!stream.canDestroyStream()) {\n      // Track that this stream is not expecting any additional calls apart from\n      // codec notification.\n      stream.state_.is_zombie_stream_ = true;\n      return;\n    }\n\n    if (stream.response_encoder_ != nullptr) {\n      stream.response_encoder_->getStream().registerCodecEventCallbacks(nullptr);\n    }\n  }\n\n  stream.completeRequest();\n\n  // If refresh rtt after request is required explicitly, then try to get rtt again set it into\n  // connection info.\n  if (refresh_rtt_after_request_) {\n    // Set roundtrip time in connectionInfoSetter before OnStreamComplete\n    absl::optional<std::chrono::milliseconds> t = read_callbacks_->connection().lastRoundTripTime();\n    if (t.has_value()) {\n      read_callbacks_->connection().connectionInfoSetter().setRoundTripTime(t.value());\n    }\n  }\n\n  stream.filter_manager_.onStreamComplete();\n\n  // For HTTP/3, skip access logging here and add deferred logging info\n  // to stream info for QuicStatsGatherer to use later.\n  if (codec_ && codec_->protocol() == Protocol::Http3 &&\n      // There was a downstream reset, log immediately.\n      !stream.filter_manager_.sawDownstreamReset() &&\n      // On recreate stream, log immediately.\n      stream.response_encoder_ != nullptr &&\n      Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.quic_defer_logging_to_ack_listener\")) {\n    stream.deferHeadersAndTrailers();\n  } else {\n    // For HTTP/1 and HTTP/2, log here as usual.\n    stream.filter_manager_.log(AccessLog::AccessLogType::DownstreamEnd);\n  }\n\n  stream.filter_manager_.destroyFilters();\n\n  dispatcher_->deferredDelete(stream.removeFromList(streams_));\n\n  // The response_encoder should never be dangling (unless we're destroying a\n  // stream we are recreating) as the codec level stream will either outlive the\n  // ActiveStream, or be alive in deferred deletion queue at this point.\n  if (stream.response_encoder_) {\n    stream.response_encoder_->getStream().removeCallbacks(stream);\n  }\n\n  if (connection_idle_timer_ && streams_.empty()) {\n    connection_idle_timer_->enableTimer(config_.idleTimeout().value());\n  }\n  maybeDrainDueToPrematureResets();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,10 @@\n void ConnectionManagerImpl::doDeferredStreamDestroy(ActiveStream& stream) {\n+  if (!stream.state_.is_internally_destroyed_) {\n+    ++closed_non_internally_destroyed_requests_;\n+    if (isPrematureRstStream(stream)) {\n+      ++number_premature_stream_resets_;\n+    }\n+  }\n   if (stream.max_stream_duration_timer_ != nullptr) {\n     stream.max_stream_duration_timer_->disableTimer();\n     stream.max_stream_duration_timer_ = nullptr;\n@@ -75,4 +81,5 @@\n   if (connection_idle_timer_ && streams_.empty()) {\n     connection_idle_timer_->enableTimer(config_.idleTimeout().value());\n   }\n+  maybeDrainDueToPrematureResets();\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (!stream.state_.is_internally_destroyed_) {",
                "    ++closed_non_internally_destroyed_requests_;",
                "    if (isPrematureRstStream(stream)) {",
                "      ++number_premature_stream_resets_;",
                "    }",
                "  }",
                "  maybeDrainDueToPrematureResets();"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/Media_AddSample",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "GF_Err Media_AddSample(GF_MediaBox *mdia, u64 data_offset, const GF_ISOSample *sample, u32 StreamDescIndex, u32 syncShadowNumber)\n{\n\tGF_Err e;\n\tGF_SampleTableBox *stbl;\n\tu32 sampleNumber, i;\n\tif (!mdia || !sample) return GF_BAD_PARAM;\n\n\tstbl = mdia->information->sampleTable;\n\n\t//get a valid sampleNumber for this new guy\n\te = stbl_AddDTS(stbl, sample->DTS, &sampleNumber, mdia->mediaHeader->timeScale, sample->nb_pack);\n\tif (e) return e;\n\n\t//add size\n\te = stbl_AddSize(stbl->SampleSize, sampleNumber, sample->dataLength, sample->nb_pack);\n\tif (e) return e;\n\n\t//adds CTS offset\n\tif (sample->CTS_Offset) {\n\t\t//if we don't have a CTS table, add it...\n\t\tif (!stbl->CompositionOffset) {\n\t\t\tstbl->CompositionOffset = (GF_CompositionOffsetBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_CTTS);\n\t\t\tif (!stbl->CompositionOffset) return GF_OUT_OF_MEM;\n\t\t}\n\t\t//then add our CTS (the prev samples with no CTS offset will be automatically added...\n\t\te = stbl_AddCTS(stbl, sampleNumber, sample->CTS_Offset);\n\t\tif (e) return e;\n\t} else if (stbl->CompositionOffset) {\n\t\te = stbl_AddCTS(stbl, sampleNumber, sample->CTS_Offset);\n\t\tif (e) return e;\n\t}\n\n\t//The first non sync sample we see must create a syncTable\n\tif (sample->IsRAP) {\n\t\t//insert it only if we have a sync table and if we have an IDR slice\n\t\tif (stbl->SyncSample && ((sample->IsRAP == RAP) || (sample->IsRAP == SAP_TYPE_2))) {\n\t\t\te = stbl_AddRAP(stbl->SyncSample, sampleNumber);\n\t\t\tif (e) return e;\n\t\t}\n\t} else {\n\t\t//non-sync sample. Create a SyncSample table if needed\n\t\tif (!stbl->SyncSample) {\n\t\t\tstbl->SyncSample = (GF_SyncSampleBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_STSS);\n\t\t\tif (!stbl->SyncSample) return GF_OUT_OF_MEM;\n\t\t\t//all the prev samples are sync\n\t\t\tfor (i=0; i<stbl->SampleSize->sampleCount; i++) {\n\t\t\t\tif (i+1 != sampleNumber) {\n\t\t\t\t\te = stbl_AddRAP(stbl->SyncSample, i+1);\n\t\t\t\t\tif (e) return e;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (sample->IsRAP==RAP_REDUNDANT) {\n\t\te = stbl_AddRedundant(stbl, sampleNumber);\n\t\tif (e) return e;\n\t}\n\n\tif (!mdia->mediaTrack->chunk_cache) {\n\t\t//and update the chunks\n\t\te = stbl_AddChunkOffset(mdia, sampleNumber, StreamDescIndex, data_offset, sample->nb_pack);\n\t\tif (e) return e;\n\t}\n\t\n\tif (!syncShadowNumber) return GF_OK;\n\tif (!stbl->ShadowSync) {\n\t\tstbl->ShadowSync = (GF_ShadowSyncBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_STSH);\n\t\tif (!stbl->ShadowSync) return GF_OUT_OF_MEM;\n\t}\n\treturn stbl_AddShadow(mdia->information->sampleTable->ShadowSync, sampleNumber, syncShadowNumber);\n}",
        "func": "GF_Err Media_AddSample(GF_MediaBox *mdia, u64 data_offset, const GF_ISOSample *sample, u32 StreamDescIndex, u32 syncShadowNumber)\n{\n\tGF_Err e;\n\tGF_SampleTableBox *stbl;\n\tu32 sampleNumber, i;\n\tif (!mdia || !sample) return GF_BAD_PARAM;\n\n\tstbl = mdia->information->sampleTable;\n\n\t//get a valid sampleNumber for this new guy\n\te = stbl_AddDTS(stbl, sample->DTS, &sampleNumber, mdia->mediaHeader->timeScale, sample->nb_pack);\n\tif (e) return e;\n\n\t//add size\n\te = stbl_AddSize(stbl->SampleSize, sampleNumber, sample->dataLength, sample->nb_pack);\n\tif (e) return e;\n\n\t//adds CTS offset\n\tif (sample->CTS_Offset) {\n\t\t//if we don't have a CTS table, add it...\n\t\tif (!stbl->CompositionOffset) {\n\t\t\tstbl->CompositionOffset = (GF_CompositionOffsetBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_CTTS);\n\t\t\tif (!stbl->CompositionOffset) return GF_OUT_OF_MEM;\n\t\t}\n\t\t//then add our CTS (the prev samples with no CTS offset will be automatically added...\n\t\te = stbl_AddCTS(stbl, sampleNumber, sample->CTS_Offset);\n\t\tif (e) return e;\n\t} else if (stbl->CompositionOffset) {\n\t\te = stbl_AddCTS(stbl, sampleNumber, sample->CTS_Offset);\n\t\tif (e) return e;\n\t}\n\n\t//The first non sync sample we see must create a syncTable\n\tif (sample->IsRAP) {\n\t\t//insert it only if we have a sync table and if we have an IDR slice\n\t\tif (stbl->SyncSample && ((sample->IsRAP == RAP) || (sample->IsRAP == SAP_TYPE_2))) {\n\t\t\te = stbl_AddRAP(stbl->SyncSample, sampleNumber);\n\t\t\tif (e) return e;\n\t\t}\n\t} else {\n\t\t//non-sync sample. Create a SyncSample table if needed\n\t\tif (!stbl->SyncSample) {\n\t\t\tstbl->SyncSample = (GF_SyncSampleBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_STSS);\n\t\t\tif (!stbl->SyncSample) return GF_OUT_OF_MEM;\n\t\t\t//all the prev samples are sync\n\t\t\tfor (i=0; i<stbl->SampleSize->sampleCount; i++) {\n\t\t\t\tif (i+1 != sampleNumber) {\n\t\t\t\t\te = stbl_AddRAP(stbl->SyncSample, i+1);\n\t\t\t\t\tif (e) return e;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (sample->IsRAP==RAP_REDUNDANT) {\n\t\te = stbl_AddRedundant(stbl, sampleNumber);\n\t\tif (e) return e;\n\t}\n\n\tif (!mdia->mediaTrack->chunk_cache) {\n\t\t//and update the chunks\n\t\te = stbl_AddChunkOffset(mdia, sampleNumber, StreamDescIndex, data_offset, sample->nb_pack);\n\t\tif (e) return e;\n\t}\n\n\tif (!syncShadowNumber) return GF_OK;\n\tif (!stbl->ShadowSync) {\n\t\tstbl->ShadowSync = (GF_ShadowSyncBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_STSH);\n\t\tif (!stbl->ShadowSync) return GF_OUT_OF_MEM;\n\t}\n\treturn stbl_AddShadow(mdia->information->sampleTable->ShadowSync, sampleNumber, syncShadowNumber);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,7 +61,7 @@\n \t\te = stbl_AddChunkOffset(mdia, sampleNumber, StreamDescIndex, data_offset, sample->nb_pack);\n \t\tif (e) return e;\n \t}\n-\t\n+\n \tif (!syncShadowNumber) return GF_OK;\n \tif (!stbl->ShadowSync) {\n \t\tstbl->ShadowSync = (GF_ShadowSyncBox *) gf_isom_box_new_parent(&stbl->child_boxes, GF_ISOM_BOX_TYPE_STSH);",
        "diff_line_info": {
            "deleted_lines": [
                "\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/Media_GetESD",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "GF_Err Media_GetESD(GF_MediaBox *mdia, u32 sampleDescIndex, GF_ESD **out_esd, Bool true_desc_only)\n{\n\tu32 type;\n\tGF_ESD *esd;\n\tGF_MPEGSampleEntryBox *entry = NULL;\n\tGF_ESDBox *ESDa;\n\tGF_ProtectionSchemeInfoBox *sinf;\n\tGF_SampleDescriptionBox *stsd = mdia->information->sampleTable->SampleDescription;\n\n\t*out_esd = NULL;\n\tif (!stsd || !stsd->child_boxes || !sampleDescIndex || (sampleDescIndex > gf_list_count(stsd->child_boxes)) )\n\t\treturn GF_BAD_PARAM;\n\n\tesd = NULL;\n\tentry = (GF_MPEGSampleEntryBox*)gf_list_get(stsd->child_boxes, sampleDescIndex - 1);\n\tif (! entry) return GF_ISOM_INVALID_MEDIA;\n\n\t*out_esd = NULL;\n\tESDa = NULL;\n\ttype = entry->type;\n\tswitch (type) {\n\tcase GF_ISOM_BOX_TYPE_ENCV:\n\tcase GF_ISOM_BOX_TYPE_ENCA:\n\tcase GF_ISOM_BOX_TYPE_ENCS:\n\tcase GF_ISOM_BOX_TYPE_ENCF:\n\tcase GF_ISOM_BOX_TYPE_ENCM:\n\tcase GF_ISOM_BOX_TYPE_ENCT:\n\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_SINF);\n\t\tif (sinf && sinf->original_format) {\n\t\t\ttype = sinf->original_format->data_format;\n\t\t}\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_RESV:\n\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_RINF);\n\t\tif (sinf && sinf->original_format) {\n\t\t\ttype = sinf->original_format->data_format;\n\t\t}\n\t\tbreak;\n\t}\n\n\n\tswitch (type) {\n\tcase GF_ISOM_BOX_TYPE_MP4V:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tESDa = ((GF_MPEGVisualSampleEntryBox*)entry)->esd;\n\t\tif (ESDa) esd = (GF_ESD *) ESDa->desc;\n\t\t/*avc1 encrypted*/\n\t\telse esd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_AVC1:\n\tcase GF_ISOM_BOX_TYPE_AVC2:\n\tcase GF_ISOM_BOX_TYPE_AVC3:\n\tcase GF_ISOM_BOX_TYPE_AVC4:\n\tcase GF_ISOM_BOX_TYPE_HVC1:\n\tcase GF_ISOM_BOX_TYPE_HEV1:\n\tcase GF_ISOM_BOX_TYPE_HVC2:\n\tcase GF_ISOM_BOX_TYPE_HEV2:\n\tcase GF_ISOM_BOX_TYPE_HVT1:\n\tcase GF_ISOM_BOX_TYPE_264B:\n\tcase GF_ISOM_BOX_TYPE_265B:\n\tcase GF_ISOM_BOX_TYPE_DVHE:\n\tcase GF_ISOM_BOX_TYPE_DVH1:\n\tcase GF_ISOM_BOX_TYPE_DVA1:\n\tcase GF_ISOM_BOX_TYPE_DVAV:\n\tcase GF_ISOM_BOX_TYPE_VVC1:\n\tcase GF_ISOM_BOX_TYPE_VVI1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_SVC1:\n\tcase GF_ISOM_BOX_TYPE_MVC1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif ((mdia->mediaTrack->extractor_mode & 0x0000FFFF) != GF_ISOM_NALU_EXTRACT_INSPECT)\n\t\t\tAVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, mdia);\n\t\telse\n\t\t\tAVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, NULL);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_LHE1:\n\tcase GF_ISOM_BOX_TYPE_LHV1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif ((mdia->mediaTrack->extractor_mode & 0x0000FFFF) != GF_ISOM_NALU_EXTRACT_INSPECT)\n\t\t\tHEVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, mdia);\n\t\telse\n\t\t\tHEVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, NULL);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_DAV1:\n\tcase GF_ISOM_BOX_TYPE_AV01:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tAV1_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*)entry, mdia);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*)entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_VP08:\n\tcase GF_ISOM_BOX_TYPE_VP09:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tVP9_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*)entry, mdia);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*)entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4A:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n        {\n            GF_MPEGAudioSampleEntryBox *ase = (GF_MPEGAudioSampleEntryBox*)entry;\n            ESDa = ase->esd;\n            if (ESDa) {\n\t\t\t\tesd = (GF_ESD *) ESDa->desc;\n            } else if (!true_desc_only) {\n\t\t\t\tBool make_mp4a = GF_FALSE;\n\t\t\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_SINF);\n\n\t\t\t\tif (sinf && sinf->original_format) {\n\t\t\t\t\tif (sinf->original_format->data_format==GF_ISOM_BOX_TYPE_MP4A) {\n\t\t\t\t\t\tmake_mp4a = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Assuming that if no ESD is provided the stream is Basic MPEG-4 AAC LC\n\t\t\t\t\tmake_mp4a = GF_TRUE;\n\t\t\t\t}\n\t\t\t\tif (make_mp4a) {\n#ifndef GPAC_DISABLE_AV_PARSERS\n\t\t\t\t\tGF_M4ADecSpecInfo aacinfo;\n\t\t\t\t\tmemset(&aacinfo, 0, sizeof(GF_M4ADecSpecInfo));\n\t\t\t\t\taacinfo.nb_chan = ase->channel_count;\n\t\t\t\t\taacinfo.base_object_type = GF_M4A_AAC_LC;\n\t\t\t\t\taacinfo.base_sr = ase->samplerate_hi;\n\t\t\t\t\t*out_esd = gf_odf_desc_esd_new(0);\n\t\t\t\t\t(*out_esd)->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\t\t\t(*out_esd)->decoderConfig->objectTypeIndication = GF_CODECID_AAC_MPEG4;\n\t\t\t\t\tgf_m4a_write_config(&aacinfo, &(*out_esd)->decoderConfig->decoderSpecificInfo->data, &(*out_esd)->decoderConfig->decoderSpecificInfo->dataLength);\n#else\n\t\t\t\t\treturn GF_NOT_SUPPORTED;\n#endif\n\t\t\t\t}\n            }\n        }\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4S:\n\t\tif (entry->internal_type==GF_ISOM_SAMPLE_ENTRY_MP4S) {\n\t\t\tESDa = entry->esd;\n\t\t\tif (ESDa) esd = (GF_ESD *) ESDa->desc;\n\t\t}\n\t\tbreak;\n#ifndef GPAC_DISABLE_TTXT\n\tcase GF_ISOM_BOX_TYPE_TX3G:\n\tcase GF_ISOM_BOX_TYPE_TEXT:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\tif (!true_desc_only && mdia->mediaTrack->moov->mov->convert_streaming_text) {\n\t\t\tGF_Err e = gf_isom_get_ttxt_esd(mdia, out_esd);\n\t\t\tif (e) return e;\n\t\t\tbreak;\n\t\t}\n\t\telse\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n#endif\n#ifndef GPAC_DISABLE_VTT\n\tcase GF_ISOM_BOX_TYPE_WVTT:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t{\n\t\tGF_WebVTTSampleEntryBox*vtte = (GF_WebVTTSampleEntryBox*)entry;\n\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t*out_esd = esd;\n\t\tesd->decoderConfig->streamType = GF_STREAM_TEXT;\n\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_WEBVTT;\n\t\tif (vtte->config) {\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength = (u32) strlen(vtte->config->string);\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data = gf_malloc(sizeof(char)*esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t\tmemcpy(esd->decoderConfig->decoderSpecificInfo->data, vtte->config->string, esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t}\n\t}\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_STPP:\n\tcase GF_ISOM_BOX_TYPE_SBTT:\n\tcase GF_ISOM_BOX_TYPE_STXT:\n\t\tbreak;\n#endif\n\n\tcase GF_ISOM_SUBTYPE_3GP_AMR:\n\tcase GF_ISOM_SUBTYPE_3GP_AMR_WB:\n\tcase GF_ISOM_SUBTYPE_3GP_EVRC:\n\tcase GF_ISOM_SUBTYPE_3GP_QCELP:\n\tcase GF_ISOM_SUBTYPE_3GP_SMV:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (!true_desc_only) {\n\t\t\tGF_Err e = gf_isom_get_3gpp_audio_esd(mdia->information->sampleTable, type, (GF_GenericAudioSampleEntryBox*)entry, out_esd);\n\t\t\tif (e) return e;\n\t\t\tbreak;\n\t\t} else return GF_ISOM_INVALID_MEDIA;\n\n\tcase GF_ISOM_SUBTYPE_OPUS:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t{\n\t\tGF_OpusSpecificBox *opus_c;\n\t\tif (true_desc_only)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\topus_c = ((GF_MPEGAudioSampleEntryBox*)entry)->cfg_opus;\n\t\tif (!opus_c) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"ESD not found for Opus\\n)\"));\n\t\t\tbreak;\n\t\t}\n\t\t*out_esd = gf_odf_desc_esd_new(2);\n\t\t(*out_esd)->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t(*out_esd)->decoderConfig->objectTypeIndication = GF_CODECID_OPUS;\n\t\tgf_odf_opus_cfg_write(&opus_c->opcfg, & (*out_esd)->decoderConfig->decoderSpecificInfo->data, & (*out_esd)->decoderConfig->decoderSpecificInfo->dataLength);\n\t\tbreak;\n\t}\n\tcase GF_ISOM_SUBTYPE_3GP_H263:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_VISUAL;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_H263;\n\t\t\tbreak;\n\t\t}\n\n\tcase GF_ISOM_SUBTYPE_MP3:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MPEG_AUDIO;\n\t\t\tbreak;\n\t\t}\n\n\tcase GF_ISOM_SUBTYPE_LSR1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tGF_LASeRSampleEntryBox*ptr = (GF_LASeRSampleEntryBox*)entry;\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_SCENE;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_LASER;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength = ptr->lsr_config->hdr_size;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data = gf_malloc(sizeof(char)*ptr->lsr_config->hdr_size);\n\t\t\tif (!esd->decoderConfig->decoderSpecificInfo->data) return GF_OUT_OF_MEM;\n\t\t\tmemcpy(esd->decoderConfig->decoderSpecificInfo->data, ptr->lsr_config->hdr, sizeof(char)*ptr->lsr_config->hdr_size);\n\t\t\tbreak;\n\t\t}\n\tcase GF_ISOM_SUBTYPE_MH3D_MHA1:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHA2:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHM1:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHM2:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tGF_MPEGAudioSampleEntryBox*ptr = (GF_MPEGAudioSampleEntryBox*)entry;\n\t\t\tesd = gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\tif ((type==GF_ISOM_SUBTYPE_MH3D_MHA1) || (type==GF_ISOM_SUBTYPE_MH3D_MHA2))\n\t\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MPHA;\n\t\t\telse\n\t\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MHAS;\n\t\t\tif (ptr->cfg_mha) {\n\t\t\t\tGF_BitStream *bs = gf_bs_new(NULL, 0, GF_BITSTREAM_WRITE);\n\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->configuration_version);\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->mha_pl_indication);\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->reference_channel_layout);\n\t\t\t\tgf_bs_write_u16(bs, ptr->cfg_mha->mha_config ? ptr->cfg_mha->mha_config_size : 0);\n\t\t\t\tif (ptr->cfg_mha->mha_config && ptr->cfg_mha->mha_config_size)\n\t\t\t\t\tgf_bs_write_data(bs, ptr->cfg_mha->mha_config, ptr->cfg_mha->mha_config_size);\n\n\t\t\t\tgf_bs_get_content(bs, &esd->decoderConfig->decoderSpecificInfo->data, &esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t\t\tgf_bs_del(bs);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn GF_ISOM_INVALID_MEDIA;\n\t}\n\n\tif (true_desc_only) {\n\t\tif (!esd) return GF_ISOM_INVALID_MEDIA;\n\t\t*out_esd = esd;\n\t\treturn GF_OK;\n\t} else {\n\t\tif (!esd && !*out_esd) return GF_ISOM_INVALID_MEDIA;\n\t\tif (*out_esd == NULL) return gf_odf_desc_copy((GF_Descriptor *)esd, (GF_Descriptor **)out_esd);\n\t}\n\treturn GF_OK;\n}",
        "func": "GF_Err Media_GetESD(GF_MediaBox *mdia, u32 sampleDescIndex, GF_ESD **out_esd, Bool true_desc_only)\n{\n\tu32 type;\n\tGF_ESD *esd;\n\tGF_MPEGSampleEntryBox *entry = NULL;\n\tGF_ESDBox *ESDa;\n\tGF_ProtectionSchemeInfoBox *sinf;\n\tGF_SampleDescriptionBox *stsd = mdia->information->sampleTable->SampleDescription;\n\n\t*out_esd = NULL;\n\tif (!stsd || !stsd->child_boxes || !sampleDescIndex || (sampleDescIndex > gf_list_count(stsd->child_boxes)) )\n\t\treturn GF_BAD_PARAM;\n\n\tesd = NULL;\n\tentry = (GF_MPEGSampleEntryBox*)gf_list_get(stsd->child_boxes, sampleDescIndex - 1);\n\tif (! entry) return GF_ISOM_INVALID_MEDIA;\n\n\t*out_esd = NULL;\n\tESDa = NULL;\n\ttype = entry->type;\n\tswitch (type) {\n\tcase GF_ISOM_BOX_TYPE_ENCV:\n\tcase GF_ISOM_BOX_TYPE_ENCA:\n\tcase GF_ISOM_BOX_TYPE_ENCS:\n\tcase GF_ISOM_BOX_TYPE_ENCF:\n\tcase GF_ISOM_BOX_TYPE_ENCM:\n\tcase GF_ISOM_BOX_TYPE_ENCT:\n\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_SINF);\n\t\tif (sinf && sinf->original_format) {\n\t\t\ttype = sinf->original_format->data_format;\n\t\t}\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_RESV:\n\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_RINF);\n\t\tif (sinf && sinf->original_format) {\n\t\t\ttype = sinf->original_format->data_format;\n\t\t}\n\t\tbreak;\n\t}\n\n\n\tswitch (type) {\n\tcase GF_ISOM_BOX_TYPE_MP4V:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tESDa = ((GF_MPEGVisualSampleEntryBox*)entry)->esd;\n\t\tif (ESDa) esd = (GF_ESD *) ESDa->desc;\n\t\t/*avc1 encrypted*/\n\t\telse esd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_AVC1:\n\tcase GF_ISOM_BOX_TYPE_AVC2:\n\tcase GF_ISOM_BOX_TYPE_AVC3:\n\tcase GF_ISOM_BOX_TYPE_AVC4:\n\tcase GF_ISOM_BOX_TYPE_HVC1:\n\tcase GF_ISOM_BOX_TYPE_HEV1:\n\tcase GF_ISOM_BOX_TYPE_HVC2:\n\tcase GF_ISOM_BOX_TYPE_HEV2:\n\tcase GF_ISOM_BOX_TYPE_HVT1:\n\tcase GF_ISOM_BOX_TYPE_264B:\n\tcase GF_ISOM_BOX_TYPE_265B:\n\tcase GF_ISOM_BOX_TYPE_DVHE:\n\tcase GF_ISOM_BOX_TYPE_DVH1:\n\tcase GF_ISOM_BOX_TYPE_DVA1:\n\tcase GF_ISOM_BOX_TYPE_DVAV:\n\tcase GF_ISOM_BOX_TYPE_VVC1:\n\tcase GF_ISOM_BOX_TYPE_VVI1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_SVC1:\n\tcase GF_ISOM_BOX_TYPE_MVC1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif ((mdia->mediaTrack->extractor_mode & 0x0000FFFF) != GF_ISOM_NALU_EXTRACT_INSPECT)\n\t\t\tAVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, mdia);\n\t\telse\n\t\t\tAVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, NULL);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_LHE1:\n\tcase GF_ISOM_BOX_TYPE_LHV1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif ((mdia->mediaTrack->extractor_mode & 0x0000FFFF) != GF_ISOM_NALU_EXTRACT_INSPECT)\n\t\t\tHEVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, mdia);\n\t\telse\n\t\t\tHEVC_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*) entry, NULL);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*) entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_DAV1:\n\tcase GF_ISOM_BOX_TYPE_AV01:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tAV1_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*)entry, mdia);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*)entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_VP08:\n\tcase GF_ISOM_BOX_TYPE_VP09:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tVP9_RewriteESDescriptorEx((GF_MPEGVisualSampleEntryBox*)entry, mdia);\n\t\tesd = ((GF_MPEGVisualSampleEntryBox*)entry)->emul_esd;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4A:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n        {\n            GF_MPEGAudioSampleEntryBox *ase = (GF_MPEGAudioSampleEntryBox*)entry;\n            ESDa = ase->esd;\n            if (ESDa) {\n\t\t\t\tesd = (GF_ESD *) ESDa->desc;\n            } else if (!true_desc_only) {\n\t\t\t\tBool make_mp4a = GF_FALSE;\n\t\t\t\tsinf = (GF_ProtectionSchemeInfoBox *) gf_isom_box_find_child(entry->child_boxes, GF_ISOM_BOX_TYPE_SINF);\n\n\t\t\t\tif (sinf && sinf->original_format) {\n\t\t\t\t\tif (sinf->original_format->data_format==GF_ISOM_BOX_TYPE_MP4A) {\n\t\t\t\t\t\tmake_mp4a = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Assuming that if no ESD is provided the stream is Basic MPEG-4 AAC LC\n\t\t\t\t\tmake_mp4a = GF_TRUE;\n\t\t\t\t}\n\t\t\t\tif (make_mp4a) {\n#ifndef GPAC_DISABLE_AV_PARSERS\n\t\t\t\t\tGF_M4ADecSpecInfo aacinfo;\n\t\t\t\t\tmemset(&aacinfo, 0, sizeof(GF_M4ADecSpecInfo));\n\t\t\t\t\taacinfo.nb_chan = ase->channel_count;\n\t\t\t\t\taacinfo.base_object_type = GF_M4A_AAC_LC;\n\t\t\t\t\taacinfo.base_sr = ase->samplerate_hi;\n\t\t\t\t\t*out_esd = gf_odf_desc_esd_new(0);\n\t\t\t\t\t(*out_esd)->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\t\t\t(*out_esd)->decoderConfig->objectTypeIndication = GF_CODECID_AAC_MPEG4;\n\t\t\t\t\tgf_m4a_write_config(&aacinfo, &(*out_esd)->decoderConfig->decoderSpecificInfo->data, &(*out_esd)->decoderConfig->decoderSpecificInfo->dataLength);\n#else\n\t\t\t\t\treturn GF_NOT_SUPPORTED;\n#endif\n\t\t\t\t}\n            }\n        }\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4S:\n\t\tif (entry->internal_type==GF_ISOM_SAMPLE_ENTRY_MP4S) {\n\t\t\tESDa = entry->esd;\n\t\t\tif (ESDa) esd = (GF_ESD *) ESDa->desc;\n\t\t}\n\t\tbreak;\n#ifndef GPAC_DISABLE_TTXT\n\tcase GF_ISOM_BOX_TYPE_TX3G:\n\tcase GF_ISOM_BOX_TYPE_TEXT:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\tif (!true_desc_only && mdia->mediaTrack->moov->mov->convert_streaming_text) {\n\t\t\tGF_Err e = gf_isom_get_ttxt_esd(mdia, out_esd);\n\t\t\tif (e) return e;\n\t\t\tbreak;\n\t\t}\n\t\telse\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n#endif\n#ifndef GPAC_DISABLE_VTT\n\tcase GF_ISOM_BOX_TYPE_WVTT:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t{\n\t\tGF_WebVTTSampleEntryBox*vtte = (GF_WebVTTSampleEntryBox*)entry;\n\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t*out_esd = esd;\n\t\tesd->decoderConfig->streamType = GF_STREAM_TEXT;\n\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_WEBVTT;\n\t\tif (vtte->config) {\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength = (u32) strlen(vtte->config->string);\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data = gf_malloc(sizeof(char)*esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t\tmemcpy(esd->decoderConfig->decoderSpecificInfo->data, vtte->config->string, esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t}\n\t}\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_STPP:\n\tcase GF_ISOM_BOX_TYPE_SBTT:\n\tcase GF_ISOM_BOX_TYPE_STXT:\n\t\tbreak;\n#endif\n\n\tcase GF_ISOM_SUBTYPE_3GP_AMR:\n\tcase GF_ISOM_SUBTYPE_3GP_AMR_WB:\n\tcase GF_ISOM_SUBTYPE_3GP_EVRC:\n\tcase GF_ISOM_SUBTYPE_3GP_QCELP:\n\tcase GF_ISOM_SUBTYPE_3GP_SMV:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (!true_desc_only) {\n\t\t\tGF_Err e = gf_isom_get_3gpp_audio_esd(mdia->information->sampleTable, type, (GF_GenericAudioSampleEntryBox*)entry, out_esd);\n\t\t\tif (e) return e;\n\t\t\tbreak;\n\t\t} else return GF_ISOM_INVALID_MEDIA;\n\n\tcase GF_ISOM_SUBTYPE_OPUS:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t{\n\t\tGF_OpusSpecificBox *opus_c;\n\t\tif (true_desc_only)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\topus_c = ((GF_MPEGAudioSampleEntryBox*)entry)->cfg_opus;\n\t\tif (!opus_c) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"ESD not found for Opus\\n)\"));\n\t\t\tbreak;\n\t\t}\n\t\t*out_esd = gf_odf_desc_esd_new(2);\n\t\t(*out_esd)->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t(*out_esd)->decoderConfig->objectTypeIndication = GF_CODECID_OPUS;\n\t\tgf_odf_opus_cfg_write(&opus_c->opcfg, & (*out_esd)->decoderConfig->decoderSpecificInfo->data, & (*out_esd)->decoderConfig->decoderSpecificInfo->dataLength);\n\t\tbreak;\n\t}\n\tcase GF_ISOM_SUBTYPE_3GP_H263:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_VIDEO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_VISUAL;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_H263;\n\t\t\tbreak;\n\t\t}\n\n\tcase GF_ISOM_SUBTYPE_MP3:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MPEG_AUDIO;\n\t\t\tbreak;\n\t\t}\n\n\tcase GF_ISOM_SUBTYPE_LSR1:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_GENERIC)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tGF_LASeRSampleEntryBox*ptr = (GF_LASeRSampleEntryBox*)entry;\n\t\t\tif (!ptr || !ptr->lsr_config || !ptr->lsr_config->hdr_size)\n\t\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t\tesd =  gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_SCENE;\n\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_LASER;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength = ptr->lsr_config->hdr_size;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data = gf_malloc(sizeof(char)*ptr->lsr_config->hdr_size);\n\t\t\tif (!esd->decoderConfig->decoderSpecificInfo->data) return GF_OUT_OF_MEM;\n\t\t\tmemcpy(esd->decoderConfig->decoderSpecificInfo->data, ptr->lsr_config->hdr, sizeof(char)*ptr->lsr_config->hdr_size);\n\t\t\tbreak;\n\t\t}\n\tcase GF_ISOM_SUBTYPE_MH3D_MHA1:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHA2:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHM1:\n\tcase GF_ISOM_SUBTYPE_MH3D_MHM2:\n\t\tif (entry->internal_type != GF_ISOM_SAMPLE_ENTRY_AUDIO)\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\n\t\tif (true_desc_only) {\n\t\t\treturn GF_ISOM_INVALID_MEDIA;\n\t\t} else {\n\t\t\tGF_MPEGAudioSampleEntryBox*ptr = (GF_MPEGAudioSampleEntryBox*)entry;\n\t\t\tesd = gf_odf_desc_esd_new(2);\n\t\t\t*out_esd = esd;\n\t\t\tesd->decoderConfig->streamType = GF_STREAM_AUDIO;\n\t\t\tif ((type==GF_ISOM_SUBTYPE_MH3D_MHA1) || (type==GF_ISOM_SUBTYPE_MH3D_MHA2))\n\t\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MPHA;\n\t\t\telse\n\t\t\t\tesd->decoderConfig->objectTypeIndication = GF_CODECID_MHAS;\n\t\t\tif (ptr->cfg_mha) {\n\t\t\t\tGF_BitStream *bs = gf_bs_new(NULL, 0, GF_BITSTREAM_WRITE);\n\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->configuration_version);\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->mha_pl_indication);\n\t\t\t\tgf_bs_write_u8(bs, ptr->cfg_mha->reference_channel_layout);\n\t\t\t\tgf_bs_write_u16(bs, ptr->cfg_mha->mha_config ? ptr->cfg_mha->mha_config_size : 0);\n\t\t\t\tif (ptr->cfg_mha->mha_config && ptr->cfg_mha->mha_config_size)\n\t\t\t\t\tgf_bs_write_data(bs, ptr->cfg_mha->mha_config, ptr->cfg_mha->mha_config_size);\n\n\t\t\t\tgf_bs_get_content(bs, &esd->decoderConfig->decoderSpecificInfo->data, &esd->decoderConfig->decoderSpecificInfo->dataLength);\n\t\t\t\tgf_bs_del(bs);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn GF_ISOM_INVALID_MEDIA;\n\t}\n\n\tif (true_desc_only) {\n\t\tif (!esd) return GF_ISOM_INVALID_MEDIA;\n\t\t*out_esd = esd;\n\t\treturn GF_OK;\n\t} else {\n\t\tif (!esd && !*out_esd) return GF_ISOM_INVALID_MEDIA;\n\t\tif (*out_esd == NULL) return gf_odf_desc_copy((GF_Descriptor *)esd, (GF_Descriptor **)out_esd);\n\t}\n\treturn GF_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -249,6 +249,8 @@\n \t\t\treturn GF_ISOM_INVALID_MEDIA;\n \t\t} else {\n \t\t\tGF_LASeRSampleEntryBox*ptr = (GF_LASeRSampleEntryBox*)entry;\n+\t\t\tif (!ptr || !ptr->lsr_config || !ptr->lsr_config->hdr_size)\n+\t\t\t\treturn GF_ISOM_INVALID_MEDIA;\n \t\t\tesd =  gf_odf_desc_esd_new(2);\n \t\t\t*out_esd = esd;\n \t\t\tesd->decoderConfig->streamType = GF_STREAM_SCENE;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tif (!ptr || !ptr->lsr_config || !ptr->lsr_config->hdr_size)",
                "\t\t\t\treturn GF_ISOM_INVALID_MEDIA;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/gf_isom_parse_text_sample",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "GF_EXPORT\nGF_TextSample *gf_isom_parse_text_sample(GF_BitStream *bs)\n{\n\tGF_TextSample *s = gf_isom_new_text_sample();\n\n\t/*empty sample*/\n\tif (!bs || !gf_bs_available(bs)) return s;\n\n\ts->len = gf_bs_read_u16(bs);\n\tif (s->len) {\n\t\t/*2 extra bytes for UTF-16 term char just in case (we don't know if a BOM marker is present or\n\t\tnot since this may be a sample carried over RTP*/\n\t\ts->text = (char *) gf_malloc(sizeof(char)*(s->len+2) );\n\t\tif (!s->text) return NULL;\n\t\ts->text[s->len] = 0;\n\t\ts->text[s->len+1] = 0;\n\t\tgf_bs_read_data(bs, s->text, s->len);\n\t}\n\n\twhile (gf_bs_available(bs)) {\n\t\tGF_Box *a;\n\t\tGF_Err e = gf_isom_box_parse(&a, bs);\n\t\tif (e) break;\n\n\t\tswitch (a->type) {\n\t\tcase GF_ISOM_BOX_TYPE_STYL:\n\t\t\tif (s->styles) {\n\t\t\t\tGF_TextStyleBox *st2 = (GF_TextStyleBox *)a;\n\t\t\t\tif (!s->styles->entry_count) {\n\t\t\t\t\tgf_isom_box_del((GF_Box*)s->styles);\n\t\t\t\t\ts->styles = st2;\n\t\t\t\t} else {\n\t\t\t\t\ts->styles->styles = (GF_StyleRecord*)gf_realloc(s->styles->styles, sizeof(GF_StyleRecord) * (s->styles->entry_count + st2->entry_count));\n\t\t\t\t\tmemcpy(&s->styles->styles[s->styles->entry_count], st2->styles, sizeof(GF_StyleRecord) * st2->entry_count);\n\t\t\t\t\ts->styles->entry_count += st2->entry_count;\n\t\t\t\t\tgf_isom_box_del(a);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ts->styles = (GF_TextStyleBox*)a;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_KROK:\n\t\t\ts->cur_karaoke = (GF_TextKaraokeBox*)a;\n\t\tcase GF_ISOM_BOX_TYPE_HLIT:\n\t\tcase GF_ISOM_BOX_TYPE_HREF:\n\t\tcase GF_ISOM_BOX_TYPE_BLNK:\n\t\t\tgf_list_add(s->others, a);\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_HCLR:\n\t\t\tif (s->highlight_color) gf_isom_box_del(a);\n\t\t\telse s->highlight_color = (GF_TextHighlightColorBox *) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_DLAY:\n\t\t\tif (s->scroll_delay) gf_isom_box_del(a);\n\t\t\telse s->scroll_delay= (GF_TextScrollDelayBox*) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_TBOX:\n\t\t\tif (s->box) gf_isom_box_del(a);\n\t\t\telse s->box= (GF_TextBoxBox *) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_TWRP:\n\t\t\tif (s->wrap) gf_isom_box_del(a);\n\t\t\telse s->wrap= (GF_TextWrapBox*) a;\n\t\t\tbreak;\n\t\tcase GF_QT_BOX_TYPE_FRCD:\n\t\t\ts->is_forced = GF_TRUE;\n\t\t\tgf_isom_box_del(a);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgf_isom_box_del(a);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn s;\n}",
        "func": "GF_EXPORT\nGF_TextSample *gf_isom_parse_text_sample(GF_BitStream *bs)\n{\n\tGF_TextSample *s = gf_isom_new_text_sample();\n\n\t/*empty sample*/\n\tif (!bs || !gf_bs_available(bs)) return s;\n\n\ts->len = gf_bs_read_u16(bs);\n\tif (s->len) {\n\t\t/*2 extra bytes for UTF-16 term char just in case (we don't know if a BOM marker is present or\n\t\tnot since this may be a sample carried over RTP*/\n\t\ts->text = (char *) gf_malloc(sizeof(char)*(s->len+2) );\n\t\tif (!s->text) return NULL;\n\t\ts->text[s->len] = 0;\n\t\ts->text[s->len+1] = 0;\n\t\tgf_bs_read_data(bs, s->text, s->len);\n\t}\n\n\twhile (gf_bs_available(bs)) {\n\t\tGF_Box *a = NULL;\n\t\tGF_Err e = gf_isom_box_parse(&a, bs);\n\t\tif (e) {\n\t\t\tif (a) gf_isom_box_del(a);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (a->type) {\n\t\tcase GF_ISOM_BOX_TYPE_STYL:\n\t\t\tif (s->styles) {\n\t\t\t\tGF_TextStyleBox *st2 = (GF_TextStyleBox *)a;\n\t\t\t\tif (!s->styles->entry_count) {\n\t\t\t\t\tgf_isom_box_del((GF_Box*)s->styles);\n\t\t\t\t\ts->styles = st2;\n\t\t\t\t} else {\n\t\t\t\t\ts->styles->styles = (GF_StyleRecord*)gf_realloc(s->styles->styles, sizeof(GF_StyleRecord) * (s->styles->entry_count + st2->entry_count));\n\t\t\t\t\tmemcpy(&s->styles->styles[s->styles->entry_count], st2->styles, sizeof(GF_StyleRecord) * st2->entry_count);\n\t\t\t\t\ts->styles->entry_count += st2->entry_count;\n\t\t\t\t\tgf_isom_box_del(a);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ts->styles = (GF_TextStyleBox*)a;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_KROK:\n\t\t\ts->cur_karaoke = (GF_TextKaraokeBox*)a;\n\t\tcase GF_ISOM_BOX_TYPE_HLIT:\n\t\tcase GF_ISOM_BOX_TYPE_HREF:\n\t\tcase GF_ISOM_BOX_TYPE_BLNK:\n\t\t\tgf_list_add(s->others, a);\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_HCLR:\n\t\t\tif (s->highlight_color) gf_isom_box_del(a);\n\t\t\telse s->highlight_color = (GF_TextHighlightColorBox *) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_DLAY:\n\t\t\tif (s->scroll_delay) gf_isom_box_del(a);\n\t\t\telse s->scroll_delay= (GF_TextScrollDelayBox*) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_TBOX:\n\t\t\tif (s->box) gf_isom_box_del(a);\n\t\t\telse s->box= (GF_TextBoxBox *) a;\n\t\t\tbreak;\n\t\tcase GF_ISOM_BOX_TYPE_TWRP:\n\t\t\tif (s->wrap) gf_isom_box_del(a);\n\t\t\telse s->wrap= (GF_TextWrapBox*) a;\n\t\t\tbreak;\n\t\tcase GF_QT_BOX_TYPE_FRCD:\n\t\t\ts->is_forced = GF_TRUE;\n\t\t\tgf_isom_box_del(a);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgf_isom_box_del(a);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn s;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,9 +18,12 @@\n \t}\n \n \twhile (gf_bs_available(bs)) {\n-\t\tGF_Box *a;\n+\t\tGF_Box *a = NULL;\n \t\tGF_Err e = gf_isom_box_parse(&a, bs);\n-\t\tif (e) break;\n+\t\tif (e) {\n+\t\t\tif (a) gf_isom_box_del(a);\n+\t\t\tbreak;\n+\t\t}\n \n \t\tswitch (a->type) {\n \t\tcase GF_ISOM_BOX_TYPE_STYL:",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tGF_Box *a;",
                "\t\tif (e) break;"
            ],
            "added_lines": [
                "\t\tGF_Box *a = NULL;",
                "\t\tif (e) {",
                "\t\t\tif (a) gf_isom_box_del(a);",
                "\t\t\tbreak;",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/gf_isom_get_text_description",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "GF_EXPORT\nGF_Err gf_isom_get_text_description(GF_ISOFile *movie, u32 trackNumber, u32 descriptionIndex, GF_TextSampleDescriptor **out_desc)\n{\n\tGF_TrackBox *trak;\n\tu32 i;\n\tGF_Tx3gSampleEntryBox *txt = NULL;\n\tGF_TextSampleEntryBox *qt_txt = NULL;\n\tif (!descriptionIndex || !out_desc) return GF_BAD_PARAM;\n\n\ttrak = gf_isom_get_track_from_file(movie, trackNumber);\n\tif (!trak || !trak->Media) return GF_BAD_PARAM;\n\n\tswitch (trak->Media->handler->handlerType) {\n\tcase GF_ISOM_MEDIA_TEXT:\n\tcase GF_ISOM_MEDIA_SUBT:\n\tcase GF_ISOM_MEDIA_MPEG_SUBT:\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\ttxt = (GF_Tx3gSampleEntryBox*)gf_list_get(trak->Media->information->sampleTable->SampleDescription->child_boxes, descriptionIndex - 1);\n\tif (!txt) return GF_BAD_PARAM;\n\tswitch (txt->type) {\n\tcase GF_ISOM_BOX_TYPE_TX3G:\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_TEXT:\n\t\tqt_txt = (GF_TextSampleEntryBox *)txt;\n\t\ttxt = NULL;\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\t(*out_desc) = (GF_TextSampleDescriptor *) gf_odf_desc_new(GF_ODF_TX3G_TAG);\n\tif (! (*out_desc) ) return GF_OUT_OF_MEM;\n\n\tif (qt_txt) {\n\t\t(*out_desc)->back_color = rgb_48_to_32(qt_txt->background_color);\n\t\t(*out_desc)->default_pos = qt_txt->default_box;\n\t\t(*out_desc)->default_style.style_flags = 0; //todo, expose qt_txt->fontFace;\n\t\t(*out_desc)->default_style.text_color = rgb_48_to_32(qt_txt->foreground_color);\n\t\t(*out_desc)->displayFlags = qt_txt->displayFlags;\n\t\t(*out_desc)->vert_justif = -1;\n\t\t(*out_desc)->horiz_justif = qt_txt->textJustification;\n\t\tif (qt_txt->textName) {\n\t\t\t(*out_desc)->font_count = 1;\n\t\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord));\n\t\t\t(*out_desc)->fonts[0].fontName = gf_strdup(qt_txt->textName);\n\t\t}\n\t} else {\n\t\t(*out_desc)->back_color = txt->back_color;\n\t\t(*out_desc)->default_pos = txt->default_box;\n\t\t(*out_desc)->default_style = txt->default_style;\n\t\t(*out_desc)->displayFlags = txt->displayFlags;\n\t\t(*out_desc)->vert_justif = txt->vertical_justification;\n\t\t(*out_desc)->horiz_justif = txt->horizontal_justification;\n\t\t(*out_desc)->font_count = txt->font_table->entry_count;\n\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);\n\t\tfor (i=0; i<txt->font_table->entry_count; i++) {\n\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;\n\t\t\tif (txt->font_table->fonts[i].fontName)\n\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "func": "GF_EXPORT\nGF_Err gf_isom_get_text_description(GF_ISOFile *movie, u32 trackNumber, u32 descriptionIndex, GF_TextSampleDescriptor **out_desc)\n{\n\tGF_TrackBox *trak;\n\tu32 i;\n\tGF_Tx3gSampleEntryBox *txt = NULL;\n\tGF_TextSampleEntryBox *qt_txt = NULL;\n\tif (!descriptionIndex || !out_desc) return GF_BAD_PARAM;\n\n\ttrak = gf_isom_get_track_from_file(movie, trackNumber);\n\tif (!trak || !trak->Media) return GF_BAD_PARAM;\n\n\tswitch (trak->Media->handler->handlerType) {\n\tcase GF_ISOM_MEDIA_TEXT:\n\tcase GF_ISOM_MEDIA_SUBT:\n\tcase GF_ISOM_MEDIA_MPEG_SUBT:\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\ttxt = (GF_Tx3gSampleEntryBox*)gf_list_get(trak->Media->information->sampleTable->SampleDescription->child_boxes, descriptionIndex - 1);\n\tif (!txt) return GF_BAD_PARAM;\n\tswitch (txt->type) {\n\tcase GF_ISOM_BOX_TYPE_TX3G:\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_TEXT:\n\t\tqt_txt = (GF_TextSampleEntryBox *)txt;\n\t\ttxt = NULL;\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\t(*out_desc) = (GF_TextSampleDescriptor *) gf_odf_desc_new(GF_ODF_TX3G_TAG);\n\tif (! (*out_desc) ) return GF_OUT_OF_MEM;\n\n\tif (qt_txt) {\n\t\t(*out_desc)->back_color = rgb_48_to_32(qt_txt->background_color);\n\t\t(*out_desc)->default_pos = qt_txt->default_box;\n\t\t(*out_desc)->default_style.style_flags = 0; //todo, expose qt_txt->fontFace;\n\t\t(*out_desc)->default_style.text_color = rgb_48_to_32(qt_txt->foreground_color);\n\t\t(*out_desc)->displayFlags = qt_txt->displayFlags;\n\t\t(*out_desc)->vert_justif = -1;\n\t\t(*out_desc)->horiz_justif = qt_txt->textJustification;\n\t\tif (qt_txt->textName) {\n\t\t\t(*out_desc)->font_count = 1;\n\t\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord));\n\t\t\t(*out_desc)->fonts[0].fontName = gf_strdup(qt_txt->textName);\n\t\t}\n\t} else {\n\t\t(*out_desc)->back_color = txt->back_color;\n\t\t(*out_desc)->default_pos = txt->default_box;\n\t\t(*out_desc)->default_style = txt->default_style;\n\t\t(*out_desc)->displayFlags = txt->displayFlags;\n\t\t(*out_desc)->vert_justif = txt->vertical_justification;\n\t\t(*out_desc)->horiz_justif = txt->horizontal_justification;\n\t\tif (txt->font_table && txt->font_table->entry_count) {\n\t\t\t(*out_desc)->font_count = txt->font_table->entry_count;\n\t\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);\n\t\t\tfor (i=0; i<txt->font_table->entry_count; i++) {\n\t\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;\n\t\t\t\tif (txt->font_table->fonts[i].fontName)\n\t\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);\n\t\t\t}\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -55,12 +55,14 @@\n \t\t(*out_desc)->displayFlags = txt->displayFlags;\n \t\t(*out_desc)->vert_justif = txt->vertical_justification;\n \t\t(*out_desc)->horiz_justif = txt->horizontal_justification;\n-\t\t(*out_desc)->font_count = txt->font_table->entry_count;\n-\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);\n-\t\tfor (i=0; i<txt->font_table->entry_count; i++) {\n-\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;\n-\t\t\tif (txt->font_table->fonts[i].fontName)\n-\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);\n+\t\tif (txt->font_table && txt->font_table->entry_count) {\n+\t\t\t(*out_desc)->font_count = txt->font_table->entry_count;\n+\t\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);\n+\t\t\tfor (i=0; i<txt->font_table->entry_count; i++) {\n+\t\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;\n+\t\t\t\tif (txt->font_table->fonts[i].fontName)\n+\t\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);\n+\t\t\t}\n \t\t}\n \t}\n \treturn GF_OK;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t(*out_desc)->font_count = txt->font_table->entry_count;",
                "\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);",
                "\t\tfor (i=0; i<txt->font_table->entry_count; i++) {",
                "\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;",
                "\t\t\tif (txt->font_table->fonts[i].fontName)",
                "\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);"
            ],
            "added_lines": [
                "\t\tif (txt->font_table && txt->font_table->entry_count) {",
                "\t\t\t(*out_desc)->font_count = txt->font_table->entry_count;",
                "\t\t\t(*out_desc)->fonts = (GF_FontRecord *) gf_malloc(sizeof(GF_FontRecord) * txt->font_table->entry_count);",
                "\t\t\tfor (i=0; i<txt->font_table->entry_count; i++) {",
                "\t\t\t\t(*out_desc)->fonts[i].fontID = txt->font_table->fonts[i].fontID;",
                "\t\t\t\tif (txt->font_table->fonts[i].fontName)",
                "\t\t\t\t\t(*out_desc)->fonts[i].fontName = gf_strdup(txt->font_table->fonts[i].fontName);",
                "\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/isor_get_chapters",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "static void isor_get_chapters(GF_ISOFile *file, GF_FilterPid *opid)\n{\n\tu32 i, count;\n\tGF_PropertyValue p;\n\tGF_PropUIntList times;\n\tGF_PropStringList names;\n\tcount = gf_isom_get_chapter_count(file, 0);\n\tif (count) {\n\t\ttimes.vals = gf_malloc(sizeof(u32)*count);\n\t\tnames.vals = gf_malloc(sizeof(char *)*count);\n\t\ttimes.nb_items = names.nb_items = count;\n\n\t\tfor (i=0; i<count; i++) {\n\t\t\tconst char *name;\n\t\t\tu64 start;\n\t\t\tgf_isom_get_chapter(file, 0, i+1, &start, &name);\n\t\t\ttimes.vals[i] = (u32) start;\n\t\t\tnames.vals[i] = gf_strdup(name);\n\t\t}\n\t\tp.type = GF_PROP_UINT_LIST;\n\t\tp.value.uint_list = times;\n\t\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_TIMES, &p);\n\t\tgf_free(times.vals);\n\n\t\tp.type = GF_PROP_STRING_LIST;\n\t\tp.value.string_list = names;\n\t\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_NAMES, &p);\n\t\t//no free for string lists\n\t\treturn;\n\t}\n\n\tu32 chap_tk=0;\n\tcount = gf_isom_get_track_count(file);\n\tfor (i=0; i<count; i++) {\n\t\tu32 nb_ref = gf_isom_get_reference_count(file, i+1, GF_ISOM_REF_CHAP);\n\t\tif (nb_ref) {\n\t\t\tgf_isom_get_reference(file, i+1, GF_ISOM_REF_CHAP, 1, &chap_tk);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (chap_tk) {\n\t\tcount = gf_isom_get_sample_count(file, chap_tk);\n\t\tif (!count) chap_tk=0;\n\t}\n\tif (!chap_tk) return;\n\n\ttimes.vals = gf_malloc(sizeof(u32)*count);\n\tnames.vals = gf_malloc(sizeof(char *)*count);\n\ttimes.nb_items = names.nb_items = count;\n\n\tfor (i=0; i<count; i++) {\n\t\tu32 di;\n\t\tGF_ISOSample *s = gf_isom_get_sample(file, chap_tk, i+1, &di);\n\t\tif (!s) continue;\n\t\tGF_BitStream *bs = gf_bs_new(s->data, s->dataLength, GF_BITSTREAM_READ);\n\t\tGF_TextSample *txt = gf_isom_parse_text_sample(bs);\n\t\tif (txt) {\n\t\t\ttimes.vals[i] = (u32) s->DTS;\n\t\t\tnames.vals[i] = gf_strdup(txt->text);\n\t\t\tgf_isom_delete_text_sample(txt);\n\t\t}\n\t\tgf_bs_del(bs);\n\t\tgf_isom_sample_del(&s);\n\t}\n\tp.type = GF_PROP_UINT_LIST;\n\tp.value.uint_list = times;\n\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_TIMES, &p);\n\tgf_free(times.vals);\n\n\tp.type = GF_PROP_STRING_LIST;\n\tp.value.string_list = names;\n\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_NAMES, &p);\n\n}",
        "func": "static void isor_get_chapters(GF_ISOFile *file, GF_FilterPid *opid)\n{\n\tu32 i, count;\n\tGF_PropertyValue p;\n\tGF_PropUIntList times;\n\tGF_PropStringList names;\n\tcount = gf_isom_get_chapter_count(file, 0);\n\tif (count) {\n\t\ttimes.vals = gf_malloc(sizeof(u32)*count);\n\t\tnames.vals = gf_malloc(sizeof(char *)*count);\n\t\ttimes.nb_items = names.nb_items = count;\n\n\t\tfor (i=0; i<count; i++) {\n\t\t\tconst char *name;\n\t\t\tu64 start;\n\t\t\tgf_isom_get_chapter(file, 0, i+1, &start, &name);\n\t\t\ttimes.vals[i] = (u32) start;\n\t\t\tnames.vals[i] = gf_strdup(name);\n\t\t}\n\t\tp.type = GF_PROP_UINT_LIST;\n\t\tp.value.uint_list = times;\n\t\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_TIMES, &p);\n\t\tgf_free(times.vals);\n\n\t\tp.type = GF_PROP_STRING_LIST;\n\t\tp.value.string_list = names;\n\t\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_NAMES, &p);\n\t\t//no free for string lists\n\t\treturn;\n\t}\n\n\tu32 chap_tk=0;\n\tcount = gf_isom_get_track_count(file);\n\tfor (i=0; i<count; i++) {\n\t\tu32 nb_ref = gf_isom_get_reference_count(file, i+1, GF_ISOM_REF_CHAP);\n\t\tif (nb_ref) {\n\t\t\tgf_isom_get_reference(file, i+1, GF_ISOM_REF_CHAP, 1, &chap_tk);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (chap_tk) {\n\t\tcount = gf_isom_get_sample_count(file, chap_tk);\n\t\tif (!count) chap_tk=0;\n\t}\n\tif (!chap_tk) return;\n\n\ttimes.vals = gf_malloc(sizeof(u32)*count);\n\tnames.vals = gf_malloc(sizeof(char *)*count);\n\ttimes.nb_items = names.nb_items = count;\n\n\tfor (i=0; i<count; i++) {\n\t\tu32 di;\n\t\tGF_ISOSample *s = gf_isom_get_sample(file, chap_tk, i+1, &di);\n\t\tif (!s) continue;\n\t\tGF_BitStream *bs = gf_bs_new(s->data, s->dataLength, GF_BITSTREAM_READ);\n\t\tGF_TextSample *txt = gf_isom_parse_text_sample(bs);\n\t\tif (txt) {\n\t\t\ttimes.vals[i] = (u32) s->DTS;\n\t\t\tnames.vals[i] = gf_strdup(txt->text ? txt->text : \"\");\n\t\t\tgf_isom_delete_text_sample(txt);\n\t\t}\n\t\tgf_bs_del(bs);\n\t\tgf_isom_sample_del(&s);\n\t}\n\tp.type = GF_PROP_UINT_LIST;\n\tp.value.uint_list = times;\n\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_TIMES, &p);\n\tgf_free(times.vals);\n\n\tp.type = GF_PROP_STRING_LIST;\n\tp.value.string_list = names;\n\tgf_filter_pid_set_property(opid, GF_PROP_PID_CHAP_NAMES, &p);\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,7 +56,7 @@\n \t\tGF_TextSample *txt = gf_isom_parse_text_sample(bs);\n \t\tif (txt) {\n \t\t\ttimes.vals[i] = (u32) s->DTS;\n-\t\t\tnames.vals[i] = gf_strdup(txt->text);\n+\t\t\tnames.vals[i] = gf_strdup(txt->text ? txt->text : \"\");\n \t\t\tgf_isom_delete_text_sample(txt);\n \t\t}\n \t\tgf_bs_del(bs);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tnames.vals[i] = gf_strdup(txt->text);"
            ],
            "added_lines": [
                "\t\t\tnames.vals[i] = gf_strdup(txt->text ? txt->text : \"\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/isor_declare_item_properties",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "Bool isor_declare_item_properties(ISOMReader *read, ISOMChannel *ch, u32 item_idx)\n{\n\tGF_ImageItemProperties props;\n\tGF_FilterPid *pid;\n\tGF_ESD *esd;\n\tu32 item_id=0;\n\tu32 scheme_type=0, scheme_version=0, item_type;\n\tu32 item_codecid=0;\n\tconst char *item_name, *item_mime_type, *item_encoding;\n\nretry:\n\n\tif (item_idx>gf_isom_get_meta_item_count(read->mov, GF_TRUE, 0))\n\t\treturn GF_FALSE;\n\n\titem_name = item_mime_type = item_encoding = NULL;\n\tgf_isom_get_meta_item_info(read->mov, GF_TRUE, 0, item_idx, &item_id, &item_type, &scheme_type, &scheme_version, NULL, NULL, NULL, &item_name, &item_mime_type, &item_encoding);\n\n\tif (!item_id) return GF_FALSE;\n\tif (item_type==GF_ISOM_ITEM_TYPE_AUXI) return GF_FALSE;\n\tif (read->play_only_track_id && (read->play_only_track_id!=item_id)) return GF_FALSE;\n\n\tgf_isom_get_meta_image_props(read->mov, GF_TRUE, 0, item_id, &props, NULL);\n\n\t//check we support the protection scheme\n\tswitch (scheme_type) {\n\tcase GF_ISOM_CBC_SCHEME:\n\tcase GF_ISOM_CENS_SCHEME:\n\tcase GF_ISOM_CBCS_SCHEME:\n\tcase GF_ISOM_CENC_SCHEME:\n\tcase GF_ISOM_PIFF_SCHEME:\n\tcase 0:\n\t\tbreak;\n\tdefault:\n\t\treturn GF_FALSE;\n\t}\n\n\n\tesd = gf_media_map_item_esd(read->mov, item_id);\n\tif (!esd) {\n\t\tswitch (item_type) {\n\t\tcase GF_ISOM_SUBTYPE_HVT1:\n\t\t\titem_codecid = GF_CODECID_HEVC_TILES;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t//unsupported item, try next\n\t\t\titem_idx++;\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\titem_codecid = esd->decoderConfig->objectTypeIndication;\n\t}\n\n\t//OK declare PID\n\tif (!ch)\n\t\tpid = gf_filter_pid_new(read->filter);\n\telse {\n\t\tpid = ch->pid;\n\t\tch->item_id = item_id;\n\t}\n\n\n\t//do not override PID ID if itt is set\n\tif (!ch) {\n\t\tif (read->pid)\n\t\t\tgf_filter_pid_copy_properties(pid, read->pid);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ID, &PROP_UINT(esd ? esd->ESID : item_id));\n\t}\n\n\tif (read->itemid)\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ITEM_ID, &PROP_UINT(item_id));\n\t\t\n\tif ((item_codecid==GF_CODECID_HEVC) && gf_isom_meta_item_has_ref(read->mov, GF_TRUE, 0, item_id, GF_ISOM_REF_TBAS)) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_TILE_BASE, &PROP_BOOL(GF_TRUE));\n\t}\n\n\n\t//TODO: no support for LHEVC images\n\t//gf_filter_pid_set_property(pid, GF_PROP_PID_DEPENDENCY_ID, &PROP_UINT(esd->dependsOnESID));\n\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_VISUAL));\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_CODECID, &PROP_UINT(item_codecid));\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_TIMESCALE, &PROP_UINT(1000));\n\tif (esd) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_TIMESCALE, &PROP_UINT(1000));\n\t\tif (esd->decoderConfig->decoderSpecificInfo && esd->decoderConfig->decoderSpecificInfo->data) {\n\t\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_DECODER_CONFIG, &PROP_DATA_NO_COPY(esd->decoderConfig->decoderSpecificInfo->data, esd->decoderConfig->decoderSpecificInfo->dataLength));\n\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data=NULL;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength=0;\n\t\t}\n\t\tgf_odf_desc_del((GF_Descriptor *)esd);\n\t}\n\n\tif (props.width && props.height) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_WIDTH, &PROP_UINT(props.width));\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_HEIGHT, &PROP_UINT(props.height));\n\t}\n\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_HIDDEN, props.hidden ? &PROP_BOOL(GF_TRUE) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_ALPHA, props.alpha ? &PROP_BOOL(GF_TRUE) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_MIRROR, props.mirror ? &PROP_UINT(props.mirror) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_ROTATE, props.alpha ? &PROP_UINT(props.angle) : NULL);\n\n\tif (props.clap_wden) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_W, &PROP_FRAC_INT(props.clap_wnum,props.clap_wden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_H, &PROP_FRAC_INT(props.clap_hnum,props.clap_hden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_X, &PROP_FRAC_INT(props.clap_honum,props.clap_hoden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_Y, &PROP_FRAC_INT(props.clap_vonum,props.clap_voden) );\n\t} else {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_W, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_H, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_X, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_Y, NULL);\n\t}\n\n\tif (gf_isom_get_meta_primary_item_id(read->mov, GF_TRUE, 0) == item_id) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PRIMARY_ITEM, &PROP_BOOL(GF_TRUE));\n\t} else {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PRIMARY_ITEM, &PROP_BOOL(GF_FALSE));\n\t}\n\n\tif (!gf_sys_is_test_mode() && !read->itt)\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ITEM_NUM, &PROP_UINT(item_idx) );\n\n\tgf_filter_pid_set_property_str(pid, \"meta:mime\", item_mime_type ? &PROP_STRING(item_mime_type) : NULL );\n\tgf_filter_pid_set_property_str(pid, \"meta:name\", item_name ? &PROP_STRING(item_name) : NULL );\n\tgf_filter_pid_set_property_str(pid, \"meta:encoding\", item_encoding ? &PROP_STRING(item_encoding) : NULL );\n\n\tif ((item_type == GF_ISOM_SUBTYPE_UNCV) || (item_type == GF_ISOM_ITEM_TYPE_UNCI)) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PIXFMT, &PROP_UINT(GF_PIXEL_UNCV) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CODECID, &PROP_UINT(GF_CODECID_RAW_UNCV) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ISOM_SUBTYPE,  &PROP_4CC(GF_ISOM_ITEM_TYPE_UNCI) );\n\t}\n\n\tif (item_codecid == GF_CODECID_HEVC_TILES) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CROP_POS, &PROP_VEC2I_INT(props.hOffset, props.vOffset) );\n\n\t\tu32 base_id = gf_isom_meta_get_item_ref_id(read->mov, GF_TRUE, 0, item_id, GF_ISOM_REF_TBAS, 1);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_DEPENDENCY_ID, base_id ? &PROP_UINT(base_id) : NULL );\n\t}\n\n\t//setup cenc\n\tif (scheme_type) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PROTECTION_SCHEME_TYPE, &PROP_4CC(scheme_type) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PROTECTION_SCHEME_VERSION, &PROP_UINT(scheme_version) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ENCRYPTED, &PROP_BOOL(GF_TRUE) );\n\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ORIG_STREAM_TYPE, &PROP_UINT(GF_STREAM_VISUAL));\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_ENCRYPTED) );\n\t}\n\n\tif (!ch) {\n\t\tch = isor_create_channel(read, pid, 0, item_id, GF_FALSE);\n\t\tif (ch && scheme_type) {\n\t\t\tch->is_cenc = 1;\n\t\t\tch->is_encrypted = 1;\n\n\t\t\tisor_declare_pssh(ch);\n\n\t\t}\n\t}\n\treturn GF_TRUE;\n}",
        "func": "Bool isor_declare_item_properties(ISOMReader *read, ISOMChannel *ch, u32 item_idx)\n{\n\tGF_ImageItemProperties props;\n\tGF_FilterPid *pid;\n\tGF_ESD *esd;\n\tu32 item_id=0;\n\tu32 scheme_type=0, scheme_version=0, item_type;\n\tu32 item_codecid=0;\n\tconst char *item_name, *item_mime_type, *item_encoding;\n\nretry:\n\n\tif (item_idx>gf_isom_get_meta_item_count(read->mov, GF_TRUE, 0))\n\t\treturn GF_FALSE;\n\n\titem_name = item_mime_type = item_encoding = NULL;\n\tgf_isom_get_meta_item_info(read->mov, GF_TRUE, 0, item_idx, &item_id, &item_type, &scheme_type, &scheme_version, NULL, NULL, NULL, &item_name, &item_mime_type, &item_encoding);\n\n\tif (!item_id) return GF_FALSE;\n\tif (item_type==GF_ISOM_ITEM_TYPE_AUXI) return GF_FALSE;\n\tif (read->play_only_track_id && (read->play_only_track_id!=item_id)) return GF_FALSE;\n\n\tgf_isom_get_meta_image_props(read->mov, GF_TRUE, 0, item_id, &props, NULL);\n\n\t//check we support the protection scheme\n\tswitch (scheme_type) {\n\tcase GF_ISOM_CBC_SCHEME:\n\tcase GF_ISOM_CENS_SCHEME:\n\tcase GF_ISOM_CBCS_SCHEME:\n\tcase GF_ISOM_CENC_SCHEME:\n\tcase GF_ISOM_PIFF_SCHEME:\n\tcase 0:\n\t\tbreak;\n\tdefault:\n\t\treturn GF_FALSE;\n\t}\n\n\n\tesd = gf_media_map_item_esd(read->mov, item_id);\n\tif (!esd) {\n\t\tswitch (item_type) {\n\t\tcase GF_ISOM_SUBTYPE_HVT1:\n\t\t\titem_codecid = GF_CODECID_HEVC_TILES;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t//unsupported item, try next\n\t\t\titem_idx++;\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\titem_codecid = esd->decoderConfig->objectTypeIndication;\n\t}\n\n\t//OK declare PID\n\tif (!ch)\n\t\tpid = gf_filter_pid_new(read->filter);\n\telse {\n\t\tpid = ch->pid;\n\t\tch->item_id = item_id;\n\t}\n\n\n\t//do not override PID ID if itt is set\n\tif (!ch) {\n\t\tif (read->pid)\n\t\t\tgf_filter_pid_copy_properties(pid, read->pid);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ID, &PROP_UINT(esd ? esd->ESID : item_id));\n\t}\n\n\tif (read->itemid)\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ITEM_ID, &PROP_UINT(item_id));\n\n\tif ((item_codecid==GF_CODECID_HEVC) && gf_isom_meta_item_has_ref(read->mov, GF_TRUE, 0, item_id, GF_ISOM_REF_TBAS)) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_TILE_BASE, &PROP_BOOL(GF_TRUE));\n\t}\n\n\n\t//TODO: no support for LHEVC images\n\t//gf_filter_pid_set_property(pid, GF_PROP_PID_DEPENDENCY_ID, &PROP_UINT(esd->dependsOnESID));\n\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_VISUAL));\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_CODECID, &PROP_UINT(item_codecid));\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_TIMESCALE, &PROP_UINT(1000));\n\tif (esd) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_TIMESCALE, &PROP_UINT(1000));\n\t\tif (esd->decoderConfig->decoderSpecificInfo && esd->decoderConfig->decoderSpecificInfo->data) {\n\t\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_DECODER_CONFIG, &PROP_DATA_NO_COPY(esd->decoderConfig->decoderSpecificInfo->data, esd->decoderConfig->decoderSpecificInfo->dataLength));\n\n\t\t\tesd->decoderConfig->decoderSpecificInfo->data=NULL;\n\t\t\tesd->decoderConfig->decoderSpecificInfo->dataLength=0;\n\t\t}\n\t\tgf_odf_desc_del((GF_Descriptor *)esd);\n\t}\n\n\tif (props.width && props.height) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_WIDTH, &PROP_UINT(props.width));\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_HEIGHT, &PROP_UINT(props.height));\n\t}\n\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_HIDDEN, props.hidden ? &PROP_BOOL(GF_TRUE) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_ALPHA, props.alpha ? &PROP_BOOL(GF_TRUE) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_MIRROR, props.mirror ? &PROP_UINT(props.mirror) : NULL);\n\tgf_filter_pid_set_property(pid, GF_PROP_PID_ROTATE, props.alpha ? &PROP_UINT(props.angle) : NULL);\n\n\tif (props.clap_wden) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_W, &PROP_FRAC_INT(props.clap_wnum,props.clap_wden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_H, &PROP_FRAC_INT(props.clap_hnum,props.clap_hden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_X, &PROP_FRAC_INT(props.clap_honum,props.clap_hoden) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_Y, &PROP_FRAC_INT(props.clap_vonum,props.clap_voden) );\n\t} else {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_W, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_H, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_X, NULL);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CLAP_Y, NULL);\n\t}\n\n\tif (gf_isom_get_meta_primary_item_id(read->mov, GF_TRUE, 0) == item_id) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PRIMARY_ITEM, &PROP_BOOL(GF_TRUE));\n\t} else {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PRIMARY_ITEM, &PROP_BOOL(GF_FALSE));\n\t}\n\n\tif (!gf_sys_is_test_mode() && !read->itt)\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ITEM_NUM, &PROP_UINT(item_idx) );\n\n\tgf_filter_pid_set_property_str(pid, \"meta:mime\", item_mime_type ? &PROP_STRING(item_mime_type) : NULL );\n\tgf_filter_pid_set_property_str(pid, \"meta:name\", item_name ? &PROP_STRING(item_name) : NULL );\n\tgf_filter_pid_set_property_str(pid, \"meta:encoding\", item_encoding ? &PROP_STRING(item_encoding) : NULL );\n\n\tif ((item_type == GF_ISOM_SUBTYPE_UNCV) || (item_type == GF_ISOM_ITEM_TYPE_UNCI)) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PIXFMT, &PROP_UINT(GF_PIXEL_UNCV) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CODECID, &PROP_UINT(GF_CODECID_RAW_UNCV) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ISOM_SUBTYPE,  &PROP_4CC(GF_ISOM_ITEM_TYPE_UNCI) );\n\t}\n\n\tif (item_codecid == GF_CODECID_HEVC_TILES) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_CROP_POS, &PROP_VEC2I_INT(props.hOffset, props.vOffset) );\n\n\t\tu32 base_id = gf_isom_meta_get_item_ref_id(read->mov, GF_TRUE, 0, item_id, GF_ISOM_REF_TBAS, 1);\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_DEPENDENCY_ID, base_id ? &PROP_UINT(base_id) : NULL );\n\t}\n\n\t//setup cenc\n\tif (scheme_type) {\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PROTECTION_SCHEME_TYPE, &PROP_4CC(scheme_type) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_PROTECTION_SCHEME_VERSION, &PROP_UINT(scheme_version) );\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ENCRYPTED, &PROP_BOOL(GF_TRUE) );\n\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ORIG_STREAM_TYPE, &PROP_UINT(GF_STREAM_VISUAL));\n\t\tgf_filter_pid_set_property(pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_ENCRYPTED) );\n\t}\n\n\tif (!ch) {\n\t\tch = isor_create_channel(read, pid, 0, item_id, GF_FALSE);\n\t\tif (ch && scheme_type) {\n\t\t\tch->is_cenc = 1;\n\t\t\tch->is_encrypted = 1;\n\n\t\t\tisor_declare_pssh(ch);\n\n\t\t}\n\t}\n\treturn GF_TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -69,7 +69,7 @@\n \n \tif (read->itemid)\n \t\tgf_filter_pid_set_property(pid, GF_PROP_PID_ITEM_ID, &PROP_UINT(item_id));\n-\t\t\n+\n \tif ((item_codecid==GF_CODECID_HEVC) && gf_isom_meta_item_has_ref(read->mov, GF_TRUE, 0, item_id, GF_ISOM_REF_TBAS)) {\n \t\tgf_filter_pid_set_property(pid, GF_PROP_PID_TILE_BASE, &PROP_BOOL(GF_TRUE));\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/isor_declare_objects",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "GF_Err isor_declare_objects(ISOMReader *read)\n{\n\tconst u8 *tag;\n\tu32 tlen;\n\tu32 i, count, j, track_id;\n\tBool highest_stream;\n\tBool single_media_found = GF_FALSE;\n\tBool use_iod = GF_FALSE;\n\tBool tk_found = GF_FALSE;\n\tGF_Err e;\n\tBool isom_contains_video = GF_FALSE;\n\tGF_Descriptor *od = gf_isom_get_root_od(read->mov);\n\tif (od && gf_list_count(((GF_ObjectDescriptor*)od)->ESDescriptors)) {\n\t\tuse_iod = GF_TRUE;\n\t}\n\tif (od) gf_odf_desc_del(od);\n\n\t/*TODO\n\t check for alternate tracks\n    */\n\tcount = gf_isom_get_track_count(read->mov);\n\tfor (i=0; i<count; i++) {\n\t\tu32 mtype, m_subtype, streamtype, stsd_idx;\n\n\t\tmtype = gf_isom_get_media_type(read->mov, i+1);\n\n\t\tif (read->tkid) {\n\t\t\tu32 for_id=0;\n\t\t\tif (sscanf(read->tkid, \"%d\", &for_id)) {\n\t\t\t\tu32 id = gf_isom_get_track_id(read->mov, i+1);\n\t\t\t\tif (id != for_id) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"audio\")) {\n\t\t\t\tif (mtype!=GF_ISOM_MEDIA_AUDIO) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"video\")) {\n\t\t\t\tif (mtype!=GF_ISOM_MEDIA_VISUAL) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"text\")) {\n\t\t\t\tif ((mtype!=GF_ISOM_MEDIA_TEXT) && (mtype!=GF_ISOM_MEDIA_SUBT) && (mtype!=GF_ISOM_MEDIA_MPEG_SUBT)) continue;\n\t\t\t} else if (strlen(read->tkid)==4) {\n\t\t\t\tu32 t = GF_4CC(read->tkid[0], read->tkid[1], read->tkid[2], read->tkid[3]);\n\t\t\t\tif (mtype!=t) continue;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] Bad format for tkid option %s, no match\\n\", read->tkid));\n\t\t\t\treturn GF_BAD_PARAM;\n\t\t\t}\n\t\t\ttk_found = GF_TRUE;\n\t\t}\n\n\t\tswitch (mtype) {\n\t\tcase GF_ISOM_MEDIA_AUDIO:\n\t\t\tstreamtype = GF_STREAM_AUDIO;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_VISUAL:\n\t\tcase GF_ISOM_MEDIA_AUXV:\n\t\tcase GF_ISOM_MEDIA_PICT:\n\t\tcase GF_ISOM_MEDIA_QTVR:\n\t\t\tstreamtype = GF_STREAM_VISUAL;\n\t\t\tisom_contains_video = GF_TRUE;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_TEXT:\n\t\tcase GF_ISOM_MEDIA_SUBT:\n\t\tcase GF_ISOM_MEDIA_SUBPIC:\n\t\tcase GF_ISOM_MEDIA_MPEG_SUBT:\n\t\tcase GF_ISOM_MEDIA_CLOSED_CAPTION:\n\t\t\tstreamtype = GF_STREAM_TEXT;\n\t\t\tmtype = GF_ISOM_MEDIA_TEXT;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_FLASH:\n\t\tcase GF_ISOM_MEDIA_DIMS:\n\t\tcase GF_ISOM_MEDIA_SCENE:\n\t\t\tstreamtype = GF_STREAM_SCENE;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_OD:\n\t\t\tstreamtype = GF_STREAM_OD;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_META:\n\t\tcase GF_ISOM_MEDIA_TIMECODE:\n\t\t\tstreamtype = GF_STREAM_METADATA;\n\t\t\tbreak;\n\t\t/*hint tracks are never exported*/\n\t\tcase GF_ISOM_MEDIA_HINT:\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (!read->allt) {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d type %s not supported, ignoring track - you may retry by specifying allt option\\n\", i+1, gf_4cc_to_str(mtype) ));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstreamtype = GF_STREAM_UNKNOWN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!read->alltk && !read->tkid && !gf_isom_is_track_enabled(read->mov, i+1)) {\n\t\t\tif (count>1) {\n\t\t\t\tu32 type = gf_isom_get_media_type(read->mov, i+1);\n\n\t\t\t\t//we don't warn for disabled text tracks due to chapters and forced subs\n\t\t\t\tif ((type==GF_ISOM_SUBTYPE_TEXT) || (type==GF_ISOM_MEDIA_SUBT))\n\t\t\t\t\tcontinue;\n\t\t\t\t//disabled tracks using QT chapter refs, do not warn\n\t\t\t\tif (gf_isom_is_track_referenced(read->mov, i+1, GF_ISOM_REF_CHAP)) continue;\n\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d is disabled, ignoring track - you may retry by specifying alltk option\\n\", i+1));\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d is disabled but single track in file, considering it enabled\\n\", i+1 ));\n\t\t\t}\n\t\t}\n\n\t\tstsd_idx = read->stsd ? read->stsd : 1;\n\t\t//some subtypes are not declared as readable objects\n\t\tm_subtype = gf_isom_get_media_subtype(read->mov, i+1, stsd_idx);\n\t\tswitch (m_subtype) {\n\t\tcase GF_ISOM_SUBTYPE_HVT1:\n\t\t\tif (read->smode == MP4DMX_SINGLE)\n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t/*we declare only the highest video track (i.e the track we play)*/\n\t\thighest_stream = GF_TRUE;\n\t\ttrack_id = gf_isom_get_track_id(read->mov, i+1);\n\t\tif (read->play_only_track_id && (read->play_only_track_id != track_id)) continue;\n\n\t\tif (read->play_only_first_media) {\n\t\t\tif (read->play_only_first_media != mtype) continue;\n\t\t\tif (single_media_found) continue;\n\t\t\tsingle_media_found = GF_TRUE;\n\t\t}\n\n\t\tfor (j = 0; j < count; j++) {\n\t\t\tif (gf_isom_has_track_reference(read->mov, j+1, GF_ISOM_REF_SCAL, track_id) > 0) {\n\t\t\t\thighest_stream = GF_FALSE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (gf_isom_has_track_reference(read->mov, j+1, GF_ISOM_REF_BASE, track_id) > 0) {\n\t\t\t\thighest_stream = GF_FALSE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif ((read->smode==MP4DMX_SINGLE) && (gf_isom_get_media_type(read->mov, i+1) == GF_ISOM_MEDIA_VISUAL) && !highest_stream)\n\t\t\tcontinue;\n\n\n\t\tisor_declare_track(read, NULL, i+1, stsd_idx, streamtype, use_iod);\n\n\t\tif (read->tkid)\n\t\t\tbreak;\n\t}\n\n\tif (!read->tkid) {\n\t\t/*declare image items*/\n\t\tcount = gf_isom_get_meta_item_count(read->mov, GF_TRUE, 0);\n\t\tfor (i=0; i<count; i++) {\n\t\t\tif (! isor_declare_item_properties(read, NULL, i+1))\n\t\t\t\tcontinue;\n\n\t\t\tif (read->itt) break;\n\t\t}\n\t} else {\n\t\tif (!tk_found) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] TrackID %s not found in file\\n\", read->tkid ));\n\t\t\treturn GF_BAD_PARAM;\n\t\t}\n\t}\n\tif (! gf_list_count(read->channels)) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] No suitable tracks in file\\n\"));\n\t\treturn GF_NOT_SUPPORTED;\n\t}\n\t\n\t/*if cover art, declare a video pid*/\n\tif (gf_isom_apple_get_tag(read->mov, GF_ISOM_ITUNE_COVER_ART, &tag, &tlen)==GF_OK) {\n\n\t\t/*write cover data*/\n\t\tassert(!(tlen & 0x80000000));\n\t\ttlen &= 0x7FFFFFFF;\n\n\t\tif (read->expart && !isom_contains_video) {\n\t\t\tGF_FilterPid *cover_pid=NULL;\n\t\t\te = gf_filter_pid_raw_new(read->filter, NULL, NULL, NULL, NULL, (char *) tag, tlen, GF_FALSE, &cover_pid);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] error setting up video pid for cover art: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t\tif (cover_pid) {\n\t\t\t\tu8 *out_buffer;\n\t\t\t\tGF_FilterPacket *dst_pck;\n\t\t\t\tgf_filter_pid_set_property(cover_pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_FILE) );\n\t\t\t\tgf_filter_pid_set_name(cover_pid, \"CoverArt\");\n\t\t\t\tdst_pck = gf_filter_pck_new_alloc(cover_pid, tlen, &out_buffer);\n\t\t\t\tif (dst_pck) {\n\t\t\t\t\tgf_filter_pck_set_framing(dst_pck, GF_TRUE, GF_TRUE);\n\t\t\t\t\tmemcpy(out_buffer, tag, tlen);\n\t\t\t\t\tgf_filter_pck_send(dst_pck);\n\t\t\t\t}\n\t\t\t\tgf_filter_pid_set_eos(cover_pid);\n\t\t\t}\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "func": "GF_Err isor_declare_objects(ISOMReader *read)\n{\n\tconst u8 *tag;\n\tu32 tlen;\n\tu32 i, count, j, track_id;\n\tBool highest_stream;\n\tBool single_media_found = GF_FALSE;\n\tBool use_iod = GF_FALSE;\n\tBool tk_found = GF_FALSE;\n\tGF_Err e;\n\tBool isom_contains_video = GF_FALSE;\n\tGF_Descriptor *od = gf_isom_get_root_od(read->mov);\n\tif (od && gf_list_count(((GF_ObjectDescriptor*)od)->ESDescriptors)) {\n\t\tuse_iod = GF_TRUE;\n\t}\n\tif (od) gf_odf_desc_del(od);\n\n\t/*TODO\n\t check for alternate tracks\n    */\n\tcount = gf_isom_get_track_count(read->mov);\n\tfor (i=0; i<count; i++) {\n\t\tu32 mtype, m_subtype, streamtype, stsd_idx;\n\n\t\tmtype = gf_isom_get_media_type(read->mov, i+1);\n\n\t\tif (read->tkid) {\n\t\t\tu32 for_id=0;\n\t\t\tif (sscanf(read->tkid, \"%d\", &for_id)) {\n\t\t\t\tu32 id = gf_isom_get_track_id(read->mov, i+1);\n\t\t\t\tif (id != for_id) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"audio\")) {\n\t\t\t\tif (mtype!=GF_ISOM_MEDIA_AUDIO) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"video\")) {\n\t\t\t\tif (mtype!=GF_ISOM_MEDIA_VISUAL) continue;\n\t\t\t} else if (!strcmp(read->tkid, \"text\")) {\n\t\t\t\tif ((mtype!=GF_ISOM_MEDIA_TEXT) && (mtype!=GF_ISOM_MEDIA_SUBT) && (mtype!=GF_ISOM_MEDIA_MPEG_SUBT)) continue;\n\t\t\t} else if (strlen(read->tkid)==4) {\n\t\t\t\tu32 t = GF_4CC(read->tkid[0], read->tkid[1], read->tkid[2], read->tkid[3]);\n\t\t\t\tif (mtype!=t) continue;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] Bad format for tkid option %s, no match\\n\", read->tkid));\n\t\t\t\treturn GF_BAD_PARAM;\n\t\t\t}\n\t\t\ttk_found = GF_TRUE;\n\t\t}\n\n\t\tswitch (mtype) {\n\t\tcase GF_ISOM_MEDIA_AUDIO:\n\t\t\tstreamtype = GF_STREAM_AUDIO;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_VISUAL:\n\t\tcase GF_ISOM_MEDIA_AUXV:\n\t\tcase GF_ISOM_MEDIA_PICT:\n\t\tcase GF_ISOM_MEDIA_QTVR:\n\t\t\tstreamtype = GF_STREAM_VISUAL;\n\t\t\tisom_contains_video = GF_TRUE;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_TEXT:\n\t\tcase GF_ISOM_MEDIA_SUBT:\n\t\tcase GF_ISOM_MEDIA_SUBPIC:\n\t\tcase GF_ISOM_MEDIA_MPEG_SUBT:\n\t\tcase GF_ISOM_MEDIA_CLOSED_CAPTION:\n\t\t\tstreamtype = GF_STREAM_TEXT;\n\t\t\tmtype = GF_ISOM_MEDIA_TEXT;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_FLASH:\n\t\tcase GF_ISOM_MEDIA_DIMS:\n\t\tcase GF_ISOM_MEDIA_SCENE:\n\t\t\tstreamtype = GF_STREAM_SCENE;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_OD:\n\t\t\tstreamtype = GF_STREAM_OD;\n\t\t\tbreak;\n\t\tcase GF_ISOM_MEDIA_META:\n\t\tcase GF_ISOM_MEDIA_TIMECODE:\n\t\t\tstreamtype = GF_STREAM_METADATA;\n\t\t\tbreak;\n\t\t/*hint tracks are never exported*/\n\t\tcase GF_ISOM_MEDIA_HINT:\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tif (!read->allt) {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d type %s not supported, ignoring track - you may retry by specifying allt option\\n\", i+1, gf_4cc_to_str(mtype) ));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstreamtype = GF_STREAM_UNKNOWN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!read->alltk && !read->tkid && !gf_isom_is_track_enabled(read->mov, i+1)) {\n\t\t\tif (count>1) {\n\t\t\t\tu32 type = gf_isom_get_media_type(read->mov, i+1);\n\n\t\t\t\t//we don't warn for disabled text tracks due to chapters and forced subs\n\t\t\t\tif ((type==GF_ISOM_SUBTYPE_TEXT) || (type==GF_ISOM_MEDIA_SUBT))\n\t\t\t\t\tcontinue;\n\t\t\t\t//disabled tracks using QT chapter refs, do not warn\n\t\t\t\tif (gf_isom_is_track_referenced(read->mov, i+1, GF_ISOM_REF_CHAP)) continue;\n\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d is disabled, ignoring track - you may retry by specifying alltk option\\n\", i+1));\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[IsoMedia] Track %d is disabled but single track in file, considering it enabled\\n\", i+1 ));\n\t\t\t}\n\t\t}\n\n\t\tstsd_idx = read->stsd ? read->stsd : 1;\n\t\t//some subtypes are not declared as readable objects\n\t\tm_subtype = gf_isom_get_media_subtype(read->mov, i+1, stsd_idx);\n\t\tswitch (m_subtype) {\n\t\tcase GF_ISOM_SUBTYPE_HVT1:\n\t\t\tif (read->smode == MP4DMX_SINGLE)\n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t/*we declare only the highest video track (i.e the track we play)*/\n\t\thighest_stream = GF_TRUE;\n\t\ttrack_id = gf_isom_get_track_id(read->mov, i+1);\n\t\tif (read->play_only_track_id && (read->play_only_track_id != track_id)) continue;\n\n\t\tif (read->play_only_first_media) {\n\t\t\tif (read->play_only_first_media != mtype) continue;\n\t\t\tif (single_media_found) continue;\n\t\t\tsingle_media_found = GF_TRUE;\n\t\t}\n\n\t\tfor (j = 0; j < count; j++) {\n\t\t\tif (gf_isom_has_track_reference(read->mov, j+1, GF_ISOM_REF_SCAL, track_id) > 0) {\n\t\t\t\thighest_stream = GF_FALSE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (gf_isom_has_track_reference(read->mov, j+1, GF_ISOM_REF_BASE, track_id) > 0) {\n\t\t\t\thighest_stream = GF_FALSE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif ((read->smode==MP4DMX_SINGLE) && (gf_isom_get_media_type(read->mov, i+1) == GF_ISOM_MEDIA_VISUAL) && !highest_stream)\n\t\t\tcontinue;\n\n\n\t\tisor_declare_track(read, NULL, i+1, stsd_idx, streamtype, use_iod);\n\n\t\tif (read->tkid)\n\t\t\tbreak;\n\t}\n\n\tif (!read->tkid) {\n\t\t/*declare image items*/\n\t\tcount = gf_isom_get_meta_item_count(read->mov, GF_TRUE, 0);\n\t\tfor (i=0; i<count; i++) {\n\t\t\tif (! isor_declare_item_properties(read, NULL, i+1))\n\t\t\t\tcontinue;\n\n\t\t\tif (read->itt) break;\n\t\t}\n\t} else {\n\t\tif (!tk_found) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] TrackID %s not found in file\\n\", read->tkid ));\n\t\t\treturn GF_BAD_PARAM;\n\t\t}\n\t}\n\tif (! gf_list_count(read->channels)) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] No suitable tracks in file\\n\"));\n\t\treturn GF_NOT_SUPPORTED;\n\t}\n\n\t/*if cover art, declare a video pid*/\n\tif (gf_isom_apple_get_tag(read->mov, GF_ISOM_ITUNE_COVER_ART, &tag, &tlen)==GF_OK) {\n\n\t\t/*write cover data*/\n\t\tassert(!(tlen & 0x80000000));\n\t\ttlen &= 0x7FFFFFFF;\n\n\t\tif (read->expart && !isom_contains_video) {\n\t\t\tGF_FilterPid *cover_pid=NULL;\n\t\t\te = gf_filter_pid_raw_new(read->filter, NULL, NULL, NULL, NULL, (char *) tag, tlen, GF_FALSE, &cover_pid);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] error setting up video pid for cover art: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t\tif (cover_pid) {\n\t\t\t\tu8 *out_buffer;\n\t\t\t\tGF_FilterPacket *dst_pck;\n\t\t\t\tgf_filter_pid_set_property(cover_pid, GF_PROP_PID_STREAM_TYPE, &PROP_UINT(GF_STREAM_FILE) );\n\t\t\t\tgf_filter_pid_set_name(cover_pid, \"CoverArt\");\n\t\t\t\tdst_pck = gf_filter_pck_new_alloc(cover_pid, tlen, &out_buffer);\n\t\t\t\tif (dst_pck) {\n\t\t\t\t\tgf_filter_pck_set_framing(dst_pck, GF_TRUE, GF_TRUE);\n\t\t\t\t\tmemcpy(out_buffer, tag, tlen);\n\t\t\t\t\tgf_filter_pck_send(dst_pck);\n\t\t\t\t}\n\t\t\t\tgf_filter_pid_set_eos(cover_pid);\n\t\t\t}\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -169,7 +169,7 @@\n \t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[IsoMedia] No suitable tracks in file\\n\"));\n \t\treturn GF_NOT_SUPPORTED;\n \t}\n-\t\n+\n \t/*if cover art, declare a video pid*/\n \tif (gf_isom_apple_get_tag(read->mov, GF_ISOM_ITUNE_COVER_ART, &tag, &tlen)==GF_OK) {\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/mp4_mux_done",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "static GF_Err mp4_mux_done(GF_MP4MuxCtx *ctx, Bool is_final)\n{\n\tGF_Err e = GF_OK;\n\tu32 i, count;\n\tGF_PropertyEntry *pe=NULL;\n\n\tcount = gf_list_count(ctx->tracks);\n\tfor (i=0; i<count; i++) {\n\t\tu32 ctts_mode = ctx->ctmode;\n\t\tconst GF_PropertyValue *p;\n\t\tBool has_bframes = GF_FALSE;\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ISOM_FORCE_NEGCTTS);\n\t\tif (p && p->value.boolean) ctts_mode = MP4MX_CT_NEGCTTS;\n\n\t\tif (tkw->min_neg_ctts<0) {\n\t\t\t//use ctts v1 negative offsets\n\t\t\tif (ctts_mode==MP4MX_CT_NEGCTTS) {\n\t\t\t\tgf_isom_set_ctts_v1(ctx->file, tkw->track_num, (u32) -tkw->min_neg_ctts);\n\t\t\t}\n\t\t\t//ctts v0\n\t\t\telse {\n\t\t\t\tgf_isom_set_cts_packing(ctx->file, tkw->track_num, GF_TRUE);\n\t\t\t\tgf_isom_shift_cts_offset(ctx->file, tkw->track_num, (s32) tkw->min_neg_ctts);\n\t\t\t\tgf_isom_set_cts_packing(ctx->file, tkw->track_num, GF_FALSE);\n\t\t\t\tgf_isom_set_composition_offset_mode(ctx->file, tkw->track_num, GF_FALSE);\n\n\t\t\t\tmp4_mux_update_edit_list_for_bframes(ctx, tkw, ctts_mode);\n\t\t\t}\n\t\t\thas_bframes = GF_TRUE;\n\t\t} else if (tkw->has_ctts && (tkw->stream_type==GF_STREAM_VISUAL)) {\n\t\t\tmp4_mux_update_edit_list_for_bframes(ctx, tkw, ctts_mode);\n\n\t\t\thas_bframes = GF_TRUE;\n\t\t} else if (tkw->ts_delay || tkw->empty_init_dur) {\n\t\t\tgf_isom_update_edit_list_duration(ctx->file, tkw->track_num);\n\t\t}\n\n\t\tif (tkw->min_ts_seek_plus_one) {\n\t\t\tu64 min_ts = tkw->min_ts_seek_plus_one - 1;\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\t\t\tu32 delay = 0;\n\t\t\tif (tkw->clamp_ts_plus_one) {\n\t\t\t\tmdur = tkw->max_cts - tkw->min_cts;\n\t\t\t\tmdur += tkw->max_cts_samp_dur;\n\t\t\t}\n\t\t\tif (mdur > min_ts)\n\t\t\t\tmdur -= min_ts;\n\t\t\telse\n\t\t\t\tmdur = 0;\n\n\t\t\tif ((ctts_mode != MP4MX_CT_NEGCTTS) && (tkw->ts_delay<0) && (tkw->stream_type==GF_STREAM_VISUAL)) {\n\t\t\t\tdelay = (u32) -tkw->ts_delay;\n\t\t\t}\n\n\t\t\tif (tkw->src_timescale != tkw->tk_timescale) {\n\t\t\t\tmin_ts = gf_timestamp_rescale(min_ts, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t\tdelay = (u32) gf_timestamp_rescale(delay, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t}\n\t\t\tmdur += delay;\n\n\t\t\tif (ctx->moovts != tkw->tk_timescale) {\n\t\t\t\tmdur = gf_timestamp_rescale(mdur, tkw->tk_timescale, ctx->moovts);\n\t\t\t}\n\t\t\tgf_isom_remove_edits(ctx->file, tkw->track_num);\n\t\t\tif (tkw->empty_init_dur)\n\t\t\t\tgf_isom_set_edit(ctx->file, tkw->track_num, 0, tkw->empty_init_dur, 0, GF_ISOM_EDIT_EMPTY);\n\t\t\tgf_isom_set_edit(ctx->file, tkw->track_num, tkw->empty_init_dur, mdur, min_ts, GF_ISOM_EDIT_NORMAL);\n\t\t}\n\n\t\tif (tkw->force_ctts) {\n\t\t\tGF_Err gf_isom_force_ctts(GF_ISOFile *file, u32 track);\n\t\t\tgf_isom_force_ctts(ctx->file, tkw->track_num);\n\t\t}\n\n\t\tgf_isom_purge_track_reference(ctx->file, tkw->track_num);\n\t\t\n\t\tif (ctx->importer && ctx->dur.num && ctx->dur.den) {\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\t\t\tu64 pdur = gf_isom_get_track_duration(ctx->file, tkw->track_num);\n\t\t\tif (pdur==mdur) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Imported %d frames - duration %g\\n\", tkw->nb_samples, ((Double)mdur)/tkw->tk_timescale ));\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Imported %d frames - media duration %g - track duration %g\\n\", tkw->nb_samples, ((Double)mdur)/tkw->tk_timescale, ((Double)pdur)/ctx->moovts ));\n\t\t\t}\n\t\t}\n\n\t\t/*this is plain ugly but since some encoders (divx) don't use the video PL correctly\n\t\t we force the system video_pl to ASP@L5 since we have I, P, B in base layer*/\n\t\tif (tkw->codecid == GF_CODECID_MPEG4_PART2) {\n\t\t\tBool force_rewrite = GF_FALSE;\n\t\t\tu32 PL = tkw->media_profile_level;\n\t\t\tif (!PL) PL = 0x01;\n\n\t\t\tif (ctx->importer) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"Indicated Profile: %s\\n\", gf_m4v_get_profile_name((u8) PL) ));\n\t\t\t}\n\n\t\t\tif (has_bframes && (tkw->media_profile_level <= 3)) {\n\t\t\t\tPL = 0xF5;\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] Indicated profile doesn't include B-VOPs - forcing %s\\n\", gf_m4v_get_profile_name((u8) PL) ));\n\t\t\t\tforce_rewrite = GF_TRUE;\n\t\t\t}\n\t\t\tif (PL != tkw->media_profile_level) {\n\t\t\t\tif (force_rewrite) {\n#ifndef GPAC_DISABLE_AV_PARSERS\n\t\t\t\t\tGF_ESD *esd = gf_isom_get_esd(ctx->file, tkw->track_num, tkw->stsd_idx);\n\t\t\t\t\tassert(esd);\n\t\t\t\t\tgf_m4v_rewrite_pl(&esd->decoderConfig->decoderSpecificInfo->data, &esd->decoderConfig->decoderSpecificInfo->dataLength, (u8) PL);\n\t\t\t\t\tgf_isom_change_mpeg4_description(ctx->file, tkw->track_num, tkw->stsd_idx, esd);\n\t\t\t\t\tgf_odf_desc_del((GF_Descriptor*)esd);\n#endif\n\n\t\t\t\t}\n\t\t\t\tif (!ctx->make_qt)\n\t\t\t\t\tgf_isom_set_pl_indication(ctx->file, GF_ISOM_PL_VISUAL, PL);\n\t\t\t}\n\t\t}\n\n\n\t\tif (tkw->has_append)\n\t\t\tgf_isom_refresh_size_info(ctx->file, tkw->track_num);\n\n\t\tif ((tkw->nb_samples == 1) && (ctx->dur.num>0) && ctx->dur.den) {\n\t\t\tu32 dur = (u32) gf_timestamp_rescale(ctx->dur.num, ctx->dur.den, tkw->tk_timescale);\n\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, dur);\n\t\t}\n\n\t\tif (tkw->has_open_gop) {\n\t\t\tif (ctx->importer) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"OpenGOP detected - adjusting file brand\\n\"));\n\t\t\t}\n\t\t\tgf_isom_modify_alternate_brand(ctx->file, GF_ISOM_BRAND_ISO6, GF_TRUE);\n\t\t}\n\n\t\tmp4_mux_set_hevc_groups(ctx, tkw);\n\n\t\tp = gf_filter_pid_get_info_str(tkw->ipid, \"ttxt:rem_last\", &pe);\n\t\tif (p && p->value.boolean)\n\t\t\tgf_isom_remove_sample(ctx->file, tkw->track_num, tkw->nb_samples);\n\n\t\tp = gf_filter_pid_get_info_str(tkw->ipid, \"ttxt:last_dur\", &pe);\n\t\tif (p) {\n\t\t\tu64 val = p->value.uint;\n\t\t\tif (tkw->src_timescale != tkw->tk_timescale) {\n\t\t\t\tval = gf_timestamp_rescale(val, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t}\n\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, (u32) val);\n\t\t}\n\t\tp = gf_filter_pid_get_info(tkw->ipid, GF_PROP_PID_FORCED_SUB, &pe);\n\t\tif (p) {\n\t\t\tgf_isom_set_forced_text(ctx->file, tkw->track_num, tkw->stsd_idx, p->value.uint);\n\t\t}\n\n\t\tif (tkw->is_nalu && ctx->pack_nal && (gf_isom_get_mode(ctx->file)!=GF_ISOM_OPEN_WRITE)) {\n\t\t\tu32 msize = 0;\n\t\t\tBool do_rewrite = GF_FALSE;\n\t\t\tu32 j, stsd_count = gf_isom_get_sample_description_count(ctx->file, tkw->track_num);\n\t\t\tp = gf_filter_pid_get_info(tkw->ipid, GF_PROP_PID_MAX_NALU_SIZE, &pe);\n\t\t\tmsize = gf_get_bit_size(p->value.uint);\n\t\t\tif (msize<8) msize = 8;\n\t\t\telse if (msize<16) msize = 16;\n\t\t\telse msize = 32;\n\n\t\t\tif (msize<=0xFFFF) {\n\t\t\t\tfor (j=0; j<stsd_count; j++) {\n\t\t\t\t\tu32 k = 8 * gf_isom_get_nalu_length_field(ctx->file, tkw->track_num, j+1);\n\t\t\t\t\tif (k > msize) {\n\t\t\t\t\t\tdo_rewrite = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (do_rewrite) {\n\t\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Adjusting NALU SizeLength to %d bits\\n\", msize ));\n\t\t\t\t\tgf_media_nal_rewrite_samples(ctx->file, tkw->track_num, msize);\n\t\t\t\t\tmsize /= 8;\n\t\t\t\t\tfor (j=0; j<stsd_count; j++) {\n\t\t\t\t\t\tgf_isom_set_nalu_length_field(ctx->file, tkw->track_num, j+1, msize);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t//don't update bitrate info for single sample tracks, unless MPEG-4 Systems - compatibility with old arch\n\t\tif (ctx->btrt && !tkw->skip_bitrate_update && ((tkw->nb_samples>1) || ctx->m4sys) )\n\t\t\tgf_media_update_bitrate(ctx->file, tkw->track_num);\n\n\t\tif (!tkw->box_patched) {\n\t\t\tp = gf_filter_pid_get_property_str(tkw->ipid, \"boxpatch\");\n\t\t\tif (p && p->value.string) {\n\t\t\t\te = gf_isom_apply_box_patch(ctx->file, tkw->track_id ? tkw->track_id : tkw->item_id, p->value.string, GF_FALSE);\n\t\t\t\tif (e) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Unable to apply box patch %s to track %d: %s\\n\",\n\t\t\t\t\t\tp->value.string, tkw->track_id, gf_error_to_string(e) ));\n\t\t\t\t}\n\t\t\t}\n\t\t\ttkw->box_patched = GF_TRUE;\n\t\t}\n\t}\n\n\tgf_filter_release_property(pe);\n\n\tif (ctx->boxpatch && !ctx->box_patched) {\n\t\te = gf_isom_apply_box_patch(ctx->file, 0, ctx->boxpatch, GF_FALSE);\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Unable to apply box patch %s: %s\\n\", ctx->boxpatch, gf_error_to_string(e) ));\n\t\t}\n\t\tctx->box_patched = GF_TRUE;\n\t}\n\n\n\tif (ctx->owns_mov) {\n\t\tif (ctx->moovpad)\n\t\t\tgf_isom_set_inplace_padding(ctx->file, ctx->moovpad);\n\n\t\tswitch (ctx->store) {\n\t\tcase MP4MX_MODE_INTER:\n\t\t\tif (ctx->cdur.num==0) {\n\t\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_STREAMABLE);\n\t\t\t} else {\n\t\t\t\tif (ctx->cdur.num < 0) ctx->cdur.num = 1000;\n\t\t\t\te = gf_isom_make_interleave_ex(ctx->file, &ctx->cdur);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_FLAT:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_FLAT);\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_FASTSTART:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_FASTSTART);\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_TIGHT:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_TIGHT);\n\t\t\tbreak;\n\t\t}\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set storage mode: %s\\n\", gf_error_to_string(e) ));\n\t\t\tgf_isom_delete(ctx->file);\n\t\t} else {\n\t\t\te = gf_isom_close(ctx->file);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to write file: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t\tctx->file = NULL;\n\t\tif (is_final)\n\t\t\tgf_filter_pid_set_eos(ctx->opid);\n\t} else {\n\t\tctx->file = NULL;\n\t}\n\treturn e;\n}",
        "func": "static GF_Err mp4_mux_done(GF_MP4MuxCtx *ctx, Bool is_final)\n{\n\tGF_Err e = GF_OK;\n\tu32 i, count;\n\tGF_PropertyEntry *pe=NULL;\n\n\tcount = gf_list_count(ctx->tracks);\n\tfor (i=0; i<count; i++) {\n\t\tu32 ctts_mode = ctx->ctmode;\n\t\tconst GF_PropertyValue *p;\n\t\tBool has_bframes = GF_FALSE;\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ISOM_FORCE_NEGCTTS);\n\t\tif (p && p->value.boolean) ctts_mode = MP4MX_CT_NEGCTTS;\n\n\t\tif (tkw->min_neg_ctts<0) {\n\t\t\t//use ctts v1 negative offsets\n\t\t\tif (ctts_mode==MP4MX_CT_NEGCTTS) {\n\t\t\t\tgf_isom_set_ctts_v1(ctx->file, tkw->track_num, (u32) -tkw->min_neg_ctts);\n\t\t\t}\n\t\t\t//ctts v0\n\t\t\telse {\n\t\t\t\tgf_isom_set_cts_packing(ctx->file, tkw->track_num, GF_TRUE);\n\t\t\t\tgf_isom_shift_cts_offset(ctx->file, tkw->track_num, (s32) tkw->min_neg_ctts);\n\t\t\t\tgf_isom_set_cts_packing(ctx->file, tkw->track_num, GF_FALSE);\n\t\t\t\tgf_isom_set_composition_offset_mode(ctx->file, tkw->track_num, GF_FALSE);\n\n\t\t\t\tmp4_mux_update_edit_list_for_bframes(ctx, tkw, ctts_mode);\n\t\t\t}\n\t\t\thas_bframes = GF_TRUE;\n\t\t} else if (tkw->has_ctts && (tkw->stream_type==GF_STREAM_VISUAL)) {\n\t\t\tmp4_mux_update_edit_list_for_bframes(ctx, tkw, ctts_mode);\n\n\t\t\thas_bframes = GF_TRUE;\n\t\t} else if (tkw->ts_delay || tkw->empty_init_dur) {\n\t\t\tgf_isom_update_edit_list_duration(ctx->file, tkw->track_num);\n\t\t}\n\n\t\tif (tkw->min_ts_seek_plus_one) {\n\t\t\tu64 min_ts = tkw->min_ts_seek_plus_one - 1;\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\t\t\tu32 delay = 0;\n\t\t\tif (tkw->clamp_ts_plus_one) {\n\t\t\t\tmdur = tkw->max_cts - tkw->min_cts;\n\t\t\t\tmdur += tkw->max_cts_samp_dur;\n\t\t\t}\n\t\t\tif (mdur > min_ts)\n\t\t\t\tmdur -= min_ts;\n\t\t\telse\n\t\t\t\tmdur = 0;\n\n\t\t\tif ((ctts_mode != MP4MX_CT_NEGCTTS) && (tkw->ts_delay<0) && (tkw->stream_type==GF_STREAM_VISUAL)) {\n\t\t\t\tdelay = (u32) -tkw->ts_delay;\n\t\t\t}\n\n\t\t\tif (tkw->src_timescale != tkw->tk_timescale) {\n\t\t\t\tmin_ts = gf_timestamp_rescale(min_ts, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t\tdelay = (u32) gf_timestamp_rescale(delay, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t}\n\t\t\tmdur += delay;\n\n\t\t\tif (ctx->moovts != tkw->tk_timescale) {\n\t\t\t\tmdur = gf_timestamp_rescale(mdur, tkw->tk_timescale, ctx->moovts);\n\t\t\t}\n\t\t\tgf_isom_remove_edits(ctx->file, tkw->track_num);\n\t\t\tif (tkw->empty_init_dur)\n\t\t\t\tgf_isom_set_edit(ctx->file, tkw->track_num, 0, tkw->empty_init_dur, 0, GF_ISOM_EDIT_EMPTY);\n\t\t\tgf_isom_set_edit(ctx->file, tkw->track_num, tkw->empty_init_dur, mdur, min_ts, GF_ISOM_EDIT_NORMAL);\n\t\t}\n\n\t\tif (tkw->force_ctts) {\n\t\t\tGF_Err gf_isom_force_ctts(GF_ISOFile *file, u32 track);\n\t\t\tgf_isom_force_ctts(ctx->file, tkw->track_num);\n\t\t}\n\n\t\tgf_isom_purge_track_reference(ctx->file, tkw->track_num);\n\n\t\tif (ctx->importer && ctx->dur.num && ctx->dur.den) {\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\t\t\tu64 pdur = gf_isom_get_track_duration(ctx->file, tkw->track_num);\n\t\t\tif (pdur==mdur) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Imported %d frames - duration %g\\n\", tkw->nb_samples, ((Double)mdur)/tkw->tk_timescale ));\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Imported %d frames - media duration %g - track duration %g\\n\", tkw->nb_samples, ((Double)mdur)/tkw->tk_timescale, ((Double)pdur)/ctx->moovts ));\n\t\t\t}\n\t\t}\n\n\t\t/*this is plain ugly but since some encoders (divx) don't use the video PL correctly\n\t\t we force the system video_pl to ASP@L5 since we have I, P, B in base layer*/\n\t\tif (tkw->codecid == GF_CODECID_MPEG4_PART2) {\n\t\t\tBool force_rewrite = GF_FALSE;\n\t\t\tu32 PL = tkw->media_profile_level;\n\t\t\tif (!PL) PL = 0x01;\n\n\t\t\tif (ctx->importer) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"Indicated Profile: %s\\n\", gf_m4v_get_profile_name((u8) PL) ));\n\t\t\t}\n\n\t\t\tif (has_bframes && (tkw->media_profile_level <= 3)) {\n\t\t\t\tPL = 0xF5;\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] Indicated profile doesn't include B-VOPs - forcing %s\\n\", gf_m4v_get_profile_name((u8) PL) ));\n\t\t\t\tforce_rewrite = GF_TRUE;\n\t\t\t}\n\t\t\tif (PL != tkw->media_profile_level) {\n\t\t\t\tif (force_rewrite) {\n#ifndef GPAC_DISABLE_AV_PARSERS\n\t\t\t\t\tGF_ESD *esd = gf_isom_get_esd(ctx->file, tkw->track_num, tkw->stsd_idx);\n\t\t\t\t\tassert(esd);\n\t\t\t\t\tgf_m4v_rewrite_pl(&esd->decoderConfig->decoderSpecificInfo->data, &esd->decoderConfig->decoderSpecificInfo->dataLength, (u8) PL);\n\t\t\t\t\tgf_isom_change_mpeg4_description(ctx->file, tkw->track_num, tkw->stsd_idx, esd);\n\t\t\t\t\tgf_odf_desc_del((GF_Descriptor*)esd);\n#endif\n\n\t\t\t\t}\n\t\t\t\tif (!ctx->make_qt)\n\t\t\t\t\tgf_isom_set_pl_indication(ctx->file, GF_ISOM_PL_VISUAL, PL);\n\t\t\t}\n\t\t}\n\n\n\t\tif (tkw->has_append)\n\t\t\tgf_isom_refresh_size_info(ctx->file, tkw->track_num);\n\n\t\tif ((tkw->nb_samples == 1) && (ctx->dur.num>0) && ctx->dur.den) {\n\t\t\tu32 dur = (u32) gf_timestamp_rescale(ctx->dur.num, ctx->dur.den, tkw->tk_timescale);\n\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, dur);\n\t\t}\n\n\t\tif (tkw->has_open_gop) {\n\t\t\tif (ctx->importer) {\n\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"OpenGOP detected - adjusting file brand\\n\"));\n\t\t\t}\n\t\t\tgf_isom_modify_alternate_brand(ctx->file, GF_ISOM_BRAND_ISO6, GF_TRUE);\n\t\t}\n\n\t\tmp4_mux_set_hevc_groups(ctx, tkw);\n\n\t\tp = gf_filter_pid_get_info_str(tkw->ipid, \"ttxt:rem_last\", &pe);\n\t\tif (p && p->value.boolean)\n\t\t\tgf_isom_remove_sample(ctx->file, tkw->track_num, tkw->nb_samples);\n\n\t\tp = gf_filter_pid_get_info_str(tkw->ipid, \"ttxt:last_dur\", &pe);\n\t\tif (p) {\n\t\t\tu64 val = p->value.uint;\n\t\t\tif (tkw->src_timescale != tkw->tk_timescale) {\n\t\t\t\tval = gf_timestamp_rescale(val, tkw->src_timescale, tkw->tk_timescale);\n\t\t\t}\n\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, (u32) val);\n\t\t}\n\t\tp = gf_filter_pid_get_info(tkw->ipid, GF_PROP_PID_FORCED_SUB, &pe);\n\t\tif (p) {\n\t\t\tgf_isom_set_forced_text(ctx->file, tkw->track_num, tkw->stsd_idx, p->value.uint);\n\t\t}\n\n\t\tif (tkw->is_nalu && ctx->pack_nal && (gf_isom_get_mode(ctx->file)!=GF_ISOM_OPEN_WRITE)) {\n\t\t\tu32 msize = 0;\n\t\t\tBool do_rewrite = GF_FALSE;\n\t\t\tu32 j, stsd_count = gf_isom_get_sample_description_count(ctx->file, tkw->track_num);\n\t\t\tp = gf_filter_pid_get_info(tkw->ipid, GF_PROP_PID_MAX_NALU_SIZE, &pe);\n\t\t\tmsize = gf_get_bit_size(p->value.uint);\n\t\t\tif (msize<8) msize = 8;\n\t\t\telse if (msize<16) msize = 16;\n\t\t\telse msize = 32;\n\n\t\t\tif (msize<=0xFFFF) {\n\t\t\t\tfor (j=0; j<stsd_count; j++) {\n\t\t\t\t\tu32 k = 8 * gf_isom_get_nalu_length_field(ctx->file, tkw->track_num, j+1);\n\t\t\t\t\tif (k > msize) {\n\t\t\t\t\t\tdo_rewrite = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (do_rewrite) {\n\t\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_MEDIA, (\"[MP4Mux] Adjusting NALU SizeLength to %d bits\\n\", msize ));\n\t\t\t\t\tgf_media_nal_rewrite_samples(ctx->file, tkw->track_num, msize);\n\t\t\t\t\tmsize /= 8;\n\t\t\t\t\tfor (j=0; j<stsd_count; j++) {\n\t\t\t\t\t\tgf_isom_set_nalu_length_field(ctx->file, tkw->track_num, j+1, msize);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t//don't update bitrate info for single sample tracks, unless MPEG-4 Systems - compatibility with old arch\n\t\tif (ctx->btrt && !tkw->skip_bitrate_update && ((tkw->nb_samples>1) || ctx->m4sys) )\n\t\t\tgf_media_update_bitrate(ctx->file, tkw->track_num);\n\n\t\tif (!tkw->box_patched) {\n\t\t\tp = gf_filter_pid_get_property_str(tkw->ipid, \"boxpatch\");\n\t\t\tif (p && p->value.string) {\n\t\t\t\te = gf_isom_apply_box_patch(ctx->file, tkw->track_id ? tkw->track_id : tkw->item_id, p->value.string, GF_FALSE);\n\t\t\t\tif (e) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Unable to apply box patch %s to track %d: %s\\n\",\n\t\t\t\t\t\tp->value.string, tkw->track_id, gf_error_to_string(e) ));\n\t\t\t\t}\n\t\t\t}\n\t\t\ttkw->box_patched = GF_TRUE;\n\t\t}\n\t}\n\n\tgf_filter_release_property(pe);\n\n\tif (ctx->boxpatch && !ctx->box_patched) {\n\t\te = gf_isom_apply_box_patch(ctx->file, 0, ctx->boxpatch, GF_FALSE);\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Unable to apply box patch %s: %s\\n\", ctx->boxpatch, gf_error_to_string(e) ));\n\t\t}\n\t\tctx->box_patched = GF_TRUE;\n\t}\n\n\n\tif (ctx->owns_mov) {\n\t\tif (ctx->moovpad)\n\t\t\tgf_isom_set_inplace_padding(ctx->file, ctx->moovpad);\n\n\t\tswitch (ctx->store) {\n\t\tcase MP4MX_MODE_INTER:\n\t\t\tif (ctx->cdur.num==0) {\n\t\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_STREAMABLE);\n\t\t\t} else {\n\t\t\t\tif (ctx->cdur.num < 0) ctx->cdur.num = 1000;\n\t\t\t\te = gf_isom_make_interleave_ex(ctx->file, &ctx->cdur);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_FLAT:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_FLAT);\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_FASTSTART:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_FASTSTART);\n\t\t\tbreak;\n\t\tcase MP4MX_MODE_TIGHT:\n\t\t\te = gf_isom_set_storage_mode(ctx->file, GF_ISOM_STORE_TIGHT);\n\t\t\tbreak;\n\t\t}\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set storage mode: %s\\n\", gf_error_to_string(e) ));\n\t\t\tgf_isom_delete(ctx->file);\n\t\t} else {\n\t\t\te = gf_isom_close(ctx->file);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to write file: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t\tctx->file = NULL;\n\t\tif (is_final)\n\t\t\tgf_filter_pid_set_eos(ctx->opid);\n\t} else {\n\t\tctx->file = NULL;\n\t}\n\treturn e;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -75,7 +75,7 @@\n \t\t}\n \n \t\tgf_isom_purge_track_reference(ctx->file, tkw->track_num);\n-\t\t\n+\n \t\tif (ctx->importer && ctx->dur.num && ctx->dur.den) {\n \t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n \t\t\tu64 pdur = gf_isom_get_track_duration(ctx->file, tkw->track_num);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/mp4_mux_cenc_update",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "static GF_Err mp4_mux_cenc_update(GF_MP4MuxCtx *ctx, TrackWriter *tkw, GF_FilterPacket *pck, u32 act_type, u32 pck_size, u32 injected_hdr_size)\n{\n\tconst GF_PropertyValue *p;\n\tGF_Err e;\n\tBool pck_is_encrypted;\n\tu32 IV_size=0;\n\tu8 *fake_sai = NULL;\n\tu8 *sai = NULL;\n\tu32 sai_size = 0;\n\tBool needs_seig = GF_FALSE;\n\tu32 sample_num;\n\n\tif (tkw->cenc_state == CENC_SETUP_ERROR)\n\t\treturn GF_SERVICE_ERROR;\n\n\n\tif (pck) {\n\t\tp = gf_filter_pck_get_property(pck, GF_PROP_PCK_CENC_SAI);\n\t\tif (p) {\n\t\t\tsai = p->value.data.ptr;\n\t\t\tsai_size = p->value.data.size;\n\t\t}\n\t}\n\n\n\t//initial setup\n\tif (tkw->cenc_state==CENC_NEED_SETUP) {\n\t\tu32 scheme_type=0;\n\t\tu32 scheme_version=0;\n\t\tu32 cenc_stsd_mode=0;\n\t\tu32 container_type = GF_ISOM_BOX_TYPE_SENC;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_PROTECTION_SCHEME_TYPE);\n\t\tif (p) scheme_type = p->value.uint;\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_PROTECTION_SCHEME_VERSION);\n\t\tif (p) scheme_version = p->value.uint;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_CENC_STSD_MODE);\n\t\tif (p) cenc_stsd_mode = p->value.uint;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ENCRYPTED);\n\t\tif (p) pck_is_encrypted = p->value.boolean;\n\t\telse pck_is_encrypted = GF_FALSE;\n\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_CENC_STORE);\n\t\tif (p && p->value.uint) container_type = p->value.uint;\n\n\t\ttkw->clear_stsd_idx = 0;\n\t\tif (cenc_stsd_mode) {\n\t\t\tu32 clone_stsd_idx;\n\t\t\te = gf_isom_clone_sample_description(ctx->file, tkw->track_num, ctx->file, tkw->track_num, tkw->stsd_idx, NULL, NULL, &clone_stsd_idx);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to clone sample description: %s\\n\", gf_error_to_string(e) ));\n\t\t\t\treturn e;\n\t\t\t}\n\t\t\t//current stsd reused, use clone one for encrypted stsd\n\t\t\tif (tkw->reused_stsd) {\n\t\t\t\ttkw->clear_stsd_idx = tkw->stsd_idx;\n\t\t\t\ttkw->stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t\t//before\n\t\t\telse if (cenc_stsd_mode==1) {\n\t\t\t\ttkw->clear_stsd_idx = tkw->stsd_idx;\n\t\t\t\ttkw->stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t\t//after\n\t\t\telse {\n\t\t\t\ttkw->clear_stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t}\n\t\ttkw->def_crypt_byte_block = tkw->crypt_byte_block;\n\t\ttkw->def_skip_byte_block = tkw->skip_byte_block;\n\n\t\ttkw->cenc_state = CENC_SETUP_DONE;\n\t\ttkw->def_cenc_key_info_crc = tkw->cenc_key_info_crc;\n\t\tif (tkw->cenc_ki) {\n\t\t\te = gf_isom_set_cenc_protection(ctx->file, tkw->track_num, tkw->stsd_idx, scheme_type, scheme_version, pck_is_encrypted, tkw->def_crypt_byte_block, tkw->def_skip_byte_block, tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size);\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Missing CENC Key config, cannot mux\\n\"));\n\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t}\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to setup CENC information: %s\\n\", gf_error_to_string(e) ));\n\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\treturn e;\n\t\t}\n\n\t\t//purge duplicates\n\t\tu32 k, nb_sdesc = gf_isom_get_sample_description_count(ctx->file, tkw->track_num);\n\t\tif (nb_sdesc>2) {\n\t\t\tfor (k=0; k<nb_sdesc; k++) {\n\t\t\t\tif (k+1 == tkw->stsd_idx) continue;\n\n\t\t\t\tif (gf_isom_is_same_sample_description(ctx->file, tkw->track_num, tkw->stsd_idx, ctx->file, tkw->track_num, k+1) ) {\n\t\t\t\t\tgf_isom_remove_stream_description(ctx->file, tkw->track_num, tkw->stsd_idx);\n\t\t\t\t\ttkw->stsd_idx = k+1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((ctx->psshs == MP4MX_PSSH_MOOV) || (ctx->psshs == MP4MX_PSSH_BOTH))\n\t\t\tmp4_mux_cenc_insert_pssh(ctx, tkw, NULL, 0);\n\n\t\tif (!tkw->has_brands && (scheme_type==GF_ISOM_OMADRM_SCHEME))\n\t\t\tgf_isom_modify_alternate_brand(ctx->file, GF_ISOM_BRAND_OPF2, GF_TRUE);\n\n\t\tif (container_type) {\n\t\t\tif (container_type==GF_ISOM_BOX_UUID_PSEC) {\n\t\t\t\te = gf_isom_piff_allocate_storage(ctx->file, tkw->track_num, 0, 0, NULL);\n\t\t\t} else {\n\t\t\t\te = gf_isom_cenc_allocate_storage(ctx->file, tkw->track_num);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to setup CENC storage: %s\\n\", gf_error_to_string(e) ));\n\t\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\t\treturn e;\n\t\t\t}\n\t\t}\n\t}\n\tif (act_type==CENC_CONFIG) return GF_OK;\n\n\tpck_is_encrypted = GF_FALSE;\n\tif (pck)\n\t\tpck_is_encrypted = gf_filter_pck_get_crypt_flags(pck);\n\n\t//!! tkw->nb_samples / tkw->samples_in_frag not yet incremented !!\n\tif (act_type == CENC_ADD_FRAG) {\n\t\tsample_num = tkw->samples_in_frag + 1;\n\n\t\tif (ctx->cmaf) {\n\t\t\tif (!tkw->samples_in_frag) {\n\t\t\t\ttkw->cenc_frag_protected = pck_is_encrypted;\n\t\t\t} else {\n\t\t\t\tif (tkw->cenc_frag_protected != pck_is_encrypted) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] CMAF forbids mixing protected and unprotected samples in a single fragment, please re-encrypt or change target segment duration\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tsample_num = tkw->nb_samples + 1;\n\t}\n\tif (!pck_is_encrypted) {\n\t\tif (tkw->clear_stsd_idx) {\n\t\t\tif (act_type==CENC_ADD_FRAG) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\treturn gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, NULL, 0, GF_FALSE, ctx->saio32, tkw->cenc_multikey);\n#else\n\t\t\t\treturn GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\treturn gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, NULL, 0, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t\t}\n\t\t} else {\n\t\t\tchar dumb_key[20];\n\t\t\tmemset(dumb_key, 0, 20); //dumb key, IV size 0, not protected\n\t\t\te = gf_isom_set_sample_cenc_group(ctx->file, tkw->track_num, sample_num, GF_FALSE, 0, 0, dumb_key, 20);\n\t\t\tIV_size = 0;\n\t\t\ttkw->has_seig = GF_TRUE;\n\t\t}\n\t} else {\n\t\n\t\te = GF_OK;\n\t\t//multikey ALWAYS uses seig\n\t\tif (tkw->cenc_ki->value.data.ptr[0])\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_crypt_byte_block != tkw->crypt_byte_block)\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_skip_byte_block != tkw->skip_byte_block)\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_cenc_key_info_crc != tkw->cenc_key_info_crc)\n\t\t\tneeds_seig = GF_TRUE;\n\n\t\tif (needs_seig) {\n\t\t\te = gf_isom_set_sample_cenc_group(ctx->file, tkw->track_num, sample_num, 1, tkw->crypt_byte_block, tkw->skip_byte_block, tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size);\n\t\t\ttkw->has_seig = GF_TRUE;\n\t\t} else if (tkw->has_seig) {\n\t\t\te = gf_isom_set_sample_cenc_default_group(ctx->file, tkw->track_num, sample_num);\n\t\t}\n\t}\n\tif (e) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample encryption group entry: %s)\\n\", gf_error_to_string(e) ));\n\t\treturn e;\n\t}\n\n\tp = gf_filter_pck_get_property(pck, GF_PROP_PID_CENC_PSSH);\n\tif (p && (p->type == GF_PROP_DATA) && p->value.data.ptr) {\n\t\tif (ctx->store>=MP4MX_MODE_FRAG) {\n\t\t\tmp4_mux_cenc_insert_pssh(ctx, tkw, p, 0);\n\t\t} else {\n\t\t\tgf_isom_set_sample_group_description(ctx->file, tkw->track_num, sample_num, GF_4CC('P','S','S','H'), 0, p->value.data.ptr, p->value.data.size, 0);\n\t\t}\n\t}\n\n\tif (!sai) {\n\t\tif (tkw->constant_IV_size && !tkw->cenc_subsamples)\n\t\t\treturn GF_OK;\n\n\t\tif (IV_size) {\n\t\t\t//generate clear SAI data with a non-0 IV\n\t\t\tu32 olen = pck_size;\n\t\t\tGF_BitStream *bs = gf_bs_new(NULL, 9, GF_BITSTREAM_WRITE);\n\t\t\tif (tkw->cenc_multikey) {\n\t\t\t\tgf_bs_write_u16(bs, 0);\n\t\t\t} else {\n\t\t\t\tgf_bs_write_long_int(bs, 0, IV_size*8);\n\t\t\t}\n\n\t\t\tif (tkw->cenc_subsamples) {\n\t\t\t\tu32 i;\n\t\t\t\tu32 subsample_count = 1;\n\t\t\t\twhile (olen>0xFFFF) {\n\t\t\t\t\tolen -= 0xFFFF;\n\t\t\t\t\tsubsample_count ++;\n\t\t\t\t}\n\t\t\t\tgf_bs_write_u16(bs, subsample_count);\n\t\t\t\tolen = pck_size;\n\t\t\t\tfor (i = 0; i < subsample_count; i++) {\n\t\t\t\t\tu32 clear_size;\n\t\t\t\t\tif (olen<0xFFFF) {\n\t\t\t\t\t\tclear_size = olen;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tclear_size = 0xFFFF;\n\t\t\t\t\t\tolen -= 0xFFFF;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (tkw->cenc_multikey)\n\t\t\t\t\t\tgf_bs_write_u16(bs, 0);\n\t\t\t\t\tgf_bs_write_u16(bs, clear_size);\n\t\t\t\t\tgf_bs_write_u32(bs, 0);\n\t\t\t\t}\n\t\t\t}\n\t\t\tgf_bs_get_content(bs, &fake_sai, &sai_size);\n\t\t\tgf_bs_del(bs);\n\t\t\tsai = fake_sai;\n\t\t}\n\t}\n\t//we injected xPS at the beginning of the sample (AVC/HEVC/VVC), we must patch the first subsample\n\t//of SAI data\n\telse if (injected_hdr_size) {\n\t\tu32 offset = 0;\n\t\tu32 first_sub_clear, sub_count_size;\n\t\tu8 *sai_d;\n\t\tu8 key_info_get_iv_size(const u8 *key_info, u32 nb_keys, u32 idx, u8 *const_iv_size, const u8 **const_iv);\n\n\t\tassert(tkw->cenc_subsamples);\n\n\t\t//multi-key skip all IV inits\n\t\tif (tkw->cenc_ki->value.data.ptr[0]) {\n\t\t\tu32 remain;\n\t\t\tu32 j, nb_iv_init = sai[0];\n\t\t\tnb_iv_init <<= 8;\n\t\t\tnb_iv_init |= sai[1];\n\t\t\tu8 *sai_p = sai + 2;\n\t\t\tremain = sai_size-2;\n\n\t\t\tfor (j=0; j<nb_iv_init; j++) {\n\t\t\t\tu32 mk_iv_size;\n\t\t\t\tu32 idx = sai_p[0];\n\t\t\t\tidx<<=8;\n\t\t\t\tidx |= sai_p[1];\n\n\t\t\t\tmk_iv_size = key_info_get_iv_size(tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size, idx, NULL, NULL);\n\t\t\t\tmk_iv_size += 2; //idx\n\t\t\t\tif (mk_iv_size > remain) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid multi-key CENC SAI, cannot modify first subsample !\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t\tsai_p += mk_iv_size;\n\t\t\t\tremain -= mk_iv_size;\n\t\t\t\tif (remain && (remain<=2)) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid multi-key CENC SAI, cannot modify first subsample !\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t}\n\t\t\toffset = (u32) (sai_p - sai);\n\t\t\tsub_count_size = 4; //32bit sub count\n\n\t\t} else {\n\t\t\toffset = key_info_get_iv_size(tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size, 1, NULL, NULL);\n\t\t\tsub_count_size = 2; //16bit sub count\n\t\t}\n\t\tif (sai_size < offset + sub_count_size + 6) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid CENC SAI !\\n\"));\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t}\n\t\t//get size of first subsample\n\t\toffset += sub_count_size;\n\t\tsai_d = sai + offset;\n\t\tfirst_sub_clear = sai_d[0];\n\t\tfirst_sub_clear<<=8;\n\t\tfirst_sub_clear |= sai_d[1];\n\t\tfirst_sub_clear += injected_hdr_size;\n\t\t//fits, only patch first subsample size\n\t\tif (first_sub_clear < 0xFFFF) {\n\t\t\tfake_sai = gf_malloc(sizeof(u8) * sai_size);\n\t\t\tif (!fake_sai) return GF_OUT_OF_MEM;\n\t\t\tmemcpy(fake_sai, sai, sizeof(u8) * sai_size);\n\t\t\tsai_d = fake_sai + offset;\n\t\t\tsai_d[0] = (first_sub_clear>>8) & 0xFF;\n\t\t\tsai_d[1] = (first_sub_clear) & 0xFF;\n\t\t\tsai = fake_sai;\n\t\t}\n\t\t//injected header size does not fit in first subsample, add a new subsample\n\t\telse {\n\t\t\tfake_sai = gf_malloc(sizeof(u8) * (sai_size+6));\n\t\t\tif (!fake_sai) return GF_OUT_OF_MEM;\n\t\t\t//copy till start of first subsample (including subsample_count)\n\t\t\tmemcpy(fake_sai, sai, sizeof(u8) * offset);\n\t\t\t//copy all subsamples\n\t\t\tmemcpy(fake_sai+offset+6, sai+offset, sizeof(u8) * (sai_size - offset) );\n\t\t\t//insert subsample\n\t\t\tsai_d = fake_sai + offset;\n\t\t\tsai_d[0] = (injected_hdr_size>>8) & 0xFF;\n\t\t\tsai_d[1] = (injected_hdr_size) & 0xFF;\n\t\t\tsai_d[2] = sai_d[3] = sai_d[4] = sai_d[5] = 0;\n\t\t\t//update subsample count\n\t\t\tsai_d = fake_sai + offset - sub_count_size;\n\t\t\tif (sub_count_size==2) {\n\t\t\t\tu32 cnt = ((u32) sai_d[0]) << 8 | (u32) sai_d[1];\n\t\t\t\tcnt++;\n\t\t\t\tsai_d[0] = (cnt>>8) & 0xFF;\n\t\t\t\tsai_d[1] = (cnt) & 0xFF;\n\t\t\t} else {\n\t\t\t\tu32 cnt = GF_4CC( sai_d[0], sai_d[1], sai_d[2], sai_d[3]);\n\t\t\t\tcnt++;\n\t\t\t\tsai_d[0] = (cnt>>24) & 0xFF;\n\t\t\t\tsai_d[1] = (cnt>>16) & 0xFF;\n\t\t\t\tsai_d[2] = (cnt>>8) & 0xFF;\n\t\t\t\tsai_d[3] = (cnt) & 0xFF;\n\t\t\t}\n\t\t\tsai = fake_sai;\n\t\t\tsai_size += 6;\n\t\t}\n\t}\n\n\tif (act_type==CENC_ADD_FRAG) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\tif (pck_is_encrypted) {\n\t\t\te = gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, sai, sai_size, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t} else {\n\t\t\te = gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, NULL, 0, GF_FALSE, ctx->saio32, tkw->cenc_multikey);\n\t\t}\n#else\n\t\te = GF_NOT_SUPPORTED;\n#endif\n\t} else {\n\t\tif (sai) {\n\t\t\te = gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, sai, sai_size, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t} else if (!pck_is_encrypted) {\n\t\t\te = gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, NULL, 0, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t}\n\t}\n\tif (fake_sai) gf_free(fake_sai);\n\treturn e;\n}",
        "func": "static GF_Err mp4_mux_cenc_update(GF_MP4MuxCtx *ctx, TrackWriter *tkw, GF_FilterPacket *pck, u32 act_type, u32 pck_size, u32 injected_hdr_size)\n{\n\tconst GF_PropertyValue *p;\n\tGF_Err e;\n\tBool pck_is_encrypted;\n\tu32 IV_size=0;\n\tu8 *fake_sai = NULL;\n\tu8 *sai = NULL;\n\tu32 sai_size = 0;\n\tBool needs_seig = GF_FALSE;\n\tu32 sample_num;\n\n\tif (tkw->cenc_state == CENC_SETUP_ERROR)\n\t\treturn GF_SERVICE_ERROR;\n\n\n\tif (pck) {\n\t\tp = gf_filter_pck_get_property(pck, GF_PROP_PCK_CENC_SAI);\n\t\tif (p) {\n\t\t\tsai = p->value.data.ptr;\n\t\t\tsai_size = p->value.data.size;\n\t\t}\n\t}\n\n\n\t//initial setup\n\tif (tkw->cenc_state==CENC_NEED_SETUP) {\n\t\tu32 scheme_type=0;\n\t\tu32 scheme_version=0;\n\t\tu32 cenc_stsd_mode=0;\n\t\tu32 container_type = GF_ISOM_BOX_TYPE_SENC;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_PROTECTION_SCHEME_TYPE);\n\t\tif (p) scheme_type = p->value.uint;\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_PROTECTION_SCHEME_VERSION);\n\t\tif (p) scheme_version = p->value.uint;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_CENC_STSD_MODE);\n\t\tif (p) cenc_stsd_mode = p->value.uint;\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ENCRYPTED);\n\t\tif (p) pck_is_encrypted = p->value.boolean;\n\t\telse pck_is_encrypted = GF_FALSE;\n\n\n\t\tp = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_CENC_STORE);\n\t\tif (p && p->value.uint) container_type = p->value.uint;\n\n\t\ttkw->clear_stsd_idx = 0;\n\t\tif (cenc_stsd_mode) {\n\t\t\tu32 clone_stsd_idx;\n\t\t\te = gf_isom_clone_sample_description(ctx->file, tkw->track_num, ctx->file, tkw->track_num, tkw->stsd_idx, NULL, NULL, &clone_stsd_idx);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to clone sample description: %s\\n\", gf_error_to_string(e) ));\n\t\t\t\treturn e;\n\t\t\t}\n\t\t\t//current stsd reused, use clone one for encrypted stsd\n\t\t\tif (tkw->reused_stsd) {\n\t\t\t\ttkw->clear_stsd_idx = tkw->stsd_idx;\n\t\t\t\ttkw->stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t\t//before\n\t\t\telse if (cenc_stsd_mode==1) {\n\t\t\t\ttkw->clear_stsd_idx = tkw->stsd_idx;\n\t\t\t\ttkw->stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t\t//after\n\t\t\telse {\n\t\t\t\ttkw->clear_stsd_idx = clone_stsd_idx;\n\t\t\t}\n\t\t}\n\t\ttkw->def_crypt_byte_block = tkw->crypt_byte_block;\n\t\ttkw->def_skip_byte_block = tkw->skip_byte_block;\n\n\t\ttkw->cenc_state = CENC_SETUP_DONE;\n\t\ttkw->def_cenc_key_info_crc = tkw->cenc_key_info_crc;\n\t\tif (tkw->cenc_ki) {\n\t\t\te = gf_isom_set_cenc_protection(ctx->file, tkw->track_num, tkw->stsd_idx, scheme_type, scheme_version, pck_is_encrypted, tkw->def_crypt_byte_block, tkw->def_skip_byte_block, tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size);\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Missing CENC Key config, cannot mux\\n\"));\n\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t}\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to setup CENC information: %s\\n\", gf_error_to_string(e) ));\n\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\treturn e;\n\t\t}\n\n\t\t//purge duplicates\n\t\tu32 k, nb_sdesc = gf_isom_get_sample_description_count(ctx->file, tkw->track_num);\n\t\tif (nb_sdesc>2) {\n\t\t\tfor (k=0; k<nb_sdesc; k++) {\n\t\t\t\tif (k+1 == tkw->stsd_idx) continue;\n\n\t\t\t\tif (gf_isom_is_same_sample_description(ctx->file, tkw->track_num, tkw->stsd_idx, ctx->file, tkw->track_num, k+1) ) {\n\t\t\t\t\tgf_isom_remove_stream_description(ctx->file, tkw->track_num, tkw->stsd_idx);\n\t\t\t\t\ttkw->stsd_idx = k+1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif ((ctx->psshs == MP4MX_PSSH_MOOV) || (ctx->psshs == MP4MX_PSSH_BOTH))\n\t\t\tmp4_mux_cenc_insert_pssh(ctx, tkw, NULL, 0);\n\n\t\tif (!tkw->has_brands && (scheme_type==GF_ISOM_OMADRM_SCHEME))\n\t\t\tgf_isom_modify_alternate_brand(ctx->file, GF_ISOM_BRAND_OPF2, GF_TRUE);\n\n\t\tif (container_type) {\n\t\t\tif (container_type==GF_ISOM_BOX_UUID_PSEC) {\n\t\t\t\te = gf_isom_piff_allocate_storage(ctx->file, tkw->track_num, 0, 0, NULL);\n\t\t\t} else {\n\t\t\t\te = gf_isom_cenc_allocate_storage(ctx->file, tkw->track_num);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to setup CENC storage: %s\\n\", gf_error_to_string(e) ));\n\t\t\t\ttkw->cenc_state = CENC_SETUP_ERROR;\n\t\t\t\treturn e;\n\t\t\t}\n\t\t}\n\t}\n\tif (act_type==CENC_CONFIG) return GF_OK;\n\n\tpck_is_encrypted = GF_FALSE;\n\tif (pck)\n\t\tpck_is_encrypted = gf_filter_pck_get_crypt_flags(pck);\n\n\t//!! tkw->nb_samples / tkw->samples_in_frag not yet incremented !!\n\tif (act_type == CENC_ADD_FRAG) {\n\t\tsample_num = tkw->samples_in_frag + 1;\n\n\t\tif (ctx->cmaf) {\n\t\t\tif (!tkw->samples_in_frag) {\n\t\t\t\ttkw->cenc_frag_protected = pck_is_encrypted;\n\t\t\t} else {\n\t\t\t\tif (tkw->cenc_frag_protected != pck_is_encrypted) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] CMAF forbids mixing protected and unprotected samples in a single fragment, please re-encrypt or change target segment duration\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tsample_num = tkw->nb_samples + 1;\n\t}\n\tif (!pck_is_encrypted) {\n\t\tif (tkw->clear_stsd_idx) {\n\t\t\tif (act_type==CENC_ADD_FRAG) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\treturn gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, NULL, 0, GF_FALSE, ctx->saio32, tkw->cenc_multikey);\n#else\n\t\t\t\treturn GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\treturn gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, NULL, 0, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t\t}\n\t\t} else {\n\t\t\tchar dumb_key[20];\n\t\t\tmemset(dumb_key, 0, 20); //dumb key, IV size 0, not protected\n\t\t\te = gf_isom_set_sample_cenc_group(ctx->file, tkw->track_num, sample_num, GF_FALSE, 0, 0, dumb_key, 20);\n\t\t\tIV_size = 0;\n\t\t\ttkw->has_seig = GF_TRUE;\n\t\t}\n\t} else {\n\n\t\te = GF_OK;\n\t\t//multikey ALWAYS uses seig\n\t\tif (tkw->cenc_ki->value.data.ptr[0])\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_crypt_byte_block != tkw->crypt_byte_block)\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_skip_byte_block != tkw->skip_byte_block)\n\t\t\tneeds_seig = GF_TRUE;\n\t\telse if (tkw->def_cenc_key_info_crc != tkw->cenc_key_info_crc)\n\t\t\tneeds_seig = GF_TRUE;\n\n\t\tif (needs_seig) {\n\t\t\te = gf_isom_set_sample_cenc_group(ctx->file, tkw->track_num, sample_num, 1, tkw->crypt_byte_block, tkw->skip_byte_block, tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size);\n\t\t\ttkw->has_seig = GF_TRUE;\n\t\t} else if (tkw->has_seig) {\n\t\t\te = gf_isom_set_sample_cenc_default_group(ctx->file, tkw->track_num, sample_num);\n\t\t}\n\t}\n\tif (e) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample encryption group entry: %s)\\n\", gf_error_to_string(e) ));\n\t\treturn e;\n\t}\n\n\tp = gf_filter_pck_get_property(pck, GF_PROP_PID_CENC_PSSH);\n\tif (p && (p->type == GF_PROP_DATA) && p->value.data.ptr) {\n\t\tif (ctx->store>=MP4MX_MODE_FRAG) {\n\t\t\tmp4_mux_cenc_insert_pssh(ctx, tkw, p, 0);\n\t\t} else {\n\t\t\tgf_isom_set_sample_group_description(ctx->file, tkw->track_num, sample_num, GF_4CC('P','S','S','H'), 0, p->value.data.ptr, p->value.data.size, 0);\n\t\t}\n\t}\n\n\tif (!sai) {\n\t\tif (tkw->constant_IV_size && !tkw->cenc_subsamples)\n\t\t\treturn GF_OK;\n\n\t\tif (IV_size) {\n\t\t\t//generate clear SAI data with a non-0 IV\n\t\t\tu32 olen = pck_size;\n\t\t\tGF_BitStream *bs = gf_bs_new(NULL, 9, GF_BITSTREAM_WRITE);\n\t\t\tif (tkw->cenc_multikey) {\n\t\t\t\tgf_bs_write_u16(bs, 0);\n\t\t\t} else {\n\t\t\t\tgf_bs_write_long_int(bs, 0, IV_size*8);\n\t\t\t}\n\n\t\t\tif (tkw->cenc_subsamples) {\n\t\t\t\tu32 i;\n\t\t\t\tu32 subsample_count = 1;\n\t\t\t\twhile (olen>0xFFFF) {\n\t\t\t\t\tolen -= 0xFFFF;\n\t\t\t\t\tsubsample_count ++;\n\t\t\t\t}\n\t\t\t\tgf_bs_write_u16(bs, subsample_count);\n\t\t\t\tolen = pck_size;\n\t\t\t\tfor (i = 0; i < subsample_count; i++) {\n\t\t\t\t\tu32 clear_size;\n\t\t\t\t\tif (olen<0xFFFF) {\n\t\t\t\t\t\tclear_size = olen;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tclear_size = 0xFFFF;\n\t\t\t\t\t\tolen -= 0xFFFF;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (tkw->cenc_multikey)\n\t\t\t\t\t\tgf_bs_write_u16(bs, 0);\n\t\t\t\t\tgf_bs_write_u16(bs, clear_size);\n\t\t\t\t\tgf_bs_write_u32(bs, 0);\n\t\t\t\t}\n\t\t\t}\n\t\t\tgf_bs_get_content(bs, &fake_sai, &sai_size);\n\t\t\tgf_bs_del(bs);\n\t\t\tsai = fake_sai;\n\t\t}\n\t}\n\t//we injected xPS at the beginning of the sample (AVC/HEVC/VVC), we must patch the first subsample\n\t//of SAI data\n\telse if (injected_hdr_size) {\n\t\tu32 offset = 0;\n\t\tu32 first_sub_clear, sub_count_size;\n\t\tu8 *sai_d;\n\t\tu8 key_info_get_iv_size(const u8 *key_info, u32 nb_keys, u32 idx, u8 *const_iv_size, const u8 **const_iv);\n\n\t\tassert(tkw->cenc_subsamples);\n\n\t\t//multi-key skip all IV inits\n\t\tif (tkw->cenc_ki->value.data.ptr[0]) {\n\t\t\tu32 remain;\n\t\t\tu32 j, nb_iv_init = sai[0];\n\t\t\tnb_iv_init <<= 8;\n\t\t\tnb_iv_init |= sai[1];\n\t\t\tu8 *sai_p = sai + 2;\n\t\t\tremain = sai_size-2;\n\n\t\t\tfor (j=0; j<nb_iv_init; j++) {\n\t\t\t\tu32 mk_iv_size;\n\t\t\t\tu32 idx = sai_p[0];\n\t\t\t\tidx<<=8;\n\t\t\t\tidx |= sai_p[1];\n\n\t\t\t\tmk_iv_size = key_info_get_iv_size(tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size, idx, NULL, NULL);\n\t\t\t\tmk_iv_size += 2; //idx\n\t\t\t\tif (mk_iv_size > remain) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid multi-key CENC SAI, cannot modify first subsample !\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t\tsai_p += mk_iv_size;\n\t\t\t\tremain -= mk_iv_size;\n\t\t\t\tif (remain && (remain<=2)) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid multi-key CENC SAI, cannot modify first subsample !\\n\"));\n\t\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t\t}\n\t\t\t}\n\t\t\toffset = (u32) (sai_p - sai);\n\t\t\tsub_count_size = 4; //32bit sub count\n\n\t\t} else {\n\t\t\toffset = key_info_get_iv_size(tkw->cenc_ki->value.data.ptr, tkw->cenc_ki->value.data.size, 1, NULL, NULL);\n\t\t\tsub_count_size = 2; //16bit sub count\n\t\t}\n\t\tif (sai_size < offset + sub_count_size + 6) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Invalid CENC SAI !\\n\"));\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t}\n\t\t//get size of first subsample\n\t\toffset += sub_count_size;\n\t\tsai_d = sai + offset;\n\t\tfirst_sub_clear = sai_d[0];\n\t\tfirst_sub_clear<<=8;\n\t\tfirst_sub_clear |= sai_d[1];\n\t\tfirst_sub_clear += injected_hdr_size;\n\t\t//fits, only patch first subsample size\n\t\tif (first_sub_clear < 0xFFFF) {\n\t\t\tfake_sai = gf_malloc(sizeof(u8) * sai_size);\n\t\t\tif (!fake_sai) return GF_OUT_OF_MEM;\n\t\t\tmemcpy(fake_sai, sai, sizeof(u8) * sai_size);\n\t\t\tsai_d = fake_sai + offset;\n\t\t\tsai_d[0] = (first_sub_clear>>8) & 0xFF;\n\t\t\tsai_d[1] = (first_sub_clear) & 0xFF;\n\t\t\tsai = fake_sai;\n\t\t}\n\t\t//injected header size does not fit in first subsample, add a new subsample\n\t\telse {\n\t\t\tfake_sai = gf_malloc(sizeof(u8) * (sai_size+6));\n\t\t\tif (!fake_sai) return GF_OUT_OF_MEM;\n\t\t\t//copy till start of first subsample (including subsample_count)\n\t\t\tmemcpy(fake_sai, sai, sizeof(u8) * offset);\n\t\t\t//copy all subsamples\n\t\t\tmemcpy(fake_sai+offset+6, sai+offset, sizeof(u8) * (sai_size - offset) );\n\t\t\t//insert subsample\n\t\t\tsai_d = fake_sai + offset;\n\t\t\tsai_d[0] = (injected_hdr_size>>8) & 0xFF;\n\t\t\tsai_d[1] = (injected_hdr_size) & 0xFF;\n\t\t\tsai_d[2] = sai_d[3] = sai_d[4] = sai_d[5] = 0;\n\t\t\t//update subsample count\n\t\t\tsai_d = fake_sai + offset - sub_count_size;\n\t\t\tif (sub_count_size==2) {\n\t\t\t\tu32 cnt = ((u32) sai_d[0]) << 8 | (u32) sai_d[1];\n\t\t\t\tcnt++;\n\t\t\t\tsai_d[0] = (cnt>>8) & 0xFF;\n\t\t\t\tsai_d[1] = (cnt) & 0xFF;\n\t\t\t} else {\n\t\t\t\tu32 cnt = GF_4CC( sai_d[0], sai_d[1], sai_d[2], sai_d[3]);\n\t\t\t\tcnt++;\n\t\t\t\tsai_d[0] = (cnt>>24) & 0xFF;\n\t\t\t\tsai_d[1] = (cnt>>16) & 0xFF;\n\t\t\t\tsai_d[2] = (cnt>>8) & 0xFF;\n\t\t\t\tsai_d[3] = (cnt) & 0xFF;\n\t\t\t}\n\t\t\tsai = fake_sai;\n\t\t\tsai_size += 6;\n\t\t}\n\t}\n\n\tif (act_type==CENC_ADD_FRAG) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\tif (pck_is_encrypted) {\n\t\t\te = gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, sai, sai_size, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t} else {\n\t\t\te = gf_isom_fragment_set_cenc_sai(ctx->file, tkw->track_id, NULL, 0, GF_FALSE, ctx->saio32, tkw->cenc_multikey);\n\t\t}\n#else\n\t\te = GF_NOT_SUPPORTED;\n#endif\n\t} else {\n\t\tif (sai) {\n\t\t\te = gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, sai, sai_size, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t} else if (!pck_is_encrypted) {\n\t\t\te = gf_isom_track_cenc_add_sample_info(ctx->file, tkw->track_num, GF_ISOM_BOX_TYPE_SENC, NULL, 0, tkw->cenc_subsamples, ctx->saio32, tkw->cenc_multikey);\n\t\t}\n\t}\n\tif (fake_sai) gf_free(fake_sai);\n\treturn e;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -162,7 +162,7 @@\n \t\t\ttkw->has_seig = GF_TRUE;\n \t\t}\n \t} else {\n-\t\n+\n \t\te = GF_OK;\n \t\t//multikey ALWAYS uses seig\n \t\tif (tkw->cenc_ki->value.data.ptr[0])",
        "diff_line_info": {
            "deleted_lines": [
                "\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/mp4_mux_process_sample",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "static GF_Err mp4_mux_process_sample(GF_MP4MuxCtx *ctx, TrackWriter *tkw, GF_FilterPacket *pck, Bool for_fragment)\n{\n\tGF_Err e=GF_OK;\n\tu64 cts, prev_dts;\n\tu32 prev_size=0;\n\tu32 duration = 0;\n\tu32 timescale = 0;\n\tconst GF_PropertyValue *subs;\n\tGF_FilterSAPType sap_type;\n\tu32 insert_subsample_dsi_size = 0;\n\tu32 first_nal_is_audelim = GF_FALSE;\n\tu32 sample_desc_index = tkw->stsd_idx;\n\tBool sample_timing_ok = GF_TRUE;\n\n\ttimescale = gf_filter_pck_get_timescale(pck);\n\n\tprev_dts = tkw->nb_samples ? tkw->sample.DTS : GF_FILTER_NO_TS;\n\tprev_size = tkw->sample.dataLength;\n\ttkw->sample.CTS_Offset = 0;\n\tif (gf_filter_pck_get_frame_interface(pck)) {\n\t\ttkw->dgl_copy = gf_filter_pck_dangling_copy(pck, tkw->dgl_copy);\n\t\tif (!tkw->dgl_copy) return GF_IO_ERR;\n\t\ttkw->sample.data = (char *)gf_filter_pck_get_data(tkw->dgl_copy, &tkw->sample.dataLength);\n\t} else {\n\t\ttkw->sample.data = (char *)gf_filter_pck_get_data(pck, &tkw->sample.dataLength);\n\t}\n\n\tctx->update_report = GF_TRUE;\n\tctx->total_bytes_in += tkw->sample.dataLength;\n\tctx->total_samples++;\n\n\ttkw->sample.DTS = gf_filter_pck_get_dts(pck);\n\tcts = gf_filter_pck_get_cts(pck);\n\n\tif (tkw->sample.DTS == GF_FILTER_NO_TS) {\n\t\tif (cts == GF_FILTER_NO_TS) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Sample with no DTS/CTS, cannot add (last DTS \"LLU\", last size %d)\\n\", prev_dts, prev_size ));\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t} else {\n\t\t\tu32 min_pck_dur = gf_filter_pid_get_min_pck_duration(tkw->ipid);\n\t\t\tif (min_pck_dur) {\n\t\t\t\ttkw->sample.DTS = prev_dts;\n\t\t\t\t//transform back to inpput timescale\n\t\t\t\tif (timescale != tkw->tk_timescale) {\n\t\t\t\t\ttkw->sample.DTS = gf_timestamp_rescale(tkw->sample.DTS, tkw->tk_timescale, timescale);\n\t\t\t\t}\n\t\t\t\ttkw->sample.DTS += min_pck_dur;\n\t\t\t} else {\n\t\t\t\ttkw->sample.DTS = cts;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttkw->sample.CTS_Offset = (s32) ((s64) cts - (s64) tkw->sample.DTS);\n\t}\n\n\t//do our best to patch init ts if timing config aborted\n\tif (tkw->si_min_ts_plus_one) {\n\t\tu64 si_min_ts = tkw->si_min_ts_plus_one - 1;\n\t\ttkw->si_min_ts_plus_one = 0;\n\t\ttkw->ts_shift = tkw->sample.DTS;\n\t\tmp4_mux_update_init_edit(ctx, tkw, si_min_ts, GF_FALSE);\n\t}\n\t//tkw->ts_shift is in source timescale, apply it before rescaling TSs/duration\n\tif (tkw->ts_shift) {\n\t\tif (ctx->is_rewind) {\n\t\t\tif (tkw->sample.DTS <= tkw->ts_shift) {\n\t\t\t\ttkw->sample.DTS = tkw->ts_shift - tkw->sample.DTS;\n\t\t\t\tcts = tkw->ts_shift - cts;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] broken timing in track, initial ts \"LLU\" less than TS \"LLU\"\\n\", tkw->ts_shift, tkw->sample.DTS));\n\t\t\t\tsample_timing_ok = GF_FALSE;\n\t\t\t}\n\t\t} else {\n\t\t\tif (tkw->sample.DTS >= tkw->ts_shift) {\n\t\t\t\ttkw->sample.DTS -= tkw->ts_shift;\n\t\t\t\tcts -= tkw->ts_shift;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] broken timing in track, initial ts \"LLU\" greater than TS \"LLU\"\\n\", tkw->ts_shift, tkw->sample.DTS));\n\t\t\t\tsample_timing_ok = GF_FALSE;\n\t\t\t}\n\t\t}\n\t}\n\n\t//sample-accurate seek info, start logging min CTS of packets marked as non-sync\n\tif (tkw->check_seek_ts && !gf_filter_pck_get_seek_flag(pck)) {\n\t\tu64 ts_check = cts;\n\t\tsubs = gf_filter_pck_get_property(pck, GF_PROP_PCK_SKIP_BEGIN);\n\t\tif (subs)\n\t\t\tts_check += subs->value.uint;\n\n\t\tif (!tkw->min_ts_seek_plus_one) {\n\t\t\ttkw->min_ts_seek_plus_one = ts_check + 1;\n\t\t} else if (tkw->min_ts_seek_plus_one > ts_check + 1) {\n\t\t\ttkw->min_ts_seek_plus_one = ts_check + 1;\n\t\t} else {\n\t\t\t//TS is greater than last non-seek packet TS, we're done seeking\n\t\t\ttkw->check_seek_ts = GF_FALSE;\n\t\t}\n\t}\n\n\tduration = gf_filter_pck_get_duration(pck);\n\tif (timescale != tkw->tk_timescale) {\n\t\ts64 ctso;\n\t\ttkw->sample.DTS = gf_timestamp_rescale(tkw->sample.DTS, timescale, tkw->tk_timescale);\n\n\t\tctso = (s64) tkw->sample.CTS_Offset;\n\t\tctso *= tkw->tk_timescale;\n\t\tctso /= timescale;\n\t\ttkw->sample.CTS_Offset = (s32) ctso;\n\t\tduration *= tkw->tk_timescale;\n\t\tduration /= timescale;\n\n\t\tif (cts != GF_FILTER_NO_TS) {\n\t\t\tcts = gf_timestamp_rescale(cts, timescale, tkw->tk_timescale);\n\t\t}\n\t}\n\n\ttkw->sample.IsRAP = 0;\n\tif (tkw->codecid==GF_CODECID_RAW) {\n\t\tsap_type = GF_FILTER_SAP_1;\n\t} else {\n\t\tsap_type = mp4_mux_get_sap(ctx, pck);\n\n\t\t//if pps inband mode is used, turn sap3 into sap1\n\t\tif ((tkw->xps_inband==XPS_IB_PPS) && sap_type==GF_FILTER_SAP_3)\n\t\t\tsap_type=GF_FILTER_SAP_1;\n\t}\n\tif (sap_type==GF_FILTER_SAP_1)\n\t\ttkw->sample.IsRAP = SAP_TYPE_1;\n\telse if (sap_type==GF_FILTER_SAP_2)\n\t\ttkw->sample.IsRAP = SAP_TYPE_2;\n\telse if ( (sap_type == GF_FILTER_SAP_4) && (tkw->stream_type != GF_STREAM_VISUAL) )\n\t\ttkw->sample.IsRAP = SAP_TYPE_1;\n\n\t/*RFC8216bis is not clear here:\n\t\"if the Partial Segment contains an independent frame.\"\n\t\t-> this would allow SAP1,2,3 (independent being only defined for segments)\n\n\tbut\n\n\t\"Partial Segment containing an independent frame SHOULD carry it to increase the efficiency with which clients can join and switch Renditions\"\n\t\t-> if used for switching, this only allows SAP 1 and 2\n\n\tSpec should be fixed to allow for both cases (fast tune-in or in-segment switching)\n\t*/\n\tif ((tkw->sample.IsRAP == SAP_TYPE_1) || (tkw->sample.IsRAP == SAP_TYPE_2))\n\t\tctx->frag_has_intra = GF_TRUE;\n\n\ttkw->sample.DTS += tkw->dts_patch;\n\tif (tkw->nb_samples && (prev_dts >= tkw->sample.DTS) ) {\n\t\t//the fragmented API will patch the duration on the fly\n\t\tif (!for_fragment && ctx->patch_dts) {\n\t\t\tgf_isom_patch_last_sample_duration(ctx->file, tkw->track_num, prev_dts ? prev_dts : 1);\n\t\t}\n\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] PID %s ID %d Sample %d with DTS \"LLU\" less than previous sample DTS \"LLU\", patching DTS%s\\n\", gf_filter_pid_get_name(tkw->ipid), tkw->track_id, tkw->nb_samples+1, tkw->sample.DTS, prev_dts, ctx->patch_dts ? \"and adjusting prev sample duration\" : \"\" ));\n\t\tsample_timing_ok = GF_FALSE;\n\n\t\tif (prev_dts) {\n\t\t\ttkw->dts_patch = prev_dts - tkw->sample.DTS;\n\t\t\ttkw->sample.DTS += tkw->dts_patch+1; //+1 to avoid 0-dur samples\n\t\t} else {\n\t\t\ttkw->sample.DTS += 1;\n\t\t\tif (tkw->sample.CTS_Offset) tkw->sample.CTS_Offset -= 1;\n\t\t\tduration-=1;\n\t\t}\n\t}\n\n\n\tif (tkw->negctts_shift)\n\t\ttkw->sample.CTS_Offset -= tkw->negctts_shift;\n\n\tif (sample_timing_ok) {\n\t\tif (tkw->probe_min_ctts) {\n\t\t\ts32 diff = (s32) ((s64) cts - (s64) tkw->sample.DTS);\n\t\t\tif (diff < tkw->min_neg_ctts)\n\t\t\t\ttkw->min_neg_ctts = diff;\n\t\t}\n\t\tif (tkw->sample.CTS_Offset) tkw->has_ctts = GF_TRUE;\n\n\t\tif (tkw->sample.CTS_Offset < tkw->min_neg_ctts)\n\t\t\ttkw->min_neg_ctts = tkw->sample.CTS_Offset;\n\t}\n\n\ttkw->sample.nb_pack = 0;\n\tif (tkw->raw_audio_bytes_per_sample) {\n\t\ttkw->sample.nb_pack = tkw->sample.dataLength / tkw->raw_audio_bytes_per_sample;\n\t\tif (tkw->sample.nb_pack) {\n\t\t\tduration = 1;\n\t\t\tif (tkw->raw_samplerate && (tkw->tk_timescale != tkw->raw_samplerate)) {\n\t\t\t\tduration *= tkw->tk_timescale;\n\t\t\t\tduration /= tkw->raw_samplerate;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tkw->cenc_state && tkw->clear_stsd_idx && !gf_filter_pck_get_crypt_flags(pck)) {\n\t\tsample_desc_index = tkw->clear_stsd_idx;\n\t}\n\n\tif (tkw->sparse_inject && (prev_dts!=GF_FILTER_NO_TS) && (tkw->sample.DTS!=GF_FILTER_NO_TS) && tkw->prev_duration) {\n\t\tu64 est_time = prev_dts + tkw->prev_duration;\n\t\tif (est_time < tkw->sample.DTS) {\n\t\t\tu32 ins_dur;\n\t\t\tGF_ISOSample s;\n\t\t\tmemset(&s, 0, sizeof(GF_ISOSample));\n\t\t\ts.DTS = est_time;\n\n\t\t\ts.IsRAP = SAP_TYPE_1;\n\t\t\tins_dur = (u32) (tkw->sample.DTS - est_time);\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_add_sample(ctx->file, tkw->track_id, &s, tkw->stsd_idx, ins_dur, 0, 0, 0);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, tkw->stsd_idx, &s);\n\t\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, ins_dur);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tkw->use_dref) {\n\t\tu64 data_offset = gf_filter_pck_get_byte_offset(pck);\n\t\tif (data_offset != GF_FILTER_NO_BO) {\n\t\t\te = gf_isom_add_sample_reference(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample, data_offset);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to add sample DTS \"LLU\" from %s as reference: %s\\n\", tkw->sample.DTS, gf_filter_pid_get_name(tkw->ipid), gf_error_to_string(e) ));\n\t\t\t}\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Cannot add sample reference at DTS \"LLU\" , input sample data is not continous in source\\n\", tkw->sample.DTS ));\n\t\t}\n\t} else if (tkw->nb_frames_per_sample && (tkw->nb_samples % tkw->nb_frames_per_sample)) {\n\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t \te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, tkw->sample.data, tkw->sample.dataLength, 0);\n#else\n\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t} else {\n\t\t\te = gf_isom_append_sample_data(ctx->file, tkw->track_num, tkw->sample.data, tkw->sample.dataLength);\n\t\t}\n\t\ttkw->has_append = GF_TRUE;\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to append sample DTS \"LLU\" data: %s\\n\", tkw->sample.DTS, gf_error_to_string(e) ));\n\t\t}\n\t} else {\n\t\tBool inject_pps = ctx->pps_inband;\n\t\tif (ctx->xps_inband==XPS_IB_AUTO) {\n\t\t\tconst GF_PropertyValue *p = gf_filter_pck_get_property(pck, GF_PROP_PCK_XPS_MASK);\n\t\t\tif (p && (p->value.uint & (1<<2) ) )\n\t\t\t\tinject_pps = GF_TRUE;\n\t\t}\n\n\t\tif ((tkw->sample.IsRAP || tkw->force_inband_inject || inject_pps) && tkw->xps_inband) {\n\t\t\tu8 *inband_xps;\n\t\t\tu32 inband_xps_size;\n\t\t\tchar *au_delim=NULL;\n\t\t\tu32 au_delim_size=0;\n\t\t\tchar *pck_data = tkw->sample.data;\n\t\t\tu32 pck_data_len = tkw->sample.dataLength;\n\t\t\tif (tkw->sample.IsRAP || tkw->force_inband_inject) {\n\t\t\t\tinband_xps = tkw->inband_hdr;\n\t\t\t\tinband_xps_size = tkw->inband_hdr_size;\n\t\t\t\ttkw->force_inband_inject = GF_FALSE;\n\t\t\t} else {\n\t\t\t\tinband_xps = tkw->inband_hdr_non_rap;\n\t\t\t\tinband_xps_size = tkw->inband_hdr_non_rap_size;\n\t\t\t}\n\t\t\ttkw->sample.data = inband_xps;\n\t\t\ttkw->sample.dataLength = inband_xps_size;\n\n\t\t\tif (tkw->is_nalu==NALU_AVC) {\n\t\t\t\tif (pck_data_len >= 2 + tkw->nal_unit_size) {\n\t\t\t\t\tchar *nal = pck_data + tkw->nal_unit_size;\n\t\t\t\t\tif ((nal[0] & 0x1F) == GF_AVC_NALU_ACCESS_UNIT) {\n\t\t\t\t\t\tfirst_nal_is_audelim = au_delim_size = 2 + tkw->nal_unit_size;\n\t\t\t\t\t\tau_delim = pck_data;\n\t\t\t\t\t\tif (au_delim_size >= pck_data_len) au_delim = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (pck_data_len >= 3 + tkw->nal_unit_size) {\n\t\t\t\t\tchar *nal = pck_data + tkw->nal_unit_size;\n\t\t\t\t\tif (((nal[0] & 0x7E)>>1) == GF_HEVC_NALU_ACCESS_UNIT) {\n\t\t\t\t\t\tfirst_nal_is_audelim = au_delim_size = 3 + tkw->nal_unit_size;\n\t\t\t\t\t\tau_delim = pck_data;\n\t\t\t\t\t\tif (au_delim_size >= pck_data_len) au_delim = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (au_delim) {\n\t\t\t\ttkw->sample.data = au_delim;\n\t\t\t\ttkw->sample.dataLength = au_delim_size;\n\t\t\t\tpck_data += au_delim_size;\n\t\t\t\tpck_data_len -= au_delim_size;\n\t\t\t}\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t//force using ref mode\n\t\t\t\tvoid *ref=NULL;\n\t\t\t\te = gf_isom_fragment_add_sample_ex(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0, &ref, 0);\n\t\t\t\tif (!e && au_delim) {\n\t\t\t\t\te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, inband_xps, inband_xps_size, 0);\n\t\t\t\t}\n\t\t\t\tif (!e) {\n\t\t\t\t\tif (gf_filter_pck_is_blocking_ref(pck)) {\n\t\t\t\t\t\te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, pck_data, pck_data_len, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgf_filter_pck_ref(&pck);\n\t\t\t\t\t\tGF_FilterPacket *ref = pck;\n\t\t\t\t\t\te = gf_isom_fragment_append_data_ex(ctx->file, tkw->track_id, pck_data, pck_data_len, 0, (void**)&ref, au_delim ? au_delim_size : 0);\n\t\t\t\t\t\tif (!ref) {\n\t\t\t\t\t\t\tgf_list_add(ctx->ref_pcks, pck);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tgf_filter_pck_unref(pck);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif // GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t} else {\n\t\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample);\n\t\t\t\tif (au_delim && !e) {\n\t\t\t\t\te = gf_isom_append_sample_data(ctx->file, tkw->track_num, inband_xps, inband_xps_size);\n\t\t\t\t}\n\t\t\t\tif (!e) e = gf_isom_append_sample_data(ctx->file, tkw->track_num, pck_data, pck_data_len);\n\t\t\t}\n\t\t\tinsert_subsample_dsi_size = inband_xps_size;\n\t\t} else if (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\tif (gf_filter_pck_is_blocking_ref(pck)) {\n\t\t\t\te = gf_isom_fragment_add_sample(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0);\n\t\t\t} else {\n\t\t\t\tgf_filter_pck_ref(&pck);\n\t\t\t\tGF_FilterPacket *ref = pck;\n\t\t\t\te = gf_isom_fragment_add_sample_ex(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0, (void**) &ref, 0);\n\t\t\t\tif (!ref) {\n\t\t\t\t\tgf_list_add(ctx->ref_pcks, pck);\n\t\t\t\t} else {\n\t\t\t\t\tgf_filter_pck_unref(pck);\n\t\t\t\t}\n\t\t\t}\n#else\n\t\t\te = GF_NOT_SUPPORTED;\n#endif // GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t} else {\n\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample);\n\t\t\tif (!e && !duration) {\n\t\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, 0);\n\t\t\t}\n\t\t}\n\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to add sample DTS \"LLU\" from %s - prev DTS \"LLU\": %s\\n\", tkw->sample.DTS, gf_filter_pid_get_name(tkw->ipid), prev_dts, gf_error_to_string(e) ));\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_CONTAINER, (\"[MP4Mux] added sample DTS \"LLU\" - prev DTS \"LLU\" - prev size %d\\n\", tkw->sample.DTS, prev_dts, prev_size));\n\t\t}\n\n\t\tif (!e && tkw->cenc_state) {\n\t\t\te = mp4_mux_cenc_update(ctx, tkw, pck, for_fragment ? CENC_ADD_FRAG : CENC_ADD_NORMAL, tkw->sample.dataLength, insert_subsample_dsi_size);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample CENC information: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t}\n\n\ttkw->nb_samples++;\n\ttkw->samples_in_stsd++;\n\ttkw->samples_in_frag++;\n\n\tif (e) return e;\n\n\tif (!for_fragment && sample_timing_ok) {\n\t\tu64 samp_cts;\n\t\tif (!tkw->clamp_ts_plus_one) {\n\t\t\tconst GF_PropertyValue *skp = gf_filter_pck_get_property(pck, GF_PROP_PCK_SKIP_PRES);\n\t\t\tif (skp && skp->value.boolean) {\n\t\t\t\ttkw->clamp_ts_plus_one = 1 + tkw->sample.DTS + tkw->sample.CTS_Offset;\n\t\t\t}\n\t\t}\n\t\t//store min max cts for edit list updates\n\t\tsamp_cts = tkw->sample.DTS + tkw->sample.CTS_Offset;\n\t\tif (!tkw->clamp_ts_plus_one || (samp_cts + 1 < tkw->clamp_ts_plus_one)) {\n\t\t\tif (samp_cts > tkw->max_cts) {\n\t\t\t\ttkw->max_cts = samp_cts;\n\t\t\t\ttkw->max_cts_samp_dur = duration;\n\t\t\t}\n\n\t\t\tif (tkw->min_cts > samp_cts)\n\t\t\t\ttkw->min_cts = samp_cts;\n\t\t}\n\t}\n\n\t//compat with old arch: write sample to group info for all samples\n\tif ((sap_type==3) || tkw->has_open_gop)  {\n\t\tif (!ctx->norap) {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_set_sample_rap_group(ctx->file, tkw->track_id, tkw->samples_in_frag, (sap_type==3) ? GF_TRUE : GF_FALSE, 0);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else if (sap_type==3) {\n\t\t\t\te = gf_isom_set_sample_rap_group(ctx->file, tkw->track_num, tkw->nb_samples, GF_TRUE /*(sap_type==3) ? GF_TRUE : GF_FALSE*/, 0);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample DTS \"LLU\" SAP 3 in RAP group: %s\\n\", tkw->sample.DTS, gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t\ttkw->has_open_gop = GF_TRUE;\n\t}\n\tif (!ctx->noroll) {\n\t\tif ((sap_type==GF_FILTER_SAP_4) || (sap_type==GF_FILTER_SAP_4_PROL) || tkw->gdr_type) {\n\t\t\tGF_ISOSampleRollType roll_type = 0;\n\t\t\ts16 roll = gf_filter_pck_get_roll_info(pck);\n\t\t\tif (sap_type==GF_FILTER_SAP_4) roll_type = GF_ISOM_SAMPLE_ROLL;\n\t\t\telse if (sap_type==GF_FILTER_SAP_4_PROL) roll_type = GF_ISOM_SAMPLE_PREROLL;\n\t\t\telse if (tkw->gdr_type==GF_FILTER_SAP_4_PROL) {\n\t\t\t\troll_type = GF_ISOM_SAMPLE_PREROLL_NONE;\n\t\t\t}\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_set_sample_roll_group(ctx->file, tkw->track_id, tkw->samples_in_frag, roll_type, roll);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\te = gf_isom_set_sample_roll_group(ctx->file, tkw->track_num, tkw->nb_samples, roll_type, roll);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample DTS \"LLU\" SAP 4 roll %s in roll group: %s\\n\", tkw->sample.DTS, roll, gf_error_to_string(e) ));\n\t\t\t}\n\t\t\tif (sap_type && !tkw->gdr_type)\n\t\t\t\ttkw->gdr_type = sap_type;\n\t\t}\n\t}\n\t\n\tsubs = gf_filter_pck_get_property(pck, GF_PROP_PCK_SUBS);\n\tif (subs) {\n\t\t//if no AUDelim nal and inband header injection, push new subsample\n\t\tif (!first_nal_is_audelim && insert_subsample_dsi_size) {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, insert_subsample_dsi_size, 0, 0, 0);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, 0, insert_subsample_dsi_size, 0, 0, 0);\n\t\t\t}\n\t\t\tinsert_subsample_dsi_size = 0;\n\t\t}\n\t\ttkw->has_subs = GF_TRUE;\n\n\t\tif (!ctx->bs_r) ctx->bs_r = gf_bs_new(subs->value.data.ptr, subs->value.data.size, GF_BITSTREAM_READ);\n\t\telse gf_bs_reassign_buffer(ctx->bs_r, subs->value.data.ptr, subs->value.data.size);\n\n\t\twhile (gf_bs_available(ctx->bs_r)) {\n\t\t\tu32 flags = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu32 subs_size = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu32 reserved = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu8 priority = gf_bs_read_u8(ctx->bs_r);\n\t\t\tu8 discardable = gf_bs_read_u8(ctx->bs_r);\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, flags, subs_size, priority, reserved, discardable);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, flags, subs_size, priority, reserved, discardable);\n\t\t\t}\n\n\t\t\t//we have AUDelim nal and inband header injection, push new subsample for inband header once we have pushed the first subsample (au delim)\n\t\t\tif (insert_subsample_dsi_size) {\n\t\t\t\tif (first_nal_is_audelim != subs_size) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] inserting inband param after AU delimiter NALU, but sample has subsample information not aligned on NALU (got %d subsample size but expecting %d) - file might be broken!\\n\", subs_size, first_nal_is_audelim));\n\t\t\t\t}\n\t\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, insert_subsample_dsi_size, 0, 0, 0);\n#endif\n\t\t\t\t} else {\n\t\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, 0, insert_subsample_dsi_size, 0, 0, 0);\n\t\t\t\t}\n\t\t\t\tinsert_subsample_dsi_size = GF_FALSE;\n\t\t\t}\n\t\t}\n\t} else if (for_fragment && tkw->has_subs && ctx->cmaf && (tkw->codecid==GF_CODECID_SUBS_XML)) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t//tentative implemntation of CMAF 7.5.20 which is just nonsense text !!:\n\t\t//\"the value of subsample_count shall equal 1 for the first image sub-sample, and the subsample_count of the TTML document shall equal 0.\"\n\t\t//\n\t\t//we simply signal a single subsample\n\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, tkw->sample.dataLength, 0, 0, 0);\n#endif\n\t}\n\n\tif (ctx->deps) {\n\t\tu8 dep_flags = gf_filter_pck_get_dependency_flags(pck);\n\t\tif (dep_flags) {\n\t\t\tu32 is_leading = (dep_flags>>6) & 0x3;\n\t\t\tu32 depends_on = (dep_flags>>4) & 0x3;\n\t\t\tu32 depended_on = (dep_flags>>2) & 0x3;\n\t\t\tu32 redundant = (dep_flags) & 0x3;\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_set_sample_flags(ctx->file, tkw->track_id, is_leading, depends_on, depended_on, redundant);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_set_sample_flags(ctx->file, tkw->track_num, tkw->nb_samples, is_leading, depends_on, depended_on, redundant);\n\t\t\t}\n\t\t}\n\t}\n\n\tu32 idx = 0;\n\twhile (1) {\n\t\tBool is_sample_group=GF_FALSE;\n\t\tu32 aux_type=0, aux_info=0, sg_flags=0;\n\t\tu32 p4cc;\n\t\tconst char *pname=NULL;\n\t\tconst GF_PropertyValue *p = gf_filter_pck_enum_properties(pck, &idx, &p4cc, &pname);\n\t\tif (!p) break;\n\t\tif ((p->type!=GF_PROP_DATA) && (p->type!=GF_PROP_CONST_DATA)) continue;\n\t\tif (!p->value.data.size || !p->value.data.ptr) continue;\n\t\tif (!pname) continue;\n\n\t\tif (!strncmp(pname, \"sai_\", 4)) {\n\n\t\t} else if (!strncmp(pname, \"grp_\", 4)) {\n\t\t\t//discard emsg if fragmented, otherwise add as internal sample group - TODO, support for EventMessage tracks\n\t\t\tif (!strcmp(pname, \"grp_EMSG\") && (ctx->store>=MP4MX_MODE_FRAG)) continue;\n\t\t\tis_sample_group = GF_TRUE;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\n\t\tpname+=4;\n\t\tu32 plen = (u32) strlen(pname);\n\t\tif (plen==3) {\n\t\t\taux_type = GF_4CC(pname[0], pname[1], pname[2], ' ');\n\t\t\tpname+=3;\n\t\t} else if (plen >= 4) {\n\t\t\taux_type = GF_4CC(pname[0], pname[1], pname[2], pname[3]);\n\t\t\tpname+=4;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\t\tif (pname[0] == '_') {\n\t\t\tif (is_sample_group) {\n\t\t\t\tchar *flags = strstr(pname, \"_z\");\n\t\t\t\tif (flags) flags[0]=0;\n\t\t\t\tif (pname[0]) aux_info = atoi(pname);\n\t\t\t\tif (flags) {\n\t\t\t\t\tsscanf(flags+2, \"%x\", &sg_flags);\n\t\t\t\t\tflags[0]='_';\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\taux_info = atoi(pname);\n\t\t\t}\n\t\t}\n\t\tif (!aux_type) continue;\n\n\t\tif (is_sample_group) {\n\t\t\tif (aux_type==GF_ISOM_SAMPLE_GROUP_ESGH) {\n\t\t\t\tGF_Err gf_isom_set_sample_description_restricted(GF_ISOFile *movie, u32 trackNumber, u32 sampleDescIndex, u32 scheme_type);\n\t\t\t\tgf_isom_set_sample_description_restricted(ctx->file, tkw->track_num, tkw->stsd_idx, GF_4CC( 'e', 's', 's', 'g'));\n\n\t\t\t\tsg_flags |= 0x40000000;\n\t\t\t}\n\n\t\t\tgf_isom_set_sample_group_description(ctx->file, tkw->track_num, for_fragment ? 0 : tkw->nb_samples, aux_type, aux_info, p->value.data.ptr, p->value.data.size, sg_flags);\n\n\t\t} else {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_set_sample_aux_info(ctx->file, tkw->track_id, tkw->samples_in_frag, aux_type, aux_info, p->value.data.ptr, p->value.data.size);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_sample_aux_info(ctx->file, tkw->track_num, tkw->nb_samples, aux_type, aux_info, p->value.data.ptr, p->value.data.size);\n\t\t\t}\n\t\t}\n\t}\n\n\ttkw->prev_duration = duration;\n\tif (duration && !for_fragment && !tkw->raw_audio_bytes_per_sample)\n\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, duration);\n\n\tif (ctx->dur.num) {\n\t\tBool abort = GF_FALSE;\n\t\tif (ctx->dur.num>0) {\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\n\t\t\t/*patch to align to old arch */\n\t\t\tif (gf_sys_old_arch_compat()) {\n\t\t\t\tif (tkw->stream_type==GF_STREAM_VISUAL) {\n\t\t\t\t\tmdur = tkw->sample.DTS;\n\t\t\t\t}\n\t\t\t}\n\t\t\t//adjust if shift is below half sec (to take AV delay into account)\n\t\t\t//if larger, we imported from non-0 initial ts, do not compensate\n\t\t\telse if (tkw->ts_shift<tkw->tk_timescale/2) {\n\t\t\t\tmdur += tkw->ts_shift;\n\t\t\t}\n\n\t\t\tif (ctx->importer) {\n\t\t\t\ttkw->prog_done = mdur * ctx->dur.den;\n\t\t\t\ttkw->prog_total =  ((u64)tkw->tk_timescale) * ctx->dur.num;\n\t\t\t}\n\n\t\t\t/*patch to align to old arch */\n\t\t\tif (gf_sys_old_arch_compat()) {\n\t\t\t\tif (gf_timestamp_greater(mdur, tkw->tk_timescale, ctx->dur.num, ctx->dur.den))\n\t\t\t\t\tabort = GF_TRUE;\n\t\t\t} else {\n\t\t\t\tif (gf_timestamp_greater_or_equal(mdur, tkw->tk_timescale, ctx->dur.num, ctx->dur.den))\n\t\t\t\t\tabort = GF_TRUE;\n\t\t\t}\n\t\t} else {\n\t\t\tif ((s32) tkw->nb_samples >= -ctx->dur.num)\n\t\t\t\tabort = GF_TRUE;\n\t\t}\n\n\t\tif (abort) {\n\t\t\tGF_FilterEvent evt;\n\t\t\tGF_FEVT_INIT(evt, GF_FEVT_STOP, tkw->ipid);\n\t\t\tgf_filter_pid_send_event(tkw->ipid, &evt);\n\n\t\t\ttkw->aborted = GF_TRUE;\n\t\t}\n\t} else if (ctx->importer) {\n\t\tif (tkw->nb_frames) {\n\t\t\ttkw->prog_done = tkw->nb_samples + tkw->frame_offset;\n\t\t\ttkw->prog_total = tkw->nb_frames;\n\t\t} else {\n\t\t\tu64 data_offset = gf_filter_pck_get_byte_offset(pck);\n\t\t\tif (data_offset == GF_FILTER_NO_BO) {\n\t\t\t\tdata_offset = tkw->down_bytes;\n\t\t\t}\n\t\t\tif ((data_offset != GF_FILTER_NO_BO) && tkw->down_size) {\n\t\t\t\ttkw->prog_done = data_offset;\n\t\t\t\ttkw->prog_total = tkw->down_size;\n\t\t\t} else {\n\t\t\t\tif (tkw->pid_dur.den && tkw->pid_dur.num) {\n\t\t\t\t\ttkw->prog_done = tkw->sample.DTS * tkw->pid_dur.den;\n\t\t\t\t\ttkw->prog_total = tkw->pid_dur.num * tkw->tk_timescale;\n\t\t\t\t} else {\n\t\t\t\t\ttkw->prog_done = 0;\n\t\t\t\t\ttkw->prog_total = 1;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "func": "static GF_Err mp4_mux_process_sample(GF_MP4MuxCtx *ctx, TrackWriter *tkw, GF_FilterPacket *pck, Bool for_fragment)\n{\n\tGF_Err e=GF_OK;\n\tu64 cts, prev_dts;\n\tu32 prev_size=0;\n\tu32 duration = 0;\n\tu32 timescale = 0;\n\tconst GF_PropertyValue *subs;\n\tGF_FilterSAPType sap_type;\n\tu32 insert_subsample_dsi_size = 0;\n\tu32 first_nal_is_audelim = GF_FALSE;\n\tu32 sample_desc_index = tkw->stsd_idx;\n\tBool sample_timing_ok = GF_TRUE;\n\n\ttimescale = gf_filter_pck_get_timescale(pck);\n\n\tprev_dts = tkw->nb_samples ? tkw->sample.DTS : GF_FILTER_NO_TS;\n\tprev_size = tkw->sample.dataLength;\n\ttkw->sample.CTS_Offset = 0;\n\tif (gf_filter_pck_get_frame_interface(pck)) {\n\t\ttkw->dgl_copy = gf_filter_pck_dangling_copy(pck, tkw->dgl_copy);\n\t\tif (!tkw->dgl_copy) return GF_IO_ERR;\n\t\ttkw->sample.data = (char *)gf_filter_pck_get_data(tkw->dgl_copy, &tkw->sample.dataLength);\n\t} else {\n\t\ttkw->sample.data = (char *)gf_filter_pck_get_data(pck, &tkw->sample.dataLength);\n\t}\n\n\tctx->update_report = GF_TRUE;\n\tctx->total_bytes_in += tkw->sample.dataLength;\n\tctx->total_samples++;\n\n\ttkw->sample.DTS = gf_filter_pck_get_dts(pck);\n\tcts = gf_filter_pck_get_cts(pck);\n\n\tif (tkw->sample.DTS == GF_FILTER_NO_TS) {\n\t\tif (cts == GF_FILTER_NO_TS) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Sample with no DTS/CTS, cannot add (last DTS \"LLU\", last size %d)\\n\", prev_dts, prev_size ));\n\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t} else {\n\t\t\tu32 min_pck_dur = gf_filter_pid_get_min_pck_duration(tkw->ipid);\n\t\t\tif (min_pck_dur) {\n\t\t\t\ttkw->sample.DTS = prev_dts;\n\t\t\t\t//transform back to inpput timescale\n\t\t\t\tif (timescale != tkw->tk_timescale) {\n\t\t\t\t\ttkw->sample.DTS = gf_timestamp_rescale(tkw->sample.DTS, tkw->tk_timescale, timescale);\n\t\t\t\t}\n\t\t\t\ttkw->sample.DTS += min_pck_dur;\n\t\t\t} else {\n\t\t\t\ttkw->sample.DTS = cts;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttkw->sample.CTS_Offset = (s32) ((s64) cts - (s64) tkw->sample.DTS);\n\t}\n\n\t//do our best to patch init ts if timing config aborted\n\tif (tkw->si_min_ts_plus_one) {\n\t\tu64 si_min_ts = tkw->si_min_ts_plus_one - 1;\n\t\ttkw->si_min_ts_plus_one = 0;\n\t\ttkw->ts_shift = tkw->sample.DTS;\n\t\tmp4_mux_update_init_edit(ctx, tkw, si_min_ts, GF_FALSE);\n\t}\n\t//tkw->ts_shift is in source timescale, apply it before rescaling TSs/duration\n\tif (tkw->ts_shift) {\n\t\tif (ctx->is_rewind) {\n\t\t\tif (tkw->sample.DTS <= tkw->ts_shift) {\n\t\t\t\ttkw->sample.DTS = tkw->ts_shift - tkw->sample.DTS;\n\t\t\t\tcts = tkw->ts_shift - cts;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] broken timing in track, initial ts \"LLU\" less than TS \"LLU\"\\n\", tkw->ts_shift, tkw->sample.DTS));\n\t\t\t\tsample_timing_ok = GF_FALSE;\n\t\t\t}\n\t\t} else {\n\t\t\tif (tkw->sample.DTS >= tkw->ts_shift) {\n\t\t\t\ttkw->sample.DTS -= tkw->ts_shift;\n\t\t\t\tcts -= tkw->ts_shift;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] broken timing in track, initial ts \"LLU\" greater than TS \"LLU\"\\n\", tkw->ts_shift, tkw->sample.DTS));\n\t\t\t\tsample_timing_ok = GF_FALSE;\n\t\t\t}\n\t\t}\n\t}\n\n\t//sample-accurate seek info, start logging min CTS of packets marked as non-sync\n\tif (tkw->check_seek_ts && !gf_filter_pck_get_seek_flag(pck)) {\n\t\tu64 ts_check = cts;\n\t\tsubs = gf_filter_pck_get_property(pck, GF_PROP_PCK_SKIP_BEGIN);\n\t\tif (subs)\n\t\t\tts_check += subs->value.uint;\n\n\t\tif (!tkw->min_ts_seek_plus_one) {\n\t\t\ttkw->min_ts_seek_plus_one = ts_check + 1;\n\t\t} else if (tkw->min_ts_seek_plus_one > ts_check + 1) {\n\t\t\ttkw->min_ts_seek_plus_one = ts_check + 1;\n\t\t} else {\n\t\t\t//TS is greater than last non-seek packet TS, we're done seeking\n\t\t\ttkw->check_seek_ts = GF_FALSE;\n\t\t}\n\t}\n\n\tduration = gf_filter_pck_get_duration(pck);\n\tif (timescale != tkw->tk_timescale) {\n\t\ts64 ctso;\n\t\ttkw->sample.DTS = gf_timestamp_rescale(tkw->sample.DTS, timescale, tkw->tk_timescale);\n\n\t\tctso = (s64) tkw->sample.CTS_Offset;\n\t\tctso *= tkw->tk_timescale;\n\t\tctso /= timescale;\n\t\ttkw->sample.CTS_Offset = (s32) ctso;\n\t\tduration *= tkw->tk_timescale;\n\t\tduration /= timescale;\n\n\t\tif (cts != GF_FILTER_NO_TS) {\n\t\t\tcts = gf_timestamp_rescale(cts, timescale, tkw->tk_timescale);\n\t\t}\n\t}\n\n\ttkw->sample.IsRAP = 0;\n\tif (tkw->codecid==GF_CODECID_RAW) {\n\t\tsap_type = GF_FILTER_SAP_1;\n\t} else {\n\t\tsap_type = mp4_mux_get_sap(ctx, pck);\n\n\t\t//if pps inband mode is used, turn sap3 into sap1\n\t\tif ((tkw->xps_inband==XPS_IB_PPS) && sap_type==GF_FILTER_SAP_3)\n\t\t\tsap_type=GF_FILTER_SAP_1;\n\t}\n\tif (sap_type==GF_FILTER_SAP_1)\n\t\ttkw->sample.IsRAP = SAP_TYPE_1;\n\telse if (sap_type==GF_FILTER_SAP_2)\n\t\ttkw->sample.IsRAP = SAP_TYPE_2;\n\telse if ( (sap_type == GF_FILTER_SAP_4) && (tkw->stream_type != GF_STREAM_VISUAL) )\n\t\ttkw->sample.IsRAP = SAP_TYPE_1;\n\n\t/*RFC8216bis is not clear here:\n\t\"if the Partial Segment contains an independent frame.\"\n\t\t-> this would allow SAP1,2,3 (independent being only defined for segments)\n\n\tbut\n\n\t\"Partial Segment containing an independent frame SHOULD carry it to increase the efficiency with which clients can join and switch Renditions\"\n\t\t-> if used for switching, this only allows SAP 1 and 2\n\n\tSpec should be fixed to allow for both cases (fast tune-in or in-segment switching)\n\t*/\n\tif ((tkw->sample.IsRAP == SAP_TYPE_1) || (tkw->sample.IsRAP == SAP_TYPE_2))\n\t\tctx->frag_has_intra = GF_TRUE;\n\n\ttkw->sample.DTS += tkw->dts_patch;\n\tif (tkw->nb_samples && (prev_dts >= tkw->sample.DTS) ) {\n\t\t//the fragmented API will patch the duration on the fly\n\t\tif (!for_fragment && ctx->patch_dts) {\n\t\t\tgf_isom_patch_last_sample_duration(ctx->file, tkw->track_num, prev_dts ? prev_dts : 1);\n\t\t}\n\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] PID %s ID %d Sample %d with DTS \"LLU\" less than previous sample DTS \"LLU\", patching DTS%s\\n\", gf_filter_pid_get_name(tkw->ipid), tkw->track_id, tkw->nb_samples+1, tkw->sample.DTS, prev_dts, ctx->patch_dts ? \"and adjusting prev sample duration\" : \"\" ));\n\t\tsample_timing_ok = GF_FALSE;\n\n\t\tif (prev_dts) {\n\t\t\ttkw->dts_patch = prev_dts - tkw->sample.DTS;\n\t\t\ttkw->sample.DTS += tkw->dts_patch+1; //+1 to avoid 0-dur samples\n\t\t} else {\n\t\t\ttkw->sample.DTS += 1;\n\t\t\tif (tkw->sample.CTS_Offset) tkw->sample.CTS_Offset -= 1;\n\t\t\tduration-=1;\n\t\t}\n\t}\n\n\n\tif (tkw->negctts_shift)\n\t\ttkw->sample.CTS_Offset -= tkw->negctts_shift;\n\n\tif (sample_timing_ok) {\n\t\tif (tkw->probe_min_ctts) {\n\t\t\ts32 diff = (s32) ((s64) cts - (s64) tkw->sample.DTS);\n\t\t\tif (diff < tkw->min_neg_ctts)\n\t\t\t\ttkw->min_neg_ctts = diff;\n\t\t}\n\t\tif (tkw->sample.CTS_Offset) tkw->has_ctts = GF_TRUE;\n\n\t\tif (tkw->sample.CTS_Offset < tkw->min_neg_ctts)\n\t\t\ttkw->min_neg_ctts = tkw->sample.CTS_Offset;\n\t}\n\n\ttkw->sample.nb_pack = 0;\n\tif (tkw->raw_audio_bytes_per_sample) {\n\t\ttkw->sample.nb_pack = tkw->sample.dataLength / tkw->raw_audio_bytes_per_sample;\n\t\tif (tkw->sample.nb_pack) {\n\t\t\tduration = 1;\n\t\t\tif (tkw->raw_samplerate && (tkw->tk_timescale != tkw->raw_samplerate)) {\n\t\t\t\tduration *= tkw->tk_timescale;\n\t\t\t\tduration /= tkw->raw_samplerate;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tkw->cenc_state && tkw->clear_stsd_idx && !gf_filter_pck_get_crypt_flags(pck)) {\n\t\tsample_desc_index = tkw->clear_stsd_idx;\n\t}\n\n\tif (tkw->sparse_inject && (prev_dts!=GF_FILTER_NO_TS) && (tkw->sample.DTS!=GF_FILTER_NO_TS) && tkw->prev_duration) {\n\t\tu64 est_time = prev_dts + tkw->prev_duration;\n\t\tif (est_time < tkw->sample.DTS) {\n\t\t\tu32 ins_dur;\n\t\t\tGF_ISOSample s;\n\t\t\tmemset(&s, 0, sizeof(GF_ISOSample));\n\t\t\ts.DTS = est_time;\n\n\t\t\ts.IsRAP = SAP_TYPE_1;\n\t\t\tins_dur = (u32) (tkw->sample.DTS - est_time);\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_add_sample(ctx->file, tkw->track_id, &s, tkw->stsd_idx, ins_dur, 0, 0, 0);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, tkw->stsd_idx, &s);\n\t\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, ins_dur);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tkw->use_dref) {\n\t\tu64 data_offset = gf_filter_pck_get_byte_offset(pck);\n\t\tif (data_offset != GF_FILTER_NO_BO) {\n\t\t\te = gf_isom_add_sample_reference(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample, data_offset);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to add sample DTS \"LLU\" from %s as reference: %s\\n\", tkw->sample.DTS, gf_filter_pid_get_name(tkw->ipid), gf_error_to_string(e) ));\n\t\t\t}\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Cannot add sample reference at DTS \"LLU\" , input sample data is not continous in source\\n\", tkw->sample.DTS ));\n\t\t}\n\t} else if (tkw->nb_frames_per_sample && (tkw->nb_samples % tkw->nb_frames_per_sample)) {\n\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t \te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, tkw->sample.data, tkw->sample.dataLength, 0);\n#else\n\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t} else {\n\t\t\te = gf_isom_append_sample_data(ctx->file, tkw->track_num, tkw->sample.data, tkw->sample.dataLength);\n\t\t}\n\t\ttkw->has_append = GF_TRUE;\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to append sample DTS \"LLU\" data: %s\\n\", tkw->sample.DTS, gf_error_to_string(e) ));\n\t\t}\n\t} else {\n\t\tBool inject_pps = ctx->pps_inband;\n\t\tif (ctx->xps_inband==XPS_IB_AUTO) {\n\t\t\tconst GF_PropertyValue *p = gf_filter_pck_get_property(pck, GF_PROP_PCK_XPS_MASK);\n\t\t\tif (p && (p->value.uint & (1<<2) ) )\n\t\t\t\tinject_pps = GF_TRUE;\n\t\t}\n\n\t\tif ((tkw->sample.IsRAP || tkw->force_inband_inject || inject_pps) && tkw->xps_inband) {\n\t\t\tu8 *inband_xps;\n\t\t\tu32 inband_xps_size;\n\t\t\tchar *au_delim=NULL;\n\t\t\tu32 au_delim_size=0;\n\t\t\tchar *pck_data = tkw->sample.data;\n\t\t\tu32 pck_data_len = tkw->sample.dataLength;\n\t\t\tif (tkw->sample.IsRAP || tkw->force_inband_inject) {\n\t\t\t\tinband_xps = tkw->inband_hdr;\n\t\t\t\tinband_xps_size = tkw->inband_hdr_size;\n\t\t\t\ttkw->force_inband_inject = GF_FALSE;\n\t\t\t} else {\n\t\t\t\tinband_xps = tkw->inband_hdr_non_rap;\n\t\t\t\tinband_xps_size = tkw->inband_hdr_non_rap_size;\n\t\t\t}\n\t\t\ttkw->sample.data = inband_xps;\n\t\t\ttkw->sample.dataLength = inband_xps_size;\n\n\t\t\tif (tkw->is_nalu==NALU_AVC) {\n\t\t\t\tif (pck_data_len >= 2 + tkw->nal_unit_size) {\n\t\t\t\t\tchar *nal = pck_data + tkw->nal_unit_size;\n\t\t\t\t\tif ((nal[0] & 0x1F) == GF_AVC_NALU_ACCESS_UNIT) {\n\t\t\t\t\t\tfirst_nal_is_audelim = au_delim_size = 2 + tkw->nal_unit_size;\n\t\t\t\t\t\tau_delim = pck_data;\n\t\t\t\t\t\tif (au_delim_size >= pck_data_len) au_delim = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (pck_data_len >= 3 + tkw->nal_unit_size) {\n\t\t\t\t\tchar *nal = pck_data + tkw->nal_unit_size;\n\t\t\t\t\tif (((nal[0] & 0x7E)>>1) == GF_HEVC_NALU_ACCESS_UNIT) {\n\t\t\t\t\t\tfirst_nal_is_audelim = au_delim_size = 3 + tkw->nal_unit_size;\n\t\t\t\t\t\tau_delim = pck_data;\n\t\t\t\t\t\tif (au_delim_size >= pck_data_len) au_delim = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (au_delim) {\n\t\t\t\ttkw->sample.data = au_delim;\n\t\t\t\ttkw->sample.dataLength = au_delim_size;\n\t\t\t\tpck_data += au_delim_size;\n\t\t\t\tpck_data_len -= au_delim_size;\n\t\t\t}\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t//force using ref mode\n\t\t\t\tvoid *ref=NULL;\n\t\t\t\te = gf_isom_fragment_add_sample_ex(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0, &ref, 0);\n\t\t\t\tif (!e && au_delim) {\n\t\t\t\t\te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, inband_xps, inband_xps_size, 0);\n\t\t\t\t}\n\t\t\t\tif (!e) {\n\t\t\t\t\tif (gf_filter_pck_is_blocking_ref(pck)) {\n\t\t\t\t\t\te = gf_isom_fragment_append_data(ctx->file, tkw->track_id, pck_data, pck_data_len, 0);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgf_filter_pck_ref(&pck);\n\t\t\t\t\t\tGF_FilterPacket *ref = pck;\n\t\t\t\t\t\te = gf_isom_fragment_append_data_ex(ctx->file, tkw->track_id, pck_data, pck_data_len, 0, (void**)&ref, au_delim ? au_delim_size : 0);\n\t\t\t\t\t\tif (!ref) {\n\t\t\t\t\t\t\tgf_list_add(ctx->ref_pcks, pck);\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tgf_filter_pck_unref(pck);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif // GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t} else {\n\t\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample);\n\t\t\t\tif (au_delim && !e) {\n\t\t\t\t\te = gf_isom_append_sample_data(ctx->file, tkw->track_num, inband_xps, inband_xps_size);\n\t\t\t\t}\n\t\t\t\tif (!e) e = gf_isom_append_sample_data(ctx->file, tkw->track_num, pck_data, pck_data_len);\n\t\t\t}\n\t\t\tinsert_subsample_dsi_size = inband_xps_size;\n\t\t} else if (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\tif (gf_filter_pck_is_blocking_ref(pck)) {\n\t\t\t\te = gf_isom_fragment_add_sample(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0);\n\t\t\t} else {\n\t\t\t\tgf_filter_pck_ref(&pck);\n\t\t\t\tGF_FilterPacket *ref = pck;\n\t\t\t\te = gf_isom_fragment_add_sample_ex(ctx->file, tkw->track_id, &tkw->sample, sample_desc_index, duration, 0, 0, 0, (void**) &ref, 0);\n\t\t\t\tif (!ref) {\n\t\t\t\t\tgf_list_add(ctx->ref_pcks, pck);\n\t\t\t\t} else {\n\t\t\t\t\tgf_filter_pck_unref(pck);\n\t\t\t\t}\n\t\t\t}\n#else\n\t\t\te = GF_NOT_SUPPORTED;\n#endif // GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t} else {\n\t\t\te = gf_isom_add_sample(ctx->file, tkw->track_num, sample_desc_index, &tkw->sample);\n\t\t\tif (!e && !duration) {\n\t\t\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, 0);\n\t\t\t}\n\t\t}\n\n\t\tif (e) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to add sample DTS \"LLU\" from %s - prev DTS \"LLU\": %s\\n\", tkw->sample.DTS, gf_filter_pid_get_name(tkw->ipid), prev_dts, gf_error_to_string(e) ));\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_CONTAINER, (\"[MP4Mux] added sample DTS \"LLU\" - prev DTS \"LLU\" - prev size %d\\n\", tkw->sample.DTS, prev_dts, prev_size));\n\t\t}\n\n\t\tif (!e && tkw->cenc_state) {\n\t\t\te = mp4_mux_cenc_update(ctx, tkw, pck, for_fragment ? CENC_ADD_FRAG : CENC_ADD_NORMAL, tkw->sample.dataLength, insert_subsample_dsi_size);\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample CENC information: %s\\n\", gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t}\n\n\ttkw->nb_samples++;\n\ttkw->samples_in_stsd++;\n\ttkw->samples_in_frag++;\n\n\tif (e) return e;\n\n\tif (!for_fragment && sample_timing_ok) {\n\t\tu64 samp_cts;\n\t\tif (!tkw->clamp_ts_plus_one) {\n\t\t\tconst GF_PropertyValue *skp = gf_filter_pck_get_property(pck, GF_PROP_PCK_SKIP_PRES);\n\t\t\tif (skp && skp->value.boolean) {\n\t\t\t\ttkw->clamp_ts_plus_one = 1 + tkw->sample.DTS + tkw->sample.CTS_Offset;\n\t\t\t}\n\t\t}\n\t\t//store min max cts for edit list updates\n\t\tsamp_cts = tkw->sample.DTS + tkw->sample.CTS_Offset;\n\t\tif (!tkw->clamp_ts_plus_one || (samp_cts + 1 < tkw->clamp_ts_plus_one)) {\n\t\t\tif (samp_cts > tkw->max_cts) {\n\t\t\t\ttkw->max_cts = samp_cts;\n\t\t\t\ttkw->max_cts_samp_dur = duration;\n\t\t\t}\n\n\t\t\tif (tkw->min_cts > samp_cts)\n\t\t\t\ttkw->min_cts = samp_cts;\n\t\t}\n\t}\n\n\t//compat with old arch: write sample to group info for all samples\n\tif ((sap_type==3) || tkw->has_open_gop)  {\n\t\tif (!ctx->norap) {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_set_sample_rap_group(ctx->file, tkw->track_id, tkw->samples_in_frag, (sap_type==3) ? GF_TRUE : GF_FALSE, 0);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else if (sap_type==3) {\n\t\t\t\te = gf_isom_set_sample_rap_group(ctx->file, tkw->track_num, tkw->nb_samples, GF_TRUE /*(sap_type==3) ? GF_TRUE : GF_FALSE*/, 0);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample DTS \"LLU\" SAP 3 in RAP group: %s\\n\", tkw->sample.DTS, gf_error_to_string(e) ));\n\t\t\t}\n\t\t}\n\t\ttkw->has_open_gop = GF_TRUE;\n\t}\n\tif (!ctx->noroll) {\n\t\tif ((sap_type==GF_FILTER_SAP_4) || (sap_type==GF_FILTER_SAP_4_PROL) || tkw->gdr_type) {\n\t\t\tGF_ISOSampleRollType roll_type = 0;\n\t\t\ts16 roll = gf_filter_pck_get_roll_info(pck);\n\t\t\tif (sap_type==GF_FILTER_SAP_4) roll_type = GF_ISOM_SAMPLE_ROLL;\n\t\t\telse if (sap_type==GF_FILTER_SAP_4_PROL) roll_type = GF_ISOM_SAMPLE_PREROLL;\n\t\t\telse if (tkw->gdr_type==GF_FILTER_SAP_4_PROL) {\n\t\t\t\troll_type = GF_ISOM_SAMPLE_PREROLL_NONE;\n\t\t\t}\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\te = gf_isom_fragment_set_sample_roll_group(ctx->file, tkw->track_id, tkw->samples_in_frag, roll_type, roll);\n#else\n\t\t\t\te = GF_NOT_SUPPORTED;\n#endif\n\t\t\t} else {\n\t\t\t\te = gf_isom_set_sample_roll_group(ctx->file, tkw->track_num, tkw->nb_samples, roll_type, roll);\n\t\t\t}\n\t\t\tif (e) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] Failed to set sample DTS \"LLU\" SAP 4 roll %s in roll group: %s\\n\", tkw->sample.DTS, roll, gf_error_to_string(e) ));\n\t\t\t}\n\t\t\tif (sap_type && !tkw->gdr_type)\n\t\t\t\ttkw->gdr_type = sap_type;\n\t\t}\n\t}\n\n\tsubs = gf_filter_pck_get_property(pck, GF_PROP_PCK_SUBS);\n\tif (subs) {\n\t\t//if no AUDelim nal and inband header injection, push new subsample\n\t\tif (!first_nal_is_audelim && insert_subsample_dsi_size) {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, insert_subsample_dsi_size, 0, 0, 0);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, 0, insert_subsample_dsi_size, 0, 0, 0);\n\t\t\t}\n\t\t\tinsert_subsample_dsi_size = 0;\n\t\t}\n\t\ttkw->has_subs = GF_TRUE;\n\n\t\tif (!ctx->bs_r) ctx->bs_r = gf_bs_new(subs->value.data.ptr, subs->value.data.size, GF_BITSTREAM_READ);\n\t\telse gf_bs_reassign_buffer(ctx->bs_r, subs->value.data.ptr, subs->value.data.size);\n\n\t\twhile (gf_bs_available(ctx->bs_r)) {\n\t\t\tu32 flags = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu32 subs_size = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu32 reserved = gf_bs_read_u32(ctx->bs_r);\n\t\t\tu8 priority = gf_bs_read_u8(ctx->bs_r);\n\t\t\tu8 discardable = gf_bs_read_u8(ctx->bs_r);\n\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, flags, subs_size, priority, reserved, discardable);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, flags, subs_size, priority, reserved, discardable);\n\t\t\t}\n\n\t\t\t//we have AUDelim nal and inband header injection, push new subsample for inband header once we have pushed the first subsample (au delim)\n\t\t\tif (insert_subsample_dsi_size) {\n\t\t\t\tif (first_nal_is_audelim != subs_size) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_CONTAINER, (\"[MP4Mux] inserting inband param after AU delimiter NALU, but sample has subsample information not aligned on NALU (got %d subsample size but expecting %d) - file might be broken!\\n\", subs_size, first_nal_is_audelim));\n\t\t\t\t}\n\t\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, insert_subsample_dsi_size, 0, 0, 0);\n#endif\n\t\t\t\t} else {\n\t\t\t\t\tgf_isom_add_subsample(ctx->file, tkw->track_num, tkw->nb_samples, 0, insert_subsample_dsi_size, 0, 0, 0);\n\t\t\t\t}\n\t\t\t\tinsert_subsample_dsi_size = GF_FALSE;\n\t\t\t}\n\t\t}\n\t} else if (for_fragment && tkw->has_subs && ctx->cmaf && (tkw->codecid==GF_CODECID_SUBS_XML)) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t//tentative implemntation of CMAF 7.5.20 which is just nonsense text !!:\n\t\t//\"the value of subsample_count shall equal 1 for the first image sub-sample, and the subsample_count of the TTML document shall equal 0.\"\n\t\t//\n\t\t//we simply signal a single subsample\n\t\tgf_isom_fragment_add_subsample(ctx->file, tkw->track_id, 0, tkw->sample.dataLength, 0, 0, 0);\n#endif\n\t}\n\n\tif (ctx->deps) {\n\t\tu8 dep_flags = gf_filter_pck_get_dependency_flags(pck);\n\t\tif (dep_flags) {\n\t\t\tu32 is_leading = (dep_flags>>6) & 0x3;\n\t\t\tu32 depends_on = (dep_flags>>4) & 0x3;\n\t\t\tu32 depended_on = (dep_flags>>2) & 0x3;\n\t\t\tu32 redundant = (dep_flags) & 0x3;\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_set_sample_flags(ctx->file, tkw->track_id, is_leading, depends_on, depended_on, redundant);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_set_sample_flags(ctx->file, tkw->track_num, tkw->nb_samples, is_leading, depends_on, depended_on, redundant);\n\t\t\t}\n\t\t}\n\t}\n\n\tu32 idx = 0;\n\twhile (1) {\n\t\tBool is_sample_group=GF_FALSE;\n\t\tu32 aux_type=0, aux_info=0, sg_flags=0;\n\t\tu32 p4cc;\n\t\tconst char *pname=NULL;\n\t\tconst GF_PropertyValue *p = gf_filter_pck_enum_properties(pck, &idx, &p4cc, &pname);\n\t\tif (!p) break;\n\t\tif ((p->type!=GF_PROP_DATA) && (p->type!=GF_PROP_CONST_DATA)) continue;\n\t\tif (!p->value.data.size || !p->value.data.ptr) continue;\n\t\tif (!pname) continue;\n\n\t\tif (!strncmp(pname, \"sai_\", 4)) {\n\n\t\t} else if (!strncmp(pname, \"grp_\", 4)) {\n\t\t\t//discard emsg if fragmented, otherwise add as internal sample group - TODO, support for EventMessage tracks\n\t\t\tif (!strcmp(pname, \"grp_EMSG\") && (ctx->store>=MP4MX_MODE_FRAG)) continue;\n\t\t\tis_sample_group = GF_TRUE;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\n\t\tpname+=4;\n\t\tu32 plen = (u32) strlen(pname);\n\t\tif (plen==3) {\n\t\t\taux_type = GF_4CC(pname[0], pname[1], pname[2], ' ');\n\t\t\tpname+=3;\n\t\t} else if (plen >= 4) {\n\t\t\taux_type = GF_4CC(pname[0], pname[1], pname[2], pname[3]);\n\t\t\tpname+=4;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\t\tif (pname[0] == '_') {\n\t\t\tif (is_sample_group) {\n\t\t\t\tchar *flags = strstr(pname, \"_z\");\n\t\t\t\tif (flags) flags[0]=0;\n\t\t\t\tif (pname[0]) aux_info = atoi(pname);\n\t\t\t\tif (flags) {\n\t\t\t\t\tsscanf(flags+2, \"%x\", &sg_flags);\n\t\t\t\t\tflags[0]='_';\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\taux_info = atoi(pname);\n\t\t\t}\n\t\t}\n\t\tif (!aux_type) continue;\n\n\t\tif (is_sample_group) {\n\t\t\tif (aux_type==GF_ISOM_SAMPLE_GROUP_ESGH) {\n\t\t\t\tGF_Err gf_isom_set_sample_description_restricted(GF_ISOFile *movie, u32 trackNumber, u32 sampleDescIndex, u32 scheme_type);\n\t\t\t\tgf_isom_set_sample_description_restricted(ctx->file, tkw->track_num, tkw->stsd_idx, GF_4CC( 'e', 's', 's', 'g'));\n\n\t\t\t\tsg_flags |= 0x40000000;\n\t\t\t}\n\n\t\t\tgf_isom_set_sample_group_description(ctx->file, tkw->track_num, for_fragment ? 0 : tkw->nb_samples, aux_type, aux_info, p->value.data.ptr, p->value.data.size, sg_flags);\n\n\t\t} else {\n\t\t\tif (for_fragment) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\tgf_isom_fragment_set_sample_aux_info(ctx->file, tkw->track_id, tkw->samples_in_frag, aux_type, aux_info, p->value.data.ptr, p->value.data.size);\n#endif\n\t\t\t} else {\n\t\t\t\tgf_isom_add_sample_aux_info(ctx->file, tkw->track_num, tkw->nb_samples, aux_type, aux_info, p->value.data.ptr, p->value.data.size);\n\t\t\t}\n\t\t}\n\t}\n\n\ttkw->prev_duration = duration;\n\tif (duration && !for_fragment && !tkw->raw_audio_bytes_per_sample)\n\t\tgf_isom_set_last_sample_duration(ctx->file, tkw->track_num, duration);\n\n\tif (ctx->dur.num) {\n\t\tBool abort = GF_FALSE;\n\t\tif (ctx->dur.num>0) {\n\t\t\tu64 mdur = gf_isom_get_media_duration(ctx->file, tkw->track_num);\n\n\t\t\t/*patch to align to old arch */\n\t\t\tif (gf_sys_old_arch_compat()) {\n\t\t\t\tif (tkw->stream_type==GF_STREAM_VISUAL) {\n\t\t\t\t\tmdur = tkw->sample.DTS;\n\t\t\t\t}\n\t\t\t}\n\t\t\t//adjust if shift is below half sec (to take AV delay into account)\n\t\t\t//if larger, we imported from non-0 initial ts, do not compensate\n\t\t\telse if (tkw->ts_shift<tkw->tk_timescale/2) {\n\t\t\t\tmdur += tkw->ts_shift;\n\t\t\t}\n\n\t\t\tif (ctx->importer) {\n\t\t\t\ttkw->prog_done = mdur * ctx->dur.den;\n\t\t\t\ttkw->prog_total =  ((u64)tkw->tk_timescale) * ctx->dur.num;\n\t\t\t}\n\n\t\t\t/*patch to align to old arch */\n\t\t\tif (gf_sys_old_arch_compat()) {\n\t\t\t\tif (gf_timestamp_greater(mdur, tkw->tk_timescale, ctx->dur.num, ctx->dur.den))\n\t\t\t\t\tabort = GF_TRUE;\n\t\t\t} else {\n\t\t\t\tif (gf_timestamp_greater_or_equal(mdur, tkw->tk_timescale, ctx->dur.num, ctx->dur.den))\n\t\t\t\t\tabort = GF_TRUE;\n\t\t\t}\n\t\t} else {\n\t\t\tif ((s32) tkw->nb_samples >= -ctx->dur.num)\n\t\t\t\tabort = GF_TRUE;\n\t\t}\n\n\t\tif (abort) {\n\t\t\tGF_FilterEvent evt;\n\t\t\tGF_FEVT_INIT(evt, GF_FEVT_STOP, tkw->ipid);\n\t\t\tgf_filter_pid_send_event(tkw->ipid, &evt);\n\n\t\t\ttkw->aborted = GF_TRUE;\n\t\t}\n\t} else if (ctx->importer) {\n\t\tif (tkw->nb_frames) {\n\t\t\ttkw->prog_done = tkw->nb_samples + tkw->frame_offset;\n\t\t\ttkw->prog_total = tkw->nb_frames;\n\t\t} else {\n\t\t\tu64 data_offset = gf_filter_pck_get_byte_offset(pck);\n\t\t\tif (data_offset == GF_FILTER_NO_BO) {\n\t\t\t\tdata_offset = tkw->down_bytes;\n\t\t\t}\n\t\t\tif ((data_offset != GF_FILTER_NO_BO) && tkw->down_size) {\n\t\t\t\ttkw->prog_done = data_offset;\n\t\t\t\ttkw->prog_total = tkw->down_size;\n\t\t\t} else {\n\t\t\t\tif (tkw->pid_dur.den && tkw->pid_dur.num) {\n\t\t\t\t\ttkw->prog_done = tkw->sample.DTS * tkw->pid_dur.den;\n\t\t\t\t\ttkw->prog_total = tkw->pid_dur.num * tkw->tk_timescale;\n\t\t\t\t} else {\n\t\t\t\t\ttkw->prog_done = 0;\n\t\t\t\t\ttkw->prog_total = 1;\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n\treturn GF_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -439,7 +439,7 @@\n \t\t\t\ttkw->gdr_type = sap_type;\n \t\t}\n \t}\n-\t\n+\n \tsubs = gf_filter_pck_get_property(pck, GF_PROP_PCK_SUBS);\n \tif (subs) {\n \t\t//if no AUDelim nal and inband header injection, push new subsample",
        "diff_line_info": {
            "deleted_lines": [
                "\t"
            ],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-5595",
        "func_name": "gpac/mp4_mux_config_timing",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.3.0-DEV.",
        "git_url": "https://github.com/gpac/gpac/commit/7a6f636db3360bb16d18078d51e8c596f31302a1",
        "commit_title": "fixes #2633 - 3 segv + memleak",
        "commit_text": "",
        "func_before": "static void mp4_mux_config_timing(GF_MP4MuxCtx *ctx)\n{\n\tif ((ctx->store>=MP4MX_MODE_FRAG) && !ctx->tsalign) {\n\t\tctx->config_timing = GF_FALSE;\n\t\treturn;\n\t}\n\tGF_List *services = gf_list_new();\n\tu32 i, count;\n\tBool not_ready, blocking_refs, has_ready;\n\nretry_all:\n\tcount = gf_list_count(ctx->tracks);\n\tnot_ready = GF_FALSE;\n\tblocking_refs = GF_FALSE;\n\thas_ready = GF_FALSE;\n\n\tfor (i=0; i<gf_list_count(services);i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\tsi->nb_non_sparse = si->nb_non_sparse_ready = 0;\n\t\tsi->nb_sparse = si->nb_sparse_ready = 0;\n\t}\n\n\t//compute min dts of first packet on each track - this assume all tracks are synchronized, might need adjustment for MPEG4 Systems\n\tfor (i=0; i<count; i++) {\n\t\tu64 ts, dts_min;\n\t\tGF_FilterPacket *pck;\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\t\tif (tkw->fake_track) continue;\n\t\t//get associated service\n\t\tstruct _service_info *si = get_service_info(services, tkw);\n\n\t\t//already setup (happens when new PIDs are declared after a packet has already been written on other PIDs)\n\t\tif (tkw->nb_samples) {\n\t\t\tdts_min = gf_timestamp_rescale(tkw->ts_shift, tkw->src_timescale, 1000000);\n\n\t\t\tif (si->first_ts_min > dts_min) {\n\t\t\t\tsi->first_ts_min = (u64) dts_min;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\nretry_pck:\n\t\tpck = gf_filter_pid_get_packet(tkw->ipid);\n\t\t//check this after fetching a packet since it may reconfigure the track\n\t\tif (!tkw->track_num) {\n\t\t\tif (gf_filter_pid_is_eos(tkw->ipid)) {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] PID has no input packet and configuration not known after 10 retries, aborting initial timing sync\\n\"));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnot_ready = GF_TRUE;\n\t\t\ttkw->ts_shift = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pck) {\n\t\t\tif (gf_filter_pck_is_blocking_ref(pck))\n\t\t\t\tblocking_refs = GF_TRUE;\n\t\t\tif (tkw->wait_sap) {\n\t\t\t\tGF_FilterSAPType sap = gf_filter_pck_get_sap(pck);\n\t\t\t\tBool seek = gf_filter_pck_get_seek_flag(pck);\n\t\t\t\tif (seek || !sap) {\n\t\t\t\t\tgf_filter_pid_drop_packet(tkw->ipid);\n\t\t\t\t\tgoto retry_pck;\n\t\t\t\t} else {\n\t\t\t\t\ttkw->wait_sap = GF_FALSE;\n\n\t\t\t\t\tif (!ctx->wait_dts_plus_one) {\n\t\t\t\t\t\tctx->wait_dts_plus_one = 1 + gf_filter_pck_get_dts(pck);\n\t\t\t\t\t\tctx->wait_dts_timescale = tkw->src_timescale;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (ctx->wait_dts_plus_one) {\n\t\t\t\tts = gf_filter_pck_get_dts(pck);\n\t\t\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\t\t\tts = gf_filter_pck_get_cts(pck);\n\t\t\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\t\t\tts=0;\n\n\t\t\t\tif (gf_timestamp_less(ts, tkw->src_timescale, (ctx->wait_dts_plus_one-1), ctx->wait_dts_timescale)) {\n\t\t\t\t\tgf_filter_pid_drop_packet(tkw->ipid);\n\t\t\t\t\tgoto retry_pck;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tswitch (tkw->stream_type) {\n\t\tcase GF_STREAM_VISUAL:\n\t\tcase GF_STREAM_AUDIO:\n\t\t\tsi->nb_non_sparse++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tsi->nb_sparse++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!pck) {\n\t\t\t//eos (wether real or flush event), setup cenc\n\t\t\tif (gf_filter_pid_is_eos(tkw->ipid)) {\n\t\t\t\tif (tkw->cenc_state==CENC_NEED_SETUP)\n\t\t\t\t\tmp4_mux_cenc_update(ctx, tkw, NULL, CENC_CONFIG, 0, 0);\n\n\t\t\t\tif (!tkw->nb_samples) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t\tconst GF_PropertyValue *p = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ISOM_TREX_TEMPLATE);\n\t\t\t\t\tif (p) {\n\t\t\t\t\t\tgf_isom_setup_track_fragment_template(ctx->file, tkw->track_id, p->value.data.ptr, p->value.data.size, ctx->nofragdef);\n\t\t\t\t\t}\n#endif\n\t\t\t\t}\n\t\t\t\tif (tkw->dgl_copy) {\n\t\t\t\t\tgf_filter_pck_discard(tkw->dgl_copy);\n\t\t\t\t\ttkw->dgl_copy = NULL;\n\t\t\t\t}\n\t\t\t\tswitch (tkw->stream_type) {\n\t\t\t\tcase GF_STREAM_VISUAL:\n\t\t\t\tcase GF_STREAM_AUDIO:\n\t\t\t\t\tsi->nb_non_sparse_ready++;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tsi->nb_sparse_ready++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttkw->ts_shift = 0;\n\t\t\ttkw->si_min_ts_plus_one = 1;\n\t\t\tcontinue;\n\t\t}\n\t\t//we may have reorder tracks after the get_packet, redo\n\t\tif (gf_list_find(ctx->tracks, tkw) != i) {\n\t\t\tgoto retry_all;\n\t\t}\n\t\tts = gf_filter_pck_get_dts(pck);\n\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\tts = gf_filter_pck_get_cts(pck);\n\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\tts=0;\n\n\t\tdts_min = gf_timestamp_rescale(ts, tkw->src_timescale, 1000000);\n\n\t\tif (si->first_ts_min > dts_min) {\n\t\t\tsi->first_ts_min = (u64) dts_min;\n\t\t\thas_ready = GF_TRUE;\n\t\t}\n\n\t\tswitch (tkw->stream_type) {\n\t\tcase GF_STREAM_VISUAL:\n\t\tcase GF_STREAM_AUDIO:\n\t\t\tsi->nb_non_sparse_ready++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tsi->nb_sparse_ready++;\n\t\t\tbreak;\n\t\t}\n\n\t\ttkw->ts_shift = ts;\n\t\ttkw->si_min_ts_plus_one = 0;\n\t}\n\n\tfor (i=0; i<gf_list_count(services); i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\t//if some non-sparse streams are not ready, try to wait\n\t\tif (si->nb_non_sparse) {\n\t\t\tif (si->nb_non_sparse > si->nb_non_sparse_ready) not_ready = GF_TRUE;\n\t\t}\n\t\t//otherwise (only sparse stream), wait until first\n\t\telse if (si->nb_sparse) {\n\t\t\tif (!si->nb_sparse_ready) not_ready = GF_TRUE;\n\t\t}\n\t}\n\n\tif (not_ready) {\n\t\tif (blocking_refs && has_ready) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] Blocking input packets present, aborting initial timing sync\\n\"));\n\t\t}\n\t\t//this may be quite long until we have a packet in case input pid is video encoding \n\t\telse if (ctx->config_retry_start && (gf_sys_clock() - ctx->config_retry_start > 10000)) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] No input packets present on one or more inputs for more than 10s, aborting initial timing sync\\n\"));\n\t\t} else {\n\t\t\tctx->config_retry_start = gf_sys_clock();\n\t\t\tdel_service_info(services);\n\t\t\treturn;\n\t\t}\n\t}\n\tctx->config_retry_start = 0;\n\tfor (i=0; i<gf_list_count(services); i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\tif (si->first_ts_min==(u64)-1)\n\t\t\tsi->first_ts_min = 0;\n\t}\n\n\t//for all packets with dts greater than min dts, we need to add a pause\n\tfor (i=0; i<count; i++) {\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\t\tstruct _service_info *si = get_service_info(services, tkw);\n\t\tif (tkw->si_min_ts_plus_one) {\n\t\t\ttkw->si_min_ts_plus_one = si->first_ts_min + 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t//if single text track don't reset back to 0\n\t\tmp4_mux_update_init_edit(ctx, tkw, si->first_ts_min, ((count==1) && (tkw->stream_type == GF_STREAM_TEXT)) ? GF_TRUE : GF_FALSE);\n\t}\n\n\tctx->config_timing = GF_FALSE;\n\tdel_service_info(services);\n}",
        "func": "static void mp4_mux_config_timing(GF_MP4MuxCtx *ctx)\n{\n\tif ((ctx->store>=MP4MX_MODE_FRAG) && !ctx->tsalign) {\n\t\tctx->config_timing = GF_FALSE;\n\t\treturn;\n\t}\n\tGF_List *services = gf_list_new();\n\tu32 i, count;\n\tBool not_ready, blocking_refs, has_ready;\n\nretry_all:\n\tcount = gf_list_count(ctx->tracks);\n\tnot_ready = GF_FALSE;\n\tblocking_refs = GF_FALSE;\n\thas_ready = GF_FALSE;\n\n\tfor (i=0; i<gf_list_count(services);i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\tsi->nb_non_sparse = si->nb_non_sparse_ready = 0;\n\t\tsi->nb_sparse = si->nb_sparse_ready = 0;\n\t}\n\n\t//compute min dts of first packet on each track - this assume all tracks are synchronized, might need adjustment for MPEG4 Systems\n\tfor (i=0; i<count; i++) {\n\t\tu64 ts, dts_min;\n\t\tGF_FilterPacket *pck;\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\t\tif (tkw->fake_track) continue;\n\t\t//get associated service\n\t\tstruct _service_info *si = get_service_info(services, tkw);\n\n\t\t//already setup (happens when new PIDs are declared after a packet has already been written on other PIDs)\n\t\tif (tkw->nb_samples) {\n\t\t\tdts_min = gf_timestamp_rescale(tkw->ts_shift, tkw->src_timescale, 1000000);\n\n\t\t\tif (si->first_ts_min > dts_min) {\n\t\t\t\tsi->first_ts_min = (u64) dts_min;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\nretry_pck:\n\t\tpck = gf_filter_pid_get_packet(tkw->ipid);\n\t\t//check this after fetching a packet since it may reconfigure the track\n\t\tif (!tkw->track_num) {\n\t\t\tif (gf_filter_pid_is_eos(tkw->ipid)) {\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] PID has no input packet and configuration not known after 10 retries, aborting initial timing sync\\n\"));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnot_ready = GF_TRUE;\n\t\t\ttkw->ts_shift = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pck) {\n\t\t\tif (gf_filter_pck_is_blocking_ref(pck))\n\t\t\t\tblocking_refs = GF_TRUE;\n\t\t\tif (tkw->wait_sap) {\n\t\t\t\tGF_FilterSAPType sap = gf_filter_pck_get_sap(pck);\n\t\t\t\tBool seek = gf_filter_pck_get_seek_flag(pck);\n\t\t\t\tif (seek || !sap) {\n\t\t\t\t\tgf_filter_pid_drop_packet(tkw->ipid);\n\t\t\t\t\tgoto retry_pck;\n\t\t\t\t} else {\n\t\t\t\t\ttkw->wait_sap = GF_FALSE;\n\n\t\t\t\t\tif (!ctx->wait_dts_plus_one) {\n\t\t\t\t\t\tctx->wait_dts_plus_one = 1 + gf_filter_pck_get_dts(pck);\n\t\t\t\t\t\tctx->wait_dts_timescale = tkw->src_timescale;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (ctx->wait_dts_plus_one) {\n\t\t\t\tts = gf_filter_pck_get_dts(pck);\n\t\t\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\t\t\tts = gf_filter_pck_get_cts(pck);\n\t\t\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\t\t\tts=0;\n\n\t\t\t\tif (gf_timestamp_less(ts, tkw->src_timescale, (ctx->wait_dts_plus_one-1), ctx->wait_dts_timescale)) {\n\t\t\t\t\tgf_filter_pid_drop_packet(tkw->ipid);\n\t\t\t\t\tgoto retry_pck;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tswitch (tkw->stream_type) {\n\t\tcase GF_STREAM_VISUAL:\n\t\tcase GF_STREAM_AUDIO:\n\t\t\tsi->nb_non_sparse++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tsi->nb_sparse++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!pck) {\n\t\t\t//eos (wether real or flush event), setup cenc\n\t\t\tif (gf_filter_pid_is_eos(tkw->ipid)) {\n\t\t\t\tif (tkw->cenc_state==CENC_NEED_SETUP)\n\t\t\t\t\tmp4_mux_cenc_update(ctx, tkw, NULL, CENC_CONFIG, 0, 0);\n\n\t\t\t\tif (!tkw->nb_samples) {\n#ifndef GPAC_DISABLE_ISOM_FRAGMENTS\n\t\t\t\t\tconst GF_PropertyValue *p = gf_filter_pid_get_property(tkw->ipid, GF_PROP_PID_ISOM_TREX_TEMPLATE);\n\t\t\t\t\tif (p) {\n\t\t\t\t\t\tgf_isom_setup_track_fragment_template(ctx->file, tkw->track_id, p->value.data.ptr, p->value.data.size, ctx->nofragdef);\n\t\t\t\t\t}\n#endif\n\t\t\t\t}\n\t\t\t\tif (tkw->dgl_copy) {\n\t\t\t\t\tgf_filter_pck_discard(tkw->dgl_copy);\n\t\t\t\t\ttkw->dgl_copy = NULL;\n\t\t\t\t}\n\t\t\t\tswitch (tkw->stream_type) {\n\t\t\t\tcase GF_STREAM_VISUAL:\n\t\t\t\tcase GF_STREAM_AUDIO:\n\t\t\t\t\tsi->nb_non_sparse_ready++;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tsi->nb_sparse_ready++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttkw->ts_shift = 0;\n\t\t\ttkw->si_min_ts_plus_one = 1;\n\t\t\tcontinue;\n\t\t}\n\t\t//we may have reorder tracks after the get_packet, redo\n\t\tif (gf_list_find(ctx->tracks, tkw) != i) {\n\t\t\tgoto retry_all;\n\t\t}\n\t\tts = gf_filter_pck_get_dts(pck);\n\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\tts = gf_filter_pck_get_cts(pck);\n\t\tif (ts==GF_FILTER_NO_TS)\n\t\t\tts=0;\n\n\t\tdts_min = gf_timestamp_rescale(ts, tkw->src_timescale, 1000000);\n\n\t\tif (si->first_ts_min > dts_min) {\n\t\t\tsi->first_ts_min = (u64) dts_min;\n\t\t\thas_ready = GF_TRUE;\n\t\t}\n\n\t\tswitch (tkw->stream_type) {\n\t\tcase GF_STREAM_VISUAL:\n\t\tcase GF_STREAM_AUDIO:\n\t\t\tsi->nb_non_sparse_ready++;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tsi->nb_sparse_ready++;\n\t\t\tbreak;\n\t\t}\n\n\t\ttkw->ts_shift = ts;\n\t\ttkw->si_min_ts_plus_one = 0;\n\t}\n\n\tfor (i=0; i<gf_list_count(services); i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\t//if some non-sparse streams are not ready, try to wait\n\t\tif (si->nb_non_sparse) {\n\t\t\tif (si->nb_non_sparse > si->nb_non_sparse_ready) not_ready = GF_TRUE;\n\t\t}\n\t\t//otherwise (only sparse stream), wait until first\n\t\telse if (si->nb_sparse) {\n\t\t\tif (!si->nb_sparse_ready) not_ready = GF_TRUE;\n\t\t}\n\t}\n\n\tif (not_ready) {\n\t\tif (blocking_refs && has_ready) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] Blocking input packets present, aborting initial timing sync\\n\"));\n\t\t}\n\t\t//this may be quite long until we have a packet in case input pid is video encoding\n\t\telse if (ctx->config_retry_start && (gf_sys_clock() - ctx->config_retry_start > 10000)) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] No input packets present on one or more inputs for more than 10s, aborting initial timing sync\\n\"));\n\t\t} else {\n\t\t\tctx->config_retry_start = gf_sys_clock();\n\t\t\tdel_service_info(services);\n\t\t\treturn;\n\t\t}\n\t}\n\tctx->config_retry_start = 0;\n\tfor (i=0; i<gf_list_count(services); i++) {\n\t\tstruct _service_info *si = gf_list_get(services, i);\n\t\tif (si->first_ts_min==(u64)-1)\n\t\t\tsi->first_ts_min = 0;\n\t}\n\n\t//for all packets with dts greater than min dts, we need to add a pause\n\tfor (i=0; i<count; i++) {\n\t\tTrackWriter *tkw = gf_list_get(ctx->tracks, i);\n\t\tstruct _service_info *si = get_service_info(services, tkw);\n\t\tif (tkw->si_min_ts_plus_one) {\n\t\t\ttkw->si_min_ts_plus_one = si->first_ts_min + 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t//if single text track don't reset back to 0\n\t\tmp4_mux_update_init_edit(ctx, tkw, si->first_ts_min, ((count==1) && (tkw->stream_type == GF_STREAM_TEXT)) ? GF_TRUE : GF_FALSE);\n\t}\n\n\tctx->config_timing = GF_FALSE;\n\tdel_service_info(services);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -173,7 +173,7 @@\n \t\tif (blocking_refs && has_ready) {\n \t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] Blocking input packets present, aborting initial timing sync\\n\"));\n \t\t}\n-\t\t//this may be quite long until we have a packet in case input pid is video encoding \n+\t\t//this may be quite long until we have a packet in case input pid is video encoding\n \t\telse if (ctx->config_retry_start && (gf_sys_clock() - ctx->config_retry_start > 10000)) {\n \t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_CONTAINER, (\"[MP4Mux] No input packets present on one or more inputs for more than 10s, aborting initial timing sync\\n\"));\n \t\t} else {",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t//this may be quite long until we have a packet in case input pid is video encoding "
            ],
            "added_lines": [
                "\t\t//this may be quite long until we have a packet in case input pid is video encoding"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/EstimateStripByteCounts",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/d6bbe53a96b031ab8b53d20241825ddf9e8bf8f1",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614. ",
        "func_before": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint64_t filesize;\n        uint16_t n;\n        filesize = TIFFGetFileSize(tif);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "func": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size. */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, module,\n                        \"Requested memory size for StripByteCounts of %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return -1;\n    }\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint64_t filesize;\n        uint16_t n;\n        filesize = TIFFGetFileSize(tif);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,20 @@\n     /* Do not try to load stripbytecount as we will compute it */\n     if (!_TIFFFillStrilesInternal(tif, 0))\n         return -1;\n+\n+    /* Before allocating a huge amount of memory for corrupted files, check if\n+     * size of requested memory is not greater than file size. */\n+    uint64_t filesize = TIFFGetFileSize(tif);\n+    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n+    if (allocsize > filesize)\n+    {\n+        TIFFWarningExtR(tif, module,\n+                        \"Requested memory size for StripByteCounts of %\" PRIu64\n+                        \" is greather than filesize %\" PRIu64\n+                        \". Memory not allocated\",\n+                        allocsize, filesize);\n+        return -1;\n+    }\n \n     if (td->td_stripbytecount_p)\n         _TIFFfreeExt(tif, td->td_stripbytecount_p);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Before allocating a huge amount of memory for corrupted files, check if",
                "     * size of requested memory is not greater than file size. */",
                "    uint64_t filesize = TIFFGetFileSize(tif);",
                "    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);",
                "    if (allocsize > filesize)",
                "    {",
                "        TIFFWarningExtR(tif, module,",
                "                        \"Requested memory size for StripByteCounts of %\" PRIu64",
                "                        \" is greather than filesize %\" PRIu64",
                "                        \". Memory not allocated\",",
                "                        allocsize, filesize);",
                "        return -1;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/TIFFFetchDirectory",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/d6bbe53a96b031ab8b53d20241825ddf9e8bf8f1",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614. ",
        "func_before": "static uint16_t TIFFFetchDirectory(TIFF *tif, uint64_t diroff,\n                                   TIFFDirEntry **pdir, uint64_t *nextdiroff)\n{\n    static const char module[] = \"TIFFFetchDirectory\";\n\n    void *origdir;\n    uint16_t dircount16;\n    uint32_t dirsize;\n    TIFFDirEntry *dir;\n    uint8_t *ma;\n    TIFFDirEntry *mb;\n    uint16_t n;\n\n    assert(pdir);\n\n    tif->tif_diroff = diroff;\n    if (nextdiroff)\n        *nextdiroff = 0;\n    if (!isMapped(tif))\n    {\n        if (!SeekOK(tif, tif->tif_diroff))\n        {\n            TIFFErrorExtR(tif, module,\n                          \"%s: Seek error accessing TIFF directory\",\n                          tif->tif_name);\n            return 0;\n        }\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            if (!ReadOK(tif, &dircount16, sizeof(uint16_t)))\n            {\n                TIFFErrorExtR(tif, module,\n                              \"%s: Can not read TIFF directory count\",\n                              tif->tif_name);\n                return 0;\n            }\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabShort(&dircount16);\n            if (dircount16 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dirsize = 12;\n        }\n        else\n        {\n            uint64_t dircount64;\n            if (!ReadOK(tif, &dircount64, sizeof(uint64_t)))\n            {\n                TIFFErrorExtR(tif, module,\n                              \"%s: Can not read TIFF directory count\",\n                              tif->tif_name);\n                return 0;\n            }\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&dircount64);\n            if (dircount64 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dircount16 = (uint16_t)dircount64;\n            dirsize = 20;\n        }\n        origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                   \"to read TIFF directory\");\n        if (origdir == NULL)\n            return 0;\n        if (!ReadOK(tif, origdir, (tmsize_t)(dircount16 * dirsize)))\n        {\n            TIFFErrorExtR(tif, module, \"%.100s: Can not read TIFF directory\",\n                          tif->tif_name);\n            _TIFFfreeExt(tif, origdir);\n            return 0;\n        }\n        /*\n         * Read offset to next directory for sequential scans if\n         * needed.\n         */\n        if (nextdiroff)\n        {\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                uint32_t nextdiroff32;\n                if (!ReadOK(tif, &nextdiroff32, sizeof(uint32_t)))\n                    nextdiroff32 = 0;\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong(&nextdiroff32);\n                *nextdiroff = nextdiroff32;\n            }\n            else\n            {\n                if (!ReadOK(tif, nextdiroff, sizeof(uint64_t)))\n                    *nextdiroff = 0;\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong8(nextdiroff);\n            }\n        }\n    }\n    else\n    {\n        tmsize_t m;\n        tmsize_t off;\n        if (tif->tif_diroff > (uint64_t)INT64_MAX)\n        {\n            TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n            return (0);\n        }\n        off = (tmsize_t)tif->tif_diroff;\n\n        /*\n         * Check for integer overflow when validating the dir_off,\n         * otherwise a very high offset may cause an OOB read and\n         * crash the client. Make two comparisons instead of\n         *\n         *  off + sizeof(uint16_t) > tif->tif_size\n         *\n         * to avoid overflow.\n         */\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            m = off + sizeof(uint16_t);\n            if ((m < off) || (m < (tmsize_t)sizeof(uint16_t)) ||\n                (m > tif->tif_size))\n            {\n                TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n                return 0;\n            }\n            else\n            {\n                _TIFFmemcpy(&dircount16, tif->tif_base + off, sizeof(uint16_t));\n            }\n            off += sizeof(uint16_t);\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabShort(&dircount16);\n            if (dircount16 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dirsize = 12;\n        }\n        else\n        {\n            uint64_t dircount64;\n            m = off + sizeof(uint64_t);\n            if ((m < off) || (m < (tmsize_t)sizeof(uint64_t)) ||\n                (m > tif->tif_size))\n            {\n                TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n                return 0;\n            }\n            else\n            {\n                _TIFFmemcpy(&dircount64, tif->tif_base + off, sizeof(uint64_t));\n            }\n            off += sizeof(uint64_t);\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&dircount64);\n            if (dircount64 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dircount16 = (uint16_t)dircount64;\n            dirsize = 20;\n        }\n        if (dircount16 == 0)\n        {\n            TIFFErrorExtR(tif, module,\n                          \"Sanity check on directory count failed, zero tag \"\n                          \"directories not supported\");\n            return 0;\n        }\n        origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                   \"to read TIFF directory\");\n        if (origdir == NULL)\n            return 0;\n        m = off + dircount16 * dirsize;\n        if ((m < off) || (m < (tmsize_t)(dircount16 * dirsize)) ||\n            (m > tif->tif_size))\n        {\n            TIFFErrorExtR(tif, module, \"Can not read TIFF directory\");\n            _TIFFfreeExt(tif, origdir);\n            return 0;\n        }\n        else\n        {\n            _TIFFmemcpy(origdir, tif->tif_base + off, dircount16 * dirsize);\n        }\n        if (nextdiroff)\n        {\n            off += dircount16 * dirsize;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                uint32_t nextdiroff32;\n                m = off + sizeof(uint32_t);\n                if ((m < off) || (m < (tmsize_t)sizeof(uint32_t)) ||\n                    (m > tif->tif_size))\n                    nextdiroff32 = 0;\n                else\n                    _TIFFmemcpy(&nextdiroff32, tif->tif_base + off,\n                                sizeof(uint32_t));\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong(&nextdiroff32);\n                *nextdiroff = nextdiroff32;\n            }\n            else\n            {\n                m = off + sizeof(uint64_t);\n                if ((m < off) || (m < (tmsize_t)sizeof(uint64_t)) ||\n                    (m > tif->tif_size))\n                    *nextdiroff = 0;\n                else\n                    _TIFFmemcpy(nextdiroff, tif->tif_base + off,\n                                sizeof(uint64_t));\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong8(nextdiroff);\n            }\n        }\n    }\n    dir = (TIFFDirEntry *)_TIFFCheckMalloc(\n        tif, dircount16, sizeof(TIFFDirEntry), \"to read TIFF directory\");\n    if (dir == 0)\n    {\n        _TIFFfreeExt(tif, origdir);\n        return 0;\n    }\n    ma = (uint8_t *)origdir;\n    mb = dir;\n    for (n = 0; n < dircount16; n++)\n    {\n        mb->tdir_ignore = FALSE;\n        if (tif->tif_flags & TIFF_SWAB)\n            TIFFSwabShort((uint16_t *)ma);\n        mb->tdir_tag = *(uint16_t *)ma;\n        ma += sizeof(uint16_t);\n        if (tif->tif_flags & TIFF_SWAB)\n            TIFFSwabShort((uint16_t *)ma);\n        mb->tdir_type = *(uint16_t *)ma;\n        ma += sizeof(uint16_t);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong((uint32_t *)ma);\n            mb->tdir_count = (uint64_t)(*(uint32_t *)ma);\n            ma += sizeof(uint32_t);\n            mb->tdir_offset.toff_long8 = 0;\n            *(uint32_t *)(&mb->tdir_offset) = *(uint32_t *)ma;\n            ma += sizeof(uint32_t);\n        }\n        else\n        {\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8((uint64_t *)ma);\n            mb->tdir_count = TIFFReadUInt64(ma);\n            ma += sizeof(uint64_t);\n            mb->tdir_offset.toff_long8 = TIFFReadUInt64(ma);\n            ma += sizeof(uint64_t);\n        }\n        mb++;\n    }\n    _TIFFfreeExt(tif, origdir);\n    *pdir = dir;\n    return dircount16;\n}",
        "func": "static uint16_t TIFFFetchDirectory(TIFF *tif, uint64_t diroff,\n                                   TIFFDirEntry **pdir, uint64_t *nextdiroff)\n{\n    static const char module[] = \"TIFFFetchDirectory\";\n\n    void *origdir;\n    uint16_t dircount16;\n    uint32_t dirsize;\n    TIFFDirEntry *dir;\n    uint8_t *ma;\n    TIFFDirEntry *mb;\n    uint16_t n;\n\n    assert(pdir);\n\n    tif->tif_diroff = diroff;\n    if (nextdiroff)\n        *nextdiroff = 0;\n    if (!isMapped(tif))\n    {\n        if (!SeekOK(tif, tif->tif_diroff))\n        {\n            TIFFErrorExtR(tif, module,\n                          \"%s: Seek error accessing TIFF directory\",\n                          tif->tif_name);\n            return 0;\n        }\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            if (!ReadOK(tif, &dircount16, sizeof(uint16_t)))\n            {\n                TIFFErrorExtR(tif, module,\n                              \"%s: Can not read TIFF directory count\",\n                              tif->tif_name);\n                return 0;\n            }\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabShort(&dircount16);\n            if (dircount16 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dirsize = 12;\n        }\n        else\n        {\n            uint64_t dircount64;\n            if (!ReadOK(tif, &dircount64, sizeof(uint64_t)))\n            {\n                TIFFErrorExtR(tif, module,\n                              \"%s: Can not read TIFF directory count\",\n                              tif->tif_name);\n                return 0;\n            }\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&dircount64);\n            if (dircount64 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dircount16 = (uint16_t)dircount64;\n            dirsize = 20;\n        }\n        /* Before allocating a huge amount of memory for corrupted files, check\n         * if size of requested memory is not greater than file size. */\n        uint64_t filesize = TIFFGetFileSize(tif);\n        uint64_t allocsize = (uint64_t)dircount16 * dirsize;\n        if (allocsize > filesize)\n        {\n            TIFFWarningExtR(\n                tif, module,\n                \"Requested memory size for TIFF directory of %\" PRIu64\n                \" is greather than filesize %\" PRIu64\n                \". Memory not allocated, TIFF directory not read\",\n                allocsize, filesize);\n            return 0;\n        }\n        origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                   \"to read TIFF directory\");\n        if (origdir == NULL)\n            return 0;\n        if (!ReadOK(tif, origdir, (tmsize_t)(dircount16 * dirsize)))\n        {\n            TIFFErrorExtR(tif, module, \"%.100s: Can not read TIFF directory\",\n                          tif->tif_name);\n            _TIFFfreeExt(tif, origdir);\n            return 0;\n        }\n        /*\n         * Read offset to next directory for sequential scans if\n         * needed.\n         */\n        if (nextdiroff)\n        {\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                uint32_t nextdiroff32;\n                if (!ReadOK(tif, &nextdiroff32, sizeof(uint32_t)))\n                    nextdiroff32 = 0;\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong(&nextdiroff32);\n                *nextdiroff = nextdiroff32;\n            }\n            else\n            {\n                if (!ReadOK(tif, nextdiroff, sizeof(uint64_t)))\n                    *nextdiroff = 0;\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong8(nextdiroff);\n            }\n        }\n    }\n    else\n    {\n        tmsize_t m;\n        tmsize_t off;\n        if (tif->tif_diroff > (uint64_t)INT64_MAX)\n        {\n            TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n            return (0);\n        }\n        off = (tmsize_t)tif->tif_diroff;\n\n        /*\n         * Check for integer overflow when validating the dir_off,\n         * otherwise a very high offset may cause an OOB read and\n         * crash the client. Make two comparisons instead of\n         *\n         *  off + sizeof(uint16_t) > tif->tif_size\n         *\n         * to avoid overflow.\n         */\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            m = off + sizeof(uint16_t);\n            if ((m < off) || (m < (tmsize_t)sizeof(uint16_t)) ||\n                (m > tif->tif_size))\n            {\n                TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n                return 0;\n            }\n            else\n            {\n                _TIFFmemcpy(&dircount16, tif->tif_base + off, sizeof(uint16_t));\n            }\n            off += sizeof(uint16_t);\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabShort(&dircount16);\n            if (dircount16 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dirsize = 12;\n        }\n        else\n        {\n            uint64_t dircount64;\n            m = off + sizeof(uint64_t);\n            if ((m < off) || (m < (tmsize_t)sizeof(uint64_t)) ||\n                (m > tif->tif_size))\n            {\n                TIFFErrorExtR(tif, module, \"Can not read TIFF directory count\");\n                return 0;\n            }\n            else\n            {\n                _TIFFmemcpy(&dircount64, tif->tif_base + off, sizeof(uint64_t));\n            }\n            off += sizeof(uint64_t);\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&dircount64);\n            if (dircount64 > 4096)\n            {\n                TIFFErrorExtR(tif, module,\n                              \"Sanity check on directory count failed, this is \"\n                              \"probably not a valid IFD offset\");\n                return 0;\n            }\n            dircount16 = (uint16_t)dircount64;\n            dirsize = 20;\n        }\n        if (dircount16 == 0)\n        {\n            TIFFErrorExtR(tif, module,\n                          \"Sanity check on directory count failed, zero tag \"\n                          \"directories not supported\");\n            return 0;\n        }\n        /* Before allocating a huge amount of memory for corrupted files, check\n         * if size of requested memory is not greater than file size. */\n        uint64_t filesize = TIFFGetFileSize(tif);\n        uint64_t allocsize = (uint64_t)dircount16 * dirsize;\n        if (allocsize > filesize)\n        {\n            TIFFWarningExtR(\n                tif, module,\n                \"Requested memory size for TIFF directory of %\" PRIu64\n                \" is greather than filesize %\" PRIu64\n                \". Memory not allocated, TIFF directory not read\",\n                allocsize, filesize);\n            return 0;\n        }\n        origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                   \"to read TIFF directory\");\n        if (origdir == NULL)\n            return 0;\n        m = off + dircount16 * dirsize;\n        if ((m < off) || (m < (tmsize_t)(dircount16 * dirsize)) ||\n            (m > tif->tif_size))\n        {\n            TIFFErrorExtR(tif, module, \"Can not read TIFF directory\");\n            _TIFFfreeExt(tif, origdir);\n            return 0;\n        }\n        else\n        {\n            _TIFFmemcpy(origdir, tif->tif_base + off, dircount16 * dirsize);\n        }\n        if (nextdiroff)\n        {\n            off += dircount16 * dirsize;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                uint32_t nextdiroff32;\n                m = off + sizeof(uint32_t);\n                if ((m < off) || (m < (tmsize_t)sizeof(uint32_t)) ||\n                    (m > tif->tif_size))\n                    nextdiroff32 = 0;\n                else\n                    _TIFFmemcpy(&nextdiroff32, tif->tif_base + off,\n                                sizeof(uint32_t));\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong(&nextdiroff32);\n                *nextdiroff = nextdiroff32;\n            }\n            else\n            {\n                m = off + sizeof(uint64_t);\n                if ((m < off) || (m < (tmsize_t)sizeof(uint64_t)) ||\n                    (m > tif->tif_size))\n                    *nextdiroff = 0;\n                else\n                    _TIFFmemcpy(nextdiroff, tif->tif_base + off,\n                                sizeof(uint64_t));\n                if (tif->tif_flags & TIFF_SWAB)\n                    TIFFSwabLong8(nextdiroff);\n            }\n        }\n    }\n    /* No check against filesize needed here because \"dir\" should have same size\n     * than \"origdir\" checked above. */\n    dir = (TIFFDirEntry *)_TIFFCheckMalloc(\n        tif, dircount16, sizeof(TIFFDirEntry), \"to read TIFF directory\");\n    if (dir == 0)\n    {\n        _TIFFfreeExt(tif, origdir);\n        return 0;\n    }\n    ma = (uint8_t *)origdir;\n    mb = dir;\n    for (n = 0; n < dircount16; n++)\n    {\n        mb->tdir_ignore = FALSE;\n        if (tif->tif_flags & TIFF_SWAB)\n            TIFFSwabShort((uint16_t *)ma);\n        mb->tdir_tag = *(uint16_t *)ma;\n        ma += sizeof(uint16_t);\n        if (tif->tif_flags & TIFF_SWAB)\n            TIFFSwabShort((uint16_t *)ma);\n        mb->tdir_type = *(uint16_t *)ma;\n        ma += sizeof(uint16_t);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n        {\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong((uint32_t *)ma);\n            mb->tdir_count = (uint64_t)(*(uint32_t *)ma);\n            ma += sizeof(uint32_t);\n            mb->tdir_offset.toff_long8 = 0;\n            *(uint32_t *)(&mb->tdir_offset) = *(uint32_t *)ma;\n            ma += sizeof(uint32_t);\n        }\n        else\n        {\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8((uint64_t *)ma);\n            mb->tdir_count = TIFFReadUInt64(ma);\n            ma += sizeof(uint64_t);\n            mb->tdir_offset.toff_long8 = TIFFReadUInt64(ma);\n            ma += sizeof(uint64_t);\n        }\n        mb++;\n    }\n    _TIFFfreeExt(tif, origdir);\n    *pdir = dir;\n    return dircount16;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -67,6 +67,20 @@\n             dircount16 = (uint16_t)dircount64;\n             dirsize = 20;\n         }\n+        /* Before allocating a huge amount of memory for corrupted files, check\n+         * if size of requested memory is not greater than file size. */\n+        uint64_t filesize = TIFFGetFileSize(tif);\n+        uint64_t allocsize = (uint64_t)dircount16 * dirsize;\n+        if (allocsize > filesize)\n+        {\n+            TIFFWarningExtR(\n+                tif, module,\n+                \"Requested memory size for TIFF directory of %\" PRIu64\n+                \" is greather than filesize %\" PRIu64\n+                \". Memory not allocated, TIFF directory not read\",\n+                allocsize, filesize);\n+            return 0;\n+        }\n         origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                    \"to read TIFF directory\");\n         if (origdir == NULL)\n@@ -181,6 +195,20 @@\n                           \"directories not supported\");\n             return 0;\n         }\n+        /* Before allocating a huge amount of memory for corrupted files, check\n+         * if size of requested memory is not greater than file size. */\n+        uint64_t filesize = TIFFGetFileSize(tif);\n+        uint64_t allocsize = (uint64_t)dircount16 * dirsize;\n+        if (allocsize > filesize)\n+        {\n+            TIFFWarningExtR(\n+                tif, module,\n+                \"Requested memory size for TIFF directory of %\" PRIu64\n+                \" is greather than filesize %\" PRIu64\n+                \". Memory not allocated, TIFF directory not read\",\n+                allocsize, filesize);\n+            return 0;\n+        }\n         origdir = _TIFFCheckMalloc(tif, dircount16, dirsize,\n                                    \"to read TIFF directory\");\n         if (origdir == NULL)\n@@ -228,6 +256,8 @@\n             }\n         }\n     }\n+    /* No check against filesize needed here because \"dir\" should have same size\n+     * than \"origdir\" checked above. */\n     dir = (TIFFDirEntry *)_TIFFCheckMalloc(\n         tif, dircount16, sizeof(TIFFDirEntry), \"to read TIFF directory\");\n     if (dir == 0)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /* Before allocating a huge amount of memory for corrupted files, check",
                "         * if size of requested memory is not greater than file size. */",
                "        uint64_t filesize = TIFFGetFileSize(tif);",
                "        uint64_t allocsize = (uint64_t)dircount16 * dirsize;",
                "        if (allocsize > filesize)",
                "        {",
                "            TIFFWarningExtR(",
                "                tif, module,",
                "                \"Requested memory size for TIFF directory of %\" PRIu64",
                "                \" is greather than filesize %\" PRIu64",
                "                \". Memory not allocated, TIFF directory not read\",",
                "                allocsize, filesize);",
                "            return 0;",
                "        }",
                "        /* Before allocating a huge amount of memory for corrupted files, check",
                "         * if size of requested memory is not greater than file size. */",
                "        uint64_t filesize = TIFFGetFileSize(tif);",
                "        uint64_t allocsize = (uint64_t)dircount16 * dirsize;",
                "        if (allocsize > filesize)",
                "        {",
                "            TIFFWarningExtR(",
                "                tif, module,",
                "                \"Requested memory size for TIFF directory of %\" PRIu64",
                "                \" is greather than filesize %\" PRIu64",
                "                \". Memory not allocated, TIFF directory not read\",",
                "                allocsize, filesize);",
                "            return 0;",
                "        }",
                "    /* No check against filesize needed here because \"dir\" should have same size",
                "     * than \"origdir\" checked above. */"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/TIFFFetchStripThing",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/d6bbe53a96b031ab8b53d20241825ddf9e8bf8f1",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614. ",
        "func_before": "static int TIFFFetchStripThing(TIFF *tif, TIFFDirEntry *dir, uint32_t nstrips,\n                               uint64_t **lpp)\n{\n    static const char module[] = \"TIFFFetchStripThing\";\n    enum TIFFReadDirEntryErr err;\n    uint64_t *data;\n    err = TIFFReadDirEntryLong8ArrayWithLimit(tif, dir, &data, nstrips);\n    if (err != TIFFReadDirEntryErrOk)\n    {\n        const TIFFField *fip = TIFFFieldWithTag(tif, dir->tdir_tag);\n        TIFFReadDirEntryOutputErr(tif, err, module,\n                                  fip ? fip->field_name : \"unknown tagname\", 0);\n        return (0);\n    }\n    if (dir->tdir_count < (uint64_t)nstrips)\n    {\n        uint64_t *resizeddata;\n        const TIFFField *fip = TIFFFieldWithTag(tif, dir->tdir_tag);\n        const char *pszMax = getenv(\"LIBTIFF_STRILE_ARRAY_MAX_RESIZE_COUNT\");\n        uint32_t max_nstrips = 1000000;\n        if (pszMax)\n            max_nstrips = (uint32_t)atoi(pszMax);\n        TIFFReadDirEntryOutputErr(tif, TIFFReadDirEntryErrCount, module,\n                                  fip ? fip->field_name : \"unknown tagname\",\n                                  (nstrips <= max_nstrips));\n\n        if (nstrips > max_nstrips)\n        {\n            _TIFFfreeExt(tif, data);\n            return (0);\n        }\n\n        resizeddata = (uint64_t *)_TIFFCheckMalloc(\n            tif, nstrips, sizeof(uint64_t), \"for strip array\");\n        if (resizeddata == 0)\n        {\n            _TIFFfreeExt(tif, data);\n            return (0);\n        }\n        if (dir->tdir_count)\n            _TIFFmemcpy(resizeddata, data,\n                        (uint32_t)dir->tdir_count * sizeof(uint64_t));\n        _TIFFmemset(resizeddata + (uint32_t)dir->tdir_count, 0,\n                    (nstrips - (uint32_t)dir->tdir_count) * sizeof(uint64_t));\n        _TIFFfreeExt(tif, data);\n        data = resizeddata;\n    }\n    *lpp = data;\n    return (1);\n}",
        "func": "static int TIFFFetchStripThing(TIFF *tif, TIFFDirEntry *dir, uint32_t nstrips,\n                               uint64_t **lpp)\n{\n    static const char module[] = \"TIFFFetchStripThing\";\n    enum TIFFReadDirEntryErr err;\n    uint64_t *data;\n    err = TIFFReadDirEntryLong8ArrayWithLimit(tif, dir, &data, nstrips);\n    if (err != TIFFReadDirEntryErrOk)\n    {\n        const TIFFField *fip = TIFFFieldWithTag(tif, dir->tdir_tag);\n        TIFFReadDirEntryOutputErr(tif, err, module,\n                                  fip ? fip->field_name : \"unknown tagname\", 0);\n        return (0);\n    }\n    if (dir->tdir_count < (uint64_t)nstrips)\n    {\n        uint64_t *resizeddata;\n        const TIFFField *fip = TIFFFieldWithTag(tif, dir->tdir_tag);\n        const char *pszMax = getenv(\"LIBTIFF_STRILE_ARRAY_MAX_RESIZE_COUNT\");\n        uint32_t max_nstrips = 1000000;\n        if (pszMax)\n            max_nstrips = (uint32_t)atoi(pszMax);\n        TIFFReadDirEntryOutputErr(tif, TIFFReadDirEntryErrCount, module,\n                                  fip ? fip->field_name : \"unknown tagname\",\n                                  (nstrips <= max_nstrips));\n\n        if (nstrips > max_nstrips)\n        {\n            _TIFFfreeExt(tif, data);\n            return (0);\n        }\n\n        /* Before allocating a huge amount of memory for corrupted files, check\n         * if size of requested memory is not greater than file size. */\n        uint64_t filesize = TIFFGetFileSize(tif);\n        uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t);\n        if (allocsize > filesize)\n        {\n            TIFFWarningExtR(tif, module,\n                            \"Requested memory size for StripArray of %\" PRIu64\n                            \" is greather than filesize %\" PRIu64\n                            \". Memory not allocated\",\n                            allocsize, filesize);\n            _TIFFfreeExt(tif, data);\n            return (0);\n        }\n        resizeddata = (uint64_t *)_TIFFCheckMalloc(\n            tif, nstrips, sizeof(uint64_t), \"for strip array\");\n        if (resizeddata == 0)\n        {\n            _TIFFfreeExt(tif, data);\n            return (0);\n        }\n        if (dir->tdir_count)\n            _TIFFmemcpy(resizeddata, data,\n                        (uint32_t)dir->tdir_count * sizeof(uint64_t));\n        _TIFFmemset(resizeddata + (uint32_t)dir->tdir_count, 0,\n                    (nstrips - (uint32_t)dir->tdir_count) * sizeof(uint64_t));\n        _TIFFfreeExt(tif, data);\n        data = resizeddata;\n    }\n    *lpp = data;\n    return (1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -30,6 +30,20 @@\n             return (0);\n         }\n \n+        /* Before allocating a huge amount of memory for corrupted files, check\n+         * if size of requested memory is not greater than file size. */\n+        uint64_t filesize = TIFFGetFileSize(tif);\n+        uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t);\n+        if (allocsize > filesize)\n+        {\n+            TIFFWarningExtR(tif, module,\n+                            \"Requested memory size for StripArray of %\" PRIu64\n+                            \" is greather than filesize %\" PRIu64\n+                            \". Memory not allocated\",\n+                            allocsize, filesize);\n+            _TIFFfreeExt(tif, data);\n+            return (0);\n+        }\n         resizeddata = (uint64_t *)_TIFFCheckMalloc(\n             tif, nstrips, sizeof(uint64_t), \"for strip array\");\n         if (resizeddata == 0)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /* Before allocating a huge amount of memory for corrupted files, check",
                "         * if size of requested memory is not greater than file size. */",
                "        uint64_t filesize = TIFFGetFileSize(tif);",
                "        uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t);",
                "        if (allocsize > filesize)",
                "        {",
                "            TIFFWarningExtR(tif, module,",
                "                            \"Requested memory size for StripArray of %\" PRIu64",
                "                            \" is greather than filesize %\" PRIu64",
                "                            \". Memory not allocated\",",
                "                            allocsize, filesize);",
                "            _TIFFfreeExt(tif, data);",
                "            return (0);",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/allocChoppedUpStripArrays",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/d6bbe53a96b031ab8b53d20241825ddf9e8bf8f1",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614. ",
        "func_before": "static void allocChoppedUpStripArrays(TIFF *tif, uint32_t nstrips,\n                                      uint64_t stripbytes,\n                                      uint32_t rowsperstrip)\n{\n    TIFFDirectory *td = &tif->tif_dir;\n    uint64_t bytecount;\n    uint64_t offset;\n    uint64_t last_offset;\n    uint64_t last_bytecount;\n    uint32_t i;\n    uint64_t *newcounts;\n    uint64_t *newoffsets;\n\n    offset = TIFFGetStrileOffset(tif, 0);\n    last_offset = TIFFGetStrileOffset(tif, td->td_nstrips - 1);\n    last_bytecount = TIFFGetStrileByteCount(tif, td->td_nstrips - 1);\n    if (last_offset > UINT64_MAX - last_bytecount ||\n        last_offset + last_bytecount < offset)\n    {\n        return;\n    }\n    bytecount = last_offset + last_bytecount - offset;\n\n    newcounts =\n        (uint64_t *)_TIFFCheckMalloc(tif, nstrips, sizeof(uint64_t),\n                                     \"for chopped \\\"StripByteCounts\\\" array\");\n    newoffsets = (uint64_t *)_TIFFCheckMalloc(\n        tif, nstrips, sizeof(uint64_t), \"for chopped \\\"StripOffsets\\\" array\");\n    if (newcounts == NULL || newoffsets == NULL)\n    {\n        /*\n         * Unable to allocate new strip information, give up and use\n         * the original one strip information.\n         */\n        if (newcounts != NULL)\n            _TIFFfreeExt(tif, newcounts);\n        if (newoffsets != NULL)\n            _TIFFfreeExt(tif, newoffsets);\n        return;\n    }\n\n    /*\n     * Fill the strip information arrays with new bytecounts and offsets\n     * that reflect the broken-up format.\n     */\n    for (i = 0; i < nstrips; i++)\n    {\n        if (stripbytes > bytecount)\n            stripbytes = bytecount;\n        newcounts[i] = stripbytes;\n        newoffsets[i] = stripbytes ? offset : 0;\n        offset += stripbytes;\n        bytecount -= stripbytes;\n    }\n\n    /*\n     * Replace old single strip info with multi-strip info.\n     */\n    td->td_stripsperimage = td->td_nstrips = nstrips;\n    TIFFSetField(tif, TIFFTAG_ROWSPERSTRIP, rowsperstrip);\n\n    _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    _TIFFfreeExt(tif, td->td_stripoffset_p);\n    td->td_stripbytecount_p = newcounts;\n    td->td_stripoffset_p = newoffsets;\n#ifdef STRIPBYTECOUNTSORTED_UNUSED\n    td->td_stripbytecountsorted = 1;\n#endif\n    tif->tif_flags |= TIFF_CHOPPEDUPARRAYS;\n}",
        "func": "static void allocChoppedUpStripArrays(TIFF *tif, uint32_t nstrips,\n                                      uint64_t stripbytes,\n                                      uint32_t rowsperstrip)\n{\n    TIFFDirectory *td = &tif->tif_dir;\n    uint64_t bytecount;\n    uint64_t offset;\n    uint64_t last_offset;\n    uint64_t last_bytecount;\n    uint32_t i;\n    uint64_t *newcounts;\n    uint64_t *newoffsets;\n\n    offset = TIFFGetStrileOffset(tif, 0);\n    last_offset = TIFFGetStrileOffset(tif, td->td_nstrips - 1);\n    last_bytecount = TIFFGetStrileByteCount(tif, td->td_nstrips - 1);\n    if (last_offset > UINT64_MAX - last_bytecount ||\n        last_offset + last_bytecount < offset)\n    {\n        return;\n    }\n    bytecount = last_offset + last_bytecount - offset;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of StripByteCount and StripOffset tags is not greater than\n     * file size.\n     */\n    uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t) * 2;\n    uint64_t filesize = TIFFGetFileSize(tif);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, \"allocChoppedUpStripArrays\",\n                        \"Requested memory size for StripByteCount and \"\n                        \"StripOffsets %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return;\n    }\n\n    newcounts =\n        (uint64_t *)_TIFFCheckMalloc(tif, nstrips, sizeof(uint64_t),\n                                     \"for chopped \\\"StripByteCounts\\\" array\");\n    newoffsets = (uint64_t *)_TIFFCheckMalloc(\n        tif, nstrips, sizeof(uint64_t), \"for chopped \\\"StripOffsets\\\" array\");\n    if (newcounts == NULL || newoffsets == NULL)\n    {\n        /*\n         * Unable to allocate new strip information, give up and use\n         * the original one strip information.\n         */\n        if (newcounts != NULL)\n            _TIFFfreeExt(tif, newcounts);\n        if (newoffsets != NULL)\n            _TIFFfreeExt(tif, newoffsets);\n        return;\n    }\n\n    /*\n     * Fill the strip information arrays with new bytecounts and offsets\n     * that reflect the broken-up format.\n     */\n    for (i = 0; i < nstrips; i++)\n    {\n        if (stripbytes > bytecount)\n            stripbytes = bytecount;\n        newcounts[i] = stripbytes;\n        newoffsets[i] = stripbytes ? offset : 0;\n        offset += stripbytes;\n        bytecount -= stripbytes;\n    }\n\n    /*\n     * Replace old single strip info with multi-strip info.\n     */\n    td->td_stripsperimage = td->td_nstrips = nstrips;\n    TIFFSetField(tif, TIFFTAG_ROWSPERSTRIP, rowsperstrip);\n\n    _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    _TIFFfreeExt(tif, td->td_stripoffset_p);\n    td->td_stripbytecount_p = newcounts;\n    td->td_stripoffset_p = newoffsets;\n#ifdef STRIPBYTECOUNTSORTED_UNUSED\n    td->td_stripbytecountsorted = 1;\n#endif\n    tif->tif_flags |= TIFF_CHOPPEDUPARRAYS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,23 @@\n         return;\n     }\n     bytecount = last_offset + last_bytecount - offset;\n+\n+    /* Before allocating a huge amount of memory for corrupted files, check if\n+     * size of StripByteCount and StripOffset tags is not greater than\n+     * file size.\n+     */\n+    uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t) * 2;\n+    uint64_t filesize = TIFFGetFileSize(tif);\n+    if (allocsize > filesize)\n+    {\n+        TIFFWarningExtR(tif, \"allocChoppedUpStripArrays\",\n+                        \"Requested memory size for StripByteCount and \"\n+                        \"StripOffsets %\" PRIu64\n+                        \" is greather than filesize %\" PRIu64\n+                        \". Memory not allocated\",\n+                        allocsize, filesize);\n+        return;\n+    }\n \n     newcounts =\n         (uint64_t *)_TIFFCheckMalloc(tif, nstrips, sizeof(uint64_t),",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Before allocating a huge amount of memory for corrupted files, check if",
                "     * size of StripByteCount and StripOffset tags is not greater than",
                "     * file size.",
                "     */",
                "    uint64_t allocsize = (uint64_t)nstrips * sizeof(uint64_t) * 2;",
                "    uint64_t filesize = TIFFGetFileSize(tif);",
                "    if (allocsize > filesize)",
                "    {",
                "        TIFFWarningExtR(tif, \"allocChoppedUpStripArrays\",",
                "                        \"Requested memory size for StripByteCount and \"",
                "                        \"StripOffsets %\" PRIu64",
                "                        \" is greather than filesize %\" PRIu64",
                "                        \". Memory not allocated\",",
                "                        allocsize, filesize);",
                "        return;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/TIFFReadDirEntryArrayWithLimit",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/d6bbe53a96b031ab8b53d20241825ddf9e8bf8f1",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614. ",
        "func_before": "static enum TIFFReadDirEntryErr\nTIFFReadDirEntryArrayWithLimit(TIFF *tif, TIFFDirEntry *direntry,\n                               uint32_t *count, uint32_t desttypesize,\n                               void **value, uint64_t maxcount)\n{\n    int typesize;\n    uint32_t datasize;\n    void *data;\n    uint64_t target_count64;\n    int original_datasize_clamped;\n    typesize = TIFFDataWidth(direntry->tdir_type);\n\n    target_count64 =\n        (direntry->tdir_count > maxcount) ? maxcount : direntry->tdir_count;\n\n    if ((target_count64 == 0) || (typesize == 0))\n    {\n        *value = 0;\n        return (TIFFReadDirEntryErrOk);\n    }\n    (void)desttypesize;\n\n    /* We just want to know if the original tag size is more than 4 bytes\n     * (classic TIFF) or 8 bytes (BigTIFF)\n     */\n    original_datasize_clamped =\n        ((direntry->tdir_count > 10) ? 10 : (int)direntry->tdir_count) *\n        typesize;\n\n    /*\n     * As a sanity check, make sure we have no more than a 2GB tag array\n     * in either the current data type or the dest data type.  This also\n     * avoids problems with overflow of tmsize_t on 32bit systems.\n     */\n    if ((uint64_t)(MAX_SIZE_TAG_DATA / typesize) < target_count64)\n        return (TIFFReadDirEntryErrSizesan);\n    if ((uint64_t)(MAX_SIZE_TAG_DATA / desttypesize) < target_count64)\n        return (TIFFReadDirEntryErrSizesan);\n\n    *count = (uint32_t)target_count64;\n    datasize = (*count) * typesize;\n    assert((tmsize_t)datasize > 0);\n\n    if (isMapped(tif) && datasize > (uint64_t)tif->tif_size)\n        return TIFFReadDirEntryErrIo;\n\n    if (!isMapped(tif) && (((tif->tif_flags & TIFF_BIGTIFF) && datasize > 8) ||\n                           (!(tif->tif_flags & TIFF_BIGTIFF) && datasize > 4)))\n    {\n        data = NULL;\n    }\n    else\n    {\n        data = _TIFFCheckMalloc(tif, *count, typesize, \"ReadDirEntryArray\");\n        if (data == 0)\n            return (TIFFReadDirEntryErrAlloc);\n    }\n    if (!(tif->tif_flags & TIFF_BIGTIFF))\n    {\n        /* Only the condition on original_datasize_clamped. The second\n         * one is implied, but Coverity Scan cannot see it. */\n        if (original_datasize_clamped <= 4 && datasize <= 4)\n            _TIFFmemcpy(data, &direntry->tdir_offset, datasize);\n        else\n        {\n            enum TIFFReadDirEntryErr err;\n            uint32_t offset = direntry->tdir_offset.toff_long;\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong(&offset);\n            if (isMapped(tif))\n                err = TIFFReadDirEntryData(tif, (uint64_t)offset,\n                                           (tmsize_t)datasize, data);\n            else\n                err = TIFFReadDirEntryDataAndRealloc(tif, (uint64_t)offset,\n                                                     (tmsize_t)datasize, &data);\n            if (err != TIFFReadDirEntryErrOk)\n            {\n                _TIFFfreeExt(tif, data);\n                return (err);\n            }\n        }\n    }\n    else\n    {\n        /* See above comment for the Classic TIFF case */\n        if (original_datasize_clamped <= 8 && datasize <= 8)\n            _TIFFmemcpy(data, &direntry->tdir_offset, datasize);\n        else\n        {\n            enum TIFFReadDirEntryErr err;\n            uint64_t offset = direntry->tdir_offset.toff_long8;\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&offset);\n            if (isMapped(tif))\n                err = TIFFReadDirEntryData(tif, (uint64_t)offset,\n                                           (tmsize_t)datasize, data);\n            else\n                err = TIFFReadDirEntryDataAndRealloc(tif, (uint64_t)offset,\n                                                     (tmsize_t)datasize, &data);\n            if (err != TIFFReadDirEntryErrOk)\n            {\n                _TIFFfreeExt(tif, data);\n                return (err);\n            }\n        }\n    }\n    *value = data;\n    return (TIFFReadDirEntryErrOk);\n}",
        "func": "static enum TIFFReadDirEntryErr\nTIFFReadDirEntryArrayWithLimit(TIFF *tif, TIFFDirEntry *direntry,\n                               uint32_t *count, uint32_t desttypesize,\n                               void **value, uint64_t maxcount)\n{\n    int typesize;\n    uint32_t datasize;\n    void *data;\n    uint64_t target_count64;\n    int original_datasize_clamped;\n    typesize = TIFFDataWidth(direntry->tdir_type);\n\n    target_count64 =\n        (direntry->tdir_count > maxcount) ? maxcount : direntry->tdir_count;\n\n    if ((target_count64 == 0) || (typesize == 0))\n    {\n        *value = 0;\n        return (TIFFReadDirEntryErrOk);\n    }\n    (void)desttypesize;\n\n    /* We just want to know if the original tag size is more than 4 bytes\n     * (classic TIFF) or 8 bytes (BigTIFF)\n     */\n    original_datasize_clamped =\n        ((direntry->tdir_count > 10) ? 10 : (int)direntry->tdir_count) *\n        typesize;\n\n    /*\n     * As a sanity check, make sure we have no more than a 2GB tag array\n     * in either the current data type or the dest data type.  This also\n     * avoids problems with overflow of tmsize_t on 32bit systems.\n     */\n    if ((uint64_t)(MAX_SIZE_TAG_DATA / typesize) < target_count64)\n        return (TIFFReadDirEntryErrSizesan);\n    if ((uint64_t)(MAX_SIZE_TAG_DATA / desttypesize) < target_count64)\n        return (TIFFReadDirEntryErrSizesan);\n\n    *count = (uint32_t)target_count64;\n    datasize = (*count) * typesize;\n    assert((tmsize_t)datasize > 0);\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size.\n     */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    if (datasize > filesize)\n    {\n        TIFFWarningExtR(tif, \"ReadDirEntryArray\",\n                        \"Requested memory size for tag %d (0x%x) %\" PRIu32\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated, tag not read\",\n                        direntry->tdir_tag, direntry->tdir_tag, datasize,\n                        filesize);\n        return (TIFFReadDirEntryErrAlloc);\n    }\n\n    if (isMapped(tif) && datasize > (uint64_t)tif->tif_size)\n        return TIFFReadDirEntryErrIo;\n\n    if (!isMapped(tif) && (((tif->tif_flags & TIFF_BIGTIFF) && datasize > 8) ||\n                           (!(tif->tif_flags & TIFF_BIGTIFF) && datasize > 4)))\n    {\n        data = NULL;\n    }\n    else\n    {\n        data = _TIFFCheckMalloc(tif, *count, typesize, \"ReadDirEntryArray\");\n        if (data == 0)\n            return (TIFFReadDirEntryErrAlloc);\n    }\n    if (!(tif->tif_flags & TIFF_BIGTIFF))\n    {\n        /* Only the condition on original_datasize_clamped. The second\n         * one is implied, but Coverity Scan cannot see it. */\n        if (original_datasize_clamped <= 4 && datasize <= 4)\n            _TIFFmemcpy(data, &direntry->tdir_offset, datasize);\n        else\n        {\n            enum TIFFReadDirEntryErr err;\n            uint32_t offset = direntry->tdir_offset.toff_long;\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong(&offset);\n            if (isMapped(tif))\n                err = TIFFReadDirEntryData(tif, (uint64_t)offset,\n                                           (tmsize_t)datasize, data);\n            else\n                err = TIFFReadDirEntryDataAndRealloc(tif, (uint64_t)offset,\n                                                     (tmsize_t)datasize, &data);\n            if (err != TIFFReadDirEntryErrOk)\n            {\n                _TIFFfreeExt(tif, data);\n                return (err);\n            }\n        }\n    }\n    else\n    {\n        /* See above comment for the Classic TIFF case */\n        if (original_datasize_clamped <= 8 && datasize <= 8)\n            _TIFFmemcpy(data, &direntry->tdir_offset, datasize);\n        else\n        {\n            enum TIFFReadDirEntryErr err;\n            uint64_t offset = direntry->tdir_offset.toff_long8;\n            if (tif->tif_flags & TIFF_SWAB)\n                TIFFSwabLong8(&offset);\n            if (isMapped(tif))\n                err = TIFFReadDirEntryData(tif, (uint64_t)offset,\n                                           (tmsize_t)datasize, data);\n            else\n                err = TIFFReadDirEntryDataAndRealloc(tif, (uint64_t)offset,\n                                                     (tmsize_t)datasize, &data);\n            if (err != TIFFReadDirEntryErrOk)\n            {\n                _TIFFfreeExt(tif, data);\n                return (err);\n            }\n        }\n    }\n    *value = data;\n    return (TIFFReadDirEntryErrOk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,6 +40,21 @@\n     *count = (uint32_t)target_count64;\n     datasize = (*count) * typesize;\n     assert((tmsize_t)datasize > 0);\n+\n+    /* Before allocating a huge amount of memory for corrupted files, check if\n+     * size of requested memory is not greater than file size.\n+     */\n+    uint64_t filesize = TIFFGetFileSize(tif);\n+    if (datasize > filesize)\n+    {\n+        TIFFWarningExtR(tif, \"ReadDirEntryArray\",\n+                        \"Requested memory size for tag %d (0x%x) %\" PRIu32\n+                        \" is greather than filesize %\" PRIu64\n+                        \". Memory not allocated, tag not read\",\n+                        direntry->tdir_tag, direntry->tdir_tag, datasize,\n+                        filesize);\n+        return (TIFFReadDirEntryErrAlloc);\n+    }\n \n     if (isMapped(tif) && datasize > (uint64_t)tif->tif_size)\n         return TIFFReadDirEntryErrIo;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* Before allocating a huge amount of memory for corrupted files, check if",
                "     * size of requested memory is not greater than file size.",
                "     */",
                "    uint64_t filesize = TIFFGetFileSize(tif);",
                "    if (datasize > filesize)",
                "    {",
                "        TIFFWarningExtR(tif, \"ReadDirEntryArray\",",
                "                        \"Requested memory size for tag %d (0x%x) %\" PRIu32",
                "                        \" is greather than filesize %\" PRIu64",
                "                        \". Memory not allocated, tag not read\",",
                "                        direntry->tdir_tag, direntry->tdir_tag, datasize,",
                "                        filesize);",
                "        return (TIFFReadDirEntryErrAlloc);",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/EstimateStripByteCounts",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/abb4476fd2be87fc8ded3078e019f22f84ee0e8c",
        "commit_title": "Apply 1 suggestion(s) to 1 file(s)",
        "commit_text": "",
        "func_before": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size. */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, module,\n                        \"Requested memory size for StripByteCounts of %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return -1;\n    }\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint16_t n;\n        filesize = TIFFGetFileSize(tif);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "func": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size. */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, module,\n                        \"Requested memory size for StripByteCounts of %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return -1;\n    }\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint16_t n;\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,7 +36,6 @@\n     {\n         uint64_t space;\n         uint16_t n;\n-        filesize = TIFFGetFileSize(tif);\n         if (!(tif->tif_flags & TIFF_BIGTIFF))\n             space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n         else",
        "diff_line_info": {
            "deleted_lines": [
                "        filesize = TIFFGetFileSize(tif);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-6277",
        "func_name": "libtiff/EstimateStripByteCounts",
        "description": "An out-of-memory flaw was found in libtiff. Passing a crafted tiff file to TIFFOpen() API may allow a remote attacker to cause a denial of service via a craft input with size smaller than 379 KB.",
        "git_url": "https://gitlab.com/libtiff/libtiff/-/commit/264a28eff71cf0038ba7b235238512fa594fa42f",
        "commit_title": "At image reading, compare data size of some tags / data structures (StripByteCounts, StripOffsets, StripArray, TIFF directory) with file size to prevent provoked out-of-memory attacks.",
        "commit_text": " See issue #614.  Correct declaration of filesize shadows a previous local. ",
        "func_before": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size. */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, module,\n                        \"Requested memory size for StripByteCounts of %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return -1;\n    }\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint64_t filesize;\n        uint16_t n;\n        filesize = TIFFGetFileSize(tif);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "func": "static int EstimateStripByteCounts(TIFF *tif, TIFFDirEntry *dir,\n                                   uint16_t dircount)\n{\n    static const char module[] = \"EstimateStripByteCounts\";\n\n    TIFFDirEntry *dp;\n    TIFFDirectory *td = &tif->tif_dir;\n    uint32_t strip;\n\n    /* Do not try to load stripbytecount as we will compute it */\n    if (!_TIFFFillStrilesInternal(tif, 0))\n        return -1;\n\n    /* Before allocating a huge amount of memory for corrupted files, check if\n     * size of requested memory is not greater than file size. */\n    uint64_t filesize = TIFFGetFileSize(tif);\n    uint64_t allocsize = (uint64_t)td->td_nstrips * sizeof(uint64_t);\n    if (allocsize > filesize)\n    {\n        TIFFWarningExtR(tif, module,\n                        \"Requested memory size for StripByteCounts of %\" PRIu64\n                        \" is greather than filesize %\" PRIu64\n                        \". Memory not allocated\",\n                        allocsize, filesize);\n        return -1;\n    }\n\n    if (td->td_stripbytecount_p)\n        _TIFFfreeExt(tif, td->td_stripbytecount_p);\n    td->td_stripbytecount_p = (uint64_t *)_TIFFCheckMalloc(\n        tif, td->td_nstrips, sizeof(uint64_t), \"for \\\"StripByteCounts\\\" array\");\n    if (td->td_stripbytecount_p == NULL)\n        return -1;\n\n    if (td->td_compression != COMPRESSION_NONE)\n    {\n        uint64_t space;\n        uint16_t n;\n        filesize = TIFFGetFileSize(tif);\n        if (!(tif->tif_flags & TIFF_BIGTIFF))\n            space = sizeof(TIFFHeaderClassic) + 2 + dircount * 12 + 4;\n        else\n            space = sizeof(TIFFHeaderBig) + 8 + dircount * 20 + 8;\n        /* calculate amount of space used by indirect values */\n        for (dp = dir, n = dircount; n > 0; n--, dp++)\n        {\n            uint32_t typewidth;\n            uint64_t datasize;\n            typewidth = TIFFDataWidth((TIFFDataType)dp->tdir_type);\n            if (typewidth == 0)\n            {\n                TIFFErrorExtR(\n                    tif, module,\n                    \"Cannot determine size of unknown tag type %\" PRIu16,\n                    dp->tdir_type);\n                return -1;\n            }\n            if (dp->tdir_count > UINT64_MAX / typewidth)\n                return -1;\n            datasize = (uint64_t)typewidth * dp->tdir_count;\n            if (!(tif->tif_flags & TIFF_BIGTIFF))\n            {\n                if (datasize <= 4)\n                    datasize = 0;\n            }\n            else\n            {\n                if (datasize <= 8)\n                    datasize = 0;\n            }\n            if (space > UINT64_MAX - datasize)\n                return -1;\n            space += datasize;\n        }\n        if (filesize < space)\n            /* we should perhaps return in error ? */\n            space = filesize;\n        else\n            space = filesize - space;\n        if (td->td_planarconfig == PLANARCONFIG_SEPARATE)\n            space /= td->td_samplesperpixel;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = space;\n        /*\n         * This gross hack handles the case were the offset to\n         * the last strip is past the place where we think the strip\n         * should begin.  Since a strip of data must be contiguous,\n         * it's safe to assume that we've overestimated the amount\n         * of data in the strip and trim this number back accordingly.\n         */\n        strip--;\n        if (td->td_stripoffset_p[strip] >\n            UINT64_MAX - td->td_stripbytecount_p[strip])\n            return -1;\n        if (td->td_stripoffset_p[strip] + td->td_stripbytecount_p[strip] >\n            filesize)\n        {\n            if (td->td_stripoffset_p[strip] >= filesize)\n            {\n                /* Not sure what we should in that case... */\n                td->td_stripbytecount_p[strip] = 0;\n            }\n            else\n            {\n                td->td_stripbytecount_p[strip] =\n                    filesize - td->td_stripoffset_p[strip];\n            }\n        }\n    }\n    else if (isTiled(tif))\n    {\n        uint64_t bytespertile = TIFFTileSize64(tif);\n\n        for (strip = 0; strip < td->td_nstrips; strip++)\n            td->td_stripbytecount_p[strip] = bytespertile;\n    }\n    else\n    {\n        uint64_t rowbytes = TIFFScanlineSize64(tif);\n        uint32_t rowsperstrip = td->td_imagelength / td->td_stripsperimage;\n        for (strip = 0; strip < td->td_nstrips; strip++)\n        {\n            if (rowbytes > 0 && rowsperstrip > UINT64_MAX / rowbytes)\n                return -1;\n            td->td_stripbytecount_p[strip] = rowbytes * rowsperstrip;\n        }\n    }\n    TIFFSetFieldBit(tif, FIELD_STRIPBYTECOUNTS);\n    if (!TIFFFieldSet(tif, FIELD_ROWSPERSTRIP))\n        td->td_rowsperstrip = td->td_imagelength;\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -35,7 +35,6 @@\n     if (td->td_compression != COMPRESSION_NONE)\n     {\n         uint64_t space;\n-        uint64_t filesize;\n         uint16_t n;\n         filesize = TIFFGetFileSize(tif);\n         if (!(tif->tif_flags & TIFF_BIGTIFF))",
        "diff_line_info": {
            "deleted_lines": [
                "        uint64_t filesize;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-32617",
        "func_name": "Exiv2/exiv2/ProcessUTF8Portion",
        "description": "Exiv2 is a command-line utility and C++ library for reading, writing, deleting, and modifying the metadata of image files. An inefficient algorithm (quadratic complexity) was found in Exiv2 versions v0.27.3 and earlier. The inefficient algorithm is triggered when Exiv2 is used to write metadata into a crafted image file. An attacker could potentially exploit the vulnerability to cause a denial of service, if they can trick the victim into running Exiv2 on a crafted image file. The bug is fixed in version v0.27.4. Note that this bug is only triggered when _writing_ the metadata, which is a less frequently used Exiv2 operation than _reading_ the metadata. For example, to trigger the bug in the Exiv2 command-line application, you need to add an extra command-line argument such as `rm`.",
        "git_url": "https://github.com/Exiv2/exiv2/commit/c261fbaa2567687eec6a595d3016212fd6ae648d",
        "commit_title": "Fix quadratic complexity performance bug.",
        "commit_text": "",
        "func_before": "static size_t\nProcessUTF8Portion ( XMLParserAdapter * xmlParser,\n\t\t\t\t\t const XMP_Uns8 *   buffer,\n\t\t\t\t\t size_t\t\t\t\tlength,\n\t\t\t\t\t bool\t\t\t\tlast )\n{\n\tconst XMP_Uns8 * bufEnd = buffer + length;\n\t\n\tconst XMP_Uns8 * spanStart = buffer;\n\tconst XMP_Uns8 * spanEnd;\n\t\t\n\tfor ( spanEnd = spanStart; spanEnd < bufEnd; ++spanEnd ) {\n\n\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) continue;\t// A regular ASCII character.\n\n\t\tif ( *spanEnd >= 0x80 ) {\n\t\t\n\t\t\t// See if this is a multi-byte UTF-8 sequence, or a Latin-1 character to replace.\n\n\t\t\tint uniLen = CountUTF8 ( spanEnd, bufEnd );\n\n\t\t\tif ( uniLen > 0 ) {\n\n\t\t\t\t// A valid UTF-8 character, keep it as-is.\n\t\t\t\tspanEnd += uniLen - 1;\t// ! The loop increment will put back the +1.\n\n\t\t\t} else if ( (uniLen < 0) && (! last) ) {\n\n\t\t\t\t// Have a partial UTF-8 character at the end of the buffer and more input coming.\n\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\t\t\t\treturn (spanEnd - buffer);\n\n\t\t\t} else {\n\n\t\t\t\t// Not a valid UTF-8 sequence. Replace the first byte with the Latin-1 equivalent.\n\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\t\t\t\tconst char * replacement = kReplaceLatin1 [ *spanEnd - 0x80 ];\n\t\t\t\txmlParser->ParseBuffer ( replacement, strlen ( replacement ), false );\n\t\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".\n\n\t\t\t}\n\t\t\n\t\t} else if ( (*spanEnd < 0x20) || (*spanEnd == 0x7F) ) {\n\n\t\t\t// Replace ASCII controls other than tab, LF, and CR with a space.\n\n\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) continue;\n\n\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\t\t\txmlParser->ParseBuffer ( \" \", 1, false );\n\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".\n\t\t\n\t\t} else {\n\t\t\n\t\t\t// See if this is a numeric escape sequence for a prohibited ASCII control.\n\t\t\t\n\t\t\tXMP_Assert ( *spanEnd == '&' );\n\t\t\tint escLen = CountControlEscape ( spanEnd, bufEnd );\n\t\t\t\n\t\t\tif ( escLen < 0 ) {\n\n\t\t\t\t// Have a partial numeric escape in this buffer, wait for more input.\n\t\t\t\tif ( last ) continue;\t// No more buffers, not an escape, absorb as normal input.\n\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\t\t\t\treturn (spanEnd - buffer);\n\n\t\t\t} else if ( escLen > 0 ) {\n\n\t\t\t\t// Have a complete numeric escape to replace.\n\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\t\t\t\txmlParser->ParseBuffer ( \" \", 1, false );\n\t\t\t\tspanStart = spanEnd + escLen;\n\t\t\t\tspanEnd = spanStart - 1;\t// ! The loop continuation will increment spanEnd!\n\n\t\t\t}\n\n\t\t}\n\t\t\n\t}\n\t\n\tXMP_Assert ( spanEnd == bufEnd );\n\n\tif ( spanStart < bufEnd ) xmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n\tif ( last ) xmlParser->ParseBuffer ( \" \", 1, true );\n\t\n\treturn length;\n\n}",
        "func": "static size_t\nProcessUTF8Portion ( XMLParserAdapter * xmlParser,\n\t\t\t\t\t const XMP_Uns8 *   buffer,\n\t\t\t\t\t size_t\t\t\t\tlength,\n\t\t\t\t\t bool\t\t\t\tlast )\n{\n\tconst XMP_Uns8 * bufEnd = buffer + length;\n\t\n\tconst XMP_Uns8 * spanEnd;\n\n\t// `buffer` is copied into this std::string. If `buffer` only\n\t// contains valid UTF-8 and no escape characters, then the copy\n\t// will be identical to the original, but invalid characters are\n\t// replaced - usually with a space character.  This std::string was\n\t// added as a performance fix for:\n\t// https://github.com/Exiv2/exiv2/security/advisories/GHSA-w8mv-g8qq-36mj\n\t// Previously, the code was repeatedly calling\n\t// `xmlParser->ParseBuffer()`, which turned out to have quadratic\n\t// complexity, because expat kept reparsing the entire string from\n\t// the beginning.\n\tstd::string copy;\n\t\t\n\tfor ( spanEnd = buffer; spanEnd < bufEnd; ++spanEnd ) {\n\n\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) {\n\t\t\tcopy.push_back(*spanEnd);\n\t\t\tcontinue;\t// A regular ASCII character.\n\t\t}\n\n\t\tif ( *spanEnd >= 0x80 ) {\n\t\t\n\t\t\t// See if this is a multi-byte UTF-8 sequence, or a Latin-1 character to replace.\n\n\t\t\tint uniLen = CountUTF8 ( spanEnd, bufEnd );\n\n\t\t\tif ( uniLen > 0 ) {\n\n\t\t\t\t// A valid UTF-8 character, keep it as-is.\n\t\t\t\tcopy.append((const char*)spanEnd, uniLen);\n\t\t\t\tspanEnd += uniLen - 1;\t// ! The loop increment will put back the +1.\n\n\t\t\t} else if ( (uniLen < 0) && (! last) ) {\n\n\t\t\t\t// Have a partial UTF-8 character at the end of the buffer and more input coming.\n\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );\n\t\t\t\treturn (spanEnd - buffer);\n\n\t\t\t} else {\n\n\t\t\t\t// Not a valid UTF-8 sequence. Replace the first byte with the Latin-1 equivalent.\n\t\t\t\tconst char * replacement = kReplaceLatin1 [ *spanEnd - 0x80 ];\n\t\t\t\tcopy.append ( replacement );\n\n\t\t\t}\n\t\t\n\t\t} else if ( (*spanEnd < 0x20) || (*spanEnd == 0x7F) ) {\n\n\t\t\t// Replace ASCII controls other than tab, LF, and CR with a space.\n\n\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) {\n\t\t\t\tcopy.push_back(*spanEnd);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tcopy.push_back(' ');\n\t\t\n\t\t} else {\n\t\t\n\t\t\t// See if this is a numeric escape sequence for a prohibited ASCII control.\n\t\t\t\n\t\t\tXMP_Assert ( *spanEnd == '&' );\n\t\t\tint escLen = CountControlEscape ( spanEnd, bufEnd );\n\t\t\t\n\t\t\tif ( escLen < 0 ) {\n\n\t\t\t\t// Have a partial numeric escape in this buffer, wait for more input.\n\t\t\t\tif ( last ) {\n\t\t\t\t\tcopy.push_back('&');\n\t\t\t\t\tcontinue;\t// No more buffers, not an escape, absorb as normal input.\n\t\t\t\t}\n\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );\n\t\t\t\treturn (spanEnd - buffer);\n\n\t\t\t} else if ( escLen > 0 ) {\n\n\t\t\t\t// Have a complete numeric escape to replace.\n\t\t\t\tcopy.push_back(' ');\n\t\t\t\tspanEnd = spanEnd + escLen - 1;\t// ! The loop continuation will increment spanEnd!\n\n\t\t\t} else {\n\t\t\t\tcopy.push_back('&');\n\t\t\t}\n\n\t\t}\n\t\t\n\t}\n\t\n\tXMP_Assert ( spanEnd == bufEnd );\n\tcopy.push_back(' ');\n\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), true );\n\treturn length;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,12 +6,26 @@\n {\n \tconst XMP_Uns8 * bufEnd = buffer + length;\n \t\n-\tconst XMP_Uns8 * spanStart = buffer;\n \tconst XMP_Uns8 * spanEnd;\n+\n+\t// `buffer` is copied into this std::string. If `buffer` only\n+\t// contains valid UTF-8 and no escape characters, then the copy\n+\t// will be identical to the original, but invalid characters are\n+\t// replaced - usually with a space character.  This std::string was\n+\t// added as a performance fix for:\n+\t// https://github.com/Exiv2/exiv2/security/advisories/GHSA-w8mv-g8qq-36mj\n+\t// Previously, the code was repeatedly calling\n+\t// `xmlParser->ParseBuffer()`, which turned out to have quadratic\n+\t// complexity, because expat kept reparsing the entire string from\n+\t// the beginning.\n+\tstd::string copy;\n \t\t\n-\tfor ( spanEnd = spanStart; spanEnd < bufEnd; ++spanEnd ) {\n+\tfor ( spanEnd = buffer; spanEnd < bufEnd; ++spanEnd ) {\n \n-\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) continue;\t// A regular ASCII character.\n+\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) {\n+\t\t\tcopy.push_back(*spanEnd);\n+\t\t\tcontinue;\t// A regular ASCII character.\n+\t\t}\n \n \t\tif ( *spanEnd >= 0x80 ) {\n \t\t\n@@ -22,21 +36,20 @@\n \t\t\tif ( uniLen > 0 ) {\n \n \t\t\t\t// A valid UTF-8 character, keep it as-is.\n+\t\t\t\tcopy.append((const char*)spanEnd, uniLen);\n \t\t\t\tspanEnd += uniLen - 1;\t// ! The loop increment will put back the +1.\n \n \t\t\t} else if ( (uniLen < 0) && (! last) ) {\n \n \t\t\t\t// Have a partial UTF-8 character at the end of the buffer and more input coming.\n-\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n+\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );\n \t\t\t\treturn (spanEnd - buffer);\n \n \t\t\t} else {\n \n \t\t\t\t// Not a valid UTF-8 sequence. Replace the first byte with the Latin-1 equivalent.\n-\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n \t\t\t\tconst char * replacement = kReplaceLatin1 [ *spanEnd - 0x80 ];\n-\t\t\t\txmlParser->ParseBuffer ( replacement, strlen ( replacement ), false );\n-\t\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".\n+\t\t\t\tcopy.append ( replacement );\n \n \t\t\t}\n \t\t\n@@ -44,11 +57,12 @@\n \n \t\t\t// Replace ASCII controls other than tab, LF, and CR with a space.\n \n-\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) continue;\n+\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) {\n+\t\t\t\tcopy.push_back(*spanEnd);\n+\t\t\t\tcontinue;\n+\t\t\t}\n \n-\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n-\t\t\txmlParser->ParseBuffer ( \" \", 1, false );\n-\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".\n+\t\t\tcopy.push_back(' ');\n \t\t\n \t\t} else {\n \t\t\n@@ -60,18 +74,21 @@\n \t\t\tif ( escLen < 0 ) {\n \n \t\t\t\t// Have a partial numeric escape in this buffer, wait for more input.\n-\t\t\t\tif ( last ) continue;\t// No more buffers, not an escape, absorb as normal input.\n-\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n+\t\t\t\tif ( last ) {\n+\t\t\t\t\tcopy.push_back('&');\n+\t\t\t\t\tcontinue;\t// No more buffers, not an escape, absorb as normal input.\n+\t\t\t\t}\n+\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );\n \t\t\t\treturn (spanEnd - buffer);\n \n \t\t\t} else if ( escLen > 0 ) {\n \n \t\t\t\t// Have a complete numeric escape to replace.\n-\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n-\t\t\t\txmlParser->ParseBuffer ( \" \", 1, false );\n-\t\t\t\tspanStart = spanEnd + escLen;\n-\t\t\t\tspanEnd = spanStart - 1;\t// ! The loop continuation will increment spanEnd!\n+\t\t\t\tcopy.push_back(' ');\n+\t\t\t\tspanEnd = spanEnd + escLen - 1;\t// ! The loop continuation will increment spanEnd!\n \n+\t\t\t} else {\n+\t\t\t\tcopy.push_back('&');\n \t\t\t}\n \n \t\t}\n@@ -79,10 +96,8 @@\n \t}\n \t\n \tXMP_Assert ( spanEnd == bufEnd );\n-\n-\tif ( spanStart < bufEnd ) xmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );\n-\tif ( last ) xmlParser->ParseBuffer ( \" \", 1, true );\n-\t\n+\tcopy.push_back(' ');\n+\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), true );\n \treturn length;\n \n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tconst XMP_Uns8 * spanStart = buffer;",
                "\tfor ( spanEnd = spanStart; spanEnd < bufEnd; ++spanEnd ) {",
                "\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) continue;\t// A regular ASCII character.",
                "\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\t\t\t\txmlParser->ParseBuffer ( replacement, strlen ( replacement ), false );",
                "\t\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".",
                "\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) continue;",
                "\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\t\t\txmlParser->ParseBuffer ( \" \", 1, false );",
                "\t\t\tspanStart = spanEnd + 1;\t// ! The loop increment will do \"spanEnd = spanStart\".",
                "\t\t\t\tif ( last ) continue;\t// No more buffers, not an escape, absorb as normal input.",
                "\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\t\t\t\txmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\t\t\t\txmlParser->ParseBuffer ( \" \", 1, false );",
                "\t\t\t\tspanStart = spanEnd + escLen;",
                "\t\t\t\tspanEnd = spanStart - 1;\t// ! The loop continuation will increment spanEnd!",
                "",
                "\tif ( spanStart < bufEnd ) xmlParser->ParseBuffer ( spanStart, (spanEnd - spanStart), false );",
                "\tif ( last ) xmlParser->ParseBuffer ( \" \", 1, true );",
                "\t"
            ],
            "added_lines": [
                "",
                "\t// `buffer` is copied into this std::string. If `buffer` only",
                "\t// contains valid UTF-8 and no escape characters, then the copy",
                "\t// will be identical to the original, but invalid characters are",
                "\t// replaced - usually with a space character.  This std::string was",
                "\t// added as a performance fix for:",
                "\t// https://github.com/Exiv2/exiv2/security/advisories/GHSA-w8mv-g8qq-36mj",
                "\t// Previously, the code was repeatedly calling",
                "\t// `xmlParser->ParseBuffer()`, which turned out to have quadratic",
                "\t// complexity, because expat kept reparsing the entire string from",
                "\t// the beginning.",
                "\tstd::string copy;",
                "\tfor ( spanEnd = buffer; spanEnd < bufEnd; ++spanEnd ) {",
                "\t\tif ( (0x20 <= *spanEnd) && (*spanEnd <= 0x7E) && (*spanEnd != '&') ) {",
                "\t\t\tcopy.push_back(*spanEnd);",
                "\t\t\tcontinue;\t// A regular ASCII character.",
                "\t\t}",
                "\t\t\t\tcopy.append((const char*)spanEnd, uniLen);",
                "\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );",
                "\t\t\t\tcopy.append ( replacement );",
                "\t\t\tif ( (*spanEnd == kTab) || (*spanEnd == kLF) || (*spanEnd == kCR) ) {",
                "\t\t\t\tcopy.push_back(*spanEnd);",
                "\t\t\t\tcontinue;",
                "\t\t\t}",
                "\t\t\tcopy.push_back(' ');",
                "\t\t\t\tif ( last ) {",
                "\t\t\t\t\tcopy.push_back('&');",
                "\t\t\t\t\tcontinue;\t// No more buffers, not an escape, absorb as normal input.",
                "\t\t\t\t}",
                "\t\t\t\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), false );",
                "\t\t\t\tcopy.push_back(' ');",
                "\t\t\t\tspanEnd = spanEnd + escLen - 1;\t// ! The loop continuation will increment spanEnd!",
                "\t\t\t} else {",
                "\t\t\t\tcopy.push_back('&');",
                "\tcopy.push_back(' ');",
                "\txmlParser->ParseBuffer ( copy.c_str(), copy.size(), true );"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29225",
        "func_name": "envoyproxy/envoy/ZlibDecompressorImpl::decompress",
        "description": "Envoy is a cloud-native high-performance proxy. In versions prior to 1.22.1 secompressors accumulate decompressed data into an intermediate buffer before overwriting the body in the decode/encodeBody. This may allow an attacker to zip bomb the decompressor by sending a small highly compressed payload. Maliciously constructed zip files may exhaust system memory and cause a denial of service. Users are advised to upgrade. Users unable to upgrade may consider disabling decompression.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cb4ef0b09200c720dfdb07e097092dd105450343",
        "commit_title": "decompressors: stop decompressing upon excessive compression ratio (#733)",
        "commit_text": " Co-authored-by: Ryan Hamilton <rch@google.com>",
        "func_before": "void ZlibDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                      Buffer::Instance& output_buffer) {\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    zstream_ptr_->avail_in = input_slice.len_;\n    zstream_ptr_->next_in = static_cast<Bytef*>(input_slice.mem_);\n    while (inflateNext()) {\n      if (zstream_ptr_->avail_out == 0) {\n        updateOutput(output_buffer);\n      }\n    }\n  }\n\n  // Flush z_stream and reset its buffer. Otherwise the stale content of the buffer\n  // will pollute output upon the next call to decompress().\n  updateOutput(output_buffer);\n}",
        "func": "void ZlibDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                      Buffer::Instance& output_buffer) {\n  uint64_t limit = MaxInflateRatio * input_buffer.length();\n\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    zstream_ptr_->avail_in = input_slice.len_;\n    zstream_ptr_->next_in = static_cast<Bytef*>(input_slice.mem_);\n    while (inflateNext()) {\n      if (zstream_ptr_->avail_out == 0) {\n        updateOutput(output_buffer);\n      }\n\n      if (Runtime::runtimeFeatureEnabled(\n              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n          (output_buffer.length() > limit)) {\n        stats_.zlib_data_error_.inc();\n        ENVOY_LOG(trace,\n                  \"excessive decompression ratio detected: output \"\n                  \"size {} for input size {}\",\n                  output_buffer.length(), input_buffer.length());\n        return;\n      }\n    }\n  }\n\n  // Flush z_stream and reset its buffer. Otherwise the stale content of the buffer\n  // will pollute output upon the next call to decompress().\n  updateOutput(output_buffer);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,24 @@\n void ZlibDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                       Buffer::Instance& output_buffer) {\n+  uint64_t limit = MaxInflateRatio * input_buffer.length();\n+\n   for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n     zstream_ptr_->avail_in = input_slice.len_;\n     zstream_ptr_->next_in = static_cast<Bytef*>(input_slice.mem_);\n     while (inflateNext()) {\n       if (zstream_ptr_->avail_out == 0) {\n         updateOutput(output_buffer);\n+      }\n+\n+      if (Runtime::runtimeFeatureEnabled(\n+              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n+          (output_buffer.length() > limit)) {\n+        stats_.zlib_data_error_.inc();\n+        ENVOY_LOG(trace,\n+                  \"excessive decompression ratio detected: output \"\n+                  \"size {} for input size {}\",\n+                  output_buffer.length(), input_buffer.length());\n+        return;\n       }\n     }\n   }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  uint64_t limit = MaxInflateRatio * input_buffer.length();",
                "",
                "      }",
                "",
                "      if (Runtime::runtimeFeatureEnabled(",
                "              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&",
                "          (output_buffer.length() > limit)) {",
                "        stats_.zlib_data_error_.inc();",
                "        ENVOY_LOG(trace,",
                "                  \"excessive decompression ratio detected: output \"",
                "                  \"size {} for input size {}\",",
                "                  output_buffer.length(), input_buffer.length());",
                "        return;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29225",
        "func_name": "envoyproxy/envoy/ZstdDecompressorImpl::decompress",
        "description": "Envoy is a cloud-native high-performance proxy. In versions prior to 1.22.1 secompressors accumulate decompressed data into an intermediate buffer before overwriting the body in the decode/encodeBody. This may allow an attacker to zip bomb the decompressor by sending a small highly compressed payload. Maliciously constructed zip files may exhaust system memory and cause a denial of service. Users are advised to upgrade. Users unable to upgrade may consider disabling decompression.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cb4ef0b09200c720dfdb07e097092dd105450343",
        "commit_title": "decompressors: stop decompressing upon excessive compression ratio (#733)",
        "commit_text": " Co-authored-by: Ryan Hamilton <rch@google.com>",
        "func_before": "void ZstdDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                      Buffer::Instance& output_buffer) {\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    if (input_slice.len_ > 0) {\n      if (ddict_manager_ && !is_dictionary_set_) {\n        is_dictionary_set_ = true;\n        // If id == 0, it means that dictionary id could not be decoded.\n        dictionary_id_ =\n            ZSTD_getDictID_fromFrame(static_cast<uint8_t*>(input_slice.mem_), input_slice.len_);\n        if (dictionary_id_ != 0) {\n          auto dictionary = ddict_manager_->getDictionaryById(dictionary_id_);\n          if (!dictionary) {\n            stats_.zstd_dictionary_error_.inc();\n            return;\n          }\n          const size_t result = ZSTD_DCtx_refDDict(dctx_.get(), dictionary);\n          if (isError(result)) {\n            return;\n          }\n        }\n      }\n\n      setInput(input_slice);\n      if (!process(output_buffer)) {\n        return;\n      }\n    }\n  }\n}",
        "func": "void ZstdDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                      Buffer::Instance& output_buffer) {\n  uint64_t limit = MaxInflateRatio * input_buffer.length();\n\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    if (input_slice.len_ > 0) {\n      if (ddict_manager_ && !is_dictionary_set_) {\n        is_dictionary_set_ = true;\n        // If id == 0, it means that dictionary id could not be decoded.\n        dictionary_id_ =\n            ZSTD_getDictID_fromFrame(static_cast<uint8_t*>(input_slice.mem_), input_slice.len_);\n        if (dictionary_id_ != 0) {\n          auto dictionary = ddict_manager_->getDictionaryById(dictionary_id_);\n          if (!dictionary) {\n            stats_.zstd_dictionary_error_.inc();\n            return;\n          }\n          const size_t result = ZSTD_DCtx_refDDict(dctx_.get(), dictionary);\n          if (isError(result)) {\n            return;\n          }\n        }\n      }\n\n      setInput(input_slice);\n      if (!process(output_buffer)) {\n        return;\n      }\n      if (Runtime::runtimeFeatureEnabled(\n              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n          (output_buffer.length() > limit)) {\n        stats_.zstd_generic_error_.inc();\n        ENVOY_LOG(trace,\n                  \"excessive decompression ratio detected: output \"\n                  \"size {} for input size {}\",\n                  output_buffer.length(), input_buffer.length());\n        return;\n      }\n    }\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n void ZstdDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                       Buffer::Instance& output_buffer) {\n+  uint64_t limit = MaxInflateRatio * input_buffer.length();\n+\n   for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n     if (input_slice.len_ > 0) {\n       if (ddict_manager_ && !is_dictionary_set_) {\n@@ -24,6 +26,16 @@\n       if (!process(output_buffer)) {\n         return;\n       }\n+      if (Runtime::runtimeFeatureEnabled(\n+              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n+          (output_buffer.length() > limit)) {\n+        stats_.zstd_generic_error_.inc();\n+        ENVOY_LOG(trace,\n+                  \"excessive decompression ratio detected: output \"\n+                  \"size {} for input size {}\",\n+                  output_buffer.length(), input_buffer.length());\n+        return;\n+      }\n     }\n   }\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  uint64_t limit = MaxInflateRatio * input_buffer.length();",
                "",
                "      if (Runtime::runtimeFeatureEnabled(",
                "              \"envoy.reloadable_features.enable_compression_bomb_protection\") &&",
                "          (output_buffer.length() > limit)) {",
                "        stats_.zstd_generic_error_.inc();",
                "        ENVOY_LOG(trace,",
                "                  \"excessive decompression ratio detected: output \"",
                "                  \"size {} for input size {}\",",
                "                  output_buffer.length(), input_buffer.length());",
                "        return;",
                "      }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29225",
        "func_name": "envoyproxy/envoy/BrotliDecompressorImpl::decompress",
        "description": "Envoy is a cloud-native high-performance proxy. In versions prior to 1.22.1 secompressors accumulate decompressed data into an intermediate buffer before overwriting the body in the decode/encodeBody. This may allow an attacker to zip bomb the decompressor by sending a small highly compressed payload. Maliciously constructed zip files may exhaust system memory and cause a denial of service. Users are advised to upgrade. Users unable to upgrade may consider disabling decompression.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cb4ef0b09200c720dfdb07e097092dd105450343",
        "commit_title": "decompressors: stop decompressing upon excessive compression ratio (#733)",
        "commit_text": " Co-authored-by: Ryan Hamilton <rch@google.com>",
        "func_before": "void BrotliDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                        Buffer::Instance& output_buffer) {\n  Common::BrotliContext ctx(chunk_size_);\n\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    ctx.avail_in_ = input_slice.len_;\n    ctx.next_in_ = static_cast<uint8_t*>(input_slice.mem_);\n\n    while (ctx.avail_in_ > 0) {\n      if (!process(ctx, output_buffer)) {\n        ctx.finalizeOutput(output_buffer);\n        return;\n      }\n    }\n  }\n\n  // Even though the input has been fully consumed by the decoder it still can\n  // be unfolded into output not fitting the output chunk. Thus keep processing\n  // until the decoder's output is fully depleted.\n  bool success;\n  do {\n    success = process(ctx, output_buffer);\n  } while (success && BrotliDecoderHasMoreOutput(state_.get()));\n\n  ctx.finalizeOutput(output_buffer);\n}",
        "func": "void BrotliDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                        Buffer::Instance& output_buffer) {\n  Common::BrotliContext ctx(chunk_size_, MaxInflateRatio * input_buffer.length());\n\n  for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n    ctx.avail_in_ = input_slice.len_;\n    ctx.next_in_ = static_cast<uint8_t*>(input_slice.mem_);\n\n    while (ctx.avail_in_ > 0) {\n      if (!process(ctx, output_buffer)) {\n        ctx.finalizeOutput(output_buffer);\n        return;\n      }\n    }\n  }\n\n  // Even though the input has been fully consumed by the decoder it still can\n  // be unfolded into output not fitting the output chunk. Thus keep processing\n  // until the decoder's output is fully depleted.\n  bool success;\n  do {\n    success = process(ctx, output_buffer);\n  } while (success && BrotliDecoderHasMoreOutput(state_.get()));\n\n  ctx.finalizeOutput(output_buffer);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n void BrotliDecompressorImpl::decompress(const Buffer::Instance& input_buffer,\n                                         Buffer::Instance& output_buffer) {\n-  Common::BrotliContext ctx(chunk_size_);\n+  Common::BrotliContext ctx(chunk_size_, MaxInflateRatio * input_buffer.length());\n \n   for (const Buffer::RawSlice& input_slice : input_buffer.getRawSlices()) {\n     ctx.avail_in_ = input_slice.len_;",
        "diff_line_info": {
            "deleted_lines": [
                "  Common::BrotliContext ctx(chunk_size_);"
            ],
            "added_lines": [
                "  Common::BrotliContext ctx(chunk_size_, MaxInflateRatio * input_buffer.length());"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29225",
        "func_name": "envoyproxy/envoy/BrotliDecompressorImpl::process",
        "description": "Envoy is a cloud-native high-performance proxy. In versions prior to 1.22.1 secompressors accumulate decompressed data into an intermediate buffer before overwriting the body in the decode/encodeBody. This may allow an attacker to zip bomb the decompressor by sending a small highly compressed payload. Maliciously constructed zip files may exhaust system memory and cause a denial of service. Users are advised to upgrade. Users unable to upgrade may consider disabling decompression.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cb4ef0b09200c720dfdb07e097092dd105450343",
        "commit_title": "decompressors: stop decompressing upon excessive compression ratio (#733)",
        "commit_text": " Co-authored-by: Ryan Hamilton <rch@google.com>",
        "func_before": "bool BrotliDecompressorImpl::process(Common::BrotliContext& ctx, Buffer::Instance& output_buffer) {\n  BrotliDecoderResult result;\n  result = BrotliDecoderDecompressStream(state_.get(), &ctx.avail_in_, &ctx.next_in_,\n                                         &ctx.avail_out_, &ctx.next_out_, nullptr);\n  if (result == BROTLI_DECODER_RESULT_ERROR) {\n    // TODO(rojkov): currently the Brotli library doesn't specify possible errors in its API. Add\n    // more detailed stats when they are documented.\n    stats_.brotli_error_.inc();\n    return false;\n  }\n\n  ctx.updateOutput(output_buffer);\n\n  return true;\n}",
        "func": "bool BrotliDecompressorImpl::process(Common::BrotliContext& ctx, Buffer::Instance& output_buffer) {\n  BrotliDecoderResult result;\n  result = BrotliDecoderDecompressStream(state_.get(), &ctx.avail_in_, &ctx.next_in_,\n                                         &ctx.avail_out_, &ctx.next_out_, nullptr);\n  if (result == BROTLI_DECODER_RESULT_ERROR) {\n    // TODO(rojkov): currently the Brotli library doesn't specify possible errors in its API. Add\n    // more detailed stats when they are documented.\n    stats_.brotli_error_.inc();\n    return false;\n  }\n\n  if (Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n      (output_buffer.length() > ctx.max_output_size_)) {\n    stats_.brotli_error_.inc();\n    return false;\n  }\n\n  ctx.updateOutput(output_buffer);\n\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,13 @@\n     return false;\n   }\n \n+  if (Runtime::runtimeFeatureEnabled(\n+          \"envoy.reloadable_features.enable_compression_bomb_protection\") &&\n+      (output_buffer.length() > ctx.max_output_size_)) {\n+    stats_.brotli_error_.inc();\n+    return false;\n+  }\n+\n   ctx.updateOutput(output_buffer);\n \n   return true;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (Runtime::runtimeFeatureEnabled(",
                "          \"envoy.reloadable_features.enable_compression_bomb_protection\") &&",
                "      (output_buffer.length() > ctx.max_output_size_)) {",
                "    stats_.brotli_error_.inc();",
                "    return false;",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-29225",
        "func_name": "envoyproxy/envoy/BrotliContext::BrotliContext",
        "description": "Envoy is a cloud-native high-performance proxy. In versions prior to 1.22.1 secompressors accumulate decompressed data into an intermediate buffer before overwriting the body in the decode/encodeBody. This may allow an attacker to zip bomb the decompressor by sending a small highly compressed payload. Maliciously constructed zip files may exhaust system memory and cause a denial of service. Users are advised to upgrade. Users unable to upgrade may consider disabling decompression.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/cb4ef0b09200c720dfdb07e097092dd105450343",
        "commit_title": "decompressors: stop decompressing upon excessive compression ratio (#733)",
        "commit_text": " Co-authored-by: Ryan Hamilton <rch@google.com>",
        "func_before": "BrotliContext::BrotliContext(const uint32_t chunk_size)\n    : chunk_size_{chunk_size}, chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{},\n      next_out_{chunk_ptr_.get()}, avail_in_{0}, avail_out_{chunk_size} {}",
        "func": "BrotliContext::BrotliContext(uint32_t chunk_size, uint32_t max_output_size)\n    : max_output_size_{max_output_size}, chunk_size_{chunk_size},\n      chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{}, next_out_{chunk_ptr_.get()},\n      avail_in_{0}, avail_out_{chunk_size} {}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,3 +1,4 @@\n-BrotliContext::BrotliContext(const uint32_t chunk_size)\n-    : chunk_size_{chunk_size}, chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{},\n-      next_out_{chunk_ptr_.get()}, avail_in_{0}, avail_out_{chunk_size} {}\n+BrotliContext::BrotliContext(uint32_t chunk_size, uint32_t max_output_size)\n+    : max_output_size_{max_output_size}, chunk_size_{chunk_size},\n+      chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{}, next_out_{chunk_ptr_.get()},\n+      avail_in_{0}, avail_out_{chunk_size} {}",
        "diff_line_info": {
            "deleted_lines": [
                "BrotliContext::BrotliContext(const uint32_t chunk_size)",
                "    : chunk_size_{chunk_size}, chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{},",
                "      next_out_{chunk_ptr_.get()}, avail_in_{0}, avail_out_{chunk_size} {}"
            ],
            "added_lines": [
                "BrotliContext::BrotliContext(uint32_t chunk_size, uint32_t max_output_size)",
                "    : max_output_size_{max_output_size}, chunk_size_{chunk_size},",
                "      chunk_ptr_{std::make_unique<uint8_t[]>(chunk_size)}, next_in_{}, next_out_{chunk_ptr_.get()},",
                "      avail_in_{0}, avail_out_{chunk_size} {}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1605",
        "func_name": "radareorg/radare2/patch_relocs",
        "description": "Denial of Service in GitHub repository radareorg/radare2 prior to 5.8.6.",
        "git_url": "https://github.com/radareorg/radare2/commit/508a6307045441defd1bef0999a1f7052097613f",
        "commit_title": "fix segfault when bin->symbols is NULL #21503",
        "commit_text": "",
        "func_before": "static RList *patch_relocs(RBin *b) {\n\tr_return_val_if_fail (b && b->iob.io && b->iob.io->desc, NULL);\n\tRBinObject *bo = r_bin_cur_object (b);\n\tRIO *io = b->iob.io;\n\tif (!bo || !bo->bin_obj) {\n\t\treturn NULL;\n\t}\n\tstruct r_bin_coff_obj *bin = (struct r_bin_coff_obj*)bo->bin_obj;\n\tif (bin->hdr.f_flags & COFF_FLAGS_TI_F_EXEC) {\n\t\treturn NULL;\n\t}\n\tif (!(io->cached & R_PERM_W)) {\n\t\teprintf (\n\t\t\t\"Warning: please run r2 with -e io.cache=true to patch \"\n\t\t\t\"relocations\\n\");\n\t\treturn NULL;\n\t}\n\n\tsize_t nimports = 0;\n\tint i;\n\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {\n\t\tif (is_imported_symbol (&bin->symbols[i])) {\n\t\t\tnimports++;\n\t\t}\n\t\ti += bin->symbols[i].n_numaux;\n\t}\n\tut64 m_vaddr = UT64_MAX;\n\tif (nimports) {\n\t\tut64 offset = 0;\n\t\tRIOBank *bank = b->iob.bank_get (io, io->bank);\n\t\tRListIter *iter;\n\t\tRIOMapRef *mapref;\n\t\tr_list_foreach (bank->maprefs, iter, mapref) {\n\t\t\tRIOMap *map = b->iob.map_get (io, mapref->id);\n\t\t\tif (r_io_map_end (map) > offset) {\n\t\t\t\toffset = r_io_map_end (map);\n\t\t\t}\n\t\t}\n\t\tm_vaddr = R_ROUND (offset, 16);\n\t\tut64 size = nimports * BYTES_PER_IMP_RELOC;\n\t\tchar *muri = r_str_newf (\"malloc://%\" PFMT64u, size);\n\t\tRIODesc *desc = b->iob.open_at (io, muri, R_PERM_R, 0664, m_vaddr);\n\t\tfree (muri);\n\t\tif (!desc) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\tRIOMap *map = b->iob.map_get_at (io, m_vaddr);\n\t\tif (!map) {\n\t\t\treturn NULL;\n\t\t}\n\t\tmap->name = strdup (\".imports.r2\");\n\t}\n\n\treturn _relocs_list (b, bin, true, m_vaddr);\n}",
        "func": "static RList *patch_relocs(RBin *b) {\n\tr_return_val_if_fail (b && b->iob.io && b->iob.io->desc, NULL);\n\tRBinObject *bo = r_bin_cur_object (b);\n\tRIO *io = b->iob.io;\n\tif (!bo || !bo->bin_obj) {\n\t\treturn NULL;\n\t}\n\tstruct r_bin_coff_obj *bin = (struct r_bin_coff_obj*)bo->bin_obj;\n\tif (bin->hdr.f_flags & COFF_FLAGS_TI_F_EXEC) {\n\t\treturn NULL;\n\t}\n\tif (!(io->cached & R_PERM_W)) {\n\t\teprintf (\n\t\t\t\"Warning: please run r2 with -e io.cache=true to patch \"\n\t\t\t\"relocations\\n\");\n\t\treturn NULL;\n\t}\n\n\tsize_t nimports = 0;\n\tint i;\n\tif (bin->symbols) {\n\t\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {\n\t\t\tif (is_imported_symbol (&bin->symbols[i])) {\n\t\t\t\tnimports++;\n\t\t\t}\n\t\t\ti += bin->symbols[i].n_numaux;\n\t\t}\n\t}\n\tut64 m_vaddr = UT64_MAX;\n\tif (nimports) {\n\t\tut64 offset = 0;\n\t\tRIOBank *bank = b->iob.bank_get (io, io->bank);\n\t\tRListIter *iter;\n\t\tRIOMapRef *mapref;\n\t\tr_list_foreach (bank->maprefs, iter, mapref) {\n\t\t\tRIOMap *map = b->iob.map_get (io, mapref->id);\n\t\t\tif (r_io_map_end (map) > offset) {\n\t\t\t\toffset = r_io_map_end (map);\n\t\t\t}\n\t\t}\n\t\tm_vaddr = R_ROUND (offset, 16);\n\t\tut64 size = nimports * BYTES_PER_IMP_RELOC;\n\t\tchar *muri = r_str_newf (\"malloc://%\" PFMT64u, size);\n\t\tRIODesc *desc = b->iob.open_at (io, muri, R_PERM_R, 0664, m_vaddr);\n\t\tfree (muri);\n\t\tif (!desc) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\tRIOMap *map = b->iob.map_get_at (io, m_vaddr);\n\t\tif (!map) {\n\t\t\treturn NULL;\n\t\t}\n\t\tmap->name = strdup (\".imports.r2\");\n\t}\n\n\treturn _relocs_list (b, bin, true, m_vaddr);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,11 +18,13 @@\n \n \tsize_t nimports = 0;\n \tint i;\n-\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {\n-\t\tif (is_imported_symbol (&bin->symbols[i])) {\n-\t\t\tnimports++;\n+\tif (bin->symbols) {\n+\t\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {\n+\t\t\tif (is_imported_symbol (&bin->symbols[i])) {\n+\t\t\t\tnimports++;\n+\t\t\t}\n+\t\t\ti += bin->symbols[i].n_numaux;\n \t\t}\n-\t\ti += bin->symbols[i].n_numaux;\n \t}\n \tut64 m_vaddr = UT64_MAX;\n \tif (nimports) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {",
                "\t\tif (is_imported_symbol (&bin->symbols[i])) {",
                "\t\t\tnimports++;",
                "\t\ti += bin->symbols[i].n_numaux;"
            ],
            "added_lines": [
                "\tif (bin->symbols) {",
                "\t\tfor (i = 0; i < bin->hdr.f_nsyms; i++) {",
                "\t\t\tif (is_imported_symbol (&bin->symbols[i])) {",
                "\t\t\t\tnimports++;",
                "\t\t\t}",
                "\t\t\ti += bin->symbols[i].n_numaux;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1654",
        "func_name": "gpac/gf_filter_pid_merge_properties_internal",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.4.0.",
        "git_url": "https://github.com/gpac/gpac/commit/2c055153d401b8c49422971e3a0159869652d3da",
        "commit_title": "fixed #2429",
        "commit_text": "",
        "func_before": "static GF_Err gf_filter_pid_merge_properties_internal(GF_FilterPid *dst_pid, GF_FilterPid *src_pid, gf_filter_prop_filter filter_prop, void *cbk, Bool is_merge)\n{\n\tGF_PropertyMap *dst_props, *src_props = NULL, *old_dst_props=NULL;\n\tif (PID_IS_INPUT(dst_pid)) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Attempt to reset all properties on input PID in filter %s - ignoring\\n\", dst_pid->filter->name));\n\t\treturn GF_BAD_PARAM;\n\t}\n\tif (is_merge) {\n\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\told_dst_props = gf_list_last(dst_pid->properties);\n\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t}\n\n\t//don't merge properties with old state we merge with source pid\n\tdst_props = check_new_pid_props(dst_pid, GF_FALSE);\n\n\tif (!dst_props) {\n\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"No properties for destination pid in filter %s, ignoring reset\\n\", dst_pid->filter->name));\n\t\treturn GF_OUT_OF_MEM;\n\t}\n\t//if pid is input, use the current properties - cf filter_pid_get_prop_map\n\tif (PID_IS_INPUT(src_pid)) {\n\t\tGF_FilterPidInst *pidi = (GF_FilterPidInst *)src_pid;\n\t\tif (!pidi->props) {\n\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\t\tpidi->props = gf_list_get(src_pid->pid->properties, 0);\n\t\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t\t\tassert(pidi->props);\n\t\t\tsafe_int_inc(&pidi->props->reference_count);\n\t\t}\n\t\tsrc_props = pidi->props;\n\t}\n\t//move to rela pid\n\tsrc_pid = src_pid->pid;\n\t//this is a copy props on output pid\n\tif (!src_props) {\n\t\t//our list is not thread-safe, so we must lock the filter when destroying the props\n\t\t//otherwise gf_list_last() (this caller) might use the last entry while another thread sets this last entry to NULL\n\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\tsrc_props = gf_list_last(src_pid->properties);\n\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t\tif (!src_props) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"No properties to copy from pid %s in filter %s, ignoring merge\\n\", src_pid->name, src_pid->filter->name));\n\t\t\treturn GF_OK;\n\t\t}\n\t}\n\tif (src_pid->name && !old_dst_props)\n\t\tgf_filter_pid_set_name(dst_pid, src_pid->name);\n\n\tif (!is_merge) {\n\t\tgf_props_reset(dst_props);\n\t} else {\n\t\t//we created a new map\n\t\tif (old_dst_props && (old_dst_props!=dst_props)) {\n\t\t\tGF_Err e = gf_props_merge_property(dst_props, old_dst_props, NULL, NULL);\n\t\t\tif (e) return e;\n\t\t}\n\t}\n\treturn gf_props_merge_property(dst_props, src_props, filter_prop, cbk);\n}",
        "func": "static GF_Err gf_filter_pid_merge_properties_internal(GF_FilterPid *dst_pid, GF_FilterPid *src_pid, gf_filter_prop_filter filter_prop, void *cbk, Bool is_merge)\n{\n\tGF_PropertyMap *dst_props, *src_props = NULL, *old_dst_props=NULL;\n\tif (PID_IS_INPUT(dst_pid)) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Attempt to reset all properties on input PID in filter %s - ignoring\\n\", dst_pid->filter->name));\n\t\treturn GF_BAD_PARAM;\n\t}\n\tif (is_merge) {\n\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\told_dst_props = gf_list_last(dst_pid->properties);\n\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t}\n\n\t//don't merge properties with old state we merge with source pid\n\tdst_props = check_new_pid_props(dst_pid, GF_FALSE);\n\n\tif (!dst_props) {\n\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"No properties for destination pid in filter %s, ignoring reset\\n\", dst_pid->filter->name));\n\t\treturn GF_OUT_OF_MEM;\n\t}\n\t//if pid is input, use the current properties - cf filter_pid_get_prop_map\n\tif (PID_IS_INPUT(src_pid)) {\n\t\tGF_FilterPidInst *pidi = (GF_FilterPidInst *)src_pid;\n\t\tif (!pidi->props) {\n\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\t\tpidi->props = gf_list_get(src_pid->pid->properties, 0);\n\t\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t\t\tassert(pidi->props);\n\t\t\tsafe_int_inc(&pidi->props->reference_count);\n\t\t}\n\t\tsrc_props = pidi->props;\n\t}\n\t//move to real pid\n\tsrc_pid = src_pid->pid;\n\t//this is a copy props on output pid\n\tif (!src_props) {\n\t\t//our list is not thread-safe, so we must lock the filter when destroying the props\n\t\t//otherwise gf_list_last() (this caller) might use the last entry while another thread sets this last entry to NULL\n\t\tgf_mx_p(src_pid->filter->tasks_mx);\n\t\tsrc_props = gf_list_last(src_pid->properties);\n\t\tgf_mx_v(src_pid->filter->tasks_mx);\n\t\tif (!src_props) {\n\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"No properties to copy from pid %s in filter %s, ignoring merge\\n\", src_pid->name, src_pid->filter->name));\n\t\t\treturn GF_OK;\n\t\t}\n\t}\n\tif (src_pid->name && !old_dst_props)\n\t\tgf_filter_pid_set_name(dst_pid, src_pid->name);\n\n\tif (!is_merge) {\n\t\tgf_props_reset(dst_props);\n\t} else {\n\t\t//we created a new map\n\t\tif (old_dst_props && (old_dst_props!=dst_props)) {\n\t\t\tGF_Err e = gf_props_merge_property(dst_props, old_dst_props, NULL, NULL);\n\t\t\tif (e) return e;\n\t\t}\n\t}\n\treturn gf_props_merge_property(dst_props, src_props, filter_prop, cbk);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,7 @@\n \t\t}\n \t\tsrc_props = pidi->props;\n \t}\n-\t//move to rela pid\n+\t//move to real pid\n \tsrc_pid = src_pid->pid;\n \t//this is a copy props on output pid\n \tif (!src_props) {",
        "diff_line_info": {
            "deleted_lines": [
                "\t//move to rela pid"
            ],
            "added_lines": [
                "\t//move to real pid"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1654",
        "func_name": "gpac/gf_filter_pid_detach_task",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.4.0.",
        "git_url": "https://github.com/gpac/gpac/commit/2c055153d401b8c49422971e3a0159869652d3da",
        "commit_title": "fixed #2429",
        "commit_text": "",
        "func_before": "void gf_filter_pid_detach_task(GF_FSTask *task)\n{\n\tu32 i, count;\n\tGF_Filter *filter = task->filter;\n\tGF_FilterPid *pid = task->pid->pid;\n\tGF_FilterPidInst *pidinst=NULL;\n\tGF_Filter *new_chain_input = task->udta;\n\n\t//we may have concurrent reset (due to play/stop/seek) and caps renegotiation\n\t//wait for the pid to be reset before detaching\n\tif (pid->filter->stream_reset_pending) {\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\tif (new_chain_input->in_pid_connection_pending) {\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\n\tcount = pid->num_destinations;\n\tfor (i=0; i<count; i++) {\n\t\tpidinst = gf_list_get(pid->destinations, i);\n\t\tif (pidinst->filter==filter) {\n\t\t\tbreak;\n\t\t}\n\t\tpidinst=NULL;\n\t}\n\t//flush any packets dispatched before detaching\n\tif (pidinst && gf_fq_count(pidinst->packets)) {\n\t\tBool in_process = filter->in_process;\n\t\tfilter->in_process = GF_FALSE;\n\t\t//prevent pid_would_block calls\n\t\tfilter->in_force_flush = GF_TRUE;\n\t\tpidinst->force_flush = GF_TRUE;\n\t\tgf_filter_process_inline(filter);\n\t\tpidinst->force_flush = GF_FALSE;\n\t\tfilter->in_force_flush = GF_FALSE;\n\t\tfilter->in_process = in_process;\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\n\tassert(filter->freg->configure_pid);\n\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Filter %s pid %s detach from %s\\n\", task->pid->pid->filter->name, task->pid->pid->name, task->filter->name));\n\tassert(pid->filter->detach_pid_tasks_pending);\n\tsafe_int_dec(&pid->filter->detach_pid_tasks_pending);\n\n\t//first connection of this PID to this filter\n\tif (!pidinst) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to detach PID %s not present in filter %s inputs\\n\",  pid->name, filter->name));\n\t\t//assert(!new_chain_input->swap_pidinst_dst);\n\t\tassert(!new_chain_input->swap_pidinst_src);\n\t\tnew_chain_input->swap_needs_init = GF_FALSE;\n\t\treturn;\n\t}\n\n\t//detach props\n\tif (pidinst->props) {\n\t\tassert(pidinst->props->reference_count);\n\t\tif (safe_int_dec(& pidinst->props->reference_count) == 0) {\n\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n\t\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_props_del(pidinst->props);\n\t\t}\n\t}\n\tpidinst->props = NULL;\n\n\tgf_mx_p(filter->tasks_mx);\n\t//detach pid - remove all packets in our pid instance and also update filter pending_packets\n\tcount = gf_fq_count(pidinst->packets);\n\tassert(count <= filter->pending_packets);\n\tsafe_int_sub(&filter->pending_packets, (s32) count);\n\tgf_filter_pid_inst_reset(pidinst);\n\tpidinst->pid = NULL;\n\tgf_list_del_item(pid->destinations, pidinst);\n\tpid->num_destinations = gf_list_count(pid->destinations);\n\tgf_list_del_item(filter->input_pids, pidinst);\n\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\tif (!filter->num_input_pids)\n\t\tfilter->single_source = NULL;\n\tgf_mx_v(filter->tasks_mx);\n\n\tif (!filter->detached_pid_inst) {\n\t\tfilter->detached_pid_inst = gf_list_new();\n\t}\n\tif (gf_list_find(filter->detached_pid_inst, pidinst)<0)\n\t\tgf_list_add(filter->detached_pid_inst, pidinst);\n\n\t//we are done, reset filter swap instance so that connection can take place\n\tif (new_chain_input->swap_needs_init) {\n\t\tnew_chain_input->swap_pidinst_dst = NULL;\n\t\tnew_chain_input->swap_pidinst_src = NULL;\n\t\tnew_chain_input->swap_needs_init = GF_FALSE;\n\t}\n\tassert(new_chain_input->detach_pid_tasks_pending);\n\tsafe_int_dec(&new_chain_input->detach_pid_tasks_pending);\n}",
        "func": "void gf_filter_pid_detach_task(GF_FSTask *task)\n{\n\tu32 i, count;\n\tGF_Filter *filter = task->filter;\n\tGF_FilterPid *pid = task->pid->pid;\n\tGF_FilterPidInst *pidinst=NULL;\n\tGF_Filter *new_chain_input = task->udta;\n\n\t//we may have concurrent reset (due to play/stop/seek) and caps renegotiation\n\t//wait for the pid to be reset before detaching\n\tif (pid->filter->stream_reset_pending) {\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\tif (new_chain_input->in_pid_connection_pending) {\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\n\tcount = pid->num_destinations;\n\tfor (i=0; i<count; i++) {\n\t\tpidinst = gf_list_get(pid->destinations, i);\n\t\tif (pidinst->filter==filter) {\n\t\t\tbreak;\n\t\t}\n\t\tpidinst=NULL;\n\t}\n\t//flush any packets dispatched before detaching\n\tif (pidinst && gf_fq_count(pidinst->packets)) {\n\t\tBool in_process = filter->in_process;\n\t\tfilter->in_process = GF_FALSE;\n\t\t//prevent pid_would_block calls\n\t\tfilter->in_force_flush = GF_TRUE;\n\t\tpidinst->force_flush = GF_TRUE;\n\t\tgf_filter_process_inline(filter);\n\t\tpidinst->force_flush = GF_FALSE;\n\t\tfilter->in_force_flush = GF_FALSE;\n\t\tfilter->in_process = in_process;\n\t\tTASK_REQUEUE(task)\n\t\treturn;\n\t}\n\n\tassert(filter->freg->configure_pid);\n\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Filter %s pid %s detach from %s\\n\", task->pid->pid->filter->name, task->pid->pid->name, task->filter->name));\n\tassert(pid->filter->detach_pid_tasks_pending);\n\tsafe_int_dec(&pid->filter->detach_pid_tasks_pending);\n\n\t//first connection of this PID to this filter\n\tif (!pidinst) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to detach PID %s not present in filter %s inputs\\n\",  pid->name, filter->name));\n\t\t//when swaping encoder, we may have swap_pidinst_dst not NULL so only check swap_pidinst_src\n\t\tassert(!new_chain_input->swap_pidinst_src);\n\t\tnew_chain_input->swap_needs_init = GF_FALSE;\n\t\treturn;\n\t}\n\n\t//detach props\n\tif (pidinst->props) {\n\t\tassert(pidinst->props->reference_count);\n\t\tif (safe_int_dec(& pidinst->props->reference_count) == 0) {\n\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n\t\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_props_del(pidinst->props);\n\t\t}\n\t}\n\tpidinst->props = NULL;\n\n\tgf_mx_p(filter->tasks_mx);\n\t//detach pid - remove all packets in our pid instance and also update filter pending_packets\n\tcount = gf_fq_count(pidinst->packets);\n\tassert(count <= filter->pending_packets);\n\tsafe_int_sub(&filter->pending_packets, (s32) count);\n\tgf_filter_pid_inst_reset(pidinst);\n\tpidinst->pid = NULL;\n\tgf_list_del_item(pid->destinations, pidinst);\n\tpid->num_destinations = gf_list_count(pid->destinations);\n\tgf_list_del_item(filter->input_pids, pidinst);\n\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\tif (!filter->num_input_pids)\n\t\tfilter->single_source = NULL;\n\tgf_mx_v(filter->tasks_mx);\n\n\tif (!filter->detached_pid_inst) {\n\t\tfilter->detached_pid_inst = gf_list_new();\n\t}\n\tif (gf_list_find(filter->detached_pid_inst, pidinst)<0)\n\t\tgf_list_add(filter->detached_pid_inst, pidinst);\n\n\t//we are done, reset filter swap instance so that connection can take place\n\tif (new_chain_input->swap_needs_init) {\n\t\tnew_chain_input->swap_pidinst_dst = NULL;\n\t\tnew_chain_input->swap_pidinst_src = NULL;\n\t\tnew_chain_input->swap_needs_init = GF_FALSE;\n\t}\n\tassert(new_chain_input->detach_pid_tasks_pending);\n\tsafe_int_dec(&new_chain_input->detach_pid_tasks_pending);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -48,7 +48,7 @@\n \t//first connection of this PID to this filter\n \tif (!pidinst) {\n \t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to detach PID %s not present in filter %s inputs\\n\",  pid->name, filter->name));\n-\t\t//assert(!new_chain_input->swap_pidinst_dst);\n+\t\t//when swaping encoder, we may have swap_pidinst_dst not NULL so only check swap_pidinst_src\n \t\tassert(!new_chain_input->swap_pidinst_src);\n \t\tnew_chain_input->swap_needs_init = GF_FALSE;\n \t\treturn;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t//assert(!new_chain_input->swap_pidinst_dst);"
            ],
            "added_lines": [
                "\t\t//when swaping encoder, we may have swap_pidinst_dst not NULL so only check swap_pidinst_src"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1654",
        "func_name": "gpac/gf_filter_pid_configure",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.4.0.",
        "git_url": "https://github.com/gpac/gpac/commit/2c055153d401b8c49422971e3a0159869652d3da",
        "commit_title": "fixed #2429",
        "commit_text": "",
        "func_before": "static GF_Err gf_filter_pid_configure(GF_Filter *filter, GF_FilterPid *pid, GF_PidConnectType ctype)\n{\n\tu32 i, count;\n\tGF_Err e;\n\tBool refire_events=GF_FALSE;\n\tBool new_pid_inst=GF_FALSE;\n\tBool remove_filter=GF_FALSE;\n\tGF_FilterPidInst *pidinst=NULL;\n\tGF_Filter *alias_orig = NULL;\n\n\tif (filter->multi_sink_target) {\n\t\talias_orig = filter;\n\t\tfilter = filter->multi_sink_target;\n\t}\n\n\tassert(filter->freg->configure_pid);\n\tif (filter->finalized) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to configure PID %s in finalized filter %s\\n\",  pid->name, filter->name));\n\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\tassert(pid->filter->out_pid_connection_pending);\n\t\t\tsafe_int_dec(&pid->filter->out_pid_connection_pending);\n\t\t}\n\t\treturn GF_SERVICE_ERROR;\n\t}\n\n\tif (filter->detached_pid_inst) {\n\t\tcount = gf_list_count(filter->detached_pid_inst);\n\t\tfor (i=0; i<count; i++) {\n\t\t\tpidinst = gf_list_get(filter->detached_pid_inst, i);\n\t\t\tif (pidinst->filter==filter) {\n\t\t\t\tgf_list_rem(filter->detached_pid_inst, i);\n\t\t\t\t//reattach new filter and pid\n\t\t\t\tpidinst->filter = filter;\n\t\t\t\tpidinst->pid = pid;\n\n\t\t\t\tassert(!pidinst->props);\n\n\t\t\t\t//and treat as new pid inst\n\t\t\t\tif (ctype == GF_PID_CONF_CONNECT) {\n\t\t\t\t\tnew_pid_inst=GF_TRUE;\n\t\t\t\t\tif (!pid->filter->nb_pids_playing && (pidinst->is_playing || pidinst->is_paused))\n\t\t\t\t\t\trefire_events = GF_TRUE;\n\t\t\t\t}\n\t\t\t\tassert(pidinst->detach_pending);\n\t\t\t\tsafe_int_dec(&pidinst->detach_pending);\n\t\t\t\t//revert temp sticky flag\n\t\t\t\tif (filter->sticky == 2)\n\t\t\t\t\tfilter->sticky = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpidinst=NULL;\n\t\t}\n\t\tif (! gf_list_count(filter->detached_pid_inst)) {\n\t\t\tgf_list_del(filter->detached_pid_inst);\n\t\t\tfilter->detached_pid_inst = NULL;\n\t\t}\n\t}\n\tif (!pidinst) {\n\t\tcount = pid->num_destinations;\n\t\tfor (i=0; i<count; i++) {\n\t\t\tpidinst = gf_list_get(pid->destinations, i);\n\t\t\tif (pidinst->filter==filter) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpidinst=NULL;\n\t\t}\n\t}\n\n\t//first connection of this PID to this filter\n\tif (!pidinst) {\n\t\tif (ctype != GF_PID_CONF_CONNECT) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to disconnect PID %s not present in filter %s inputs\\n\",  pid->name, filter->name));\n\t\t\treturn GF_SERVICE_ERROR;\n\t\t}\n\t\tpidinst = gf_filter_pid_inst_new(filter, pid);\n\t\tnew_pid_inst=GF_TRUE;\n\t}\n\tif (!pidinst->alias_orig)\n\t\tpidinst->alias_orig = alias_orig;\n\n\t//if new, add the PID to input/output before calling configure\n\tif (new_pid_inst) {\n\t\tassert(pidinst);\n\t\tgf_mx_p(pid->filter->tasks_mx);\n\n\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Registering %s:%s as destination for %s:%s\\n\", pid->filter->name, pid->name, pidinst->filter->name, pidinst->pid->name));\n\t\tgf_list_add(pid->destinations, pidinst);\n\t\tpid->num_destinations = gf_list_count(pid->destinations);\n\n\t\tgf_mx_v(pid->filter->tasks_mx);\n\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tif (!filter->input_pids) filter->input_pids = gf_list_new();\n\t\tgf_list_add(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (filter->num_input_pids==1) {\n\t\t\tfilter->single_source = pidinst->pid->filter;\n\t\t} else if (filter->single_source != pidinst->pid->filter) {\n\t\t\tfilter->single_source = NULL;\n\t\t}\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t//new connection, update caps in case we have events using caps (buffer req) being sent\n\t\t//while processing the configure (they would be dispatched on the source filter, not the dest one being\n\t\t//processed here)\n\t\tgf_filter_pid_update_caps(pid);\n\t}\n\n\t//we are swaping a PID instance (dyn insert of a filter), do it before reconnecting\n\t//in order to have properties in place\n\t//TODO: handle error case, we might need to re-switch the pid inst!\n\tif (filter->swap_pending) {\n\t\tgf_filter_pid_inst_swap(filter, pidinst);\n\t\tfilter->swap_pending = GF_FALSE;\n\t}\n\n\tfilter->in_connect_err = GF_EOS;\n\t//commented out: audio thread may be pulling packets out of the pid but not in the compositor:process, which\n\t//could be called for video at the same time...\n#if 0\n\tFSESS_CHECK_THREAD(filter)\n#endif\n\n\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Filter %s PID %s reconfigure\\n\", pidinst->filter->name, pidinst->pid->name));\n\te = filter->freg->configure_pid(filter, (GF_FilterPid*) pidinst, (ctype==GF_PID_CONF_REMOVE) ? GF_TRUE : GF_FALSE);\n\n#ifdef GPAC_MEMORY_TRACKING\n\tif (filter->session->check_allocs) {\n\t\tif (filter->nb_consecutive_process >= filter->max_nb_consecutive_process) {\n\t\t\tfilter->max_nb_consecutive_process = filter->nb_consecutive_process;\n\t\t\tfilter->max_nb_process = filter->nb_process_since_reset;\n\t\t\tfilter->max_stats_nb_alloc = filter->stats_nb_alloc;\n\t\t\tfilter->max_stats_nb_calloc = filter->stats_nb_calloc;\n\t\t\tfilter->max_stats_nb_realloc = filter->stats_nb_realloc;\n\t\t\tfilter->max_stats_nb_free = filter->stats_nb_free;\n\t\t}\n\t\tfilter->stats_mem_allocated = 0;\n\t\tfilter->stats_nb_alloc = filter->stats_nb_realloc = filter->stats_nb_free = 0;\n\t\tfilter->nb_process_since_reset = filter->nb_consecutive_process = 0;\n\t}\n#endif\n\tif ((e==GF_OK) && (filter->in_connect_err<GF_OK))\n\t\te = filter->in_connect_err;\n\n\tfilter->in_connect_err = GF_OK;\n\t\n\tif (e==GF_OK) {\n\t\t//if new, register the new pid instance, and the source pid as input to this filter\n\t\tif (new_pid_inst) {\n\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Filter %s (%p) PID %s (%p) (%d fan-out) connected to filter %s (%p)\\n\", pid->filter->name, pid->filter, pid->name, pid, pid->num_destinations, filter->name, filter));\n\t\t}\n\t\t//reset blacklist on source if connect OK - this is required when reconfiguring multiple times to the same filter, eg\n\t\t//jpeg->raw->jpeg, the first jpeg->raw would blacklist jpeg dec from source, preventing resolution to work at raw->jpeg switch\n\t\tgf_list_reset(pidinst->pid->filter->blacklisted);\n\t}\n\t//failure on reconfigure, try reloading a filter chain\n\telse if ((ctype==GF_PID_CONF_RECONFIG) && (e != GF_FILTER_NOT_SUPPORTED)) {\n\t\t//mark pid as end of stream to let filter flush\n\t\tpidinst->is_end_of_stream = GF_TRUE;\n\t\tif (e==GF_BAD_PARAM) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to reconfigure PID %s:%s in filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\n\t\t\tfilter->session->last_connect_error = e;\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Failed to reconfigure PID %s:%s in filter %s: %s, reloading filter graph\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t\tgf_list_add(pid->filter->blacklisted, (void *) filter->freg);\n\t\t\tgf_filter_relink_dst(pidinst, e);\n\t\t}\n\t} else {\n\n\t\t//error, remove from input and output\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tgf_list_del_item(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (!filter->num_input_pids)\n\t\t\tfilter->single_source = NULL;\n\t\tfilter->freg->configure_pid(filter, (GF_FilterPid *) pidinst, GF_TRUE);\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\tgf_list_del_item(pidinst->pid->destinations, pidinst);\n\t\tpidinst->pid->num_destinations = gf_list_count(pidinst->pid->destinations);\n\t\t//detach filter from pid instance\n\t\tgf_filter_instance_detach_pid(pidinst);\n\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\n\t\t//if connect and error, direct delete of pid\n\t\tif (new_pid_inst) {\n\t\t\tgf_mx_p(pid->filter->tasks_mx);\n\t\t\tgf_list_del_item(pid->destinations, pidinst);\n\t\t\tpid->num_destinations = gf_list_count(pid->destinations);\n\n\t\t\t//cancel all tasks targeting this pid\n\t\t\tgf_mx_p(pid->filter->tasks_mx);\n\t\t\tcount = gf_fq_count(pid->filter->tasks);\n\t\t\tfor (i=0; i<count; i++) {\n\t\t\t\tGF_FSTask *t = gf_fq_get(pid->filter->tasks, i);\n\t\t\t\tif (t->pid == (GF_FilterPid *) pidinst) {\n\t\t\t\t\tt->run_task = task_canceled;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgf_mx_v(pid->filter->tasks_mx);\n\n\t\t\t//destroy pid instance\n\t\t\tgf_filter_pid_inst_del(pidinst);\n\t\t\tgf_mx_v(pid->filter->tasks_mx);\n\t\t}\n\n\n\t\tif (e==GF_REQUIRES_NEW_INSTANCE) {\n\t\t\t//TODO: copy over args from current filter\n\t\t\tGF_Filter *new_filter = gf_filter_clone(filter, pid->filter);\n\t\t\tif (new_filter) {\n\t\t\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Clone filter %s, new instance for pid %s\\n\", filter->name, pid->name));\n\t\t\t\tgf_filter_pid_post_connect_task(new_filter, pid);\n\t\t\t\treturn GF_OK;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to clone filter %s\\n\", filter->name));\n\t\t\t\te = GF_OUT_OF_MEM;\n\t\t\t}\n\t\t}\n\t\tif (e && (ctype==GF_PID_CONF_REMOVE)) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to disconnect filter %s PID %s from filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t}\n\t\telse if (e) {\n\t\t\tif (e!= GF_EOS) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to connect filter %s PID %s to filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t\t}\n\n\t\t\tif ((e==GF_BAD_PARAM)\n\t\t\t\t|| (e==GF_SERVICE_ERROR)\n\t\t\t\t|| (e==GF_REMOTE_SERVICE_ERROR)\n\t\t\t\t|| (e==GF_FILTER_NOT_SUPPORTED)\n\t\t\t\t|| (e==GF_EOS)\n\t\t\t\t|| (filter->session->flags & GF_FS_FLAG_NO_REASSIGN)\n\t\t\t) {\n\t\t\t\tif (filter->session->flags & GF_FS_FLAG_NO_REASSIGN) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Filter reassignment disabled, skippping chain reload for filter %s PID %s\\n\", pid->filter->name, pid->name ));\n\t\t\t\t}\n\t\t\t\tif (e!= GF_EOS) {\n\t\t\t\t\tfilter->session->last_connect_error = e;\n\t\t\t\t}\n\n\t\t\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\t\t\tGF_FilterEvent evt;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PLAY, pid);\n\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_STOP, pid);\n\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\n\t\t\t\t\tgf_filter_pid_set_eos(pid);\n\n\t\t\t\t\tif (pid->filter->freg->process_event) {\n\t\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_CONNECT_FAIL, pid);\n\t\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\t\t\t\t\t}\n\t\t\t\t\tif (!filter->num_input_pids && !filter->num_output_pids) {\n\t\t\t\t\t\tremove_filter = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (filter->has_out_caps) {\n\t\t\t\tBool unload_filter = GF_TRUE;\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"Blacklisting %s as output from %s and retrying connections\\n\", filter->name, pid->filter->name));\n\t\t\t\t//try to load another filter to handle that connection\n\t\t\t\t//1-blacklist this filter\n\t\t\t\tgf_list_add(pid->filter->blacklisted, (void *) filter->freg);\n\t\t\t\t//2-disconnect all other inputs, and post a re-init\n\t\t\t\tgf_mx_p(filter->tasks_mx);\n\t\t\t\twhile (gf_list_count(filter->input_pids)) {\n\t\t\t\t\tGF_FilterPidInst *a_pidinst = gf_list_pop_back(filter->input_pids);\n\t\t\t\t\tFSESS_CHECK_THREAD(filter)\n\t\t\t\t\tfilter->num_input_pids--;\n\t\t\t\t\tfilter->freg->configure_pid(filter, (GF_FilterPid *) a_pidinst, GF_TRUE);\n\n\t\t\t\t\tgf_filter_pid_post_init_task(a_pidinst->pid->filter, a_pidinst->pid);\n\t\t\t\t\tgf_fs_post_task(filter->session, gf_filter_pid_inst_delete_task, a_pidinst->pid->filter, a_pidinst->pid, \"pid_inst_delete\", a_pidinst);\n\n\t\t\t\t\tunload_filter = GF_FALSE;\n\t\t\t\t}\n\t\t\t\tfilter->num_input_pids = 0;\n\t\t\t\tfilter->single_source = NULL;\n\t\t\t\tfilter->removed = 1;\n\t\t\t\tfilter->has_pending_pids = GF_FALSE;\n\t\t\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t\t\t//do not assign session->last_connect_error since we are retrying a connection\n\n\t\t\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\t\t\tassert(pid->filter->out_pid_connection_pending);\n\t\t\t\t\tsafe_int_dec(&pid->filter->out_pid_connection_pending);\n\t\t\t\t}\n\t\t\t\t//3- post a re-init on this pid\n\t\t\t\tgf_filter_pid_post_init_task(pid->filter, pid);\n\n\t\t\t\tif (unload_filter) {\n\t\t\t\t\tassert(!gf_list_count(filter->input_pids));\n\n\t\t\t\t\tif (filter->num_output_pids) {\n\t\t\t\t\t\tfor (i=0; i<filter->num_output_pids; i++) {\n\t\t\t\t\t\t\tu32 j;\n\t\t\t\t\t\t\tGF_FilterPid *opid = gf_list_get(filter->output_pids, i);\n\t\t\t\t\t\t\tfor (j=0; j< opid->num_destinations; j++) {\n\t\t\t\t\t\t\t\tGF_FilterPidInst *a_pidi = gf_list_get(opid->destinations, j);\n\t\t\t\t\t\t\t\ta_pidi->pid = NULL;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tgf_list_reset(opid->destinations);\n\t\t\t\t\t\t\topid->num_destinations = 0;\n\t\t\t\t\t\t\tgf_filter_pid_remove(opid);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfilter->swap_pidinst_src = NULL;\n\t\t\t\t\tif (filter->swap_pidinst_dst) {\n\t\t\t\t\t\tGF_Filter *target = filter->swap_pidinst_dst->filter;\n\t\t\t\t\t\tassert(target);\n\t\t\t\t\t\tif (!target->detached_pid_inst) {\n\t\t\t\t\t\t\ttarget->detached_pid_inst = gf_list_new();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//detach props but don't delete them\n\t\t\t\t\t\tif (filter->swap_pidinst_dst->props) {\n\t\t\t\t\t\t\tfilter->swap_pidinst_dst->props = NULL;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfilter->swap_pidinst_dst->pid = NULL;\n\t\t\t\t\t\tif (gf_list_find(target->detached_pid_inst, filter->swap_pidinst_dst)<0)\n\t\t\t\t\t\t\tgf_list_add(target->detached_pid_inst, filter->swap_pidinst_dst);\n\t\t\t\t\t}\n\t\t\t\t\tfilter->swap_pidinst_dst = NULL;\n\t\t\t\t\tif (filter->on_setup_error) {\n\t\t\t\t\t\tgf_filter_notification_failure(filter, e, GF_TRUE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgf_filter_post_remove(filter);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn e;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to reconfigure input of sink %s, cannot rebuild graph\\n\", filter->name));\n\t\t\t\tif (pid->filter->freg->process_event) {\n\t\t\t\t\tGF_FilterEvent evt;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_CONNECT_FAIL, pid);\n\t\t\t\t\tpid->filter->freg->process_event(pid->filter, &evt);\n\t\t\t\t}\n\t\t\t\tfilter->session->last_connect_error = e;\n\t\t\t}\n\t\t} else {\n\t\t\tfilter->session->last_connect_error = GF_OK;\n\t\t}\n\n\t\t//try to run filter no matter what\n\t\tif (filter->session->requires_solved_graph)\n\t\t\treturn e;\n\t}\n\n\t//flush all pending pid init requests following the call to init\n\tif (filter->has_pending_pids) {\n\t\tfilter->has_pending_pids = GF_FALSE;\n\t\twhile (gf_fq_count(filter->pending_pids)) {\n\t\t\tGF_FilterPid *a_pid=gf_fq_pop(filter->pending_pids);\n\t\t\t//filter is a pid adaptation filter (dynamically loaded to solve prop negociation)\n\t\t\t//copy over play state if the input PID was already playing\n\t\t\tif (pid->is_playing && filter->is_pid_adaptation_filter)\n\t\t\t\ta_pid->is_playing = GF_TRUE;\n\n\t\t\tgf_filter_pid_post_init_task(filter, a_pid);\n\t\t}\n\t}\n\n\tif (ctype==GF_PID_CONF_REMOVE) {\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tgf_list_del_item(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (!filter->num_input_pids)\n\t\t\tfilter->single_source = NULL;\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t//PID instance is no longer in graph, we must remove it from pid destination to avoid propagating events\n\t\t//on to-be freed pid instance.\n\t\t//however we must have fan-outs (N>1 pid instance per PID), and removing the pid inst would trigger a pid destruction\n\t\t//on the first gf_filter_pid_inst_delete_task executed.\n\t\t//we therefore track at the PID level the number of gf_filter_pid_inst_delete_task tasks pending and\n\t\t//won't destroy the PID until that number is O\n\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\tpidinst->pid->num_pidinst_del_pending ++;\n\t\tgf_list_del_item(pidinst->pid->destinations, pidinst);\n\t\tpidinst->pid->num_destinations = gf_list_count(pidinst->pid->destinations);\n\t\tgf_filter_instance_detach_pid(pidinst);\n\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\n\t\t//disconnected the last input, flag as removed\n\t\tif (!filter->num_input_pids && !filter->sticky) {\n\t\t\tgf_filter_reset_pending_packets(filter);\n\t\t\tfilter->removed = 1;\n\t\t}\n\t\t//post a pid_delete task to also trigger removal of the filter if needed\n\t\tgf_fs_post_task(filter->session, gf_filter_pid_inst_delete_task, pid->filter, pid, \"pid_inst_delete\", pidinst);\n\n\t\treturn e;\n\t}\n\n\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\tassert(pid->filter->out_pid_connection_pending);\n\t\tif (safe_int_dec(&pid->filter->out_pid_connection_pending) == 0) {\n\n\t\t\t//we must resent play/pause events when a new pid is reattached to an old pid instance\n\t\t\t//in case one of the injected filter(s) monitors play state of the pids (eg reframers)\n\t\t\tif (refire_events) {\n\t\t\t\tGF_FilterEvent evt;\n\t\t\t\tif (pidinst->is_playing) {\n\t\t\t\t\tpidinst->is_playing = GF_FALSE;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PLAY, (GF_FilterPid*)pidinst);\n\t\t\t\t\tgf_filter_pid_send_event((GF_FilterPid *)pidinst, &evt);\n\t\t\t\t}\n\t\t\t\tif (pidinst->is_paused) {\n\t\t\t\t\tpidinst->is_paused = GF_FALSE;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PAUSE, (GF_FilterPid*)pidinst);\n\t\t\t\t\tgf_filter_pid_send_event((GF_FilterPid *)pidinst, &evt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (e==GF_OK) {\n\t\t\t\t//postponed packets dispatched by source while setting up PID, flush through process()\n\t\t\t\t//pending packets (not yet consumed but in PID buffer), start processing\n\t\t\t\tif (pid->filter->postponed_packets || pid->filter->pending_packets || pid->filter->nb_caps_renegociate) {\n\t\t\t\t\tgf_filter_post_process_task(pid->filter);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (remove_filter && !filter->sticky)\n\t\t\tgf_filter_post_remove(filter);\n\t}\n\t//once all pid have been (re)connected, update any internal caps\n\tgf_filter_pid_update_caps(pid);\n\treturn e;\n}",
        "func": "static GF_Err gf_filter_pid_configure(GF_Filter *filter, GF_FilterPid *pid, GF_PidConnectType ctype)\n{\n\tu32 i, count;\n\tGF_Err e;\n\tBool refire_events=GF_FALSE;\n\tBool new_pid_inst=GF_FALSE;\n\tBool remove_filter=GF_FALSE;\n\tGF_FilterPidInst *pidinst=NULL;\n\tGF_Filter *alias_orig = NULL;\n\n\tif (filter->multi_sink_target) {\n\t\talias_orig = filter;\n\t\tfilter = filter->multi_sink_target;\n\t}\n\n\tassert(filter->freg->configure_pid);\n\tif (filter->finalized) {\n\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to configure PID %s in finalized filter %s\\n\",  pid->name, filter->name));\n\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\tassert(pid->filter->out_pid_connection_pending);\n\t\t\tsafe_int_dec(&pid->filter->out_pid_connection_pending);\n\t\t}\n\t\treturn GF_SERVICE_ERROR;\n\t}\n\n\tif (filter->detached_pid_inst) {\n\t\tcount = gf_list_count(filter->detached_pid_inst);\n\t\tfor (i=0; i<count; i++) {\n\t\t\tpidinst = gf_list_get(filter->detached_pid_inst, i);\n\t\t\tif (pidinst->filter==filter) {\n\t\t\t\tgf_list_rem(filter->detached_pid_inst, i);\n\t\t\t\t//reattach new filter and pid\n\t\t\t\tpidinst->filter = filter;\n\t\t\t\tpidinst->pid = pid;\n\n\t\t\t\tassert(!pidinst->props);\n\n\t\t\t\t//and treat as new pid inst\n\t\t\t\tif (ctype == GF_PID_CONF_CONNECT) {\n\t\t\t\t\tnew_pid_inst=GF_TRUE;\n\t\t\t\t\tif (!pid->filter->nb_pids_playing && (pidinst->is_playing || pidinst->is_paused))\n\t\t\t\t\t\trefire_events = GF_TRUE;\n\t\t\t\t}\n\t\t\t\tassert(pidinst->detach_pending);\n\t\t\t\tsafe_int_dec(&pidinst->detach_pending);\n\t\t\t\t//revert temp sticky flag\n\t\t\t\tif (filter->sticky == 2)\n\t\t\t\t\tfilter->sticky = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpidinst=NULL;\n\t\t}\n\t\tif (! gf_list_count(filter->detached_pid_inst)) {\n\t\t\tgf_list_del(filter->detached_pid_inst);\n\t\t\tfilter->detached_pid_inst = NULL;\n\t\t}\n\t}\n\tif (!pidinst) {\n\t\tcount = pid->num_destinations;\n\t\tfor (i=0; i<count; i++) {\n\t\t\tpidinst = gf_list_get(pid->destinations, i);\n\t\t\tif (pidinst->filter==filter) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpidinst=NULL;\n\t\t}\n\t}\n\n\t//first connection of this PID to this filter\n\tif (!pidinst) {\n\t\tif (ctype != GF_PID_CONF_CONNECT) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Trying to disconnect PID %s not present in filter %s inputs\\n\",  pid->name, filter->name));\n\t\t\treturn GF_SERVICE_ERROR;\n\t\t}\n\t\tpidinst = gf_filter_pid_inst_new(filter, pid);\n\t\tnew_pid_inst=GF_TRUE;\n\t}\n\tif (!pidinst->alias_orig)\n\t\tpidinst->alias_orig = alias_orig;\n\n\t//if new, add the PID to input/output before calling configure\n\tif (new_pid_inst) {\n\t\tassert(pidinst);\n\t\tgf_mx_p(pid->filter->tasks_mx);\n\n\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Registering %s:%s as destination for %s:%s\\n\", pid->filter->name, pid->name, pidinst->filter->name, pidinst->pid->name));\n\t\tgf_list_add(pid->destinations, pidinst);\n\t\tpid->num_destinations = gf_list_count(pid->destinations);\n\n\t\tgf_mx_v(pid->filter->tasks_mx);\n\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tif (!filter->input_pids) filter->input_pids = gf_list_new();\n\t\tgf_list_add(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (filter->num_input_pids==1) {\n\t\t\tfilter->single_source = pidinst->pid->filter;\n\t\t} else if (filter->single_source != pidinst->pid->filter) {\n\t\t\tfilter->single_source = NULL;\n\t\t}\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t//new connection, update caps in case we have events using caps (buffer req) being sent\n\t\t//while processing the configure (they would be dispatched on the source filter, not the dest one being\n\t\t//processed here)\n\t\tgf_filter_pid_update_caps(pid);\n\t}\n\n\t//we are swaping a PID instance (dyn insert of a filter), do it before reconnecting\n\t//in order to have properties in place\n\t//TODO: handle error case, we might need to re-switch the pid inst!\n\tif (filter->swap_pending) {\n\t\tgf_filter_pid_inst_swap(filter, pidinst);\n\t\tfilter->swap_pending = GF_FALSE;\n\t}\n\n\tfilter->in_connect_err = GF_EOS;\n\t//commented out: audio thread may be pulling packets out of the pid but not in the compositor:process, which\n\t//could be called for video at the same time...\n#if 0\n\tFSESS_CHECK_THREAD(filter)\n#endif\n\n\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Filter %s PID %s reconfigure\\n\", pidinst->filter->name, pidinst->pid->name));\n\te = filter->freg->configure_pid(filter, (GF_FilterPid*) pidinst, (ctype==GF_PID_CONF_REMOVE) ? GF_TRUE : GF_FALSE);\n\n#ifdef GPAC_MEMORY_TRACKING\n\tif (filter->session->check_allocs) {\n\t\tif (filter->nb_consecutive_process >= filter->max_nb_consecutive_process) {\n\t\t\tfilter->max_nb_consecutive_process = filter->nb_consecutive_process;\n\t\t\tfilter->max_nb_process = filter->nb_process_since_reset;\n\t\t\tfilter->max_stats_nb_alloc = filter->stats_nb_alloc;\n\t\t\tfilter->max_stats_nb_calloc = filter->stats_nb_calloc;\n\t\t\tfilter->max_stats_nb_realloc = filter->stats_nb_realloc;\n\t\t\tfilter->max_stats_nb_free = filter->stats_nb_free;\n\t\t}\n\t\tfilter->stats_mem_allocated = 0;\n\t\tfilter->stats_nb_alloc = filter->stats_nb_realloc = filter->stats_nb_free = 0;\n\t\tfilter->nb_process_since_reset = filter->nb_consecutive_process = 0;\n\t}\n#endif\n\tif ((e==GF_OK) && (filter->in_connect_err<GF_OK))\n\t\te = filter->in_connect_err;\n\n\tfilter->in_connect_err = GF_OK;\n\t\n\tif (e==GF_OK) {\n\t\t//if new, register the new pid instance, and the source pid as input to this filter\n\t\tif (new_pid_inst) {\n\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Filter %s (%p) PID %s (%p) (%d fan-out) connected to filter %s (%p)\\n\", pid->filter->name, pid->filter, pid->name, pid, pid->num_destinations, filter->name, filter));\n\t\t}\n\t\t//reset blacklist on source if connect OK - this is required when reconfiguring multiple times to the same filter, eg\n\t\t//jpeg->raw->jpeg, the first jpeg->raw would blacklist jpeg dec from source, preventing resolution to work at raw->jpeg switch\n\t\tgf_list_reset(pidinst->pid->filter->blacklisted);\n\t}\n\t//failure on reconfigure, try reloading a filter chain\n\telse if ((ctype==GF_PID_CONF_RECONFIG) && (e != GF_FILTER_NOT_SUPPORTED)) {\n\t\t//mark pid as end of stream to let filter flush\n\t\tpidinst->is_end_of_stream = GF_TRUE;\n\t\tif (e==GF_BAD_PARAM) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to reconfigure PID %s:%s in filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\n\t\t\tfilter->session->last_connect_error = e;\n\t\t} else {\n\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_FILTER, (\"Failed to reconfigure PID %s:%s in filter %s: %s, reloading filter graph\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t\tgf_list_add(pid->filter->blacklisted, (void *) filter->freg);\n\t\t\tgf_filter_relink_dst(pidinst, e);\n\t\t}\n\t} else {\n\n\t\t//error, remove from input and output\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tgf_list_del_item(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (!filter->num_input_pids)\n\t\t\tfilter->single_source = NULL;\n\t\tfilter->freg->configure_pid(filter, (GF_FilterPid *) pidinst, GF_TRUE);\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\tgf_list_del_item(pidinst->pid->destinations, pidinst);\n\t\tpidinst->pid->num_destinations = gf_list_count(pidinst->pid->destinations);\n\t\t//detach filter from pid instance\n\t\tgf_filter_instance_detach_pid(pidinst);\n\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\n\t\t//if connect and error, direct delete of pid\n\t\tif (new_pid_inst) {\n\t\t\tgf_mx_p(pid->filter->tasks_mx);\n\t\t\tgf_list_del_item(pid->destinations, pidinst);\n\t\t\tpid->num_destinations = gf_list_count(pid->destinations);\n\n\t\t\t//cancel all tasks targeting this pid\n\t\t\tgf_mx_p(pid->filter->tasks_mx);\n\t\t\tcount = gf_fq_count(pid->filter->tasks);\n\t\t\tfor (i=0; i<count; i++) {\n\t\t\t\tGF_FSTask *t = gf_fq_get(pid->filter->tasks, i);\n\t\t\t\tif (t->pid == (GF_FilterPid *) pidinst) {\n\t\t\t\t\tt->run_task = task_canceled;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgf_mx_v(pid->filter->tasks_mx);\n\n\t\t\t//destroy pid instance\n\t\t\tgf_filter_pid_inst_del(pidinst);\n\t\t\tgf_mx_v(pid->filter->tasks_mx);\n\t\t}\n\n\n\t\tif (e==GF_REQUIRES_NEW_INSTANCE) {\n\t\t\t//TODO: copy over args from current filter\n\t\t\tGF_Filter *new_filter = gf_filter_clone(filter, pid->filter);\n\t\t\tif (new_filter) {\n\t\t\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_FILTER, (\"Clone filter %s, new instance for pid %s\\n\", filter->name, pid->name));\n\t\t\t\tgf_filter_pid_post_connect_task(new_filter, pid);\n\t\t\t\treturn GF_OK;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to clone filter %s\\n\", filter->name));\n\t\t\t\te = GF_OUT_OF_MEM;\n\t\t\t}\n\t\t}\n\t\tif (e && (ctype==GF_PID_CONF_REMOVE)) {\n\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to disconnect filter %s PID %s from filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t}\n\t\telse if (e) {\n\t\t\tif (e!= GF_EOS) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to connect filter %s PID %s to filter %s: %s\\n\", pid->filter->name, pid->name, filter->name, gf_error_to_string(e) ));\n\t\t\t}\n\n\t\t\tif ((e==GF_BAD_PARAM)\n\t\t\t\t|| (e==GF_SERVICE_ERROR)\n\t\t\t\t|| (e==GF_REMOTE_SERVICE_ERROR)\n\t\t\t\t|| (e==GF_FILTER_NOT_SUPPORTED)\n\t\t\t\t|| (e==GF_EOS)\n\t\t\t\t|| (filter->session->flags & GF_FS_FLAG_NO_REASSIGN)\n\t\t\t) {\n\t\t\t\tif (filter->session->flags & GF_FS_FLAG_NO_REASSIGN) {\n\t\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Filter reassignment disabled, skippping chain reload for filter %s PID %s\\n\", pid->filter->name, pid->name ));\n\t\t\t\t}\n\t\t\t\tif (e!= GF_EOS) {\n\t\t\t\t\tfilter->session->last_connect_error = e;\n\t\t\t\t}\n\n\t\t\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\t\t\tGF_FilterEvent evt;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PLAY, pid);\n\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_STOP, pid);\n\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\n\t\t\t\t\tgf_filter_pid_set_eos(pid);\n\n\t\t\t\t\tif (pid->filter->freg->process_event) {\n\t\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_CONNECT_FAIL, pid);\n\t\t\t\t\t\tgf_filter_pid_send_event_internal(pid, &evt, GF_TRUE);\n\t\t\t\t\t}\n\t\t\t\t\tif (!filter->num_input_pids && !filter->num_output_pids) {\n\t\t\t\t\t\tremove_filter = GF_TRUE;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else if (filter->has_out_caps) {\n\t\t\t\tBool unload_filter = GF_TRUE;\n\t\t\t\tGF_LOG(GF_LOG_WARNING, GF_LOG_FILTER, (\"Blacklisting %s as output from %s and retrying connections\\n\", filter->name, pid->filter->name));\n\t\t\t\t//try to load another filter to handle that connection\n\t\t\t\t//1-blacklist this filter\n\t\t\t\tgf_list_add(pid->filter->blacklisted, (void *) filter->freg);\n\t\t\t\t//2-disconnect all other inputs, and post a re-init\n\t\t\t\tgf_mx_p(filter->tasks_mx);\n\t\t\t\twhile (gf_list_count(filter->input_pids)) {\n\t\t\t\t\tGF_FilterPidInst *a_pidinst = gf_list_pop_back(filter->input_pids);\n\t\t\t\t\tFSESS_CHECK_THREAD(filter)\n\t\t\t\t\tfilter->num_input_pids--;\n\t\t\t\t\tfilter->freg->configure_pid(filter, (GF_FilterPid *) a_pidinst, GF_TRUE);\n\n\t\t\t\t\tgf_filter_pid_post_init_task(a_pidinst->pid->filter, a_pidinst->pid);\n\t\t\t\t\tgf_fs_post_task(filter->session, gf_filter_pid_inst_delete_task, a_pidinst->pid->filter, a_pidinst->pid, \"pid_inst_delete\", a_pidinst);\n\n\t\t\t\t\tunload_filter = GF_FALSE;\n\t\t\t\t}\n\t\t\t\tfilter->num_input_pids = 0;\n\t\t\t\tfilter->single_source = NULL;\n\t\t\t\tfilter->removed = 1;\n\t\t\t\tfilter->has_pending_pids = GF_FALSE;\n\t\t\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t\t\t//do not assign session->last_connect_error since we are retrying a connection\n\n\t\t\t\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\t\t\t\tassert(pid->filter->out_pid_connection_pending);\n\t\t\t\t\tsafe_int_dec(&pid->filter->out_pid_connection_pending);\n\t\t\t\t}\n\t\t\t\t//3- post a re-init on this pid\n\t\t\t\tgf_filter_pid_post_init_task(pid->filter, pid);\n\n\t\t\t\tif (unload_filter) {\n\t\t\t\t\tassert(!gf_list_count(filter->input_pids));\n\n\t\t\t\t\tif (filter->num_output_pids) {\n\t\t\t\t\t\tfor (i=0; i<filter->num_output_pids; i++) {\n\t\t\t\t\t\t\tu32 j;\n\t\t\t\t\t\t\tGF_FilterPid *opid = gf_list_get(filter->output_pids, i);\n\t\t\t\t\t\t\tfor (j=0; j< opid->num_destinations; j++) {\n\t\t\t\t\t\t\t\tGF_FilterPidInst *a_pidi = gf_list_get(opid->destinations, j);\n\t\t\t\t\t\t\t\ta_pidi->pid = NULL;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tgf_list_reset(opid->destinations);\n\t\t\t\t\t\t\topid->num_destinations = 0;\n\t\t\t\t\t\t\tgf_filter_pid_remove(opid);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfilter->swap_pidinst_src = NULL;\n\t\t\t\t\tif (filter->swap_pidinst_dst) {\n\t\t\t\t\t\tGF_Filter *target = filter->swap_pidinst_dst->filter;\n\t\t\t\t\t\tassert(target);\n\t\t\t\t\t\tif (!target->detached_pid_inst) {\n\t\t\t\t\t\t\ttarget->detached_pid_inst = gf_list_new();\n\t\t\t\t\t\t}\n\t\t\t\t\t\t//detach props\n\t\t\t\t\t\tif (filter->swap_pidinst_dst->props) {\n\t\t\t\t\t\t\tGF_FilterPidInst *swap_pidi = filter->swap_pidinst_dst;\n\t\t\t\t\t\t\tif (safe_int_dec(&swap_pidi->props->reference_count)==0) {\n\t\t\t\t\t\t\t\tgf_mx_p(swap_pidi->pid->filter->tasks_mx);\n\t\t\t\t\t\t\t\tgf_list_del_item(swap_pidi->pid->properties, pidinst->props);\n\t\t\t\t\t\t\t\tgf_mx_v(swap_pidi->pid->filter->tasks_mx);\n\t\t\t\t\t\t\t\tgf_props_del(pidinst->props);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfilter->swap_pidinst_dst->props = NULL;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfilter->swap_pidinst_dst->pid = NULL;\n\t\t\t\t\t\tif (gf_list_find(target->detached_pid_inst, filter->swap_pidinst_dst)<0)\n\t\t\t\t\t\t\tgf_list_add(target->detached_pid_inst, filter->swap_pidinst_dst);\n\t\t\t\t\t}\n\t\t\t\t\tfilter->swap_pidinst_dst = NULL;\n\t\t\t\t\tif (filter->on_setup_error) {\n\t\t\t\t\t\tgf_filter_notification_failure(filter, e, GF_TRUE);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgf_filter_post_remove(filter);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn e;\n\t\t\t} else {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_FILTER, (\"Failed to reconfigure input of sink %s, cannot rebuild graph\\n\", filter->name));\n\t\t\t\tif (pid->filter->freg->process_event) {\n\t\t\t\t\tGF_FilterEvent evt;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_CONNECT_FAIL, pid);\n\t\t\t\t\tpid->filter->freg->process_event(pid->filter, &evt);\n\t\t\t\t}\n\t\t\t\tfilter->session->last_connect_error = e;\n\t\t\t}\n\t\t} else {\n\t\t\tfilter->session->last_connect_error = GF_OK;\n\t\t}\n\n\t\t//try to run filter no matter what\n\t\tif (filter->session->requires_solved_graph)\n\t\t\treturn e;\n\t}\n\n\t//flush all pending pid init requests following the call to init\n\tif (filter->has_pending_pids) {\n\t\tfilter->has_pending_pids = GF_FALSE;\n\t\twhile (gf_fq_count(filter->pending_pids)) {\n\t\t\tGF_FilterPid *a_pid=gf_fq_pop(filter->pending_pids);\n\t\t\t//filter is a pid adaptation filter (dynamically loaded to solve prop negociation)\n\t\t\t//copy over play state if the input PID was already playing\n\t\t\tif (pid->is_playing && filter->is_pid_adaptation_filter)\n\t\t\t\ta_pid->is_playing = GF_TRUE;\n\n\t\t\tgf_filter_pid_post_init_task(filter, a_pid);\n\t\t}\n\t}\n\n\tif (ctype==GF_PID_CONF_REMOVE) {\n\t\tgf_mx_p(filter->tasks_mx);\n\t\tgf_list_del_item(filter->input_pids, pidinst);\n\t\tfilter->num_input_pids = gf_list_count(filter->input_pids);\n\t\tif (!filter->num_input_pids)\n\t\t\tfilter->single_source = NULL;\n\t\tgf_mx_v(filter->tasks_mx);\n\n\t\t//PID instance is no longer in graph, we must remove it from pid destination to avoid propagating events\n\t\t//on to-be freed pid instance.\n\t\t//however we must have fan-outs (N>1 pid instance per PID), and removing the pid inst would trigger a pid destruction\n\t\t//on the first gf_filter_pid_inst_delete_task executed.\n\t\t//we therefore track at the PID level the number of gf_filter_pid_inst_delete_task tasks pending and\n\t\t//won't destroy the PID until that number is O\n\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\tpidinst->pid->num_pidinst_del_pending ++;\n\t\tgf_list_del_item(pidinst->pid->destinations, pidinst);\n\t\tpidinst->pid->num_destinations = gf_list_count(pidinst->pid->destinations);\n\t\tgf_filter_instance_detach_pid(pidinst);\n\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\n\t\t//disconnected the last input, flag as removed\n\t\tif (!filter->num_input_pids && !filter->sticky) {\n\t\t\tgf_filter_reset_pending_packets(filter);\n\t\t\tfilter->removed = 1;\n\t\t}\n\t\t//post a pid_delete task to also trigger removal of the filter if needed\n\t\tgf_fs_post_task(filter->session, gf_filter_pid_inst_delete_task, pid->filter, pid, \"pid_inst_delete\", pidinst);\n\n\t\treturn e;\n\t}\n\n\tif (ctype==GF_PID_CONF_CONNECT) {\n\t\tassert(pid->filter->out_pid_connection_pending);\n\t\tif (safe_int_dec(&pid->filter->out_pid_connection_pending) == 0) {\n\n\t\t\t//we must resent play/pause events when a new pid is reattached to an old pid instance\n\t\t\t//in case one of the injected filter(s) monitors play state of the pids (eg reframers)\n\t\t\tif (refire_events) {\n\t\t\t\tGF_FilterEvent evt;\n\t\t\t\tif (pidinst->is_playing) {\n\t\t\t\t\tpidinst->is_playing = GF_FALSE;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PLAY, (GF_FilterPid*)pidinst);\n\t\t\t\t\tgf_filter_pid_send_event((GF_FilterPid *)pidinst, &evt);\n\t\t\t\t}\n\t\t\t\tif (pidinst->is_paused) {\n\t\t\t\t\tpidinst->is_paused = GF_FALSE;\n\t\t\t\t\tGF_FEVT_INIT(evt, GF_FEVT_PAUSE, (GF_FilterPid*)pidinst);\n\t\t\t\t\tgf_filter_pid_send_event((GF_FilterPid *)pidinst, &evt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (e==GF_OK) {\n\t\t\t\t//postponed packets dispatched by source while setting up PID, flush through process()\n\t\t\t\t//pending packets (not yet consumed but in PID buffer), start processing\n\t\t\t\tif (pid->filter->postponed_packets || pid->filter->pending_packets || pid->filter->nb_caps_renegociate) {\n\t\t\t\t\tgf_filter_post_process_task(pid->filter);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (remove_filter && !filter->sticky)\n\t\t\tgf_filter_post_remove(filter);\n\t}\n\t//once all pid have been (re)connected, update any internal caps\n\tgf_filter_pid_update_caps(pid);\n\treturn e;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -316,8 +316,15 @@\n \t\t\t\t\t\tif (!target->detached_pid_inst) {\n \t\t\t\t\t\t\ttarget->detached_pid_inst = gf_list_new();\n \t\t\t\t\t\t}\n-\t\t\t\t\t\t//detach props but don't delete them\n+\t\t\t\t\t\t//detach props\n \t\t\t\t\t\tif (filter->swap_pidinst_dst->props) {\n+\t\t\t\t\t\t\tGF_FilterPidInst *swap_pidi = filter->swap_pidinst_dst;\n+\t\t\t\t\t\t\tif (safe_int_dec(&swap_pidi->props->reference_count)==0) {\n+\t\t\t\t\t\t\t\tgf_mx_p(swap_pidi->pid->filter->tasks_mx);\n+\t\t\t\t\t\t\t\tgf_list_del_item(swap_pidi->pid->properties, pidinst->props);\n+\t\t\t\t\t\t\t\tgf_mx_v(swap_pidi->pid->filter->tasks_mx);\n+\t\t\t\t\t\t\t\tgf_props_del(pidinst->props);\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\tfilter->swap_pidinst_dst->props = NULL;\n \t\t\t\t\t\t}\n \t\t\t\t\t\tfilter->swap_pidinst_dst->pid = NULL;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t\t\t//detach props but don't delete them"
            ],
            "added_lines": [
                "\t\t\t\t\t\t//detach props",
                "\t\t\t\t\t\t\tGF_FilterPidInst *swap_pidi = filter->swap_pidinst_dst;",
                "\t\t\t\t\t\t\tif (safe_int_dec(&swap_pidi->props->reference_count)==0) {",
                "\t\t\t\t\t\t\t\tgf_mx_p(swap_pidi->pid->filter->tasks_mx);",
                "\t\t\t\t\t\t\t\tgf_list_del_item(swap_pidi->pid->properties, pidinst->props);",
                "\t\t\t\t\t\t\t\tgf_mx_v(swap_pidi->pid->filter->tasks_mx);",
                "\t\t\t\t\t\t\t\tgf_props_del(pidinst->props);",
                "\t\t\t\t\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1654",
        "func_name": "gpac/gf_filter_pid_inst_del",
        "description": "Denial of Service in GitHub repository gpac/gpac prior to 2.4.0.",
        "git_url": "https://github.com/gpac/gpac/commit/2c055153d401b8c49422971e3a0159869652d3da",
        "commit_title": "fixed #2429",
        "commit_text": "",
        "func_before": "void gf_filter_pid_inst_del(GF_FilterPidInst *pidinst)\n{\n\tassert(pidinst);\n\tgf_filter_pid_inst_reset(pidinst);\n\n \tgf_fq_del(pidinst->packets, (gf_destruct_fun) pcki_del);\n\tgf_mx_del(pidinst->pck_mx);\n\tgf_list_del(pidinst->pck_reassembly);\n\tif (pidinst->props) {\n\t\tassert(pidinst->props->reference_count);\n\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {\n\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n\t\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\t\t\tgf_props_del(pidinst->props);\n\t\t}\n\t}\n\tgf_free(pidinst);\n}",
        "func": "void gf_filter_pid_inst_del(GF_FilterPidInst *pidinst)\n{\n\tassert(pidinst);\n\tgf_filter_pid_inst_reset(pidinst);\n\n \tgf_fq_del(pidinst->packets, (gf_destruct_fun) pcki_del);\n\tgf_mx_del(pidinst->pck_mx);\n\tgf_list_del(pidinst->pck_reassembly);\n\tif (pidinst->props) {\n\t\tassert(pidinst->props->reference_count);\n\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n\t\t//not in parent pid, may happen when reattaching a pid inst to a different pid\n\t\t//in this case do NOT delete the props\n\t\tif (gf_list_find(pidinst->pid->properties, pidinst->props)>=0) {\n\t\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {\n\t\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n\t\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n\t\t\t\tgf_props_del(pidinst->props);\n\t\t\t}\n\t\t}\n\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n\t}\n\tgf_free(pidinst);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,13 +8,17 @@\n \tgf_list_del(pidinst->pck_reassembly);\n \tif (pidinst->props) {\n \t\tassert(pidinst->props->reference_count);\n-\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {\n-\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n-\t\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n-\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n-\t\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n-\t\t\tgf_props_del(pidinst->props);\n+\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);\n+\t\t//not in parent pid, may happen when reattaching a pid inst to a different pid\n+\t\t//in this case do NOT delete the props\n+\t\tif (gf_list_find(pidinst->pid->properties, pidinst->props)>=0) {\n+\t\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {\n+\t\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex\n+\t\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);\n+\t\t\t\tgf_props_del(pidinst->props);\n+\t\t\t}\n \t\t}\n+\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);\n \t}\n \tgf_free(pidinst);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {",
                "\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex",
                "\t\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);",
                "\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);",
                "\t\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);",
                "\t\t\tgf_props_del(pidinst->props);"
            ],
            "added_lines": [
                "\t\tgf_mx_p(pidinst->pid->filter->tasks_mx);",
                "\t\t//not in parent pid, may happen when reattaching a pid inst to a different pid",
                "\t\t//in this case do NOT delete the props",
                "\t\tif (gf_list_find(pidinst->pid->properties, pidinst->props)>=0) {",
                "\t\t\tif (safe_int_dec(&pidinst->props->reference_count) == 0) {",
                "\t\t\t\t//see \\ref gf_filter_pid_merge_properties_internal for mutex",
                "\t\t\t\tgf_list_del_item(pidinst->pid->properties, pidinst->props);",
                "\t\t\t\tgf_props_del(pidinst->props);",
                "\t\t\t}",
                "\t\tgf_mx_v(pidinst->pid->filter->tasks_mx);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-4899",
        "func_name": "facebook/zstd/mallocAndJoin2Dir",
        "description": "A vulnerability was found in zstd v1.4.10, where an attacker can supply empty string as an argument to the command line tool to cause buffer overrun.",
        "git_url": "https://github.com/facebook/zstd/commit/e1873ad576cb478fff0e6e44ad99599cd5fd2846",
        "commit_title": "Fix buffer underflow for null dir1",
        "commit_text": "",
        "func_before": "static char* mallocAndJoin2Dir(const char *dir1, const char *dir2)\n{\n    const size_t dir1Size = strlen(dir1);\n    const size_t dir2Size = strlen(dir2);\n    char *outDirBuffer, *buffer, trailingChar;\n\n    assert(dir1 != NULL && dir2 != NULL);\n    outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);\n    CONTROL(outDirBuffer != NULL);\n\n    memcpy(outDirBuffer, dir1, dir1Size);\n    outDirBuffer[dir1Size] = '\\0';\n\n    if (dir2[0] == '.')\n        return outDirBuffer;\n\n    buffer = outDirBuffer + dir1Size;\n    trailingChar = *(buffer - 1);\n    if (trailingChar != PATH_SEP) {\n        *buffer = PATH_SEP;\n        buffer++;\n    }\n    memcpy(buffer, dir2, dir2Size);\n    buffer[dir2Size] = '\\0';\n\n    return outDirBuffer;\n}",
        "func": "static char* mallocAndJoin2Dir(const char *dir1, const char *dir2)\n{\n    assert(dir1 != NULL && dir2 != NULL);\n    {   const size_t dir1Size = strlen(dir1);\n        const size_t dir2Size = strlen(dir2);\n        char *outDirBuffer, *buffer;\n\n        outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);\n        CONTROL(outDirBuffer != NULL);\n\n        memcpy(outDirBuffer, dir1, dir1Size);\n        outDirBuffer[dir1Size] = '\\0';\n\n        if (dir2[0] == '.')\n            return outDirBuffer;\n\n        buffer = outDirBuffer + dir1Size;\n        if (dir1Size > 0 && *(buffer - 1) != PATH_SEP) {\n            *buffer = PATH_SEP;\n            buffer++;\n        }\n        memcpy(buffer, dir2, dir2Size);\n        buffer[dir2Size] = '\\0';\n\n        return outDirBuffer;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,27 +1,27 @@\n static char* mallocAndJoin2Dir(const char *dir1, const char *dir2)\n {\n-    const size_t dir1Size = strlen(dir1);\n-    const size_t dir2Size = strlen(dir2);\n-    char *outDirBuffer, *buffer, trailingChar;\n+    assert(dir1 != NULL && dir2 != NULL);\n+    {   const size_t dir1Size = strlen(dir1);\n+        const size_t dir2Size = strlen(dir2);\n+        char *outDirBuffer, *buffer;\n \n-    assert(dir1 != NULL && dir2 != NULL);\n-    outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);\n-    CONTROL(outDirBuffer != NULL);\n+        outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);\n+        CONTROL(outDirBuffer != NULL);\n \n-    memcpy(outDirBuffer, dir1, dir1Size);\n-    outDirBuffer[dir1Size] = '\\0';\n+        memcpy(outDirBuffer, dir1, dir1Size);\n+        outDirBuffer[dir1Size] = '\\0';\n \n-    if (dir2[0] == '.')\n+        if (dir2[0] == '.')\n+            return outDirBuffer;\n+\n+        buffer = outDirBuffer + dir1Size;\n+        if (dir1Size > 0 && *(buffer - 1) != PATH_SEP) {\n+            *buffer = PATH_SEP;\n+            buffer++;\n+        }\n+        memcpy(buffer, dir2, dir2Size);\n+        buffer[dir2Size] = '\\0';\n+\n         return outDirBuffer;\n-\n-    buffer = outDirBuffer + dir1Size;\n-    trailingChar = *(buffer - 1);\n-    if (trailingChar != PATH_SEP) {\n-        *buffer = PATH_SEP;\n-        buffer++;\n     }\n-    memcpy(buffer, dir2, dir2Size);\n-    buffer[dir2Size] = '\\0';\n-\n-    return outDirBuffer;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    const size_t dir1Size = strlen(dir1);",
                "    const size_t dir2Size = strlen(dir2);",
                "    char *outDirBuffer, *buffer, trailingChar;",
                "    assert(dir1 != NULL && dir2 != NULL);",
                "    outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);",
                "    CONTROL(outDirBuffer != NULL);",
                "    memcpy(outDirBuffer, dir1, dir1Size);",
                "    outDirBuffer[dir1Size] = '\\0';",
                "    if (dir2[0] == '.')",
                "",
                "    buffer = outDirBuffer + dir1Size;",
                "    trailingChar = *(buffer - 1);",
                "    if (trailingChar != PATH_SEP) {",
                "        *buffer = PATH_SEP;",
                "        buffer++;",
                "    memcpy(buffer, dir2, dir2Size);",
                "    buffer[dir2Size] = '\\0';",
                "",
                "    return outDirBuffer;"
            ],
            "added_lines": [
                "    assert(dir1 != NULL && dir2 != NULL);",
                "    {   const size_t dir1Size = strlen(dir1);",
                "        const size_t dir2Size = strlen(dir2);",
                "        char *outDirBuffer, *buffer;",
                "        outDirBuffer = (char *) malloc(dir1Size + dir2Size + 2);",
                "        CONTROL(outDirBuffer != NULL);",
                "        memcpy(outDirBuffer, dir1, dir1Size);",
                "        outDirBuffer[dir1Size] = '\\0';",
                "        if (dir2[0] == '.')",
                "            return outDirBuffer;",
                "",
                "        buffer = outDirBuffer + dir1Size;",
                "        if (dir1Size > 0 && *(buffer - 1) != PATH_SEP) {",
                "            *buffer = PATH_SEP;",
                "            buffer++;",
                "        }",
                "        memcpy(buffer, dir2, dir2Size);",
                "        buffer[dir2Size] = '\\0';",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-4899",
        "func_name": "facebook/zstd/main",
        "description": "A vulnerability was found in zstd v1.4.10, where an attacker can supply empty string as an argument to the command line tool to cause buffer overrun.",
        "git_url": "https://github.com/facebook/zstd/commit/f9f27de91c89d826c6a39c3ef44fb1b02f9a43aa",
        "commit_title": "Disallow empty output directory",
        "commit_text": "",
        "func_before": "int main(int argCount, const char* argv[])\n{\n    int argNb,\n        followLinks = 0,\n        allowBlockDevices = 0,\n        forceStdin = 0,\n        forceStdout = 0,\n        hasStdout = 0,\n        ldmFlag = 0,\n        main_pause = 0,\n        nbWorkers = 0,\n        adapt = 0,\n        useRowMatchFinder = 0,\n        adaptMin = MINCLEVEL,\n        adaptMax = MAXCLEVEL,\n        rsyncable = 0,\n        nextArgumentsAreFiles = 0,\n        operationResult = 0,\n        separateFiles = 0,\n        setRealTimePrio = 0,\n        singleThread = 0,\n        defaultLogicalCores = 0,\n        showDefaultCParams = 0,\n        ultra=0,\n        contentSize=1;\n    double compressibility = 0.5;\n    unsigned bench_nbSeconds = 3;   /* would be better if this value was synchronized from bench */\n    size_t blockSize = 0;\n\n    FIO_prefs_t* const prefs = FIO_createPreferences();\n    FIO_ctx_t* const fCtx = FIO_createContext();\n    zstd_operation_mode operation = zom_compress;\n    ZSTD_compressionParameters compressionParams;\n    int cLevel = init_cLevel();\n    int cLevelLast = MINCLEVEL - 1;  /* lower than minimum */\n    unsigned recursive = 0;\n    unsigned memLimit = 0;\n    FileNamesTable* filenames = UTIL_allocateFileNamesTable((size_t)argCount);  /* argCount >= 1 */\n    FileNamesTable* file_of_names = UTIL_allocateFileNamesTable((size_t)argCount);  /* argCount >= 1 */\n    const char* programName = argv[0];\n    const char* outFileName = NULL;\n    const char* outDirName = NULL;\n    const char* outMirroredDirName = NULL;\n    const char* dictFileName = NULL;\n    const char* patchFromDictFileName = NULL;\n    const char* suffix = ZSTD_EXTENSION;\n    unsigned maxDictSize = g_defaultMaxDictSize;\n    unsigned dictID = 0;\n    size_t streamSrcSize = 0;\n    size_t targetCBlockSize = 0;\n    size_t srcSizeHint = 0;\n    size_t nbInputFileNames = 0;\n    int dictCLevel = g_defaultDictCLevel;\n    unsigned dictSelect = g_defaultSelectivityLevel;\n#ifndef ZSTD_NODICT\n    ZDICT_cover_params_t coverParams = defaultCoverParams();\n    ZDICT_fastCover_params_t fastCoverParams = defaultFastCoverParams();\n    dictType dict = fastCover;\n#endif\n#ifndef ZSTD_NOBENCH\n    BMK_advancedParams_t benchParams = BMK_initAdvancedParams();\n#endif\n    ZSTD_paramSwitch_e literalCompressionMode = ZSTD_ps_auto;\n\n\n    /* init */\n    checkLibVersion();\n    (void)recursive; (void)cLevelLast;    /* not used when ZSTD_NOBENCH set */\n    (void)memLimit;\n    assert(argCount >= 1);\n    if ((filenames==NULL) || (file_of_names==NULL)) { DISPLAY(\"zstd: allocation error \\n\"); exit(1); }\n    programName = lastNameFromPath(programName);\n#ifdef ZSTD_MULTITHREAD\n    nbWorkers = init_nbThreads();\n#endif\n\n    /* preset behaviors */\n    if (exeNameMatch(programName, ZSTD_ZSTDMT)) nbWorkers=0, singleThread=0;\n    if (exeNameMatch(programName, ZSTD_UNZSTD)) operation=zom_decompress;\n    if (exeNameMatch(programName, ZSTD_CAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }     /* supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_ZCAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }    /* behave like zcat, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_GZ)) {   /* behave like gzip */\n        suffix = GZ_EXTENSION; FIO_setCompressionType(prefs, FIO_gzipCompression); FIO_setRemoveSrcFile(prefs, 1);\n        dictCLevel = cLevel = 6;  /* gzip default is -6 */\n    }\n    if (exeNameMatch(programName, ZSTD_GUNZIP)) { operation=zom_decompress; FIO_setRemoveSrcFile(prefs, 1); }                                                     /* behave like gunzip, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_GZCAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }   /* behave like gzcat, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_LZMA)) { suffix = LZMA_EXTENSION; FIO_setCompressionType(prefs, FIO_lzmaCompression); FIO_setRemoveSrcFile(prefs, 1); }    /* behave like lzma */\n    if (exeNameMatch(programName, ZSTD_UNLZMA)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_lzmaCompression); FIO_setRemoveSrcFile(prefs, 1); } /* behave like unlzma, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_XZ)) { suffix = XZ_EXTENSION; FIO_setCompressionType(prefs, FIO_xzCompression); FIO_setRemoveSrcFile(prefs, 1); }          /* behave like xz */\n    if (exeNameMatch(programName, ZSTD_UNXZ)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_xzCompression); FIO_setRemoveSrcFile(prefs, 1); }     /* behave like unxz, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_LZ4)) { suffix = LZ4_EXTENSION; FIO_setCompressionType(prefs, FIO_lz4Compression); }                                       /* behave like lz4 */\n    if (exeNameMatch(programName, ZSTD_UNLZ4)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_lz4Compression); }                                   /* behave like unlz4, also supports multiple formats */\n    memset(&compressionParams, 0, sizeof(compressionParams));\n\n    /* init crash handler */\n    FIO_addAbortHandler();\n\n    /* command switches */\n    for (argNb=1; argNb<argCount; argNb++) {\n        const char* argument = argv[argNb];\n        if (!argument) continue;   /* Protection if argument empty */\n\n        if (nextArgumentsAreFiles) {\n            UTIL_refFilename(filenames, argument);\n            continue;\n        }\n\n        /* \"-\" means stdin/stdout */\n        if (!strcmp(argument, \"-\")){\n            UTIL_refFilename(filenames, stdinmark);\n            continue;\n        }\n\n        /* Decode commands (note : aggregated commands are allowed) */\n        if (argument[0]=='-') {\n\n            if (argument[1]=='-') {\n                /* long commands (--long-word) */\n                if (!strcmp(argument, \"--\")) { nextArgumentsAreFiles=1; continue; }   /* only file names allowed from now on */\n                if (!strcmp(argument, \"--list\")) { operation=zom_list; continue; }\n                if (!strcmp(argument, \"--compress\")) { operation=zom_compress; continue; }\n                if (!strcmp(argument, \"--decompress\")) { operation=zom_decompress; continue; }\n                if (!strcmp(argument, \"--uncompress\")) { operation=zom_decompress; continue; }\n                if (!strcmp(argument, \"--force\")) { FIO_overwriteMode(prefs); forceStdin=1; forceStdout=1; followLinks=1; allowBlockDevices=1; continue; }\n                if (!strcmp(argument, \"--version\")) { printVersion(); CLEAN_RETURN(0); }\n                if (!strcmp(argument, \"--help\")) { usage_advanced(programName); CLEAN_RETURN(0); }\n                if (!strcmp(argument, \"--verbose\")) { g_displayLevel++; continue; }\n                if (!strcmp(argument, \"--quiet\")) { g_displayLevel--; continue; }\n                if (!strcmp(argument, \"--stdout\")) { forceStdout=1; outFileName=stdoutmark; FIO_setRemoveSrcFile(prefs, 0); g_displayLevel-=(g_displayLevel==2); continue; }\n                if (!strcmp(argument, \"--ultra\")) { ultra=1; continue; }\n                if (!strcmp(argument, \"--check\")) { FIO_setChecksumFlag(prefs, 2); continue; }\n                if (!strcmp(argument, \"--no-check\")) { FIO_setChecksumFlag(prefs, 0); continue; }\n                if (!strcmp(argument, \"--sparse\")) { FIO_setSparseWrite(prefs, 2); continue; }\n                if (!strcmp(argument, \"--no-sparse\")) { FIO_setSparseWrite(prefs, 0); continue; }\n                if (!strcmp(argument, \"--test\")) { operation=zom_test; continue; }\n                if (!strcmp(argument, \"--asyncio\")) { FIO_setAsyncIOFlag(prefs, 1); continue;}\n                if (!strcmp(argument, \"--no-asyncio\")) { FIO_setAsyncIOFlag(prefs, 0); continue;}\n                if (!strcmp(argument, \"--train\")) { operation=zom_train; if (outFileName==NULL) outFileName=g_defaultDictName; continue; }\n                if (!strcmp(argument, \"--no-dictID\")) { FIO_setDictIDFlag(prefs, 0); continue; }\n                if (!strcmp(argument, \"--keep\")) { FIO_setRemoveSrcFile(prefs, 0); continue; }\n                if (!strcmp(argument, \"--rm\")) { FIO_setRemoveSrcFile(prefs, 1); continue; }\n                if (!strcmp(argument, \"--priority=rt\")) { setRealTimePrio = 1; continue; }\n                if (!strcmp(argument, \"--show-default-cparams\")) { showDefaultCParams = 1; continue; }\n                if (!strcmp(argument, \"--content-size\")) { contentSize = 1; continue; }\n                if (!strcmp(argument, \"--no-content-size\")) { contentSize = 0; continue; }\n                if (!strcmp(argument, \"--adapt\")) { adapt = 1; continue; }\n                if (!strcmp(argument, \"--no-row-match-finder\")) { useRowMatchFinder = 1; continue; }\n                if (!strcmp(argument, \"--row-match-finder\")) { useRowMatchFinder = 2; continue; }\n                if (longCommandWArg(&argument, \"--adapt=\")) { adapt = 1; if (!parseAdaptParameters(argument, &adaptMin, &adaptMax)) { badusage(programName); CLEAN_RETURN(1); } continue; }\n                if (!strcmp(argument, \"--single-thread\")) { nbWorkers = 0; singleThread = 1; continue; }\n                if (!strcmp(argument, \"--format=zstd\")) { suffix = ZSTD_EXTENSION; FIO_setCompressionType(prefs, FIO_zstdCompression); continue; }\n#ifdef ZSTD_GZCOMPRESS\n                if (!strcmp(argument, \"--format=gzip\")) { suffix = GZ_EXTENSION; FIO_setCompressionType(prefs, FIO_gzipCompression); continue; }\n                if (exeNameMatch(programName, ZSTD_GZ)) {     /* behave like gzip */\n                    if (!strcmp(argument, \"--best\")) { dictCLevel = cLevel = 9; continue; }\n                    if (!strcmp(argument, \"--no-name\")) { /* ignore for now */; continue; }\n                }\n#endif\n#ifdef ZSTD_LZMACOMPRESS\n                if (!strcmp(argument, \"--format=lzma\")) { suffix = LZMA_EXTENSION; FIO_setCompressionType(prefs, FIO_lzmaCompression);  continue; }\n                if (!strcmp(argument, \"--format=xz\")) { suffix = XZ_EXTENSION; FIO_setCompressionType(prefs, FIO_xzCompression);  continue; }\n#endif\n#ifdef ZSTD_LZ4COMPRESS\n                if (!strcmp(argument, \"--format=lz4\")) { suffix = LZ4_EXTENSION; FIO_setCompressionType(prefs, FIO_lz4Compression);  continue; }\n#endif\n                if (!strcmp(argument, \"--rsyncable\")) { rsyncable = 1; continue; }\n                if (!strcmp(argument, \"--compress-literals\")) { literalCompressionMode = ZSTD_ps_enable; continue; }\n                if (!strcmp(argument, \"--no-compress-literals\")) { literalCompressionMode = ZSTD_ps_disable; continue; }\n                if (!strcmp(argument, \"--no-progress\")) { FIO_setProgressSetting(FIO_ps_never); continue; }\n                if (!strcmp(argument, \"--progress\")) { FIO_setProgressSetting(FIO_ps_always); continue; }\n                if (!strcmp(argument, \"--exclude-compressed\")) { FIO_setExcludeCompressedFile(prefs, 1); continue; }\n\n                /* long commands with arguments */\n#ifndef ZSTD_NODICT\n                if (longCommandWArg(&argument, \"--train-cover\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = cover;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { memset(&coverParams, 0, sizeof(coverParams)); }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseCoverParameters(argument, &coverParams)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n                if (longCommandWArg(&argument, \"--train-fastcover\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = fastCover;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { memset(&fastCoverParams, 0, sizeof(fastCoverParams)); }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseFastCoverParameters(argument, &fastCoverParams)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n                if (longCommandWArg(&argument, \"--train-legacy\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = legacy;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { continue; }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseLegacyParameters(argument, &dictSelect)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n#endif\n                if (longCommandWArg(&argument, \"--threads\")) { NEXT_UINT32(nbWorkers); continue; }\n                if (longCommandWArg(&argument, \"--memlimit\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--memory\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--memlimit-decompress\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--block-size=\")) { blockSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--maxdict\")) { NEXT_UINT32(maxDictSize); continue; }\n                if (longCommandWArg(&argument, \"--dictID\")) { NEXT_UINT32(dictID); continue; }\n                if (longCommandWArg(&argument, \"--zstd=\")) { if (!parseCompressionParameters(argument, &compressionParams)) { badusage(programName); CLEAN_RETURN(1); } continue; }\n                if (longCommandWArg(&argument, \"--stream-size=\")) { streamSrcSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--target-compressed-block-size=\")) { targetCBlockSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--size-hint=\")) { srcSizeHint = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--output-dir-flat\")) { NEXT_FIELD(outDirName); continue; }\n                if (longCommandWArg(&argument, \"--auto-threads\")) {\n                    const char* threadDefault = NULL;\n                    NEXT_FIELD(threadDefault);\n                    if (strcmp(threadDefault, \"logical\") == 0)\n                        defaultLogicalCores = 1;\n                    continue;\n                }\n#ifdef UTIL_HAS_MIRRORFILELIST\n                if (longCommandWArg(&argument, \"--output-dir-mirror\")) { NEXT_FIELD(outMirroredDirName); continue; }\n#endif\n#ifndef ZSTD_NOTRACE\n                if (longCommandWArg(&argument, \"--trace\")) { char const* traceFile; NEXT_FIELD(traceFile); TRACE_enable(traceFile); continue; }\n#endif\n                if (longCommandWArg(&argument, \"--patch-from\")) { NEXT_FIELD(patchFromDictFileName); continue; }\n                if (longCommandWArg(&argument, \"--long\")) {\n                    unsigned ldmWindowLog = 0;\n                    ldmFlag = 1;\n                    /* Parse optional window log */\n                    if (*argument == '=') {\n                        ++argument;\n                        ldmWindowLog = readU32FromChar(&argument);\n                    } else if (*argument != 0) {\n                        /* Invalid character following --long */\n                        badusage(programName);\n                        CLEAN_RETURN(1);\n                    }\n                    /* Only set windowLog if not already set by --zstd */\n                    if (compressionParams.windowLog == 0)\n                        compressionParams.windowLog = ldmWindowLog;\n                    continue;\n                }\n#ifndef ZSTD_NOCOMPRESS   /* linking ZSTD_minCLevel() requires compression support */\n                if (longCommandWArg(&argument, \"--fast\")) {\n                    /* Parse optional acceleration factor */\n                    if (*argument == '=') {\n                        U32 const maxFast = (U32)-ZSTD_minCLevel();\n                        U32 fastLevel;\n                        ++argument;\n                        fastLevel = readU32FromChar(&argument);\n                        if (fastLevel > maxFast) fastLevel = maxFast;\n                        if (fastLevel) {\n                            dictCLevel = cLevel = -(int)fastLevel;\n                        } else {\n                            badusage(programName);\n                            CLEAN_RETURN(1);\n                        }\n                    } else if (*argument != 0) {\n                        /* Invalid character following --fast */\n                        badusage(programName);\n                        CLEAN_RETURN(1);\n                    } else {\n                        cLevel = -1;  /* default for --fast */\n                    }\n                    continue;\n                }\n#endif\n\n                if (longCommandWArg(&argument, \"--filelist\")) {\n                    const char* listName;\n                    NEXT_FIELD(listName);\n                    UTIL_refFilename(file_of_names, listName);\n                    continue;\n                }\n\n                /* fall-through, will trigger bad_usage() later on */\n            }\n\n            argument++;\n            while (argument[0]!=0) {\n\n#ifndef ZSTD_NOCOMPRESS\n                /* compression Level */\n                if ((*argument>='0') && (*argument<='9')) {\n                    dictCLevel = cLevel = (int)readU32FromChar(&argument);\n                    continue;\n                }\n#endif\n\n                switch(argument[0])\n                {\n                    /* Display help */\n                case 'V': printVersion(); CLEAN_RETURN(0);   /* Version Only */\n                case 'H': usage_advanced(programName); CLEAN_RETURN(0);\n                case 'h': usage(stdout, programName); CLEAN_RETURN(0);\n\n                     /* Compress */\n                case 'z': operation=zom_compress; argument++; break;\n\n                     /* Decoding */\n                case 'd':\n#ifndef ZSTD_NOBENCH\n                        benchParams.mode = BMK_decodeOnly;\n                        if (operation==zom_bench) { argument++; break; }  /* benchmark decode (hidden option) */\n#endif\n                        operation=zom_decompress; argument++; break;\n\n                    /* Force stdout, even if stdout==console */\n                case 'c': forceStdout=1; outFileName=stdoutmark; FIO_setRemoveSrcFile(prefs, 0); argument++; break;\n\n                    /* do not store filename - gzip compatibility - nothing to do */\n                case 'n': argument++; break;\n\n                    /* Use file content as dictionary */\n                case 'D': argument++; NEXT_FIELD(dictFileName); break;\n\n                    /* Overwrite */\n                case 'f': FIO_overwriteMode(prefs); forceStdin=1; forceStdout=1; followLinks=1; allowBlockDevices=1; argument++; break;\n\n                    /* Verbose mode */\n                case 'v': g_displayLevel++; argument++; break;\n\n                    /* Quiet mode */\n                case 'q': g_displayLevel--; argument++; break;\n\n                    /* keep source file (default) */\n                case 'k': FIO_setRemoveSrcFile(prefs, 0); argument++; break;\n\n                    /* Checksum */\n                case 'C': FIO_setChecksumFlag(prefs, 2); argument++; break;\n\n                    /* test compressed file */\n                case 't': operation=zom_test; argument++; break;\n\n                    /* destination file name */\n                case 'o': argument++; NEXT_FIELD(outFileName); break;\n\n                    /* limit memory */\n                case 'M':\n                    argument++;\n                    memLimit = readU32FromChar(&argument);\n                    break;\n                case 'l': operation=zom_list; argument++; break;\n#ifdef UTIL_HAS_CREATEFILELIST\n                    /* recursive */\n                case 'r': recursive=1; argument++; break;\n#endif\n\n#ifndef ZSTD_NOBENCH\n                    /* Benchmark */\n                case 'b':\n                    operation=zom_bench;\n                    argument++;\n                    break;\n\n                    /* range bench (benchmark only) */\n                case 'e':\n                    /* compression Level */\n                    argument++;\n                    cLevelLast = (int)readU32FromChar(&argument);\n                    break;\n\n                    /* Modify Nb Iterations (benchmark only) */\n                case 'i':\n                    argument++;\n                    bench_nbSeconds = readU32FromChar(&argument);\n                    break;\n\n                    /* cut input into blocks (benchmark only) */\n                case 'B':\n                    argument++;\n                    blockSize = readU32FromChar(&argument);\n                    break;\n\n                    /* benchmark files separately (hidden option) */\n                case 'S':\n                    argument++;\n                    separateFiles = 1;\n                    break;\n\n#endif   /* ZSTD_NOBENCH */\n\n                    /* nb of threads (hidden option) */\n                case 'T':\n                    argument++;\n                    nbWorkers = (int)readU32FromChar(&argument);\n                    break;\n\n                    /* Dictionary Selection level */\n                case 's':\n                    argument++;\n                    dictSelect = readU32FromChar(&argument);\n                    break;\n\n                    /* Pause at the end (-p) or set an additional param (-p#) (hidden option) */\n                case 'p': argument++;\n#ifndef ZSTD_NOBENCH\n                    if ((*argument>='0') && (*argument<='9')) {\n                        benchParams.additionalParam = (int)readU32FromChar(&argument);\n                    } else\n#endif\n                        main_pause=1;\n                    break;\n\n                    /* Select compressibility of synthetic sample */\n                case 'P':\n                    argument++;\n                    compressibility = (double)readU32FromChar(&argument) / 100;\n                    break;\n\n                    /* unknown command */\n                default : badusage(programName); CLEAN_RETURN(1);\n                }\n            }\n            continue;\n        }   /* if (argument[0]=='-') */\n\n        /* none of the above : add filename to list */\n        UTIL_refFilename(filenames, argument);\n    }\n\n    /* Welcome message (if verbose) */\n    DISPLAYLEVEL(3, WELCOME_MESSAGE);\n\n#ifdef ZSTD_MULTITHREAD\n    if ((nbWorkers==0) && (!singleThread)) {\n        /* automatically set # workers based on # of reported cpus */\n        if (defaultLogicalCores) {\n            nbWorkers = UTIL_countLogicalCores();\n            DISPLAYLEVEL(3, \"Note: %d logical core(s) detected \\n\", nbWorkers);\n        } else {\n            nbWorkers = UTIL_countPhysicalCores();\n            DISPLAYLEVEL(3, \"Note: %d physical core(s) detected \\n\", nbWorkers);\n        }\n    }\n#else\n    (void)singleThread; (void)nbWorkers; (void)defaultLogicalCores;\n#endif\n\n    g_utilDisplayLevel = g_displayLevel;\n\n#ifdef UTIL_HAS_CREATEFILELIST\n    if (!followLinks) {\n        unsigned u, fileNamesNb;\n        unsigned const nbFilenames = (unsigned)filenames->tableSize;\n        for (u=0, fileNamesNb=0; u<nbFilenames; u++) {\n            if ( UTIL_isLink(filenames->fileNames[u])\n             && !UTIL_isFIFO(filenames->fileNames[u])\n            ) {\n                DISPLAYLEVEL(2, \"Warning : %s is a symbolic link, ignoring \\n\", filenames->fileNames[u]);\n            } else {\n                filenames->fileNames[fileNamesNb++] = filenames->fileNames[u];\n        }   }\n        if (fileNamesNb == 0 && nbFilenames > 0)  /* all names are eliminated */\n            CLEAN_RETURN(1);\n        filenames->tableSize = fileNamesNb;\n    }   /* if (!followLinks) */\n\n    /* read names from a file */\n    if (file_of_names->tableSize) {\n        size_t const nbFileLists = file_of_names->tableSize;\n        size_t flNb;\n        for (flNb=0; flNb < nbFileLists; flNb++) {\n            FileNamesTable* const fnt = UTIL_createFileNamesTable_fromFileName(file_of_names->fileNames[flNb]);\n            if (fnt==NULL) {\n                DISPLAYLEVEL(1, \"zstd: error reading %s \\n\", file_of_names->fileNames[flNb]);\n                CLEAN_RETURN(1);\n            }\n            filenames = UTIL_mergeFileNamesTable(filenames, fnt);\n        }\n    }\n\n    nbInputFileNames = filenames->tableSize; /* saving number of input files */\n\n    if (recursive) {  /* at this stage, filenameTable is a list of paths, which can contain both files and directories */\n        UTIL_expandFNT(&filenames, followLinks);\n    }\n#else\n    (void)followLinks;\n#endif\n\n    if (operation == zom_list) {\n#ifndef ZSTD_NODECOMPRESS\n        int const ret = FIO_listMultipleFiles((unsigned)filenames->tableSize, filenames->fileNames, g_displayLevel);\n        CLEAN_RETURN(ret);\n#else\n        DISPLAY(\"file information is not supported \\n\");\n        CLEAN_RETURN(1);\n#endif\n    }\n\n    /* Check if benchmark is selected */\n    if (operation==zom_bench) {\n#ifndef ZSTD_NOBENCH\n        benchParams.blockSize = blockSize;\n        benchParams.nbWorkers = nbWorkers;\n        benchParams.realTime = (unsigned)setRealTimePrio;\n        benchParams.nbSeconds = bench_nbSeconds;\n        benchParams.ldmFlag = ldmFlag;\n        benchParams.ldmMinMatch = (int)g_ldmMinMatch;\n        benchParams.ldmHashLog = (int)g_ldmHashLog;\n        benchParams.useRowMatchFinder = useRowMatchFinder;\n        if (g_ldmBucketSizeLog != LDM_PARAM_DEFAULT) {\n            benchParams.ldmBucketSizeLog = (int)g_ldmBucketSizeLog;\n        }\n        if (g_ldmHashRateLog != LDM_PARAM_DEFAULT) {\n            benchParams.ldmHashRateLog = (int)g_ldmHashRateLog;\n        }\n        benchParams.literalCompressionMode = literalCompressionMode;\n\n        if (cLevel > ZSTD_maxCLevel()) cLevel = ZSTD_maxCLevel();\n        if (cLevelLast > ZSTD_maxCLevel()) cLevelLast = ZSTD_maxCLevel();\n        if (cLevelLast < cLevel) cLevelLast = cLevel;\n        if (cLevelLast > cLevel)\n            DISPLAYLEVEL(3, \"Benchmarking levels from %d to %d\\n\", cLevel, cLevelLast);\n        if (filenames->tableSize > 0) {\n            if(separateFiles) {\n                unsigned i;\n                for(i = 0; i < filenames->tableSize; i++) {\n                    int c;\n                    DISPLAYLEVEL(3, \"Benchmarking %s \\n\", filenames->fileNames[i]);\n                    for(c = cLevel; c <= cLevelLast; c++) {\n                        BMK_benchFilesAdvanced(&filenames->fileNames[i], 1, dictFileName, c, &compressionParams, g_displayLevel, &benchParams);\n                }   }\n            } else {\n                for(; cLevel <= cLevelLast; cLevel++) {\n                    BMK_benchFilesAdvanced(filenames->fileNames, (unsigned)filenames->tableSize, dictFileName, cLevel, &compressionParams, g_displayLevel, &benchParams);\n            }   }\n        } else {\n            for(; cLevel <= cLevelLast; cLevel++) {\n                BMK_syntheticTest(cLevel, compressibility, &compressionParams, g_displayLevel, &benchParams);\n        }   }\n\n#else\n        (void)bench_nbSeconds; (void)blockSize; (void)setRealTimePrio; (void)separateFiles; (void)compressibility;\n#endif\n        goto _end;\n    }\n\n    /* Check if dictionary builder is selected */\n    if (operation==zom_train) {\n#ifndef ZSTD_NODICT\n        ZDICT_params_t zParams;\n        zParams.compressionLevel = dictCLevel;\n        zParams.notificationLevel = (unsigned)g_displayLevel;\n        zParams.dictID = dictID;\n        if (dict == cover) {\n            int const optimize = !coverParams.k || !coverParams.d;\n            coverParams.nbThreads = (unsigned)nbWorkers;\n            coverParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, NULL, &coverParams, NULL, optimize, memLimit);\n        } else if (dict == fastCover) {\n            int const optimize = !fastCoverParams.k || !fastCoverParams.d;\n            fastCoverParams.nbThreads = (unsigned)nbWorkers;\n            fastCoverParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, NULL, NULL, &fastCoverParams, optimize, memLimit);\n        } else {\n            ZDICT_legacy_params_t dictParams;\n            memset(&dictParams, 0, sizeof(dictParams));\n            dictParams.selectivityLevel = dictSelect;\n            dictParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, &dictParams, NULL, NULL, 0, memLimit);\n        }\n#else\n        (void)dictCLevel; (void)dictSelect; (void)dictID;  (void)maxDictSize; /* not used when ZSTD_NODICT set */\n        DISPLAYLEVEL(1, \"training mode not available \\n\");\n        operationResult = 1;\n#endif\n        goto _end;\n    }\n\n#ifndef ZSTD_NODECOMPRESS\n    if (operation==zom_test) { FIO_setTestMode(prefs, 1); outFileName=nulmark; FIO_setRemoveSrcFile(prefs, 0); }  /* test mode */\n#endif\n\n    /* No input filename ==> use stdin and stdout */\n    if (filenames->tableSize == 0) {\n      /* It is possible that the input\n       was a number of empty directories. In this case\n       stdin and stdout should not be used */\n       if (nbInputFileNames > 0 ){\n        DISPLAYLEVEL(1, \"please provide correct input file(s) or non-empty directories -- ignored \\n\");\n        CLEAN_RETURN(0);\n       }\n       UTIL_refFilename(filenames, stdinmark);\n    }\n\n    if (!strcmp(filenames->fileNames[0], stdinmark) && !outFileName)\n        outFileName = stdoutmark;  /* when input is stdin, default output is stdout */\n\n    /* Check if input/output defined as console; trigger an error in this case */\n    if (!forceStdin\n     && !strcmp(filenames->fileNames[0], stdinmark)\n     && IS_CONSOLE(stdin) ) {\n        DISPLAYLEVEL(1, \"stdin is a console, aborting\\n\");\n        CLEAN_RETURN(1);\n    }\n    if ( outFileName && !strcmp(outFileName, stdoutmark)\n      && IS_CONSOLE(stdout)\n      && !strcmp(filenames->fileNames[0], stdinmark)\n      && !forceStdout\n      && operation!=zom_decompress ) {\n        DISPLAYLEVEL(1, \"stdout is a console, aborting\\n\");\n        CLEAN_RETURN(1);\n    }\n\n#ifndef ZSTD_NOCOMPRESS\n    /* check compression level limits */\n    {   int const maxCLevel = ultra ? ZSTD_maxCLevel() : ZSTDCLI_CLEVEL_MAX;\n        if (cLevel > maxCLevel) {\n            DISPLAYLEVEL(2, \"Warning : compression level higher than max, reduced to %i \\n\", maxCLevel);\n            cLevel = maxCLevel;\n    }   }\n#endif\n\n    if (showDefaultCParams) {\n        if (operation == zom_decompress) {\n            DISPLAY(\"error : can't use --show-default-cparams in decompression mode \\n\");\n            CLEAN_RETURN(1);\n        }\n    }\n\n    if (dictFileName != NULL && patchFromDictFileName != NULL) {\n        DISPLAY(\"error : can't use -D and --patch-from=# at the same time \\n\");\n        CLEAN_RETURN(1);\n    }\n\n    if (patchFromDictFileName != NULL && filenames->tableSize > 1) {\n        DISPLAY(\"error : can't use --patch-from=# on multiple files \\n\");\n        CLEAN_RETURN(1);\n    }\n\n    /* No status message in pipe mode (stdin - stdout) */\n    hasStdout = outFileName && !strcmp(outFileName,stdoutmark);\n\n    if ((hasStdout || !IS_CONSOLE(stderr)) && (g_displayLevel==2)) g_displayLevel=1;\n\n    /* IO Stream/File */\n    FIO_setHasStdoutOutput(fCtx, hasStdout);\n    FIO_setNbFilesTotal(fCtx, (int)filenames->tableSize);\n    FIO_determineHasStdinInput(fCtx, filenames);\n    FIO_setNotificationLevel(g_displayLevel);\n    FIO_setAllowBlockDevices(prefs, allowBlockDevices);\n    FIO_setPatchFromMode(prefs, patchFromDictFileName != NULL);\n    if (memLimit == 0) {\n        if (compressionParams.windowLog == 0) {\n            memLimit = (U32)1 << g_defaultMaxWindowLog;\n        } else {\n            memLimit = (U32)1 << (compressionParams.windowLog & 31);\n    }   }\n    if (patchFromDictFileName != NULL)\n        dictFileName = patchFromDictFileName;\n    FIO_setMemLimit(prefs, memLimit);\n    if (operation==zom_compress) {\n#ifndef ZSTD_NOCOMPRESS\n        FIO_setContentSize(prefs, contentSize);\n        FIO_setNbWorkers(prefs, nbWorkers);\n        FIO_setBlockSize(prefs, (int)blockSize);\n        if (g_overlapLog!=OVERLAP_LOG_DEFAULT) FIO_setOverlapLog(prefs, (int)g_overlapLog);\n        FIO_setLdmFlag(prefs, (unsigned)ldmFlag);\n        FIO_setLdmHashLog(prefs, (int)g_ldmHashLog);\n        FIO_setLdmMinMatch(prefs, (int)g_ldmMinMatch);\n        if (g_ldmBucketSizeLog != LDM_PARAM_DEFAULT) FIO_setLdmBucketSizeLog(prefs, (int)g_ldmBucketSizeLog);\n        if (g_ldmHashRateLog != LDM_PARAM_DEFAULT) FIO_setLdmHashRateLog(prefs, (int)g_ldmHashRateLog);\n        FIO_setAdaptiveMode(prefs, (unsigned)adapt);\n        FIO_setUseRowMatchFinder(prefs, useRowMatchFinder);\n        FIO_setAdaptMin(prefs, adaptMin);\n        FIO_setAdaptMax(prefs, adaptMax);\n        FIO_setRsyncable(prefs, rsyncable);\n        FIO_setStreamSrcSize(prefs, streamSrcSize);\n        FIO_setTargetCBlockSize(prefs, targetCBlockSize);\n        FIO_setSrcSizeHint(prefs, srcSizeHint);\n        FIO_setLiteralCompressionMode(prefs, literalCompressionMode);\n        FIO_setSparseWrite(prefs, 0);\n        if (adaptMin > cLevel) cLevel = adaptMin;\n        if (adaptMax < cLevel) cLevel = adaptMax;\n\n        /* Compare strategies constant with the ground truth */\n        { ZSTD_bounds strategyBounds = ZSTD_cParam_getBounds(ZSTD_c_strategy);\n          assert(ZSTD_NB_STRATEGIES == strategyBounds.upperBound);\n          (void)strategyBounds; }\n\n        if (showDefaultCParams || g_displayLevel >= 4) {\n            size_t fileNb;\n            for (fileNb = 0; fileNb < (size_t)filenames->tableSize; fileNb++) {\n                if (showDefaultCParams)\n                    printDefaultCParams(filenames->fileNames[fileNb], dictFileName, cLevel);\n                if (g_displayLevel >= 4)\n                    printActualCParams(filenames->fileNames[fileNb], dictFileName, cLevel, &compressionParams);\n            }\n        }\n\n        if (g_displayLevel >= 4)\n            FIO_displayCompressionParameters(prefs);\n        if ((filenames->tableSize==1) && outFileName)\n            operationResult = FIO_compressFilename(fCtx, prefs, outFileName, filenames->fileNames[0], dictFileName, cLevel, compressionParams);\n        else\n            operationResult = FIO_compressMultipleFilenames(fCtx, prefs, filenames->fileNames, outMirroredDirName, outDirName, outFileName, suffix, dictFileName, cLevel, compressionParams);\n#else\n        (void)contentSize; (void)suffix; (void)adapt; (void)rsyncable; (void)ultra; (void)cLevel; (void)ldmFlag; (void)literalCompressionMode; (void)targetCBlockSize; (void)streamSrcSize; (void)srcSizeHint; (void)ZSTD_strategyMap; (void)useRowMatchFinder; /* not used when ZSTD_NOCOMPRESS set */\n        DISPLAY(\"Compression not supported \\n\");\n#endif\n    } else {  /* decompression or test */\n#ifndef ZSTD_NODECOMPRESS\n        if (filenames->tableSize == 1 && outFileName) {\n            operationResult = FIO_decompressFilename(fCtx, prefs, outFileName, filenames->fileNames[0], dictFileName);\n        } else {\n            operationResult = FIO_decompressMultipleFilenames(fCtx, prefs, filenames->fileNames, outMirroredDirName, outDirName, outFileName, dictFileName);\n        }\n#else\n        DISPLAY(\"Decompression not supported \\n\");\n#endif\n    }\n\n_end:\n    FIO_freePreferences(prefs);\n    FIO_freeContext(fCtx);\n    if (main_pause) waitEnter();\n    UTIL_freeFileNamesTable(filenames);\n    UTIL_freeFileNamesTable(file_of_names);\n#ifndef ZSTD_NOTRACE\n    TRACE_finish();\n#endif\n\n    return operationResult;\n}",
        "func": "int main(int argCount, const char* argv[])\n{\n    int argNb,\n        followLinks = 0,\n        allowBlockDevices = 0,\n        forceStdin = 0,\n        forceStdout = 0,\n        hasStdout = 0,\n        ldmFlag = 0,\n        main_pause = 0,\n        nbWorkers = 0,\n        adapt = 0,\n        useRowMatchFinder = 0,\n        adaptMin = MINCLEVEL,\n        adaptMax = MAXCLEVEL,\n        rsyncable = 0,\n        nextArgumentsAreFiles = 0,\n        operationResult = 0,\n        separateFiles = 0,\n        setRealTimePrio = 0,\n        singleThread = 0,\n        defaultLogicalCores = 0,\n        showDefaultCParams = 0,\n        ultra=0,\n        contentSize=1;\n    double compressibility = 0.5;\n    unsigned bench_nbSeconds = 3;   /* would be better if this value was synchronized from bench */\n    size_t blockSize = 0;\n\n    FIO_prefs_t* const prefs = FIO_createPreferences();\n    FIO_ctx_t* const fCtx = FIO_createContext();\n    zstd_operation_mode operation = zom_compress;\n    ZSTD_compressionParameters compressionParams;\n    int cLevel = init_cLevel();\n    int cLevelLast = MINCLEVEL - 1;  /* lower than minimum */\n    unsigned recursive = 0;\n    unsigned memLimit = 0;\n    FileNamesTable* filenames = UTIL_allocateFileNamesTable((size_t)argCount);  /* argCount >= 1 */\n    FileNamesTable* file_of_names = UTIL_allocateFileNamesTable((size_t)argCount);  /* argCount >= 1 */\n    const char* programName = argv[0];\n    const char* outFileName = NULL;\n    const char* outDirName = NULL;\n    const char* outMirroredDirName = NULL;\n    const char* dictFileName = NULL;\n    const char* patchFromDictFileName = NULL;\n    const char* suffix = ZSTD_EXTENSION;\n    unsigned maxDictSize = g_defaultMaxDictSize;\n    unsigned dictID = 0;\n    size_t streamSrcSize = 0;\n    size_t targetCBlockSize = 0;\n    size_t srcSizeHint = 0;\n    size_t nbInputFileNames = 0;\n    int dictCLevel = g_defaultDictCLevel;\n    unsigned dictSelect = g_defaultSelectivityLevel;\n#ifndef ZSTD_NODICT\n    ZDICT_cover_params_t coverParams = defaultCoverParams();\n    ZDICT_fastCover_params_t fastCoverParams = defaultFastCoverParams();\n    dictType dict = fastCover;\n#endif\n#ifndef ZSTD_NOBENCH\n    BMK_advancedParams_t benchParams = BMK_initAdvancedParams();\n#endif\n    ZSTD_paramSwitch_e literalCompressionMode = ZSTD_ps_auto;\n\n\n    /* init */\n    checkLibVersion();\n    (void)recursive; (void)cLevelLast;    /* not used when ZSTD_NOBENCH set */\n    (void)memLimit;\n    assert(argCount >= 1);\n    if ((filenames==NULL) || (file_of_names==NULL)) { DISPLAY(\"zstd: allocation error \\n\"); exit(1); }\n    programName = lastNameFromPath(programName);\n#ifdef ZSTD_MULTITHREAD\n    nbWorkers = init_nbThreads();\n#endif\n\n    /* preset behaviors */\n    if (exeNameMatch(programName, ZSTD_ZSTDMT)) nbWorkers=0, singleThread=0;\n    if (exeNameMatch(programName, ZSTD_UNZSTD)) operation=zom_decompress;\n    if (exeNameMatch(programName, ZSTD_CAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }     /* supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_ZCAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }    /* behave like zcat, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_GZ)) {   /* behave like gzip */\n        suffix = GZ_EXTENSION; FIO_setCompressionType(prefs, FIO_gzipCompression); FIO_setRemoveSrcFile(prefs, 1);\n        dictCLevel = cLevel = 6;  /* gzip default is -6 */\n    }\n    if (exeNameMatch(programName, ZSTD_GUNZIP)) { operation=zom_decompress; FIO_setRemoveSrcFile(prefs, 1); }                                                     /* behave like gunzip, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_GZCAT)) { operation=zom_decompress; FIO_overwriteMode(prefs); forceStdout=1; followLinks=1; outFileName=stdoutmark; g_displayLevel=1; }   /* behave like gzcat, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_LZMA)) { suffix = LZMA_EXTENSION; FIO_setCompressionType(prefs, FIO_lzmaCompression); FIO_setRemoveSrcFile(prefs, 1); }    /* behave like lzma */\n    if (exeNameMatch(programName, ZSTD_UNLZMA)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_lzmaCompression); FIO_setRemoveSrcFile(prefs, 1); } /* behave like unlzma, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_XZ)) { suffix = XZ_EXTENSION; FIO_setCompressionType(prefs, FIO_xzCompression); FIO_setRemoveSrcFile(prefs, 1); }          /* behave like xz */\n    if (exeNameMatch(programName, ZSTD_UNXZ)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_xzCompression); FIO_setRemoveSrcFile(prefs, 1); }     /* behave like unxz, also supports multiple formats */\n    if (exeNameMatch(programName, ZSTD_LZ4)) { suffix = LZ4_EXTENSION; FIO_setCompressionType(prefs, FIO_lz4Compression); }                                       /* behave like lz4 */\n    if (exeNameMatch(programName, ZSTD_UNLZ4)) { operation=zom_decompress; FIO_setCompressionType(prefs, FIO_lz4Compression); }                                   /* behave like unlz4, also supports multiple formats */\n    memset(&compressionParams, 0, sizeof(compressionParams));\n\n    /* init crash handler */\n    FIO_addAbortHandler();\n\n    /* command switches */\n    for (argNb=1; argNb<argCount; argNb++) {\n        const char* argument = argv[argNb];\n        if (!argument) continue;   /* Protection if argument empty */\n\n        if (nextArgumentsAreFiles) {\n            UTIL_refFilename(filenames, argument);\n            continue;\n        }\n\n        /* \"-\" means stdin/stdout */\n        if (!strcmp(argument, \"-\")){\n            UTIL_refFilename(filenames, stdinmark);\n            continue;\n        }\n\n        /* Decode commands (note : aggregated commands are allowed) */\n        if (argument[0]=='-') {\n\n            if (argument[1]=='-') {\n                /* long commands (--long-word) */\n                if (!strcmp(argument, \"--\")) { nextArgumentsAreFiles=1; continue; }   /* only file names allowed from now on */\n                if (!strcmp(argument, \"--list\")) { operation=zom_list; continue; }\n                if (!strcmp(argument, \"--compress\")) { operation=zom_compress; continue; }\n                if (!strcmp(argument, \"--decompress\")) { operation=zom_decompress; continue; }\n                if (!strcmp(argument, \"--uncompress\")) { operation=zom_decompress; continue; }\n                if (!strcmp(argument, \"--force\")) { FIO_overwriteMode(prefs); forceStdin=1; forceStdout=1; followLinks=1; allowBlockDevices=1; continue; }\n                if (!strcmp(argument, \"--version\")) { printVersion(); CLEAN_RETURN(0); }\n                if (!strcmp(argument, \"--help\")) { usage_advanced(programName); CLEAN_RETURN(0); }\n                if (!strcmp(argument, \"--verbose\")) { g_displayLevel++; continue; }\n                if (!strcmp(argument, \"--quiet\")) { g_displayLevel--; continue; }\n                if (!strcmp(argument, \"--stdout\")) { forceStdout=1; outFileName=stdoutmark; FIO_setRemoveSrcFile(prefs, 0); g_displayLevel-=(g_displayLevel==2); continue; }\n                if (!strcmp(argument, \"--ultra\")) { ultra=1; continue; }\n                if (!strcmp(argument, \"--check\")) { FIO_setChecksumFlag(prefs, 2); continue; }\n                if (!strcmp(argument, \"--no-check\")) { FIO_setChecksumFlag(prefs, 0); continue; }\n                if (!strcmp(argument, \"--sparse\")) { FIO_setSparseWrite(prefs, 2); continue; }\n                if (!strcmp(argument, \"--no-sparse\")) { FIO_setSparseWrite(prefs, 0); continue; }\n                if (!strcmp(argument, \"--test\")) { operation=zom_test; continue; }\n                if (!strcmp(argument, \"--asyncio\")) { FIO_setAsyncIOFlag(prefs, 1); continue;}\n                if (!strcmp(argument, \"--no-asyncio\")) { FIO_setAsyncIOFlag(prefs, 0); continue;}\n                if (!strcmp(argument, \"--train\")) { operation=zom_train; if (outFileName==NULL) outFileName=g_defaultDictName; continue; }\n                if (!strcmp(argument, \"--no-dictID\")) { FIO_setDictIDFlag(prefs, 0); continue; }\n                if (!strcmp(argument, \"--keep\")) { FIO_setRemoveSrcFile(prefs, 0); continue; }\n                if (!strcmp(argument, \"--rm\")) { FIO_setRemoveSrcFile(prefs, 1); continue; }\n                if (!strcmp(argument, \"--priority=rt\")) { setRealTimePrio = 1; continue; }\n                if (!strcmp(argument, \"--show-default-cparams\")) { showDefaultCParams = 1; continue; }\n                if (!strcmp(argument, \"--content-size\")) { contentSize = 1; continue; }\n                if (!strcmp(argument, \"--no-content-size\")) { contentSize = 0; continue; }\n                if (!strcmp(argument, \"--adapt\")) { adapt = 1; continue; }\n                if (!strcmp(argument, \"--no-row-match-finder\")) { useRowMatchFinder = 1; continue; }\n                if (!strcmp(argument, \"--row-match-finder\")) { useRowMatchFinder = 2; continue; }\n                if (longCommandWArg(&argument, \"--adapt=\")) { adapt = 1; if (!parseAdaptParameters(argument, &adaptMin, &adaptMax)) { badusage(programName); CLEAN_RETURN(1); } continue; }\n                if (!strcmp(argument, \"--single-thread\")) { nbWorkers = 0; singleThread = 1; continue; }\n                if (!strcmp(argument, \"--format=zstd\")) { suffix = ZSTD_EXTENSION; FIO_setCompressionType(prefs, FIO_zstdCompression); continue; }\n#ifdef ZSTD_GZCOMPRESS\n                if (!strcmp(argument, \"--format=gzip\")) { suffix = GZ_EXTENSION; FIO_setCompressionType(prefs, FIO_gzipCompression); continue; }\n                if (exeNameMatch(programName, ZSTD_GZ)) {     /* behave like gzip */\n                    if (!strcmp(argument, \"--best\")) { dictCLevel = cLevel = 9; continue; }\n                    if (!strcmp(argument, \"--no-name\")) { /* ignore for now */; continue; }\n                }\n#endif\n#ifdef ZSTD_LZMACOMPRESS\n                if (!strcmp(argument, \"--format=lzma\")) { suffix = LZMA_EXTENSION; FIO_setCompressionType(prefs, FIO_lzmaCompression);  continue; }\n                if (!strcmp(argument, \"--format=xz\")) { suffix = XZ_EXTENSION; FIO_setCompressionType(prefs, FIO_xzCompression);  continue; }\n#endif\n#ifdef ZSTD_LZ4COMPRESS\n                if (!strcmp(argument, \"--format=lz4\")) { suffix = LZ4_EXTENSION; FIO_setCompressionType(prefs, FIO_lz4Compression);  continue; }\n#endif\n                if (!strcmp(argument, \"--rsyncable\")) { rsyncable = 1; continue; }\n                if (!strcmp(argument, \"--compress-literals\")) { literalCompressionMode = ZSTD_ps_enable; continue; }\n                if (!strcmp(argument, \"--no-compress-literals\")) { literalCompressionMode = ZSTD_ps_disable; continue; }\n                if (!strcmp(argument, \"--no-progress\")) { FIO_setProgressSetting(FIO_ps_never); continue; }\n                if (!strcmp(argument, \"--progress\")) { FIO_setProgressSetting(FIO_ps_always); continue; }\n                if (!strcmp(argument, \"--exclude-compressed\")) { FIO_setExcludeCompressedFile(prefs, 1); continue; }\n\n                /* long commands with arguments */\n#ifndef ZSTD_NODICT\n                if (longCommandWArg(&argument, \"--train-cover\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = cover;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { memset(&coverParams, 0, sizeof(coverParams)); }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseCoverParameters(argument, &coverParams)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n                if (longCommandWArg(&argument, \"--train-fastcover\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = fastCover;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { memset(&fastCoverParams, 0, sizeof(fastCoverParams)); }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseFastCoverParameters(argument, &fastCoverParams)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n                if (longCommandWArg(&argument, \"--train-legacy\")) {\n                  operation = zom_train;\n                  if (outFileName == NULL)\n                      outFileName = g_defaultDictName;\n                  dict = legacy;\n                  /* Allow optional arguments following an = */\n                  if (*argument == 0) { continue; }\n                  else if (*argument++ != '=') { badusage(programName); CLEAN_RETURN(1); }\n                  else if (!parseLegacyParameters(argument, &dictSelect)) { badusage(programName); CLEAN_RETURN(1); }\n                  continue;\n                }\n#endif\n                if (longCommandWArg(&argument, \"--threads\")) { NEXT_UINT32(nbWorkers); continue; }\n                if (longCommandWArg(&argument, \"--memlimit\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--memory\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--memlimit-decompress\")) { NEXT_UINT32(memLimit); continue; }\n                if (longCommandWArg(&argument, \"--block-size=\")) { blockSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--maxdict\")) { NEXT_UINT32(maxDictSize); continue; }\n                if (longCommandWArg(&argument, \"--dictID\")) { NEXT_UINT32(dictID); continue; }\n                if (longCommandWArg(&argument, \"--zstd=\")) { if (!parseCompressionParameters(argument, &compressionParams)) { badusage(programName); CLEAN_RETURN(1); } continue; }\n                if (longCommandWArg(&argument, \"--stream-size=\")) { streamSrcSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--target-compressed-block-size=\")) { targetCBlockSize = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--size-hint=\")) { srcSizeHint = readSizeTFromChar(&argument); continue; }\n                if (longCommandWArg(&argument, \"--output-dir-flat\")) {\n                    NEXT_FIELD(outDirName);\n                    if (strlen(outDirName) == 0) {\n                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");\n                        CLEAN_RETURN(1);\n                    }\n                    continue;\n                }\n                if (longCommandWArg(&argument, \"--auto-threads\")) {\n                    const char* threadDefault = NULL;\n                    NEXT_FIELD(threadDefault);\n                    if (strcmp(threadDefault, \"logical\") == 0)\n                        defaultLogicalCores = 1;\n                    continue;\n                }\n#ifdef UTIL_HAS_MIRRORFILELIST\n                if (longCommandWArg(&argument, \"--output-dir-mirror\")) {\n                    NEXT_FIELD(outMirroredDirName);\n                    if (strlen(outMirroredDirName) == 0) {\n                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");\n                        CLEAN_RETURN(1);\n                    }\n                    continue;\n                }\n#endif\n#ifndef ZSTD_NOTRACE\n                if (longCommandWArg(&argument, \"--trace\")) { char const* traceFile; NEXT_FIELD(traceFile); TRACE_enable(traceFile); continue; }\n#endif\n                if (longCommandWArg(&argument, \"--patch-from\")) { NEXT_FIELD(patchFromDictFileName); continue; }\n                if (longCommandWArg(&argument, \"--long\")) {\n                    unsigned ldmWindowLog = 0;\n                    ldmFlag = 1;\n                    /* Parse optional window log */\n                    if (*argument == '=') {\n                        ++argument;\n                        ldmWindowLog = readU32FromChar(&argument);\n                    } else if (*argument != 0) {\n                        /* Invalid character following --long */\n                        badusage(programName);\n                        CLEAN_RETURN(1);\n                    }\n                    /* Only set windowLog if not already set by --zstd */\n                    if (compressionParams.windowLog == 0)\n                        compressionParams.windowLog = ldmWindowLog;\n                    continue;\n                }\n#ifndef ZSTD_NOCOMPRESS   /* linking ZSTD_minCLevel() requires compression support */\n                if (longCommandWArg(&argument, \"--fast\")) {\n                    /* Parse optional acceleration factor */\n                    if (*argument == '=') {\n                        U32 const maxFast = (U32)-ZSTD_minCLevel();\n                        U32 fastLevel;\n                        ++argument;\n                        fastLevel = readU32FromChar(&argument);\n                        if (fastLevel > maxFast) fastLevel = maxFast;\n                        if (fastLevel) {\n                            dictCLevel = cLevel = -(int)fastLevel;\n                        } else {\n                            badusage(programName);\n                            CLEAN_RETURN(1);\n                        }\n                    } else if (*argument != 0) {\n                        /* Invalid character following --fast */\n                        badusage(programName);\n                        CLEAN_RETURN(1);\n                    } else {\n                        cLevel = -1;  /* default for --fast */\n                    }\n                    continue;\n                }\n#endif\n\n                if (longCommandWArg(&argument, \"--filelist\")) {\n                    const char* listName;\n                    NEXT_FIELD(listName);\n                    UTIL_refFilename(file_of_names, listName);\n                    continue;\n                }\n\n                /* fall-through, will trigger bad_usage() later on */\n            }\n\n            argument++;\n            while (argument[0]!=0) {\n\n#ifndef ZSTD_NOCOMPRESS\n                /* compression Level */\n                if ((*argument>='0') && (*argument<='9')) {\n                    dictCLevel = cLevel = (int)readU32FromChar(&argument);\n                    continue;\n                }\n#endif\n\n                switch(argument[0])\n                {\n                    /* Display help */\n                case 'V': printVersion(); CLEAN_RETURN(0);   /* Version Only */\n                case 'H': usage_advanced(programName); CLEAN_RETURN(0);\n                case 'h': usage(stdout, programName); CLEAN_RETURN(0);\n\n                     /* Compress */\n                case 'z': operation=zom_compress; argument++; break;\n\n                     /* Decoding */\n                case 'd':\n#ifndef ZSTD_NOBENCH\n                        benchParams.mode = BMK_decodeOnly;\n                        if (operation==zom_bench) { argument++; break; }  /* benchmark decode (hidden option) */\n#endif\n                        operation=zom_decompress; argument++; break;\n\n                    /* Force stdout, even if stdout==console */\n                case 'c': forceStdout=1; outFileName=stdoutmark; FIO_setRemoveSrcFile(prefs, 0); argument++; break;\n\n                    /* do not store filename - gzip compatibility - nothing to do */\n                case 'n': argument++; break;\n\n                    /* Use file content as dictionary */\n                case 'D': argument++; NEXT_FIELD(dictFileName); break;\n\n                    /* Overwrite */\n                case 'f': FIO_overwriteMode(prefs); forceStdin=1; forceStdout=1; followLinks=1; allowBlockDevices=1; argument++; break;\n\n                    /* Verbose mode */\n                case 'v': g_displayLevel++; argument++; break;\n\n                    /* Quiet mode */\n                case 'q': g_displayLevel--; argument++; break;\n\n                    /* keep source file (default) */\n                case 'k': FIO_setRemoveSrcFile(prefs, 0); argument++; break;\n\n                    /* Checksum */\n                case 'C': FIO_setChecksumFlag(prefs, 2); argument++; break;\n\n                    /* test compressed file */\n                case 't': operation=zom_test; argument++; break;\n\n                    /* destination file name */\n                case 'o': argument++; NEXT_FIELD(outFileName); break;\n\n                    /* limit memory */\n                case 'M':\n                    argument++;\n                    memLimit = readU32FromChar(&argument);\n                    break;\n                case 'l': operation=zom_list; argument++; break;\n#ifdef UTIL_HAS_CREATEFILELIST\n                    /* recursive */\n                case 'r': recursive=1; argument++; break;\n#endif\n\n#ifndef ZSTD_NOBENCH\n                    /* Benchmark */\n                case 'b':\n                    operation=zom_bench;\n                    argument++;\n                    break;\n\n                    /* range bench (benchmark only) */\n                case 'e':\n                    /* compression Level */\n                    argument++;\n                    cLevelLast = (int)readU32FromChar(&argument);\n                    break;\n\n                    /* Modify Nb Iterations (benchmark only) */\n                case 'i':\n                    argument++;\n                    bench_nbSeconds = readU32FromChar(&argument);\n                    break;\n\n                    /* cut input into blocks (benchmark only) */\n                case 'B':\n                    argument++;\n                    blockSize = readU32FromChar(&argument);\n                    break;\n\n                    /* benchmark files separately (hidden option) */\n                case 'S':\n                    argument++;\n                    separateFiles = 1;\n                    break;\n\n#endif   /* ZSTD_NOBENCH */\n\n                    /* nb of threads (hidden option) */\n                case 'T':\n                    argument++;\n                    nbWorkers = (int)readU32FromChar(&argument);\n                    break;\n\n                    /* Dictionary Selection level */\n                case 's':\n                    argument++;\n                    dictSelect = readU32FromChar(&argument);\n                    break;\n\n                    /* Pause at the end (-p) or set an additional param (-p#) (hidden option) */\n                case 'p': argument++;\n#ifndef ZSTD_NOBENCH\n                    if ((*argument>='0') && (*argument<='9')) {\n                        benchParams.additionalParam = (int)readU32FromChar(&argument);\n                    } else\n#endif\n                        main_pause=1;\n                    break;\n\n                    /* Select compressibility of synthetic sample */\n                case 'P':\n                    argument++;\n                    compressibility = (double)readU32FromChar(&argument) / 100;\n                    break;\n\n                    /* unknown command */\n                default : badusage(programName); CLEAN_RETURN(1);\n                }\n            }\n            continue;\n        }   /* if (argument[0]=='-') */\n\n        /* none of the above : add filename to list */\n        UTIL_refFilename(filenames, argument);\n    }\n\n    /* Welcome message (if verbose) */\n    DISPLAYLEVEL(3, WELCOME_MESSAGE);\n\n#ifdef ZSTD_MULTITHREAD\n    if ((nbWorkers==0) && (!singleThread)) {\n        /* automatically set # workers based on # of reported cpus */\n        if (defaultLogicalCores) {\n            nbWorkers = UTIL_countLogicalCores();\n            DISPLAYLEVEL(3, \"Note: %d logical core(s) detected \\n\", nbWorkers);\n        } else {\n            nbWorkers = UTIL_countPhysicalCores();\n            DISPLAYLEVEL(3, \"Note: %d physical core(s) detected \\n\", nbWorkers);\n        }\n    }\n#else\n    (void)singleThread; (void)nbWorkers; (void)defaultLogicalCores;\n#endif\n\n    g_utilDisplayLevel = g_displayLevel;\n\n#ifdef UTIL_HAS_CREATEFILELIST\n    if (!followLinks) {\n        unsigned u, fileNamesNb;\n        unsigned const nbFilenames = (unsigned)filenames->tableSize;\n        for (u=0, fileNamesNb=0; u<nbFilenames; u++) {\n            if ( UTIL_isLink(filenames->fileNames[u])\n             && !UTIL_isFIFO(filenames->fileNames[u])\n            ) {\n                DISPLAYLEVEL(2, \"Warning : %s is a symbolic link, ignoring \\n\", filenames->fileNames[u]);\n            } else {\n                filenames->fileNames[fileNamesNb++] = filenames->fileNames[u];\n        }   }\n        if (fileNamesNb == 0 && nbFilenames > 0)  /* all names are eliminated */\n            CLEAN_RETURN(1);\n        filenames->tableSize = fileNamesNb;\n    }   /* if (!followLinks) */\n\n    /* read names from a file */\n    if (file_of_names->tableSize) {\n        size_t const nbFileLists = file_of_names->tableSize;\n        size_t flNb;\n        for (flNb=0; flNb < nbFileLists; flNb++) {\n            FileNamesTable* const fnt = UTIL_createFileNamesTable_fromFileName(file_of_names->fileNames[flNb]);\n            if (fnt==NULL) {\n                DISPLAYLEVEL(1, \"zstd: error reading %s \\n\", file_of_names->fileNames[flNb]);\n                CLEAN_RETURN(1);\n            }\n            filenames = UTIL_mergeFileNamesTable(filenames, fnt);\n        }\n    }\n\n    nbInputFileNames = filenames->tableSize; /* saving number of input files */\n\n    if (recursive) {  /* at this stage, filenameTable is a list of paths, which can contain both files and directories */\n        UTIL_expandFNT(&filenames, followLinks);\n    }\n#else\n    (void)followLinks;\n#endif\n\n    if (operation == zom_list) {\n#ifndef ZSTD_NODECOMPRESS\n        int const ret = FIO_listMultipleFiles((unsigned)filenames->tableSize, filenames->fileNames, g_displayLevel);\n        CLEAN_RETURN(ret);\n#else\n        DISPLAY(\"file information is not supported \\n\");\n        CLEAN_RETURN(1);\n#endif\n    }\n\n    /* Check if benchmark is selected */\n    if (operation==zom_bench) {\n#ifndef ZSTD_NOBENCH\n        benchParams.blockSize = blockSize;\n        benchParams.nbWorkers = nbWorkers;\n        benchParams.realTime = (unsigned)setRealTimePrio;\n        benchParams.nbSeconds = bench_nbSeconds;\n        benchParams.ldmFlag = ldmFlag;\n        benchParams.ldmMinMatch = (int)g_ldmMinMatch;\n        benchParams.ldmHashLog = (int)g_ldmHashLog;\n        benchParams.useRowMatchFinder = useRowMatchFinder;\n        if (g_ldmBucketSizeLog != LDM_PARAM_DEFAULT) {\n            benchParams.ldmBucketSizeLog = (int)g_ldmBucketSizeLog;\n        }\n        if (g_ldmHashRateLog != LDM_PARAM_DEFAULT) {\n            benchParams.ldmHashRateLog = (int)g_ldmHashRateLog;\n        }\n        benchParams.literalCompressionMode = literalCompressionMode;\n\n        if (cLevel > ZSTD_maxCLevel()) cLevel = ZSTD_maxCLevel();\n        if (cLevelLast > ZSTD_maxCLevel()) cLevelLast = ZSTD_maxCLevel();\n        if (cLevelLast < cLevel) cLevelLast = cLevel;\n        if (cLevelLast > cLevel)\n            DISPLAYLEVEL(3, \"Benchmarking levels from %d to %d\\n\", cLevel, cLevelLast);\n        if (filenames->tableSize > 0) {\n            if(separateFiles) {\n                unsigned i;\n                for(i = 0; i < filenames->tableSize; i++) {\n                    int c;\n                    DISPLAYLEVEL(3, \"Benchmarking %s \\n\", filenames->fileNames[i]);\n                    for(c = cLevel; c <= cLevelLast; c++) {\n                        BMK_benchFilesAdvanced(&filenames->fileNames[i], 1, dictFileName, c, &compressionParams, g_displayLevel, &benchParams);\n                }   }\n            } else {\n                for(; cLevel <= cLevelLast; cLevel++) {\n                    BMK_benchFilesAdvanced(filenames->fileNames, (unsigned)filenames->tableSize, dictFileName, cLevel, &compressionParams, g_displayLevel, &benchParams);\n            }   }\n        } else {\n            for(; cLevel <= cLevelLast; cLevel++) {\n                BMK_syntheticTest(cLevel, compressibility, &compressionParams, g_displayLevel, &benchParams);\n        }   }\n\n#else\n        (void)bench_nbSeconds; (void)blockSize; (void)setRealTimePrio; (void)separateFiles; (void)compressibility;\n#endif\n        goto _end;\n    }\n\n    /* Check if dictionary builder is selected */\n    if (operation==zom_train) {\n#ifndef ZSTD_NODICT\n        ZDICT_params_t zParams;\n        zParams.compressionLevel = dictCLevel;\n        zParams.notificationLevel = (unsigned)g_displayLevel;\n        zParams.dictID = dictID;\n        if (dict == cover) {\n            int const optimize = !coverParams.k || !coverParams.d;\n            coverParams.nbThreads = (unsigned)nbWorkers;\n            coverParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, NULL, &coverParams, NULL, optimize, memLimit);\n        } else if (dict == fastCover) {\n            int const optimize = !fastCoverParams.k || !fastCoverParams.d;\n            fastCoverParams.nbThreads = (unsigned)nbWorkers;\n            fastCoverParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, NULL, NULL, &fastCoverParams, optimize, memLimit);\n        } else {\n            ZDICT_legacy_params_t dictParams;\n            memset(&dictParams, 0, sizeof(dictParams));\n            dictParams.selectivityLevel = dictSelect;\n            dictParams.zParams = zParams;\n            operationResult = DiB_trainFromFiles(outFileName, maxDictSize, filenames->fileNames, (int)filenames->tableSize, blockSize, &dictParams, NULL, NULL, 0, memLimit);\n        }\n#else\n        (void)dictCLevel; (void)dictSelect; (void)dictID;  (void)maxDictSize; /* not used when ZSTD_NODICT set */\n        DISPLAYLEVEL(1, \"training mode not available \\n\");\n        operationResult = 1;\n#endif\n        goto _end;\n    }\n\n#ifndef ZSTD_NODECOMPRESS\n    if (operation==zom_test) { FIO_setTestMode(prefs, 1); outFileName=nulmark; FIO_setRemoveSrcFile(prefs, 0); }  /* test mode */\n#endif\n\n    /* No input filename ==> use stdin and stdout */\n    if (filenames->tableSize == 0) {\n      /* It is possible that the input\n       was a number of empty directories. In this case\n       stdin and stdout should not be used */\n       if (nbInputFileNames > 0 ){\n        DISPLAYLEVEL(1, \"please provide correct input file(s) or non-empty directories -- ignored \\n\");\n        CLEAN_RETURN(0);\n       }\n       UTIL_refFilename(filenames, stdinmark);\n    }\n\n    if (!strcmp(filenames->fileNames[0], stdinmark) && !outFileName)\n        outFileName = stdoutmark;  /* when input is stdin, default output is stdout */\n\n    /* Check if input/output defined as console; trigger an error in this case */\n    if (!forceStdin\n     && !strcmp(filenames->fileNames[0], stdinmark)\n     && IS_CONSOLE(stdin) ) {\n        DISPLAYLEVEL(1, \"stdin is a console, aborting\\n\");\n        CLEAN_RETURN(1);\n    }\n    if ( outFileName && !strcmp(outFileName, stdoutmark)\n      && IS_CONSOLE(stdout)\n      && !strcmp(filenames->fileNames[0], stdinmark)\n      && !forceStdout\n      && operation!=zom_decompress ) {\n        DISPLAYLEVEL(1, \"stdout is a console, aborting\\n\");\n        CLEAN_RETURN(1);\n    }\n\n#ifndef ZSTD_NOCOMPRESS\n    /* check compression level limits */\n    {   int const maxCLevel = ultra ? ZSTD_maxCLevel() : ZSTDCLI_CLEVEL_MAX;\n        if (cLevel > maxCLevel) {\n            DISPLAYLEVEL(2, \"Warning : compression level higher than max, reduced to %i \\n\", maxCLevel);\n            cLevel = maxCLevel;\n    }   }\n#endif\n\n    if (showDefaultCParams) {\n        if (operation == zom_decompress) {\n            DISPLAY(\"error : can't use --show-default-cparams in decompression mode \\n\");\n            CLEAN_RETURN(1);\n        }\n    }\n\n    if (dictFileName != NULL && patchFromDictFileName != NULL) {\n        DISPLAY(\"error : can't use -D and --patch-from=# at the same time \\n\");\n        CLEAN_RETURN(1);\n    }\n\n    if (patchFromDictFileName != NULL && filenames->tableSize > 1) {\n        DISPLAY(\"error : can't use --patch-from=# on multiple files \\n\");\n        CLEAN_RETURN(1);\n    }\n\n    /* No status message in pipe mode (stdin - stdout) */\n    hasStdout = outFileName && !strcmp(outFileName,stdoutmark);\n\n    if ((hasStdout || !IS_CONSOLE(stderr)) && (g_displayLevel==2)) g_displayLevel=1;\n\n    /* IO Stream/File */\n    FIO_setHasStdoutOutput(fCtx, hasStdout);\n    FIO_setNbFilesTotal(fCtx, (int)filenames->tableSize);\n    FIO_determineHasStdinInput(fCtx, filenames);\n    FIO_setNotificationLevel(g_displayLevel);\n    FIO_setAllowBlockDevices(prefs, allowBlockDevices);\n    FIO_setPatchFromMode(prefs, patchFromDictFileName != NULL);\n    if (memLimit == 0) {\n        if (compressionParams.windowLog == 0) {\n            memLimit = (U32)1 << g_defaultMaxWindowLog;\n        } else {\n            memLimit = (U32)1 << (compressionParams.windowLog & 31);\n    }   }\n    if (patchFromDictFileName != NULL)\n        dictFileName = patchFromDictFileName;\n    FIO_setMemLimit(prefs, memLimit);\n    if (operation==zom_compress) {\n#ifndef ZSTD_NOCOMPRESS\n        FIO_setContentSize(prefs, contentSize);\n        FIO_setNbWorkers(prefs, nbWorkers);\n        FIO_setBlockSize(prefs, (int)blockSize);\n        if (g_overlapLog!=OVERLAP_LOG_DEFAULT) FIO_setOverlapLog(prefs, (int)g_overlapLog);\n        FIO_setLdmFlag(prefs, (unsigned)ldmFlag);\n        FIO_setLdmHashLog(prefs, (int)g_ldmHashLog);\n        FIO_setLdmMinMatch(prefs, (int)g_ldmMinMatch);\n        if (g_ldmBucketSizeLog != LDM_PARAM_DEFAULT) FIO_setLdmBucketSizeLog(prefs, (int)g_ldmBucketSizeLog);\n        if (g_ldmHashRateLog != LDM_PARAM_DEFAULT) FIO_setLdmHashRateLog(prefs, (int)g_ldmHashRateLog);\n        FIO_setAdaptiveMode(prefs, (unsigned)adapt);\n        FIO_setUseRowMatchFinder(prefs, useRowMatchFinder);\n        FIO_setAdaptMin(prefs, adaptMin);\n        FIO_setAdaptMax(prefs, adaptMax);\n        FIO_setRsyncable(prefs, rsyncable);\n        FIO_setStreamSrcSize(prefs, streamSrcSize);\n        FIO_setTargetCBlockSize(prefs, targetCBlockSize);\n        FIO_setSrcSizeHint(prefs, srcSizeHint);\n        FIO_setLiteralCompressionMode(prefs, literalCompressionMode);\n        FIO_setSparseWrite(prefs, 0);\n        if (adaptMin > cLevel) cLevel = adaptMin;\n        if (adaptMax < cLevel) cLevel = adaptMax;\n\n        /* Compare strategies constant with the ground truth */\n        { ZSTD_bounds strategyBounds = ZSTD_cParam_getBounds(ZSTD_c_strategy);\n          assert(ZSTD_NB_STRATEGIES == strategyBounds.upperBound);\n          (void)strategyBounds; }\n\n        if (showDefaultCParams || g_displayLevel >= 4) {\n            size_t fileNb;\n            for (fileNb = 0; fileNb < (size_t)filenames->tableSize; fileNb++) {\n                if (showDefaultCParams)\n                    printDefaultCParams(filenames->fileNames[fileNb], dictFileName, cLevel);\n                if (g_displayLevel >= 4)\n                    printActualCParams(filenames->fileNames[fileNb], dictFileName, cLevel, &compressionParams);\n            }\n        }\n\n        if (g_displayLevel >= 4)\n            FIO_displayCompressionParameters(prefs);\n        if ((filenames->tableSize==1) && outFileName)\n            operationResult = FIO_compressFilename(fCtx, prefs, outFileName, filenames->fileNames[0], dictFileName, cLevel, compressionParams);\n        else\n            operationResult = FIO_compressMultipleFilenames(fCtx, prefs, filenames->fileNames, outMirroredDirName, outDirName, outFileName, suffix, dictFileName, cLevel, compressionParams);\n#else\n        (void)contentSize; (void)suffix; (void)adapt; (void)rsyncable; (void)ultra; (void)cLevel; (void)ldmFlag; (void)literalCompressionMode; (void)targetCBlockSize; (void)streamSrcSize; (void)srcSizeHint; (void)ZSTD_strategyMap; (void)useRowMatchFinder; /* not used when ZSTD_NOCOMPRESS set */\n        DISPLAY(\"Compression not supported \\n\");\n#endif\n    } else {  /* decompression or test */\n#ifndef ZSTD_NODECOMPRESS\n        if (filenames->tableSize == 1 && outFileName) {\n            operationResult = FIO_decompressFilename(fCtx, prefs, outFileName, filenames->fileNames[0], dictFileName);\n        } else {\n            operationResult = FIO_decompressMultipleFilenames(fCtx, prefs, filenames->fileNames, outMirroredDirName, outDirName, outFileName, dictFileName);\n        }\n#else\n        DISPLAY(\"Decompression not supported \\n\");\n#endif\n    }\n\n_end:\n    FIO_freePreferences(prefs);\n    FIO_freeContext(fCtx);\n    if (main_pause) waitEnter();\n    UTIL_freeFileNamesTable(filenames);\n    UTIL_freeFileNamesTable(file_of_names);\n#ifndef ZSTD_NOTRACE\n    TRACE_finish();\n#endif\n\n    return operationResult;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -218,7 +218,14 @@\n                 if (longCommandWArg(&argument, \"--stream-size=\")) { streamSrcSize = readSizeTFromChar(&argument); continue; }\n                 if (longCommandWArg(&argument, \"--target-compressed-block-size=\")) { targetCBlockSize = readSizeTFromChar(&argument); continue; }\n                 if (longCommandWArg(&argument, \"--size-hint=\")) { srcSizeHint = readSizeTFromChar(&argument); continue; }\n-                if (longCommandWArg(&argument, \"--output-dir-flat\")) { NEXT_FIELD(outDirName); continue; }\n+                if (longCommandWArg(&argument, \"--output-dir-flat\")) {\n+                    NEXT_FIELD(outDirName);\n+                    if (strlen(outDirName) == 0) {\n+                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");\n+                        CLEAN_RETURN(1);\n+                    }\n+                    continue;\n+                }\n                 if (longCommandWArg(&argument, \"--auto-threads\")) {\n                     const char* threadDefault = NULL;\n                     NEXT_FIELD(threadDefault);\n@@ -227,7 +234,14 @@\n                     continue;\n                 }\n #ifdef UTIL_HAS_MIRRORFILELIST\n-                if (longCommandWArg(&argument, \"--output-dir-mirror\")) { NEXT_FIELD(outMirroredDirName); continue; }\n+                if (longCommandWArg(&argument, \"--output-dir-mirror\")) {\n+                    NEXT_FIELD(outMirroredDirName);\n+                    if (strlen(outMirroredDirName) == 0) {\n+                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");\n+                        CLEAN_RETURN(1);\n+                    }\n+                    continue;\n+                }\n #endif\n #ifndef ZSTD_NOTRACE\n                 if (longCommandWArg(&argument, \"--trace\")) { char const* traceFile; NEXT_FIELD(traceFile); TRACE_enable(traceFile); continue; }",
        "diff_line_info": {
            "deleted_lines": [
                "                if (longCommandWArg(&argument, \"--output-dir-flat\")) { NEXT_FIELD(outDirName); continue; }",
                "                if (longCommandWArg(&argument, \"--output-dir-mirror\")) { NEXT_FIELD(outMirroredDirName); continue; }"
            ],
            "added_lines": [
                "                if (longCommandWArg(&argument, \"--output-dir-flat\")) {",
                "                    NEXT_FIELD(outDirName);",
                "                    if (strlen(outDirName) == 0) {",
                "                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");",
                "                        CLEAN_RETURN(1);",
                "                    }",
                "                    continue;",
                "                }",
                "                if (longCommandWArg(&argument, \"--output-dir-mirror\")) {",
                "                    NEXT_FIELD(outMirroredDirName);",
                "                    if (strlen(outMirroredDirName) == 0) {",
                "                        DISPLAY(\"error: output dir cannot be empty string (did you mean to pass '.' instead?)\\n\");",
                "                        CLEAN_RETURN(1);",
                "                    }",
                "                    continue;",
                "                }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1992",
        "func_name": "wireshark/dissect_rpcrdma",
        "description": "RPCoRDMA dissector crash in Wireshark 4.0.0 to 4.0.4 and 3.6.0 to 3.6.12 allows denial of service via packet injection or crafted capture file",
        "git_url": "https://gitlab.com/wireshark/wireshark/-/commit/3c8be14c827f1587da3c2b3bb0d9c04faff57413",
        "commit_title": "RPCoRDMA: Frame end cleanup for global write offsets",
        "commit_text": " Add a frame end routine for a global which is assigned to packet scoped memory. It really should be made proto data, but is used in a function in the header (that doesn't take the packet info struct as an argument) and this fix needs to be made in stable branches.  Fix #18852 ",
        "func_before": "static int\ndissect_rpcrdma(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_)\n{\n    tvbuff_t *volatile next_tvb;\n    tvbuff_t *frag_tvb;\n    proto_item *ti;\n    proto_tree *rpcordma_tree;\n    guint offset;\n    guint32 msg_type;\n    guint32 xid;\n    guint32 val;\n    guint32 *p_msgid;\n    guint write_size;\n    int save_visited;\n    rdma_lists_t rdma_lists = { NULL, NULL, NULL };\n\n    /* tvb_get_ntohl() should not throw an exception while checking if\n       this is an rpcrdma packet */\n    if (tvb_captured_length(tvb) < MIN_RPCRDMA_HDR_SZ)\n        return 0;\n\n    if (tvb_get_ntohl(tvb, 4) != 1)  /* vers */\n        return 0;\n\n    xid = tvb_get_ntohl(tvb, 0);\n    msg_type = tvb_get_ntohl(tvb, 12);\n\n    col_set_str(pinfo->cinfo, COL_PROTOCOL, \"RPCoRDMA\");\n    col_add_fstr(pinfo->cinfo, COL_INFO, \"%s XID 0x%x\",\n        val_to_str(msg_type, rpcordma_message_type, \"Unknown (%d)\"), xid);\n\n    ti = proto_tree_add_item(tree, proto_rpcordma, tvb, 0, MIN_RPCRDMA_HDR_SZ, ENC_NA);\n\n    rpcordma_tree = proto_item_add_subtree(ti, ett_rpcordma);\n\n    offset = 0;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_xid, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_vers, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_flow_control, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_message_type, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n\n    switch (msg_type) {\n    case RDMA_MSG:\n        /* Parse rpc_rdma_header */\n        offset = parse_rdma_header(tvb, offset, rpcordma_tree, &rdma_lists);\n\n        proto_item_set_len(ti, offset);\n\n        frag_tvb = get_reassembled_data(tvb, offset, pinfo, tree);\n        if (frag_tvb) {\n            /* Reassembled message has already been cached -- call upper dissector */\n            return call_dissector(rpc_handler, frag_tvb, pinfo, tree);\n        } else if (pinfo->fd->visited && !g_needs_reassembly && rdma_lists.p_read_list) {\n            /* This frame has already been added as a read fragment */\n            return 0;\n        } else {\n            next_tvb = tvb_new_subset_remaining(tvb, offset);\n\n            /*\n             * Get the total number of bytes for the write chunk list.\n             * It returns 0 if there is no write chunk list, or this is an\n             * RPC call (list has just been set up) or it is an RPC reply but\n             * there is an error so the reply message has not been reduced.\n             */\n            write_size = get_rdma_list_size(rdma_lists.p_write_list, pinfo);\n\n            if (write_size > 0 && !pinfo->fd->visited) {\n                /* Initialize array of write chunk offsets */\n                gp_rdma_write_offsets = wmem_array_new(wmem_packet_scope(), sizeof(gint));\n                TRY {\n                    /*\n                     * Call the upper layer dissector to get a list of offsets\n                     * where message has been reduced.\n                     * This is done on the first pass (visited = 0)\n                     */\n                    g_rpcrdma_reduced = TRUE;\n                    call_dissector(rpc_handler, next_tvb, pinfo, tree);\n                }\n                FINALLY {\n                    /* Make sure to disable reduced data processing */\n                    g_rpcrdma_reduced = FALSE;\n                }\n                ENDTRY;\n            } else if (write_size > 0 && pinfo->fd->visited) {\n                /*\n                 * Reassembly is done on the second pass (visited = 1)\n                 * This is done because dissecting the upper layer(s) again\n                 * causes the upper layer(s) to be displayed twice if it is\n                 * done on the same pass.\n                 */\n                p_msgid = (guint32 *)p_get_proto_data(wmem_file_scope(), pinfo, proto_rpcordma, RPCRDMA_MSG_ID);\n                if (p_msgid) {\n                    /*\n                     * All fragments were added during the first pass,\n                     * reassembly just needs to be completed here\n                     */\n                    save_visited = pinfo->fd->visited;\n                    pinfo->fd->visited = 0;\n                    end_reassembly(*p_msgid, NULL, pinfo);\n                    pinfo->fd->visited = save_visited;\n                }\n            }\n\n            /*\n             * If there is a write chunk list, process_rdma_lists will convert\n             * the offsets returned by the upper layer into xdr positions\n             * and break the current reduced message into separate fragments\n             * and insert them into the reassembly table in the first pass.\n             * On the second pass, the reassembly has just been done so\n             * process_rdma_lists should only call process_reassembled_data\n             * to get the reassembled data and call the dissector for the\n             * upper layer with the reassembled message.\n             */\n            frag_tvb = process_rdma_lists(next_tvb, 0, &rdma_lists, pinfo, tree);\n            gp_rdma_write_offsets = NULL;\n            if (rdma_lists.p_read_list) {\n                /*\n                 * If there is a read chunk list, do not dissect upper layer\n                 * just label rest of packet as \"Data\" since the reassembly\n                 * will be done on the last read response.\n                 */\n                call_data_dissector(next_tvb, pinfo, tree);\n                break;\n            } else if (frag_tvb) {\n                /* Replace current frame data with the reassembled data */\n                next_tvb = frag_tvb;\n            }\n        }",
        "func": "static int\ndissect_rpcrdma(tvbuff_t *tvb, packet_info *pinfo, proto_tree *tree, void* data _U_)\n{\n    tvbuff_t *volatile next_tvb;\n    tvbuff_t *frag_tvb;\n    proto_item *ti;\n    proto_tree *rpcordma_tree;\n    guint offset;\n    guint32 msg_type;\n    guint32 xid;\n    guint32 val;\n    guint32 *p_msgid;\n    guint write_size;\n    int save_visited;\n    rdma_lists_t rdma_lists = { NULL, NULL, NULL };\n\n    /* tvb_get_ntohl() should not throw an exception while checking if\n       this is an rpcrdma packet */\n    if (tvb_captured_length(tvb) < MIN_RPCRDMA_HDR_SZ)\n        return 0;\n\n    if (tvb_get_ntohl(tvb, 4) != 1)  /* vers */\n        return 0;\n\n    xid = tvb_get_ntohl(tvb, 0);\n    msg_type = tvb_get_ntohl(tvb, 12);\n\n    col_set_str(pinfo->cinfo, COL_PROTOCOL, \"RPCoRDMA\");\n    col_add_fstr(pinfo->cinfo, COL_INFO, \"%s XID 0x%x\",\n        val_to_str(msg_type, rpcordma_message_type, \"Unknown (%d)\"), xid);\n\n    ti = proto_tree_add_item(tree, proto_rpcordma, tvb, 0, MIN_RPCRDMA_HDR_SZ, ENC_NA);\n\n    rpcordma_tree = proto_item_add_subtree(ti, ett_rpcordma);\n\n    offset = 0;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_xid, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_vers, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_flow_control, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n    proto_tree_add_item(rpcordma_tree, hf_rpcordma_message_type, tvb,\n                offset, 4, ENC_BIG_ENDIAN);\n    offset += 4;\n\n    switch (msg_type) {\n    case RDMA_MSG:\n        /* Parse rpc_rdma_header */\n        offset = parse_rdma_header(tvb, offset, rpcordma_tree, &rdma_lists);\n\n        proto_item_set_len(ti, offset);\n\n        frag_tvb = get_reassembled_data(tvb, offset, pinfo, tree);\n        if (frag_tvb) {\n            /* Reassembled message has already been cached -- call upper dissector */\n            return call_dissector(rpc_handler, frag_tvb, pinfo, tree);\n        } else if (pinfo->fd->visited && !g_needs_reassembly && rdma_lists.p_read_list) {\n            /* This frame has already been added as a read fragment */\n            return 0;\n        } else {\n            next_tvb = tvb_new_subset_remaining(tvb, offset);\n\n            /*\n             * Get the total number of bytes for the write chunk list.\n             * It returns 0 if there is no write chunk list, or this is an\n             * RPC call (list has just been set up) or it is an RPC reply but\n             * there is an error so the reply message has not been reduced.\n             */\n            write_size = get_rdma_list_size(rdma_lists.p_write_list, pinfo);\n\n            if (write_size > 0 && !pinfo->fd->visited) {\n                /* Initialize array of write chunk offsets */\n                gp_rdma_write_offsets = wmem_array_new(wmem_packet_scope(), sizeof(gint));\n                register_frame_end_routine(pinfo, reset_write_offsets);\n                TRY {\n                    /*\n                     * Call the upper layer dissector to get a list of offsets\n                     * where message has been reduced.\n                     * This is done on the first pass (visited = 0)\n                     */\n                    g_rpcrdma_reduced = TRUE;\n                    call_dissector(rpc_handler, next_tvb, pinfo, tree);\n                }\n                FINALLY {\n                    /* Make sure to disable reduced data processing */\n                    g_rpcrdma_reduced = FALSE;\n                }\n                ENDTRY;\n            } else if (write_size > 0 && pinfo->fd->visited) {\n                /*\n                 * Reassembly is done on the second pass (visited = 1)\n                 * This is done because dissecting the upper layer(s) again\n                 * causes the upper layer(s) to be displayed twice if it is\n                 * done on the same pass.\n                 */\n                p_msgid = (guint32 *)p_get_proto_data(wmem_file_scope(), pinfo, proto_rpcordma, RPCRDMA_MSG_ID);\n                if (p_msgid) {\n                    /*\n                     * All fragments were added during the first pass,\n                     * reassembly just needs to be completed here\n                     */\n                    save_visited = pinfo->fd->visited;\n                    pinfo->fd->visited = 0;\n                    end_reassembly(*p_msgid, NULL, pinfo);\n                    pinfo->fd->visited = save_visited;\n                }\n            }\n\n            /*\n             * If there is a write chunk list, process_rdma_lists will convert\n             * the offsets returned by the upper layer into xdr positions\n             * and break the current reduced message into separate fragments\n             * and insert them into the reassembly table in the first pass.\n             * On the second pass, the reassembly has just been done so\n             * process_rdma_lists should only call process_reassembled_data\n             * to get the reassembled data and call the dissector for the\n             * upper layer with the reassembled message.\n             */\n            frag_tvb = process_rdma_lists(next_tvb, 0, &rdma_lists, pinfo, tree);\n            gp_rdma_write_offsets = NULL;\n            if (rdma_lists.p_read_list) {\n                /*\n                 * If there is a read chunk list, do not dissect upper layer\n                 * just label rest of packet as \"Data\" since the reassembly\n                 * will be done on the last read response.\n                 */\n                call_data_dissector(next_tvb, pinfo, tree);\n                break;\n            } else if (frag_tvb) {\n                /* Replace current frame data with the reassembled data */\n                next_tvb = frag_tvb;\n            }\n        }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -75,6 +75,7 @@\n             if (write_size > 0 && !pinfo->fd->visited) {\n                 /* Initialize array of write chunk offsets */\n                 gp_rdma_write_offsets = wmem_array_new(wmem_packet_scope(), sizeof(gint));\n+                register_frame_end_routine(pinfo, reset_write_offsets);\n                 TRY {\n                     /*\n                      * Call the upper layer dissector to get a list of offsets",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                register_frame_end_routine(pinfo, reset_write_offsets);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1994",
        "func_name": "wireshark/dissect_gquic_frame_type",
        "description": "GQUIC dissector crash in Wireshark 4.0.0 to 4.0.4 and 3.6.0 to 3.6.12 allows denial of service via packet injection or crafted capture file",
        "git_url": "https://gitlab.com/wireshark/wireshark/-/commit/ee314ace8ae2d2fa8c6f7280231010252054fd7b",
        "commit_title": "GQUIC: Fix a null pointer exception",
        "commit_text": " Ensure that dissect_gquic_frame_type has a valid info pointer.  Fixes #18947. ",
        "func_before": "int\ndissect_gquic_frame_type(tvbuff_t *tvb, packet_info *pinfo, proto_tree *gquic_tree, guint offset, guint8 len_pkn, gquic_info_data_t *gquic_info){\n    proto_item *ti, *ti_ft, *ti_ftflags /*, *expert_ti*/;\n    proto_tree *ft_tree, *ftflags_tree;\n    guint8 frame_type;\n    guint8 num_ranges, num_revived, num_blocks = 0, num_timestamp;\n    guint32 len_stream = 0, len_offset = 0, len_data = 0, len_largest_observed = 1, len_missing_packet = 1;\n\n    ti_ft = proto_tree_add_item(gquic_tree, hf_gquic_frame, tvb, offset, 1, ENC_NA);\n    ft_tree = proto_item_add_subtree(ti_ft, ett_gquic_ft);\n\n    /* Frame type */\n    ti_ftflags = proto_tree_add_item(ft_tree, hf_gquic_frame_type, tvb, offset, 1, ENC_NA);\n    frame_type = tvb_get_guint8(tvb, offset);\n    proto_item_set_text(ti_ft, \"%s\", rval_to_str(frame_type, frame_type_vals, \"Unknown\"));\n\n    if((frame_type & FTFLAGS_SPECIAL) == 0 && frame_type != FT_CRYPTO){ /* Regular Stream Flags */\n        offset += 1;\n        switch(frame_type){\n            case FT_PADDING:{\n                proto_item *ti_pad_len;\n                guint32 pad_len = tvb_reported_length_remaining(tvb, offset);\n\n                ti_pad_len = proto_tree_add_uint(ft_tree, hf_gquic_frame_type_padding_length, tvb, offset, 0, pad_len);\n                proto_item_set_generated(ti_pad_len);\n                proto_item_append_text(ti_ft, \" Length: %u\", pad_len);\n                if(pad_len > 0) /* Avoid Malformed Exception with pad_len == 0 */\n\t\t    proto_tree_add_item(ft_tree, hf_gquic_frame_type_padding, tvb, offset, -1, ENC_NA);\n                offset += pad_len;\n                }\n            break;\n            case FT_RST_STREAM:{\n                guint32 stream_id, error_code;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_rsts_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_rsts_byte_offset, tvb, offset, 8, gquic_info->encoding);\n                offset += 8;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_rsts_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_item_append_text(ti_ft, \" Stream ID: %u, Error code: %s\", stream_id, val_to_str_ext(error_code, &rststream_error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"RST STREAM\");\n                }\n            break;\n            case FT_CONNECTION_CLOSE:{\n                guint16 len_reason;\n                guint32 error_code;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_cc_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_cc_reason_phrase_length, tvb, offset, 2, gquic_info->encoding);\n                len_reason = tvb_get_guint16(tvb, offset, gquic_info->encoding);\n                offset += 2;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_cc_reason_phrase, tvb, offset, len_reason, ENC_ASCII);\n                offset += len_reason;\n                proto_item_append_text(ti_ft, \" Error code: %s\", val_to_str_ext(error_code, &error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"Connection Close\");\n                }\n            break;\n            case FT_GOAWAY:{\n                guint16 len_reason;\n                guint32 error_code, last_good_stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_goaway_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_goaway_last_good_stream_id, tvb, offset, 4, gquic_info->encoding, &last_good_stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_goaway_reason_phrase_length, tvb, offset, 2, gquic_info->encoding);\n                len_reason = tvb_get_guint16(tvb, offset, gquic_info->encoding);\n                offset += 2;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_goaway_reason_phrase, tvb, offset, len_reason, ENC_ASCII);\n                offset += len_reason;\n                proto_item_append_text(ti_ft, \" Stream ID: %u, Error code: %s\", last_good_stream_id, val_to_str_ext(error_code, &error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"GOAWAY\");\n                }\n            break;\n            case FT_WINDOW_UPDATE:{\n                guint32 stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_wu_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_wu_byte_offset, tvb, offset, 8, gquic_info->encoding);\n                offset += 8;\n                proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n                }\n            break;\n            case FT_BLOCKED:{\n                guint32 stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_blocked_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n                }\n            break;\n            case FT_STOP_WAITING:{\n                guint8 send_entropy;\n                if(gquic_info->version_valid && gquic_info->version < 34){ /* No longer Entropy after Q034 */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_sw_send_entropy, tvb, offset, 1, ENC_NA);\n                    send_entropy = tvb_get_guint8(tvb, offset);\n                    proto_item_append_text(ti_ft, \" Send Entropy: %u\", send_entropy);\n                    offset += 1;\n                }\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_sw_least_unacked_delta, tvb, offset, len_pkn, gquic_info->encoding);\n                offset += len_pkn;\n\n                }\n            break;\n            case FT_PING: /* No Payload */\n            default: /* No default */\n            break;\n        }\n    }\n    else { /* Special Frame Types */\n        guint32 stream_id, message_tag;\n        const guint8* message_tag_str;\n        proto_item *ti_stream;\n\n        ftflags_tree = proto_item_add_subtree(ti_ftflags, ett_gquic_ftflags);\n        proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream , tvb, offset, 1, ENC_NA);\n\n        if(frame_type == FT_CRYPTO) {\n            guint64 crypto_offset, crypto_length;\n            gint32 lenvar;\n\n            DISSECTOR_ASSERT(gquic_info->version_valid && gquic_info->version >= 50);\n\n            col_append_fstr(pinfo->cinfo, COL_INFO, \", CRYPTO\");\n            offset += 1;\n            proto_tree_add_item_ret_varint(ft_tree, hf_gquic_crypto_offset, tvb, offset, -1, ENC_VARINT_QUIC, &crypto_offset, &lenvar);\n            offset += lenvar;\n            proto_tree_add_item_ret_varint(ft_tree, hf_gquic_crypto_length, tvb, offset, -1, ENC_VARINT_QUIC, &crypto_length, &lenvar);\n            offset += lenvar;\n            proto_tree_add_item(ft_tree, hf_gquic_crypto_crypto_data, tvb, offset, (guint32)crypto_length, ENC_NA);\n\n            if (gquic_info->version == 50) {\n\t        message_tag = tvb_get_ntohl(tvb, offset);\n                ti = proto_tree_add_item_ret_string(ft_tree, hf_gquic_tag, tvb, offset, 4, ENC_ASCII|ENC_NA, pinfo->pool, &message_tag_str);\n                proto_item_append_text(ti, \" (%s)\", val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                col_add_fstr(pinfo->cinfo, COL_INFO, \"%s\", val_to_str(message_tag, message_tag_vals, \"Unknown\"));\n                offset += 4;\n\n                offset = dissect_gquic_tags(tvb, pinfo, ft_tree, offset);\n\t    } else { /* T050 and T051 */\n                tvbuff_t *next_tvb = tvb_new_subset_length(tvb, offset, (int)crypto_length);\n                col_set_writable(pinfo->cinfo, -1, FALSE);\n                call_dissector_with_data(tls13_handshake_handle, next_tvb, pinfo, ft_tree, GUINT_TO_POINTER(crypto_offset));\n                col_set_writable(pinfo->cinfo, -1, TRUE);\n                offset += (guint32)crypto_length;\n\t    }\n\n\t} else if(frame_type & FTFLAGS_STREAM){ /* Stream Flags */\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_f, tvb, offset, 1, ENC_NA);\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_d, tvb, offset, 1, ENC_NA);\n            if(frame_type & FTFLAGS_STREAM_D){\n                len_data = 2;\n            }\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_ooo, tvb, offset, 1, ENC_NA);\n\n            len_offset = get_len_offset(frame_type);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_ss, tvb, offset, 1, ENC_NA);\n            len_stream = get_len_stream(frame_type);\n            offset += 1;\n\n            ti_stream = proto_tree_add_item_ret_uint(ft_tree, hf_gquic_stream_id, tvb, offset, len_stream, gquic_info->encoding, &stream_id);\n            offset += len_stream;\n\n            proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n\n            if(len_offset) {\n                proto_tree_add_item(ft_tree, hf_gquic_offset, tvb, offset, len_offset, gquic_info->encoding);\n                offset += len_offset;\n            }\n\n            if(len_data) {\n                proto_tree_add_item(ft_tree, hf_gquic_data_len, tvb, offset, len_data, gquic_info->encoding);\n                offset += len_data;\n            }\n\n            /* Check if there is some reserved streams (Chapiter 6.1 of draft-shade-gquic-http2-mapping-00) */\n\n            switch(stream_id) {\n                case 1: { /* Reserved (G)QUIC (handshake, crypto, config updates...) */\n                    message_tag = tvb_get_ntohl(tvb, offset);\n                    ti = proto_tree_add_item_ret_string(ft_tree, hf_gquic_tag, tvb, offset, 4, ENC_ASCII|ENC_NA, pinfo->pool, &message_tag_str);\n\n                    proto_item_append_text(ti_stream, \" (Reserved for (G)QUIC handshake, crypto, config updates...)\");\n                    proto_item_append_text(ti, \" (%s)\", val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                    proto_item_append_text(ti_ft, \", Type: %s (%s)\", message_tag_str, val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                    col_add_fstr(pinfo->cinfo, COL_INFO, \"%s\", val_to_str(message_tag, message_tag_vals, \"Unknown\"));\n                    offset += 4;\n\n                    offset = dissect_gquic_tags(tvb, pinfo, ft_tree, offset);\n                break;\n                }\n                case 3: { /* Reserved H2 HEADERS (or PUSH_PROMISE..) */\n                    tvbuff_t* tvb_h2;\n\n                    proto_item_append_text(ti_stream, \" (Reserved for H2 HEADERS)\");\n\n                    col_add_str(pinfo->cinfo, COL_INFO, \"H2\");\n\n                    tvb_h2 = tvb_new_subset_remaining(tvb, offset);\n\n                    offset += dissect_http2_pdu(tvb_h2, pinfo, ft_tree, NULL);\n                }\n                break;\n                default: { /* Data... */\n                    int data_len = tvb_reported_length_remaining(tvb, offset);\n\n                    col_add_str(pinfo->cinfo, COL_INFO, \"DATA\");\n\n                    proto_tree_add_item(ft_tree, hf_gquic_stream_data, tvb, offset, data_len, ENC_NA);\n                    offset += data_len;\n                }\n                break;\n            }\n        } else if (frame_type & FTFLAGS_ACK) {     /* ACK Flags */\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack, tvb, offset, 1, ENC_NA);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_n, tvb, offset, 1, ENC_NA);\n\n            if(gquic_info->version_valid && gquic_info->version < 34){ /* No longer NACK after Q034 */\n                proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_t, tvb, offset, 1, ENC_NA);\n            } else {\n                proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_u, tvb, offset, 1, ENC_NA);\n            }\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_ll, tvb, offset, 1, ENC_NA);\n\n            len_largest_observed = get_len_largest_observed(frame_type);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_mm, tvb, offset, 1, ENC_NA);\n            len_missing_packet = get_len_missing_packet(frame_type);\n            offset += 1;\n\n            if(gquic_info->version_valid && gquic_info->version < 34){ /* Big change after Q034 */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_received_entropy, tvb, offset, 1, ENC_NA);\n                offset += 1;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_observed, tvb, offset, len_largest_observed, gquic_info->encoding);\n                offset += len_largest_observed;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_ack_delay_time, tvb, offset, 2, gquic_info->encoding);\n                offset += 2;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_timestamp, tvb, offset, 1, ENC_NA);\n                num_timestamp = tvb_get_guint8(tvb, offset);\n                offset += 1;\n\n                if(num_timestamp){\n\n                    /* Delta Largest Observed */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_observed, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* First Timestamp */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_first_timestamp, tvb, offset, 4, gquic_info->encoding);\n                    offset += 4;\n\n                    num_timestamp -= 1;\n                    /* Num Timestamp (-1) x (Delta Largest Observed + Time Since Previous Timestamp) */\n                    while(num_timestamp){\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_observed, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_previous_timestamp, tvb, offset, 2, gquic_info->encoding);\n                        offset += 2;\n\n                        num_timestamp--;\n                    }\n                }\n\n                if(frame_type & FTFLAGS_ACK_N){\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_ranges, tvb, offset, 1, ENC_NA);\n                    num_ranges = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                    while(num_ranges){\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_missing_packet, tvb, offset, len_missing_packet, gquic_info->encoding);\n                        offset += len_missing_packet;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_range_length, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n                        num_ranges--;\n                    }\n\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_revived, tvb, offset, 1, ENC_NA);\n                    num_revived = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                    while(num_revived){\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_revived_packet, tvb, offset, len_largest_observed, gquic_info->encoding);\n                        offset += len_largest_observed;\n                        num_revived--;\n\n                    }\n\n                }\n\n            } else {\n\n                /* Largest Acked */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_acked, tvb, offset, len_largest_observed, gquic_info->encoding);\n                offset += len_largest_observed;\n\n                /* Largest Acked Delta Time*/\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_acked_delta_time, tvb, offset, 2, gquic_info->encoding);\n                offset += 2;\n\n                /* Ack Block */\n                if(frame_type & FTFLAGS_ACK_N){\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_blocks, tvb, offset, 1, ENC_NA);\n                    num_blocks = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                }\n\n                /* First Ack Block Length */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_first_ack_block_length, tvb, offset, len_missing_packet, gquic_info->encoding);\n                offset += len_missing_packet;\n\n                while(num_blocks){\n                    /* Gap to next block */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_gap_to_next_block, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* Ack Block Length */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_ack_block_length, tvb, offset, len_missing_packet, gquic_info->encoding);\n                    offset += len_missing_packet;\n\n                    num_blocks--;\n                }\n\n                /* Timestamp */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_timestamp, tvb, offset, 1, ENC_NA);\n                num_timestamp = tvb_get_guint8(tvb, offset);\n                offset += 1;\n\n                if(num_timestamp){\n\n                    /* Delta Largest Acked */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_acked, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* Time Since Largest Acked */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_largest_acked, tvb, offset, 4, gquic_info->encoding);\n                    offset += 4;\n\n                    num_timestamp -= 1;\n                    /* Num Timestamp x (Delta Largest Acked + Time Since Previous Timestamp) */\n                    while(num_timestamp){\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_acked, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_previous_timestamp, tvb, offset, 2, gquic_info->encoding);\n                        offset += 2;\n\n                        num_timestamp--;\n                    }\n                }\n\n            }\n\n        } else { /* Other ...*/\n            offset += 1;\n        }\n    }\n    return offset;\n\n}",
        "func": "int\ndissect_gquic_frame_type(tvbuff_t *tvb, packet_info *pinfo, proto_tree *gquic_tree, guint offset, guint8 len_pkn, gquic_info_data_t *gquic_info){\n    if (!gquic_info) {\n        expert_add_info(pinfo, gquic_tree, &ei_gquic_data_invalid);\n        return offset + tvb_reported_length_remaining(tvb, offset);\n    }\n\n    proto_item *ti, *ti_ft, *ti_ftflags /*, *expert_ti*/;\n    proto_tree *ft_tree, *ftflags_tree;\n    guint8 frame_type;\n    guint8 num_ranges, num_revived, num_blocks = 0, num_timestamp;\n    guint32 len_stream = 0, len_offset = 0, len_data = 0, len_largest_observed = 1, len_missing_packet = 1;\n\n    ti_ft = proto_tree_add_item(gquic_tree, hf_gquic_frame, tvb, offset, 1, ENC_NA);\n    ft_tree = proto_item_add_subtree(ti_ft, ett_gquic_ft);\n\n    /* Frame type */\n    ti_ftflags = proto_tree_add_item(ft_tree, hf_gquic_frame_type, tvb, offset, 1, ENC_NA);\n    frame_type = tvb_get_guint8(tvb, offset);\n    proto_item_set_text(ti_ft, \"%s\", rval_to_str(frame_type, frame_type_vals, \"Unknown\"));\n\n    if((frame_type & FTFLAGS_SPECIAL) == 0 && frame_type != FT_CRYPTO){ /* Regular Stream Flags */\n        offset += 1;\n        switch(frame_type){\n            case FT_PADDING:{\n                proto_item *ti_pad_len;\n                guint32 pad_len = tvb_reported_length_remaining(tvb, offset);\n\n                ti_pad_len = proto_tree_add_uint(ft_tree, hf_gquic_frame_type_padding_length, tvb, offset, 0, pad_len);\n                proto_item_set_generated(ti_pad_len);\n                proto_item_append_text(ti_ft, \" Length: %u\", pad_len);\n                if(pad_len > 0) /* Avoid Malformed Exception with pad_len == 0 */\n\t\t    proto_tree_add_item(ft_tree, hf_gquic_frame_type_padding, tvb, offset, -1, ENC_NA);\n                offset += pad_len;\n                }\n            break;\n            case FT_RST_STREAM:{\n                guint32 stream_id, error_code;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_rsts_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_rsts_byte_offset, tvb, offset, 8, gquic_info->encoding);\n                offset += 8;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_rsts_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_item_append_text(ti_ft, \" Stream ID: %u, Error code: %s\", stream_id, val_to_str_ext(error_code, &rststream_error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"RST STREAM\");\n                }\n            break;\n            case FT_CONNECTION_CLOSE:{\n                guint16 len_reason;\n                guint32 error_code;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_cc_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_cc_reason_phrase_length, tvb, offset, 2, gquic_info->encoding);\n                len_reason = tvb_get_guint16(tvb, offset, gquic_info->encoding);\n                offset += 2;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_cc_reason_phrase, tvb, offset, len_reason, ENC_ASCII);\n                offset += len_reason;\n                proto_item_append_text(ti_ft, \" Error code: %s\", val_to_str_ext(error_code, &error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"Connection Close\");\n                }\n            break;\n            case FT_GOAWAY:{\n                guint16 len_reason;\n                guint32 error_code, last_good_stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_goaway_error_code, tvb, offset, 4, gquic_info->encoding, &error_code);\n                offset += 4;\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_goaway_last_good_stream_id, tvb, offset, 4, gquic_info->encoding, &last_good_stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_goaway_reason_phrase_length, tvb, offset, 2, gquic_info->encoding);\n                len_reason = tvb_get_guint16(tvb, offset, gquic_info->encoding);\n                offset += 2;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_goaway_reason_phrase, tvb, offset, len_reason, ENC_ASCII);\n                offset += len_reason;\n                proto_item_append_text(ti_ft, \" Stream ID: %u, Error code: %s\", last_good_stream_id, val_to_str_ext(error_code, &error_code_vals_ext, \"Unknown (%d)\"));\n                col_set_str(pinfo->cinfo, COL_INFO, \"GOAWAY\");\n                }\n            break;\n            case FT_WINDOW_UPDATE:{\n                guint32 stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_wu_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_wu_byte_offset, tvb, offset, 8, gquic_info->encoding);\n                offset += 8;\n                proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n                }\n            break;\n            case FT_BLOCKED:{\n                guint32 stream_id;\n\n                proto_tree_add_item_ret_uint(ft_tree, hf_gquic_frame_type_blocked_stream_id, tvb, offset, 4, gquic_info->encoding, &stream_id);\n                offset += 4;\n                proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n                }\n            break;\n            case FT_STOP_WAITING:{\n                guint8 send_entropy;\n                if(gquic_info->version_valid && gquic_info->version < 34){ /* No longer Entropy after Q034 */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_sw_send_entropy, tvb, offset, 1, ENC_NA);\n                    send_entropy = tvb_get_guint8(tvb, offset);\n                    proto_item_append_text(ti_ft, \" Send Entropy: %u\", send_entropy);\n                    offset += 1;\n                }\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_sw_least_unacked_delta, tvb, offset, len_pkn, gquic_info->encoding);\n                offset += len_pkn;\n\n                }\n            break;\n            case FT_PING: /* No Payload */\n            default: /* No default */\n            break;\n        }\n    }\n    else { /* Special Frame Types */\n        guint32 stream_id, message_tag;\n        const guint8* message_tag_str;\n        proto_item *ti_stream;\n\n        ftflags_tree = proto_item_add_subtree(ti_ftflags, ett_gquic_ftflags);\n        proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream , tvb, offset, 1, ENC_NA);\n\n        if(frame_type == FT_CRYPTO) {\n            guint64 crypto_offset, crypto_length;\n            gint32 lenvar;\n\n            DISSECTOR_ASSERT(gquic_info->version_valid && gquic_info->version >= 50);\n\n            col_append_fstr(pinfo->cinfo, COL_INFO, \", CRYPTO\");\n            offset += 1;\n            proto_tree_add_item_ret_varint(ft_tree, hf_gquic_crypto_offset, tvb, offset, -1, ENC_VARINT_QUIC, &crypto_offset, &lenvar);\n            offset += lenvar;\n            proto_tree_add_item_ret_varint(ft_tree, hf_gquic_crypto_length, tvb, offset, -1, ENC_VARINT_QUIC, &crypto_length, &lenvar);\n            offset += lenvar;\n            proto_tree_add_item(ft_tree, hf_gquic_crypto_crypto_data, tvb, offset, (guint32)crypto_length, ENC_NA);\n\n            if (gquic_info->version == 50) {\n\t        message_tag = tvb_get_ntohl(tvb, offset);\n                ti = proto_tree_add_item_ret_string(ft_tree, hf_gquic_tag, tvb, offset, 4, ENC_ASCII|ENC_NA, pinfo->pool, &message_tag_str);\n                proto_item_append_text(ti, \" (%s)\", val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                col_add_fstr(pinfo->cinfo, COL_INFO, \"%s\", val_to_str(message_tag, message_tag_vals, \"Unknown\"));\n                offset += 4;\n\n                offset = dissect_gquic_tags(tvb, pinfo, ft_tree, offset);\n\t    } else { /* T050 and T051 */\n                tvbuff_t *next_tvb = tvb_new_subset_length(tvb, offset, (int)crypto_length);\n                col_set_writable(pinfo->cinfo, -1, FALSE);\n                call_dissector_with_data(tls13_handshake_handle, next_tvb, pinfo, ft_tree, GUINT_TO_POINTER(crypto_offset));\n                col_set_writable(pinfo->cinfo, -1, TRUE);\n                offset += (guint32)crypto_length;\n\t    }\n\n\t} else if(frame_type & FTFLAGS_STREAM){ /* Stream Flags */\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_f, tvb, offset, 1, ENC_NA);\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_d, tvb, offset, 1, ENC_NA);\n            if(frame_type & FTFLAGS_STREAM_D){\n                len_data = 2;\n            }\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_ooo, tvb, offset, 1, ENC_NA);\n\n            len_offset = get_len_offset(frame_type);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_stream_ss, tvb, offset, 1, ENC_NA);\n            len_stream = get_len_stream(frame_type);\n            offset += 1;\n\n            ti_stream = proto_tree_add_item_ret_uint(ft_tree, hf_gquic_stream_id, tvb, offset, len_stream, gquic_info->encoding, &stream_id);\n            offset += len_stream;\n\n            proto_item_append_text(ti_ft, \" Stream ID: %u\", stream_id);\n\n            if(len_offset) {\n                proto_tree_add_item(ft_tree, hf_gquic_offset, tvb, offset, len_offset, gquic_info->encoding);\n                offset += len_offset;\n            }\n\n            if(len_data) {\n                proto_tree_add_item(ft_tree, hf_gquic_data_len, tvb, offset, len_data, gquic_info->encoding);\n                offset += len_data;\n            }\n\n            /* Check if there is some reserved streams (Chapiter 6.1 of draft-shade-gquic-http2-mapping-00) */\n\n            switch(stream_id) {\n                case 1: { /* Reserved (G)QUIC (handshake, crypto, config updates...) */\n                    message_tag = tvb_get_ntohl(tvb, offset);\n                    ti = proto_tree_add_item_ret_string(ft_tree, hf_gquic_tag, tvb, offset, 4, ENC_ASCII|ENC_NA, pinfo->pool, &message_tag_str);\n\n                    proto_item_append_text(ti_stream, \" (Reserved for (G)QUIC handshake, crypto, config updates...)\");\n                    proto_item_append_text(ti, \" (%s)\", val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                    proto_item_append_text(ti_ft, \", Type: %s (%s)\", message_tag_str, val_to_str(message_tag, message_tag_vals, \"Unknown Tag\"));\n                    col_add_fstr(pinfo->cinfo, COL_INFO, \"%s\", val_to_str(message_tag, message_tag_vals, \"Unknown\"));\n                    offset += 4;\n\n                    offset = dissect_gquic_tags(tvb, pinfo, ft_tree, offset);\n                break;\n                }\n                case 3: { /* Reserved H2 HEADERS (or PUSH_PROMISE..) */\n                    tvbuff_t* tvb_h2;\n\n                    proto_item_append_text(ti_stream, \" (Reserved for H2 HEADERS)\");\n\n                    col_add_str(pinfo->cinfo, COL_INFO, \"H2\");\n\n                    tvb_h2 = tvb_new_subset_remaining(tvb, offset);\n\n                    offset += dissect_http2_pdu(tvb_h2, pinfo, ft_tree, NULL);\n                }\n                break;\n                default: { /* Data... */\n                    int data_len = tvb_reported_length_remaining(tvb, offset);\n\n                    col_add_str(pinfo->cinfo, COL_INFO, \"DATA\");\n\n                    proto_tree_add_item(ft_tree, hf_gquic_stream_data, tvb, offset, data_len, ENC_NA);\n                    offset += data_len;\n                }\n                break;\n            }\n        } else if (frame_type & FTFLAGS_ACK) {     /* ACK Flags */\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack, tvb, offset, 1, ENC_NA);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_n, tvb, offset, 1, ENC_NA);\n\n            if(gquic_info->version_valid && gquic_info->version < 34){ /* No longer NACK after Q034 */\n                proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_t, tvb, offset, 1, ENC_NA);\n            } else {\n                proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_u, tvb, offset, 1, ENC_NA);\n            }\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_ll, tvb, offset, 1, ENC_NA);\n\n            len_largest_observed = get_len_largest_observed(frame_type);\n\n            proto_tree_add_item(ftflags_tree, hf_gquic_frame_type_ack_mm, tvb, offset, 1, ENC_NA);\n            len_missing_packet = get_len_missing_packet(frame_type);\n            offset += 1;\n\n            if(gquic_info->version_valid && gquic_info->version < 34){ /* Big change after Q034 */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_received_entropy, tvb, offset, 1, ENC_NA);\n                offset += 1;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_observed, tvb, offset, len_largest_observed, gquic_info->encoding);\n                offset += len_largest_observed;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_ack_delay_time, tvb, offset, 2, gquic_info->encoding);\n                offset += 2;\n\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_timestamp, tvb, offset, 1, ENC_NA);\n                num_timestamp = tvb_get_guint8(tvb, offset);\n                offset += 1;\n\n                if(num_timestamp){\n\n                    /* Delta Largest Observed */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_observed, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* First Timestamp */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_first_timestamp, tvb, offset, 4, gquic_info->encoding);\n                    offset += 4;\n\n                    num_timestamp -= 1;\n                    /* Num Timestamp (-1) x (Delta Largest Observed + Time Since Previous Timestamp) */\n                    while(num_timestamp){\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_observed, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_previous_timestamp, tvb, offset, 2, gquic_info->encoding);\n                        offset += 2;\n\n                        num_timestamp--;\n                    }\n                }\n\n                if(frame_type & FTFLAGS_ACK_N){\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_ranges, tvb, offset, 1, ENC_NA);\n                    num_ranges = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                    while(num_ranges){\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_missing_packet, tvb, offset, len_missing_packet, gquic_info->encoding);\n                        offset += len_missing_packet;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_range_length, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n                        num_ranges--;\n                    }\n\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_revived, tvb, offset, 1, ENC_NA);\n                    num_revived = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                    while(num_revived){\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_revived_packet, tvb, offset, len_largest_observed, gquic_info->encoding);\n                        offset += len_largest_observed;\n                        num_revived--;\n\n                    }\n\n                }\n\n            } else {\n\n                /* Largest Acked */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_acked, tvb, offset, len_largest_observed, gquic_info->encoding);\n                offset += len_largest_observed;\n\n                /* Largest Acked Delta Time*/\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_largest_acked_delta_time, tvb, offset, 2, gquic_info->encoding);\n                offset += 2;\n\n                /* Ack Block */\n                if(frame_type & FTFLAGS_ACK_N){\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_blocks, tvb, offset, 1, ENC_NA);\n                    num_blocks = tvb_get_guint8(tvb, offset);\n                    offset += 1;\n                }\n\n                /* First Ack Block Length */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_first_ack_block_length, tvb, offset, len_missing_packet, gquic_info->encoding);\n                offset += len_missing_packet;\n\n                while(num_blocks){\n                    /* Gap to next block */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_gap_to_next_block, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* Ack Block Length */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_ack_block_length, tvb, offset, len_missing_packet, gquic_info->encoding);\n                    offset += len_missing_packet;\n\n                    num_blocks--;\n                }\n\n                /* Timestamp */\n                proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_num_timestamp, tvb, offset, 1, ENC_NA);\n                num_timestamp = tvb_get_guint8(tvb, offset);\n                offset += 1;\n\n                if(num_timestamp){\n\n                    /* Delta Largest Acked */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_acked, tvb, offset, 1, ENC_NA);\n                    offset += 1;\n\n                    /* Time Since Largest Acked */\n                    proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_largest_acked, tvb, offset, 4, gquic_info->encoding);\n                    offset += 4;\n\n                    num_timestamp -= 1;\n                    /* Num Timestamp x (Delta Largest Acked + Time Since Previous Timestamp) */\n                    while(num_timestamp){\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_delta_largest_acked, tvb, offset, 1, ENC_NA);\n                        offset += 1;\n\n                        proto_tree_add_item(ft_tree, hf_gquic_frame_type_ack_time_since_previous_timestamp, tvb, offset, 2, gquic_info->encoding);\n                        offset += 2;\n\n                        num_timestamp--;\n                    }\n                }\n\n            }\n\n        } else { /* Other ...*/\n            offset += 1;\n        }\n    }\n    return offset;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,10 @@\n int\n dissect_gquic_frame_type(tvbuff_t *tvb, packet_info *pinfo, proto_tree *gquic_tree, guint offset, guint8 len_pkn, gquic_info_data_t *gquic_info){\n+    if (!gquic_info) {\n+        expert_add_info(pinfo, gquic_tree, &ei_gquic_data_invalid);\n+        return offset + tvb_reported_length_remaining(tvb, offset);\n+    }\n+\n     proto_item *ti, *ti_ft, *ti_ftflags /*, *expert_ti*/;\n     proto_tree *ft_tree, *ftflags_tree;\n     guint8 frame_type;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (!gquic_info) {",
                "        expert_add_info(pinfo, gquic_tree, &ei_gquic_data_invalid);",
                "        return offset + tvb_reported_length_remaining(tvb, offset);",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1994",
        "func_name": "wireshark/proto_register_gquic",
        "description": "GQUIC dissector crash in Wireshark 4.0.0 to 4.0.4 and 3.6.0 to 3.6.12 allows denial of service via packet injection or crafted capture file",
        "git_url": "https://gitlab.com/wireshark/wireshark/-/commit/ee314ace8ae2d2fa8c6f7280231010252054fd7b",
        "commit_title": "GQUIC: Fix a null pointer exception",
        "commit_text": " Ensure that dissect_gquic_frame_type has a valid info pointer.  Fixes #18947. ",
        "func_before": "void\nproto_register_gquic(void)\n{\n    module_t *gquic_module;\n\n    static hf_register_info hf[] = {\n        /* Long/Short header for Q046 */\n        { &hf_gquic_header_form,\n          { \"Header Form\", \"gquic.header_form\",\n            FT_UINT8, BASE_DEC, VALS(gquic_short_long_header_vals), 0x80,\n            \"The most significant bit (0x80) of the first octet is set to 1 for long headers and 0 for short headers.\", HFILL }\n        },\n        { &hf_gquic_fixed_bit,\n          { \"Fixed Bit\", \"gquic.fixed_bit\",\n            FT_BOOLEAN, 8, NULL, 0x40,\n            \"Must be 1\", HFILL }\n        },\n        { &hf_gquic_long_packet_type,\n          { \"Packet Type\", \"gquic.long.packet_type\",\n            FT_UINT8, BASE_DEC, VALS(gquic_long_packet_type_vals), 0x30,\n            \"Long Header Packet Type\", HFILL }\n        },\n        { &hf_gquic_long_reserved,\n          { \"Reserved\", \"gquic.long.reserved\",\n            FT_UINT8, BASE_DEC, NULL, 0x0c,\n            \"Reserved bits\", HFILL }\n        },\n        { &hf_gquic_packet_number_length,\n          { \"Packet Number Length\", \"gquic.packet_number_length\",\n            FT_UINT8, BASE_DEC, VALS(gquic_packet_number_lengths), 0x03,\n            \"Packet Number field length\", HFILL }\n\t},\n        { &hf_gquic_dcil,\n          { \"Destination Connection ID Length\", \"gquic.dcil\",\n            FT_UINT8, BASE_DEC, VALS(quic_cid_lengths), 0xF0,\n            NULL, HFILL }\n        },\n        { &hf_gquic_scil,\n          { \"Source Connection ID Length\", \"gquic.scil\",\n            FT_UINT8, BASE_DEC, VALS(quic_cid_lengths), 0x0F,\n            NULL, HFILL }\n        },\n\n        /* Public header for < Q046 */\n        { &hf_gquic_puflags,\n            { \"Public Flags\", \"gquic.puflags\",\n              FT_UINT8, BASE_HEX, NULL, 0x0,\n              \"Specifying per-packet public flags\", HFILL }\n        },\n        { &hf_gquic_puflags_vrsn,\n            { \"Version\", \"gquic.puflags.version\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_VRSN,\n              \"Signifies that this packet also contains the version of the (Google)QUIC protocol\", HFILL }\n        },\n        { &hf_gquic_puflags_rst,\n            { \"Reset\", \"gquic.puflags.reset\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_RST,\n              \"Signifies that this packet is a public reset packet\", HFILL }\n        },\n        { &hf_gquic_puflags_dnonce,\n            { \"Diversification nonce\", \"gquic.puflags.nonce\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_DNONCE,\n              \"Indicates the presence of a 32 byte diversification nonce\", HFILL }\n        },\n        { &hf_gquic_puflags_cid,\n            { \"CID Length\", \"gquic.puflags.cid\",\n              FT_BOOLEAN, 8, TFS(&puflags_cid_tfs), PUFLAGS_CID,\n              \"Indicates the full 8 byte Connection ID is present\", HFILL }\n        },\n        { &hf_gquic_puflags_cid_old,\n            { \"CID Length\", \"gquic.puflags.cid.old\",\n              FT_UINT8, BASE_HEX, VALS(puflags_cid_old_vals), PUFLAGS_CID_OLD,\n              \"Signifies the Length of CID\", HFILL }\n        },\n        { &hf_gquic_puflags_pkn,\n            { \"Packet Number Length\", \"gquic.puflags.pkn\",\n              FT_UINT8, BASE_HEX, VALS(puflags_pkn_vals), PUFLAGS_PKN,\n              \"Signifies the Length of packet number\", HFILL }\n        },\n        { &hf_gquic_puflags_mpth,\n            { \"Multipath\", \"gquic.puflags.mpth\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_MPTH,\n              \"Reserved for multipath use\", HFILL }\n        },\n        { &hf_gquic_puflags_rsv,\n            { \"Reserved\", \"gquic.puflags.rsv\",\n              FT_UINT8, BASE_HEX, NULL, PUFLAGS_RSV,\n              \"Must be Zero\", HFILL }\n        },\n        { &hf_gquic_cid,\n            { \"CID\", \"gquic.cid\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Connection ID 64 bit pseudo random number\", HFILL }\n        },\n        { &hf_gquic_version,\n            { \"Version\", \"gquic.version\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"32 bit opaque tag that represents the version of the (Google)QUIC\", HFILL }\n        },\n        { &hf_gquic_diversification_nonce,\n            { \"Diversification nonce\", \"gquic.diversification_nonce\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_packet_number,\n            { \"Packet Number\", \"gquic.packet_number\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"The lower 8, 16, 32, or 48 bits of the packet number\", HFILL }\n        },\n\n        { &hf_gquic_prflags,\n            { \"Private Flags\", \"gquic.prflags\",\n              FT_UINT8, BASE_HEX, NULL, 0x0,\n              \"Specifying per-packet Private flags\", HFILL }\n        },\n\n        { &hf_gquic_prflags_entropy,\n            { \"Entropy\", \"gquic.prflags.entropy\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_ENTROPY,\n              \"For data packets, signifies that this packet contains the 1 bit of entropy, for fec packets, contains the xor of the entropy of protected packets\", HFILL }\n        },\n        { &hf_gquic_prflags_fecg,\n            { \"FEC Group\", \"gquic.prflags.fecg\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_FECG,\n              \"Indicates whether the fec byte is present.\", HFILL }\n        },\n        { &hf_gquic_prflags_fec,\n            { \"FEC\", \"gquic.prflags.fec\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_FEC,\n              \"Signifies that this packet represents an FEC packet\", HFILL }\n        },\n        { &hf_gquic_prflags_rsv,\n            { \"Reserved\", \"gquic.prflags.rsv\",\n              FT_UINT8, BASE_HEX, NULL, PRFLAGS_RSV,\n              \"Must be Zero\", HFILL }\n        },\n\n        { &hf_gquic_message_authentication_hash,\n            { \"Message Authentication Hash\", \"gquic.message_authentication_hash\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"The hash is an FNV1a-128 hash, serialized in little endian order\", HFILL }\n        },\n        { &hf_gquic_frame,\n            { \"Frame\", \"gquic.frame\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type,\n            { \"Frame Type\", \"gquic.frame_type\",\n              FT_UINT8 ,BASE_RANGE_STRING | BASE_HEX, RVALS(frame_type_vals), 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_padding_length,\n            { \"Padding Length\", \"gquic.frame_type.padding.length\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_padding,\n            { \"Padding\", \"gquic.frame_type.padding\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"Must be zero\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.rsts.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Stream ID of the stream being terminated\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_byte_offset,\n            { \"Byte offset\", \"gquic.frame_type.rsts.byte_offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Indicating the absolute byte offset of the end of data for this stream\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_error_code,\n            { \"Error code\", \"gquic.frame_type.rsts.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &rststream_error_code_vals_ext, 0x0,\n              \"Indicates why the stream is being closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_error_code,\n            { \"Error code\", \"gquic.frame_type.cc.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &error_code_vals_ext, 0x0,\n              \"Indicates the reason for closing this connection\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_reason_phrase_length,\n            { \"Reason phrase Length\", \"gquic.frame_type.cc.reason_phrase.length\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the length of the reason phrase\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_reason_phrase,\n            { \"Reason phrase\", \"gquic.frame_type.cc.reason_phrase\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"An optional human-readable explanation for why the connection was closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_error_code,\n            { \"Error code\", \"gquic.frame_type.goaway.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &error_code_vals_ext, 0x0,\n              \"Indicates the reason for closing this connection\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_last_good_stream_id,\n            { \"Last Good Stream ID\", \"gquic.frame_type.goaway.last_good_stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"last Stream ID which was accepted by the sender of the GOAWAY message\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_reason_phrase_length,\n            { \"Reason phrase Length\", \"gquic.frame_type.goaway.reason_phrase.length\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the length of the reason phrase\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_reason_phrase,\n            { \"Reason phrase\", \"gquic.frame_type.goaway.reason_phrase\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"An optional human-readable explanation for why the connection was closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_wu_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.wu.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"ID of the stream whose flow control windows is begin updated, or 0 to specify the connection-level flow control window\", HFILL }\n        },\n        { &hf_gquic_frame_type_wu_byte_offset,\n            { \"Byte offset\", \"gquic.frame_type.wu.byte_offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Indicating the absolute byte offset of data which can be sent on the given stream\", HFILL }\n        },\n        { &hf_gquic_frame_type_blocked_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.blocked.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Indicating the stream which is flow control blocked\", HFILL }\n        },\n        { &hf_gquic_frame_type_sw_send_entropy,\n            { \"Send Entropy\", \"gquic.frame_type.sw.send_entropy\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the cumulative hash of entropy in all sent packets up to the packet with packet number one less than the least unacked packet\", HFILL }\n        },\n        { &hf_gquic_frame_type_sw_least_unacked_delta,\n            { \"Least unacked delta\", \"gquic.frame_type.sw.least_unacked_delta\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"A variable length packet number delta with the same length as the packet header's packet number\", HFILL }\n        },\n        { &hf_gquic_crypto_offset,\n            { \"Offset\", \"gquic.crypto.offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Byte offset into the stream\", HFILL }\n        },\n        { &hf_gquic_crypto_length,\n            { \"Length\", \"gquic.crypto.length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Length of the Crypto Data field\", HFILL }\n        },\n        { &hf_gquic_crypto_crypto_data,\n            { \"Crypto Data\", \"gquic.crypto.crypto_data\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              \"The cryptographic message data\", HFILL }\n        },\n        { &hf_gquic_frame_type_stream,\n            { \"Stream\", \"gquic.frame_type.stream\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_STREAM,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_f,\n            { \"FIN\", \"gquic.frame_type.stream.f\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_STREAM_F,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_d,\n            { \"Data Length\", \"gquic.frame_type.stream.d\",\n              FT_BOOLEAN, 8, TFS(&len_data_vals), FTFLAGS_STREAM_D,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_ooo,\n            { \"Offset Length\", \"gquic.frame_type.stream.ooo\",\n              FT_UINT8, BASE_DEC, VALS(len_offset_vals), FTFLAGS_STREAM_OOO,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_ss,\n            { \"Stream Length\", \"gquic.frame_type.stream.ss\",\n              FT_UINT8, BASE_DEC, VALS(len_stream_vals), FTFLAGS_STREAM_SS,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack,\n            { \"ACK\", \"gquic.frame_type.ack\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_n,\n            { \"NACK\", \"gquic.frame_type.ack.n\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_N,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_u,\n            { \"Unused\", \"gquic.frame_type.ack.u\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_U,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_t,\n            { \"Truncated\", \"gquic.frame_type.ack.t\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_T,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ll,\n            { \"Largest Observed Length\", \"gquic.frame_type.ack.ll\",\n              FT_UINT8, BASE_DEC, VALS(len_largest_observed_vals), FTFLAGS_ACK_LL,\n              \"Length of the Largest Observed field as 1, 2, 4, or 6 bytes long\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_mm,\n            { \"Missing Packet Length\", \"gquic.frame_type.ack.mm\",\n              FT_UINT8, BASE_DEC, VALS(len_missing_packet_vals), FTFLAGS_ACK_MM,\n              \"Length of the Missing Packet Number Delta field as 1, 2, 4, or 6 bytes long\", HFILL }\n        },\n        /* ACK before Q034 */\n        { &hf_gquic_frame_type_ack_received_entropy,\n            { \"Received Entropy\", \"gquic.frame_type.ack.received_entropy\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the cumulative hash of entropy in all received packets up to the largest observed packet\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_largest_observed,\n            { \"Largest Observed\", \"gquic.frame_type.ack.largest_observed\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing the largest packet number the peer has observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ack_delay_time,\n            { \"Ack Delay time\", \"gquic.frame_type.ack.ack_delay_time\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the time elapsed in microseconds from when largest observed was received until this Ack frame was sent\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_timestamp,\n            { \"Num Timestamp\", \"gquic.frame_type.ack.num_timestamp\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of TCP timestamps that are included in this frame\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_delta_largest_observed,\n            { \"Delta Largest Observed\", \"gquic.frame_type.ack.delta_largest_observed\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the packet number delta from the first timestamp to the largest observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_first_timestamp,\n            { \"First Timestamp\", \"gquic.frame_type.ack.first_timestamp\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Specifying the time delta in microseconds, from the beginning of the connection of the arrival of the packet specified by Largest Observed minus Delta Largest Observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_time_since_previous_timestamp,\n            { \"Time since Previous timestamp\", \"gquic.frame_type.ack.time_since_previous_timestamp\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"This is the time delta from the previous timestamp\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_ranges,\n            { \"Num Ranges\", \"gquic.frame_type.ack.num_ranges\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of missing packet ranges between largest observed and least unacked\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_missing_packet,\n            { \"Missing Packet Number Delta\", \"gquic.frame_type.ack.missing_packet\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_range_length,\n            { \"Range Length\", \"gquic.frame_type.ack.range_length\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying one less than the number of sequential nacks in the range\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_revived,\n            { \"Num Revived\", \"gquic.frame_type.ack.num_revived\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of revived packets, recovered via FEC\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_revived_packet,\n            { \"Revived Packet Number\", \"gquic.frame_type.ack.revived_packet\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing a packet the peer has revived via FEC\", HFILL }\n        },\n        /* ACK after Q034 */\n        { &hf_gquic_frame_type_ack_largest_acked,\n            { \"Largest Acked\", \"gquic.frame_type.ack.largest_acked\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing the largest packet number the peer has observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_largest_acked_delta_time,\n            { \"Largest Acked Delta Time\", \"gquic.frame_type.ack.largest_acked_delta_time\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the time elapsed in microseconds from when largest acked was received until this Ack frame was sent\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_blocks,\n            { \"Num blocks\", \"gquic.frame_type.ack.num_blocks\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying one less than the number of ack blocks\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_first_ack_block_length,\n            { \"First Ack block length\", \"gquic.frame_type.ack.first_ack_block_length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_gap_to_next_block,\n            { \"Gap to next block\", \"gquic.frame_type.ack.gap_to_next_block\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of packets between ack blocks\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ack_block_length,\n            { \"Ack block length\", \"gquic.frame_type.ack.ack_block_length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_delta_largest_acked,\n            { \"Delta Largest Observed\", \"gquic.frame_type.ack.delta_largest_acked\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the packet number delta from the first timestamp to the largest observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_time_since_largest_acked,\n            { \"Time Since Largest Acked\", \"gquic.frame_type.ack.time_since_largest_acked\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Specifying the time delta in microseconds, from the beginning of the connection of the arrival of the packet specified by Largest Observed minus Delta Largest Observed\", HFILL }\n        },\n\n\n\n        { &hf_gquic_stream_id,\n            { \"Stream ID\", \"gquic.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_offset,\n            { \"Offset\", \"gquic.offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_data_len,\n            { \"Data Length\", \"gquic.data_len\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag,\n            { \"Tag\", \"gquic.tag\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_number,\n            { \"Tag Number\", \"gquic.tag_number\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tags,\n            { \"Tag/value\", \"gquic.tags\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_type,\n            { \"Tag Type\", \"gquic.tag_type\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_offset_end,\n            { \"Tag offset end\", \"gquic.tag_offset_end\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_length,\n            { \"Tag length\", \"gquic.tag_offset_length\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_value,\n            { \"Tag/value\", \"gquic.tag_value\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sni,\n            { \"Server Name Indication\", \"gquic.tag.sni\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"The fully qualified DNS name of the server, canonicalised to lowercase with no trailing period\", HFILL }\n        },\n        { &hf_gquic_tag_pad,\n            { \"Padding\", \"gquic.tag.pad\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"Pad.....\", HFILL }\n        },\n        { &hf_gquic_tag_ver,\n            { \"Version\", \"gquic.tag.version\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"Version of gquic supported\", HFILL }\n        },\n        { &hf_gquic_tag_pdmd,\n            { \"Proof demand\", \"gquic.tag.pdmd\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"a list of tags describing the types of proof acceptable to the client, in preference order\", HFILL }\n        },\n        { &hf_gquic_tag_ccs,\n            { \"Common certificate sets\", \"gquic.tag.ccs\",\n              FT_UINT64, BASE_HEX, NULL, 0x0,\n              \"A series of 64-bit, FNV-1a hashes of sets of common certificates that the client possesses\", HFILL }\n        },\n        { &hf_gquic_tag_uaid,\n            { \"Client's User Agent ID\", \"gquic.tag.uaid\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_stk,\n            { \"Source-address token\", \"gquic.tag.stk\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sno,\n            { \"Server nonce\", \"gquic.tag.sno\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_prof,\n            { \"Proof (Signature)\", \"gquic.tag.prof\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scfg,\n            { \"Server Config Tag\", \"gquic.tag.scfg\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scfg_number,\n            { \"Number Server Config Tag\", \"gquic.tag.scfg.number\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rrej,\n            { \"Reasons for server sending\", \"gquic.tag.rrej\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &handshake_failure_reason_vals_ext, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_crt,\n            { \"Certificate chain\", \"gquic.tag.crt\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_aead,\n            { \"Authenticated encryption algorithms\", \"gquic.tag.aead\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"A list of tags, in preference order, specifying the AEAD primitives supported by the server\", HFILL }\n        },\n        { &hf_gquic_tag_scid,\n            { \"Server Config ID\", \"gquic.tag.scid\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"An opaque, 16-byte identifier for this server config\", HFILL }\n        },\n        { &hf_gquic_tag_pubs,\n            { \"Public value\", \"gquic.tag.pubs\",\n              FT_UINT24, BASE_DEC_HEX, NULL, 0x0,\n              \"A list of public values, 24-bit, little-endian length prefixed\", HFILL }\n        },\n        { &hf_gquic_tag_kexs,\n            { \"Key exchange algorithms\", \"gquic.tag.kexs\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"A list of tags, in preference order, specifying the key exchange algorithms that the server supports\", HFILL }\n        },\n        { &hf_gquic_tag_obit,\n            { \"Server orbit\", \"gquic.tag.obit\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_expy,\n            { \"Expiry\", \"gquic.tag.expy\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"a 64-bit expiry time for the server config in UNIX epoch seconds\", HFILL }\n        },\n        { &hf_gquic_tag_nonc,\n            { \"Client nonce\", \"gquic.tag.nonc\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"32 bytes consisting of 4 bytes of timestamp (big-endian, UNIX epoch seconds), 8 bytes of server orbit and 20 bytes of random data\", HFILL }\n        },\n        { &hf_gquic_tag_mspc,\n            { \"Max streams per connection\", \"gquic.tag.mspc\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_tcid,\n            { \"Connection ID truncation\", \"gquic.tag.tcid\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_srbf,\n            { \"Socket receive buffer\", \"gquic.tag.srbf\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_icsl,\n            { \"Idle connection state\", \"gquic.tag.icsl\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scls,\n            { \"Silently close on timeout\", \"gquic.tag.scls\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_copt,\n            { \"Connection options\", \"gquic.tag.copt\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_ccrt,\n            { \"Cached certificates\", \"gquic.tag.ccrt\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_irtt,\n            { \"Estimated initial RTT\", \"gquic.tag.irtt\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"in us\", HFILL }\n        },\n        { &hf_gquic_tag_cfcw,\n            { \"Initial session/connection\", \"gquic.tag.cfcw\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sfcw,\n            { \"Initial stream flow control\", \"gquic.tag.sfcw\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cetv,\n            { \"Client encrypted tag-value\", \"gquic.tag.cetv\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_xlct,\n            { \"Expected leaf certificate\", \"gquic.tag.xlct\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_nonp,\n            { \"Client Proof nonce\", \"gquic.tag.nonp\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_csct,\n            { \"Signed cert timestamp\", \"gquic.tag.csct\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_ctim,\n            { \"Client Timestamp\", \"gquic.tag.ctim\",\n              FT_ABSOLUTE_TIME, ABSOLUTE_TIME_UTC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rnon,\n            { \"Public reset nonce proof\", \"gquic.tag.rnon\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rseq,\n            { \"Rejected Packet Number\", \"gquic.tag.rseq\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"a 64-bit packet number\", HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_type,\n            { \"Client IP Address Type\", \"gquic.tag.caddr.addr.type\",\n              FT_UINT16, BASE_DEC, VALS(cadr_type_vals), 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_ipv4,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr.ipv4\",\n              FT_IPv4, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_ipv6,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr.ipv6\",\n              FT_IPv6, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_port,\n            { \"Client Port (Source)\", \"gquic.tag.caddr.port\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_mids,\n            { \"Max incoming dynamic streams\", \"gquic.tag.mids\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_fhol,\n            { \"Force Head Of Line blocking\", \"gquic.tag.fhol\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sttl,\n            { \"Server Config TTL\", \"gquic.tag.sttl\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_smhl,\n            { \"Support Max Header List (size)\", \"gquic.tag.smhl\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_tbkp,\n            { \"Token Binding Key Params.\", \"gquic.tag.tbkp\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_mad0,\n            { \"Max Ack Delay\", \"gquic.tag.mad0\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_qlve,\n            { \"Legacy Version Encapsulation\", \"gquic.tag.qlve\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cgst,\n            { \"Congestion Control Feedback Type\", \"gquic.tag.cgst\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_epid,\n            { \"Endpoint identifier\", \"gquic.tag.epid\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_srst,\n            { \"Stateless Reset Token\", \"gquic.tag.srst\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n\n        { &hf_gquic_tag_unknown,\n            { \"Unknown tag\", \"gquic.tag.unknown\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_padding,\n            { \"Padding\", \"gquic.padding\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_stream_data,\n            { \"Stream Data\", \"gquic.stream_data\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_payload,\n            { \"Payload\", \"gquic.payload\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"(Google) QUIC Payload..\", HFILL }\n        },\n    };\n\n\n    static gint *ett[] = {\n        &ett_gquic,\n        &ett_gquic_puflags,\n        &ett_gquic_prflags,\n        &ett_gquic_ft,\n        &ett_gquic_ftflags,\n        &ett_gquic_tag_value\n    };\n\n    static ei_register_info ei[] = {\n        { &ei_gquic_tag_undecoded, { \"gquic.tag.undecoded\", PI_UNDECODED, PI_NOTE, \"Dissector for (Google)QUIC Tag code not implemented, Contact Wireshark developers if you want this supported\", EXPFILL }},\n        { &ei_gquic_tag_length, { \"gquic.tag.length.truncated\", PI_MALFORMED, PI_NOTE, \"Truncated Tag Length...\", EXPFILL }},\n        { &ei_gquic_tag_unknown, { \"gquic.tag.unknown.data\", PI_UNDECODED, PI_NOTE, \"Unknown Data\", EXPFILL }},\n        { &ei_gquic_version_invalid, { \"gquic.version.invalid\", PI_MALFORMED, PI_ERROR, \"Invalid Version\", EXPFILL }},\n        { &ei_gquic_invalid_parameter, { \"gquic.invalid.parameter\", PI_MALFORMED, PI_ERROR, \"Invalid Parameter\", EXPFILL }},\n        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }}\n    };\n\n    expert_module_t *expert_gquic;\n\n    proto_gquic = proto_register_protocol(\"GQUIC (Google Quick UDP Internet Connections)\", \"GQUIC\", \"gquic\");\n\n    proto_register_field_array(proto_gquic, hf, array_length(hf));\n    proto_register_subtree_array(ett, array_length(ett));\n\n    gquic_module = prefs_register_protocol(proto_gquic, NULL);\n\n    prefs_register_bool_preference(gquic_module, \"debug.quic\",\n                       \"Force decode of all (Google) QUIC Payload\",\n                       \"Help for debug...\",\n                       &g_gquic_debug);\n\n    expert_gquic = expert_register_protocol(proto_gquic);\n    expert_register_field_array(expert_gquic, ei, array_length(ei));\n\n    gquic_handle = register_dissector(\"gquic\", dissect_gquic, proto_gquic);\n}",
        "func": "void\nproto_register_gquic(void)\n{\n    module_t *gquic_module;\n\n    static hf_register_info hf[] = {\n        /* Long/Short header for Q046 */\n        { &hf_gquic_header_form,\n          { \"Header Form\", \"gquic.header_form\",\n            FT_UINT8, BASE_DEC, VALS(gquic_short_long_header_vals), 0x80,\n            \"The most significant bit (0x80) of the first octet is set to 1 for long headers and 0 for short headers.\", HFILL }\n        },\n        { &hf_gquic_fixed_bit,\n          { \"Fixed Bit\", \"gquic.fixed_bit\",\n            FT_BOOLEAN, 8, NULL, 0x40,\n            \"Must be 1\", HFILL }\n        },\n        { &hf_gquic_long_packet_type,\n          { \"Packet Type\", \"gquic.long.packet_type\",\n            FT_UINT8, BASE_DEC, VALS(gquic_long_packet_type_vals), 0x30,\n            \"Long Header Packet Type\", HFILL }\n        },\n        { &hf_gquic_long_reserved,\n          { \"Reserved\", \"gquic.long.reserved\",\n            FT_UINT8, BASE_DEC, NULL, 0x0c,\n            \"Reserved bits\", HFILL }\n        },\n        { &hf_gquic_packet_number_length,\n          { \"Packet Number Length\", \"gquic.packet_number_length\",\n            FT_UINT8, BASE_DEC, VALS(gquic_packet_number_lengths), 0x03,\n            \"Packet Number field length\", HFILL }\n\t},\n        { &hf_gquic_dcil,\n          { \"Destination Connection ID Length\", \"gquic.dcil\",\n            FT_UINT8, BASE_DEC, VALS(quic_cid_lengths), 0xF0,\n            NULL, HFILL }\n        },\n        { &hf_gquic_scil,\n          { \"Source Connection ID Length\", \"gquic.scil\",\n            FT_UINT8, BASE_DEC, VALS(quic_cid_lengths), 0x0F,\n            NULL, HFILL }\n        },\n\n        /* Public header for < Q046 */\n        { &hf_gquic_puflags,\n            { \"Public Flags\", \"gquic.puflags\",\n              FT_UINT8, BASE_HEX, NULL, 0x0,\n              \"Specifying per-packet public flags\", HFILL }\n        },\n        { &hf_gquic_puflags_vrsn,\n            { \"Version\", \"gquic.puflags.version\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_VRSN,\n              \"Signifies that this packet also contains the version of the (Google)QUIC protocol\", HFILL }\n        },\n        { &hf_gquic_puflags_rst,\n            { \"Reset\", \"gquic.puflags.reset\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_RST,\n              \"Signifies that this packet is a public reset packet\", HFILL }\n        },\n        { &hf_gquic_puflags_dnonce,\n            { \"Diversification nonce\", \"gquic.puflags.nonce\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_DNONCE,\n              \"Indicates the presence of a 32 byte diversification nonce\", HFILL }\n        },\n        { &hf_gquic_puflags_cid,\n            { \"CID Length\", \"gquic.puflags.cid\",\n              FT_BOOLEAN, 8, TFS(&puflags_cid_tfs), PUFLAGS_CID,\n              \"Indicates the full 8 byte Connection ID is present\", HFILL }\n        },\n        { &hf_gquic_puflags_cid_old,\n            { \"CID Length\", \"gquic.puflags.cid.old\",\n              FT_UINT8, BASE_HEX, VALS(puflags_cid_old_vals), PUFLAGS_CID_OLD,\n              \"Signifies the Length of CID\", HFILL }\n        },\n        { &hf_gquic_puflags_pkn,\n            { \"Packet Number Length\", \"gquic.puflags.pkn\",\n              FT_UINT8, BASE_HEX, VALS(puflags_pkn_vals), PUFLAGS_PKN,\n              \"Signifies the Length of packet number\", HFILL }\n        },\n        { &hf_gquic_puflags_mpth,\n            { \"Multipath\", \"gquic.puflags.mpth\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PUFLAGS_MPTH,\n              \"Reserved for multipath use\", HFILL }\n        },\n        { &hf_gquic_puflags_rsv,\n            { \"Reserved\", \"gquic.puflags.rsv\",\n              FT_UINT8, BASE_HEX, NULL, PUFLAGS_RSV,\n              \"Must be Zero\", HFILL }\n        },\n        { &hf_gquic_cid,\n            { \"CID\", \"gquic.cid\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Connection ID 64 bit pseudo random number\", HFILL }\n        },\n        { &hf_gquic_version,\n            { \"Version\", \"gquic.version\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"32 bit opaque tag that represents the version of the (Google)QUIC\", HFILL }\n        },\n        { &hf_gquic_diversification_nonce,\n            { \"Diversification nonce\", \"gquic.diversification_nonce\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_packet_number,\n            { \"Packet Number\", \"gquic.packet_number\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"The lower 8, 16, 32, or 48 bits of the packet number\", HFILL }\n        },\n\n        { &hf_gquic_prflags,\n            { \"Private Flags\", \"gquic.prflags\",\n              FT_UINT8, BASE_HEX, NULL, 0x0,\n              \"Specifying per-packet Private flags\", HFILL }\n        },\n\n        { &hf_gquic_prflags_entropy,\n            { \"Entropy\", \"gquic.prflags.entropy\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_ENTROPY,\n              \"For data packets, signifies that this packet contains the 1 bit of entropy, for fec packets, contains the xor of the entropy of protected packets\", HFILL }\n        },\n        { &hf_gquic_prflags_fecg,\n            { \"FEC Group\", \"gquic.prflags.fecg\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_FECG,\n              \"Indicates whether the fec byte is present.\", HFILL }\n        },\n        { &hf_gquic_prflags_fec,\n            { \"FEC\", \"gquic.prflags.fec\",\n              FT_BOOLEAN, 8, TFS(&tfs_yes_no), PRFLAGS_FEC,\n              \"Signifies that this packet represents an FEC packet\", HFILL }\n        },\n        { &hf_gquic_prflags_rsv,\n            { \"Reserved\", \"gquic.prflags.rsv\",\n              FT_UINT8, BASE_HEX, NULL, PRFLAGS_RSV,\n              \"Must be Zero\", HFILL }\n        },\n\n        { &hf_gquic_message_authentication_hash,\n            { \"Message Authentication Hash\", \"gquic.message_authentication_hash\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"The hash is an FNV1a-128 hash, serialized in little endian order\", HFILL }\n        },\n        { &hf_gquic_frame,\n            { \"Frame\", \"gquic.frame\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type,\n            { \"Frame Type\", \"gquic.frame_type\",\n              FT_UINT8 ,BASE_RANGE_STRING | BASE_HEX, RVALS(frame_type_vals), 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_padding_length,\n            { \"Padding Length\", \"gquic.frame_type.padding.length\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_padding,\n            { \"Padding\", \"gquic.frame_type.padding\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"Must be zero\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.rsts.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Stream ID of the stream being terminated\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_byte_offset,\n            { \"Byte offset\", \"gquic.frame_type.rsts.byte_offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Indicating the absolute byte offset of the end of data for this stream\", HFILL }\n        },\n        { &hf_gquic_frame_type_rsts_error_code,\n            { \"Error code\", \"gquic.frame_type.rsts.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &rststream_error_code_vals_ext, 0x0,\n              \"Indicates why the stream is being closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_error_code,\n            { \"Error code\", \"gquic.frame_type.cc.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &error_code_vals_ext, 0x0,\n              \"Indicates the reason for closing this connection\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_reason_phrase_length,\n            { \"Reason phrase Length\", \"gquic.frame_type.cc.reason_phrase.length\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the length of the reason phrase\", HFILL }\n        },\n        { &hf_gquic_frame_type_cc_reason_phrase,\n            { \"Reason phrase\", \"gquic.frame_type.cc.reason_phrase\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"An optional human-readable explanation for why the connection was closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_error_code,\n            { \"Error code\", \"gquic.frame_type.goaway.error_code\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &error_code_vals_ext, 0x0,\n              \"Indicates the reason for closing this connection\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_last_good_stream_id,\n            { \"Last Good Stream ID\", \"gquic.frame_type.goaway.last_good_stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"last Stream ID which was accepted by the sender of the GOAWAY message\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_reason_phrase_length,\n            { \"Reason phrase Length\", \"gquic.frame_type.goaway.reason_phrase.length\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the length of the reason phrase\", HFILL }\n        },\n        { &hf_gquic_frame_type_goaway_reason_phrase,\n            { \"Reason phrase\", \"gquic.frame_type.goaway.reason_phrase\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"An optional human-readable explanation for why the connection was closed\", HFILL }\n        },\n        { &hf_gquic_frame_type_wu_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.wu.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"ID of the stream whose flow control windows is begin updated, or 0 to specify the connection-level flow control window\", HFILL }\n        },\n        { &hf_gquic_frame_type_wu_byte_offset,\n            { \"Byte offset\", \"gquic.frame_type.wu.byte_offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Indicating the absolute byte offset of data which can be sent on the given stream\", HFILL }\n        },\n        { &hf_gquic_frame_type_blocked_stream_id,\n            { \"Stream ID\", \"gquic.frame_type.blocked.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Indicating the stream which is flow control blocked\", HFILL }\n        },\n        { &hf_gquic_frame_type_sw_send_entropy,\n            { \"Send Entropy\", \"gquic.frame_type.sw.send_entropy\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the cumulative hash of entropy in all sent packets up to the packet with packet number one less than the least unacked packet\", HFILL }\n        },\n        { &hf_gquic_frame_type_sw_least_unacked_delta,\n            { \"Least unacked delta\", \"gquic.frame_type.sw.least_unacked_delta\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"A variable length packet number delta with the same length as the packet header's packet number\", HFILL }\n        },\n        { &hf_gquic_crypto_offset,\n            { \"Offset\", \"gquic.crypto.offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Byte offset into the stream\", HFILL }\n        },\n        { &hf_gquic_crypto_length,\n            { \"Length\", \"gquic.crypto.length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Length of the Crypto Data field\", HFILL }\n        },\n        { &hf_gquic_crypto_crypto_data,\n            { \"Crypto Data\", \"gquic.crypto.crypto_data\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              \"The cryptographic message data\", HFILL }\n        },\n        { &hf_gquic_frame_type_stream,\n            { \"Stream\", \"gquic.frame_type.stream\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_STREAM,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_f,\n            { \"FIN\", \"gquic.frame_type.stream.f\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_STREAM_F,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_d,\n            { \"Data Length\", \"gquic.frame_type.stream.d\",\n              FT_BOOLEAN, 8, TFS(&len_data_vals), FTFLAGS_STREAM_D,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_ooo,\n            { \"Offset Length\", \"gquic.frame_type.stream.ooo\",\n              FT_UINT8, BASE_DEC, VALS(len_offset_vals), FTFLAGS_STREAM_OOO,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_stream_ss,\n            { \"Stream Length\", \"gquic.frame_type.stream.ss\",\n              FT_UINT8, BASE_DEC, VALS(len_stream_vals), FTFLAGS_STREAM_SS,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack,\n            { \"ACK\", \"gquic.frame_type.ack\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_n,\n            { \"NACK\", \"gquic.frame_type.ack.n\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_N,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_u,\n            { \"Unused\", \"gquic.frame_type.ack.u\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_U,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_t,\n            { \"Truncated\", \"gquic.frame_type.ack.t\",\n              FT_BOOLEAN, 8, NULL, FTFLAGS_ACK_T,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ll,\n            { \"Largest Observed Length\", \"gquic.frame_type.ack.ll\",\n              FT_UINT8, BASE_DEC, VALS(len_largest_observed_vals), FTFLAGS_ACK_LL,\n              \"Length of the Largest Observed field as 1, 2, 4, or 6 bytes long\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_mm,\n            { \"Missing Packet Length\", \"gquic.frame_type.ack.mm\",\n              FT_UINT8, BASE_DEC, VALS(len_missing_packet_vals), FTFLAGS_ACK_MM,\n              \"Length of the Missing Packet Number Delta field as 1, 2, 4, or 6 bytes long\", HFILL }\n        },\n        /* ACK before Q034 */\n        { &hf_gquic_frame_type_ack_received_entropy,\n            { \"Received Entropy\", \"gquic.frame_type.ack.received_entropy\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the cumulative hash of entropy in all received packets up to the largest observed packet\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_largest_observed,\n            { \"Largest Observed\", \"gquic.frame_type.ack.largest_observed\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing the largest packet number the peer has observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ack_delay_time,\n            { \"Ack Delay time\", \"gquic.frame_type.ack.ack_delay_time\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the time elapsed in microseconds from when largest observed was received until this Ack frame was sent\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_timestamp,\n            { \"Num Timestamp\", \"gquic.frame_type.ack.num_timestamp\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of TCP timestamps that are included in this frame\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_delta_largest_observed,\n            { \"Delta Largest Observed\", \"gquic.frame_type.ack.delta_largest_observed\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the packet number delta from the first timestamp to the largest observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_first_timestamp,\n            { \"First Timestamp\", \"gquic.frame_type.ack.first_timestamp\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Specifying the time delta in microseconds, from the beginning of the connection of the arrival of the packet specified by Largest Observed minus Delta Largest Observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_time_since_previous_timestamp,\n            { \"Time since Previous timestamp\", \"gquic.frame_type.ack.time_since_previous_timestamp\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"This is the time delta from the previous timestamp\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_ranges,\n            { \"Num Ranges\", \"gquic.frame_type.ack.num_ranges\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of missing packet ranges between largest observed and least unacked\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_missing_packet,\n            { \"Missing Packet Number Delta\", \"gquic.frame_type.ack.missing_packet\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_range_length,\n            { \"Range Length\", \"gquic.frame_type.ack.range_length\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying one less than the number of sequential nacks in the range\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_revived,\n            { \"Num Revived\", \"gquic.frame_type.ack.num_revived\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of revived packets, recovered via FEC\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_revived_packet,\n            { \"Revived Packet Number\", \"gquic.frame_type.ack.revived_packet\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing a packet the peer has revived via FEC\", HFILL }\n        },\n        /* ACK after Q034 */\n        { &hf_gquic_frame_type_ack_largest_acked,\n            { \"Largest Acked\", \"gquic.frame_type.ack.largest_acked\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"Representing the largest packet number the peer has observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_largest_acked_delta_time,\n            { \"Largest Acked Delta Time\", \"gquic.frame_type.ack.largest_acked_delta_time\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              \"Specifying the time elapsed in microseconds from when largest acked was received until this Ack frame was sent\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_num_blocks,\n            { \"Num blocks\", \"gquic.frame_type.ack.num_blocks\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying one less than the number of ack blocks\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_first_ack_block_length,\n            { \"First Ack block length\", \"gquic.frame_type.ack.first_ack_block_length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_gap_to_next_block,\n            { \"Gap to next block\", \"gquic.frame_type.ack.gap_to_next_block\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the number of packets between ack blocks\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_ack_block_length,\n            { \"Ack block length\", \"gquic.frame_type.ack.ack_block_length\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_frame_type_ack_delta_largest_acked,\n            { \"Delta Largest Observed\", \"gquic.frame_type.ack.delta_largest_acked\",\n              FT_UINT8, BASE_DEC, NULL, 0x0,\n              \"Specifying the packet number delta from the first timestamp to the largest observed\", HFILL }\n        },\n        { &hf_gquic_frame_type_ack_time_since_largest_acked,\n            { \"Time Since Largest Acked\", \"gquic.frame_type.ack.time_since_largest_acked\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"Specifying the time delta in microseconds, from the beginning of the connection of the arrival of the packet specified by Largest Observed minus Delta Largest Observed\", HFILL }\n        },\n\n\n\n        { &hf_gquic_stream_id,\n            { \"Stream ID\", \"gquic.stream_id\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_offset,\n            { \"Offset\", \"gquic.offset\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_data_len,\n            { \"Data Length\", \"gquic.data_len\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag,\n            { \"Tag\", \"gquic.tag\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_number,\n            { \"Tag Number\", \"gquic.tag_number\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tags,\n            { \"Tag/value\", \"gquic.tags\",\n              FT_NONE, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_type,\n            { \"Tag Type\", \"gquic.tag_type\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_offset_end,\n            { \"Tag offset end\", \"gquic.tag_offset_end\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_length,\n            { \"Tag length\", \"gquic.tag_offset_length\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_value,\n            { \"Tag/value\", \"gquic.tag_value\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sni,\n            { \"Server Name Indication\", \"gquic.tag.sni\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"The fully qualified DNS name of the server, canonicalised to lowercase with no trailing period\", HFILL }\n        },\n        { &hf_gquic_tag_pad,\n            { \"Padding\", \"gquic.tag.pad\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"Pad.....\", HFILL }\n        },\n        { &hf_gquic_tag_ver,\n            { \"Version\", \"gquic.tag.version\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"Version of gquic supported\", HFILL }\n        },\n        { &hf_gquic_tag_pdmd,\n            { \"Proof demand\", \"gquic.tag.pdmd\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"a list of tags describing the types of proof acceptable to the client, in preference order\", HFILL }\n        },\n        { &hf_gquic_tag_ccs,\n            { \"Common certificate sets\", \"gquic.tag.ccs\",\n              FT_UINT64, BASE_HEX, NULL, 0x0,\n              \"A series of 64-bit, FNV-1a hashes of sets of common certificates that the client possesses\", HFILL }\n        },\n        { &hf_gquic_tag_uaid,\n            { \"Client's User Agent ID\", \"gquic.tag.uaid\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_stk,\n            { \"Source-address token\", \"gquic.tag.stk\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sno,\n            { \"Server nonce\", \"gquic.tag.sno\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_prof,\n            { \"Proof (Signature)\", \"gquic.tag.prof\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scfg,\n            { \"Server Config Tag\", \"gquic.tag.scfg\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scfg_number,\n            { \"Number Server Config Tag\", \"gquic.tag.scfg.number\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rrej,\n            { \"Reasons for server sending\", \"gquic.tag.rrej\",\n              FT_UINT32, BASE_DEC|BASE_EXT_STRING, &handshake_failure_reason_vals_ext, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_crt,\n            { \"Certificate chain\", \"gquic.tag.crt\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_aead,\n            { \"Authenticated encryption algorithms\", \"gquic.tag.aead\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"A list of tags, in preference order, specifying the AEAD primitives supported by the server\", HFILL }\n        },\n        { &hf_gquic_tag_scid,\n            { \"Server Config ID\", \"gquic.tag.scid\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"An opaque, 16-byte identifier for this server config\", HFILL }\n        },\n        { &hf_gquic_tag_pubs,\n            { \"Public value\", \"gquic.tag.pubs\",\n              FT_UINT24, BASE_DEC_HEX, NULL, 0x0,\n              \"A list of public values, 24-bit, little-endian length prefixed\", HFILL }\n        },\n        { &hf_gquic_tag_kexs,\n            { \"Key exchange algorithms\", \"gquic.tag.kexs\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              \"A list of tags, in preference order, specifying the key exchange algorithms that the server supports\", HFILL }\n        },\n        { &hf_gquic_tag_obit,\n            { \"Server orbit\", \"gquic.tag.obit\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_expy,\n            { \"Expiry\", \"gquic.tag.expy\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"a 64-bit expiry time for the server config in UNIX epoch seconds\", HFILL }\n        },\n        { &hf_gquic_tag_nonc,\n            { \"Client nonce\", \"gquic.tag.nonc\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"32 bytes consisting of 4 bytes of timestamp (big-endian, UNIX epoch seconds), 8 bytes of server orbit and 20 bytes of random data\", HFILL }\n        },\n        { &hf_gquic_tag_mspc,\n            { \"Max streams per connection\", \"gquic.tag.mspc\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_tcid,\n            { \"Connection ID truncation\", \"gquic.tag.tcid\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_srbf,\n            { \"Socket receive buffer\", \"gquic.tag.srbf\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_icsl,\n            { \"Idle connection state\", \"gquic.tag.icsl\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_scls,\n            { \"Silently close on timeout\", \"gquic.tag.scls\",\n              FT_UINT32, BASE_DEC_HEX, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_copt,\n            { \"Connection options\", \"gquic.tag.copt\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_ccrt,\n            { \"Cached certificates\", \"gquic.tag.ccrt\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_irtt,\n            { \"Estimated initial RTT\", \"gquic.tag.irtt\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              \"in us\", HFILL }\n        },\n        { &hf_gquic_tag_cfcw,\n            { \"Initial session/connection\", \"gquic.tag.cfcw\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sfcw,\n            { \"Initial stream flow control\", \"gquic.tag.sfcw\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cetv,\n            { \"Client encrypted tag-value\", \"gquic.tag.cetv\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_xlct,\n            { \"Expected leaf certificate\", \"gquic.tag.xlct\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_nonp,\n            { \"Client Proof nonce\", \"gquic.tag.nonp\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_csct,\n            { \"Signed cert timestamp\", \"gquic.tag.csct\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_ctim,\n            { \"Client Timestamp\", \"gquic.tag.ctim\",\n              FT_ABSOLUTE_TIME, ABSOLUTE_TIME_UTC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rnon,\n            { \"Public reset nonce proof\", \"gquic.tag.rnon\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_rseq,\n            { \"Rejected Packet Number\", \"gquic.tag.rseq\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              \"a 64-bit packet number\", HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_type,\n            { \"Client IP Address Type\", \"gquic.tag.caddr.addr.type\",\n              FT_UINT16, BASE_DEC, VALS(cadr_type_vals), 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_ipv4,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr.ipv4\",\n              FT_IPv4, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr_ipv6,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr.ipv6\",\n              FT_IPv6, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_addr,\n            { \"Client IP Address\", \"gquic.tag.caddr.addr\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cadr_port,\n            { \"Client Port (Source)\", \"gquic.tag.caddr.port\",\n              FT_UINT16, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_mids,\n            { \"Max incoming dynamic streams\", \"gquic.tag.mids\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_fhol,\n            { \"Force Head Of Line blocking\", \"gquic.tag.fhol\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_sttl,\n            { \"Server Config TTL\", \"gquic.tag.sttl\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_smhl,\n            { \"Support Max Header List (size)\", \"gquic.tag.smhl\",\n              FT_UINT64, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_tbkp,\n            { \"Token Binding Key Params.\", \"gquic.tag.tbkp\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_mad0,\n            { \"Max Ack Delay\", \"gquic.tag.mad0\",\n              FT_UINT32, BASE_DEC, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_qlve,\n            { \"Legacy Version Encapsulation\", \"gquic.tag.qlve\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_cgst,\n            { \"Congestion Control Feedback Type\", \"gquic.tag.cgst\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_epid,\n            { \"Endpoint identifier\", \"gquic.tag.epid\",\n              FT_STRING, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_tag_srst,\n            { \"Stateless Reset Token\", \"gquic.tag.srst\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n\n        { &hf_gquic_tag_unknown,\n            { \"Unknown tag\", \"gquic.tag.unknown\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_padding,\n            { \"Padding\", \"gquic.padding\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_stream_data,\n            { \"Stream Data\", \"gquic.stream_data\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              NULL, HFILL }\n        },\n        { &hf_gquic_payload,\n            { \"Payload\", \"gquic.payload\",\n              FT_BYTES, BASE_NONE, NULL, 0x0,\n              \"(Google) QUIC Payload..\", HFILL }\n        },\n    };\n\n\n    static gint *ett[] = {\n        &ett_gquic,\n        &ett_gquic_puflags,\n        &ett_gquic_prflags,\n        &ett_gquic_ft,\n        &ett_gquic_ftflags,\n        &ett_gquic_tag_value\n    };\n\n    static ei_register_info ei[] = {\n        { &ei_gquic_tag_undecoded, { \"gquic.tag.undecoded\", PI_UNDECODED, PI_NOTE, \"Dissector for (Google)QUIC Tag code not implemented, Contact Wireshark developers if you want this supported\", EXPFILL }},\n        { &ei_gquic_tag_length, { \"gquic.tag.length.truncated\", PI_MALFORMED, PI_NOTE, \"Truncated Tag Length...\", EXPFILL }},\n        { &ei_gquic_tag_unknown, { \"gquic.tag.unknown.data\", PI_UNDECODED, PI_NOTE, \"Unknown Data\", EXPFILL }},\n        { &ei_gquic_version_invalid, { \"gquic.version.invalid\", PI_MALFORMED, PI_ERROR, \"Invalid Version\", EXPFILL }},\n        { &ei_gquic_invalid_parameter, { \"gquic.invalid.parameter\", PI_MALFORMED, PI_ERROR, \"Invalid Parameter\", EXPFILL }},\n        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }},\n        { &ei_gquic_data_invalid, { \"gquic.data.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Data\", EXPFILL }},\n    };\n\n    expert_module_t *expert_gquic;\n\n    proto_gquic = proto_register_protocol(\"GQUIC (Google Quick UDP Internet Connections)\", \"GQUIC\", \"gquic\");\n\n    proto_register_field_array(proto_gquic, hf, array_length(hf));\n    proto_register_subtree_array(ett, array_length(ett));\n\n    gquic_module = prefs_register_protocol(proto_gquic, NULL);\n\n    prefs_register_bool_preference(gquic_module, \"debug.quic\",\n                       \"Force decode of all (Google) QUIC Payload\",\n                       \"Help for debug...\",\n                       &g_gquic_debug);\n\n    expert_gquic = expert_register_protocol(proto_gquic);\n    expert_register_field_array(expert_gquic, ei, array_length(ei));\n\n    gquic_handle = register_dissector(\"gquic\", dissect_gquic, proto_gquic);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -759,7 +759,8 @@\n         { &ei_gquic_tag_unknown, { \"gquic.tag.unknown.data\", PI_UNDECODED, PI_NOTE, \"Unknown Data\", EXPFILL }},\n         { &ei_gquic_version_invalid, { \"gquic.version.invalid\", PI_MALFORMED, PI_ERROR, \"Invalid Version\", EXPFILL }},\n         { &ei_gquic_invalid_parameter, { \"gquic.invalid.parameter\", PI_MALFORMED, PI_ERROR, \"Invalid Parameter\", EXPFILL }},\n-        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }}\n+        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }},\n+        { &ei_gquic_data_invalid, { \"gquic.data.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Data\", EXPFILL }},\n     };\n \n     expert_module_t *expert_gquic;",
        "diff_line_info": {
            "deleted_lines": [
                "        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }}"
            ],
            "added_lines": [
                "        { &ei_gquic_length_invalid, { \"gquic.length.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Length\", EXPFILL }},",
                "        { &ei_gquic_data_invalid, { \"gquic.data.invalid\", PI_PROTOCOL, PI_WARN, \"Invalid Data\", EXPFILL }},"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-33297",
        "func_name": "bitcoin/PeerManagerImpl::SendMessages",
        "description": "Bitcoin Core before 24.1, when debug mode is not used, allows attackers to cause a denial of service (e.g., CPU consumption) because draining the inventory-to-send queue is inefficient, as exploited in the wild in May 2023.",
        "git_url": "https://github.com/bitcoin/bitcoin/commit/5b3406094f2679dfb3763de4414257268565b943",
        "commit_title": "net_processing: Boost inv trickle rate",
        "commit_text": " If transactions are being added to the mempool at a rate faster than 7tx/s (INVENTORY_BROADCAST_PER_SECOND) then peers' inventory_to_send queue can become relatively large. If this happens, increase the number of txids we include in an INV message (normally capped at 35) by 5 for each 1000 txids in the queue.  This will tend to clear a temporary excess out reasonably quickly; an excess of 4000 invs to send will be cleared down to 1000 in about 30 minutes, while an excess of 20000 invs would be cleared down to 1000 in about 60 minutes.",
        "func_before": "bool PeerManagerImpl::SendMessages(CNode* pto)\n{\n    AssertLockHeld(g_msgproc_mutex);\n\n    PeerRef peer = GetPeerRef(pto->GetId());\n    if (!peer) return false;\n    const Consensus::Params& consensusParams = m_chainparams.GetConsensus();\n\n    // We must call MaybeDiscourageAndDisconnect first, to ensure that we'll\n    // disconnect misbehaving peers even before the version handshake is complete.\n    if (MaybeDiscourageAndDisconnect(*pto, *peer)) return true;\n\n    // Don't send anything until the version handshake is complete\n    if (!pto->fSuccessfullyConnected || pto->fDisconnect)\n        return true;\n\n    // If we get here, the outgoing message serialization version is set and can't change.\n    const CNetMsgMaker msgMaker(pto->GetCommonVersion());\n\n    const auto current_time{GetTime<std::chrono::microseconds>()};\n\n    if (pto->IsAddrFetchConn() && current_time - pto->m_connected > 10 * AVG_ADDRESS_BROADCAST_INTERVAL) {\n        LogPrint(BCLog::NET, \"addrfetch connection timeout; disconnecting peer=%d\\n\", pto->GetId());\n        pto->fDisconnect = true;\n        return true;\n    }\n\n    MaybeSendPing(*pto, *peer, current_time);\n\n    // MaybeSendPing may have marked peer for disconnection\n    if (pto->fDisconnect) return true;\n\n    MaybeSendAddr(*pto, *peer, current_time);\n\n    MaybeSendSendHeaders(*pto, *peer);\n\n    {\n        LOCK(cs_main);\n\n        CNodeState &state = *State(pto->GetId());\n\n        // Start block sync\n        if (m_chainman.m_best_header == nullptr) {\n            m_chainman.m_best_header = m_chainman.ActiveChain().Tip();\n        }\n\n        // Determine whether we might try initial headers sync or parallel\n        // block download from this peer -- this mostly affects behavior while\n        // in IBD (once out of IBD, we sync from all peers).\n        bool sync_blocks_and_headers_from_peer = false;\n        if (state.fPreferredDownload) {\n            sync_blocks_and_headers_from_peer = true;\n        } else if (CanServeBlocks(*peer) && !pto->IsAddrFetchConn()) {\n            // Typically this is an inbound peer. If we don't have any outbound\n            // peers, or if we aren't downloading any blocks from such peers,\n            // then allow block downloads from this peer, too.\n            // We prefer downloading blocks from outbound peers to avoid\n            // putting undue load on (say) some home user who is just making\n            // outbound connections to the network, but if our only source of\n            // the latest blocks is from an inbound peer, we have to be sure to\n            // eventually download it (and not just wait indefinitely for an\n            // outbound peer to have it).\n            if (m_num_preferred_download_peers == 0 || mapBlocksInFlight.empty()) {\n                sync_blocks_and_headers_from_peer = true;\n            }\n        }\n\n        if (!state.fSyncStarted && CanServeBlocks(*peer) && !m_chainman.m_blockman.LoadingBlocks()) {\n            // Only actively request headers from a single peer, unless we're close to today.\n            if ((nSyncStarted == 0 && sync_blocks_and_headers_from_peer) || m_chainman.m_best_header->Time() > GetAdjustedTime() - 24h) {\n                const CBlockIndex* pindexStart = m_chainman.m_best_header;\n                /* If possible, start at the block preceding the currently\n                   best known header.  This ensures that we always get a\n                   non-empty list of headers back as long as the peer\n                   is up-to-date.  With a non-empty response, we can initialise\n                   the peer's known best block.  This wouldn't be possible\n                   if we requested starting at m_chainman.m_best_header and\n                   got back an empty response.  */\n                if (pindexStart->pprev)\n                    pindexStart = pindexStart->pprev;\n                if (MaybeSendGetHeaders(*pto, GetLocator(pindexStart), *peer)) {\n                    LogPrint(BCLog::NET, \"initial getheaders (%d) to peer=%d (startheight:%d)\\n\", pindexStart->nHeight, pto->GetId(), peer->m_starting_height);\n\n                    state.fSyncStarted = true;\n                    peer->m_headers_sync_timeout = current_time + HEADERS_DOWNLOAD_TIMEOUT_BASE +\n                        (\n                         // Convert HEADERS_DOWNLOAD_TIMEOUT_PER_HEADER to microseconds before scaling\n                         // to maintain precision\n                         std::chrono::microseconds{HEADERS_DOWNLOAD_TIMEOUT_PER_HEADER} *\n                         Ticks<std::chrono::seconds>(GetAdjustedTime() - m_chainman.m_best_header->Time()) / consensusParams.nPowTargetSpacing\n                        );\n                    nSyncStarted++;\n                }\n            }\n        }\n\n        //\n        // Try sending block announcements via headers\n        //\n        {\n            // If we have no more than MAX_BLOCKS_TO_ANNOUNCE in our\n            // list of block hashes we're relaying, and our peer wants\n            // headers announcements, then find the first header\n            // not yet known to our peer but would connect, and send.\n            // If no header would connect, or if we have too many\n            // blocks, or if the peer doesn't want headers, just\n            // add all to the inv queue.\n            LOCK(peer->m_block_inv_mutex);\n            std::vector<CBlock> vHeaders;\n            bool fRevertToInv = ((!peer->m_prefers_headers &&\n                                 (!state.m_requested_hb_cmpctblocks || peer->m_blocks_for_headers_relay.size() > 1)) ||\n                                 peer->m_blocks_for_headers_relay.size() > MAX_BLOCKS_TO_ANNOUNCE);\n            const CBlockIndex *pBestIndex = nullptr; // last header queued for delivery\n            ProcessBlockAvailability(pto->GetId()); // ensure pindexBestKnownBlock is up-to-date\n\n            if (!fRevertToInv) {\n                bool fFoundStartingHeader = false;\n                // Try to find first header that our peer doesn't have, and\n                // then send all headers past that one.  If we come across any\n                // headers that aren't on m_chainman.ActiveChain(), give up.\n                for (const uint256& hash : peer->m_blocks_for_headers_relay) {\n                    const CBlockIndex* pindex = m_chainman.m_blockman.LookupBlockIndex(hash);\n                    assert(pindex);\n                    if (m_chainman.ActiveChain()[pindex->nHeight] != pindex) {\n                        // Bail out if we reorged away from this block\n                        fRevertToInv = true;\n                        break;\n                    }\n                    if (pBestIndex != nullptr && pindex->pprev != pBestIndex) {\n                        // This means that the list of blocks to announce don't\n                        // connect to each other.\n                        // This shouldn't really be possible to hit during\n                        // regular operation (because reorgs should take us to\n                        // a chain that has some block not on the prior chain,\n                        // which should be caught by the prior check), but one\n                        // way this could happen is by using invalidateblock /\n                        // reconsiderblock repeatedly on the tip, causing it to\n                        // be added multiple times to m_blocks_for_headers_relay.\n                        // Robustly deal with this rare situation by reverting\n                        // to an inv.\n                        fRevertToInv = true;\n                        break;\n                    }\n                    pBestIndex = pindex;\n                    if (fFoundStartingHeader) {\n                        // add this to the headers message\n                        vHeaders.push_back(pindex->GetBlockHeader());\n                    } else if (PeerHasHeader(&state, pindex)) {\n                        continue; // keep looking for the first new block\n                    } else if (pindex->pprev == nullptr || PeerHasHeader(&state, pindex->pprev)) {\n                        // Peer doesn't have this header but they do have the prior one.\n                        // Start sending headers.\n                        fFoundStartingHeader = true;\n                        vHeaders.push_back(pindex->GetBlockHeader());\n                    } else {\n                        // Peer doesn't have this header or the prior one -- nothing will\n                        // connect, so bail out.\n                        fRevertToInv = true;\n                        break;\n                    }\n                }\n            }\n            if (!fRevertToInv && !vHeaders.empty()) {\n                if (vHeaders.size() == 1 && state.m_requested_hb_cmpctblocks) {\n                    // We only send up to 1 block as header-and-ids, as otherwise\n                    // probably means we're doing an initial-ish-sync or they're slow\n                    LogPrint(BCLog::NET, \"%s sending header-and-ids %s to peer=%d\\n\", __func__,\n                            vHeaders.front().GetHash().ToString(), pto->GetId());\n\n                    std::optional<CSerializedNetMsg> cached_cmpctblock_msg;\n                    {\n                        LOCK(m_most_recent_block_mutex);\n                        if (m_most_recent_block_hash == pBestIndex->GetBlockHash()) {\n                            cached_cmpctblock_msg = msgMaker.Make(NetMsgType::CMPCTBLOCK, *m_most_recent_compact_block);\n                        }\n                    }\n                    if (cached_cmpctblock_msg.has_value()) {\n                        m_connman.PushMessage(pto, std::move(cached_cmpctblock_msg.value()));\n                    } else {\n                        CBlock block;\n                        bool ret = ReadBlockFromDisk(block, pBestIndex, consensusParams);\n                        assert(ret);\n                        CBlockHeaderAndShortTxIDs cmpctblock{block};\n                        m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::CMPCTBLOCK, cmpctblock));\n                    }\n                    state.pindexBestHeaderSent = pBestIndex;\n                } else if (peer->m_prefers_headers) {\n                    if (vHeaders.size() > 1) {\n                        LogPrint(BCLog::NET, \"%s: %u headers, range (%s, %s), to peer=%d\\n\", __func__,\n                                vHeaders.size(),\n                                vHeaders.front().GetHash().ToString(),\n                                vHeaders.back().GetHash().ToString(), pto->GetId());\n                    } else {\n                        LogPrint(BCLog::NET, \"%s: sending header %s to peer=%d\\n\", __func__,\n                                vHeaders.front().GetHash().ToString(), pto->GetId());\n                    }\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::HEADERS, vHeaders));\n                    state.pindexBestHeaderSent = pBestIndex;\n                } else\n                    fRevertToInv = true;\n            }\n            if (fRevertToInv) {\n                // If falling back to using an inv, just try to inv the tip.\n                // The last entry in m_blocks_for_headers_relay was our tip at some point\n                // in the past.\n                if (!peer->m_blocks_for_headers_relay.empty()) {\n                    const uint256& hashToAnnounce = peer->m_blocks_for_headers_relay.back();\n                    const CBlockIndex* pindex = m_chainman.m_blockman.LookupBlockIndex(hashToAnnounce);\n                    assert(pindex);\n\n                    // Warn if we're announcing a block that is not on the main chain.\n                    // This should be very rare and could be optimized out.\n                    // Just log for now.\n                    if (m_chainman.ActiveChain()[pindex->nHeight] != pindex) {\n                        LogPrint(BCLog::NET, \"Announcing block %s not on main chain (tip=%s)\\n\",\n                            hashToAnnounce.ToString(), m_chainman.ActiveChain().Tip()->GetBlockHash().ToString());\n                    }\n\n                    // If the peer's chain has this block, don't inv it back.\n                    if (!PeerHasHeader(&state, pindex)) {\n                        peer->m_blocks_for_inv_relay.push_back(hashToAnnounce);\n                        LogPrint(BCLog::NET, \"%s: sending inv peer=%d hash=%s\\n\", __func__,\n                            pto->GetId(), hashToAnnounce.ToString());\n                    }\n                }\n            }\n            peer->m_blocks_for_headers_relay.clear();\n        }\n\n        //\n        // Message: inventory\n        //\n        std::vector<CInv> vInv;\n        {\n            LOCK(peer->m_block_inv_mutex);\n            vInv.reserve(std::max<size_t>(peer->m_blocks_for_inv_relay.size(), INVENTORY_BROADCAST_MAX));\n\n            // Add blocks\n            for (const uint256& hash : peer->m_blocks_for_inv_relay) {\n                vInv.push_back(CInv(MSG_BLOCK, hash));\n                if (vInv.size() == MAX_INV_SZ) {\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                    vInv.clear();\n                }\n            }\n            peer->m_blocks_for_inv_relay.clear();\n        }\n\n        if (auto tx_relay = peer->GetTxRelay(); tx_relay != nullptr) {\n                LOCK(tx_relay->m_tx_inventory_mutex);\n                // Check whether periodic sends should happen\n                bool fSendTrickle = pto->HasPermission(NetPermissionFlags::NoBan);\n                if (tx_relay->m_next_inv_send_time < current_time) {\n                    fSendTrickle = true;\n                    if (pto->IsInboundConn()) {\n                        tx_relay->m_next_inv_send_time = NextInvToInbounds(current_time, INBOUND_INVENTORY_BROADCAST_INTERVAL);\n                    } else {\n                        tx_relay->m_next_inv_send_time = GetExponentialRand(current_time, OUTBOUND_INVENTORY_BROADCAST_INTERVAL);\n                    }\n                }\n\n                // Time to send but the peer has requested we not relay transactions.\n                if (fSendTrickle) {\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n                    if (!tx_relay->m_relay_txs) tx_relay->m_tx_inventory_to_send.clear();\n                }\n\n                // Respond to BIP35 mempool requests\n                if (fSendTrickle && tx_relay->m_send_mempool) {\n                    auto vtxinfo = m_mempool.infoAll();\n                    tx_relay->m_send_mempool = false;\n                    const CFeeRate filterrate{tx_relay->m_fee_filter_received.load()};\n\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n\n                    for (const auto& txinfo : vtxinfo) {\n                        const uint256& hash = peer->m_wtxid_relay ? txinfo.tx->GetWitnessHash() : txinfo.tx->GetHash();\n                        CInv inv(peer->m_wtxid_relay ? MSG_WTX : MSG_TX, hash);\n                        tx_relay->m_tx_inventory_to_send.erase(hash);\n                        // Don't send transactions that peers will not put into their mempool\n                        if (txinfo.fee < filterrate.GetFee(txinfo.vsize)) {\n                            continue;\n                        }\n                        if (tx_relay->m_bloom_filter) {\n                            if (!tx_relay->m_bloom_filter->IsRelevantAndUpdate(*txinfo.tx)) continue;\n                        }\n                        tx_relay->m_tx_inventory_known_filter.insert(hash);\n                        // Responses to MEMPOOL requests bypass the m_recently_announced_invs filter.\n                        vInv.push_back(inv);\n                        if (vInv.size() == MAX_INV_SZ) {\n                            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                            vInv.clear();\n                        }\n                    }\n                    tx_relay->m_last_mempool_req = std::chrono::duration_cast<std::chrono::seconds>(current_time);\n                }\n\n                // Determine transactions to relay\n                if (fSendTrickle) {\n                    // Produce a vector with all candidates for sending\n                    std::vector<std::set<uint256>::iterator> vInvTx;\n                    vInvTx.reserve(tx_relay->m_tx_inventory_to_send.size());\n                    for (std::set<uint256>::iterator it = tx_relay->m_tx_inventory_to_send.begin(); it != tx_relay->m_tx_inventory_to_send.end(); it++) {\n                        vInvTx.push_back(it);\n                    }\n                    const CFeeRate filterrate{tx_relay->m_fee_filter_received.load()};\n                    // Topologically and fee-rate sort the inventory we send for privacy and priority reasons.\n                    // A heap is used so that not all items need sorting if only a few are being sent.\n                    CompareInvMempoolOrder compareInvMempoolOrder(&m_mempool, peer->m_wtxid_relay);\n                    std::make_heap(vInvTx.begin(), vInvTx.end(), compareInvMempoolOrder);\n                    // No reason to drain out at many times the network's capacity,\n                    // especially since we have many peers and some will draw much shorter delays.\n                    unsigned int nRelayedTransactions = 0;\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n                    while (!vInvTx.empty() && nRelayedTransactions < INVENTORY_BROADCAST_MAX) {\n                        // Fetch the top element from the heap\n                        std::pop_heap(vInvTx.begin(), vInvTx.end(), compareInvMempoolOrder);\n                        std::set<uint256>::iterator it = vInvTx.back();\n                        vInvTx.pop_back();\n                        uint256 hash = *it;\n                        CInv inv(peer->m_wtxid_relay ? MSG_WTX : MSG_TX, hash);\n                        // Remove it from the to-be-sent set\n                        tx_relay->m_tx_inventory_to_send.erase(it);\n                        // Check if not in the filter already\n                        if (tx_relay->m_tx_inventory_known_filter.contains(hash)) {\n                            continue;\n                        }\n                        // Not in the mempool anymore? don't bother sending it.\n                        auto txinfo = m_mempool.info(ToGenTxid(inv));\n                        if (!txinfo.tx) {\n                            continue;\n                        }\n                        auto txid = txinfo.tx->GetHash();\n                        auto wtxid = txinfo.tx->GetWitnessHash();\n                        // Peer told you to not send transactions at that feerate? Don't bother sending it.\n                        if (txinfo.fee < filterrate.GetFee(txinfo.vsize)) {\n                            continue;\n                        }\n                        if (tx_relay->m_bloom_filter && !tx_relay->m_bloom_filter->IsRelevantAndUpdate(*txinfo.tx)) continue;\n                        // Send\n                        tx_relay->m_recently_announced_invs.insert(hash);\n                        vInv.push_back(inv);\n                        nRelayedTransactions++;\n                        {\n                            // Expire old relay messages\n                            while (!g_relay_expiration.empty() && g_relay_expiration.front().first < current_time)\n                            {\n                                mapRelay.erase(g_relay_expiration.front().second);\n                                g_relay_expiration.pop_front();\n                            }\n\n                            auto ret = mapRelay.emplace(txid, std::move(txinfo.tx));\n                            if (ret.second) {\n                                g_relay_expiration.emplace_back(current_time + RELAY_TX_CACHE_TIME, ret.first);\n                            }\n                            // Add wtxid-based lookup into mapRelay as well, so that peers can request by wtxid\n                            auto ret2 = mapRelay.emplace(wtxid, ret.first->second);\n                            if (ret2.second) {\n                                g_relay_expiration.emplace_back(current_time + RELAY_TX_CACHE_TIME, ret2.first);\n                            }\n                        }\n                        if (vInv.size() == MAX_INV_SZ) {\n                            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                            vInv.clear();\n                        }\n                        tx_relay->m_tx_inventory_known_filter.insert(hash);\n                        if (hash != txid) {\n                            // Insert txid into m_tx_inventory_known_filter, even for\n                            // wtxidrelay peers. This prevents re-adding of\n                            // unconfirmed parents to the recently_announced\n                            // filter, when a child tx is requested. See\n                            // ProcessGetData().\n                            tx_relay->m_tx_inventory_known_filter.insert(txid);\n                        }\n                    }\n                }\n        }\n        if (!vInv.empty())\n            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n\n        // Detect whether we're stalling\n        auto stalling_timeout = m_block_stalling_timeout.load();\n        if (state.m_stalling_since.count() && state.m_stalling_since < current_time - stalling_timeout) {\n            // Stalling only triggers when the block download window cannot move. During normal steady state,\n            // the download window should be much larger than the to-be-downloaded set of blocks, so disconnection\n            // should only happen during initial block download.\n            LogPrintf(\"Peer=%d is stalling block download, disconnecting\\n\", pto->GetId());\n            pto->fDisconnect = true;\n            // Increase timeout for the next peer so that we don't disconnect multiple peers if our own\n            // bandwidth is insufficient.\n            const auto new_timeout = std::min(2 * stalling_timeout, BLOCK_STALLING_TIMEOUT_MAX);\n            if (stalling_timeout != new_timeout && m_block_stalling_timeout.compare_exchange_strong(stalling_timeout, new_timeout)) {\n                LogPrint(BCLog::NET, \"Increased stalling timeout temporarily to %d seconds\\n\", count_seconds(new_timeout));\n            }\n            return true;\n        }\n        // In case there is a block that has been in flight from this peer for block_interval * (1 + 0.5 * N)\n        // (with N the number of peers from which we're downloading validated blocks), disconnect due to timeout.\n        // We compensate for other peers to prevent killing off peers due to our own downstream link\n        // being saturated. We only count validated in-flight blocks so peers can't advertise non-existing block hashes\n        // to unreasonably increase our timeout.\n        if (state.vBlocksInFlight.size() > 0) {\n            QueuedBlock &queuedBlock = state.vBlocksInFlight.front();\n            int nOtherPeersWithValidatedDownloads = m_peers_downloading_from - 1;\n            if (current_time > state.m_downloading_since + std::chrono::seconds{consensusParams.nPowTargetSpacing} * (BLOCK_DOWNLOAD_TIMEOUT_BASE + BLOCK_DOWNLOAD_TIMEOUT_PER_PEER * nOtherPeersWithValidatedDownloads)) {\n                LogPrintf(\"Timeout downloading block %s from peer=%d, disconnecting\\n\", queuedBlock.pindex->GetBlockHash().ToString(), pto->GetId());\n                pto->fDisconnect = true;\n                return true;\n            }\n        }\n        // Check for headers sync timeouts\n        if (state.fSyncStarted && peer->m_headers_sync_timeout < std::chrono::microseconds::max()) {\n            // Detect whether this is a stalling initial-headers-sync peer\n            if (m_chainman.m_best_header->Time() <= GetAdjustedTime() - 24h) {\n                if (current_time > peer->m_headers_sync_timeout && nSyncStarted == 1 && (m_num_preferred_download_peers - state.fPreferredDownload >= 1)) {\n                    // Disconnect a peer (without NetPermissionFlags::NoBan permission) if it is our only sync peer,\n                    // and we have others we could be using instead.\n                    // Note: If all our peers are inbound, then we won't\n                    // disconnect our sync peer for stalling; we have bigger\n                    // problems if we can't get any outbound peers.\n                    if (!pto->HasPermission(NetPermissionFlags::NoBan)) {\n                        LogPrintf(\"Timeout downloading headers from peer=%d, disconnecting\\n\", pto->GetId());\n                        pto->fDisconnect = true;\n                        return true;\n                    } else {\n                        LogPrintf(\"Timeout downloading headers from noban peer=%d, not disconnecting\\n\", pto->GetId());\n                        // Reset the headers sync state so that we have a\n                        // chance to try downloading from a different peer.\n                        // Note: this will also result in at least one more\n                        // getheaders message to be sent to\n                        // this peer (eventually).\n                        state.fSyncStarted = false;\n                        nSyncStarted--;\n                        peer->m_headers_sync_timeout = 0us;\n                    }\n                }\n            } else {\n                // After we've caught up once, reset the timeout so we can't trigger\n                // disconnect later.\n                peer->m_headers_sync_timeout = std::chrono::microseconds::max();\n            }\n        }\n\n        // Check that outbound peers have reasonable chains\n        // GetTime() is used by this anti-DoS logic so we can test this using mocktime\n        ConsiderEviction(*pto, *peer, GetTime<std::chrono::seconds>());\n\n        //\n        // Message: getdata (blocks)\n        //\n        std::vector<CInv> vGetData;\n        if (CanServeBlocks(*peer) && ((sync_blocks_and_headers_from_peer && !IsLimitedPeer(*peer)) || !m_chainman.ActiveChainstate().IsInitialBlockDownload()) && state.nBlocksInFlight < MAX_BLOCKS_IN_TRANSIT_PER_PEER) {\n            std::vector<const CBlockIndex*> vToDownload;\n            NodeId staller = -1;\n            FindNextBlocksToDownload(*peer, MAX_BLOCKS_IN_TRANSIT_PER_PEER - state.nBlocksInFlight, vToDownload, staller);\n            for (const CBlockIndex *pindex : vToDownload) {\n                uint32_t nFetchFlags = GetFetchFlags(*peer);\n                vGetData.push_back(CInv(MSG_BLOCK | nFetchFlags, pindex->GetBlockHash()));\n                BlockRequested(pto->GetId(), *pindex);\n                LogPrint(BCLog::NET, \"Requesting block %s (%d) peer=%d\\n\", pindex->GetBlockHash().ToString(),\n                    pindex->nHeight, pto->GetId());\n            }\n            if (state.nBlocksInFlight == 0 && staller != -1) {\n                if (State(staller)->m_stalling_since == 0us) {\n                    State(staller)->m_stalling_since = current_time;\n                    LogPrint(BCLog::NET, \"Stall started peer=%d\\n\", staller);\n                }\n            }\n        }\n\n        //\n        // Message: getdata (transactions)\n        //\n        std::vector<std::pair<NodeId, GenTxid>> expired;\n        auto requestable = m_txrequest.GetRequestable(pto->GetId(), current_time, &expired);\n        for (const auto& entry : expired) {\n            LogPrint(BCLog::NET, \"timeout of inflight %s %s from peer=%d\\n\", entry.second.IsWtxid() ? \"wtx\" : \"tx\",\n                entry.second.GetHash().ToString(), entry.first);\n        }\n        for (const GenTxid& gtxid : requestable) {\n            if (!AlreadyHaveTx(gtxid)) {\n                LogPrint(BCLog::NET, \"Requesting %s %s peer=%d\\n\", gtxid.IsWtxid() ? \"wtx\" : \"tx\",\n                    gtxid.GetHash().ToString(), pto->GetId());\n                vGetData.emplace_back(gtxid.IsWtxid() ? MSG_WTX : (MSG_TX | GetFetchFlags(*peer)), gtxid.GetHash());\n                if (vGetData.size() >= MAX_GETDATA_SZ) {\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::GETDATA, vGetData));\n                    vGetData.clear();\n                }\n                m_txrequest.RequestedTx(pto->GetId(), gtxid.GetHash(), current_time + GETDATA_TX_INTERVAL);\n            } else {\n                // We have already seen this transaction, no need to download. This is just a belt-and-suspenders, as\n                // this should already be called whenever a transaction becomes AlreadyHaveTx().\n                m_txrequest.ForgetTxHash(gtxid.GetHash());\n            }\n        }\n\n\n        if (!vGetData.empty())\n            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::GETDATA, vGetData));\n    } // release cs_main\n    MaybeSendFeefilter(*pto, *peer, current_time);\n    return true;\n}",
        "func": "bool PeerManagerImpl::SendMessages(CNode* pto)\n{\n    AssertLockHeld(g_msgproc_mutex);\n\n    PeerRef peer = GetPeerRef(pto->GetId());\n    if (!peer) return false;\n    const Consensus::Params& consensusParams = m_chainparams.GetConsensus();\n\n    // We must call MaybeDiscourageAndDisconnect first, to ensure that we'll\n    // disconnect misbehaving peers even before the version handshake is complete.\n    if (MaybeDiscourageAndDisconnect(*pto, *peer)) return true;\n\n    // Don't send anything until the version handshake is complete\n    if (!pto->fSuccessfullyConnected || pto->fDisconnect)\n        return true;\n\n    // If we get here, the outgoing message serialization version is set and can't change.\n    const CNetMsgMaker msgMaker(pto->GetCommonVersion());\n\n    const auto current_time{GetTime<std::chrono::microseconds>()};\n\n    if (pto->IsAddrFetchConn() && current_time - pto->m_connected > 10 * AVG_ADDRESS_BROADCAST_INTERVAL) {\n        LogPrint(BCLog::NET, \"addrfetch connection timeout; disconnecting peer=%d\\n\", pto->GetId());\n        pto->fDisconnect = true;\n        return true;\n    }\n\n    MaybeSendPing(*pto, *peer, current_time);\n\n    // MaybeSendPing may have marked peer for disconnection\n    if (pto->fDisconnect) return true;\n\n    MaybeSendAddr(*pto, *peer, current_time);\n\n    MaybeSendSendHeaders(*pto, *peer);\n\n    {\n        LOCK(cs_main);\n\n        CNodeState &state = *State(pto->GetId());\n\n        // Start block sync\n        if (m_chainman.m_best_header == nullptr) {\n            m_chainman.m_best_header = m_chainman.ActiveChain().Tip();\n        }\n\n        // Determine whether we might try initial headers sync or parallel\n        // block download from this peer -- this mostly affects behavior while\n        // in IBD (once out of IBD, we sync from all peers).\n        bool sync_blocks_and_headers_from_peer = false;\n        if (state.fPreferredDownload) {\n            sync_blocks_and_headers_from_peer = true;\n        } else if (CanServeBlocks(*peer) && !pto->IsAddrFetchConn()) {\n            // Typically this is an inbound peer. If we don't have any outbound\n            // peers, or if we aren't downloading any blocks from such peers,\n            // then allow block downloads from this peer, too.\n            // We prefer downloading blocks from outbound peers to avoid\n            // putting undue load on (say) some home user who is just making\n            // outbound connections to the network, but if our only source of\n            // the latest blocks is from an inbound peer, we have to be sure to\n            // eventually download it (and not just wait indefinitely for an\n            // outbound peer to have it).\n            if (m_num_preferred_download_peers == 0 || mapBlocksInFlight.empty()) {\n                sync_blocks_and_headers_from_peer = true;\n            }\n        }\n\n        if (!state.fSyncStarted && CanServeBlocks(*peer) && !m_chainman.m_blockman.LoadingBlocks()) {\n            // Only actively request headers from a single peer, unless we're close to today.\n            if ((nSyncStarted == 0 && sync_blocks_and_headers_from_peer) || m_chainman.m_best_header->Time() > GetAdjustedTime() - 24h) {\n                const CBlockIndex* pindexStart = m_chainman.m_best_header;\n                /* If possible, start at the block preceding the currently\n                   best known header.  This ensures that we always get a\n                   non-empty list of headers back as long as the peer\n                   is up-to-date.  With a non-empty response, we can initialise\n                   the peer's known best block.  This wouldn't be possible\n                   if we requested starting at m_chainman.m_best_header and\n                   got back an empty response.  */\n                if (pindexStart->pprev)\n                    pindexStart = pindexStart->pprev;\n                if (MaybeSendGetHeaders(*pto, GetLocator(pindexStart), *peer)) {\n                    LogPrint(BCLog::NET, \"initial getheaders (%d) to peer=%d (startheight:%d)\\n\", pindexStart->nHeight, pto->GetId(), peer->m_starting_height);\n\n                    state.fSyncStarted = true;\n                    peer->m_headers_sync_timeout = current_time + HEADERS_DOWNLOAD_TIMEOUT_BASE +\n                        (\n                         // Convert HEADERS_DOWNLOAD_TIMEOUT_PER_HEADER to microseconds before scaling\n                         // to maintain precision\n                         std::chrono::microseconds{HEADERS_DOWNLOAD_TIMEOUT_PER_HEADER} *\n                         Ticks<std::chrono::seconds>(GetAdjustedTime() - m_chainman.m_best_header->Time()) / consensusParams.nPowTargetSpacing\n                        );\n                    nSyncStarted++;\n                }\n            }\n        }\n\n        //\n        // Try sending block announcements via headers\n        //\n        {\n            // If we have no more than MAX_BLOCKS_TO_ANNOUNCE in our\n            // list of block hashes we're relaying, and our peer wants\n            // headers announcements, then find the first header\n            // not yet known to our peer but would connect, and send.\n            // If no header would connect, or if we have too many\n            // blocks, or if the peer doesn't want headers, just\n            // add all to the inv queue.\n            LOCK(peer->m_block_inv_mutex);\n            std::vector<CBlock> vHeaders;\n            bool fRevertToInv = ((!peer->m_prefers_headers &&\n                                 (!state.m_requested_hb_cmpctblocks || peer->m_blocks_for_headers_relay.size() > 1)) ||\n                                 peer->m_blocks_for_headers_relay.size() > MAX_BLOCKS_TO_ANNOUNCE);\n            const CBlockIndex *pBestIndex = nullptr; // last header queued for delivery\n            ProcessBlockAvailability(pto->GetId()); // ensure pindexBestKnownBlock is up-to-date\n\n            if (!fRevertToInv) {\n                bool fFoundStartingHeader = false;\n                // Try to find first header that our peer doesn't have, and\n                // then send all headers past that one.  If we come across any\n                // headers that aren't on m_chainman.ActiveChain(), give up.\n                for (const uint256& hash : peer->m_blocks_for_headers_relay) {\n                    const CBlockIndex* pindex = m_chainman.m_blockman.LookupBlockIndex(hash);\n                    assert(pindex);\n                    if (m_chainman.ActiveChain()[pindex->nHeight] != pindex) {\n                        // Bail out if we reorged away from this block\n                        fRevertToInv = true;\n                        break;\n                    }\n                    if (pBestIndex != nullptr && pindex->pprev != pBestIndex) {\n                        // This means that the list of blocks to announce don't\n                        // connect to each other.\n                        // This shouldn't really be possible to hit during\n                        // regular operation (because reorgs should take us to\n                        // a chain that has some block not on the prior chain,\n                        // which should be caught by the prior check), but one\n                        // way this could happen is by using invalidateblock /\n                        // reconsiderblock repeatedly on the tip, causing it to\n                        // be added multiple times to m_blocks_for_headers_relay.\n                        // Robustly deal with this rare situation by reverting\n                        // to an inv.\n                        fRevertToInv = true;\n                        break;\n                    }\n                    pBestIndex = pindex;\n                    if (fFoundStartingHeader) {\n                        // add this to the headers message\n                        vHeaders.push_back(pindex->GetBlockHeader());\n                    } else if (PeerHasHeader(&state, pindex)) {\n                        continue; // keep looking for the first new block\n                    } else if (pindex->pprev == nullptr || PeerHasHeader(&state, pindex->pprev)) {\n                        // Peer doesn't have this header but they do have the prior one.\n                        // Start sending headers.\n                        fFoundStartingHeader = true;\n                        vHeaders.push_back(pindex->GetBlockHeader());\n                    } else {\n                        // Peer doesn't have this header or the prior one -- nothing will\n                        // connect, so bail out.\n                        fRevertToInv = true;\n                        break;\n                    }\n                }\n            }\n            if (!fRevertToInv && !vHeaders.empty()) {\n                if (vHeaders.size() == 1 && state.m_requested_hb_cmpctblocks) {\n                    // We only send up to 1 block as header-and-ids, as otherwise\n                    // probably means we're doing an initial-ish-sync or they're slow\n                    LogPrint(BCLog::NET, \"%s sending header-and-ids %s to peer=%d\\n\", __func__,\n                            vHeaders.front().GetHash().ToString(), pto->GetId());\n\n                    std::optional<CSerializedNetMsg> cached_cmpctblock_msg;\n                    {\n                        LOCK(m_most_recent_block_mutex);\n                        if (m_most_recent_block_hash == pBestIndex->GetBlockHash()) {\n                            cached_cmpctblock_msg = msgMaker.Make(NetMsgType::CMPCTBLOCK, *m_most_recent_compact_block);\n                        }\n                    }\n                    if (cached_cmpctblock_msg.has_value()) {\n                        m_connman.PushMessage(pto, std::move(cached_cmpctblock_msg.value()));\n                    } else {\n                        CBlock block;\n                        bool ret = ReadBlockFromDisk(block, pBestIndex, consensusParams);\n                        assert(ret);\n                        CBlockHeaderAndShortTxIDs cmpctblock{block};\n                        m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::CMPCTBLOCK, cmpctblock));\n                    }\n                    state.pindexBestHeaderSent = pBestIndex;\n                } else if (peer->m_prefers_headers) {\n                    if (vHeaders.size() > 1) {\n                        LogPrint(BCLog::NET, \"%s: %u headers, range (%s, %s), to peer=%d\\n\", __func__,\n                                vHeaders.size(),\n                                vHeaders.front().GetHash().ToString(),\n                                vHeaders.back().GetHash().ToString(), pto->GetId());\n                    } else {\n                        LogPrint(BCLog::NET, \"%s: sending header %s to peer=%d\\n\", __func__,\n                                vHeaders.front().GetHash().ToString(), pto->GetId());\n                    }\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::HEADERS, vHeaders));\n                    state.pindexBestHeaderSent = pBestIndex;\n                } else\n                    fRevertToInv = true;\n            }\n            if (fRevertToInv) {\n                // If falling back to using an inv, just try to inv the tip.\n                // The last entry in m_blocks_for_headers_relay was our tip at some point\n                // in the past.\n                if (!peer->m_blocks_for_headers_relay.empty()) {\n                    const uint256& hashToAnnounce = peer->m_blocks_for_headers_relay.back();\n                    const CBlockIndex* pindex = m_chainman.m_blockman.LookupBlockIndex(hashToAnnounce);\n                    assert(pindex);\n\n                    // Warn if we're announcing a block that is not on the main chain.\n                    // This should be very rare and could be optimized out.\n                    // Just log for now.\n                    if (m_chainman.ActiveChain()[pindex->nHeight] != pindex) {\n                        LogPrint(BCLog::NET, \"Announcing block %s not on main chain (tip=%s)\\n\",\n                            hashToAnnounce.ToString(), m_chainman.ActiveChain().Tip()->GetBlockHash().ToString());\n                    }\n\n                    // If the peer's chain has this block, don't inv it back.\n                    if (!PeerHasHeader(&state, pindex)) {\n                        peer->m_blocks_for_inv_relay.push_back(hashToAnnounce);\n                        LogPrint(BCLog::NET, \"%s: sending inv peer=%d hash=%s\\n\", __func__,\n                            pto->GetId(), hashToAnnounce.ToString());\n                    }\n                }\n            }\n            peer->m_blocks_for_headers_relay.clear();\n        }\n\n        //\n        // Message: inventory\n        //\n        std::vector<CInv> vInv;\n        {\n            LOCK(peer->m_block_inv_mutex);\n            vInv.reserve(std::max<size_t>(peer->m_blocks_for_inv_relay.size(), INVENTORY_BROADCAST_MAX));\n\n            // Add blocks\n            for (const uint256& hash : peer->m_blocks_for_inv_relay) {\n                vInv.push_back(CInv(MSG_BLOCK, hash));\n                if (vInv.size() == MAX_INV_SZ) {\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                    vInv.clear();\n                }\n            }\n            peer->m_blocks_for_inv_relay.clear();\n        }\n\n        if (auto tx_relay = peer->GetTxRelay(); tx_relay != nullptr) {\n                LOCK(tx_relay->m_tx_inventory_mutex);\n                // Check whether periodic sends should happen\n                bool fSendTrickle = pto->HasPermission(NetPermissionFlags::NoBan);\n                if (tx_relay->m_next_inv_send_time < current_time) {\n                    fSendTrickle = true;\n                    if (pto->IsInboundConn()) {\n                        tx_relay->m_next_inv_send_time = NextInvToInbounds(current_time, INBOUND_INVENTORY_BROADCAST_INTERVAL);\n                    } else {\n                        tx_relay->m_next_inv_send_time = GetExponentialRand(current_time, OUTBOUND_INVENTORY_BROADCAST_INTERVAL);\n                    }\n                }\n\n                // Time to send but the peer has requested we not relay transactions.\n                if (fSendTrickle) {\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n                    if (!tx_relay->m_relay_txs) tx_relay->m_tx_inventory_to_send.clear();\n                }\n\n                // Respond to BIP35 mempool requests\n                if (fSendTrickle && tx_relay->m_send_mempool) {\n                    auto vtxinfo = m_mempool.infoAll();\n                    tx_relay->m_send_mempool = false;\n                    const CFeeRate filterrate{tx_relay->m_fee_filter_received.load()};\n\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n\n                    for (const auto& txinfo : vtxinfo) {\n                        const uint256& hash = peer->m_wtxid_relay ? txinfo.tx->GetWitnessHash() : txinfo.tx->GetHash();\n                        CInv inv(peer->m_wtxid_relay ? MSG_WTX : MSG_TX, hash);\n                        tx_relay->m_tx_inventory_to_send.erase(hash);\n                        // Don't send transactions that peers will not put into their mempool\n                        if (txinfo.fee < filterrate.GetFee(txinfo.vsize)) {\n                            continue;\n                        }\n                        if (tx_relay->m_bloom_filter) {\n                            if (!tx_relay->m_bloom_filter->IsRelevantAndUpdate(*txinfo.tx)) continue;\n                        }\n                        tx_relay->m_tx_inventory_known_filter.insert(hash);\n                        // Responses to MEMPOOL requests bypass the m_recently_announced_invs filter.\n                        vInv.push_back(inv);\n                        if (vInv.size() == MAX_INV_SZ) {\n                            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                            vInv.clear();\n                        }\n                    }\n                    tx_relay->m_last_mempool_req = std::chrono::duration_cast<std::chrono::seconds>(current_time);\n                }\n\n                // Determine transactions to relay\n                if (fSendTrickle) {\n                    // Produce a vector with all candidates for sending\n                    std::vector<std::set<uint256>::iterator> vInvTx;\n                    vInvTx.reserve(tx_relay->m_tx_inventory_to_send.size());\n                    for (std::set<uint256>::iterator it = tx_relay->m_tx_inventory_to_send.begin(); it != tx_relay->m_tx_inventory_to_send.end(); it++) {\n                        vInvTx.push_back(it);\n                    }\n                    const CFeeRate filterrate{tx_relay->m_fee_filter_received.load()};\n                    // Topologically and fee-rate sort the inventory we send for privacy and priority reasons.\n                    // A heap is used so that not all items need sorting if only a few are being sent.\n                    CompareInvMempoolOrder compareInvMempoolOrder(&m_mempool, peer->m_wtxid_relay);\n                    std::make_heap(vInvTx.begin(), vInvTx.end(), compareInvMempoolOrder);\n                    // No reason to drain out at many times the network's capacity,\n                    // especially since we have many peers and some will draw much shorter delays.\n                    unsigned int nRelayedTransactions = 0;\n                    LOCK(tx_relay->m_bloom_filter_mutex);\n                    size_t broadcast_max{INVENTORY_BROADCAST_MAX + (tx_relay->m_tx_inventory_to_send.size()/1000)*5};\n                    broadcast_max = std::min<size_t>(1000, broadcast_max);\n                    while (!vInvTx.empty() && nRelayedTransactions < broadcast_max) {\n                        // Fetch the top element from the heap\n                        std::pop_heap(vInvTx.begin(), vInvTx.end(), compareInvMempoolOrder);\n                        std::set<uint256>::iterator it = vInvTx.back();\n                        vInvTx.pop_back();\n                        uint256 hash = *it;\n                        CInv inv(peer->m_wtxid_relay ? MSG_WTX : MSG_TX, hash);\n                        // Remove it from the to-be-sent set\n                        tx_relay->m_tx_inventory_to_send.erase(it);\n                        // Check if not in the filter already\n                        if (tx_relay->m_tx_inventory_known_filter.contains(hash)) {\n                            continue;\n                        }\n                        // Not in the mempool anymore? don't bother sending it.\n                        auto txinfo = m_mempool.info(ToGenTxid(inv));\n                        if (!txinfo.tx) {\n                            continue;\n                        }\n                        auto txid = txinfo.tx->GetHash();\n                        auto wtxid = txinfo.tx->GetWitnessHash();\n                        // Peer told you to not send transactions at that feerate? Don't bother sending it.\n                        if (txinfo.fee < filterrate.GetFee(txinfo.vsize)) {\n                            continue;\n                        }\n                        if (tx_relay->m_bloom_filter && !tx_relay->m_bloom_filter->IsRelevantAndUpdate(*txinfo.tx)) continue;\n                        // Send\n                        tx_relay->m_recently_announced_invs.insert(hash);\n                        vInv.push_back(inv);\n                        nRelayedTransactions++;\n                        {\n                            // Expire old relay messages\n                            while (!g_relay_expiration.empty() && g_relay_expiration.front().first < current_time)\n                            {\n                                mapRelay.erase(g_relay_expiration.front().second);\n                                g_relay_expiration.pop_front();\n                            }\n\n                            auto ret = mapRelay.emplace(txid, std::move(txinfo.tx));\n                            if (ret.second) {\n                                g_relay_expiration.emplace_back(current_time + RELAY_TX_CACHE_TIME, ret.first);\n                            }\n                            // Add wtxid-based lookup into mapRelay as well, so that peers can request by wtxid\n                            auto ret2 = mapRelay.emplace(wtxid, ret.first->second);\n                            if (ret2.second) {\n                                g_relay_expiration.emplace_back(current_time + RELAY_TX_CACHE_TIME, ret2.first);\n                            }\n                        }\n                        if (vInv.size() == MAX_INV_SZ) {\n                            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n                            vInv.clear();\n                        }\n                        tx_relay->m_tx_inventory_known_filter.insert(hash);\n                        if (hash != txid) {\n                            // Insert txid into m_tx_inventory_known_filter, even for\n                            // wtxidrelay peers. This prevents re-adding of\n                            // unconfirmed parents to the recently_announced\n                            // filter, when a child tx is requested. See\n                            // ProcessGetData().\n                            tx_relay->m_tx_inventory_known_filter.insert(txid);\n                        }\n                    }\n                }\n        }\n        if (!vInv.empty())\n            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::INV, vInv));\n\n        // Detect whether we're stalling\n        auto stalling_timeout = m_block_stalling_timeout.load();\n        if (state.m_stalling_since.count() && state.m_stalling_since < current_time - stalling_timeout) {\n            // Stalling only triggers when the block download window cannot move. During normal steady state,\n            // the download window should be much larger than the to-be-downloaded set of blocks, so disconnection\n            // should only happen during initial block download.\n            LogPrintf(\"Peer=%d is stalling block download, disconnecting\\n\", pto->GetId());\n            pto->fDisconnect = true;\n            // Increase timeout for the next peer so that we don't disconnect multiple peers if our own\n            // bandwidth is insufficient.\n            const auto new_timeout = std::min(2 * stalling_timeout, BLOCK_STALLING_TIMEOUT_MAX);\n            if (stalling_timeout != new_timeout && m_block_stalling_timeout.compare_exchange_strong(stalling_timeout, new_timeout)) {\n                LogPrint(BCLog::NET, \"Increased stalling timeout temporarily to %d seconds\\n\", count_seconds(new_timeout));\n            }\n            return true;\n        }\n        // In case there is a block that has been in flight from this peer for block_interval * (1 + 0.5 * N)\n        // (with N the number of peers from which we're downloading validated blocks), disconnect due to timeout.\n        // We compensate for other peers to prevent killing off peers due to our own downstream link\n        // being saturated. We only count validated in-flight blocks so peers can't advertise non-existing block hashes\n        // to unreasonably increase our timeout.\n        if (state.vBlocksInFlight.size() > 0) {\n            QueuedBlock &queuedBlock = state.vBlocksInFlight.front();\n            int nOtherPeersWithValidatedDownloads = m_peers_downloading_from - 1;\n            if (current_time > state.m_downloading_since + std::chrono::seconds{consensusParams.nPowTargetSpacing} * (BLOCK_DOWNLOAD_TIMEOUT_BASE + BLOCK_DOWNLOAD_TIMEOUT_PER_PEER * nOtherPeersWithValidatedDownloads)) {\n                LogPrintf(\"Timeout downloading block %s from peer=%d, disconnecting\\n\", queuedBlock.pindex->GetBlockHash().ToString(), pto->GetId());\n                pto->fDisconnect = true;\n                return true;\n            }\n        }\n        // Check for headers sync timeouts\n        if (state.fSyncStarted && peer->m_headers_sync_timeout < std::chrono::microseconds::max()) {\n            // Detect whether this is a stalling initial-headers-sync peer\n            if (m_chainman.m_best_header->Time() <= GetAdjustedTime() - 24h) {\n                if (current_time > peer->m_headers_sync_timeout && nSyncStarted == 1 && (m_num_preferred_download_peers - state.fPreferredDownload >= 1)) {\n                    // Disconnect a peer (without NetPermissionFlags::NoBan permission) if it is our only sync peer,\n                    // and we have others we could be using instead.\n                    // Note: If all our peers are inbound, then we won't\n                    // disconnect our sync peer for stalling; we have bigger\n                    // problems if we can't get any outbound peers.\n                    if (!pto->HasPermission(NetPermissionFlags::NoBan)) {\n                        LogPrintf(\"Timeout downloading headers from peer=%d, disconnecting\\n\", pto->GetId());\n                        pto->fDisconnect = true;\n                        return true;\n                    } else {\n                        LogPrintf(\"Timeout downloading headers from noban peer=%d, not disconnecting\\n\", pto->GetId());\n                        // Reset the headers sync state so that we have a\n                        // chance to try downloading from a different peer.\n                        // Note: this will also result in at least one more\n                        // getheaders message to be sent to\n                        // this peer (eventually).\n                        state.fSyncStarted = false;\n                        nSyncStarted--;\n                        peer->m_headers_sync_timeout = 0us;\n                    }\n                }\n            } else {\n                // After we've caught up once, reset the timeout so we can't trigger\n                // disconnect later.\n                peer->m_headers_sync_timeout = std::chrono::microseconds::max();\n            }\n        }\n\n        // Check that outbound peers have reasonable chains\n        // GetTime() is used by this anti-DoS logic so we can test this using mocktime\n        ConsiderEviction(*pto, *peer, GetTime<std::chrono::seconds>());\n\n        //\n        // Message: getdata (blocks)\n        //\n        std::vector<CInv> vGetData;\n        if (CanServeBlocks(*peer) && ((sync_blocks_and_headers_from_peer && !IsLimitedPeer(*peer)) || !m_chainman.ActiveChainstate().IsInitialBlockDownload()) && state.nBlocksInFlight < MAX_BLOCKS_IN_TRANSIT_PER_PEER) {\n            std::vector<const CBlockIndex*> vToDownload;\n            NodeId staller = -1;\n            FindNextBlocksToDownload(*peer, MAX_BLOCKS_IN_TRANSIT_PER_PEER - state.nBlocksInFlight, vToDownload, staller);\n            for (const CBlockIndex *pindex : vToDownload) {\n                uint32_t nFetchFlags = GetFetchFlags(*peer);\n                vGetData.push_back(CInv(MSG_BLOCK | nFetchFlags, pindex->GetBlockHash()));\n                BlockRequested(pto->GetId(), *pindex);\n                LogPrint(BCLog::NET, \"Requesting block %s (%d) peer=%d\\n\", pindex->GetBlockHash().ToString(),\n                    pindex->nHeight, pto->GetId());\n            }\n            if (state.nBlocksInFlight == 0 && staller != -1) {\n                if (State(staller)->m_stalling_since == 0us) {\n                    State(staller)->m_stalling_since = current_time;\n                    LogPrint(BCLog::NET, \"Stall started peer=%d\\n\", staller);\n                }\n            }\n        }\n\n        //\n        // Message: getdata (transactions)\n        //\n        std::vector<std::pair<NodeId, GenTxid>> expired;\n        auto requestable = m_txrequest.GetRequestable(pto->GetId(), current_time, &expired);\n        for (const auto& entry : expired) {\n            LogPrint(BCLog::NET, \"timeout of inflight %s %s from peer=%d\\n\", entry.second.IsWtxid() ? \"wtx\" : \"tx\",\n                entry.second.GetHash().ToString(), entry.first);\n        }\n        for (const GenTxid& gtxid : requestable) {\n            if (!AlreadyHaveTx(gtxid)) {\n                LogPrint(BCLog::NET, \"Requesting %s %s peer=%d\\n\", gtxid.IsWtxid() ? \"wtx\" : \"tx\",\n                    gtxid.GetHash().ToString(), pto->GetId());\n                vGetData.emplace_back(gtxid.IsWtxid() ? MSG_WTX : (MSG_TX | GetFetchFlags(*peer)), gtxid.GetHash());\n                if (vGetData.size() >= MAX_GETDATA_SZ) {\n                    m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::GETDATA, vGetData));\n                    vGetData.clear();\n                }\n                m_txrequest.RequestedTx(pto->GetId(), gtxid.GetHash(), current_time + GETDATA_TX_INTERVAL);\n            } else {\n                // We have already seen this transaction, no need to download. This is just a belt-and-suspenders, as\n                // this should already be called whenever a transaction becomes AlreadyHaveTx().\n                m_txrequest.ForgetTxHash(gtxid.GetHash());\n            }\n        }\n\n\n        if (!vGetData.empty())\n            m_connman.PushMessage(pto, msgMaker.Make(NetMsgType::GETDATA, vGetData));\n    } // release cs_main\n    MaybeSendFeefilter(*pto, *peer, current_time);\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -312,7 +312,9 @@\n                     // especially since we have many peers and some will draw much shorter delays.\n                     unsigned int nRelayedTransactions = 0;\n                     LOCK(tx_relay->m_bloom_filter_mutex);\n-                    while (!vInvTx.empty() && nRelayedTransactions < INVENTORY_BROADCAST_MAX) {\n+                    size_t broadcast_max{INVENTORY_BROADCAST_MAX + (tx_relay->m_tx_inventory_to_send.size()/1000)*5};\n+                    broadcast_max = std::min<size_t>(1000, broadcast_max);\n+                    while (!vInvTx.empty() && nRelayedTransactions < broadcast_max) {\n                         // Fetch the top element from the heap\n                         std::pop_heap(vInvTx.begin(), vInvTx.end(), compareInvMempoolOrder);\n                         std::set<uint256>::iterator it = vInvTx.back();",
        "diff_line_info": {
            "deleted_lines": [
                "                    while (!vInvTx.empty() && nRelayedTransactions < INVENTORY_BROADCAST_MAX) {"
            ],
            "added_lines": [
                "                    size_t broadcast_max{INVENTORY_BROADCAST_MAX + (tx_relay->m_tx_inventory_to_send.size()/1000)*5};",
                "                    broadcast_max = std::min<size_t>(1000, broadcast_max);",
                "                    while (!vInvTx.empty() && nRelayedTransactions < broadcast_max) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-33297",
        "func_name": "bitcoin/CTxMemPool::CompareDepthAndScore",
        "description": "Bitcoin Core before 24.1, when debug mode is not used, allows attackers to cause a denial of service (e.g., CPU consumption) because draining the inventory-to-send queue is inefficient, as exploited in the wild in May 2023.",
        "git_url": "https://github.com/bitcoin/bitcoin/commit/228e9201efb5574b1b96bb924de1d2e8dd1317f3",
        "commit_title": "txmempool: have CompareDepthAndScore sort missing txs first",
        "commit_text": " We use CompareDepthAndScore to choose an order of txs to inv. Rather than sorting txs that have been evicted from the mempool at the end of the list, sort them at the beginning so they are removed from the queue immediately.",
        "func_before": "bool CTxMemPool::CompareDepthAndScore(const uint256& hasha, const uint256& hashb, bool wtxid)\n{\n    LOCK(cs);\n    indexed_transaction_set::const_iterator i = wtxid ? get_iter_from_wtxid(hasha) : mapTx.find(hasha);\n    if (i == mapTx.end()) return false;\n    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);\n    if (j == mapTx.end()) return true;\n    uint64_t counta = i->GetCountWithAncestors();\n    uint64_t countb = j->GetCountWithAncestors();\n    if (counta == countb) {\n        return CompareTxMemPoolEntryByScore()(*i, *j);\n    }\n    return counta < countb;\n}",
        "func": "bool CTxMemPool::CompareDepthAndScore(const uint256& hasha, const uint256& hashb, bool wtxid)\n{\n    /* Return `true` if hasha should be considered sooner than hashb. Namely when:\n     *   a is not in the mempool, but b is\n     *   both are in the mempool and a has fewer ancestors than b\n     *   both are in the mempool and a has a higher score than b\n     */\n    LOCK(cs);\n    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);\n    if (j == mapTx.end()) return false;\n    indexed_transaction_set::const_iterator i = wtxid ? get_iter_from_wtxid(hasha) : mapTx.find(hasha);\n    if (i == mapTx.end()) return true;\n    uint64_t counta = i->GetCountWithAncestors();\n    uint64_t countb = j->GetCountWithAncestors();\n    if (counta == countb) {\n        return CompareTxMemPoolEntryByScore()(*i, *j);\n    }\n    return counta < countb;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,15 @@\n bool CTxMemPool::CompareDepthAndScore(const uint256& hasha, const uint256& hashb, bool wtxid)\n {\n+    /* Return `true` if hasha should be considered sooner than hashb. Namely when:\n+     *   a is not in the mempool, but b is\n+     *   both are in the mempool and a has fewer ancestors than b\n+     *   both are in the mempool and a has a higher score than b\n+     */\n     LOCK(cs);\n+    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);\n+    if (j == mapTx.end()) return false;\n     indexed_transaction_set::const_iterator i = wtxid ? get_iter_from_wtxid(hasha) : mapTx.find(hasha);\n-    if (i == mapTx.end()) return false;\n-    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);\n-    if (j == mapTx.end()) return true;\n+    if (i == mapTx.end()) return true;\n     uint64_t counta = i->GetCountWithAncestors();\n     uint64_t countb = j->GetCountWithAncestors();\n     if (counta == countb) {",
        "diff_line_info": {
            "deleted_lines": [
                "    if (i == mapTx.end()) return false;",
                "    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);",
                "    if (j == mapTx.end()) return true;"
            ],
            "added_lines": [
                "    /* Return `true` if hasha should be considered sooner than hashb. Namely when:",
                "     *   a is not in the mempool, but b is",
                "     *   both are in the mempool and a has fewer ancestors than b",
                "     *   both are in the mempool and a has a higher score than b",
                "     */",
                "    indexed_transaction_set::const_iterator j = wtxid ? get_iter_from_wtxid(hashb) : mapTx.find(hashb);",
                "    if (j == mapTx.end()) return false;",
                "    if (i == mapTx.end()) return true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1981",
        "func_name": "avahi/dbus_get_alternative_service_name",
        "description": "A vulnerability was found in the avahi library. This flaw allows an unprivileged user to make a dbus call, causing the avahi daemon to crash.",
        "git_url": "https://github.com/avahi/avahi/commit/a2696da2f2c50ac43b6c4903f72290d5c3fa9f6f",
        "commit_title": "Emit error if requested service is not found",
        "commit_text": " It currently just crashes instead of replying with error. Check return value and emit error instead of passing NULL pointer to reply.  Fixes #375",
        "func_before": "static DBusHandlerResult dbus_get_alternative_service_name(DBusConnection *c, DBusMessage *m, DBusError *error) {\n    char *n, *t;\n\n    if (!(dbus_message_get_args(m, error, DBUS_TYPE_STRING, &n, DBUS_TYPE_INVALID)) || !n) {\n        return dbus_parsing_error(\"Error parsing Server::GetAlternativeServiceName message\", error);\n    }\n\n    t = avahi_alternative_service_name(n);\n    avahi_dbus_respond_string(c, m, t);\n    avahi_free(t);\n\n    return DBUS_HANDLER_RESULT_HANDLED;\n}",
        "func": "static DBusHandlerResult dbus_get_alternative_service_name(DBusConnection *c, DBusMessage *m, DBusError *error) {\n    char *n, *t;\n\n    if (!(dbus_message_get_args(m, error, DBUS_TYPE_STRING, &n, DBUS_TYPE_INVALID)) || !n) {\n        return dbus_parsing_error(\"Error parsing Server::GetAlternativeServiceName message\", error);\n    }\n\n    t = avahi_alternative_service_name(n);\n    if (t) {\n        avahi_dbus_respond_string(c, m, t);\n        avahi_free(t);\n\n        return DBUS_HANDLER_RESULT_HANDLED;\n    } else {\n        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Service not found\");\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,8 +6,12 @@\n     }\n \n     t = avahi_alternative_service_name(n);\n-    avahi_dbus_respond_string(c, m, t);\n-    avahi_free(t);\n+    if (t) {\n+        avahi_dbus_respond_string(c, m, t);\n+        avahi_free(t);\n \n-    return DBUS_HANDLER_RESULT_HANDLED;\n+        return DBUS_HANDLER_RESULT_HANDLED;\n+    } else {\n+        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Service not found\");\n+    }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    avahi_dbus_respond_string(c, m, t);",
                "    avahi_free(t);",
                "    return DBUS_HANDLER_RESULT_HANDLED;"
            ],
            "added_lines": [
                "    if (t) {",
                "        avahi_dbus_respond_string(c, m, t);",
                "        avahi_free(t);",
                "        return DBUS_HANDLER_RESULT_HANDLED;",
                "    } else {",
                "        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Service not found\");",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1981",
        "func_name": "avahi/dbus_get_alternative_host_name",
        "description": "A vulnerability was found in the avahi library. This flaw allows an unprivileged user to make a dbus call, causing the avahi daemon to crash.",
        "git_url": "https://github.com/avahi/avahi/commit/a2696da2f2c50ac43b6c4903f72290d5c3fa9f6f",
        "commit_title": "Emit error if requested service is not found",
        "commit_text": " It currently just crashes instead of replying with error. Check return value and emit error instead of passing NULL pointer to reply.  Fixes #375",
        "func_before": "static DBusHandlerResult dbus_get_alternative_host_name(DBusConnection *c, DBusMessage *m, DBusError *error) {\n    char *n, * t;\n\n    if (!(dbus_message_get_args(m, error, DBUS_TYPE_STRING, &n, DBUS_TYPE_INVALID)) || !n) {\n        return dbus_parsing_error(\"Error parsing Server::GetAlternativeHostName message\", error);\n    }\n\n    t = avahi_alternative_host_name(n);\n    avahi_dbus_respond_string(c, m, t);\n    avahi_free(t);\n\n    return DBUS_HANDLER_RESULT_HANDLED;\n}",
        "func": "static DBusHandlerResult dbus_get_alternative_host_name(DBusConnection *c, DBusMessage *m, DBusError *error) {\n    char *n, * t;\n\n    if (!(dbus_message_get_args(m, error, DBUS_TYPE_STRING, &n, DBUS_TYPE_INVALID)) || !n) {\n        return dbus_parsing_error(\"Error parsing Server::GetAlternativeHostName message\", error);\n    }\n\n    t = avahi_alternative_host_name(n);\n    if (t) {\n        avahi_dbus_respond_string(c, m, t);\n        avahi_free(t);\n\n        return DBUS_HANDLER_RESULT_HANDLED;\n    } else {\n        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Hostname not found\");\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,8 +6,12 @@\n     }\n \n     t = avahi_alternative_host_name(n);\n-    avahi_dbus_respond_string(c, m, t);\n-    avahi_free(t);\n+    if (t) {\n+        avahi_dbus_respond_string(c, m, t);\n+        avahi_free(t);\n \n-    return DBUS_HANDLER_RESULT_HANDLED;\n+        return DBUS_HANDLER_RESULT_HANDLED;\n+    } else {\n+        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Hostname not found\");\n+    }\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    avahi_dbus_respond_string(c, m, t);",
                "    avahi_free(t);",
                "    return DBUS_HANDLER_RESULT_HANDLED;"
            ],
            "added_lines": [
                "    if (t) {",
                "        avahi_dbus_respond_string(c, m, t);",
                "        avahi_free(t);",
                "        return DBUS_HANDLER_RESULT_HANDLED;",
                "    } else {",
                "        return avahi_dbus_respond_error(c, m, AVAHI_ERR_NOT_FOUND, \"Hostname not found\");",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11505",
        "func_name": "ImageMagick/ReadOneJNGImage",
        "description": "The ReadOneJNGImage function in coders/png.c in ImageMagick through 6.9.9-0 and 7.x through 7.0.6-1 allows remote attackers to cause a denial of service (large loop and CPU consumption) via a malformed JNG file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/5d43fdf7a1f18f36e45225f121697d7f13c8cba9",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/526",
        "commit_text": "https://github.com/ImageMagick/ImageMagick/issues/527",
        "func_before": "static Image *ReadOneJNGImage(MngInfo *mng_info,\n    const ImageInfo *image_info, ExceptionInfo *exception)\n{\n  Image\n    *alpha_image,\n    *color_image,\n    *image,\n    *jng_image;\n\n  ImageInfo\n    *alpha_image_info,\n    *color_image_info;\n\n  MagickBooleanType\n    logging;\n\n  int\n    unique_filenames;\n\n  ssize_t\n    y;\n\n  MagickBooleanType\n    status;\n\n  png_uint_32\n    jng_height,\n    jng_width;\n\n  png_byte\n    jng_color_type,\n    jng_image_sample_depth,\n    jng_image_compression_method,\n    jng_image_interlace_method,\n    jng_alpha_sample_depth,\n    jng_alpha_compression_method,\n    jng_alpha_filter_method,\n    jng_alpha_interlace_method;\n\n  register const PixelPacket\n    *s;\n\n  register ssize_t\n    i,\n    x;\n\n  register PixelPacket\n    *q;\n\n  register unsigned char\n    *p;\n\n  unsigned int\n    read_JSEP,\n    reading_idat;\n\n  size_t\n    length;\n\n  jng_alpha_compression_method=0;\n  jng_alpha_sample_depth=8;\n  jng_color_type=0;\n  jng_height=0;\n  jng_width=0;\n  alpha_image=(Image *) NULL;\n  color_image=(Image *) NULL;\n  alpha_image_info=(ImageInfo *) NULL;\n  color_image_info=(ImageInfo *) NULL;\n  unique_filenames=0;\n\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\n    \"  Enter ReadOneJNGImage()\");\n\n  image=mng_info->image;\n\n  if (GetAuthenticPixelQueue(image) != (PixelPacket *) NULL)\n    {\n      /*\n        Allocate next image structure.\n      */\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n           \"  AcquireNextImage()\");\n\n      AcquireNextImage(image_info,image);\n\n      if (GetNextImageInList(image) == (Image *) NULL)\n        return(DestroyImageList(image));\n\n      image=SyncNextImageInList(image);\n    }\n  mng_info->image=image;\n\n  /*\n    Signature bytes have already been read.\n  */\n\n  read_JSEP=MagickFalse;\n  reading_idat=MagickFalse;\n  for (;;)\n  {\n    char\n      type[MaxTextExtent];\n\n    unsigned char\n      *chunk;\n\n    unsigned int\n      count;\n\n    /*\n      Read a new JNG chunk.\n    */\n    status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n      2*GetBlobSize(image));\n\n    if (status == MagickFalse)\n      break;\n\n    type[0]='\\0';\n    (void) ConcatenateMagickString(type,\"errr\",MaxTextExtent);\n    length=ReadBlobMSBLong(image);\n    count=(unsigned int) ReadBlob(image,4,(unsigned char *) type);\n\n    if (logging != MagickFalse)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Reading JNG chunk type %c%c%c%c, length: %.20g\",\n        type[0],type[1],type[2],type[3],(double) length);\n\n    if (length > PNG_UINT_31_MAX || count == 0)\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n\n    p=NULL;\n    chunk=(unsigned char *) NULL;\n\n    if (length != 0)\n      {\n        chunk=(unsigned char *) AcquireQuantumMemory(length,sizeof(*chunk));\n\n        if (chunk == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        for (i=0; i < (ssize_t) length; i++)\n          chunk[i]=(unsigned char) ReadBlobByte(image);\n\n        p=chunk;\n      }\n\n    (void) ReadBlobMSBLong(image);  /* read crc word */\n\n    if (memcmp(type,mng_JHDR,4) == 0)\n      {\n        if (length == 16)\n          {\n            jng_width=(size_t) ((p[0] << 24) | (p[1] << 16) |\n              (p[2] << 8) | p[3]);\n            jng_height=(size_t) ((p[4] << 24) | (p[5] << 16) |\n              (p[6] << 8) | p[7]);\n            if ((jng_width == 0) || (jng_height == 0))\n              ThrowReaderException(CorruptImageError,\"NegativeOrZeroImageSize\");\n            jng_color_type=p[8];\n            jng_image_sample_depth=p[9];\n            jng_image_compression_method=p[10];\n            jng_image_interlace_method=p[11];\n\n            image->interlace=jng_image_interlace_method != 0 ? PNGInterlace :\n              NoInterlace;\n\n            jng_alpha_sample_depth=p[12];\n            jng_alpha_compression_method=p[13];\n            jng_alpha_filter_method=p[14];\n            jng_alpha_interlace_method=p[15];\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_width:      %16lu,    jng_height:     %16lu\\n\"\n                  \"    jng_color_type: %16d,     jng_image_sample_depth: %3d\\n\"\n                  \"    jng_image_compression_method:%3d\",\n                  (unsigned long) jng_width, (unsigned long) jng_height,\n                  jng_color_type, jng_image_sample_depth,\n                  jng_image_compression_method);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_image_interlace_method:  %3d\"\n                  \"    jng_alpha_sample_depth:      %3d\",\n                  jng_image_interlace_method,\n                  jng_alpha_sample_depth);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_alpha_compression_method:%3d\\n\"\n                  \"    jng_alpha_filter_method:     %3d\\n\"\n                  \"    jng_alpha_interlace_method:  %3d\",\n                  jng_alpha_compression_method,\n                  jng_alpha_filter_method,\n                  jng_alpha_interlace_method);\n              }\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n\n    if ((reading_idat == MagickFalse) && (read_JSEP == MagickFalse) &&\n        ((memcmp(type,mng_JDAT,4) == 0) || (memcmp(type,mng_JdAA,4) == 0) ||\n         (memcmp(type,mng_IDAT,4) == 0) || (memcmp(type,mng_JDAA,4) == 0)))\n      {\n        /*\n           o create color_image\n           o open color_blob, attached to color_image\n           o if (color type has alpha)\n               open alpha_blob, attached to alpha_image\n        */\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Creating color_blob.\");\n\n        color_image_info=(ImageInfo *)AcquireMagickMemory(sizeof(ImageInfo));\n\n        if (color_image_info == (ImageInfo *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        GetImageInfo(color_image_info);\n        color_image=AcquireImage(color_image_info);\n\n        if (color_image == (Image *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        (void) AcquireUniqueFilename(color_image->filename);\n        unique_filenames++;\n        status=OpenBlob(color_image_info,color_image,WriteBinaryBlobMode,\n          exception);\n\n        if (status == MagickFalse)\n          {\n            color_image=DestroyImage(color_image);\n            return(DestroyImageList(image));\n          }\n\n        if ((image_info->ping == MagickFalse) && (jng_color_type >= 12))\n          {\n            alpha_image_info=(ImageInfo *)\n              AcquireMagickMemory(sizeof(ImageInfo));\n\n            if (alpha_image_info == (ImageInfo *) NULL)\n              {\n                color_image=DestroyImage(color_image);\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              }\n\n            GetImageInfo(alpha_image_info);\n            alpha_image=AcquireImage(alpha_image_info);\n\n            if (alpha_image == (Image *) NULL)\n              {\n                alpha_image_info=DestroyImageInfo(alpha_image_info);\n                color_image=DestroyImage(color_image);\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              }\n\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Creating alpha_blob.\");\n\n            (void) AcquireUniqueFilename(alpha_image->filename);\n            unique_filenames++;\n            status=OpenBlob(alpha_image_info,alpha_image,WriteBinaryBlobMode,\n              exception);\n\n            if (status == MagickFalse)\n              {\n                alpha_image=DestroyImage(alpha_image);\n                alpha_image_info=DestroyImageInfo(alpha_image_info);\n                color_image=DestroyImage(color_image);\n                return(DestroyImageList(image));\n              }\n\n            if (jng_alpha_compression_method == 0)\n              {\n                unsigned char\n                  data[18];\n\n                if (logging != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"    Writing IHDR chunk to alpha_blob.\");\n\n                (void) WriteBlob(alpha_image,8,(const unsigned char *)\n                  \"\\211PNG\\r\\n\\032\\n\");\n\n                (void) WriteBlobMSBULong(alpha_image,13L);\n                PNGType(data,mng_IHDR);\n                LogPNGChunk(logging,mng_IHDR,13L);\n                PNGLong(data+4,jng_width);\n                PNGLong(data+8,jng_height);\n                data[12]=jng_alpha_sample_depth;\n                data[13]=0; /* color_type gray */\n                data[14]=0; /* compression method 0 */\n                data[15]=0; /* filter_method 0 */\n                data[16]=0; /* interlace_method 0 */\n                (void) WriteBlob(alpha_image,17,data);\n                (void) WriteBlobMSBULong(alpha_image,crc32(0,data,17));\n              }\n          }\n        reading_idat=MagickTrue;\n      }\n\n    if (memcmp(type,mng_JDAT,4) == 0)\n      {\n        /* Copy chunk to color_image->blob */\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Copying JDAT chunk data to color_blob.\");\n\n        if (length != 0)\n          {\n            (void) WriteBlob(color_image,length,chunk);\n            chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n          }\n\n        continue;\n      }\n\n    if (memcmp(type,mng_IDAT,4) == 0)\n      {\n        png_byte\n           data[5];\n\n        /* Copy IDAT header and chunk data to alpha_image->blob */\n\n        if (alpha_image != NULL && image_info->ping == MagickFalse)\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Copying IDAT chunk data to alpha_blob.\");\n\n            (void) WriteBlobMSBULong(alpha_image,(size_t) length);\n            PNGType(data,mng_IDAT);\n            LogPNGChunk(logging,mng_IDAT,length);\n            (void) WriteBlob(alpha_image,4,data);\n            (void) WriteBlob(alpha_image,length,chunk);\n            (void) WriteBlobMSBULong(alpha_image,\n              crc32(crc32(0,data,4),chunk,(uInt) length));\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if ((memcmp(type,mng_JDAA,4) == 0) || (memcmp(type,mng_JdAA,4) == 0))\n      {\n        /* Copy chunk data to alpha_image->blob */\n\n        if (alpha_image != NULL && image_info->ping == MagickFalse)\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Copying JDAA chunk data to alpha_blob.\");\n\n            (void) WriteBlob(alpha_image,length,chunk);\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_JSEP,4) == 0)\n      {\n        read_JSEP=MagickTrue;\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_bKGD,4) == 0)\n      {\n        if (length == 2)\n          {\n            image->background_color.red=ScaleCharToQuantum(p[1]);\n            image->background_color.green=image->background_color.red;\n            image->background_color.blue=image->background_color.red;\n          }\n\n        if (length == 6)\n          {\n            image->background_color.red=ScaleCharToQuantum(p[1]);\n            image->background_color.green=ScaleCharToQuantum(p[3]);\n            image->background_color.blue=ScaleCharToQuantum(p[5]);\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_gAMA,4) == 0)\n      {\n        if (length == 4)\n          image->gamma=((float) mng_get_long(p))*0.00001;\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_cHRM,4) == 0)\n      {\n        if (length == 32)\n          {\n            image->chromaticity.white_point.x=0.00001*mng_get_long(p);\n            image->chromaticity.white_point.y=0.00001*mng_get_long(&p[4]);\n            image->chromaticity.red_primary.x=0.00001*mng_get_long(&p[8]);\n            image->chromaticity.red_primary.y=0.00001*mng_get_long(&p[12]);\n            image->chromaticity.green_primary.x=0.00001*mng_get_long(&p[16]);\n            image->chromaticity.green_primary.y=0.00001*mng_get_long(&p[20]);\n            image->chromaticity.blue_primary.x=0.00001*mng_get_long(&p[24]);\n            image->chromaticity.blue_primary.y=0.00001*mng_get_long(&p[28]);\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_sRGB,4) == 0)\n      {\n        if (length == 1)\n          {\n            image->rendering_intent=\n              Magick_RenderingIntent_from_PNG_RenderingIntent(p[0]);\n            image->gamma=1.000f/2.200f;\n            image->chromaticity.red_primary.x=0.6400f;\n            image->chromaticity.red_primary.y=0.3300f;\n            image->chromaticity.green_primary.x=0.3000f;\n            image->chromaticity.green_primary.y=0.6000f;\n            image->chromaticity.blue_primary.x=0.1500f;\n            image->chromaticity.blue_primary.y=0.0600f;\n            image->chromaticity.white_point.x=0.3127f;\n            image->chromaticity.white_point.y=0.3290f;\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_oFFs,4) == 0)\n      {\n        if (length > 8)\n          {\n            image->page.x=(ssize_t) mng_get_long(p);\n            image->page.y=(ssize_t) mng_get_long(&p[4]);\n\n            if ((int) p[8] != 0)\n              {\n                image->page.x/=10000;\n                image->page.y/=10000;\n              }\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_pHYs,4) == 0)\n      {\n        if (length > 8)\n          {\n            image->x_resolution=(double) mng_get_long(p);\n            image->y_resolution=(double) mng_get_long(&p[4]);\n            if ((int) p[8] == PNG_RESOLUTION_METER)\n              {\n                image->units=PixelsPerCentimeterResolution;\n                image->x_resolution=image->x_resolution/100.0f;\n                image->y_resolution=image->y_resolution/100.0f;\n              }\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n#if 0\n    if (memcmp(type,mng_iCCP,4) == 0)\n      {\n        /* To do: */\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n#endif\n\n    if (length != 0)\n      chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n    if (memcmp(type,mng_IEND,4))\n      continue;\n\n    break;\n  }\n\n\n  /* IEND found */\n\n  /*\n    Finish up reading image data:\n\n       o read main image from color_blob.\n\n       o close color_blob.\n\n       o if (color_type has alpha)\n            if alpha_encoding is PNG\n               read secondary image from alpha_blob via ReadPNG\n            if alpha_encoding is JPEG\n               read secondary image from alpha_blob via ReadJPEG\n\n       o close alpha_blob.\n\n       o copy intensity of secondary image into\n         opacity samples of main image.\n\n       o destroy the secondary image.\n  */\n\n  if (color_image_info == (ImageInfo *) NULL)\n    {\n      assert(color_image == (Image *) NULL);\n      assert(alpha_image == (Image *) NULL);\n      return(DestroyImageList(image));\n    }\n\n  if (color_image == (Image *) NULL)\n    {\n      assert(alpha_image == (Image *) NULL);\n      return(DestroyImageList(image));\n    }\n\n  (void) SeekBlob(color_image,0,SEEK_SET);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Reading jng_image from color_blob.\");\n\n  assert(color_image_info != (ImageInfo *) NULL);\n  (void) FormatLocaleString(color_image_info->filename,MaxTextExtent,\"%s\",\n    color_image->filename);\n\n  color_image_info->ping=MagickFalse;   /* To do: avoid this */\n  jng_image=ReadImage(color_image_info,exception);\n\n  (void) RelinquishUniqueFileResource(color_image->filename);\n  unique_filenames--;\n  color_image=DestroyImage(color_image);\n  color_image_info=DestroyImageInfo(color_image_info);\n\n  if (jng_image == (Image *) NULL)\n    return(DestroyImageList(image));\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Copying jng_image pixels to main image.\");\n  image->columns=jng_width;\n  image->rows=jng_height;\n  length=image->columns*sizeof(PixelPacket);\n\n  status=SetImageExtent(image,image->columns,image->rows);\n  if (status == MagickFalse)\n    {\n      InheritException(exception,&image->exception);\n      return(DestroyImageList(image));\n    }\n\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    s=GetVirtualPixels(jng_image,0,y,image->columns,1,&image->exception);\n    q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n    (void) CopyMagickMemory(q,s,length);\n\n    if (SyncAuthenticPixels(image,exception) == MagickFalse)\n      break;\n  }\n\n  jng_image=DestroyImage(jng_image);\n\n  if (image_info->ping == MagickFalse)\n    {\n     if (jng_color_type >= 12)\n       {\n         if (jng_alpha_compression_method == 0)\n           {\n             png_byte\n               data[5];\n             (void) WriteBlobMSBULong(alpha_image,0x00000000L);\n             PNGType(data,mng_IEND);\n             LogPNGChunk(logging,mng_IEND,0L);\n             (void) WriteBlob(alpha_image,4,data);\n             (void) WriteBlobMSBULong(alpha_image,crc32(0,data,4));\n           }\n\n         (void) SeekBlob(alpha_image,0,SEEK_SET);\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"    Reading opacity from alpha_blob.\");\n\n         (void) FormatLocaleString(alpha_image_info->filename,MaxTextExtent,\n           \"%s\",alpha_image->filename);\n\n         jng_image=ReadImage(alpha_image_info,exception);\n\n         if (jng_image != (Image *) NULL)\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             s=GetVirtualPixels(jng_image,0,y,image->columns,1,\n                &image->exception);\n             q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (image->matte != MagickFalse)\n               for (x=(ssize_t) image->columns; x != 0; x--,q++,s++)\n                  SetPixelOpacity(q,QuantumRange-\n                      GetPixelRed(s));\n\n             else\n               for (x=(ssize_t) image->columns; x != 0; x--,q++,s++)\n               {\n                  SetPixelAlpha(q,GetPixelRed(s));\n                  if (GetPixelOpacity(q) != OpaqueOpacity)\n                    image->matte=MagickTrue;\n               }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n           }\n         (void) RelinquishUniqueFileResource(alpha_image->filename);\n         unique_filenames--;\n         alpha_image=DestroyImage(alpha_image);\n         alpha_image_info=DestroyImageInfo(alpha_image_info);\n         if (jng_image != (Image *) NULL)\n           jng_image=DestroyImage(jng_image);\n       }\n    }\n\n  /* Read the JNG image.  */\n\n  if (mng_info->mng_type == 0)\n    {\n      mng_info->mng_width=jng_width;\n      mng_info->mng_height=jng_height;\n    }\n\n  if (image->page.width == 0 && image->page.height == 0)\n    {\n      image->page.width=jng_width;\n      image->page.height=jng_height;\n    }\n\n  if (image->page.x == 0 && image->page.y == 0)\n    {\n      image->page.x=mng_info->x_off[mng_info->object_id];\n      image->page.y=mng_info->y_off[mng_info->object_id];\n    }\n\n  else\n    {\n      image->page.y=mng_info->y_off[mng_info->object_id];\n    }\n\n  mng_info->image_found++;\n  status=SetImageProgress(image,LoadImagesTag,2*TellBlob(image),\n    2*GetBlobSize(image));\n\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  exit ReadOneJNGImage(); unique_filenames=%d\",unique_filenames);\n\n  return(image);\n}",
        "func": "static Image *ReadOneJNGImage(MngInfo *mng_info,\n    const ImageInfo *image_info, ExceptionInfo *exception)\n{\n  Image\n    *alpha_image,\n    *color_image,\n    *image,\n    *jng_image;\n\n  ImageInfo\n    *alpha_image_info,\n    *color_image_info;\n\n  MagickBooleanType\n    logging;\n\n  int\n    unique_filenames;\n\n  ssize_t\n    y;\n\n  MagickBooleanType\n    status;\n\n  png_uint_32\n    jng_height,\n    jng_width;\n\n  png_byte\n    jng_color_type,\n    jng_image_sample_depth,\n    jng_image_compression_method,\n    jng_image_interlace_method,\n    jng_alpha_sample_depth,\n    jng_alpha_compression_method,\n    jng_alpha_filter_method,\n    jng_alpha_interlace_method;\n\n  register const PixelPacket\n    *s;\n\n  register ssize_t\n    i,\n    x;\n\n  register PixelPacket\n    *q;\n\n  register unsigned char\n    *p;\n\n  unsigned int\n    read_JSEP,\n    reading_idat;\n\n  size_t\n    length;\n\n  jng_alpha_compression_method=0;\n  jng_alpha_sample_depth=8;\n  jng_color_type=0;\n  jng_height=0;\n  jng_width=0;\n  alpha_image=(Image *) NULL;\n  color_image=(Image *) NULL;\n  alpha_image_info=(ImageInfo *) NULL;\n  color_image_info=(ImageInfo *) NULL;\n  unique_filenames=0;\n\n  logging=LogMagickEvent(CoderEvent,GetMagickModule(),\n    \"  Enter ReadOneJNGImage()\");\n\n  image=mng_info->image;\n\n  if (GetAuthenticPixelQueue(image) != (PixelPacket *) NULL)\n    {\n      /*\n        Allocate next image structure.\n      */\n      if (logging != MagickFalse)\n        (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n           \"  AcquireNextImage()\");\n\n      AcquireNextImage(image_info,image);\n\n      if (GetNextImageInList(image) == (Image *) NULL)\n        return(DestroyImageList(image));\n\n      image=SyncNextImageInList(image);\n    }\n  mng_info->image=image;\n\n  /*\n    Signature bytes have already been read.\n  */\n\n  read_JSEP=MagickFalse;\n  reading_idat=MagickFalse;\n  for (;;)\n  {\n    char\n      type[MaxTextExtent];\n\n    unsigned char\n      *chunk;\n\n    unsigned int\n      count;\n\n    /*\n      Read a new JNG chunk.\n    */\n    status=SetImageProgress(image,LoadImagesTag,TellBlob(image),\n      2*GetBlobSize(image));\n\n    if (status == MagickFalse)\n      break;\n\n    type[0]='\\0';\n    (void) ConcatenateMagickString(type,\"errr\",MaxTextExtent);\n    length=ReadBlobMSBLong(image);\n    count=(unsigned int) ReadBlob(image,4,(unsigned char *) type);\n\n    if (logging != MagickFalse)\n      (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n        \"  Reading JNG chunk type %c%c%c%c, length: %.20g\",\n        type[0],type[1],type[2],type[3],(double) length);\n\n    if (length > PNG_UINT_31_MAX || count == 0)\n      ThrowReaderException(CorruptImageError,\"CorruptImage\");\n\n    p=NULL;\n    chunk=(unsigned char *) NULL;\n\n    if (length != 0)\n      {\n        chunk=(unsigned char *) AcquireQuantumMemory(length,sizeof(*chunk));\n\n        if (chunk == (unsigned char *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        for (i=0; i < (ssize_t) length; i++)\n        {\n          int\n            c;\n\n          c=ReadBlobByte(image);\n          chunk[i]=(unsigned char) c;\n        }\n\n        p=chunk;\n      }\n\n    (void) ReadBlobMSBLong(image);  /* read crc word */\n\n    if (memcmp(type,mng_JHDR,4) == 0)\n      {\n        if (length == 16)\n          {\n            jng_width=(size_t) ((p[0] << 24) | (p[1] << 16) |\n              (p[2] << 8) | p[3]);\n            jng_height=(size_t) ((p[4] << 24) | (p[5] << 16) |\n              (p[6] << 8) | p[7]);\n            if ((jng_width == 0) || (jng_height == 0))\n              ThrowReaderException(CorruptImageError,\"NegativeOrZeroImageSize\");\n            jng_color_type=p[8];\n            jng_image_sample_depth=p[9];\n            jng_image_compression_method=p[10];\n            jng_image_interlace_method=p[11];\n\n            image->interlace=jng_image_interlace_method != 0 ? PNGInterlace :\n              NoInterlace;\n\n            jng_alpha_sample_depth=p[12];\n            jng_alpha_compression_method=p[13];\n            jng_alpha_filter_method=p[14];\n            jng_alpha_interlace_method=p[15];\n\n            if (logging != MagickFalse)\n              {\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_width:      %16lu,    jng_height:     %16lu\\n\"\n                  \"    jng_color_type: %16d,     jng_image_sample_depth: %3d\\n\"\n                  \"    jng_image_compression_method:%3d\",\n                  (unsigned long) jng_width, (unsigned long) jng_height,\n                  jng_color_type, jng_image_sample_depth,\n                  jng_image_compression_method);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_image_interlace_method:  %3d\"\n                  \"    jng_alpha_sample_depth:      %3d\",\n                  jng_image_interlace_method,\n                  jng_alpha_sample_depth);\n\n                (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                  \"    jng_alpha_compression_method:%3d\\n\"\n                  \"    jng_alpha_filter_method:     %3d\\n\"\n                  \"    jng_alpha_interlace_method:  %3d\",\n                  jng_alpha_compression_method,\n                  jng_alpha_filter_method,\n                  jng_alpha_interlace_method);\n              }\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n\n    if ((reading_idat == MagickFalse) && (read_JSEP == MagickFalse) &&\n        ((memcmp(type,mng_JDAT,4) == 0) || (memcmp(type,mng_JdAA,4) == 0) ||\n         (memcmp(type,mng_IDAT,4) == 0) || (memcmp(type,mng_JDAA,4) == 0)))\n      {\n        /*\n           o create color_image\n           o open color_blob, attached to color_image\n           o if (color type has alpha)\n               open alpha_blob, attached to alpha_image\n        */\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Creating color_blob.\");\n\n        color_image_info=(ImageInfo *)AcquireMagickMemory(sizeof(ImageInfo));\n\n        if (color_image_info == (ImageInfo *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        GetImageInfo(color_image_info);\n        color_image=AcquireImage(color_image_info);\n\n        if (color_image == (Image *) NULL)\n          ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n\n        (void) AcquireUniqueFilename(color_image->filename);\n        unique_filenames++;\n        status=OpenBlob(color_image_info,color_image,WriteBinaryBlobMode,\n          exception);\n\n        if (status == MagickFalse)\n          {\n            color_image=DestroyImage(color_image);\n            return(DestroyImageList(image));\n          }\n\n        if ((image_info->ping == MagickFalse) && (jng_color_type >= 12))\n          {\n            alpha_image_info=(ImageInfo *)\n              AcquireMagickMemory(sizeof(ImageInfo));\n\n            if (alpha_image_info == (ImageInfo *) NULL)\n              {\n                color_image=DestroyImage(color_image);\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              }\n\n            GetImageInfo(alpha_image_info);\n            alpha_image=AcquireImage(alpha_image_info);\n\n            if (alpha_image == (Image *) NULL)\n              {\n                alpha_image_info=DestroyImageInfo(alpha_image_info);\n                color_image=DestroyImage(color_image);\n                ThrowReaderException(ResourceLimitError,\n                  \"MemoryAllocationFailed\");\n              }\n\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Creating alpha_blob.\");\n\n            (void) AcquireUniqueFilename(alpha_image->filename);\n            unique_filenames++;\n            status=OpenBlob(alpha_image_info,alpha_image,WriteBinaryBlobMode,\n              exception);\n\n            if (status == MagickFalse)\n              {\n                alpha_image=DestroyImage(alpha_image);\n                alpha_image_info=DestroyImageInfo(alpha_image_info);\n                color_image=DestroyImage(color_image);\n                return(DestroyImageList(image));\n              }\n\n            if (jng_alpha_compression_method == 0)\n              {\n                unsigned char\n                  data[18];\n\n                if (logging != MagickFalse)\n                  (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                    \"    Writing IHDR chunk to alpha_blob.\");\n\n                (void) WriteBlob(alpha_image,8,(const unsigned char *)\n                  \"\\211PNG\\r\\n\\032\\n\");\n\n                (void) WriteBlobMSBULong(alpha_image,13L);\n                PNGType(data,mng_IHDR);\n                LogPNGChunk(logging,mng_IHDR,13L);\n                PNGLong(data+4,jng_width);\n                PNGLong(data+8,jng_height);\n                data[12]=jng_alpha_sample_depth;\n                data[13]=0; /* color_type gray */\n                data[14]=0; /* compression method 0 */\n                data[15]=0; /* filter_method 0 */\n                data[16]=0; /* interlace_method 0 */\n                (void) WriteBlob(alpha_image,17,data);\n                (void) WriteBlobMSBULong(alpha_image,crc32(0,data,17));\n              }\n          }\n        reading_idat=MagickTrue;\n      }\n\n    if (memcmp(type,mng_JDAT,4) == 0)\n      {\n        /* Copy chunk to color_image->blob */\n\n        if (logging != MagickFalse)\n          (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n            \"    Copying JDAT chunk data to color_blob.\");\n\n        if (length != 0)\n          {\n            (void) WriteBlob(color_image,length,chunk);\n            chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n          }\n\n        continue;\n      }\n\n    if (memcmp(type,mng_IDAT,4) == 0)\n      {\n        png_byte\n           data[5];\n\n        /* Copy IDAT header and chunk data to alpha_image->blob */\n\n        if (alpha_image != NULL && image_info->ping == MagickFalse)\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Copying IDAT chunk data to alpha_blob.\");\n\n            (void) WriteBlobMSBULong(alpha_image,(size_t) length);\n            PNGType(data,mng_IDAT);\n            LogPNGChunk(logging,mng_IDAT,length);\n            (void) WriteBlob(alpha_image,4,data);\n            (void) WriteBlob(alpha_image,length,chunk);\n            (void) WriteBlobMSBULong(alpha_image,\n              crc32(crc32(0,data,4),chunk,(uInt) length));\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if ((memcmp(type,mng_JDAA,4) == 0) || (memcmp(type,mng_JdAA,4) == 0))\n      {\n        /* Copy chunk data to alpha_image->blob */\n\n        if (alpha_image != NULL && image_info->ping == MagickFalse)\n          {\n            if (logging != MagickFalse)\n              (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n                \"    Copying JDAA chunk data to alpha_blob.\");\n\n            (void) WriteBlob(alpha_image,length,chunk);\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_JSEP,4) == 0)\n      {\n        read_JSEP=MagickTrue;\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_bKGD,4) == 0)\n      {\n        if (length == 2)\n          {\n            image->background_color.red=ScaleCharToQuantum(p[1]);\n            image->background_color.green=image->background_color.red;\n            image->background_color.blue=image->background_color.red;\n          }\n\n        if (length == 6)\n          {\n            image->background_color.red=ScaleCharToQuantum(p[1]);\n            image->background_color.green=ScaleCharToQuantum(p[3]);\n            image->background_color.blue=ScaleCharToQuantum(p[5]);\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_gAMA,4) == 0)\n      {\n        if (length == 4)\n          image->gamma=((float) mng_get_long(p))*0.00001;\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_cHRM,4) == 0)\n      {\n        if (length == 32)\n          {\n            image->chromaticity.white_point.x=0.00001*mng_get_long(p);\n            image->chromaticity.white_point.y=0.00001*mng_get_long(&p[4]);\n            image->chromaticity.red_primary.x=0.00001*mng_get_long(&p[8]);\n            image->chromaticity.red_primary.y=0.00001*mng_get_long(&p[12]);\n            image->chromaticity.green_primary.x=0.00001*mng_get_long(&p[16]);\n            image->chromaticity.green_primary.y=0.00001*mng_get_long(&p[20]);\n            image->chromaticity.blue_primary.x=0.00001*mng_get_long(&p[24]);\n            image->chromaticity.blue_primary.y=0.00001*mng_get_long(&p[28]);\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_sRGB,4) == 0)\n      {\n        if (length == 1)\n          {\n            image->rendering_intent=\n              Magick_RenderingIntent_from_PNG_RenderingIntent(p[0]);\n            image->gamma=1.000f/2.200f;\n            image->chromaticity.red_primary.x=0.6400f;\n            image->chromaticity.red_primary.y=0.3300f;\n            image->chromaticity.green_primary.x=0.3000f;\n            image->chromaticity.green_primary.y=0.6000f;\n            image->chromaticity.blue_primary.x=0.1500f;\n            image->chromaticity.blue_primary.y=0.0600f;\n            image->chromaticity.white_point.x=0.3127f;\n            image->chromaticity.white_point.y=0.3290f;\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n    if (memcmp(type,mng_oFFs,4) == 0)\n      {\n        if (length > 8)\n          {\n            image->page.x=(ssize_t) mng_get_long(p);\n            image->page.y=(ssize_t) mng_get_long(&p[4]);\n\n            if ((int) p[8] != 0)\n              {\n                image->page.x/=10000;\n                image->page.y/=10000;\n              }\n          }\n\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n\n    if (memcmp(type,mng_pHYs,4) == 0)\n      {\n        if (length > 8)\n          {\n            image->x_resolution=(double) mng_get_long(p);\n            image->y_resolution=(double) mng_get_long(&p[4]);\n            if ((int) p[8] == PNG_RESOLUTION_METER)\n              {\n                image->units=PixelsPerCentimeterResolution;\n                image->x_resolution=image->x_resolution/100.0f;\n                image->y_resolution=image->y_resolution/100.0f;\n              }\n          }\n\n        chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n        continue;\n      }\n\n#if 0\n    if (memcmp(type,mng_iCCP,4) == 0)\n      {\n        /* To do: */\n        if (length != 0)\n          chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n        continue;\n      }\n#endif\n\n    if (length != 0)\n      chunk=(unsigned char *) RelinquishMagickMemory(chunk);\n\n    if (memcmp(type,mng_IEND,4))\n      continue;\n\n    break;\n  }\n\n\n  /* IEND found */\n\n  /*\n    Finish up reading image data:\n\n       o read main image from color_blob.\n\n       o close color_blob.\n\n       o if (color_type has alpha)\n            if alpha_encoding is PNG\n               read secondary image from alpha_blob via ReadPNG\n            if alpha_encoding is JPEG\n               read secondary image from alpha_blob via ReadJPEG\n\n       o close alpha_blob.\n\n       o copy intensity of secondary image into\n         opacity samples of main image.\n\n       o destroy the secondary image.\n  */\n\n  if (color_image_info == (ImageInfo *) NULL)\n    {\n      assert(color_image == (Image *) NULL);\n      assert(alpha_image == (Image *) NULL);\n      return(DestroyImageList(image));\n    }\n\n  if (color_image == (Image *) NULL)\n    {\n      assert(alpha_image == (Image *) NULL);\n      return(DestroyImageList(image));\n    }\n\n  (void) SeekBlob(color_image,0,SEEK_SET);\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Reading jng_image from color_blob.\");\n\n  assert(color_image_info != (ImageInfo *) NULL);\n  (void) FormatLocaleString(color_image_info->filename,MaxTextExtent,\"%s\",\n    color_image->filename);\n\n  color_image_info->ping=MagickFalse;   /* To do: avoid this */\n  jng_image=ReadImage(color_image_info,exception);\n\n  (void) RelinquishUniqueFileResource(color_image->filename);\n  unique_filenames--;\n  color_image=DestroyImage(color_image);\n  color_image_info=DestroyImageInfo(color_image_info);\n\n  if (jng_image == (Image *) NULL)\n    return(DestroyImageList(image));\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"    Copying jng_image pixels to main image.\");\n  image->columns=jng_width;\n  image->rows=jng_height;\n  length=image->columns*sizeof(PixelPacket);\n\n  status=SetImageExtent(image,image->columns,image->rows);\n  if (status == MagickFalse)\n    {\n      InheritException(exception,&image->exception);\n      return(DestroyImageList(image));\n    }\n\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    s=GetVirtualPixels(jng_image,0,y,image->columns,1,&image->exception);\n    q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n    (void) CopyMagickMemory(q,s,length);\n\n    if (SyncAuthenticPixels(image,exception) == MagickFalse)\n      break;\n  }\n\n  jng_image=DestroyImage(jng_image);\n\n  if (image_info->ping == MagickFalse)\n    {\n     if (jng_color_type >= 12)\n       {\n         if (jng_alpha_compression_method == 0)\n           {\n             png_byte\n               data[5];\n             (void) WriteBlobMSBULong(alpha_image,0x00000000L);\n             PNGType(data,mng_IEND);\n             LogPNGChunk(logging,mng_IEND,0L);\n             (void) WriteBlob(alpha_image,4,data);\n             (void) WriteBlobMSBULong(alpha_image,crc32(0,data,4));\n           }\n\n         (void) SeekBlob(alpha_image,0,SEEK_SET);\n\n         if (logging != MagickFalse)\n           (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n             \"    Reading opacity from alpha_blob.\");\n\n         (void) FormatLocaleString(alpha_image_info->filename,MaxTextExtent,\n           \"%s\",alpha_image->filename);\n\n         jng_image=ReadImage(alpha_image_info,exception);\n\n         if (jng_image != (Image *) NULL)\n           for (y=0; y < (ssize_t) image->rows; y++)\n           {\n             s=GetVirtualPixels(jng_image,0,y,image->columns,1,\n                &image->exception);\n             q=GetAuthenticPixels(image,0,y,image->columns,1,exception);\n\n             if (image->matte != MagickFalse)\n               for (x=(ssize_t) image->columns; x != 0; x--,q++,s++)\n                  SetPixelOpacity(q,QuantumRange-\n                      GetPixelRed(s));\n\n             else\n               for (x=(ssize_t) image->columns; x != 0; x--,q++,s++)\n               {\n                  SetPixelAlpha(q,GetPixelRed(s));\n                  if (GetPixelOpacity(q) != OpaqueOpacity)\n                    image->matte=MagickTrue;\n               }\n\n             if (SyncAuthenticPixels(image,exception) == MagickFalse)\n               break;\n           }\n         (void) RelinquishUniqueFileResource(alpha_image->filename);\n         unique_filenames--;\n         alpha_image=DestroyImage(alpha_image);\n         alpha_image_info=DestroyImageInfo(alpha_image_info);\n         if (jng_image != (Image *) NULL)\n           jng_image=DestroyImage(jng_image);\n       }\n    }\n\n  /* Read the JNG image.  */\n\n  if (mng_info->mng_type == 0)\n    {\n      mng_info->mng_width=jng_width;\n      mng_info->mng_height=jng_height;\n    }\n\n  if (image->page.width == 0 && image->page.height == 0)\n    {\n      image->page.width=jng_width;\n      image->page.height=jng_height;\n    }\n\n  if (image->page.x == 0 && image->page.y == 0)\n    {\n      image->page.x=mng_info->x_off[mng_info->object_id];\n      image->page.y=mng_info->y_off[mng_info->object_id];\n    }\n\n  else\n    {\n      image->page.y=mng_info->y_off[mng_info->object_id];\n    }\n\n  mng_info->image_found++;\n  status=SetImageProgress(image,LoadImagesTag,2*TellBlob(image),\n    2*GetBlobSize(image));\n\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n\n  if (logging != MagickFalse)\n    (void) LogMagickEvent(CoderEvent,GetMagickModule(),\n      \"  exit ReadOneJNGImage(); unique_filenames=%d\",unique_filenames);\n\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -141,7 +141,13 @@\n           ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n \n         for (i=0; i < (ssize_t) length; i++)\n-          chunk[i]=(unsigned char) ReadBlobByte(image);\n+        {\n+          int\n+            c;\n+\n+          c=ReadBlobByte(image);\n+          chunk[i]=(unsigned char) c;\n+        }\n \n         p=chunk;\n       }",
        "diff_line_info": {
            "deleted_lines": [
                "          chunk[i]=(unsigned char) ReadBlobByte(image);"
            ],
            "added_lines": [
                "        {",
                "          int",
                "            c;",
                "",
                "          c=ReadBlobByte(image);",
                "          chunk[i]=(unsigned char) c;",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11521",
        "func_name": "resiprocate/SdpContents::Session::Medium::parse",
        "description": "The SdpContents::Session::Medium::parse function in resip/stack/SdpContents.cxx in reSIProcate 1.10.2 allows remote attackers to cause a denial of service (memory consumption) by triggering many media connections.",
        "git_url": "https://github.com/resiprocate/resiprocate/commit/4b8ffa5afd3291a2701f8d39c31ada443f79a5c8",
        "commit_title": "resip/stack: SdpContents: Avoid out-of-memory condition (CVE-2017-11521)",
        "commit_text": " Adding too many media connections may lead to memory exhaustion.",
        "func_before": "void\nSdpContents::Session::Medium::parse(ParseBuffer& pb)\n{\n   pb.skipChar('m');\n   const char* anchor = pb.skipChar(Symbols::EQUALS[0]);\n\n   pb.skipToChar(Symbols::SPACE[0]);\n   pb.data(mName, anchor);\n   pb.skipChar(Symbols::SPACE[0]);\n\n   mPort = pb.integer();\n\n   if (*pb.position() == Symbols::SLASH[0])\n   {\n      pb.skipChar();\n      mMulticast = pb.integer();\n   }\n\n   anchor = pb.skipChar(Symbols::SPACE[0]);\n   pb.skipToOneOf(Symbols::SPACE, Symbols::CRLF);\n   pb.data(mProtocol, anchor);\n\n   while (*pb.position() != Symbols::CR[0] &&\n          *pb.position() != Symbols::LF[0])\n   {\n      anchor = pb.skipChar(Symbols::SPACE[0]);\n      pb.skipToOneOf(Symbols::SPACE, Symbols::CRLF);\n     if(pb.position() != anchor)\n     {\n      Data format;\n      pb.data(format, anchor);\n      addFormat(format);\n     }\n   }\n\n   skipEol(pb);\n\n   if (!pb.eof() && *pb.position() == 'i')\n   {\n      pb.skipChar('i');\n      anchor = pb.skipChar(Symbols::EQUALS[0]);\n      pb.skipToOneOf(Symbols::CRLF);\n      pb.data(mInformation, anchor);\n\n      skipEol(pb);\n   }\n\n   while (!pb.eof() && *pb.position() == 'c')\n   {\n      addConnection(Connection());\n      mConnections.back().parse(pb);\n      if (!pb.eof() && *pb.position() == Symbols::SLASH[0])\n      {\n         // Note:  we only get here if there was a /<number of addresses> \n         //        parameter following the connection address. \n         pb.skipChar();\n         int num = pb.integer();\n\n         Connection& con = mConnections.back();\n         const Data& addr = con.getAddress();\n         size_t i = addr.size() - 1;\n         for (; i; i--)\n         {\n            if (addr[i] == '.' || addr[i] == ':') // ipv4 or ipv6\n            {\n               break;\n            }\n         }\n\n         if (addr[i] == '.')  // add a number of ipv4 connections\n         {\n            Data before(addr.data(), i+1);\n            ParseBuffer subpb(addr.data()+i+1, addr.size()-i-1);\n            int after = subpb.integer();\n\n            for (int i = 1; i < num; i++)\n            {\n               addConnection(con);\n               mConnections.back().mAddress = before + Data(after+i);\n            }\n         }\n         if (addr[i] == ':') // add a number of ipv6 connections\n         {\n            Data before(addr.data(), i+1);\n            int after = Helper::hex2integer(addr.data()+i+1);\n            char hexstring[9];\n\n            for (int i = 1; i < num; i++)\n            {\n               addConnection(con);\n               memset(hexstring, 0, sizeof(hexstring));\n               Helper::integer2hex(hexstring, after+i, false /* supress leading zeros */);\n               mConnections.back().mAddress = before + Data(hexstring);\n            }\n         }\n\n         skipEol(pb);\n      }\n   }\n\n   while (!pb.eof() && *pb.position() == 'b')\n   {\n      addBandwidth(Bandwidth());\n      mBandwidths.back().parse(pb);\n   }\n\n   if (!pb.eof() && *pb.position() == 'k')\n   {\n      mEncryption.parse(pb);\n   }\n\n   mAttributeHelper.parse(pb);\n}",
        "func": "void\nSdpContents::Session::Medium::parse(ParseBuffer& pb)\n{\n   pb.skipChar('m');\n   const char* anchor = pb.skipChar(Symbols::EQUALS[0]);\n\n   pb.skipToChar(Symbols::SPACE[0]);\n   pb.data(mName, anchor);\n   pb.skipChar(Symbols::SPACE[0]);\n\n   mPort = pb.integer();\n\n   if (*pb.position() == Symbols::SLASH[0])\n   {\n      pb.skipChar();\n      mMulticast = pb.integer();\n   }\n\n   anchor = pb.skipChar(Symbols::SPACE[0]);\n   pb.skipToOneOf(Symbols::SPACE, Symbols::CRLF);\n   pb.data(mProtocol, anchor);\n\n   while (*pb.position() != Symbols::CR[0] &&\n          *pb.position() != Symbols::LF[0])\n   {\n      anchor = pb.skipChar(Symbols::SPACE[0]);\n      pb.skipToOneOf(Symbols::SPACE, Symbols::CRLF);\n     if(pb.position() != anchor)\n     {\n      Data format;\n      pb.data(format, anchor);\n      addFormat(format);\n     }\n   }\n\n   skipEol(pb);\n\n   if (!pb.eof() && *pb.position() == 'i')\n   {\n      pb.skipChar('i');\n      anchor = pb.skipChar(Symbols::EQUALS[0]);\n      pb.skipToOneOf(Symbols::CRLF);\n      pb.data(mInformation, anchor);\n\n      skipEol(pb);\n   }\n\n   while (!pb.eof() && *pb.position() == 'c')\n   {\n      addConnection(Connection());\n      mConnections.back().parse(pb);\n      if (!pb.eof() && *pb.position() == Symbols::SLASH[0])\n      {\n         // Note:  we only get here if there was a /<number of addresses> \n         //        parameter following the connection address. \n         pb.skipChar();\n         int num = pb.integer();\n\n         if (num > 255)\n         {\n            pb.fail(__FILE__, __LINE__, \"Too many connection addresses\");\n         }\n\n         Connection& con = mConnections.back();\n         const Data& addr = con.getAddress();\n         size_t i = addr.size() - 1;\n         for (; i; i--)\n         {\n            if (addr[i] == '.' || addr[i] == ':') // ipv4 or ipv6\n            {\n               break;\n            }\n         }\n\n         if (addr[i] == '.')  // add a number of ipv4 connections\n         {\n            Data before(addr.data(), i+1);\n            ParseBuffer subpb(addr.data()+i+1, addr.size()-i-1);\n            int after = subpb.integer();\n\n            for (int i = 1; i < num; i++)\n            {\n               addConnection(con);\n               mConnections.back().mAddress = before + Data(after+i);\n            }\n         }\n         if (addr[i] == ':') // add a number of ipv6 connections\n         {\n            Data before(addr.data(), i+1);\n            int after = Helper::hex2integer(addr.data()+i+1);\n            char hexstring[9];\n\n            for (int i = 1; i < num; i++)\n            {\n               addConnection(con);\n               memset(hexstring, 0, sizeof(hexstring));\n               Helper::integer2hex(hexstring, after+i, false /* supress leading zeros */);\n               mConnections.back().mAddress = before + Data(hexstring);\n            }\n         }\n\n         skipEol(pb);\n      }\n   }\n\n   while (!pb.eof() && *pb.position() == 'b')\n   {\n      addBandwidth(Bandwidth());\n      mBandwidths.back().parse(pb);\n   }\n\n   if (!pb.eof() && *pb.position() == 'k')\n   {\n      mEncryption.parse(pb);\n   }\n\n   mAttributeHelper.parse(pb);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,6 +56,11 @@\n          pb.skipChar();\n          int num = pb.integer();\n \n+         if (num > 255)\n+         {\n+            pb.fail(__FILE__, __LINE__, \"Too many connection addresses\");\n+         }\n+\n          Connection& con = mConnections.back();\n          const Data& addr = con.getAddress();\n          size_t i = addr.size() - 1;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "         if (num > 255)",
                "         {",
                "            pb.fail(__FILE__, __LINE__, \"Too many connection addresses\");",
                "         }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11527",
        "func_name": "ImageMagick/RegisterDPXImage",
        "description": "The ReadDPXImage function in coders/dpx.c in ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1 allows remote attackers to cause a denial of service (memory consumption) via a crafted file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/961eb7c6fe2f1efc0be11d950c4500cd0cd17702",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/523",
        "commit_text": "",
        "func_before": "ModuleExport size_t RegisterDPXImage(void)\n{\n  MagickInfo\n    *entry;\n\n  static const char\n    *DPXNote =\n    {\n      \"Digital Moving Picture Exchange Bitmap, Version 2.0.\\n\"\n      \"See SMPTE 268M-2003 specification at http://www.smtpe.org\\n\"\n    };\n\n  entry=SetMagickInfo(\"DPX\");\n  entry->decoder=(DecodeImageHandler *) ReadDPXImage;\n  entry->encoder=(EncodeImageHandler *) WriteDPXImage;\n  entry->magick=(IsImageFormatHandler *) IsDPX;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"SMPTE 268M-2003 (DPX 2.0)\");\n  entry->note=ConstantString(DPXNote);\n  entry->module=ConstantString(\"DPX\");\n  (void) RegisterMagickInfo(entry);\n  return(MagickImageCoderSignature);\n}",
        "func": "ModuleExport size_t RegisterDPXImage(void)\n{\n  MagickInfo\n    *entry;\n\n  static const char\n    *DPXNote =\n    {\n      \"Digital Moving Picture Exchange Bitmap, Version 2.0.\\n\"\n      \"See SMPTE 268M-2003 specification at http://www.smtpe.org\\n\"\n    };\n\n  entry=SetMagickInfo(\"DPX\");\n  entry->decoder=(DecodeImageHandler *) ReadDPXImage;\n  entry->encoder=(EncodeImageHandler *) WriteDPXImage;\n  entry->magick=(IsImageFormatHandler *) IsDPX;\n  entry->seekable_stream=MagickTrue;\n  entry->adjoin=MagickFalse;\n  entry->description=ConstantString(\"SMPTE 268M-2003 (DPX 2.0)\");\n  entry->note=ConstantString(DPXNote);\n  entry->module=ConstantString(\"DPX\");\n  (void) RegisterMagickInfo(entry);\n  return(MagickImageCoderSignature);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,7 @@\n   entry->decoder=(DecodeImageHandler *) ReadDPXImage;\n   entry->encoder=(EncodeImageHandler *) WriteDPXImage;\n   entry->magick=(IsImageFormatHandler *) IsDPX;\n+  entry->seekable_stream=MagickTrue;\n   entry->adjoin=MagickFalse;\n   entry->description=ConstantString(\"SMPTE 268M-2003 (DPX 2.0)\");\n   entry->note=ConstantString(DPXNote);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  entry->seekable_stream=MagickTrue;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11527",
        "func_name": "ImageMagick/ReadDPXImage",
        "description": "The ReadDPXImage function in coders/dpx.c in ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1 allows remote attackers to cause a denial of service (memory consumption) via a crafted file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/961eb7c6fe2f1efc0be11d950c4500cd0cd17702",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/523",
        "commit_text": "",
        "func_before": "static Image *ReadDPXImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  char\n    magick[4],\n    value[MaxTextExtent];\n\n  DPXInfo\n    dpx;\n\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    extent,\n    samples_per_pixel;\n\n  ssize_t\n    count,\n    n,\n    row,\n    y;\n\n  unsigned char\n    component_type;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read DPX file header.\n  */\n  offset=0;\n  count=ReadBlob(image,4,(unsigned char *) magick);\n  offset+=count;\n  if ((count != 4) || ((LocaleNCompare(magick,\"SDPX\",4) != 0) &&\n      (LocaleNCompare((char *) magick,\"XPDS\",4) != 0)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  image->endian=LSBEndian;\n  if (LocaleNCompare(magick,\"SDPX\",4) == 0)\n    image->endian=MSBEndian;\n  (void) ResetMagickMemory(&dpx,0,sizeof(dpx));\n  dpx.file.image_offset=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(dpx.file.version),(unsigned char *)\n    dpx.file.version);\n  (void) FormatImageProperty(image,\"dpx:file.version\",\"%.8s\",dpx.file.version);\n  dpx.file.file_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.ditto_key=ReadBlobLong(image);\n  offset+=4;\n  if (dpx.file.ditto_key != ~0U)\n    (void) FormatImageProperty(image,\"dpx:file.ditto.key\",\"%u\",\n      dpx.file.ditto_key);\n  dpx.file.generic_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.industry_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.user_size=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(dpx.file.filename),(unsigned char *)\n    dpx.file.filename);\n  (void) FormatImageProperty(image,\"dpx:file.filename\",\"%.100s\",\n    dpx.file.filename);\n  (void) FormatImageProperty(image,\"document\",\"%.100s\",dpx.file.filename);\n  offset+=ReadBlob(image,sizeof(dpx.file.timestamp),(unsigned char *)\n    dpx.file.timestamp);\n  if (*dpx.file.timestamp != '\\0')\n    (void) FormatImageProperty(image,\"dpx:file.timestamp\",\"%.24s\",\n      dpx.file.timestamp);\n  offset+=ReadBlob(image,sizeof(dpx.file.creator),(unsigned char *)\n    dpx.file.creator);\n  if (*dpx.file.creator == '\\0')\n    {\n      char\n        *url;\n\n      url=GetMagickHomeURL();\n      (void) FormatImageProperty(image,\"dpx:file.creator\",\"%.100s\",url);\n      (void) FormatImageProperty(image,\"software\",\"%.100s\",url);\n      url=DestroyString(url);\n    }\n  else\n    {\n      (void) FormatImageProperty(image,\"dpx:file.creator\",\"%.100s\",\n        dpx.file.creator);\n      (void) FormatImageProperty(image,\"software\",\"%.100s\",dpx.file.creator);\n    }\n  offset+=ReadBlob(image,sizeof(dpx.file.project),(unsigned char *)\n    dpx.file.project);\n  if (*dpx.file.project != '\\0')\n    {\n      (void) FormatImageProperty(image,\"dpx:file.project\",\"%.200s\",\n        dpx.file.project);\n      (void) FormatImageProperty(image,\"comment\",\"%.100s\",dpx.file.project);\n    }\n  offset+=ReadBlob(image,sizeof(dpx.file.copyright),(unsigned char *)\n    dpx.file.copyright);\n  if (*dpx.file.copyright != '\\0')\n    {\n      (void) FormatImageProperty(image,\"dpx:file.copyright\",\"%.200s\",\n        dpx.file.copyright);\n      (void) FormatImageProperty(image,\"copyright\",\"%.100s\",\n        dpx.file.copyright);\n    }\n  dpx.file.encrypt_key=ReadBlobLong(image);\n  offset+=4;\n  if (dpx.file.encrypt_key != ~0U)\n    (void) FormatImageProperty(image,\"dpx:file.encrypt_key\",\"%u\",\n      dpx.file.encrypt_key);\n  offset+=ReadBlob(image,sizeof(dpx.file.reserve),(unsigned char *)\n    dpx.file.reserve);\n  /*\n    Read DPX image header.\n  */\n  dpx.image.orientation=ReadBlobShort(image);\n  if (dpx.image.orientation > 7)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  offset+=2;\n  if (dpx.image.orientation != (unsigned short) ~0)\n    (void) FormatImageProperty(image,\"dpx:image.orientation\",\"%d\",\n      dpx.image.orientation);\n  switch (dpx.image.orientation)\n  {\n    default:\n    case 0: image->orientation=TopLeftOrientation; break;\n    case 1: image->orientation=TopRightOrientation; break;\n    case 2: image->orientation=BottomLeftOrientation; break;\n    case 3: image->orientation=BottomRightOrientation; break;\n    case 4: image->orientation=LeftTopOrientation; break;\n    case 5: image->orientation=RightTopOrientation; break;\n    case 6: image->orientation=LeftBottomOrientation; break;\n    case 7: image->orientation=RightBottomOrientation; break;\n  }\n  dpx.image.number_elements=ReadBlobShort(image);\n  if (dpx.image.number_elements > MaxNumberImageElements)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  offset+=2;\n  dpx.image.pixels_per_line=ReadBlobLong(image);\n  offset+=4;\n  image->columns=dpx.image.pixels_per_line;\n  dpx.image.lines_per_element=ReadBlobLong(image);\n  offset+=4;\n  image->rows=dpx.image.lines_per_element;\n  for (i=0; i < 8; i++)\n  {\n    char\n      property[MaxTextExtent];\n\n    dpx.image.image_element[i].data_sign=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].low_data=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].low_quantity=ReadBlobFloat(image);\n    offset+=4;\n    dpx.image.image_element[i].high_data=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].high_quantity=ReadBlobFloat(image);\n    offset+=4;\n    dpx.image.image_element[i].descriptor=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].transfer_characteristic=(unsigned char)\n      ReadBlobByte(image);\n    (void) FormatLocaleString(property,MaxTextExtent,\n      \"dpx:image.element[%lu].transfer-characteristic\",(long) i);\n    (void) FormatImageProperty(image,property,\"%s\",\n      GetImageTransferCharacteristic((DPXTransferCharacteristic)\n      dpx.image.image_element[i].transfer_characteristic));\n    offset++;\n    dpx.image.image_element[i].colorimetric=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].bit_size=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].packing=ReadBlobShort(image);\n    offset+=2;\n    dpx.image.image_element[i].encoding=ReadBlobShort(image);\n    offset+=2;\n    dpx.image.image_element[i].data_offset=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].end_of_line_padding=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].end_of_image_padding=ReadBlobLong(image);\n    offset+=4;\n    offset+=ReadBlob(image,sizeof(dpx.image.image_element[i].description),\n      (unsigned char *) dpx.image.image_element[i].description);\n  }\n  (void) SetImageColorspace(image,RGBColorspace);\n  offset+=ReadBlob(image,sizeof(dpx.image.reserve),(unsigned char *)\n    dpx.image.reserve);\n  if (dpx.file.image_offset >= 1664U)\n    {\n      /*\n        Read DPX orientation header.\n      */\n      dpx.orientation.x_offset=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.x_offset != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_offset\",\"%u\",\n          dpx.orientation.x_offset);\n      dpx.orientation.y_offset=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.y_offset != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_offset\",\"%u\",\n          dpx.orientation.y_offset);\n      dpx.orientation.x_center=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.orientation.x_center) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_center\",\"%g\",\n          dpx.orientation.x_center);\n      dpx.orientation.y_center=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.orientation.y_center) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_center\",\"%g\",\n          dpx.orientation.y_center);\n      dpx.orientation.x_size=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.x_size != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_size\",\"%u\",\n          dpx.orientation.x_size);\n      dpx.orientation.y_size=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.y_size != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_size\",\"%u\",\n          dpx.orientation.y_size);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.filename),(unsigned char *)\n        dpx.orientation.filename);\n      if (*dpx.orientation.filename != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.filename\",\"%.100s\",\n          dpx.orientation.filename);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.timestamp),(unsigned char *)\n        dpx.orientation.timestamp);\n      if (*dpx.orientation.timestamp != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.timestamp\",\"%.24s\",\n          dpx.orientation.timestamp);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.device),(unsigned char *)\n        dpx.orientation.device);\n      if (*dpx.orientation.device != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.device\",\"%.32s\",\n          dpx.orientation.device);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.serial),(unsigned char *)\n        dpx.orientation.serial);\n      if (*dpx.orientation.serial != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.serial\",\"%.32s\",\n          dpx.orientation.serial);\n      for (i=0; i < 4; i++)\n      {\n        dpx.orientation.border[i]=ReadBlobShort(image);\n        offset+=2;\n      }\n      if ((dpx.orientation.border[0] != (unsigned short) (~0)) &&\n          (dpx.orientation.border[1] != (unsigned short) (~0)))\n        (void) FormatImageProperty(image,\"dpx:orientation.border\",\"%dx%d%+d%+d\",          dpx.orientation.border[0],dpx.orientation.border[1],\n          dpx.orientation.border[2],dpx.orientation.border[3]);\n      for (i=0; i < 2; i++)\n      {\n        dpx.orientation.aspect_ratio[i]=ReadBlobLong(image);\n        offset+=4;\n      }\n      if ((dpx.orientation.aspect_ratio[0] != ~0U) &&\n          (dpx.orientation.aspect_ratio[1] != ~0U))\n        (void) FormatImageProperty(image,\"dpx:orientation.aspect_ratio\",\n          \"%ux%u\",dpx.orientation.aspect_ratio[0],\n          dpx.orientation.aspect_ratio[1]);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.reserve),(unsigned char *)\n        dpx.orientation.reserve);\n    }\n  if (dpx.file.image_offset >= 1920U)\n    {\n      /*\n        Read DPX film header.\n      */\n      offset+=ReadBlob(image,sizeof(dpx.film.id),(unsigned char *) dpx.film.id);\n      if (*dpx.film.id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.id\",\"%.2s\",dpx.film.id);\n      offset+=ReadBlob(image,sizeof(dpx.film.type),(unsigned char *)\n        dpx.film.type);\n      if (*dpx.film.type != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.type\",\"%.2s\",dpx.film.type);\n      offset+=ReadBlob(image,sizeof(dpx.film.offset),(unsigned char *)\n        dpx.film.offset);\n      if (*dpx.film.offset != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.offset\",\"%.2s\",\n          dpx.film.offset);\n      offset+=ReadBlob(image,sizeof(dpx.film.prefix),(unsigned char *)\n        dpx.film.prefix);\n      if (*dpx.film.prefix != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.prefix\",\"%.6s\",\n          dpx.film.prefix);\n      offset+=ReadBlob(image,sizeof(dpx.film.count),(unsigned char *)\n        dpx.film.count);\n      if (*dpx.film.count != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.count\",\"%.4s\",\n          dpx.film.count);\n      offset+=ReadBlob(image,sizeof(dpx.film.format),(unsigned char *)\n        dpx.film.format);\n      if (*dpx.film.format != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.format\",\"%.4s\",\n          dpx.film.format);\n      dpx.film.frame_position=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.frame_position != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.frame_position\",\"%u\",\n          dpx.film.frame_position);\n      dpx.film.sequence_extent=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.sequence_extent != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.sequence_extent\",\"%u\",\n          dpx.film.sequence_extent);\n      dpx.film.held_count=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.held_count != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.held_count\",\"%u\",\n          dpx.film.held_count);\n      dpx.film.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.film.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.frame_rate\",\"%g\",\n          dpx.film.frame_rate);\n      dpx.film.shutter_angle=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.film.shutter_angle) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.shutter_angle\",\"%g\",\n          dpx.film.shutter_angle);\n      offset+=ReadBlob(image,sizeof(dpx.film.frame_id),(unsigned char *)\n        dpx.film.frame_id);\n      if (*dpx.film.frame_id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.frame_id\",\"%.32s\",\n          dpx.film.frame_id);\n      offset+=ReadBlob(image,sizeof(dpx.film.slate),(unsigned char *)\n        dpx.film.slate);\n      if (*dpx.film.slate != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.slate\",\"%.100s\",\n          dpx.film.slate);\n      offset+=ReadBlob(image,sizeof(dpx.film.reserve),(unsigned char *)\n        dpx.film.reserve);\n    }\n  if (dpx.file.image_offset >= 2048U)\n    {\n      /*\n        Read DPX television header.\n      */\n      dpx.television.time_code=(unsigned int) ReadBlobLong(image);\n      offset+=4;\n      TimeCodeToString(dpx.television.time_code,value);\n      (void) SetImageProperty(image,\"dpx:television.time.code\",value);\n      dpx.television.user_bits=(unsigned int) ReadBlobLong(image);\n      offset+=4;\n      TimeCodeToString(dpx.television.user_bits,value);\n      (void) SetImageProperty(image,\"dpx:television.user.bits\",value);\n      dpx.television.interlace=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.interlace != 0)\n        (void) FormatImageProperty(image,\"dpx:television.interlace\",\"%.20g\",\n          (double) dpx.television.interlace);\n      dpx.television.field_number=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.field_number != 0)\n        (void) FormatImageProperty(image,\"dpx:television.field_number\",\"%.20g\",\n          (double) dpx.television.field_number);\n      dpx.television.video_signal=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.video_signal != 0)\n        (void) FormatImageProperty(image,\"dpx:television.video_signal\",\"%.20g\",\n          (double) dpx.television.video_signal);\n      dpx.television.padding=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.padding != 0)\n        (void) FormatImageProperty(image,\"dpx:television.padding\",\"%d\",\n          dpx.television.padding);\n      dpx.television.horizontal_sample_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.horizontal_sample_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\n          \"dpx:television.horizontal_sample_rate\",\"%g\",\n          dpx.television.horizontal_sample_rate);\n      dpx.television.vertical_sample_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.vertical_sample_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.vertical_sample_rate\",\n          \"%g\",dpx.television.vertical_sample_rate);\n      dpx.television.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.frame_rate\",\"%g\",\n          dpx.television.frame_rate);\n      dpx.television.time_offset=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.time_offset) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.time_offset\",\"%g\",\n          dpx.television.time_offset);\n      dpx.television.gamma=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.gamma) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.gamma\",\"%g\",\n          dpx.television.gamma);\n      dpx.television.black_level=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.black_level) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.black_level\",\"%g\",\n          dpx.television.black_level);\n      dpx.television.black_gain=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.black_gain) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.black_gain\",\"%g\",\n          dpx.television.black_gain);\n      dpx.television.break_point=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.break_point) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.break_point\",\"%g\",\n          dpx.television.break_point);\n      dpx.television.white_level=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.white_level) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.white_level\",\"%g\",\n          dpx.television.white_level);\n      dpx.television.integration_times=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.integration_times) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.integration_times\",\n          \"%g\",dpx.television.integration_times);\n      offset+=ReadBlob(image,sizeof(dpx.television.reserve),(unsigned char *)\n        dpx.television.reserve);\n    }\n  if (dpx.file.image_offset > 2080U)\n    {\n      /*\n        Read DPX user header.\n      */\n      offset+=ReadBlob(image,sizeof(dpx.user.id),(unsigned char *) dpx.user.id);\n      if (*dpx.user.id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:user.id\",\"%.32s\",dpx.user.id);\n      if ((dpx.file.user_size != ~0U) &&\n          ((size_t) dpx.file.user_size > sizeof(dpx.user.id)))\n        {\n          StringInfo\n            *profile;\n\n           profile=BlobToStringInfo((const void *) NULL,\n             dpx.file.user_size-sizeof(dpx.user.id));\n           if (profile == (StringInfo *) NULL)\n             ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n           offset+=ReadBlob(image,GetStringInfoLength(profile),\n             GetStringInfoDatum(profile));\n           (void) SetImageProfile(image,\"dpx:user-data\",profile);\n           profile=DestroyStringInfo(profile);\n        }\n    }\n  for ( ; offset < (MagickOffsetType) dpx.file.image_offset; offset++)\n    if (ReadBlobByte(image) == EOF)\n      break;\n  if (EOFBlob(image) != MagickFalse)\n    ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n      image->filename);\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  status=SetImageExtent(image,image->columns,image->rows);\n  if (status == MagickFalse)\n    {\n      InheritException(exception,&image->exception);\n      return(DestroyImageList(image));\n    }\n  for (n=0; n < (ssize_t) dpx.image.number_elements; n++)\n  {\n    /*\n      Convert DPX raster image to pixel packets.\n    */\n    if ((dpx.image.image_element[n].data_offset != ~0U) &&\n        (dpx.image.image_element[n].data_offset != 0U))\n      {\n         MagickOffsetType\n           data_offset;\n\n         data_offset=(MagickOffsetType) dpx.image.image_element[n].data_offset;\n         if (data_offset < offset)\n           offset=SeekBlob(image,data_offset,SEEK_SET);\n         else\n           for ( ; offset < data_offset; offset++)\n             if (ReadBlobByte(image) == EOF)\n               break;\n          if (offset != data_offset)\n            ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n       }\n    SetPrimaryChromaticity((DPXColorimetric)\n      dpx.image.image_element[n].colorimetric,&image->chromaticity);\n    image->depth=dpx.image.image_element[n].bit_size;\n    samples_per_pixel=1;\n    quantum_type=GrayQuantum;\n    component_type=dpx.image.image_element[n].descriptor;\n    switch (component_type)\n    {\n      case CbYCrY422ComponentType:\n      {\n        samples_per_pixel=2;\n        quantum_type=CbYCrYQuantum;\n        break;\n      }\n      case CbYACrYA4224ComponentType:\n      case CbYCr444ComponentType:\n      {\n        samples_per_pixel=3;\n        quantum_type=CbYCrQuantum;\n        break;\n      }\n      case RGBComponentType:\n      {\n        samples_per_pixel=3;\n        quantum_type=RGBQuantum;\n        break;\n      }\n      case ABGRComponentType:\n      case RGBAComponentType:\n      {\n        image->matte=MagickTrue;\n        samples_per_pixel=4;\n        quantum_type=RGBAQuantum;\n        break;\n      }\n      default:\n        break;\n    }\n    switch (component_type)\n    {\n      case CbYCrY422ComponentType:\n      case CbYACrYA4224ComponentType:\n      case CbYCr444ComponentType:\n      {\n        (void) SetImageColorspace(image,Rec709YCbCrColorspace);\n        break;\n      }\n      case LumaComponentType:\n      {\n        (void) SetImageColorspace(image,GRAYColorspace);\n        break;\n      }\n      default:\n      {\n        (void) SetImageColorspace(image,RGBColorspace);\n        if (dpx.image.image_element[n].transfer_characteristic == LogarithmicColorimetric)\n          (void) SetImageColorspace(image,LogColorspace);\n        if (dpx.image.image_element[n].transfer_characteristic == PrintingDensityColorimetric)\n          (void) SetImageColorspace(image,LogColorspace);\n        break;\n      }\n    }\n    extent=GetBytesPerRow(image->columns,samples_per_pixel,image->depth,\n      dpx.image.image_element[n].packing == 0 ? MagickFalse : MagickTrue);\n    /*\n      DPX any-bit pixel format.\n    */\n    status=MagickTrue;\n    row=0;\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    SetQuantumQuantum(quantum_info,32);\n    SetQuantumPack(quantum_info,dpx.image.image_element[n].packing == 0 ?\n      MagickTrue : MagickFalse);\n    for (y=0; y < (ssize_t) image->rows; y++)\n    {\n      const unsigned char\n        *pixels;\n\n      MagickBooleanType\n        sync;\n\n      register PixelPacket\n        *q;\n\n      size_t\n        length;\n\n      ssize_t\n        count,\n        offset;\n\n      if (status == MagickFalse)\n        continue;\n      pixels=(const unsigned char *) ReadBlobStream(image,extent,\n        GetQuantumPixels(quantum_info),&count);\n      if (count != (ssize_t) extent)\n        status=MagickFalse;\n      if ((image->progress_monitor != (MagickProgressMonitor) NULL) &&\n          (image->previous == (Image *) NULL))\n        {\n          MagickBooleanType\n            proceed;\n\n          proceed=SetImageProgress(image,LoadImageTag,(MagickOffsetType) row,\n            image->rows);\n          if (proceed == MagickFalse)\n            status=MagickFalse;\n        }\n      offset=row++;\n      q=QueueAuthenticPixels(image,0,offset,image->columns,1,exception);\n      if (q == (PixelPacket *) NULL)\n        {\n          status=MagickFalse;\n          continue;\n        }\n      length=ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,\n        quantum_type,pixels,exception);\n      (void) length;\n      sync=SyncAuthenticPixels(image,exception);\n      if (sync == MagickFalse)\n        status=MagickFalse;\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (status == MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n    SetQuantumImageType(image,quantum_type);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n        image->filename);\n  }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}",
        "func": "static Image *ReadDPXImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  char\n    magick[4],\n    value[MaxTextExtent];\n\n  DPXInfo\n    dpx;\n\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  register ssize_t\n    i;\n\n  size_t\n    extent,\n    samples_per_pixel;\n\n  ssize_t\n    count,\n    n,\n    row,\n    y;\n\n  unsigned char\n    component_type;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickSignature);\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Read DPX file header.\n  */\n  offset=0;\n  count=ReadBlob(image,4,(unsigned char *) magick);\n  offset+=count;\n  if ((count != 4) || ((LocaleNCompare(magick,\"SDPX\",4) != 0) &&\n      (LocaleNCompare((char *) magick,\"XPDS\",4) != 0)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  image->endian=LSBEndian;\n  if (LocaleNCompare(magick,\"SDPX\",4) == 0)\n    image->endian=MSBEndian;\n  (void) ResetMagickMemory(&dpx,0,sizeof(dpx));\n  dpx.file.image_offset=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(dpx.file.version),(unsigned char *)\n    dpx.file.version);\n  (void) FormatImageProperty(image,\"dpx:file.version\",\"%.8s\",dpx.file.version);\n  dpx.file.file_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.ditto_key=ReadBlobLong(image);\n  offset+=4;\n  if (dpx.file.ditto_key != ~0U)\n    (void) FormatImageProperty(image,\"dpx:file.ditto.key\",\"%u\",\n      dpx.file.ditto_key);\n  dpx.file.generic_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.industry_size=ReadBlobLong(image);\n  offset+=4;\n  dpx.file.user_size=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(dpx.file.filename),(unsigned char *)\n    dpx.file.filename);\n  (void) FormatImageProperty(image,\"dpx:file.filename\",\"%.100s\",\n    dpx.file.filename);\n  (void) FormatImageProperty(image,\"document\",\"%.100s\",dpx.file.filename);\n  offset+=ReadBlob(image,sizeof(dpx.file.timestamp),(unsigned char *)\n    dpx.file.timestamp);\n  if (*dpx.file.timestamp != '\\0')\n    (void) FormatImageProperty(image,\"dpx:file.timestamp\",\"%.24s\",\n      dpx.file.timestamp);\n  offset+=ReadBlob(image,sizeof(dpx.file.creator),(unsigned char *)\n    dpx.file.creator);\n  if (*dpx.file.creator == '\\0')\n    {\n      char\n        *url;\n\n      url=GetMagickHomeURL();\n      (void) FormatImageProperty(image,\"dpx:file.creator\",\"%.100s\",url);\n      (void) FormatImageProperty(image,\"software\",\"%.100s\",url);\n      url=DestroyString(url);\n    }\n  else\n    {\n      (void) FormatImageProperty(image,\"dpx:file.creator\",\"%.100s\",\n        dpx.file.creator);\n      (void) FormatImageProperty(image,\"software\",\"%.100s\",dpx.file.creator);\n    }\n  offset+=ReadBlob(image,sizeof(dpx.file.project),(unsigned char *)\n    dpx.file.project);\n  if (*dpx.file.project != '\\0')\n    {\n      (void) FormatImageProperty(image,\"dpx:file.project\",\"%.200s\",\n        dpx.file.project);\n      (void) FormatImageProperty(image,\"comment\",\"%.100s\",dpx.file.project);\n    }\n  offset+=ReadBlob(image,sizeof(dpx.file.copyright),(unsigned char *)\n    dpx.file.copyright);\n  if (*dpx.file.copyright != '\\0')\n    {\n      (void) FormatImageProperty(image,\"dpx:file.copyright\",\"%.200s\",\n        dpx.file.copyright);\n      (void) FormatImageProperty(image,\"copyright\",\"%.100s\",\n        dpx.file.copyright);\n    }\n  dpx.file.encrypt_key=ReadBlobLong(image);\n  offset+=4;\n  if (dpx.file.encrypt_key != ~0U)\n    (void) FormatImageProperty(image,\"dpx:file.encrypt_key\",\"%u\",\n      dpx.file.encrypt_key);\n  offset+=ReadBlob(image,sizeof(dpx.file.reserve),(unsigned char *)\n    dpx.file.reserve);\n  /*\n    Read DPX image header.\n  */\n  dpx.image.orientation=ReadBlobShort(image);\n  if (dpx.image.orientation > 7)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  offset+=2;\n  if (dpx.image.orientation != (unsigned short) ~0)\n    (void) FormatImageProperty(image,\"dpx:image.orientation\",\"%d\",\n      dpx.image.orientation);\n  switch (dpx.image.orientation)\n  {\n    default:\n    case 0: image->orientation=TopLeftOrientation; break;\n    case 1: image->orientation=TopRightOrientation; break;\n    case 2: image->orientation=BottomLeftOrientation; break;\n    case 3: image->orientation=BottomRightOrientation; break;\n    case 4: image->orientation=LeftTopOrientation; break;\n    case 5: image->orientation=RightTopOrientation; break;\n    case 6: image->orientation=LeftBottomOrientation; break;\n    case 7: image->orientation=RightBottomOrientation; break;\n  }\n  dpx.image.number_elements=ReadBlobShort(image);\n  if (dpx.image.number_elements > MaxNumberImageElements)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  offset+=2;\n  dpx.image.pixels_per_line=ReadBlobLong(image);\n  offset+=4;\n  image->columns=dpx.image.pixels_per_line;\n  dpx.image.lines_per_element=ReadBlobLong(image);\n  offset+=4;\n  image->rows=dpx.image.lines_per_element;\n  for (i=0; i < 8; i++)\n  {\n    char\n      property[MaxTextExtent];\n\n    dpx.image.image_element[i].data_sign=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].low_data=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].low_quantity=ReadBlobFloat(image);\n    offset+=4;\n    dpx.image.image_element[i].high_data=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].high_quantity=ReadBlobFloat(image);\n    offset+=4;\n    dpx.image.image_element[i].descriptor=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].transfer_characteristic=(unsigned char)\n      ReadBlobByte(image);\n    (void) FormatLocaleString(property,MaxTextExtent,\n      \"dpx:image.element[%lu].transfer-characteristic\",(long) i);\n    (void) FormatImageProperty(image,property,\"%s\",\n      GetImageTransferCharacteristic((DPXTransferCharacteristic)\n      dpx.image.image_element[i].transfer_characteristic));\n    offset++;\n    dpx.image.image_element[i].colorimetric=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].bit_size=(unsigned char) ReadBlobByte(image);\n    offset++;\n    dpx.image.image_element[i].packing=ReadBlobShort(image);\n    offset+=2;\n    dpx.image.image_element[i].encoding=ReadBlobShort(image);\n    offset+=2;\n    dpx.image.image_element[i].data_offset=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].end_of_line_padding=ReadBlobLong(image);\n    offset+=4;\n    dpx.image.image_element[i].end_of_image_padding=ReadBlobLong(image);\n    offset+=4;\n    offset+=ReadBlob(image,sizeof(dpx.image.image_element[i].description),\n      (unsigned char *) dpx.image.image_element[i].description);\n  }\n  (void) SetImageColorspace(image,RGBColorspace);\n  offset+=ReadBlob(image,sizeof(dpx.image.reserve),(unsigned char *)\n    dpx.image.reserve);\n  if (dpx.file.image_offset >= 1664U)\n    {\n      /*\n        Read DPX orientation header.\n      */\n      dpx.orientation.x_offset=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.x_offset != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_offset\",\"%u\",\n          dpx.orientation.x_offset);\n      dpx.orientation.y_offset=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.y_offset != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_offset\",\"%u\",\n          dpx.orientation.y_offset);\n      dpx.orientation.x_center=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.orientation.x_center) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_center\",\"%g\",\n          dpx.orientation.x_center);\n      dpx.orientation.y_center=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.orientation.y_center) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_center\",\"%g\",\n          dpx.orientation.y_center);\n      dpx.orientation.x_size=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.x_size != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.x_size\",\"%u\",\n          dpx.orientation.x_size);\n      dpx.orientation.y_size=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.orientation.y_size != ~0U)\n        (void) FormatImageProperty(image,\"dpx:orientation.y_size\",\"%u\",\n          dpx.orientation.y_size);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.filename),(unsigned char *)\n        dpx.orientation.filename);\n      if (*dpx.orientation.filename != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.filename\",\"%.100s\",\n          dpx.orientation.filename);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.timestamp),(unsigned char *)\n        dpx.orientation.timestamp);\n      if (*dpx.orientation.timestamp != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.timestamp\",\"%.24s\",\n          dpx.orientation.timestamp);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.device),(unsigned char *)\n        dpx.orientation.device);\n      if (*dpx.orientation.device != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.device\",\"%.32s\",\n          dpx.orientation.device);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.serial),(unsigned char *)\n        dpx.orientation.serial);\n      if (*dpx.orientation.serial != '\\0')\n        (void) FormatImageProperty(image,\"dpx:orientation.serial\",\"%.32s\",\n          dpx.orientation.serial);\n      for (i=0; i < 4; i++)\n      {\n        dpx.orientation.border[i]=ReadBlobShort(image);\n        offset+=2;\n      }\n      if ((dpx.orientation.border[0] != (unsigned short) (~0)) &&\n          (dpx.orientation.border[1] != (unsigned short) (~0)))\n        (void) FormatImageProperty(image,\"dpx:orientation.border\",\"%dx%d%+d%+d\",          dpx.orientation.border[0],dpx.orientation.border[1],\n          dpx.orientation.border[2],dpx.orientation.border[3]);\n      for (i=0; i < 2; i++)\n      {\n        dpx.orientation.aspect_ratio[i]=ReadBlobLong(image);\n        offset+=4;\n      }\n      if ((dpx.orientation.aspect_ratio[0] != ~0U) &&\n          (dpx.orientation.aspect_ratio[1] != ~0U))\n        (void) FormatImageProperty(image,\"dpx:orientation.aspect_ratio\",\n          \"%ux%u\",dpx.orientation.aspect_ratio[0],\n          dpx.orientation.aspect_ratio[1]);\n      offset+=ReadBlob(image,sizeof(dpx.orientation.reserve),(unsigned char *)\n        dpx.orientation.reserve);\n    }\n  if (dpx.file.image_offset >= 1920U)\n    {\n      /*\n        Read DPX film header.\n      */\n      offset+=ReadBlob(image,sizeof(dpx.film.id),(unsigned char *) dpx.film.id);\n      if (*dpx.film.id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.id\",\"%.2s\",dpx.film.id);\n      offset+=ReadBlob(image,sizeof(dpx.film.type),(unsigned char *)\n        dpx.film.type);\n      if (*dpx.film.type != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.type\",\"%.2s\",dpx.film.type);\n      offset+=ReadBlob(image,sizeof(dpx.film.offset),(unsigned char *)\n        dpx.film.offset);\n      if (*dpx.film.offset != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.offset\",\"%.2s\",\n          dpx.film.offset);\n      offset+=ReadBlob(image,sizeof(dpx.film.prefix),(unsigned char *)\n        dpx.film.prefix);\n      if (*dpx.film.prefix != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.prefix\",\"%.6s\",\n          dpx.film.prefix);\n      offset+=ReadBlob(image,sizeof(dpx.film.count),(unsigned char *)\n        dpx.film.count);\n      if (*dpx.film.count != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.count\",\"%.4s\",\n          dpx.film.count);\n      offset+=ReadBlob(image,sizeof(dpx.film.format),(unsigned char *)\n        dpx.film.format);\n      if (*dpx.film.format != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.format\",\"%.4s\",\n          dpx.film.format);\n      dpx.film.frame_position=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.frame_position != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.frame_position\",\"%u\",\n          dpx.film.frame_position);\n      dpx.film.sequence_extent=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.sequence_extent != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.sequence_extent\",\"%u\",\n          dpx.film.sequence_extent);\n      dpx.film.held_count=ReadBlobLong(image);\n      offset+=4;\n      if (dpx.film.held_count != ~0U)\n        (void) FormatImageProperty(image,\"dpx:film.held_count\",\"%u\",\n          dpx.film.held_count);\n      dpx.film.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.film.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.frame_rate\",\"%g\",\n          dpx.film.frame_rate);\n      dpx.film.shutter_angle=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.film.shutter_angle) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.shutter_angle\",\"%g\",\n          dpx.film.shutter_angle);\n      offset+=ReadBlob(image,sizeof(dpx.film.frame_id),(unsigned char *)\n        dpx.film.frame_id);\n      if (*dpx.film.frame_id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.frame_id\",\"%.32s\",\n          dpx.film.frame_id);\n      offset+=ReadBlob(image,sizeof(dpx.film.slate),(unsigned char *)\n        dpx.film.slate);\n      if (*dpx.film.slate != '\\0')\n        (void) FormatImageProperty(image,\"dpx:film.slate\",\"%.100s\",\n          dpx.film.slate);\n      offset+=ReadBlob(image,sizeof(dpx.film.reserve),(unsigned char *)\n        dpx.film.reserve);\n    }\n  if (dpx.file.image_offset >= 2048U)\n    {\n      /*\n        Read DPX television header.\n      */\n      dpx.television.time_code=(unsigned int) ReadBlobLong(image);\n      offset+=4;\n      TimeCodeToString(dpx.television.time_code,value);\n      (void) SetImageProperty(image,\"dpx:television.time.code\",value);\n      dpx.television.user_bits=(unsigned int) ReadBlobLong(image);\n      offset+=4;\n      TimeCodeToString(dpx.television.user_bits,value);\n      (void) SetImageProperty(image,\"dpx:television.user.bits\",value);\n      dpx.television.interlace=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.interlace != 0)\n        (void) FormatImageProperty(image,\"dpx:television.interlace\",\"%.20g\",\n          (double) dpx.television.interlace);\n      dpx.television.field_number=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.field_number != 0)\n        (void) FormatImageProperty(image,\"dpx:television.field_number\",\"%.20g\",\n          (double) dpx.television.field_number);\n      dpx.television.video_signal=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.video_signal != 0)\n        (void) FormatImageProperty(image,\"dpx:television.video_signal\",\"%.20g\",\n          (double) dpx.television.video_signal);\n      dpx.television.padding=(unsigned char) ReadBlobByte(image);\n      offset++;\n      if (dpx.television.padding != 0)\n        (void) FormatImageProperty(image,\"dpx:television.padding\",\"%d\",\n          dpx.television.padding);\n      dpx.television.horizontal_sample_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.horizontal_sample_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\n          \"dpx:television.horizontal_sample_rate\",\"%g\",\n          dpx.television.horizontal_sample_rate);\n      dpx.television.vertical_sample_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.vertical_sample_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.vertical_sample_rate\",\n          \"%g\",dpx.television.vertical_sample_rate);\n      dpx.television.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.frame_rate\",\"%g\",\n          dpx.television.frame_rate);\n      dpx.television.time_offset=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.time_offset) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.time_offset\",\"%g\",\n          dpx.television.time_offset);\n      dpx.television.gamma=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.gamma) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.gamma\",\"%g\",\n          dpx.television.gamma);\n      dpx.television.black_level=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.black_level) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.black_level\",\"%g\",\n          dpx.television.black_level);\n      dpx.television.black_gain=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.black_gain) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.black_gain\",\"%g\",\n          dpx.television.black_gain);\n      dpx.television.break_point=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.break_point) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.break_point\",\"%g\",\n          dpx.television.break_point);\n      dpx.television.white_level=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.white_level) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.white_level\",\"%g\",\n          dpx.television.white_level);\n      dpx.television.integration_times=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(dpx.television.integration_times) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:television.integration_times\",\n          \"%g\",dpx.television.integration_times);\n      offset+=ReadBlob(image,sizeof(dpx.television.reserve),(unsigned char *)\n        dpx.television.reserve);\n    }\n  if (dpx.file.image_offset > 2080U)\n    {\n      /*\n        Read DPX user header.\n      */\n      offset+=ReadBlob(image,sizeof(dpx.user.id),(unsigned char *) dpx.user.id);\n      if (*dpx.user.id != '\\0')\n        (void) FormatImageProperty(image,\"dpx:user.id\",\"%.32s\",dpx.user.id);\n      if ((dpx.file.user_size != ~0U) &&\n          ((size_t) dpx.file.user_size > sizeof(dpx.user.id)))\n        {\n          StringInfo\n            *profile;\n\n           if (dpx.file.user_size > GetBlobSize(image))\n             ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n           profile=BlobToStringInfo((const void *) NULL,\n             dpx.file.user_size-sizeof(dpx.user.id));\n           if (profile == (StringInfo *) NULL)\n             ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n           offset+=ReadBlob(image,GetStringInfoLength(profile),\n             GetStringInfoDatum(profile));\n           (void) SetImageProfile(image,\"dpx:user-data\",profile);\n           profile=DestroyStringInfo(profile);\n        }\n    }\n  for ( ; offset < (MagickOffsetType) dpx.file.image_offset; offset++)\n    if (ReadBlobByte(image) == EOF)\n      break;\n  if (EOFBlob(image) != MagickFalse)\n    ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n      image->filename);\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(GetFirstImageInList(image));\n    }\n  status=SetImageExtent(image,image->columns,image->rows);\n  if (status == MagickFalse)\n    {\n      InheritException(exception,&image->exception);\n      return(DestroyImageList(image));\n    }\n  for (n=0; n < (ssize_t) dpx.image.number_elements; n++)\n  {\n    /*\n      Convert DPX raster image to pixel packets.\n    */\n    if ((dpx.image.image_element[n].data_offset != ~0U) &&\n        (dpx.image.image_element[n].data_offset != 0U))\n      {\n         MagickOffsetType\n           data_offset;\n\n         data_offset=(MagickOffsetType) dpx.image.image_element[n].data_offset;\n         if (data_offset < offset)\n           offset=SeekBlob(image,data_offset,SEEK_SET);\n         else\n           for ( ; offset < data_offset; offset++)\n             if (ReadBlobByte(image) == EOF)\n               break;\n          if (offset != data_offset)\n            ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n       }\n    SetPrimaryChromaticity((DPXColorimetric)\n      dpx.image.image_element[n].colorimetric,&image->chromaticity);\n    image->depth=dpx.image.image_element[n].bit_size;\n    samples_per_pixel=1;\n    quantum_type=GrayQuantum;\n    component_type=dpx.image.image_element[n].descriptor;\n    switch (component_type)\n    {\n      case CbYCrY422ComponentType:\n      {\n        samples_per_pixel=2;\n        quantum_type=CbYCrYQuantum;\n        break;\n      }\n      case CbYACrYA4224ComponentType:\n      case CbYCr444ComponentType:\n      {\n        samples_per_pixel=3;\n        quantum_type=CbYCrQuantum;\n        break;\n      }\n      case RGBComponentType:\n      {\n        samples_per_pixel=3;\n        quantum_type=RGBQuantum;\n        break;\n      }\n      case ABGRComponentType:\n      case RGBAComponentType:\n      {\n        image->matte=MagickTrue;\n        samples_per_pixel=4;\n        quantum_type=RGBAQuantum;\n        break;\n      }\n      default:\n        break;\n    }\n    switch (component_type)\n    {\n      case CbYCrY422ComponentType:\n      case CbYACrYA4224ComponentType:\n      case CbYCr444ComponentType:\n      {\n        (void) SetImageColorspace(image,Rec709YCbCrColorspace);\n        break;\n      }\n      case LumaComponentType:\n      {\n        (void) SetImageColorspace(image,GRAYColorspace);\n        break;\n      }\n      default:\n      {\n        (void) SetImageColorspace(image,RGBColorspace);\n        if (dpx.image.image_element[n].transfer_characteristic == LogarithmicColorimetric)\n          (void) SetImageColorspace(image,LogColorspace);\n        if (dpx.image.image_element[n].transfer_characteristic == PrintingDensityColorimetric)\n          (void) SetImageColorspace(image,LogColorspace);\n        break;\n      }\n    }\n    extent=GetBytesPerRow(image->columns,samples_per_pixel,image->depth,\n      dpx.image.image_element[n].packing == 0 ? MagickFalse : MagickTrue);\n    /*\n      DPX any-bit pixel format.\n    */\n    status=MagickTrue;\n    row=0;\n    quantum_info=AcquireQuantumInfo(image_info,image);\n    if (quantum_info == (QuantumInfo *) NULL)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    SetQuantumQuantum(quantum_info,32);\n    SetQuantumPack(quantum_info,dpx.image.image_element[n].packing == 0 ?\n      MagickTrue : MagickFalse);\n    for (y=0; y < (ssize_t) image->rows; y++)\n    {\n      const unsigned char\n        *pixels;\n\n      MagickBooleanType\n        sync;\n\n      register PixelPacket\n        *q;\n\n      size_t\n        length;\n\n      ssize_t\n        count,\n        offset;\n\n      if (status == MagickFalse)\n        continue;\n      pixels=(const unsigned char *) ReadBlobStream(image,extent,\n        GetQuantumPixels(quantum_info),&count);\n      if (count != (ssize_t) extent)\n        status=MagickFalse;\n      if ((image->progress_monitor != (MagickProgressMonitor) NULL) &&\n          (image->previous == (Image *) NULL))\n        {\n          MagickBooleanType\n            proceed;\n\n          proceed=SetImageProgress(image,LoadImageTag,(MagickOffsetType) row,\n            image->rows);\n          if (proceed == MagickFalse)\n            status=MagickFalse;\n        }\n      offset=row++;\n      q=QueueAuthenticPixels(image,0,offset,image->columns,1,exception);\n      if (q == (PixelPacket *) NULL)\n        {\n          status=MagickFalse;\n          continue;\n        }\n      length=ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,\n        quantum_type,pixels,exception);\n      (void) length;\n      sync=SyncAuthenticPixels(image,exception);\n      if (sync == MagickFalse)\n        status=MagickFalse;\n    }\n    quantum_info=DestroyQuantumInfo(quantum_info);\n    if (status == MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnableToReadImageData\");\n    SetQuantumImageType(image,quantum_type);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n        image->filename);\n  }\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -463,6 +463,8 @@\n           StringInfo\n             *profile;\n \n+           if (dpx.file.user_size > GetBlobSize(image))\n+             ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n            profile=BlobToStringInfo((const void *) NULL,\n              dpx.file.user_size-sizeof(dpx.user.id));\n            if (profile == (StringInfo *) NULL)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "           if (dpx.file.user_size > GetBlobSize(image))",
                "             ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-11530",
        "func_name": "ImageMagick/ReadEPTImage",
        "description": "The ReadEPTImage function in coders/ept.c in ImageMagick before 6.9.9-0 and 7.x before 7.0.6-1 allows remote attackers to cause a denial of service (memory consumption) via a crafted file.",
        "git_url": "https://github.com/ImageMagick/ImageMagick/commit/a14e7f1f78d99891132e061c05f83aefc59e049a",
        "commit_title": "https://github.com/ImageMagick/ImageMagick/issues/524",
        "commit_text": "",
        "func_before": "static Image *ReadEPTImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  EPTInfo\n    ept_info;\n\n  Image\n    *image;\n\n  ImageInfo\n    *read_info;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  ssize_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  ept_info.magick=ReadBlobLSBLong(image);\n  if (ept_info.magick != 0xc6d3d0c5ul)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  ept_info.postscript_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n  ept_info.postscript_length=ReadBlobLSBLong(image);\n  (void) ReadBlobLSBLong(image);\n  (void) ReadBlobLSBLong(image);\n  ept_info.tiff_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n  ept_info.tiff_length=ReadBlobLSBLong(image);\n  (void) ReadBlobLSBShort(image);\n  ept_info.postscript=(unsigned char *) AcquireQuantumMemory(\n    ept_info.postscript_length+1,sizeof(*ept_info.postscript));\n  if (ept_info.postscript == (unsigned char *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n  (void) ResetMagickMemory(ept_info.postscript,0,(ept_info.postscript_length+1)*\n    sizeof(*ept_info.postscript));\n  ept_info.tiff=(unsigned char *) AcquireQuantumMemory(ept_info.tiff_length+1,\n    sizeof(*ept_info.tiff));\n  if (ept_info.tiff == (unsigned char *) NULL)\n    {\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  (void) ResetMagickMemory(ept_info.tiff,0,(ept_info.tiff_length+1)*\n    sizeof(*ept_info.tiff));\n  offset=SeekBlob(image,ept_info.tiff_offset,SEEK_SET);\n  if ((ept_info.tiff_length != 0) && (offset < 30))\n    {\n      ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n  count=ReadBlob(image,ept_info.tiff_length,ept_info.tiff);\n  if (count != (ssize_t) (ept_info.tiff_length))\n    (void) ThrowMagickException(exception,GetMagickModule(),CorruptImageWarning,\n      \"InsufficientImageDataInFile\",\"`%s'\",image->filename);\n  offset=SeekBlob(image,ept_info.postscript_offset,SEEK_SET);\n  if ((ept_info.postscript_length != 0) && (offset < 30))\n    {\n      ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n  count=ReadBlob(image,ept_info.postscript_length,ept_info.postscript);\n  if (count != (ssize_t) (ept_info.postscript_length))\n    (void) ThrowMagickException(exception,GetMagickModule(),CorruptImageWarning,\n      \"InsufficientImageDataInFile\",\"`%s'\",image->filename);\n  (void) CloseBlob(image);\n  image=DestroyImage(image);\n  read_info=CloneImageInfo(image_info);\n  (void) CopyMagickString(read_info->magick,\"EPS\",MagickPathExtent);\n  image=BlobToImage(read_info,ept_info.postscript,ept_info.postscript_length,\n    exception);\n  if (image == (Image *) NULL)\n    {\n      (void) CopyMagickString(read_info->magick,\"TIFF\",MagickPathExtent);\n      image=BlobToImage(read_info,ept_info.tiff,ept_info.tiff_length,exception);\n    }\n  read_info=DestroyImageInfo(read_info);\n  if (image != (Image *) NULL)\n    {\n      (void) CopyMagickString(image->filename,image_info->filename,\n        MagickPathExtent);\n      (void) CopyMagickString(image->magick,\"EPT\",MagickPathExtent);\n    }\n  ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n  ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n    ept_info.postscript);\n  return(image);\n}",
        "func": "static Image *ReadEPTImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n  EPTInfo\n    ept_info;\n\n  Image\n    *image;\n\n  ImageInfo\n    *read_info;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  ssize_t\n    count;\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  ept_info.magick=ReadBlobLSBLong(image);\n  if (ept_info.magick != 0xc6d3d0c5ul)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  ept_info.postscript_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n  ept_info.postscript_length=ReadBlobLSBLong(image);\n  if (ept_info.postscript_length > GetBlobSize(image))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlobLSBLong(image);\n  (void) ReadBlobLSBLong(image);\n  ept_info.tiff_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n  ept_info.tiff_length=ReadBlobLSBLong(image);\n  if (ept_info.tiff_length > GetBlobSize(image))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  (void) ReadBlobLSBShort(image);\n  ept_info.postscript=(unsigned char *) AcquireQuantumMemory(\n    ept_info.postscript_length+1,sizeof(*ept_info.postscript));\n  if (ept_info.postscript == (unsigned char *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n  (void) ResetMagickMemory(ept_info.postscript,0,(ept_info.postscript_length+1)*\n    sizeof(*ept_info.postscript));\n  ept_info.tiff=(unsigned char *) AcquireQuantumMemory(ept_info.tiff_length+1,\n    sizeof(*ept_info.tiff));\n  if (ept_info.tiff == (unsigned char *) NULL)\n    {\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  (void) ResetMagickMemory(ept_info.tiff,0,(ept_info.tiff_length+1)*\n    sizeof(*ept_info.tiff));\n  offset=SeekBlob(image,ept_info.tiff_offset,SEEK_SET);\n  if ((ept_info.tiff_length != 0) && (offset < 30))\n    {\n      ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n  count=ReadBlob(image,ept_info.tiff_length,ept_info.tiff);\n  if (count != (ssize_t) (ept_info.tiff_length))\n    (void) ThrowMagickException(exception,GetMagickModule(),CorruptImageWarning,\n      \"InsufficientImageDataInFile\",\"`%s'\",image->filename);\n  offset=SeekBlob(image,ept_info.postscript_offset,SEEK_SET);\n  if ((ept_info.postscript_length != 0) && (offset < 30))\n    {\n      ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n      ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n        ept_info.postscript);\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    }\n  count=ReadBlob(image,ept_info.postscript_length,ept_info.postscript);\n  if (count != (ssize_t) (ept_info.postscript_length))\n    (void) ThrowMagickException(exception,GetMagickModule(),CorruptImageWarning,\n      \"InsufficientImageDataInFile\",\"`%s'\",image->filename);\n  (void) CloseBlob(image);\n  image=DestroyImage(image);\n  read_info=CloneImageInfo(image_info);\n  (void) CopyMagickString(read_info->magick,\"EPS\",MagickPathExtent);\n  image=BlobToImage(read_info,ept_info.postscript,ept_info.postscript_length,\n    exception);\n  if (image == (Image *) NULL)\n    {\n      (void) CopyMagickString(read_info->magick,\"TIFF\",MagickPathExtent);\n      image=BlobToImage(read_info,ept_info.tiff,ept_info.tiff_length,exception);\n    }\n  read_info=DestroyImageInfo(read_info);\n  if (image != (Image *) NULL)\n    {\n      (void) CopyMagickString(image->filename,image_info->filename,\n        MagickPathExtent);\n      (void) CopyMagickString(image->magick,\"EPT\",MagickPathExtent);\n    }\n  ept_info.tiff=(unsigned char *) RelinquishMagickMemory(ept_info.tiff);\n  ept_info.postscript=(unsigned char *) RelinquishMagickMemory(\n    ept_info.postscript);\n  return(image);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,10 +40,14 @@\n     ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n   ept_info.postscript_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n   ept_info.postscript_length=ReadBlobLSBLong(image);\n+  if (ept_info.postscript_length > GetBlobSize(image))\n+    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n   (void) ReadBlobLSBLong(image);\n   (void) ReadBlobLSBLong(image);\n   ept_info.tiff_offset=(MagickOffsetType) ReadBlobLSBLong(image);\n   ept_info.tiff_length=ReadBlobLSBLong(image);\n+  if (ept_info.tiff_length > GetBlobSize(image))\n+    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n   (void) ReadBlobLSBShort(image);\n   ept_info.postscript=(unsigned char *) AcquireQuantumMemory(\n     ept_info.postscript_length+1,sizeof(*ept_info.postscript));",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (ept_info.postscript_length > GetBlobSize(image))",
                "    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");",
                "  if (ept_info.tiff_length > GetBlobSize(image))",
                "    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3622",
        "func_name": "libguestfs/hivex/_get_children",
        "description": "A flaw was found in the hivex library. This flaw allows an attacker to input a specially crafted Windows Registry (hive) file, which would cause hivex to recursively call the _get_children() function, leading to a stack overflow. The highest threat from this vulnerability is to system availability.",
        "git_url": "https://github.com/libguestfs/hivex/commit/771728218dac2fbf6997a7e53225e75a4c6b7255",
        "commit_title": "lib/node.c: Limit recursion in ri-records (CVE-2021-3622)",
        "commit_text": " Windows Registry hive \"ri\"-records are arbitrarily nested B-tree-like structures:    +-------------+   | ri          |   |-------------|   | nr_offsets  |   |   offset[0] ------>  points to another lf/lh/li/ri block   |   offset[1] ------>   |   offset[2] ------>   +-------------+  It is possible to construct a hive with a very deeply nested tree of ri-records, causing the internal _get_children function to recurse to any depth which can cause programs linked to hivex to crash with a stack overflow.  Since it is not thought that deeply nested ri-records occur in real hives, limit recursion depth.  If you hit this limit you will see the following error and the operation will return an error instead of crashing:    \\> ls   hivex: _get_children: returning EINVAL because: ri-record nested to depth >= 32   ls: Invalid argument  Thanks to Jeremy Galindo for finding and reporting this bug.  (cherry picked from commit 781a12c4a49dd81365c9c567c5aa5e19e894ba0e)",
        "func_before": "static int\n_get_children (hive_h *h, hive_node_h blkoff,\n               offset_list *children, offset_list *blocks,\n               int flags)\n{\n  /* Add this intermediate block. */\n  if (_hivex_add_to_offset_list (blocks, blkoff) == -1)\n    return -1;\n\n  struct ntreg_hbin_block *block =\n    (struct ntreg_hbin_block *) ((char *) h->addr + blkoff);\n\n  size_t len = block_len (h, blkoff, NULL);\n\n  /* Points to lf-record?  (Note, also \"lh\" but that is basically the\n   * same as \"lf\" as far as we are concerned here).\n   */\n  if (block->id[0] == 'l' && (block->id[1] == 'f' || block->id[1] == 'h')) {\n    struct ntreg_lf_record *lf = (struct ntreg_lf_record *) block;\n\n    /* Check number of subkeys in the nk-record matches number of subkeys\n     * in the lf-record.\n     */\n    size_t nr_subkeys_in_lf = le16toh (lf->nr_keys);\n\n    if (8 + nr_subkeys_in_lf * 8 > len) {\n      SET_ERRNO (EFAULT, \"too many subkeys (%zu, %zu)\", nr_subkeys_in_lf, len);\n      return -1;\n    }\n\n    size_t i;\n    for (i = 0; i < nr_subkeys_in_lf; ++i) {\n      hive_node_h subkey = le32toh (lf->keys[i].offset);\n      subkey += 0x1000;\n      if (check_child_is_nk_block (h, subkey, flags) == -1) {\n        if (h->unsafe) {\n          DEBUG (2, \"subkey at 0x%zx is not an NK block, skipping\", subkey);\n          continue;\n        } else {\n          return -1;\n        }\n      }\n      if (_hivex_add_to_offset_list (children, subkey) == -1)\n        return -1;\n    }\n  }\n  /* Points to li-record? */\n  else if (block->id[0] == 'l' && block->id[1] == 'i') {\n    /* li-records are formatted the same as ri-records, but they\n     * contain direct links to child records (same as lf/lh), so\n     * we treat them the same way as lf/lh.\n     */\n    struct ntreg_ri_record *ri = (struct ntreg_ri_record *) block;\n\n    /* Check number of subkeys in the nk-record matches number of subkeys\n     * in the li-record.\n     */\n    size_t nr_offsets = le16toh (ri->nr_offsets);\n\n    if (8 + nr_offsets * 4 > len) {\n      SET_ERRNO (EFAULT, \"too many offsets (%zu, %zu)\", nr_offsets, len);\n      return -1;\n    }\n\n    size_t i;\n    for (i = 0; i < nr_offsets; ++i) {\n      hive_node_h subkey = le32toh (ri->offset[i]);\n      subkey += 0x1000;\n      if (check_child_is_nk_block (h, subkey, flags) == -1) {\n        if (h->unsafe) {\n          DEBUG (2, \"subkey at 0x%zx is not an NK block, skipping\", subkey);\n          continue;\n        } else {\n          return -1;\n        }\n      }\n      if (_hivex_add_to_offset_list (children, subkey) == -1)\n        return -1;\n    }\n  }\n  /* Points to ri-record? */\n  else if (block->id[0] == 'r' && block->id[1] == 'i') {\n    struct ntreg_ri_record *ri = (struct ntreg_ri_record *) block;\n\n    size_t nr_offsets = le16toh (ri->nr_offsets);\n\n    if (8 + nr_offsets * 4 > len) {\n      SET_ERRNO (EFAULT, \"too many offsets (%zu, %zu)\", nr_offsets, len);\n      return -1;\n    }\n\n    /* Copy list of children. */\n    size_t i;\n    for (i = 0; i < nr_offsets; ++i) {\n      hive_node_h offset = le32toh (ri->offset[i]);\n      offset += 0x1000;\n      if (!IS_VALID_BLOCK (h, offset)) {\n        if (h->unsafe) {\n          DEBUG (2, \"ri-offset is not a valid block (0x%zx), skipping\", offset);\n          continue;\n        } else {\n          SET_ERRNO (EFAULT, \"ri-offset is not a valid block (0x%zx)\", offset);\n          return -1;\n        }\n      }\n\n      if (_get_children (h, offset, children, blocks, flags) == -1)\n        return -1;\n    }\n  }\n  else {\n    SET_ERRNO (ENOTSUP,\n               \"subkey block is not lf/lh/li/ri (0x%zx, %d, %d)\",\n               blkoff, block->id[0], block->id[1]);\n    return -1;\n  }\n\n  return 0;\n}",
        "func": "static int\n_get_children (hive_h *h, hive_node_h blkoff,\n               offset_list *children, offset_list *blocks,\n               int flags, unsigned depth)\n{\n  /* Add this intermediate block. */\n  if (_hivex_add_to_offset_list (blocks, blkoff) == -1)\n    return -1;\n\n  struct ntreg_hbin_block *block =\n    (struct ntreg_hbin_block *) ((char *) h->addr + blkoff);\n\n  size_t len = block_len (h, blkoff, NULL);\n\n  /* Points to lf-record?  (Note, also \"lh\" but that is basically the\n   * same as \"lf\" as far as we are concerned here).\n   */\n  if (block->id[0] == 'l' && (block->id[1] == 'f' || block->id[1] == 'h')) {\n    struct ntreg_lf_record *lf = (struct ntreg_lf_record *) block;\n\n    /* Check number of subkeys in the nk-record matches number of subkeys\n     * in the lf-record.\n     */\n    size_t nr_subkeys_in_lf = le16toh (lf->nr_keys);\n\n    if (8 + nr_subkeys_in_lf * 8 > len) {\n      SET_ERRNO (EFAULT, \"too many subkeys (%zu, %zu)\", nr_subkeys_in_lf, len);\n      return -1;\n    }\n\n    size_t i;\n    for (i = 0; i < nr_subkeys_in_lf; ++i) {\n      hive_node_h subkey = le32toh (lf->keys[i].offset);\n      subkey += 0x1000;\n      if (check_child_is_nk_block (h, subkey, flags) == -1) {\n        if (h->unsafe) {\n          DEBUG (2, \"subkey at 0x%zx is not an NK block, skipping\", subkey);\n          continue;\n        } else {\n          return -1;\n        }\n      }\n      if (_hivex_add_to_offset_list (children, subkey) == -1)\n        return -1;\n    }\n  }\n  /* Points to li-record? */\n  else if (block->id[0] == 'l' && block->id[1] == 'i') {\n    /* li-records are formatted the same as ri-records, but they\n     * contain direct links to child records (same as lf/lh), so\n     * we treat them the same way as lf/lh.\n     */\n    struct ntreg_ri_record *ri = (struct ntreg_ri_record *) block;\n\n    /* Check number of subkeys in the nk-record matches number of subkeys\n     * in the li-record.\n     */\n    size_t nr_offsets = le16toh (ri->nr_offsets);\n\n    if (8 + nr_offsets * 4 > len) {\n      SET_ERRNO (EFAULT, \"too many offsets (%zu, %zu)\", nr_offsets, len);\n      return -1;\n    }\n\n    size_t i;\n    for (i = 0; i < nr_offsets; ++i) {\n      hive_node_h subkey = le32toh (ri->offset[i]);\n      subkey += 0x1000;\n      if (check_child_is_nk_block (h, subkey, flags) == -1) {\n        if (h->unsafe) {\n          DEBUG (2, \"subkey at 0x%zx is not an NK block, skipping\", subkey);\n          continue;\n        } else {\n          return -1;\n        }\n      }\n      if (_hivex_add_to_offset_list (children, subkey) == -1)\n        return -1;\n    }\n  }\n  /* Points to ri-record? */\n  else if (block->id[0] == 'r' && block->id[1] == 'i') {\n    struct ntreg_ri_record *ri = (struct ntreg_ri_record *) block;\n\n    size_t nr_offsets = le16toh (ri->nr_offsets);\n\n    if (8 + nr_offsets * 4 > len) {\n      SET_ERRNO (EFAULT, \"too many offsets (%zu, %zu)\", nr_offsets, len);\n      return -1;\n    }\n\n    /* Copy list of children. */\n    size_t i;\n    for (i = 0; i < nr_offsets; ++i) {\n      hive_node_h offset = le32toh (ri->offset[i]);\n      offset += 0x1000;\n      if (!IS_VALID_BLOCK (h, offset)) {\n        if (h->unsafe) {\n          DEBUG (2, \"ri-offset is not a valid block (0x%zx), skipping\", offset);\n          continue;\n        } else {\n          SET_ERRNO (EFAULT, \"ri-offset is not a valid block (0x%zx)\", offset);\n          return -1;\n        }\n      }\n\n      /* Although in theory hive ri records might be nested to any\n       * depth, in practice this is unlikely.  Recursing here caused\n       * CVE-2021-3622.  Thus limit the depth we will recurse to\n       * something small.\n       */\n      if (depth >= 32) {\n        SET_ERRNO (EINVAL, \"ri-record nested to depth >= %u\", depth);\n        return -1;\n      }\n\n      if (_get_children (h, offset, children, blocks, flags, depth+1) == -1)\n        return -1;\n    }\n  }\n  else {\n    SET_ERRNO (ENOTSUP,\n               \"subkey block is not lf/lh/li/ri (0x%zx, %d, %d)\",\n               blkoff, block->id[0], block->id[1]);\n    return -1;\n  }\n\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static int\n _get_children (hive_h *h, hive_node_h blkoff,\n                offset_list *children, offset_list *blocks,\n-               int flags)\n+               int flags, unsigned depth)\n {\n   /* Add this intermediate block. */\n   if (_hivex_add_to_offset_list (blocks, blkoff) == -1)\n@@ -104,7 +104,17 @@\n         }\n       }\n \n-      if (_get_children (h, offset, children, blocks, flags) == -1)\n+      /* Although in theory hive ri records might be nested to any\n+       * depth, in practice this is unlikely.  Recursing here caused\n+       * CVE-2021-3622.  Thus limit the depth we will recurse to\n+       * something small.\n+       */\n+      if (depth >= 32) {\n+        SET_ERRNO (EINVAL, \"ri-record nested to depth >= %u\", depth);\n+        return -1;\n+      }\n+\n+      if (_get_children (h, offset, children, blocks, flags, depth+1) == -1)\n         return -1;\n     }\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "               int flags)",
                "      if (_get_children (h, offset, children, blocks, flags) == -1)"
            ],
            "added_lines": [
                "               int flags, unsigned depth)",
                "      /* Although in theory hive ri records might be nested to any",
                "       * depth, in practice this is unlikely.  Recursing here caused",
                "       * CVE-2021-3622.  Thus limit the depth we will recurse to",
                "       * something small.",
                "       */",
                "      if (depth >= 32) {",
                "        SET_ERRNO (EINVAL, \"ri-record nested to depth >= %u\", depth);",
                "        return -1;",
                "      }",
                "",
                "      if (_get_children (h, offset, children, blocks, flags, depth+1) == -1)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3622",
        "func_name": "libguestfs/hivex/_hivex_get_children",
        "description": "A flaw was found in the hivex library. This flaw allows an attacker to input a specially crafted Windows Registry (hive) file, which would cause hivex to recursively call the _get_children() function, leading to a stack overflow. The highest threat from this vulnerability is to system availability.",
        "git_url": "https://github.com/libguestfs/hivex/commit/771728218dac2fbf6997a7e53225e75a4c6b7255",
        "commit_title": "lib/node.c: Limit recursion in ri-records (CVE-2021-3622)",
        "commit_text": " Windows Registry hive \"ri\"-records are arbitrarily nested B-tree-like structures:    +-------------+   | ri          |   |-------------|   | nr_offsets  |   |   offset[0] ------>  points to another lf/lh/li/ri block   |   offset[1] ------>   |   offset[2] ------>   +-------------+  It is possible to construct a hive with a very deeply nested tree of ri-records, causing the internal _get_children function to recurse to any depth which can cause programs linked to hivex to crash with a stack overflow.  Since it is not thought that deeply nested ri-records occur in real hives, limit recursion depth.  If you hit this limit you will see the following error and the operation will return an error instead of crashing:    \\> ls   hivex: _get_children: returning EINVAL because: ri-record nested to depth >= 32   ls: Invalid argument  Thanks to Jeremy Galindo for finding and reporting this bug.  (cherry picked from commit 781a12c4a49dd81365c9c567c5aa5e19e894ba0e)",
        "func_before": "int\n_hivex_get_children (hive_h *h, hive_node_h node,\n                     hive_node_h **children_ret, size_t **blocks_ret,\n                     int flags)\n{\n  if (!IS_VALID_BLOCK (h, node) || !block_id_eq (h, node, \"nk\")) {\n    SET_ERRNO (EINVAL, \"invalid block or not an 'nk' block\");\n    return -1;\n  }\n\n  struct ntreg_nk_record *nk =\n    (struct ntreg_nk_record *) ((char *) h->addr + node);\n\n  size_t nr_subkeys_in_nk = le32toh (nk->nr_subkeys);\n\n  offset_list children, blocks;\n  _hivex_init_offset_list (h, &children);\n  _hivex_init_offset_list (h, &blocks);\n\n  /* Deal with the common \"no subkeys\" case quickly. */\n  if (nr_subkeys_in_nk == 0)\n    goto out;\n\n  /* Arbitrarily limit the number of subkeys we will ever deal with. */\n  if (nr_subkeys_in_nk > HIVEX_MAX_SUBKEYS) {\n    SET_ERRNO (ERANGE,\n               \"nr_subkeys_in_nk > HIVEX_MAX_SUBKEYS (%zu > %d)\",\n               nr_subkeys_in_nk, HIVEX_MAX_SUBKEYS);\n    goto error;\n  }\n\n  /* Don't read more child nodes than the declared number of subkeys. */\n  _hivex_set_offset_list_limit (&children, nr_subkeys_in_nk);\n\n  /* Pre-1.3.8 hivex did not limit the number of intermediate blocks\n   * it would return, and there is no obvious limit to use.  However\n   * if we ever exceeded HIVEX_MAX_SUBKEYS then there's something\n   * fishy going on.\n   */\n  _hivex_set_offset_list_limit (&blocks, HIVEX_MAX_SUBKEYS);\n\n  /* Preallocate space for the children. */\n  if (_hivex_grow_offset_list (&children, nr_subkeys_in_nk) == -1)\n    goto error;\n\n  /* The subkey_lf field can point either to an lf-record, which is\n   * the common case, or if there are lots of subkeys, to an\n   * ri-record.\n   */\n  size_t subkey_lf = le32toh (nk->subkey_lf);\n  subkey_lf += 0x1000;\n  if (!IS_VALID_BLOCK (h, subkey_lf)) {\n    SET_ERRNO (EFAULT,\n               \"subkey_lf is not a valid block (0x%zx)\", subkey_lf);\n    goto error;\n  }\n\n  if (_get_children (h, subkey_lf, &children, &blocks, flags) == -1)\n    goto error;\n\n  /* Check the number of children we ended up reading matches\n   * nr_subkeys_in_nk.\n   */\n  size_t nr_children = _hivex_get_offset_list_length (&children);\n  if (nr_subkeys_in_nk != nr_children) {\n    if (!h->unsafe) {\n      SET_ERRNO (ENOTSUP,\n                 \"nr_subkeys_in_nk = %zu \"\n                 \"is not equal to number of children read %zu\",\n                 nr_subkeys_in_nk, nr_children);\n      goto error;\n    } else {\n      DEBUG (2,\n             \"nr_subkeys_in_nk = %zu \"\n             \"is not equal to number of children read %zu\",\n             nr_subkeys_in_nk, nr_children);\n    }\n  }\n\n out:\n#if 0\n  if (h->msglvl >= 2) {\n    fprintf (stderr, \"%s: %s: children = \", \"hivex\", __func__);\n    _hivex_print_offset_list (&children, stderr);\n    fprintf (stderr, \"\\n%s: %s: blocks = \", \"hivex\", __func__);\n    _hivex_print_offset_list (&blocks, stderr);\n    fprintf (stderr, \"\\n\");\n  }\n#endif\n\n  *children_ret = _hivex_return_offset_list (&children);\n  *blocks_ret = _hivex_return_offset_list (&blocks);\n  if (!*children_ret || !*blocks_ret)\n    goto error;\n  return 0;\n\n error:\n  _hivex_free_offset_list (&children);\n  _hivex_free_offset_list (&blocks);\n  return -1;\n}",
        "func": "int\n_hivex_get_children (hive_h *h, hive_node_h node,\n                     hive_node_h **children_ret, size_t **blocks_ret,\n                     int flags)\n{\n  if (!IS_VALID_BLOCK (h, node) || !block_id_eq (h, node, \"nk\")) {\n    SET_ERRNO (EINVAL, \"invalid block or not an 'nk' block\");\n    return -1;\n  }\n\n  struct ntreg_nk_record *nk =\n    (struct ntreg_nk_record *) ((char *) h->addr + node);\n\n  size_t nr_subkeys_in_nk = le32toh (nk->nr_subkeys);\n\n  offset_list children, blocks;\n  _hivex_init_offset_list (h, &children);\n  _hivex_init_offset_list (h, &blocks);\n\n  /* Deal with the common \"no subkeys\" case quickly. */\n  if (nr_subkeys_in_nk == 0)\n    goto out;\n\n  /* Arbitrarily limit the number of subkeys we will ever deal with. */\n  if (nr_subkeys_in_nk > HIVEX_MAX_SUBKEYS) {\n    SET_ERRNO (ERANGE,\n               \"nr_subkeys_in_nk > HIVEX_MAX_SUBKEYS (%zu > %d)\",\n               nr_subkeys_in_nk, HIVEX_MAX_SUBKEYS);\n    goto error;\n  }\n\n  /* Don't read more child nodes than the declared number of subkeys. */\n  _hivex_set_offset_list_limit (&children, nr_subkeys_in_nk);\n\n  /* Pre-1.3.8 hivex did not limit the number of intermediate blocks\n   * it would return, and there is no obvious limit to use.  However\n   * if we ever exceeded HIVEX_MAX_SUBKEYS then there's something\n   * fishy going on.\n   */\n  _hivex_set_offset_list_limit (&blocks, HIVEX_MAX_SUBKEYS);\n\n  /* Preallocate space for the children. */\n  if (_hivex_grow_offset_list (&children, nr_subkeys_in_nk) == -1)\n    goto error;\n\n  /* The subkey_lf field can point either to an lf-record, which is\n   * the common case, or if there are lots of subkeys, to an\n   * ri-record.\n   */\n  size_t subkey_lf = le32toh (nk->subkey_lf);\n  subkey_lf += 0x1000;\n  if (!IS_VALID_BLOCK (h, subkey_lf)) {\n    SET_ERRNO (EFAULT,\n               \"subkey_lf is not a valid block (0x%zx)\", subkey_lf);\n    goto error;\n  }\n\n  if (_get_children (h, subkey_lf, &children, &blocks, flags, 0) == -1)\n    goto error;\n\n  /* Check the number of children we ended up reading matches\n   * nr_subkeys_in_nk.\n   */\n  size_t nr_children = _hivex_get_offset_list_length (&children);\n  if (nr_subkeys_in_nk != nr_children) {\n    if (!h->unsafe) {\n      SET_ERRNO (ENOTSUP,\n                 \"nr_subkeys_in_nk = %zu \"\n                 \"is not equal to number of children read %zu\",\n                 nr_subkeys_in_nk, nr_children);\n      goto error;\n    } else {\n      DEBUG (2,\n             \"nr_subkeys_in_nk = %zu \"\n             \"is not equal to number of children read %zu\",\n             nr_subkeys_in_nk, nr_children);\n    }\n  }\n\n out:\n#if 0\n  if (h->msglvl >= 2) {\n    fprintf (stderr, \"%s: %s: children = \", \"hivex\", __func__);\n    _hivex_print_offset_list (&children, stderr);\n    fprintf (stderr, \"\\n%s: %s: blocks = \", \"hivex\", __func__);\n    _hivex_print_offset_list (&blocks, stderr);\n    fprintf (stderr, \"\\n\");\n  }\n#endif\n\n  *children_ret = _hivex_return_offset_list (&children);\n  *blocks_ret = _hivex_return_offset_list (&blocks);\n  if (!*children_ret || !*blocks_ret)\n    goto error;\n  return 0;\n\n error:\n  _hivex_free_offset_list (&children);\n  _hivex_free_offset_list (&blocks);\n  return -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -55,7 +55,7 @@\n     goto error;\n   }\n \n-  if (_get_children (h, subkey_lf, &children, &blocks, flags) == -1)\n+  if (_get_children (h, subkey_lf, &children, &blocks, flags, 0) == -1)\n     goto error;\n \n   /* Check the number of children we ended up reading matches",
        "diff_line_info": {
            "deleted_lines": [
                "  if (_get_children (h, subkey_lf, &children, &blocks, flags) == -1)"
            ],
            "added_lines": [
                "  if (_get_children (h, subkey_lf, &children, &blocks, flags, 0) == -1)"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-3534",
        "func_name": "torvalds/linux/shmem_symlink",
        "description": "The shmem_delete_inode function in mm/shmem.c in the tmpfs implementation in the Linux kernel before 2.6.26.1 allows local users to cause a denial of service (system crash) via a certain sequence of file create, remove, and overwrite operations, as demonstrated by the insserv program, related to allocation of \"useless pages\" and improper maintenance of the i_blocks count.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=14fcc23fdc78e9d32372553ccf21758a9bd56fa1",
        "commit_title": "SuSE's insserve initscript ordering program hits kernel BUG at mm/shmem.c:814",
        "commit_text": "on 2.6.26.  It's using posix_fadvise on directories, and the shmem_readpage method added in 2.6.23 is letting POSIX_FADV_WILLNEED allocate useless pages to a tmpfs directory, incrementing i_blocks count but never decrementing it.  Fix this by assigning shmem_aops (pointing to readpage and writepage and set_page_dirty) only when it's needed, on a regular file or a long symlink.  Many thanks to Kel for outstanding bugreport and steps to reproduce it.  Cc: <stable@kernel.org>\t\t[2.6.25.x, 2.6.26.x] ",
        "func_before": "static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *symname)\n{\n\tint error;\n\tint len;\n\tstruct inode *inode;\n\tstruct page *page = NULL;\n\tchar *kaddr;\n\tstruct shmem_inode_info *info;\n\n\tlen = strlen(symname) + 1;\n\tif (len > PAGE_CACHE_SIZE)\n\t\treturn -ENAMETOOLONG;\n\n\tinode = shmem_get_inode(dir->i_sb, S_IFLNK|S_IRWXUGO, 0);\n\tif (!inode)\n\t\treturn -ENOSPC;\n\n\terror = security_inode_init_security(inode, dir, NULL, NULL,\n\t\t\t\t\t     NULL);\n\tif (error) {\n\t\tif (error != -EOPNOTSUPP) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\terror = 0;\n\t}\n\n\tinfo = SHMEM_I(inode);\n\tinode->i_size = len-1;\n\tif (len <= (char *)inode - (char *)info) {\n\t\t/* do it inline */\n\t\tmemcpy(info, symname, len);\n\t\tinode->i_op = &shmem_symlink_inline_operations;\n\t} else {\n\t\terror = shmem_getpage(inode, 0, &page, SGP_WRITE, NULL);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\tunlock_page(page);\n\t\tinode->i_op = &shmem_symlink_inode_operations;\n\t\tkaddr = kmap_atomic(page, KM_USER0);\n\t\tmemcpy(kaddr, symname, len);\n\t\tkunmap_atomic(kaddr, KM_USER0);\n\t\tset_page_dirty(page);\n\t\tpage_cache_release(page);\n\t}\n\tif (dir->i_mode & S_ISGID)\n\t\tinode->i_gid = dir->i_gid;\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\n}",
        "func": "static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *symname)\n{\n\tint error;\n\tint len;\n\tstruct inode *inode;\n\tstruct page *page = NULL;\n\tchar *kaddr;\n\tstruct shmem_inode_info *info;\n\n\tlen = strlen(symname) + 1;\n\tif (len > PAGE_CACHE_SIZE)\n\t\treturn -ENAMETOOLONG;\n\n\tinode = shmem_get_inode(dir->i_sb, S_IFLNK|S_IRWXUGO, 0);\n\tif (!inode)\n\t\treturn -ENOSPC;\n\n\terror = security_inode_init_security(inode, dir, NULL, NULL,\n\t\t\t\t\t     NULL);\n\tif (error) {\n\t\tif (error != -EOPNOTSUPP) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\terror = 0;\n\t}\n\n\tinfo = SHMEM_I(inode);\n\tinode->i_size = len-1;\n\tif (len <= (char *)inode - (char *)info) {\n\t\t/* do it inline */\n\t\tmemcpy(info, symname, len);\n\t\tinode->i_op = &shmem_symlink_inline_operations;\n\t} else {\n\t\terror = shmem_getpage(inode, 0, &page, SGP_WRITE, NULL);\n\t\tif (error) {\n\t\t\tiput(inode);\n\t\t\treturn error;\n\t\t}\n\t\tunlock_page(page);\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_op = &shmem_symlink_inode_operations;\n\t\tkaddr = kmap_atomic(page, KM_USER0);\n\t\tmemcpy(kaddr, symname, len);\n\t\tkunmap_atomic(kaddr, KM_USER0);\n\t\tset_page_dirty(page);\n\t\tpage_cache_release(page);\n\t}\n\tif (dir->i_mode & S_ISGID)\n\t\tinode->i_gid = dir->i_gid;\n\tdir->i_size += BOGO_DIRENT_SIZE;\n\tdir->i_ctime = dir->i_mtime = CURRENT_TIME;\n\td_instantiate(dentry, inode);\n\tdget(dentry);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,6 +38,7 @@\n \t\t\treturn error;\n \t\t}\n \t\tunlock_page(page);\n+\t\tinode->i_mapping->a_ops = &shmem_aops;\n \t\tinode->i_op = &shmem_symlink_inode_operations;\n \t\tkaddr = kmap_atomic(page, KM_USER0);\n \t\tmemcpy(kaddr, symname, len);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tinode->i_mapping->a_ops = &shmem_aops;"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-3534",
        "func_name": "torvalds/linux/shmem_get_inode",
        "description": "The shmem_delete_inode function in mm/shmem.c in the tmpfs implementation in the Linux kernel before 2.6.26.1 allows local users to cause a denial of service (system crash) via a certain sequence of file create, remove, and overwrite operations, as demonstrated by the insserv program, related to allocation of \"useless pages\" and improper maintenance of the i_blocks count.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=14fcc23fdc78e9d32372553ccf21758a9bd56fa1",
        "commit_title": "SuSE's insserve initscript ordering program hits kernel BUG at mm/shmem.c:814",
        "commit_text": "on 2.6.26.  It's using posix_fadvise on directories, and the shmem_readpage method added in 2.6.23 is letting POSIX_FADV_WILLNEED allocate useless pages to a tmpfs directory, incrementing i_blocks count but never decrementing it.  Fix this by assigning shmem_aops (pointing to readpage and writepage and set_page_dirty) only when it's needed, on a regular file or a long symlink.  Many thanks to Kel for outstanding bugreport and steps to reproduce it.  Cc: <stable@kernel.org>\t\t[2.6.25.x, 2.6.26.x] ",
        "func_before": "static struct inode *\nshmem_get_inode(struct super_block *sb, int mode, dev_t dev)\n{\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tif (shmem_reserve_inode(sb))\n\t\treturn NULL;\n\n\tinode = new_inode(sb);\n\tif (inode) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current->fsuid;\n\t\tinode->i_gid = current->fsgid;\n\t\tinode->i_blocks = 0;\n\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\tinode->i_mapping->backing_dev_info = &shmem_backing_dev_info;\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\t\tinode->i_generation = get_seconds();\n\t\tinfo = SHMEM_I(inode);\n\t\tmemset(info, 0, (char *)inode - (char *)info);\n\t\tspin_lock_init(&info->lock);\n\t\tINIT_LIST_HEAD(&info->swaplist);\n\n\t\tswitch (mode & S_IFMT) {\n\t\tdefault:\n\t\t\tinode->i_op = &shmem_special_inode_operations;\n\t\t\tinit_special_inode(inode, mode, dev);\n\t\t\tbreak;\n\t\tcase S_IFREG:\n\t\t\tinode->i_op = &shmem_inode_operations;\n\t\t\tinode->i_fop = &shmem_file_operations;\n\t\t\tmpol_shared_policy_init(&info->policy,\n\t\t\t\t\t\t shmem_get_sbmpol(sbinfo));\n\t\t\tbreak;\n\t\tcase S_IFDIR:\n\t\t\tinc_nlink(inode);\n\t\t\t/* Some things misbehave if size == 0 on a directory */\n\t\t\tinode->i_size = 2 * BOGO_DIRENT_SIZE;\n\t\t\tinode->i_op = &shmem_dir_inode_operations;\n\t\t\tinode->i_fop = &simple_dir_operations;\n\t\t\tbreak;\n\t\tcase S_IFLNK:\n\t\t\t/*\n\t\t\t * Must not load anything in the rbtree,\n\t\t\t * mpol_free_shared_policy will not be called.\n\t\t\t */\n\t\t\tmpol_shared_policy_init(&info->policy, NULL);\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tshmem_free_inode(sb);\n\treturn inode;\n}",
        "func": "static struct inode *\nshmem_get_inode(struct super_block *sb, int mode, dev_t dev)\n{\n\tstruct inode *inode;\n\tstruct shmem_inode_info *info;\n\tstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\n\n\tif (shmem_reserve_inode(sb))\n\t\treturn NULL;\n\n\tinode = new_inode(sb);\n\tif (inode) {\n\t\tinode->i_mode = mode;\n\t\tinode->i_uid = current->fsuid;\n\t\tinode->i_gid = current->fsgid;\n\t\tinode->i_blocks = 0;\n\t\tinode->i_mapping->backing_dev_info = &shmem_backing_dev_info;\n\t\tinode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;\n\t\tinode->i_generation = get_seconds();\n\t\tinfo = SHMEM_I(inode);\n\t\tmemset(info, 0, (char *)inode - (char *)info);\n\t\tspin_lock_init(&info->lock);\n\t\tINIT_LIST_HEAD(&info->swaplist);\n\n\t\tswitch (mode & S_IFMT) {\n\t\tdefault:\n\t\t\tinode->i_op = &shmem_special_inode_operations;\n\t\t\tinit_special_inode(inode, mode, dev);\n\t\t\tbreak;\n\t\tcase S_IFREG:\n\t\t\tinode->i_mapping->a_ops = &shmem_aops;\n\t\t\tinode->i_op = &shmem_inode_operations;\n\t\t\tinode->i_fop = &shmem_file_operations;\n\t\t\tmpol_shared_policy_init(&info->policy,\n\t\t\t\t\t\t shmem_get_sbmpol(sbinfo));\n\t\t\tbreak;\n\t\tcase S_IFDIR:\n\t\t\tinc_nlink(inode);\n\t\t\t/* Some things misbehave if size == 0 on a directory */\n\t\t\tinode->i_size = 2 * BOGO_DIRENT_SIZE;\n\t\t\tinode->i_op = &shmem_dir_inode_operations;\n\t\t\tinode->i_fop = &simple_dir_operations;\n\t\t\tbreak;\n\t\tcase S_IFLNK:\n\t\t\t/*\n\t\t\t * Must not load anything in the rbtree,\n\t\t\t * mpol_free_shared_policy will not be called.\n\t\t\t */\n\t\t\tmpol_shared_policy_init(&info->policy, NULL);\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tshmem_free_inode(sb);\n\treturn inode;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,6 @@\n \t\tinode->i_uid = current->fsuid;\n \t\tinode->i_gid = current->fsgid;\n \t\tinode->i_blocks = 0;\n-\t\tinode->i_mapping->a_ops = &shmem_aops;\n \t\tinode->i_mapping->backing_dev_info = &shmem_backing_dev_info;\n \t\tinode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;\n \t\tinode->i_generation = get_seconds();\n@@ -29,6 +28,7 @@\n \t\t\tinit_special_inode(inode, mode, dev);\n \t\t\tbreak;\n \t\tcase S_IFREG:\n+\t\t\tinode->i_mapping->a_ops = &shmem_aops;\n \t\t\tinode->i_op = &shmem_inode_operations;\n \t\t\tinode->i_fop = &shmem_file_operations;\n \t\t\tmpol_shared_policy_init(&info->policy,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tinode->i_mapping->a_ops = &shmem_aops;"
            ],
            "added_lines": [
                "\t\t\tinode->i_mapping->a_ops = &shmem_aops;"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-3621",
        "func_name": "kernel/git/netdev/net/unix_stream_connect",
        "description": "net/unix/af_unix.c in the Linux kernel 2.6.31.4 and earlier allows local users to cause a denial of service (system hang) by creating an abstract-namespace AF_UNIX listening socket, performing a shutdown operation on this socket, and then performing a series of connect operations to this socket.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/davem/net.git;a=commit;h=77238f2b942b38ab4e7f3aced44084493e4a8675",
        "commit_title": "I found a deadlock bug in UNIX domain socket, which makes able to DoS",
        "commit_text": "attack against the local machine by non-root users.  How to reproduce: 1. Make a listening AF_UNIX/SOCK_STREAM socket with an abstruct     namespace(*), and shutdown(2) it.  2. Repeat connect(2)ing to the listening socket from the other sockets     until the connection backlog is full-filled.  3. connect(2) takes the CPU forever. If every core is taken, the     system hangs.  PoC code: (Run as many times as cores on SMP machines.)  int main(void) { \tint ret; \tint csd; \tint lsd; \tstruct sockaddr_un sun;  \t/* make an abstruct name address (*) */ \tmemset(&sun, 0, sizeof(sun)); \tsun.sun_family = PF_UNIX; \tsprintf(&sun.sun_path[1], \"%d\", getpid());  \t/* create the listening socket and shutdown */ \tlsd = socket(AF_UNIX, SOCK_STREAM, 0); \tbind(lsd, (struct sockaddr *)&sun, sizeof(sun)); \tlisten(lsd, 1); \tshutdown(lsd, SHUT_RDWR);  \t/* connect loop */ \talarm(15); /* forcely exit the loop after 15 sec */ \tfor (;;) { \t\tcsd = socket(AF_UNIX, SOCK_STREAM, 0); \t\tret = connect(csd, (struct sockaddr *)&sun, sizeof(sun)); \t\tif (-1 == ret) { \t\t\tperror(\"connect()\"); \t\t\tbreak; \t\t} \t\tputs(\"Connection OK\"); \t} \treturn 0; }  (*) Make sun_path[0] = 0 to use the abstruct namespace.     If a file-based socket is used, the system doesn't deadlock because     of context switches in the file system layer.  Why this happens:  Error checks between unix_socket_connect() and unix_wait_for_peer() are  inconsistent. The former calls the latter to wait until the backlog is  processed. Despite the latter returns without doing anything when the  socket is shutdown, the former doesn't check the shutdown state and  just retries calling the latter forever.  Patch:  The patch below adds shutdown check into unix_socket_connect(), so  connect(2) to the shutdown socket will return -ECONREFUSED.  ",
        "func_before": "static int unix_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int addr_len, int flags)\n{\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk), *newu, *otheru;\n\tstruct sock *newsk = NULL;\n\tstruct sock *other = NULL;\n\tstruct sk_buff *skb = NULL;\n\tunsigned hash;\n\tint st;\n\tint err;\n\tlong timeo;\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags)\n\t\t&& !u->addr && (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\t/* First of all allocate resources.\n\t   If we will make it after state is locked,\n\t   we will have to recheck all again in any case.\n\t */\n\n\terr = -ENOMEM;\n\n\t/* create new sock for complete connection */\n\tnewsk = unix_create1(sock_net(sk), NULL);\n\tif (newsk == NULL)\n\t\tgoto out;\n\n\t/* Allocate skb for sending to listening sock */\n\tskb = sock_wmalloc(newsk, 1, 0, GFP_KERNEL);\n\tif (skb == NULL)\n\t\tgoto out;\n\nrestart:\n\t/*  Find listening sock. */\n\tother = unix_find_other(net, sunaddr, addr_len, sk->sk_type, hash, &err);\n\tif (!other)\n\t\tgoto out;\n\n\t/* Latch state of peer */\n\tunix_state_lock(other);\n\n\t/* Apparently VFS overslept socket death. Retry. */\n\tif (sock_flag(other, SOCK_DEAD)) {\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = -ECONNREFUSED;\n\tif (other->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\n\tif (unix_recvq_full(other)) {\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_unlock;\n\n\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\t/* Latch our state.\n\n\t   It is tricky place. We need to grab write lock and cannot\n\t   drop lock on peer. It is dangerous because deadlock is\n\t   possible. Connect to self case and simultaneous\n\t   attempt to connect are eliminated by checking socket\n\t   state. other is TCP_LISTEN, if sk is TCP_LISTEN we\n\t   check this before attempt to grab lock.\n\n\t   Well, and we have to recheck the state after socket locked.\n\t */\n\tst = sk->sk_state;\n\n\tswitch (st) {\n\tcase TCP_CLOSE:\n\t\t/* This is ok... continue with connect */\n\t\tbreak;\n\tcase TCP_ESTABLISHED:\n\t\t/* Socket is already connected */\n\t\terr = -EISCONN;\n\t\tgoto out_unlock;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tunix_state_lock_nested(sk);\n\n\tif (sk->sk_state != st) {\n\t\tunix_state_unlock(sk);\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = security_unix_stream_connect(sock, other->sk_socket, newsk);\n\tif (err) {\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\t/* The way is open! Fastly set all the necessary fields... */\n\n\tsock_hold(sk);\n\tunix_peer(newsk)\t= sk;\n\tnewsk->sk_state\t\t= TCP_ESTABLISHED;\n\tnewsk->sk_type\t\t= sk->sk_type;\n\tnewsk->sk_peercred.pid\t= task_tgid_vnr(current);\n\tcurrent_euid_egid(&newsk->sk_peercred.uid, &newsk->sk_peercred.gid);\n\tnewu = unix_sk(newsk);\n\tnewsk->sk_sleep\t\t= &newu->peer_wait;\n\totheru = unix_sk(other);\n\n\t/* copy address information from listening to new sock*/\n\tif (otheru->addr) {\n\t\tatomic_inc(&otheru->addr->refcnt);\n\t\tnewu->addr = otheru->addr;\n\t}\n\tif (otheru->dentry) {\n\t\tnewu->dentry\t= dget(otheru->dentry);\n\t\tnewu->mnt\t= mntget(otheru->mnt);\n\t}\n\n\t/* Set credentials */\n\tsk->sk_peercred = other->sk_peercred;\n\n\tsock->state\t= SS_CONNECTED;\n\tsk->sk_state\t= TCP_ESTABLISHED;\n\tsock_hold(newsk);\n\n\tsmp_mb__after_atomic_inc();\t/* sock_hold() does an atomic_inc() */\n\tunix_peer(sk)\t= newsk;\n\n\tunix_state_unlock(sk);\n\n\t/* take ten and and send info to listening sock */\n\tspin_lock(&other->sk_receive_queue.lock);\n\t__skb_queue_tail(&other->sk_receive_queue, skb);\n\tspin_unlock(&other->sk_receive_queue.lock);\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other, 0);\n\tsock_put(other);\n\treturn 0;\n\nout_unlock:\n\tif (other)\n\t\tunix_state_unlock(other);\n\nout:\n\tkfree_skb(skb);\n\tif (newsk)\n\t\tunix_release_sock(newsk, 0);\n\tif (other)\n\t\tsock_put(other);\n\treturn err;\n}",
        "func": "static int unix_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int addr_len, int flags)\n{\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk), *newu, *otheru;\n\tstruct sock *newsk = NULL;\n\tstruct sock *other = NULL;\n\tstruct sk_buff *skb = NULL;\n\tunsigned hash;\n\tint st;\n\tint err;\n\tlong timeo;\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags)\n\t\t&& !u->addr && (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\t/* First of all allocate resources.\n\t   If we will make it after state is locked,\n\t   we will have to recheck all again in any case.\n\t */\n\n\terr = -ENOMEM;\n\n\t/* create new sock for complete connection */\n\tnewsk = unix_create1(sock_net(sk), NULL);\n\tif (newsk == NULL)\n\t\tgoto out;\n\n\t/* Allocate skb for sending to listening sock */\n\tskb = sock_wmalloc(newsk, 1, 0, GFP_KERNEL);\n\tif (skb == NULL)\n\t\tgoto out;\n\nrestart:\n\t/*  Find listening sock. */\n\tother = unix_find_other(net, sunaddr, addr_len, sk->sk_type, hash, &err);\n\tif (!other)\n\t\tgoto out;\n\n\t/* Latch state of peer */\n\tunix_state_lock(other);\n\n\t/* Apparently VFS overslept socket death. Retry. */\n\tif (sock_flag(other, SOCK_DEAD)) {\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = -ECONNREFUSED;\n\tif (other->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\tif (other->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_unlock;\n\n\tif (unix_recvq_full(other)) {\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_unlock;\n\n\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\t/* Latch our state.\n\n\t   It is tricky place. We need to grab write lock and cannot\n\t   drop lock on peer. It is dangerous because deadlock is\n\t   possible. Connect to self case and simultaneous\n\t   attempt to connect are eliminated by checking socket\n\t   state. other is TCP_LISTEN, if sk is TCP_LISTEN we\n\t   check this before attempt to grab lock.\n\n\t   Well, and we have to recheck the state after socket locked.\n\t */\n\tst = sk->sk_state;\n\n\tswitch (st) {\n\tcase TCP_CLOSE:\n\t\t/* This is ok... continue with connect */\n\t\tbreak;\n\tcase TCP_ESTABLISHED:\n\t\t/* Socket is already connected */\n\t\terr = -EISCONN;\n\t\tgoto out_unlock;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tunix_state_lock_nested(sk);\n\n\tif (sk->sk_state != st) {\n\t\tunix_state_unlock(sk);\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = security_unix_stream_connect(sock, other->sk_socket, newsk);\n\tif (err) {\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\t/* The way is open! Fastly set all the necessary fields... */\n\n\tsock_hold(sk);\n\tunix_peer(newsk)\t= sk;\n\tnewsk->sk_state\t\t= TCP_ESTABLISHED;\n\tnewsk->sk_type\t\t= sk->sk_type;\n\tnewsk->sk_peercred.pid\t= task_tgid_vnr(current);\n\tcurrent_euid_egid(&newsk->sk_peercred.uid, &newsk->sk_peercred.gid);\n\tnewu = unix_sk(newsk);\n\tnewsk->sk_sleep\t\t= &newu->peer_wait;\n\totheru = unix_sk(other);\n\n\t/* copy address information from listening to new sock*/\n\tif (otheru->addr) {\n\t\tatomic_inc(&otheru->addr->refcnt);\n\t\tnewu->addr = otheru->addr;\n\t}\n\tif (otheru->dentry) {\n\t\tnewu->dentry\t= dget(otheru->dentry);\n\t\tnewu->mnt\t= mntget(otheru->mnt);\n\t}\n\n\t/* Set credentials */\n\tsk->sk_peercred = other->sk_peercred;\n\n\tsock->state\t= SS_CONNECTED;\n\tsk->sk_state\t= TCP_ESTABLISHED;\n\tsock_hold(newsk);\n\n\tsmp_mb__after_atomic_inc();\t/* sock_hold() does an atomic_inc() */\n\tunix_peer(sk)\t= newsk;\n\n\tunix_state_unlock(sk);\n\n\t/* take ten and and send info to listening sock */\n\tspin_lock(&other->sk_receive_queue.lock);\n\t__skb_queue_tail(&other->sk_receive_queue, skb);\n\tspin_unlock(&other->sk_receive_queue.lock);\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other, 0);\n\tsock_put(other);\n\treturn 0;\n\nout_unlock:\n\tif (other)\n\t\tunix_state_unlock(other);\n\nout:\n\tkfree_skb(skb);\n\tif (newsk)\n\t\tunix_release_sock(newsk, 0);\n\tif (other)\n\t\tsock_put(other);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -59,6 +59,8 @@\n \n \terr = -ECONNREFUSED;\n \tif (other->sk_state != TCP_LISTEN)\n+\t\tgoto out_unlock;\n+\tif (other->sk_shutdown & RCV_SHUTDOWN)\n \t\tgoto out_unlock;\n \n \tif (unix_recvq_full(other)) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tgoto out_unlock;",
                "\tif (other->sk_shutdown & RCV_SHUTDOWN)"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-50020",
        "func_name": "open5gs/epoll_process",
        "description": "An issue was discovered in open5gs v2.6.6. SIGPIPE can be used to crash AMF.",
        "git_url": "https://github.com/open5gs/open5gs/commit/1aba814938e3a1b2eec7014bf6ce132d34622e08",
        "commit_title": "[SCTP] Fixed a crash on SIGPIPE (#2734)",
        "commit_text": "",
        "func_before": "static int epoll_process(ogs_pollset_t *pollset, ogs_time_t timeout)\n{\n    struct epoll_context_s *context = NULL;\n    int num_of_poll;\n    int i;\n\n    ogs_assert(pollset);\n    context = pollset->context;\n    ogs_assert(context);\n\n    num_of_poll = epoll_wait(context->epfd, context->event_list,\n            pollset->capacity,\n            timeout == OGS_INFINITE_TIME ? OGS_INFINITE_TIME :\n                ogs_time_to_msec(timeout));\n    if (num_of_poll < 0) {\n        ogs_log_message(OGS_LOG_ERROR, ogs_socket_errno, \"epoll failed\");\n        return OGS_ERROR;\n    } else if (num_of_poll == 0) {\n        return OGS_TIMEUP;\n    }\n\n    for (i = 0; i < num_of_poll; i++) {\n        struct epoll_map_s *map = NULL;\n        uint32_t received;\n        short when = 0;\n        ogs_socket_t fd;\n\n        received = context->event_list[i].events;\n        if (received & EPOLLERR) {\n        /*\n         * The libevent library has OGS_POLLOUT turned on in EPOLLERR.\n         *\n         * However, SIGPIPE can occur if write() is called\n         * when the peer connection is closed.\n         *\n         * Therefore, Open5GS turns off OGS_POLLOUT\n         * so that write() cannot be called in case of EPOLLERR.\n         *\n         * See also #2411 and #2312\n         */\n#if 0\n            when = OGS_POLLIN|OGS_POLLOUT;\n#else\n            when = OGS_POLLIN;\n#endif\n        } else if ((received & EPOLLHUP) && !(received & EPOLLRDHUP)) {\n            when = OGS_POLLIN|OGS_POLLOUT;\n        } else {\n            if (received & EPOLLIN) {\n                when |= OGS_POLLIN;\n            }\n            if (received & EPOLLOUT) {\n                when |= OGS_POLLOUT;\n            }\n            if (received & EPOLLRDHUP) {\n                when |= OGS_POLLIN;\n            }\n        }\n\n        if (!when)\n            continue;\n\n        fd = context->event_list[i].data.fd;\n        ogs_assert(fd != INVALID_SOCKET);\n\n        map = ogs_hash_get(context->map_hash, &fd, sizeof(fd));\n        if (!map) continue;\n\n        if (map->read && map->write && map->read == map->write) {\n            map->read->handler(when, map->read->fd, map->read->data);\n        } else {\n            if ((when & OGS_POLLIN) && map->read)\n                map->read->handler(when, map->read->fd, map->read->data);\n\n            /*\n             * map->read->handler() can call ogs_remove_epoll()\n             * So, we need to check map instance\n             */\n            map = ogs_hash_get(context->map_hash, &fd, sizeof(fd));\n            if (!map) continue;\n\n            if ((when & OGS_POLLOUT) && map->write)\n                map->write->handler(when, map->write->fd, map->write->data);\n        }\n    }\n    \n    return OGS_OK;\n}",
        "func": "static int epoll_process(ogs_pollset_t *pollset, ogs_time_t timeout)\n{\n    struct epoll_context_s *context = NULL;\n    int num_of_poll;\n    int i;\n\n    ogs_assert(pollset);\n    context = pollset->context;\n    ogs_assert(context);\n\n    num_of_poll = epoll_wait(context->epfd, context->event_list,\n            pollset->capacity,\n            timeout == OGS_INFINITE_TIME ? OGS_INFINITE_TIME :\n                ogs_time_to_msec(timeout));\n    if (num_of_poll < 0) {\n        ogs_log_message(OGS_LOG_ERROR, ogs_socket_errno, \"epoll failed\");\n        return OGS_ERROR;\n    } else if (num_of_poll == 0) {\n        return OGS_TIMEUP;\n    }\n\n    for (i = 0; i < num_of_poll; i++) {\n        struct epoll_map_s *map = NULL;\n        uint32_t received;\n        short when = 0;\n        ogs_socket_t fd;\n\n        received = context->event_list[i].events;\n        if (received & EPOLLERR) {\n        /*\n         * The libevent library has OGS_POLLOUT turned on in EPOLLERR.\n         *\n         * However, SIGPIPE can occur if write() is called\n         * when the peer connection is closed.\n         *\n         * Therefore, Open5GS turns off OGS_POLLOUT\n         * so that write() cannot be called in case of EPOLLERR.\n         *\n         * See also #2411 and #2312\n         */\n#if 0\n            when = OGS_POLLIN|OGS_POLLOUT;\n#else\n            when = OGS_POLLIN;\n#endif\n        } else if ((received & EPOLLHUP) && !(received & EPOLLRDHUP)) {\n            when = OGS_POLLIN|OGS_POLLOUT;\n        } else {\n            if (received & EPOLLIN) {\n                when |= OGS_POLLIN;\n            }\n            if (received & EPOLLOUT) {\n                when |= OGS_POLLOUT;\n            }\n            if (received & EPOLLRDHUP) {\n                when |= OGS_POLLIN;\n                when &= ~OGS_POLLOUT;\n            }\n        }\n\n        if (!when)\n            continue;\n\n        fd = context->event_list[i].data.fd;\n        ogs_assert(fd != INVALID_SOCKET);\n\n        map = ogs_hash_get(context->map_hash, &fd, sizeof(fd));\n        if (!map) continue;\n\n        if (map->read && map->write && map->read == map->write) {\n            map->read->handler(when, map->read->fd, map->read->data);\n        } else {\n            if ((when & OGS_POLLIN) && map->read)\n                map->read->handler(when, map->read->fd, map->read->data);\n\n            /*\n             * map->read->handler() can call ogs_remove_epoll()\n             * So, we need to check map instance\n             */\n            map = ogs_hash_get(context->map_hash, &fd, sizeof(fd));\n            if (!map) continue;\n\n            if ((when & OGS_POLLOUT) && map->write)\n                map->write->handler(when, map->write->fd, map->write->data);\n        }\n    }\n    \n    return OGS_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -54,6 +54,7 @@\n             }\n             if (received & EPOLLRDHUP) {\n                 when |= OGS_POLLIN;\n+                when &= ~OGS_POLLOUT;\n             }\n         }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                when &= ~OGS_POLLOUT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-24575",
        "func_name": "libgit2/revparse",
        "description": "libgit2 is a portable C implementation of the Git core methods provided as a linkable library with a solid API, allowing to build Git functionality into your application. Using well-crafted inputs to `git_revparse_single` can cause the function to enter an infinite loop, potentially causing a Denial of Service attack in the calling application. The revparse function in `src/libgit2/revparse.c` uses a loop to parse the user-provided spec string. There is an edge-case during parsing that allows a bad actor to force the loop conditions to access arbitrary memory. Potentially, this could also leak memory if the extracted rev spec is reflected back to the attacker. As such, libgit2 versions before 1.4.0 are not affected. Users should upgrade to version 1.6.5 or 1.7.2.",
        "git_url": "https://github.com/libgit2/libgit2/commit/add2dabb3c16aa49b33904dcdc07cd915efc12fa",
        "commit_title": "revparse: support bare '@'",
        "commit_text": " A bare '@' revision syntax represents HEAD.  Support it as such.",
        "func_before": "static int revparse(\n\tgit_object **object_out,\n\tgit_reference **reference_out,\n\tsize_t *identifier_len_out,\n\tgit_repository *repo,\n\tconst char *spec)\n{\n\tsize_t pos = 0, identifier_len = 0;\n\tint error = -1, n;\n\tgit_str buf = GIT_STR_INIT;\n\n\tgit_reference *reference = NULL;\n\tgit_object *base_rev = NULL;\n\n\tbool should_return_reference = true;\n\n\tGIT_ASSERT_ARG(object_out);\n\tGIT_ASSERT_ARG(reference_out);\n\tGIT_ASSERT_ARG(repo);\n\tGIT_ASSERT_ARG(spec);\n\n\t*object_out = NULL;\n\t*reference_out = NULL;\n\n\twhile (spec[pos]) {\n\t\tswitch (spec[pos]) {\n\t\tcase '^':\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif (spec[pos+1] == '{') {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_curly_braces_content(&buf, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_caret_curly_syntax(&temp_object, base_rev, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tgit_object_free(base_rev);\n\t\t\t\tbase_rev = temp_object;\n\t\t\t} else {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_how_many(&n, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_caret_parent_syntax(&temp_object, base_rev, n)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tgit_object_free(base_rev);\n\t\t\t\tbase_rev = temp_object;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase '~':\n\t\t{\n\t\t\tgit_object *temp_object = NULL;\n\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = extract_how_many(&n, spec, &pos)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif ((error = handle_linear_syntax(&temp_object, base_rev, n)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tgit_object_free(base_rev);\n\t\t\tbase_rev = temp_object;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase ':':\n\t\t{\n\t\t\tgit_object *temp_object = NULL;\n\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = extract_path(&buf, spec, &pos)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif (any_left_hand_identifier(base_rev, reference, identifier_len)) {\n\t\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, true)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_colon_syntax(&temp_object, base_rev, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\t\t\t} else {\n\t\t\t\tif (*git_str_cstr(&buf) == '/') {\n\t\t\t\t\tif ((error = handle_grep_syntax(&temp_object, repo, NULL, git_str_cstr(&buf) + 1)) < 0)\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t} else {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * TODO: support merge-stage path lookup (\":2:Makefile\")\n\t\t\t\t\t * and plain index blob lookup (:i-am/a/blob)\n\t\t\t\t\t */\n\t\t\t\t\tgit_error_set(GIT_ERROR_INVALID, \"unimplemented\");\n\t\t\t\t\terror = GIT_ERROR;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tgit_object_free(base_rev);\n\t\t\tbase_rev = temp_object;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase '@':\n\t\t\tif (spec[pos+1] == '{') {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_curly_braces_content(&buf, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = ensure_base_rev_is_not_known_yet(base_rev)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_at_syntax(&temp_object, &reference, spec, identifier_len, repo, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif (temp_object != NULL)\n\t\t\t\t\tbase_rev = temp_object;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fall through */\n\n\t\tdefault:\n\t\t\tif ((error = ensure_left_hand_identifier_is_not_known_yet(base_rev, reference)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tpos++;\n\t\t\tidentifier_len++;\n\t\t}\n\t}\n\n\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\tgoto cleanup;\n\n\tif (!should_return_reference) {\n\t\tgit_reference_free(reference);\n\t\treference = NULL;\n\t}\n\n\t*object_out = base_rev;\n\t*reference_out = reference;\n\t*identifier_len_out = identifier_len;\n\terror = 0;\n\ncleanup:\n\tif (error) {\n\t\tif (error == GIT_EINVALIDSPEC)\n\t\t\tgit_error_set(GIT_ERROR_INVALID,\n\t\t\t\t\"failed to parse revision specifier - Invalid pattern '%s'\", spec);\n\n\t\tgit_object_free(base_rev);\n\t\tgit_reference_free(reference);\n\t}\n\n\tgit_str_dispose(&buf);\n\treturn error;\n}",
        "func": "static int revparse(\n\tgit_object **object_out,\n\tgit_reference **reference_out,\n\tsize_t *identifier_len_out,\n\tgit_repository *repo,\n\tconst char *spec)\n{\n\tsize_t pos = 0, identifier_len = 0;\n\tint error = -1, n;\n\tgit_str buf = GIT_STR_INIT;\n\n\tgit_reference *reference = NULL;\n\tgit_object *base_rev = NULL;\n\n\tbool should_return_reference = true;\n\n\tGIT_ASSERT_ARG(object_out);\n\tGIT_ASSERT_ARG(reference_out);\n\tGIT_ASSERT_ARG(repo);\n\tGIT_ASSERT_ARG(spec);\n\n\t*object_out = NULL;\n\t*reference_out = NULL;\n\n\twhile (spec[pos]) {\n\t\tswitch (spec[pos]) {\n\t\tcase '^':\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif (spec[pos+1] == '{') {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_curly_braces_content(&buf, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_caret_curly_syntax(&temp_object, base_rev, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tgit_object_free(base_rev);\n\t\t\t\tbase_rev = temp_object;\n\t\t\t} else {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_how_many(&n, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_caret_parent_syntax(&temp_object, base_rev, n)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tgit_object_free(base_rev);\n\t\t\t\tbase_rev = temp_object;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase '~':\n\t\t{\n\t\t\tgit_object *temp_object = NULL;\n\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = extract_how_many(&n, spec, &pos)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif ((error = handle_linear_syntax(&temp_object, base_rev, n)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tgit_object_free(base_rev);\n\t\t\tbase_rev = temp_object;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase ':':\n\t\t{\n\t\t\tgit_object *temp_object = NULL;\n\n\t\t\tshould_return_reference = false;\n\n\t\t\tif ((error = extract_path(&buf, spec, &pos)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tif (any_left_hand_identifier(base_rev, reference, identifier_len)) {\n\t\t\t\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, true)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_colon_syntax(&temp_object, base_rev, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\t\t\t} else {\n\t\t\t\tif (*git_str_cstr(&buf) == '/') {\n\t\t\t\t\tif ((error = handle_grep_syntax(&temp_object, repo, NULL, git_str_cstr(&buf) + 1)) < 0)\n\t\t\t\t\t\tgoto cleanup;\n\t\t\t\t} else {\n\n\t\t\t\t\t/*\n\t\t\t\t\t * TODO: support merge-stage path lookup (\":2:Makefile\")\n\t\t\t\t\t * and plain index blob lookup (:i-am/a/blob)\n\t\t\t\t\t */\n\t\t\t\t\tgit_error_set(GIT_ERROR_INVALID, \"unimplemented\");\n\t\t\t\t\terror = GIT_ERROR;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tgit_object_free(base_rev);\n\t\t\tbase_rev = temp_object;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase '@':\n\t\t\tif (spec[pos+1] == '{') {\n\t\t\t\tgit_object *temp_object = NULL;\n\n\t\t\t\tif ((error = extract_curly_braces_content(&buf, spec, &pos)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = ensure_base_rev_is_not_known_yet(base_rev)) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif ((error = handle_at_syntax(&temp_object, &reference, spec, identifier_len, repo, git_str_cstr(&buf))) < 0)\n\t\t\t\t\tgoto cleanup;\n\n\t\t\t\tif (temp_object != NULL)\n\t\t\t\t\tbase_rev = temp_object;\n\t\t\t\tbreak;\n\t\t\t} else if (spec[pos+1] == '\\0') {\n\t\t\t\tspec = \"HEAD\";\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* fall through */\n\n\t\tdefault:\n\t\t\tif ((error = ensure_left_hand_identifier_is_not_known_yet(base_rev, reference)) < 0)\n\t\t\t\tgoto cleanup;\n\n\t\t\tpos++;\n\t\t\tidentifier_len++;\n\t\t}\n\t}\n\n\tif ((error = ensure_base_rev_loaded(&base_rev, &reference, spec, identifier_len, repo, false)) < 0)\n\t\tgoto cleanup;\n\n\tif (!should_return_reference) {\n\t\tgit_reference_free(reference);\n\t\treference = NULL;\n\t}\n\n\t*object_out = base_rev;\n\t*reference_out = reference;\n\t*identifier_len_out = identifier_len;\n\terror = 0;\n\ncleanup:\n\tif (error) {\n\t\tif (error == GIT_EINVALIDSPEC)\n\t\t\tgit_error_set(GIT_ERROR_INVALID,\n\t\t\t\t\"failed to parse revision specifier - Invalid pattern '%s'\", spec);\n\n\t\tgit_object_free(base_rev);\n\t\tgit_reference_free(reference);\n\t}\n\n\tgit_str_dispose(&buf);\n\treturn error;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -127,6 +127,9 @@\n \t\t\t\tif (temp_object != NULL)\n \t\t\t\t\tbase_rev = temp_object;\n \t\t\t\tbreak;\n+\t\t\t} else if (spec[pos+1] == '\\0') {\n+\t\t\t\tspec = \"HEAD\";\n+\t\t\t\tbreak;\n \t\t\t}\n \t\t\t/* fall through */\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\t} else if (spec[pos+1] == '\\0') {",
                "\t\t\t\tspec = \"HEAD\";",
                "\t\t\t\tbreak;"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-25452",
        "func_name": "axiomatic-systems/Bento4/AP4_DrefAtom::AP4_DrefAtom",
        "description": "Bento4 v1.6.0-640 was discovered to contain an out-of-memory bug via the AP4_UrlAtom::AP4_UrlAtom() function.",
        "git_url": "https://github.com/axiomatic-systems/Bento4/commit/a97991513bb54cb0982d69d3c0454e567c8c5124",
        "commit_title": "fix #873",
        "commit_text": "",
        "func_before": "AP4_DrefAtom::AP4_DrefAtom(AP4_UI32         size,\n                           AP4_UI08         version,\n                           AP4_UI32         flags,\n                           AP4_ByteStream&  stream,\n                           AP4_AtomFactory& atom_factory) :\n    AP4_ContainerAtom(AP4_ATOM_TYPE_DREF, size, false, version, flags)\n{\n    // read the number of entries\n    AP4_UI32 entry_count;\n    stream.ReadUI32(entry_count);\n\n    // read children\n    AP4_LargeSize bytes_available = size-AP4_FULL_ATOM_HEADER_SIZE-4;\n    while (entry_count--) {\n        AP4_Atom* atom; \n        while (AP4_SUCCEEDED(atom_factory.CreateAtomFromStream(stream, \n                                                               bytes_available,\n                                                               atom))) {\n            m_Children.Add(atom);\n        }\n    }\n}",
        "func": "AP4_DrefAtom::AP4_DrefAtom(AP4_UI32         size,\n                           AP4_UI08         version,\n                           AP4_UI32         flags,\n                           AP4_ByteStream&  stream,\n                           AP4_AtomFactory& atom_factory) :\n    AP4_ContainerAtom(AP4_ATOM_TYPE_DREF, size, false, version, flags)\n{\n    if (size <= AP4_FULL_ATOM_HEADER_SIZE + 4) {\n        return;\n    }\n    \n    // read the number of entries\n    AP4_UI32 entry_count;\n    AP4_Result result = stream.ReadUI32(entry_count);\n    if (AP4_FAILED(result)) return;\n\n    // read children\n    AP4_LargeSize bytes_available = size - (AP4_FULL_ATOM_HEADER_SIZE + 4);\n    while (entry_count--) {\n        AP4_Atom* atom; \n        while (AP4_SUCCEEDED(atom_factory.CreateAtomFromStream(stream, \n                                                               bytes_available,\n                                                               atom))) {\n            m_Children.Add(atom);\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,12 +5,17 @@\n                            AP4_AtomFactory& atom_factory) :\n     AP4_ContainerAtom(AP4_ATOM_TYPE_DREF, size, false, version, flags)\n {\n+    if (size <= AP4_FULL_ATOM_HEADER_SIZE + 4) {\n+        return;\n+    }\n+    \n     // read the number of entries\n     AP4_UI32 entry_count;\n-    stream.ReadUI32(entry_count);\n+    AP4_Result result = stream.ReadUI32(entry_count);\n+    if (AP4_FAILED(result)) return;\n \n     // read children\n-    AP4_LargeSize bytes_available = size-AP4_FULL_ATOM_HEADER_SIZE-4;\n+    AP4_LargeSize bytes_available = size - (AP4_FULL_ATOM_HEADER_SIZE + 4);\n     while (entry_count--) {\n         AP4_Atom* atom; \n         while (AP4_SUCCEEDED(atom_factory.CreateAtomFromStream(stream, ",
        "diff_line_info": {
            "deleted_lines": [
                "    stream.ReadUI32(entry_count);",
                "    AP4_LargeSize bytes_available = size-AP4_FULL_ATOM_HEADER_SIZE-4;"
            ],
            "added_lines": [
                "    if (size <= AP4_FULL_ATOM_HEADER_SIZE + 4) {",
                "        return;",
                "    }",
                "    ",
                "    AP4_Result result = stream.ReadUI32(entry_count);",
                "    if (AP4_FAILED(result)) return;",
                "    AP4_LargeSize bytes_available = size - (AP4_FULL_ATOM_HEADER_SIZE + 4);"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-25452",
        "func_name": "axiomatic-systems/Bento4/AP4_DrefAtom::WriteFields",
        "description": "Bento4 v1.6.0-640 was discovered to contain an out-of-memory bug via the AP4_UrlAtom::AP4_UrlAtom() function.",
        "git_url": "https://github.com/axiomatic-systems/Bento4/commit/a97991513bb54cb0982d69d3c0454e567c8c5124",
        "commit_title": "fix #873",
        "commit_text": "",
        "func_before": "AP4_Result\nAP4_DrefAtom::WriteFields(AP4_ByteStream& stream)\n{\n    AP4_Result result;\n\n    // write the number of entries\n    result = stream.WriteUI32(m_Children.ItemCount());\n    if (AP4_FAILED(result)) return result;\n\n    // write the children\n    return m_Children.Apply(AP4_AtomListWriter(stream));\n}",
        "func": "AP4_Result\nAP4_DrefAtom::WriteFields(AP4_ByteStream& stream)\n{\n    // write the number of entries\n    AP4_Result result = stream.WriteUI32(m_Children.ItemCount());\n    if (AP4_FAILED(result)) return result;\n\n    // write the children\n    return m_Children.Apply(AP4_AtomListWriter(stream));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,8 @@\n AP4_Result\n AP4_DrefAtom::WriteFields(AP4_ByteStream& stream)\n {\n-    AP4_Result result;\n-\n     // write the number of entries\n-    result = stream.WriteUI32(m_Children.ItemCount());\n+    AP4_Result result = stream.WriteUI32(m_Children.ItemCount());\n     if (AP4_FAILED(result)) return result;\n \n     // write the children",
        "diff_line_info": {
            "deleted_lines": [
                "    AP4_Result result;",
                "",
                "    result = stream.WriteUI32(m_Children.ItemCount());"
            ],
            "added_lines": [
                "    AP4_Result result = stream.WriteUI32(m_Children.ItemCount());"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0810",
        "func_name": "kernel/git/rt/linux-stable-rt/do_notify_resume",
        "description": "The int3 handler in the Linux kernel before 3.3 relies on a per-CPU debug stack, which allows local users to cause a denial of service (stack corruption and panic) via a crafted application that triggers certain lock contention.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/commit/?h=bcf6b1d78c0bde228929c388978ed3af9a623463",
        "commit_title": "On x86_64 we must disable preemption before we enable interrupts",
        "commit_text": "for stack faults, int3 and debugging, because the current task is using a per CPU debug stack defined by the IST. If we schedule out, another task can come in and use the same stack and cause the stack to be corrupted and crash the kernel on return.  When CONFIG_PREEMPT_RT_FULL is enabled, spin_locks become mutexes, and one of these is the spin lock used in signal handling.  Some of the debug code (int3) causes do_trap() to send a signal. This function calls a spin lock that has been converted to a mutex and has the possibility to sleep. If this happens, the above issues with the corrupted stack is possible.  Instead of calling the signal right away, for PREEMPT_RT and x86_64, the signal information is stored on the stacks task_struct and TIF_NOTIFY_RESUME is set. Then on exit of the trap, the signal resume code will send the signal when preemption is enabled.  [ rostedt: Switched from #ifdef CONFIG_PREEMPT_RT_FULL to   ARCH_RT_DELAYS_SIGNAL_SEND and added comments to the code. ]  Cc: stable-rt@vger.kernel.org ",
        "func_before": "void\ndo_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)\n{\n#ifdef CONFIG_X86_MCE\n\t/* notify userspace of pending MCEs */\n\tif (thread_info_flags & _TIF_MCE_NOTIFY)\n\t\tmce_notify_process();\n#endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n\n\t/* deal with pending signal delivery */\n\tif (thread_info_flags & _TIF_SIGPENDING)\n\t\tdo_signal(regs);\n\n\tif (thread_info_flags & _TIF_NOTIFY_RESUME) {\n\t\tclear_thread_flag(TIF_NOTIFY_RESUME);\n\t\ttracehook_notify_resume(regs);\n\t\tif (current->replacement_session_keyring)\n\t\t\tkey_replace_session_keyring();\n\t}\n\tif (thread_info_flags & _TIF_USER_RETURN_NOTIFY)\n\t\tfire_user_return_notifiers();\n\n#ifdef CONFIG_X86_32\n\tclear_thread_flag(TIF_IRET);\n#endif /* CONFIG_X86_32 */\n}",
        "func": "void\ndo_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)\n{\n#ifdef CONFIG_X86_MCE\n\t/* notify userspace of pending MCEs */\n\tif (thread_info_flags & _TIF_MCE_NOTIFY)\n\t\tmce_notify_process();\n#endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n\n#ifdef ARCH_RT_DELAYS_SIGNAL_SEND\n\tif (unlikely(current->forced_info.si_signo)) {\n\t\tstruct task_struct *t = current;\n\t\tforce_sig_info(t->forced_info.si_signo,\n\t\t\t\t\t&t->forced_info, t);\n\t\tt->forced_info.si_signo = 0;\n\t}\n#endif\n\n\t/* deal with pending signal delivery */\n\tif (thread_info_flags & _TIF_SIGPENDING)\n\t\tdo_signal(regs);\n\n\tif (thread_info_flags & _TIF_NOTIFY_RESUME) {\n\t\tclear_thread_flag(TIF_NOTIFY_RESUME);\n\t\ttracehook_notify_resume(regs);\n\t\tif (current->replacement_session_keyring)\n\t\t\tkey_replace_session_keyring();\n\t}\n\tif (thread_info_flags & _TIF_USER_RETURN_NOTIFY)\n\t\tfire_user_return_notifiers();\n\n#ifdef CONFIG_X86_32\n\tclear_thread_flag(TIF_IRET);\n#endif /* CONFIG_X86_32 */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,15 @@\n \tif (thread_info_flags & _TIF_MCE_NOTIFY)\n \t\tmce_notify_process();\n #endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n+\n+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND\n+\tif (unlikely(current->forced_info.si_signo)) {\n+\t\tstruct task_struct *t = current;\n+\t\tforce_sig_info(t->forced_info.si_signo,\n+\t\t\t\t\t&t->forced_info, t);\n+\t\tt->forced_info.si_signo = 0;\n+\t}\n+#endif\n \n \t/* deal with pending signal delivery */\n \tif (thread_info_flags & _TIF_SIGPENDING)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "#ifdef ARCH_RT_DELAYS_SIGNAL_SEND",
                "\tif (unlikely(current->forced_info.si_signo)) {",
                "\t\tstruct task_struct *t = current;",
                "\t\tforce_sig_info(t->forced_info.si_signo,",
                "\t\t\t\t\t&t->forced_info, t);",
                "\t\tt->forced_info.si_signo = 0;",
                "\t}",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0810",
        "func_name": "kernel/git/rt/linux-stable-rt/do_notify_resume",
        "description": "The int3 handler in the Linux kernel before 3.3 relies on a per-CPU debug stack, which allows local users to cause a denial of service (stack corruption and panic) via a crafted application that triggers certain lock contention.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/commit/?h=bcf6b1d78c0bde228929c388978ed3af9a623463",
        "commit_title": "On x86_64 we must disable preemption before we enable interrupts",
        "commit_text": "for stack faults, int3 and debugging, because the current task is using a per CPU debug stack defined by the IST. If we schedule out, another task can come in and use the same stack and cause the stack to be corrupted and crash the kernel on return.  When CONFIG_PREEMPT_RT_FULL is enabled, spin_locks become mutexes, and one of these is the spin lock used in signal handling.  Some of the debug code (int3) causes do_trap() to send a signal. This function calls a spin lock that has been converted to a mutex and has the possibility to sleep. If this happens, the above issues with the corrupted stack is possible.  Instead of calling the signal right away, for PREEMPT_RT and x86_64, the signal information is stored on the stacks task_struct and TIF_NOTIFY_RESUME is set. Then on exit of the trap, the signal resume code will send the signal when preemption is enabled.  [ rostedt: Switched from #ifdef CONFIG_PREEMPT_RT_FULL to   ARCH_RT_DELAYS_SIGNAL_SEND and added comments to the code. ]  Cc: stable-rt@vger.kernel.org ",
        "func_before": "void\ndo_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)\n{\n#ifdef CONFIG_X86_MCE\n\t/* notify userspace of pending MCEs */\n\tif (thread_info_flags & _TIF_MCE_NOTIFY)\n\t\tmce_notify_process();\n#endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n\n\t/* deal with pending signal delivery */\n\tif (thread_info_flags & _TIF_SIGPENDING)\n\t\tdo_signal(regs);\n\n\tif (thread_info_flags & _TIF_NOTIFY_RESUME) {\n\t\tclear_thread_flag(TIF_NOTIFY_RESUME);\n\t\ttracehook_notify_resume(regs);\n\t\tif (current->replacement_session_keyring)\n\t\t\tkey_replace_session_keyring();\n\t}\n\tif (thread_info_flags & _TIF_USER_RETURN_NOTIFY)\n\t\tfire_user_return_notifiers();\n\n#ifdef CONFIG_X86_32\n\tclear_thread_flag(TIF_IRET);\n#endif /* CONFIG_X86_32 */\n}",
        "func": "void\ndo_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)\n{\n#ifdef CONFIG_X86_MCE\n\t/* notify userspace of pending MCEs */\n\tif (thread_info_flags & _TIF_MCE_NOTIFY)\n\t\tmce_notify_process();\n#endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n\n#ifdef ARCH_RT_DELAYS_SIGNAL_SEND\n\tif (unlikely(current->forced_info.si_signo)) {\n\t\tstruct task_struct *t = current;\n\t\tforce_sig_info(t->forced_info.si_signo,\n\t\t\t\t\t&t->forced_info, t);\n\t\tt->forced_info.si_signo = 0;\n\t}\n#endif\n\n\t/* deal with pending signal delivery */\n\tif (thread_info_flags & _TIF_SIGPENDING)\n\t\tdo_signal(regs);\n\n\tif (thread_info_flags & _TIF_NOTIFY_RESUME) {\n\t\tclear_thread_flag(TIF_NOTIFY_RESUME);\n\t\ttracehook_notify_resume(regs);\n\t\tif (current->replacement_session_keyring)\n\t\t\tkey_replace_session_keyring();\n\t}\n\tif (thread_info_flags & _TIF_USER_RETURN_NOTIFY)\n\t\tfire_user_return_notifiers();\n\n#ifdef CONFIG_X86_32\n\tclear_thread_flag(TIF_IRET);\n#endif /* CONFIG_X86_32 */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,15 @@\n \tif (thread_info_flags & _TIF_MCE_NOTIFY)\n \t\tmce_notify_process();\n #endif /* CONFIG_X86_64 && CONFIG_X86_MCE */\n+\n+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND\n+\tif (unlikely(current->forced_info.si_signo)) {\n+\t\tstruct task_struct *t = current;\n+\t\tforce_sig_info(t->forced_info.si_signo,\n+\t\t\t\t\t&t->forced_info, t);\n+\t\tt->forced_info.si_signo = 0;\n+\t}\n+#endif\n \n \t/* deal with pending signal delivery */\n \tif (thread_info_flags & _TIF_SIGPENDING)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "#ifdef ARCH_RT_DELAYS_SIGNAL_SEND",
                "\tif (unlikely(current->forced_info.si_signo)) {",
                "\t\tstruct task_struct *t = current;",
                "\t\tforce_sig_info(t->forced_info.si_signo,",
                "\t\t\t\t\t&t->forced_info, t);",
                "\t\tt->forced_info.si_signo = 0;",
                "\t}",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0810",
        "func_name": "kernel/git/rt/linux-stable-rt/do_int3",
        "description": "The int3 handler in the Linux kernel before 3.3 relies on a per-CPU debug stack, which allows local users to cause a denial of service (stack corruption and panic) via a crafted application that triggers certain lock contention.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/commit/?h=e5d4e1c3ccee18c68f23d62ba77bda26e893d4f0",
        "commit_title": "Preemption must be disabled before enabling interrupts in do_trap",
        "commit_text": "on x86_64 because the stack in use for int3 and debug is a per CPU stack set by th IST. But 32bit does not have an IST and the stack still belongs to the current task and there is no problem in scheduling out the task.  Keep preemption enabled on X86_32 when enabling interrupts for do_trap().  The name of the function is changed from preempt_conditional_sti/cli() to conditional_sti/cli_ist(), to annotate that this function is used when the stack is on the IST.  Cc: stable-rt@vger.kernel.org  ",
        "func_before": "__kprobes do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n#ifdef CONFIG_KPROBES\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#else\n\tif (notify_die(DIE_TRAP, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#endif\n\n\tpreempt_conditional_sti(regs);\n\tdo_trap(3, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tpreempt_conditional_cli(regs);\n}",
        "func": "__kprobes do_int3(struct pt_regs *regs, long error_code)\n{\n#ifdef CONFIG_KGDB_LOW_LEVEL_TRAP\n\tif (kgdb_ll_trap(DIE_INT3, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#endif /* CONFIG_KGDB_LOW_LEVEL_TRAP */\n#ifdef CONFIG_KPROBES\n\tif (notify_die(DIE_INT3, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#else\n\tif (notify_die(DIE_TRAP, \"int3\", regs, error_code, 3, SIGTRAP)\n\t\t\t== NOTIFY_STOP)\n\t\treturn;\n#endif\n\n\tconditional_sti_ist(regs);\n\tdo_trap(3, SIGTRAP, \"int3\", regs, error_code, NULL);\n\tconditional_cli_ist(regs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \t\treturn;\n #endif\n \n-\tpreempt_conditional_sti(regs);\n+\tconditional_sti_ist(regs);\n \tdo_trap(3, SIGTRAP, \"int3\", regs, error_code, NULL);\n-\tpreempt_conditional_cli(regs);\n+\tconditional_cli_ist(regs);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tpreempt_conditional_sti(regs);",
                "\tpreempt_conditional_cli(regs);"
            ],
            "added_lines": [
                "\tconditional_sti_ist(regs);",
                "\tconditional_cli_ist(regs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0810",
        "func_name": "kernel/git/rt/linux-stable-rt/do_debug",
        "description": "The int3 handler in the Linux kernel before 3.3 relies on a per-CPU debug stack, which allows local users to cause a denial of service (stack corruption and panic) via a crafted application that triggers certain lock contention.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/commit/?h=e5d4e1c3ccee18c68f23d62ba77bda26e893d4f0",
        "commit_title": "Preemption must be disabled before enabling interrupts in do_trap",
        "commit_text": "on x86_64 because the stack in use for int3 and debug is a per CPU stack set by th IST. But 32bit does not have an IST and the stack still belongs to the current task and there is no problem in scheduling out the task.  Keep preemption enabled on X86_32 when enabling interrupts for do_trap().  The name of the function is changed from preempt_conditional_sti/cli() to conditional_sti/cli_ist(), to annotate that this function is used when the stack is on the IST.  Cc: stable-rt@vger.kernel.org  ",
        "func_before": "__kprobes do_debug(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk = current;\n\tint user_icebp = 0;\n\tunsigned long dr6;\n\tint si_code;\n\n\tget_debugreg(dr6, 6);\n\n\t/* Filter out all the reserved bits which are preset to 1 */\n\tdr6 &= ~DR6_RESERVED;\n\n\t/*\n\t * If dr6 has no reason to give us about the origin of this trap,\n\t * then it's very likely the result of an icebp/int01 trap.\n\t * User wants a sigtrap for that.\n\t */\n\tif (!dr6 && user_mode(regs))\n\t\tuser_icebp = 1;\n\n\t/* Catch kmemcheck conditions first of all! */\n\tif ((dr6 & DR_STEP) && kmemcheck_trap(regs))\n\t\treturn;\n\n\t/* DR6 may or may not be cleared by the CPU */\n\tset_debugreg(0, 6);\n\n\t/*\n\t * The processor cleared BTF, so don't mark that we need it set.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);\n\n\t/* Store the virtualized DR6 value */\n\ttsk->thread.debugreg6 = dr6;\n\n\tif (notify_die(DIE_DEBUG, \"debug\", regs, PTR_ERR(&dr6), error_code,\n\t\t\t\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n\n\t/* It's safe to allow irq's after DR6 has been saved */\n\tpreempt_conditional_sti(regs);\n\n\tif (regs->flags & X86_VM_MASK) {\n\t\thandle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\terror_code, 1);\n\t\tpreempt_conditional_cli(regs);\n\t\treturn;\n\t}\n\n\t/*\n\t * Single-stepping through system calls: ignore any exceptions in\n\t * kernel space, but re-enable TF when returning to user mode.\n\t *\n\t * We already checked v86 mode above, so we can check for kernel mode\n\t * by just checking the CPL of CS.\n\t */\n\tif ((dr6 & DR_STEP) && !user_mode(regs)) {\n\t\ttsk->thread.debugreg6 &= ~DR_STEP;\n\t\tset_tsk_thread_flag(tsk, TIF_SINGLESTEP);\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n\t}\n\tsi_code = get_si_code(tsk->thread.debugreg6);\n\tif (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)\n\t\tsend_sigtrap(tsk, regs, error_code, si_code);\n\tpreempt_conditional_cli(regs);\n\n\treturn;\n}",
        "func": "__kprobes do_debug(struct pt_regs *regs, long error_code)\n{\n\tstruct task_struct *tsk = current;\n\tint user_icebp = 0;\n\tunsigned long dr6;\n\tint si_code;\n\n\tget_debugreg(dr6, 6);\n\n\t/* Filter out all the reserved bits which are preset to 1 */\n\tdr6 &= ~DR6_RESERVED;\n\n\t/*\n\t * If dr6 has no reason to give us about the origin of this trap,\n\t * then it's very likely the result of an icebp/int01 trap.\n\t * User wants a sigtrap for that.\n\t */\n\tif (!dr6 && user_mode(regs))\n\t\tuser_icebp = 1;\n\n\t/* Catch kmemcheck conditions first of all! */\n\tif ((dr6 & DR_STEP) && kmemcheck_trap(regs))\n\t\treturn;\n\n\t/* DR6 may or may not be cleared by the CPU */\n\tset_debugreg(0, 6);\n\n\t/*\n\t * The processor cleared BTF, so don't mark that we need it set.\n\t */\n\tclear_tsk_thread_flag(tsk, TIF_BLOCKSTEP);\n\n\t/* Store the virtualized DR6 value */\n\ttsk->thread.debugreg6 = dr6;\n\n\tif (notify_die(DIE_DEBUG, \"debug\", regs, PTR_ERR(&dr6), error_code,\n\t\t\t\t\t\t\tSIGTRAP) == NOTIFY_STOP)\n\t\treturn;\n\n\t/* It's safe to allow irq's after DR6 has been saved */\n\tconditional_sti_ist(regs);\n\n\tif (regs->flags & X86_VM_MASK) {\n\t\thandle_vm86_trap((struct kernel_vm86_regs *) regs,\n\t\t\t\terror_code, 1);\n\t\tconditional_cli_ist(regs);\n\t\treturn;\n\t}\n\n\t/*\n\t * Single-stepping through system calls: ignore any exceptions in\n\t * kernel space, but re-enable TF when returning to user mode.\n\t *\n\t * We already checked v86 mode above, so we can check for kernel mode\n\t * by just checking the CPL of CS.\n\t */\n\tif ((dr6 & DR_STEP) && !user_mode(regs)) {\n\t\ttsk->thread.debugreg6 &= ~DR_STEP;\n\t\tset_tsk_thread_flag(tsk, TIF_SINGLESTEP);\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n\t}\n\tsi_code = get_si_code(tsk->thread.debugreg6);\n\tif (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)\n\t\tsend_sigtrap(tsk, regs, error_code, si_code);\n\tconditional_cli_ist(regs);\n\n\treturn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,12 +38,12 @@\n \t\treturn;\n \n \t/* It's safe to allow irq's after DR6 has been saved */\n-\tpreempt_conditional_sti(regs);\n+\tconditional_sti_ist(regs);\n \n \tif (regs->flags & X86_VM_MASK) {\n \t\thandle_vm86_trap((struct kernel_vm86_regs *) regs,\n \t\t\t\terror_code, 1);\n-\t\tpreempt_conditional_cli(regs);\n+\t\tconditional_cli_ist(regs);\n \t\treturn;\n \t}\n \n@@ -62,7 +62,7 @@\n \tsi_code = get_si_code(tsk->thread.debugreg6);\n \tif (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)\n \t\tsend_sigtrap(tsk, regs, error_code, si_code);\n-\tpreempt_conditional_cli(regs);\n+\tconditional_cli_ist(regs);\n \n \treturn;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tpreempt_conditional_sti(regs);",
                "\t\tpreempt_conditional_cli(regs);",
                "\tpreempt_conditional_cli(regs);"
            ],
            "added_lines": [
                "\tconditional_sti_ist(regs);",
                "\t\tconditional_cli_ist(regs);",
                "\tconditional_cli_ist(regs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0810",
        "func_name": "kernel/git/rt/linux-stable-rt/do_stack_segment",
        "description": "The int3 handler in the Linux kernel before 3.3 relies on a per-CPU debug stack, which allows local users to cause a denial of service (stack corruption and panic) via a crafted application that triggers certain lock contention.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git/commit/?h=e5d4e1c3ccee18c68f23d62ba77bda26e893d4f0",
        "commit_title": "Preemption must be disabled before enabling interrupts in do_trap",
        "commit_text": "on x86_64 because the stack in use for int3 and debug is a per CPU stack set by th IST. But 32bit does not have an IST and the stack still belongs to the current task and there is no problem in scheduling out the task.  Keep preemption enabled on X86_32 when enabling interrupts for do_trap().  The name of the function is changed from preempt_conditional_sti/cli() to conditional_sti/cli_ist(), to annotate that this function is used when the stack is on the IST.  Cc: stable-rt@vger.kernel.org  ",
        "func_before": "dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)\n{\n\tif (notify_die(DIE_TRAP, \"stack segment\", regs, error_code,\n\t\t\t12, SIGBUS) == NOTIFY_STOP)\n\t\treturn;\n\tpreempt_conditional_sti(regs);\n\tdo_trap(12, SIGBUS, \"stack segment\", regs, error_code, NULL);\n\tpreempt_conditional_cli(regs);\n}",
        "func": "dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)\n{\n\tif (notify_die(DIE_TRAP, \"stack segment\", regs, error_code,\n\t\t\t12, SIGBUS) == NOTIFY_STOP)\n\t\treturn;\n\tconditional_sti_ist(regs);\n\tdo_trap(12, SIGBUS, \"stack segment\", regs, error_code, NULL);\n\tconditional_cli_ist(regs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n \tif (notify_die(DIE_TRAP, \"stack segment\", regs, error_code,\n \t\t\t12, SIGBUS) == NOTIFY_STOP)\n \t\treturn;\n-\tpreempt_conditional_sti(regs);\n+\tconditional_sti_ist(regs);\n \tdo_trap(12, SIGBUS, \"stack segment\", regs, error_code, NULL);\n-\tpreempt_conditional_cli(regs);\n+\tconditional_cli_ist(regs);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tpreempt_conditional_sti(regs);",
                "\tpreempt_conditional_cli(regs);"
            ],
            "added_lines": [
                "\tconditional_sti_ist(regs);",
                "\tconditional_cli_ist(regs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-45873",
        "func_name": "systemd/parse_elf_object",
        "description": "systemd 250 and 251 allows local users to achieve a systemd-coredump deadlock by triggering a crash that has a long backtrace. This occurs in parse_elf_object in shared/elf-util.c. The exploitation methodology is to crash a binary calling the same function recursively, and put it in a deeply nested directory to make its backtrace large enough to cause the deadlock. This must be done 16 times when MaxConnections=16 is set for the systemd/units/systemd-coredump.socket file.",
        "git_url": "https://github.com/systemd/systemd/commit/076b807be472630692c5348c60d0c2b7b28ad437",
        "commit_title": "coredump: avoid deadlock when passing processed backtrace data",
        "commit_text": " We would deadlock when passing the data back from the forked-off process that was doing backtrace generation back to the coredump parent. This is because we fork the child and wait for it to exit. The child tries to write too much data to the output pipe, and and after the first 64k blocks on the parent because the pipe is full. The bug surfaced in Fedora because of a combination of four factors: - 87707784c70dc9894ec613df0a6e75e732a362a3 was backported to v251.5, which   allowed coredump processing to be successful. - 1a0281a3ebf4f8c16d40aa9e63103f16cd23bb2a was NOT backported, so the output   was very verbose. - Fedora has the ELF package metadata available, so a lot of output can be   generated. Most other distros just don't have the information. - gnome-calendar crashes and has a bazillion modules and 69596 bytes of output   are generated for it.  Fixes https://bugzilla.redhat.com/show_bug.cgi?id=2135778.  The code is changed to try to write data opportunistically. If we get partial information, that is still logged. In is generally better to log partial backtrace information than nothing at all.",
        "func_before": "int parse_elf_object(int fd, const char *executable, bool fork_disable_dump, char **ret, JsonVariant **ret_package_metadata) {\n        _cleanup_close_pair_ int error_pipe[2] = { -1, -1 }, return_pipe[2] = { -1, -1 }, json_pipe[2] = { -1, -1 };\n        _cleanup_(json_variant_unrefp) JsonVariant *package_metadata = NULL;\n        _cleanup_free_ char *buf = NULL;\n        int r;\n\n        assert(fd >= 0);\n\n        r = dlopen_dw();\n        if (r < 0)\n                return r;\n\n        r = dlopen_elf();\n        if (r < 0)\n                return r;\n\n        r = RET_NERRNO(pipe2(error_pipe, O_CLOEXEC|O_NONBLOCK));\n        if (r < 0)\n                return r;\n\n        if (ret) {\n                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC));\n                if (r < 0)\n                        return r;\n        }\n\n        if (ret_package_metadata) {\n                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC));\n                if (r < 0)\n                        return r;\n        }\n\n        /* Parsing possibly malformed data is crash-happy, so fork. In case we crash,\n         * the core file will not be lost, and the messages will still be attached to\n         * the journal. Reading the elf object might be slow, but it still has an upper\n         * bound since the core files have an upper size limit. It's also not doing any\n         * system call or interacting with the system in any way, besides reading from\n         * the file descriptor and writing into these four pipes. */\n        r = safe_fork_full(\"(sd-parse-elf)\",\n                           (int[]){ fd, error_pipe[1], return_pipe[1], json_pipe[1] },\n                           4,\n                           FORK_RESET_SIGNALS|FORK_CLOSE_ALL_FDS|FORK_NEW_MOUNTNS|FORK_MOUNTNS_SLAVE|FORK_NEW_USERNS|FORK_WAIT|FORK_REOPEN_LOG,\n                           NULL);\n        if (r < 0) {\n                if (r == -EPROTO) { /* We should have the errno from the child, but don't clobber original error */\n                        int e, k;\n\n                        k = read(error_pipe[0], &e, sizeof(e));\n                        if (k < 0 && errno != EAGAIN) /* Pipe is non-blocking, EAGAIN means there's nothing */\n                                return -errno;\n                        if (k == sizeof(e))\n                                return e; /* propagate error sent to us from child */\n                        if (k != 0)\n                                return -EIO;\n                }\n\n                return r;\n        }\n        if (r == 0) {\n                /* We want to avoid loops, given this can be called from systemd-coredump */\n                if (fork_disable_dump) {\n                        r = RET_NERRNO(prctl(PR_SET_DUMPABLE, 0));\n                        if (r < 0)\n                                goto child_fail;\n                }\n\n                r = parse_elf(fd, executable, ret ? &buf : NULL, ret_package_metadata ? &package_metadata : NULL);\n                if (r < 0)\n                        goto child_fail;\n\n                if (buf) {\n                        r = loop_write(return_pipe[1], buf, strlen(buf), false);\n                        if (r < 0)\n                                goto child_fail;\n\n                        return_pipe[1] = safe_close(return_pipe[1]);\n                }\n\n                if (package_metadata) {\n                        _cleanup_fclose_ FILE *json_out = NULL;\n\n                        json_out = take_fdopen(&json_pipe[1], \"w\");\n                        if (!json_out) {\n                                r = -errno;\n                                goto child_fail;\n                        }\n\n                        json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);\n                }\n\n                _exit(EXIT_SUCCESS);\n\n        child_fail:\n                (void) write(error_pipe[1], &r, sizeof(r));\n                _exit(EXIT_FAILURE);\n        }\n\n        error_pipe[1] = safe_close(error_pipe[1]);\n        return_pipe[1] = safe_close(return_pipe[1]);\n        json_pipe[1] = safe_close(json_pipe[1]);\n\n        if (ret) {\n                _cleanup_fclose_ FILE *in = NULL;\n\n                in = take_fdopen(&return_pipe[0], \"r\");\n                if (!in)\n                        return -errno;\n\n                r = read_full_stream(in, &buf, NULL);\n                if (r < 0)\n                        return r;\n        }\n\n        if (ret_package_metadata) {\n                _cleanup_fclose_ FILE *json_in = NULL;\n\n                json_in = take_fdopen(&json_pipe[0], \"r\");\n                if (!json_in)\n                        return -errno;\n\n                r = json_parse_file(json_in, NULL, 0, &package_metadata, NULL, NULL);\n                if (r < 0 && r != -ENODATA) /* ENODATA: json was empty, so we got nothing, but that's ok */\n                        return r;\n        }\n\n        if (ret)\n                *ret = TAKE_PTR(buf);\n        if (ret_package_metadata)\n                *ret_package_metadata = TAKE_PTR(package_metadata);\n\n        return 0;\n}",
        "func": "int parse_elf_object(int fd, const char *executable, bool fork_disable_dump, char **ret, JsonVariant **ret_package_metadata) {\n        _cleanup_close_pair_ int error_pipe[2] = { -1, -1 }, return_pipe[2] = { -1, -1 }, json_pipe[2] = { -1, -1 };\n        _cleanup_(json_variant_unrefp) JsonVariant *package_metadata = NULL;\n        _cleanup_free_ char *buf = NULL;\n        int r;\n\n        assert(fd >= 0);\n\n        r = dlopen_dw();\n        if (r < 0)\n                return r;\n\n        r = dlopen_elf();\n        if (r < 0)\n                return r;\n\n        r = RET_NERRNO(pipe2(error_pipe, O_CLOEXEC|O_NONBLOCK));\n        if (r < 0)\n                return r;\n\n        if (ret) {\n                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC|O_NONBLOCK));\n                if (r < 0)\n                        return r;\n        }\n\n        if (ret_package_metadata) {\n                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC|O_NONBLOCK));\n                if (r < 0)\n                        return r;\n        }\n\n        /* Parsing possibly malformed data is crash-happy, so fork. In case we crash,\n         * the core file will not be lost, and the messages will still be attached to\n         * the journal. Reading the elf object might be slow, but it still has an upper\n         * bound since the core files have an upper size limit. It's also not doing any\n         * system call or interacting with the system in any way, besides reading from\n         * the file descriptor and writing into these four pipes. */\n        r = safe_fork_full(\"(sd-parse-elf)\",\n                           (int[]){ fd, error_pipe[1], return_pipe[1], json_pipe[1] },\n                           4,\n                           FORK_RESET_SIGNALS|FORK_CLOSE_ALL_FDS|FORK_NEW_MOUNTNS|FORK_MOUNTNS_SLAVE|FORK_NEW_USERNS|FORK_WAIT|FORK_REOPEN_LOG,\n                           NULL);\n        if (r < 0) {\n                if (r == -EPROTO) { /* We should have the errno from the child, but don't clobber original error */\n                        int e, k;\n\n                        k = read(error_pipe[0], &e, sizeof(e));\n                        if (k < 0 && errno != EAGAIN) /* Pipe is non-blocking, EAGAIN means there's nothing */\n                                return -errno;\n                        if (k == sizeof(e))\n                                return e; /* propagate error sent to us from child */\n                        if (k != 0)\n                                return -EIO;\n                }\n\n                return r;\n        }\n        if (r == 0) {\n                /* We want to avoid loops, given this can be called from systemd-coredump */\n                if (fork_disable_dump) {\n                        r = RET_NERRNO(prctl(PR_SET_DUMPABLE, 0));\n                        if (r < 0)\n                                goto child_fail;\n                }\n\n                r = parse_elf(fd, executable, ret ? &buf : NULL, ret_package_metadata ? &package_metadata : NULL);\n                if (r < 0)\n                        goto child_fail;\n\n                if (buf) {\n                        size_t len = strlen(buf);\n\n                        if (len > COREDUMP_PIPE_MAX) {\n                                /* This is iffy. A backtrace can be a few hundred kilobytes, but too much is\n                                 * too much. Let's log a warning and ignore the rest. */\n                                log_warning(\"Generated backtrace is %zu bytes (more than the limit of %u bytes), backtrace will be truncated.\",\n                                            len, COREDUMP_PIPE_MAX);\n                                len = COREDUMP_PIPE_MAX;\n                        }\n\n                        /* Bump the space for the returned string.\n                         * Failure is ignored, because partial output is still useful. */\n                        (void) fcntl(return_pipe[1], F_SETPIPE_SZ, len);\n\n                        r = loop_write(return_pipe[1], buf, len, false);\n                        if (r == -EAGAIN)\n                                log_warning(\"Write failed, backtrace will be truncated.\");\n                        else if (r < 0)\n                                goto child_fail;\n\n                        return_pipe[1] = safe_close(return_pipe[1]);\n                }\n\n                if (package_metadata) {\n                        _cleanup_fclose_ FILE *json_out = NULL;\n\n                        /* Bump the space for the returned string. We don't know how much space we'll need in\n                         * advance, so we'll just try to write as much as possible and maybe fail later. */\n                        (void) fcntl(json_pipe[1], F_SETPIPE_SZ, COREDUMP_PIPE_MAX);\n\n                        json_out = take_fdopen(&json_pipe[1], \"w\");\n                        if (!json_out) {\n                                r = -errno;\n                                goto child_fail;\n                        }\n\n                        r = json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);\n                        if (r < 0)\n                                log_warning_errno(r, \"Failed to write JSON package metadata, ignoring: %m\");\n                }\n\n                _exit(EXIT_SUCCESS);\n\n        child_fail:\n                (void) write(error_pipe[1], &r, sizeof(r));\n                _exit(EXIT_FAILURE);\n        }\n\n        error_pipe[1] = safe_close(error_pipe[1]);\n        return_pipe[1] = safe_close(return_pipe[1]);\n        json_pipe[1] = safe_close(json_pipe[1]);\n\n        if (ret) {\n                _cleanup_fclose_ FILE *in = NULL;\n\n                in = take_fdopen(&return_pipe[0], \"r\");\n                if (!in)\n                        return -errno;\n\n                r = read_full_stream(in, &buf, NULL);\n                if (r < 0)\n                        return r;\n        }\n\n        if (ret_package_metadata) {\n                _cleanup_fclose_ FILE *json_in = NULL;\n\n                json_in = take_fdopen(&json_pipe[0], \"r\");\n                if (!json_in)\n                        return -errno;\n\n                r = json_parse_file(json_in, NULL, 0, &package_metadata, NULL, NULL);\n                if (r < 0 && r != -ENODATA) /* ENODATA: json was empty, so we got nothing, but that's ok */\n                        log_warning_errno(r, \"Failed to read or parse json metadata, ignoring: %m\");\n        }\n\n        if (ret)\n                *ret = TAKE_PTR(buf);\n        if (ret_package_metadata)\n                *ret_package_metadata = TAKE_PTR(package_metadata);\n\n        return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,13 +19,13 @@\n                 return r;\n \n         if (ret) {\n-                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC));\n+                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC|O_NONBLOCK));\n                 if (r < 0)\n                         return r;\n         }\n \n         if (ret_package_metadata) {\n-                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC));\n+                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC|O_NONBLOCK));\n                 if (r < 0)\n                         return r;\n         }\n@@ -69,8 +69,24 @@\n                         goto child_fail;\n \n                 if (buf) {\n-                        r = loop_write(return_pipe[1], buf, strlen(buf), false);\n-                        if (r < 0)\n+                        size_t len = strlen(buf);\n+\n+                        if (len > COREDUMP_PIPE_MAX) {\n+                                /* This is iffy. A backtrace can be a few hundred kilobytes, but too much is\n+                                 * too much. Let's log a warning and ignore the rest. */\n+                                log_warning(\"Generated backtrace is %zu bytes (more than the limit of %u bytes), backtrace will be truncated.\",\n+                                            len, COREDUMP_PIPE_MAX);\n+                                len = COREDUMP_PIPE_MAX;\n+                        }\n+\n+                        /* Bump the space for the returned string.\n+                         * Failure is ignored, because partial output is still useful. */\n+                        (void) fcntl(return_pipe[1], F_SETPIPE_SZ, len);\n+\n+                        r = loop_write(return_pipe[1], buf, len, false);\n+                        if (r == -EAGAIN)\n+                                log_warning(\"Write failed, backtrace will be truncated.\");\n+                        else if (r < 0)\n                                 goto child_fail;\n \n                         return_pipe[1] = safe_close(return_pipe[1]);\n@@ -79,13 +95,19 @@\n                 if (package_metadata) {\n                         _cleanup_fclose_ FILE *json_out = NULL;\n \n+                        /* Bump the space for the returned string. We don't know how much space we'll need in\n+                         * advance, so we'll just try to write as much as possible and maybe fail later. */\n+                        (void) fcntl(json_pipe[1], F_SETPIPE_SZ, COREDUMP_PIPE_MAX);\n+\n                         json_out = take_fdopen(&json_pipe[1], \"w\");\n                         if (!json_out) {\n                                 r = -errno;\n                                 goto child_fail;\n                         }\n \n-                        json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);\n+                        r = json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);\n+                        if (r < 0)\n+                                log_warning_errno(r, \"Failed to write JSON package metadata, ignoring: %m\");\n                 }\n \n                 _exit(EXIT_SUCCESS);\n@@ -120,7 +142,7 @@\n \n                 r = json_parse_file(json_in, NULL, 0, &package_metadata, NULL, NULL);\n                 if (r < 0 && r != -ENODATA) /* ENODATA: json was empty, so we got nothing, but that's ok */\n-                        return r;\n+                        log_warning_errno(r, \"Failed to read or parse json metadata, ignoring: %m\");\n         }\n \n         if (ret)",
        "diff_line_info": {
            "deleted_lines": [
                "                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC));",
                "                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC));",
                "                        r = loop_write(return_pipe[1], buf, strlen(buf), false);",
                "                        if (r < 0)",
                "                        json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);",
                "                        return r;"
            ],
            "added_lines": [
                "                r = RET_NERRNO(pipe2(return_pipe, O_CLOEXEC|O_NONBLOCK));",
                "                r = RET_NERRNO(pipe2(json_pipe, O_CLOEXEC|O_NONBLOCK));",
                "                        size_t len = strlen(buf);",
                "",
                "                        if (len > COREDUMP_PIPE_MAX) {",
                "                                /* This is iffy. A backtrace can be a few hundred kilobytes, but too much is",
                "                                 * too much. Let's log a warning and ignore the rest. */",
                "                                log_warning(\"Generated backtrace is %zu bytes (more than the limit of %u bytes), backtrace will be truncated.\",",
                "                                            len, COREDUMP_PIPE_MAX);",
                "                                len = COREDUMP_PIPE_MAX;",
                "                        }",
                "",
                "                        /* Bump the space for the returned string.",
                "                         * Failure is ignored, because partial output is still useful. */",
                "                        (void) fcntl(return_pipe[1], F_SETPIPE_SZ, len);",
                "",
                "                        r = loop_write(return_pipe[1], buf, len, false);",
                "                        if (r == -EAGAIN)",
                "                                log_warning(\"Write failed, backtrace will be truncated.\");",
                "                        else if (r < 0)",
                "                        /* Bump the space for the returned string. We don't know how much space we'll need in",
                "                         * advance, so we'll just try to write as much as possible and maybe fail later. */",
                "                        (void) fcntl(json_pipe[1], F_SETPIPE_SZ, COREDUMP_PIPE_MAX);",
                "",
                "                        r = json_variant_dump(package_metadata, JSON_FORMAT_FLUSH, json_out, NULL);",
                "                        if (r < 0)",
                "                                log_warning_errno(r, \"Failed to write JSON package metadata, ignoring: %m\");",
                "                        log_warning_errno(r, \"Failed to read or parse json metadata, ignoring: %m\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-48063",
        "func_name": "binutils-gdb/load_specific_debug_section",
        "description": "GNU Binutils before 2.40 was discovered to contain an excessive memory consumption vulnerability via the function load_separate_debug_files at dwarf2.c. The attacker could supply a crafted ELF file and cause a DNS attack.",
        "git_url": "https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;h=75393a2d54bcc40053e5262a3de9d70c5ebfbbfd",
        "commit_title": "",
        "commit_text": "Fix an attempt to allocate an unreasonably large amount of memory when parsing a corrupt ELF file.  \tPR  29924 \t* objdump.c (load_specific_debug_section): Check for excessively \tlarge sections. ",
        "func_before": "static bool\nload_specific_debug_section (enum dwarf_section_display_enum debug,\n\t\t\t     asection *sec, void *file)\n{\n  struct dwarf_section *section = &debug_displays [debug].section;\n  bfd *abfd = (bfd *) file;\n  bfd_byte *contents;\n  bfd_size_type amt;\n  size_t alloced;\n  bool ret;\n\n  if (section->start != NULL)\n    {\n      /* If it is already loaded, do nothing.  */\n      if (streq (section->filename, bfd_get_filename (abfd)))\n\treturn true;\n      free (section->start);\n    }\n\n  section->filename = bfd_get_filename (abfd);\n  section->reloc_info = NULL;\n  section->num_relocs = 0;\n  section->address = bfd_section_vma (sec);\n  section->size = bfd_section_size (sec);\n  /* PR 24360: On 32-bit hosts sizeof (size_t) < sizeof (bfd_size_type). */\n  alloced = amt = section->size + 1;\n  if (alloced != amt || alloced == 0)\n    {\n      section->start = NULL;\n      free_debug_section (debug);\n      printf (_(\"\\nSection '%s' has an invalid size: %#\" PRIx64 \".\\n\"),\n\t      sanitize_string (section->name),\n\t      section->size);\n      return false;\n    }\n\n  section->start = contents = xmalloc (alloced);\n  /* Ensure any string section has a terminating NUL.  */\n  section->start[section->size] = 0;\n\n  if ((abfd->flags & (EXEC_P | DYNAMIC)) == 0\n      && debug_displays [debug].relocate)\n    {\n      ret = bfd_simple_get_relocated_section_contents (abfd,\n\t\t\t\t\t\t       sec,\n\t\t\t\t\t\t       section->start,\n\t\t\t\t\t\t       syms) != NULL;\n      if (ret)\n\t{\n\t  long reloc_size = bfd_get_reloc_upper_bound (abfd, sec);\n\n\t  if (reloc_size > 0)\n\t    {\n\t      long reloc_count;\n\t      arelent **relocs;\n\n\t      relocs = (arelent **) xmalloc (reloc_size);\n\n\t      reloc_count = bfd_canonicalize_reloc (abfd, sec, relocs, syms);\n\t      if (reloc_count <= 0)\n\t\tfree (relocs);\n\t      else\n\t\t{\n\t\t  section->reloc_info = relocs;\n\t\t  section->num_relocs = reloc_count;\n\t\t}\n\t    }\n\t}\n    }\n  else\n    ret = bfd_get_full_section_contents (abfd, sec, &contents);\n\n  if (!ret)\n    {\n      free_debug_section (debug);\n      printf (_(\"\\nCan't get contents for section '%s'.\\n\"),\n\t      sanitize_string (section->name));\n      return false;\n    }\n\n  return true;\n}",
        "func": "static bool\nload_specific_debug_section (enum dwarf_section_display_enum debug,\n\t\t\t     asection *sec, void *file)\n{\n  struct dwarf_section *section = &debug_displays [debug].section;\n  bfd *abfd = (bfd *) file;\n  bfd_byte *contents;\n  bfd_size_type amt;\n  size_t alloced;\n  bool ret;\n\n  if (section->start != NULL)\n    {\n      /* If it is already loaded, do nothing.  */\n      if (streq (section->filename, bfd_get_filename (abfd)))\n\treturn true;\n      free (section->start);\n    }\n\n  section->filename = bfd_get_filename (abfd);\n  section->reloc_info = NULL;\n  section->num_relocs = 0;\n  section->address = bfd_section_vma (sec);\n  section->size = bfd_section_size (sec);\n  /* PR 24360: On 32-bit hosts sizeof (size_t) < sizeof (bfd_size_type). */\n  alloced = amt = section->size + 1;\n  if (alloced != amt\n      || alloced == 0\n      || (bfd_get_size (abfd) != 0 && alloced >= bfd_get_size (abfd)))\n    {\n      section->start = NULL;\n      free_debug_section (debug);\n      printf (_(\"\\nSection '%s' has an invalid size: %#\" PRIx64 \".\\n\"),\n\t      sanitize_string (section->name),\n\t      section->size);\n      return false;\n    }\n\n  section->start = contents = xmalloc (alloced);\n  /* Ensure any string section has a terminating NUL.  */\n  section->start[section->size] = 0;\n\n  if ((abfd->flags & (EXEC_P | DYNAMIC)) == 0\n      && debug_displays [debug].relocate)\n    {\n      ret = bfd_simple_get_relocated_section_contents (abfd,\n\t\t\t\t\t\t       sec,\n\t\t\t\t\t\t       section->start,\n\t\t\t\t\t\t       syms) != NULL;\n      if (ret)\n\t{\n\t  long reloc_size = bfd_get_reloc_upper_bound (abfd, sec);\n\n\t  if (reloc_size > 0)\n\t    {\n\t      long reloc_count;\n\t      arelent **relocs;\n\n\t      relocs = (arelent **) xmalloc (reloc_size);\n\n\t      reloc_count = bfd_canonicalize_reloc (abfd, sec, relocs, syms);\n\t      if (reloc_count <= 0)\n\t\tfree (relocs);\n\t      else\n\t\t{\n\t\t  section->reloc_info = relocs;\n\t\t  section->num_relocs = reloc_count;\n\t\t}\n\t    }\n\t}\n    }\n  else\n    ret = bfd_get_full_section_contents (abfd, sec, &contents);\n\n  if (!ret)\n    {\n      free_debug_section (debug);\n      printf (_(\"\\nCan't get contents for section '%s'.\\n\"),\n\t      sanitize_string (section->name));\n      return false;\n    }\n\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,7 +24,9 @@\n   section->size = bfd_section_size (sec);\n   /* PR 24360: On 32-bit hosts sizeof (size_t) < sizeof (bfd_size_type). */\n   alloced = amt = section->size + 1;\n-  if (alloced != amt || alloced == 0)\n+  if (alloced != amt\n+      || alloced == 0\n+      || (bfd_get_size (abfd) != 0 && alloced >= bfd_get_size (abfd)))\n     {\n       section->start = NULL;\n       free_debug_section (debug);",
        "diff_line_info": {
            "deleted_lines": [
                "  if (alloced != amt || alloced == 0)"
            ],
            "added_lines": [
                "  if (alloced != amt",
                "      || alloced == 0",
                "      || (bfd_get_size (abfd) != 0 && alloced >= bfd_get_size (abfd)))"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-48571",
        "func_name": "memcached/try_read_udp",
        "description": "memcached 1.6.7 allows a Denial of Service via multi-packet uploads in UDP.",
        "git_url": "https://github.com/memcached/memcached/commit/6b319c8c7a29e9c353dec83dc92f01905f6c8966",
        "commit_title": "udp: crash fix when receiving multi-packet uploads",
        "commit_text": " multi-packet sets aren't supported in UDP. It was trying to write an error message in response, but no mc_resp object was initialized at the time, leading to a null reference crash.  Instead drop the packet quietly since that's a pretty fatal bug anyway and keeps the parser simplified.  For credits see accompanying release notes.",
        "func_before": "static enum try_read_result try_read_udp(conn *c) {\n    int res;\n\n    assert(c != NULL);\n\n    c->request_addr_size = sizeof(c->request_addr);\n    res = recvfrom(c->sfd, c->rbuf, c->rsize,\n                   0, (struct sockaddr *)&c->request_addr,\n                   &c->request_addr_size);\n    if (res > 8) {\n        unsigned char *buf = (unsigned char *)c->rbuf;\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.bytes_read += res;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        /* Beginning of UDP packet is the request ID; save it. */\n        c->request_id = buf[0] * 256 + buf[1];\n\n        /* If this is a multi-packet request, drop it. */\n        if (buf[4] != 0 || buf[5] != 1) {\n            out_string(c, \"SERVER_ERROR multi-packet request not supported\");\n            return READ_NO_DATA_RECEIVED;\n        }\n\n        /* Don't care about any of the rest of the header. */\n        res -= 8;\n        memmove(c->rbuf, c->rbuf + 8, res);\n\n        c->rbytes = res;\n        c->rcurr = c->rbuf;\n        return READ_DATA_RECEIVED;\n    }\n    return READ_NO_DATA_RECEIVED;\n}",
        "func": "static enum try_read_result try_read_udp(conn *c) {\n    int res;\n\n    assert(c != NULL);\n\n    c->request_addr_size = sizeof(c->request_addr);\n    res = recvfrom(c->sfd, c->rbuf, c->rsize,\n                   0, (struct sockaddr *)&c->request_addr,\n                   &c->request_addr_size);\n    if (res > 8) {\n        unsigned char *buf = (unsigned char *)c->rbuf;\n        pthread_mutex_lock(&c->thread->stats.mutex);\n        c->thread->stats.bytes_read += res;\n        pthread_mutex_unlock(&c->thread->stats.mutex);\n\n        /* Beginning of UDP packet is the request ID; save it. */\n        c->request_id = buf[0] * 256 + buf[1];\n\n        /* If this is a multi-packet request, drop it. */\n        if (buf[4] != 0 || buf[5] != 1) {\n            return READ_NO_DATA_RECEIVED;\n        }\n\n        /* Don't care about any of the rest of the header. */\n        res -= 8;\n        memmove(c->rbuf, c->rbuf + 8, res);\n\n        c->rbytes = res;\n        c->rcurr = c->rbuf;\n        return READ_DATA_RECEIVED;\n    }\n    return READ_NO_DATA_RECEIVED;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,7 +18,6 @@\n \n         /* If this is a multi-packet request, drop it. */\n         if (buf[4] != 0 || buf[5] != 1) {\n-            out_string(c, \"SERVER_ERROR multi-packet request not supported\");\n             return READ_NO_DATA_RECEIVED;\n         }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "            out_string(c, \"SERVER_ERROR multi-packet request not supported\");"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-48571",
        "func_name": "memcached/out_string",
        "description": "memcached 1.6.7 allows a Denial of Service via multi-packet uploads in UDP.",
        "git_url": "https://github.com/memcached/memcached/commit/6b319c8c7a29e9c353dec83dc92f01905f6c8966",
        "commit_title": "udp: crash fix when receiving multi-packet uploads",
        "commit_text": " multi-packet sets aren't supported in UDP. It was trying to write an error message in response, but no mc_resp object was initialized at the time, leading to a null reference crash.  Instead drop the packet quietly since that's a pretty fatal bug anyway and keeps the parser simplified.  For credits see accompanying release notes.",
        "func_before": "void out_string(conn *c, const char *str) {\n    size_t len;\n    mc_resp *resp = c->resp;\n\n    assert(c != NULL);\n    // if response was original filled with something, but we're now writing\n    // out an error or similar, have to reset the object first.\n    // TODO: since this is often redundant with allocation, how many callers\n    // are actually requiring it be reset? Can we fast test by just looking at\n    // tosend and reset if nonzero?\n    resp_reset(resp);\n\n    if (c->noreply) {\n        // TODO: just invalidate the response since nothing's been attempted\n        // to send yet?\n        resp->skip = true;\n        if (settings.verbose > 1)\n            fprintf(stderr, \">%d NOREPLY %s\\n\", c->sfd, str);\n        conn_set_state(c, conn_new_cmd);\n        return;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d %s\\n\", c->sfd, str);\n\n    // Fill response object with static string.\n\n    len = strlen(str);\n    if ((len + 2) > WRITE_BUFFER_SIZE) {\n        /* ought to be always enough. just fail for simplicity */\n        str = \"SERVER_ERROR output line too long\";\n        len = strlen(str);\n    }\n\n    memcpy(resp->wbuf, str, len);\n    memcpy(resp->wbuf + len, \"\\r\\n\", 2);\n    resp_add_iov(resp, resp->wbuf, len + 2);\n\n    conn_set_state(c, conn_new_cmd);\n    return;\n}",
        "func": "void out_string(conn *c, const char *str) {\n    size_t len;\n    assert(c != NULL);\n    mc_resp *resp = c->resp;\n\n    // if response was original filled with something, but we're now writing\n    // out an error or similar, have to reset the object first.\n    // TODO: since this is often redundant with allocation, how many callers\n    // are actually requiring it be reset? Can we fast test by just looking at\n    // tosend and reset if nonzero?\n    resp_reset(resp);\n\n    if (c->noreply) {\n        // TODO: just invalidate the response since nothing's been attempted\n        // to send yet?\n        resp->skip = true;\n        if (settings.verbose > 1)\n            fprintf(stderr, \">%d NOREPLY %s\\n\", c->sfd, str);\n        conn_set_state(c, conn_new_cmd);\n        return;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d %s\\n\", c->sfd, str);\n\n    // Fill response object with static string.\n\n    len = strlen(str);\n    if ((len + 2) > WRITE_BUFFER_SIZE) {\n        /* ought to be always enough. just fail for simplicity */\n        str = \"SERVER_ERROR output line too long\";\n        len = strlen(str);\n    }\n\n    memcpy(resp->wbuf, str, len);\n    memcpy(resp->wbuf + len, \"\\r\\n\", 2);\n    resp_add_iov(resp, resp->wbuf, len + 2);\n\n    conn_set_state(c, conn_new_cmd);\n    return;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,8 +1,8 @@\n void out_string(conn *c, const char *str) {\n     size_t len;\n+    assert(c != NULL);\n     mc_resp *resp = c->resp;\n \n-    assert(c != NULL);\n     // if response was original filled with something, but we're now writing\n     // out an error or similar, have to reset the object first.\n     // TODO: since this is often redundant with allocation, how many callers",
        "diff_line_info": {
            "deleted_lines": [
                "    assert(c != NULL);"
            ],
            "added_lines": [
                "    assert(c != NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-13623",
        "func_name": "jerryscript-project/jerryscript/ecma_proxy_object_has",
        "description": "JerryScript 2.2.0 allows attackers to cause a denial of service (stack consumption) via a proxy operation.",
        "git_url": "https://github.com/jerryscript-project/jerryscript/commit/a096da74c2c753b83daa61c1c807ec43de287917",
        "commit_title": "Add stack limit check to proxy operations",
        "commit_text": " Fixes #3785.  JerryScript-DCO-1.0-Signed-off-by: Dniel Btyai dbatyai@inf.u-szeged.hu",
        "func_before": "ecma_value_t\necma_proxy_object_has (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p) /**< property name */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_HAS);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_has_property (target_obj_p, prop_name_p);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = {target, prop_value};\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 2);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  bool boolean_trap_result = ecma_op_to_boolean (trap_result);\n\n  ecma_free_value (trap_result);\n\n  /* 11. */\n  if (!boolean_trap_result)\n  {\n    ecma_property_descriptor_t target_desc;\n\n    ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (status))\n    {\n      return status;\n    }\n\n    if (ecma_is_value_true (status))\n    {\n      bool prop_is_configurable = target_desc.flags & ECMA_PROP_IS_CONFIGURABLE;\n\n      ecma_free_property_descriptor (&target_desc);\n\n      if (!prop_is_configurable)\n      {\n        return ecma_raise_type_error (ECMA_ERR_MSG (\"Trap returned falsish for property which exists \"\n                                                    \"in the proxy target as non-configurable\"));\n      }\n\n      ecma_value_t extensible_target = ecma_builtin_object_object_is_extensible (target_obj_p);\n\n      if (ECMA_IS_VALUE_ERROR (extensible_target))\n      {\n        return extensible_target;\n      }\n\n      if (ecma_is_value_false (extensible_target))\n      {\n        return ecma_raise_type_error (ECMA_ERR_MSG (\"Trap returned falsish for property but \"\n                                                    \"the proxy target is not extensible\"));\n      }\n    }\n  }\n\n  /* 12. */\n  return ecma_make_boolean_value (boolean_trap_result);\n}",
        "func": "ecma_value_t\necma_proxy_object_has (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p) /**< property name */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n  ECMA_CHECK_STACK_USAGE ();\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_HAS);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_has_property (target_obj_p, prop_name_p);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = {target, prop_value};\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 2);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  bool boolean_trap_result = ecma_op_to_boolean (trap_result);\n\n  ecma_free_value (trap_result);\n\n  /* 11. */\n  if (!boolean_trap_result)\n  {\n    ecma_property_descriptor_t target_desc;\n\n    ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (status))\n    {\n      return status;\n    }\n\n    if (ecma_is_value_true (status))\n    {\n      bool prop_is_configurable = target_desc.flags & ECMA_PROP_IS_CONFIGURABLE;\n\n      ecma_free_property_descriptor (&target_desc);\n\n      if (!prop_is_configurable)\n      {\n        return ecma_raise_type_error (ECMA_ERR_MSG (\"Trap returned falsish for property which exists \"\n                                                    \"in the proxy target as non-configurable\"));\n      }\n\n      ecma_value_t extensible_target = ecma_builtin_object_object_is_extensible (target_obj_p);\n\n      if (ECMA_IS_VALUE_ERROR (extensible_target))\n      {\n        return extensible_target;\n      }\n\n      if (ecma_is_value_false (extensible_target))\n      {\n        return ecma_raise_type_error (ECMA_ERR_MSG (\"Trap returned falsish for property but \"\n                                                    \"the proxy target is not extensible\"));\n      }\n    }\n  }\n\n  /* 12. */\n  return ecma_make_boolean_value (boolean_trap_result);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n                        ecma_string_t *prop_name_p) /**< property name */\n {\n   JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n+  ECMA_CHECK_STACK_USAGE ();\n \n   ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  ECMA_CHECK_STACK_USAGE ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-13623",
        "func_name": "jerryscript-project/jerryscript/ecma_proxy_object_get",
        "description": "JerryScript 2.2.0 allows attackers to cause a denial of service (stack consumption) via a proxy operation.",
        "git_url": "https://github.com/jerryscript-project/jerryscript/commit/a096da74c2c753b83daa61c1c807ec43de287917",
        "commit_title": "Add stack limit check to proxy operations",
        "commit_text": " Fixes #3785.  JerryScript-DCO-1.0-Signed-off-by: Dniel Btyai dbatyai@inf.u-szeged.hu",
        "func_before": "ecma_value_t\necma_proxy_object_get (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p, /**< property name */\n                       ecma_value_t receiver) /**< receiver to invoke getter function */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_GET);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_get_with_receiver (target_obj_p, prop_name_p, receiver);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = { target, prop_value, receiver };\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 3);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  /* 11. */\n  ecma_property_descriptor_t target_desc;\n\n  ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n  /* 12. */\n  if (ECMA_IS_VALUE_ERROR (status))\n  {\n    return status;\n  }\n\n  /* 13. */\n  if (ecma_is_value_true (status))\n  {\n    ecma_value_t ret_value = ECMA_VALUE_EMPTY;\n\n    if ((target_desc.flags & ECMA_PROP_IS_VALUE_DEFINED)\n        && !(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n        && !(target_desc.flags & ECMA_PROP_IS_WRITABLE)\n        && !ecma_op_same_value (trap_result, target_desc.value))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"given property is a read-only and non-configurable\"\n                                                       \" data property on the proxy target\"));\n    }\n    else if (!(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n            && (target_desc.flags & (ECMA_PROP_IS_GET_DEFINED | ECMA_PROP_IS_SET_DEFINED))\n            && target_desc.get_p == NULL\n            && !ecma_is_value_undefined (trap_result))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"given property is a non-configurable property and\"\n                                                       \" does not have a getter function\"));\n    }\n\n    ecma_free_property_descriptor (&target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (ret_value))\n    {\n      ecma_free_value (trap_result);\n\n      return ret_value;\n    }\n  }\n\n  /* 14. */\n  return trap_result;\n}",
        "func": "ecma_value_t\necma_proxy_object_get (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p, /**< property name */\n                       ecma_value_t receiver) /**< receiver to invoke getter function */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n  ECMA_CHECK_STACK_USAGE ();\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_GET);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_get_with_receiver (target_obj_p, prop_name_p, receiver);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = { target, prop_value, receiver };\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 3);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  /* 11. */\n  ecma_property_descriptor_t target_desc;\n\n  ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n  /* 12. */\n  if (ECMA_IS_VALUE_ERROR (status))\n  {\n    return status;\n  }\n\n  /* 13. */\n  if (ecma_is_value_true (status))\n  {\n    ecma_value_t ret_value = ECMA_VALUE_EMPTY;\n\n    if ((target_desc.flags & ECMA_PROP_IS_VALUE_DEFINED)\n        && !(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n        && !(target_desc.flags & ECMA_PROP_IS_WRITABLE)\n        && !ecma_op_same_value (trap_result, target_desc.value))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"given property is a read-only and non-configurable\"\n                                                       \" data property on the proxy target\"));\n    }\n    else if (!(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n            && (target_desc.flags & (ECMA_PROP_IS_GET_DEFINED | ECMA_PROP_IS_SET_DEFINED))\n            && target_desc.get_p == NULL\n            && !ecma_is_value_undefined (trap_result))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"given property is a non-configurable property and\"\n                                                       \" does not have a getter function\"));\n    }\n\n    ecma_free_property_descriptor (&target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (ret_value))\n    {\n      ecma_free_value (trap_result);\n\n      return ret_value;\n    }\n  }\n\n  /* 14. */\n  return trap_result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,7 @@\n                        ecma_value_t receiver) /**< receiver to invoke getter function */\n {\n   JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n+  ECMA_CHECK_STACK_USAGE ();\n \n   ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  ECMA_CHECK_STACK_USAGE ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-13623",
        "func_name": "jerryscript-project/jerryscript/ecma_proxy_object_set",
        "description": "JerryScript 2.2.0 allows attackers to cause a denial of service (stack consumption) via a proxy operation.",
        "git_url": "https://github.com/jerryscript-project/jerryscript/commit/a096da74c2c753b83daa61c1c807ec43de287917",
        "commit_title": "Add stack limit check to proxy operations",
        "commit_text": " Fixes #3785.  JerryScript-DCO-1.0-Signed-off-by: Dniel Btyai dbatyai@inf.u-szeged.hu",
        "func_before": "ecma_value_t\necma_proxy_object_set (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p, /**< property name */\n                       ecma_value_t value, /**< value to set */\n                       ecma_value_t receiver) /**< receiver to invoke setter function */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_SET);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_put_with_receiver (target_obj_p, prop_name_p, value, receiver, false);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_name_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = { target, prop_name_value, value, receiver };\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 4);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  bool boolean_trap_result = ecma_op_to_boolean (trap_result);\n\n  ecma_free_value (trap_result);\n\n  /* 11. */\n  if (!boolean_trap_result)\n  {\n    return ECMA_VALUE_FALSE;\n  }\n\n  /* 12. */\n  ecma_property_descriptor_t target_desc;\n\n  ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n  /* 13. */\n  if (ECMA_IS_VALUE_ERROR (status))\n  {\n    return status;\n  }\n\n  /* 14. */\n  if (ecma_is_value_true (status))\n  {\n    ecma_value_t ret_value = ECMA_VALUE_EMPTY;\n\n    if ((target_desc.flags & ECMA_PROP_IS_VALUE_DEFINED)\n        && !(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n        && !(target_desc.flags & ECMA_PROP_IS_WRITABLE)\n        && !ecma_op_same_value (value, target_desc.value))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"The property exists in the proxy target as a\"\n                                                       \" non-configurable and non-writable data property\"\n                                                       \" with a different value.\"));\n    }\n    else if (!(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n            && (target_desc.flags & (ECMA_PROP_IS_GET_DEFINED | ECMA_PROP_IS_SET_DEFINED))\n            && target_desc.set_p == NULL)\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"The property exists in the proxy target as a\"\n                                                       \" non-configurable accessor property whitout a setter.\"));\n    }\n\n    ecma_free_property_descriptor (&target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (ret_value))\n    {\n      return ret_value;\n    }\n  }\n\n  /* 15. */\n  return ECMA_VALUE_TRUE;\n}",
        "func": "ecma_value_t\necma_proxy_object_set (ecma_object_t *obj_p, /**< proxy object */\n                       ecma_string_t *prop_name_p, /**< property name */\n                       ecma_value_t value, /**< value to set */\n                       ecma_value_t receiver) /**< receiver to invoke setter function */\n{\n  JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n  ECMA_CHECK_STACK_USAGE ();\n\n  ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n\n  /* 2. */\n  ecma_value_t handler = proxy_obj_p->handler;\n\n  /* 3-6. */\n  ecma_value_t trap = ecma_validate_proxy_object (handler, LIT_MAGIC_STRING_SET);\n\n  /* 7. */\n  if (ECMA_IS_VALUE_ERROR (trap))\n  {\n    return trap;\n  }\n\n  ecma_value_t target = proxy_obj_p->target;\n  ecma_object_t *target_obj_p = ecma_get_object_from_value (target);\n\n  /* 8. */\n  if (ecma_is_value_undefined (trap))\n  {\n    return ecma_op_object_put_with_receiver (target_obj_p, prop_name_p, value, receiver, false);\n  }\n\n  ecma_object_t *func_obj_p = ecma_get_object_from_value (trap);\n  ecma_value_t prop_name_value = ecma_make_prop_name_value (prop_name_p);\n  ecma_value_t args[] = { target, prop_name_value, value, receiver };\n\n  /* 9. */\n  ecma_value_t trap_result = ecma_op_function_call (func_obj_p, handler, args, 4);\n\n  ecma_deref_object (func_obj_p);\n\n  /* 10. */\n  if (ECMA_IS_VALUE_ERROR (trap_result))\n  {\n    return trap_result;\n  }\n\n  bool boolean_trap_result = ecma_op_to_boolean (trap_result);\n\n  ecma_free_value (trap_result);\n\n  /* 11. */\n  if (!boolean_trap_result)\n  {\n    return ECMA_VALUE_FALSE;\n  }\n\n  /* 12. */\n  ecma_property_descriptor_t target_desc;\n\n  ecma_value_t status = ecma_op_object_get_own_property_descriptor (target_obj_p, prop_name_p, &target_desc);\n\n  /* 13. */\n  if (ECMA_IS_VALUE_ERROR (status))\n  {\n    return status;\n  }\n\n  /* 14. */\n  if (ecma_is_value_true (status))\n  {\n    ecma_value_t ret_value = ECMA_VALUE_EMPTY;\n\n    if ((target_desc.flags & ECMA_PROP_IS_VALUE_DEFINED)\n        && !(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n        && !(target_desc.flags & ECMA_PROP_IS_WRITABLE)\n        && !ecma_op_same_value (value, target_desc.value))\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"The property exists in the proxy target as a\"\n                                                       \" non-configurable and non-writable data property\"\n                                                       \" with a different value.\"));\n    }\n    else if (!(target_desc.flags & ECMA_PROP_IS_CONFIGURABLE)\n            && (target_desc.flags & (ECMA_PROP_IS_GET_DEFINED | ECMA_PROP_IS_SET_DEFINED))\n            && target_desc.set_p == NULL)\n    {\n      ret_value = ecma_raise_type_error (ECMA_ERR_MSG (\"The property exists in the proxy target as a\"\n                                                       \" non-configurable accessor property whitout a setter.\"));\n    }\n\n    ecma_free_property_descriptor (&target_desc);\n\n    if (ECMA_IS_VALUE_ERROR (ret_value))\n    {\n      return ret_value;\n    }\n  }\n\n  /* 15. */\n  return ECMA_VALUE_TRUE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n                        ecma_value_t receiver) /**< receiver to invoke setter function */\n {\n   JERRY_ASSERT (ECMA_OBJECT_IS_PROXY (obj_p));\n+  ECMA_CHECK_STACK_USAGE ();\n \n   ecma_proxy_object_t *proxy_obj_p = (ecma_proxy_object_t *) obj_p;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  ECMA_CHECK_STACK_USAGE ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-20812",
        "func_name": "torvalds/linux/prb_calc_retire_blk_tmo",
        "description": "An issue was discovered in the Linux kernel before 5.4.7. The prb_calc_retire_blk_tmo() function in net/packet/af_packet.c can result in a denial of service (CPU consumption and soft lockup) in a certain failure case involving TPACKET_V3, aka CID-b43d1f9f7067.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b43d1f9f7067c6759b1051e8ecb84e82cef569fe",
        "commit_title": "There is softlockup when using TPACKET_V3:",
        "commit_text": "... NMI watchdog: BUG: soft lockup - CPU#2 stuck for 60010ms! (__irq_svc) from [<c0558a0c>] (_raw_spin_unlock_irqrestore+0x44/0x54) (_raw_spin_unlock_irqrestore) from [<c027b7e8>] (mod_timer+0x210/0x25c) (mod_timer) from [<c0549c30>] (prb_retire_rx_blk_timer_expired+0x68/0x11c) (prb_retire_rx_blk_timer_expired) from [<c027a7ac>] (call_timer_fn+0x90/0x17c) (call_timer_fn) from [<c027ab6c>] (run_timer_softirq+0x2d4/0x2fc) (run_timer_softirq) from [<c021eaf4>] (__do_softirq+0x218/0x318) (__do_softirq) from [<c021eea0>] (irq_exit+0x88/0xac) (irq_exit) from [<c0240130>] (msa_irq_exit+0x11c/0x1d4) (msa_irq_exit) from [<c0209cf0>] (handle_IPI+0x650/0x7f4) (handle_IPI) from [<c02015bc>] (gic_handle_irq+0x108/0x118) (gic_handle_irq) from [<c0558ee4>] (__irq_usr+0x44/0x5c) ...  If __ethtool_get_link_ksettings() is failed in prb_calc_retire_blk_tmo(), msec and tmo will be zero, so tov_in_jiffies is zero and the timer expire for retire_blk_timer is turn to mod_timer(&pkc->retire_blk_timer, jiffies + 0), which will trigger cpu usage of softirq is 100%.  ",
        "func_before": "static int prb_calc_retire_blk_tmo(struct packet_sock *po,\n\t\t\t\tint blk_size_in_bytes)\n{\n\tstruct net_device *dev;\n\tunsigned int mbits = 0, msec = 0, div = 0, tmo = 0;\n\tstruct ethtool_link_ksettings ecmd;\n\tint err;\n\n\trtnl_lock();\n\tdev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\n\tif (unlikely(!dev)) {\n\t\trtnl_unlock();\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t}\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\trtnl_unlock();\n\tif (!err) {\n\t\t/*\n\t\t * If the link speed is so slow you don't really\n\t\t * need to worry about perf anyways\n\t\t */\n\t\tif (ecmd.base.speed < SPEED_1000 ||\n\t\t    ecmd.base.speed == SPEED_UNKNOWN) {\n\t\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t\t} else {\n\t\t\tmsec = 1;\n\t\t\tdiv = ecmd.base.speed / 1000;\n\t\t}\n\t}\n\n\tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n\n\tif (div)\n\t\tmbits /= div;\n\n\ttmo = mbits * msec;\n\n\tif (div)\n\t\treturn tmo+1;\n\treturn tmo;\n}",
        "func": "static int prb_calc_retire_blk_tmo(struct packet_sock *po,\n\t\t\t\tint blk_size_in_bytes)\n{\n\tstruct net_device *dev;\n\tunsigned int mbits = 0, msec = 0, div = 0, tmo = 0;\n\tstruct ethtool_link_ksettings ecmd;\n\tint err;\n\n\trtnl_lock();\n\tdev = __dev_get_by_index(sock_net(&po->sk), po->ifindex);\n\tif (unlikely(!dev)) {\n\t\trtnl_unlock();\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t}\n\terr = __ethtool_get_link_ksettings(dev, &ecmd);\n\trtnl_unlock();\n\tif (!err) {\n\t\t/*\n\t\t * If the link speed is so slow you don't really\n\t\t * need to worry about perf anyways\n\t\t */\n\t\tif (ecmd.base.speed < SPEED_1000 ||\n\t\t    ecmd.base.speed == SPEED_UNKNOWN) {\n\t\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\t\t} else {\n\t\t\tmsec = 1;\n\t\t\tdiv = ecmd.base.speed / 1000;\n\t\t}\n\t} else\n\t\treturn DEFAULT_PRB_RETIRE_TOV;\n\n\tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n\n\tif (div)\n\t\tmbits /= div;\n\n\ttmo = mbits * msec;\n\n\tif (div)\n\t\treturn tmo+1;\n\treturn tmo;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,7 +26,8 @@\n \t\t\tmsec = 1;\n \t\t\tdiv = ecmd.base.speed / 1000;\n \t\t}\n-\t}\n+\t} else\n+\t\treturn DEFAULT_PRB_RETIRE_TOV;\n \n \tmbits = (blk_size_in_bytes * 8) / (1024 * 1024);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t}"
            ],
            "added_lines": [
                "\t} else",
                "\t\treturn DEFAULT_PRB_RETIRE_TOV;"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0058",
        "func_name": "torvalds/linux/kiocb_batch_free",
        "description": "The kiocb_batch_free function in fs/aio.c in the Linux kernel before 3.2.2 allows local users to cause a denial of service (OOPS) via vectors that trigger incorrect iocb management.",
        "git_url": "https://github.com/torvalds/linux/commit/802f43594d6e4d2ac61086d239153c17873a0428",
        "commit_title": "Unused iocbs in a batch should not be accounted as active.",
        "commit_text": " commit 69e4747ee9727d660b88d7e1efe0f4afcb35db1b upstream.  Since commit 080d676de095 (\"aio: allocate kiocbs in batches\") iocbs are allocated in a batch during processing of first iocbs.  All iocbs in a batch are automatically added to ctx->active_reqs list and accounted in ctx->reqs_active.  If one (not the last one) of iocbs submitted by an user fails, further iocbs are not processed, but they are still present in ctx->active_reqs and accounted in ctx->reqs_active.  This causes process to stuck in a D state in wait_for_all_aios() on exit since ctx->reqs_active will never go down to zero.  Furthermore since kiocb_batch_free() frees iocb without removing it from active_reqs list the list become corrupted which may cause oops.  Fix this by removing iocb from ctx->active_reqs and updating ctx->reqs_active in kiocb_batch_free(). ",
        "func_before": "static void kiocb_batch_free(struct kiocb_batch *batch)\n{\n\tstruct kiocb *req, *n;\n\n\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\tlist_del(&req->ki_batch);\n\t\tkmem_cache_free(kiocb_cachep, req);\n\t}\n}",
        "func": "static void kiocb_batch_free(struct kioctx *ctx, struct kiocb_batch *batch)\n{\n\tstruct kiocb *req, *n;\n\n\tif (list_empty(&batch->head))\n\t\treturn;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n\t\tlist_del(&req->ki_batch);\n\t\tlist_del(&req->ki_list);\n\t\tkmem_cache_free(kiocb_cachep, req);\n\t\tctx->reqs_active--;\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,16 @@\n-static void kiocb_batch_free(struct kiocb_batch *batch)\n+static void kiocb_batch_free(struct kioctx *ctx, struct kiocb_batch *batch)\n {\n \tstruct kiocb *req, *n;\n \n+\tif (list_empty(&batch->head))\n+\t\treturn;\n+\n+\tspin_lock_irq(&ctx->ctx_lock);\n \tlist_for_each_entry_safe(req, n, &batch->head, ki_batch) {\n \t\tlist_del(&req->ki_batch);\n+\t\tlist_del(&req->ki_list);\n \t\tkmem_cache_free(kiocb_cachep, req);\n+\t\tctx->reqs_active--;\n \t}\n+\tspin_unlock_irq(&ctx->ctx_lock);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static void kiocb_batch_free(struct kiocb_batch *batch)"
            ],
            "added_lines": [
                "static void kiocb_batch_free(struct kioctx *ctx, struct kiocb_batch *batch)",
                "\tif (list_empty(&batch->head))",
                "\t\treturn;",
                "",
                "\tspin_lock_irq(&ctx->ctx_lock);",
                "\t\tlist_del(&req->ki_list);",
                "\t\tctx->reqs_active--;",
                "\tspin_unlock_irq(&ctx->ctx_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0058",
        "func_name": "torvalds/linux/do_io_submit",
        "description": "The kiocb_batch_free function in fs/aio.c in the Linux kernel before 3.2.2 allows local users to cause a denial of service (OOPS) via vectors that trigger incorrect iocb management.",
        "git_url": "https://github.com/torvalds/linux/commit/802f43594d6e4d2ac61086d239153c17873a0428",
        "commit_title": "Unused iocbs in a batch should not be accounted as active.",
        "commit_text": " commit 69e4747ee9727d660b88d7e1efe0f4afcb35db1b upstream.  Since commit 080d676de095 (\"aio: allocate kiocbs in batches\") iocbs are allocated in a batch during processing of first iocbs.  All iocbs in a batch are automatically added to ctx->active_reqs list and accounted in ctx->reqs_active.  If one (not the last one) of iocbs submitted by an user fails, further iocbs are not processed, but they are still present in ctx->active_reqs and accounted in ctx->reqs_active.  This causes process to stuck in a D state in wait_for_all_aios() on exit since ctx->reqs_active will never go down to zero.  Furthermore since kiocb_batch_free() frees iocb without removing it from active_reqs list the list become corrupted which may cause oops.  Fix this by removing iocb from ctx->active_reqs and updating ctx->reqs_active in kiocb_batch_free(). ",
        "func_before": "long do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\tstruct kiocb_batch batch;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: io_submit: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tkiocb_batch_init(&batch, nr);\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, &batch, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tkiocb_batch_free(&batch);\n\tput_ioctx(ctx);\n\treturn i ? i : ret;\n}",
        "func": "long do_io_submit(aio_context_t ctx_id, long nr,\n\t\t  struct iocb __user *__user *iocbpp, bool compat)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\tstruct kiocb_batch batch;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(nr > LONG_MAX/sizeof(*iocbpp)))\n\t\tnr = LONG_MAX/sizeof(*iocbpp);\n\n\tif (unlikely(!access_ok(VERIFY_READ, iocbpp, (nr*sizeof(*iocbpp)))))\n\t\treturn -EFAULT;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: io_submit: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tkiocb_batch_init(&batch, nr);\n\n\tblk_start_plug(&plug);\n\n\t/*\n\t * AKPM: should this return a partial result if some of the IOs were\n\t * successfully submitted?\n\t */\n\tfor (i=0; i<nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\t\tstruct iocb tmp;\n\n\t\tif (unlikely(__get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, &tmp, &batch, compat);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tblk_finish_plug(&plug);\n\n\tkiocb_batch_free(ctx, &batch);\n\tput_ioctx(ctx);\n\treturn i ? i : ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,7 +50,7 @@\n \t}\n \tblk_finish_plug(&plug);\n \n-\tkiocb_batch_free(&batch);\n+\tkiocb_batch_free(ctx, &batch);\n \tput_ioctx(ctx);\n \treturn i ? i : ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tkiocb_batch_free(&batch);"
            ],
            "added_lines": [
                "\tkiocb_batch_free(ctx, &batch);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0879",
        "func_name": "torvalds/linux/copy_process",
        "description": "The I/O implementation for block devices in the Linux kernel before 2.6.33 does not properly handle the CLONE_IO feature, which allows local users to cause a denial of service (I/O instability) by starting multiple processes that share an I/O context.",
        "git_url": "https://github.com/torvalds/linux/commit/b69f2292063d2caf37ca9aec7d63ded203701bf3",
        "commit_title": "block: Fix io_context leak after failure of clone with CLONE_IO",
        "commit_text": " With CLONE_IO, parent's io_context->nr_tasks is incremented, but never decremented whenever copy_process() fails afterwards, which prevents exit_io_context() from calling IO schedulers exit functions.  Give a task_struct to exit_io_context(), and call exit_io_context() instead of put_io_context() in copy_process() cleanup path. ",
        "func_before": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tint cgroup_callbacks_done = 0;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\tp->signal->rlim[RLIMIT_NPROC].rlim_cur) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = cputime_zero;\n\tp->stime = cputime_zero;\n\tp->gtime = cputime_zero;\n\tp->utimescaled = cputime_zero;\n\tp->stimescaled = cputime_zero;\n\tp->prev_utime = cputime_zero;\n\tp->prev_stime = cputime_zero;\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->lock_depth = -1;\t\t/* -1 = no lock */\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n \tif (IS_ERR(p->mempolicy)) {\n \t\tretval = PTR_ERR(p->mempolicy);\n \t\tp->mempolicy = NULL;\n \t\tgoto bad_fork_cleanup_cgroup;\n \t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tp->hardirqs_enabled = 1;\n#else\n\tp->hardirqs_enabled = 0;\n#endif\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n\n\tp->bts = NULL;\n\n\tp->stack_start = stack_start;\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p, clone_flags);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tif ((retval = audit_alloc(p)))\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tif ((retval = copy_semundo(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_audit;\n\tif ((retval = copy_files(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_semundo;\n\tif ((retval = copy_fs(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_files;\n\tif ((retval = copy_sighand(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_fs;\n\tif ((retval = copy_signal(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_sighand;\n\tif ((retval = copy_mm(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_signal;\n\tif ((retval = copy_namespaces(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_mm;\n\tif ((retval = copy_io(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p, regs);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\n\t\tif (clone_flags & CLONE_NEWPID) {\n\t\t\tretval = pid_ns_prepare_proc(p->nsproxy->pid_ns);\n\t\t\tif (retval < 0)\n\t\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tif (current->nsproxy != p->nsproxy) {\n\t\tretval = ns_cgroup_clone(p, pid);\n\t\tif (retval)\n\t\t\tgoto bad_fork_free_pid;\n\t}\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing should be turned off in the child regardless\n\t * of CLONE_PTRACE.\n\t */\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\n\t/* Now that the task is set up, run cgroup callbacks if\n\t * necessary. We need to run them before the task is visible\n\t * on the tasklist. */\n\tcgroup_fork_callbacks(p);\n\tcgroup_callbacks_done = 1;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/*\n\t * The task hasn't been attached yet, so its cpus_allowed mask will\n\t * not be changed, nor will its assigned CPU.\n\t *\n\t * The cpus_allowed mask of the parent may have changed after it was\n\t * copied first time - so re-copy it here, then check the child's CPU\n\t * to ensure it is on a valid CPU (and if not, just force it back to\n\t * parent's CPU). This avoids alot of nasty races.\n\t */\n\tp->cpus_allowed = current->cpus_allowed;\n\tp->rt.nr_cpus_allowed = current->rt.nr_cpus_allowed;\n\tif (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||\n\t\t\t!cpu_online(task_cpu(p))))\n\t\tset_task_cpu(p, smp_processor_id());\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n \t */\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tatomic_inc(&current->signal->count);\n\t\tatomic_inc(&current->signal->live);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\ttracehook_finish_clone(p, clone_flags, trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (clone_flags & CLONE_NEWPID)\n\t\t\t\tp->nsproxy->pid_ns->child_reaper = p;\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\ttty_kref_put(p->signal->tty);\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__get_cpu_var(process_counts)++;\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tperf_event_fork(p);\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tput_io_context(p->io_context);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\t__cleanup_signal(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tcgroup_exit(p, cgroup_callbacks_done);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "func": "static struct task_struct *copy_process(unsigned long clone_flags,\n\t\t\t\t\tunsigned long stack_start,\n\t\t\t\t\tstruct pt_regs *regs,\n\t\t\t\t\tunsigned long stack_size,\n\t\t\t\t\tint __user *child_tidptr,\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace)\n{\n\tint retval;\n\tstruct task_struct *p;\n\tint cgroup_callbacks_done = 0;\n\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tretval = security_task_create(clone_flags);\n\tif (retval)\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current);\n\tif (!p)\n\t\tgoto fork_out;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = -EAGAIN;\n\tif (atomic_read(&p->real_cred->user->processes) >=\n\t\t\tp->signal->rlim[RLIMIT_NPROC].rlim_cur) {\n\t\tif (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&\n\t\t    p->real_cred->user != INIT_USER)\n\t\t\tgoto bad_fork_free;\n\t}\n\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (nr_threads >= max_threads)\n\t\tgoto bad_fork_cleanup_count;\n\n\tif (!try_module_get(task_thread_info(p)->exec_domain->module))\n\t\tgoto bad_fork_cleanup_count;\n\n\tp->did_exec = 0;\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tcopy_flags(clone_flags, p);\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = cputime_zero;\n\tp->stime = cputime_zero;\n\tp->gtime = cputime_zero;\n\tp->utimescaled = cputime_zero;\n\tp->stimescaled = cputime_zero;\n\tp->prev_utime = cputime_zero;\n\tp->prev_stime = cputime_zero;\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cpu_timers_init(p);\n\n\tp->lock_depth = -1;\t\t/* -1 = no lock */\n\tdo_posix_clock_monotonic_gettime(&p->start_time);\n\tp->real_start_time = p->start_time;\n\tmonotonic_to_bootbased(&p->real_start_time);\n\tp->io_context = NULL;\n\tp->audit_context = NULL;\n\tcgroup_fork(p);\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n \tif (IS_ERR(p->mempolicy)) {\n \t\tretval = PTR_ERR(p->mempolicy);\n \t\tp->mempolicy = NULL;\n \t\tgoto bad_fork_cleanup_cgroup;\n \t}\n\tmpol_fix_fork_child_flag(p);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tp->irq_events = 0;\n#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW\n\tp->hardirqs_enabled = 1;\n#else\n\tp->hardirqs_enabled = 0;\n#endif\n\tp->hardirq_enable_ip = 0;\n\tp->hardirq_enable_event = 0;\n\tp->hardirq_disable_ip = _THIS_IP_;\n\tp->hardirq_disable_event = 0;\n\tp->softirqs_enabled = 1;\n\tp->softirq_enable_ip = _THIS_IP_;\n\tp->softirq_enable_event = 0;\n\tp->softirq_disable_ip = 0;\n\tp->softirq_disable_event = 0;\n\tp->hardirq_context = 0;\n\tp->softirq_context = 0;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tp->lockdep_depth = 0; /* no locks held yet */\n\tp->curr_chain_key = 0;\n\tp->lockdep_recursion = 0;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n\n\tp->bts = NULL;\n\n\tp->stack_start = stack_start;\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tsched_fork(p, clone_flags);\n\n\tretval = perf_event_init_task(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tif ((retval = audit_alloc(p)))\n\t\tgoto bad_fork_cleanup_policy;\n\t/* copy all the process information */\n\tif ((retval = copy_semundo(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_audit;\n\tif ((retval = copy_files(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_semundo;\n\tif ((retval = copy_fs(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_files;\n\tif ((retval = copy_sighand(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_fs;\n\tif ((retval = copy_signal(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_sighand;\n\tif ((retval = copy_mm(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_signal;\n\tif ((retval = copy_namespaces(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_mm;\n\tif ((retval = copy_io(clone_flags, p)))\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(clone_flags, stack_start, stack_size, p, regs);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tif (pid != &init_struct_pid) {\n\t\tretval = -ENOMEM;\n\t\tpid = alloc_pid(p->nsproxy->pid_ns);\n\t\tif (!pid)\n\t\t\tgoto bad_fork_cleanup_io;\n\n\t\tif (clone_flags & CLONE_NEWPID) {\n\t\t\tretval = pid_ns_prepare_proc(p->nsproxy->pid_ns);\n\t\t\tif (retval < 0)\n\t\t\t\tgoto bad_fork_free_pid;\n\t\t}\n\t}\n\n\tp->pid = pid_nr(pid);\n\tp->tgid = p->pid;\n\tif (clone_flags & CLONE_THREAD)\n\t\tp->tgid = current->tgid;\n\n\tif (current->nsproxy != p->nsproxy) {\n\t\tretval = ns_cgroup_clone(p, pid);\n\t\tif (retval)\n\t\t\tgoto bad_fork_free_pid;\n\t}\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;\n#ifdef CONFIG_FUTEX\n\tp->robust_list = NULL;\n#ifdef CONFIG_COMPAT\n\tp->compat_robust_list = NULL;\n#endif\n\tINIT_LIST_HEAD(&p->pi_state_list);\n\tp->pi_state_cache = NULL;\n#endif\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tp->sas_ss_sp = p->sas_ss_size = 0;\n\n\t/*\n\t * Syscall tracing should be turned off in the child regardless\n\t * of CLONE_PTRACE.\n\t */\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);\n#ifdef TIF_SYSCALL_EMU\n\tclear_tsk_thread_flag(p, TIF_SYSCALL_EMU);\n#endif\n\tclear_all_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);\n\tp->pdeath_signal = 0;\n\tp->exit_state = 0;\n\n\t/*\n\t * Ok, make it visible to the rest of the system.\n\t * We dont wake it up yet.\n\t */\n\tp->group_leader = p;\n\tINIT_LIST_HEAD(&p->thread_group);\n\n\t/* Now that the task is set up, run cgroup callbacks if\n\t * necessary. We need to run them before the task is visible\n\t * on the tasklist. */\n\tcgroup_fork_callbacks(p);\n\tcgroup_callbacks_done = 1;\n\n\t/* Need tasklist lock for parent etc handling! */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/*\n\t * The task hasn't been attached yet, so its cpus_allowed mask will\n\t * not be changed, nor will its assigned CPU.\n\t *\n\t * The cpus_allowed mask of the parent may have changed after it was\n\t * copied first time - so re-copy it here, then check the child's CPU\n\t * to ensure it is on a valid CPU (and if not, just force it back to\n\t * parent's CPU). This avoids alot of nasty races.\n\t */\n\tp->cpus_allowed = current->cpus_allowed;\n\tp->rt.nr_cpus_allowed = current->rt.nr_cpus_allowed;\n\tif (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||\n\t\t\t!cpu_online(task_cpu(p))))\n\t\tset_task_cpu(p, smp_processor_id());\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tspin_lock(&current->sighand->siglock);\n\n\t/*\n\t * Process group and session signals need to be delivered to just the\n\t * parent before the fork or both the parent and the child after the\n\t * fork. Restart if a signal comes in before we add the new process to\n\t * it's process group.\n\t * A fatal signal pending means that current will exit, so the new\n\t * thread can't slip out of an OOM kill (or normal SIGKILL).\n \t */\n\trecalc_sigpending();\n\tif (signal_pending(current)) {\n\t\tspin_unlock(&current->sighand->siglock);\n\t\twrite_unlock_irq(&tasklist_lock);\n\t\tretval = -ERESTARTNOINTR;\n\t\tgoto bad_fork_free_pid;\n\t}\n\n\tif (clone_flags & CLONE_THREAD) {\n\t\tatomic_inc(&current->signal->count);\n\t\tatomic_inc(&current->signal->live);\n\t\tp->group_leader = current->group_leader;\n\t\tlist_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);\n\t}\n\n\tif (likely(p->pid)) {\n\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\ttracehook_finish_clone(p, clone_flags, trace);\n\n\t\tif (thread_group_leader(p)) {\n\t\t\tif (clone_flags & CLONE_NEWPID)\n\t\t\t\tp->nsproxy->pid_ns->child_reaper = p;\n\n\t\t\tp->signal->leader_pid = pid;\n\t\t\ttty_kref_put(p->signal->tty);\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\tattach_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tattach_pid(p, PIDTYPE_SID, task_session(current));\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\t__get_cpu_var(process_counts)++;\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID, pid);\n\t\tnr_threads++;\n\t}\n\n\ttotal_forks++;\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\n\tproc_fork_connector(p);\n\tcgroup_post_fork(p);\n\tperf_event_fork(p);\n\treturn p;\n\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm)\n\t\tmmput(p->mm);\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\t__cleanup_signal(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_policy:\n\tperf_event_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\nbad_fork_cleanup_cgroup:\n#endif\n\tcgroup_exit(p, cgroup_callbacks_done);\n\tdelayacct_tsk_free(p);\n\tmodule_put(task_thread_info(p)->exec_domain->module);\nbad_fork_cleanup_count:\n\tatomic_dec(&p->cred->user->processes);\n\texit_creds(p);\nbad_fork_free:\n\tfree_task(p);\nfork_out:\n\treturn ERR_PTR(retval);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -339,7 +339,8 @@\n \tif (pid != &init_struct_pid)\n \t\tfree_pid(pid);\n bad_fork_cleanup_io:\n-\tput_io_context(p->io_context);\n+\tif (p->io_context)\n+\t\texit_io_context(p);\n bad_fork_cleanup_namespaces:\n \texit_task_namespaces(p);\n bad_fork_cleanup_mm:",
        "diff_line_info": {
            "deleted_lines": [
                "\tput_io_context(p->io_context);"
            ],
            "added_lines": [
                "\tif (p->io_context)",
                "\t\texit_io_context(p);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0879",
        "func_name": "torvalds/linux/exit_io_context",
        "description": "The I/O implementation for block devices in the Linux kernel before 2.6.33 does not properly handle the CLONE_IO feature, which allows local users to cause a denial of service (I/O instability) by starting multiple processes that share an I/O context.",
        "git_url": "https://github.com/torvalds/linux/commit/b69f2292063d2caf37ca9aec7d63ded203701bf3",
        "commit_title": "block: Fix io_context leak after failure of clone with CLONE_IO",
        "commit_text": " With CLONE_IO, parent's io_context->nr_tasks is incremented, but never decremented whenever copy_process() fails afterwards, which prevents exit_io_context() from calling IO schedulers exit functions.  Give a task_struct to exit_io_context(), and call exit_io_context() instead of put_io_context() in copy_process() cleanup path. ",
        "func_before": "void exit_io_context(void)\n{\n\tstruct io_context *ioc;\n\n\ttask_lock(current);\n\tioc = current->io_context;\n\tcurrent->io_context = NULL;\n\ttask_unlock(current);\n\n\tif (atomic_dec_and_test(&ioc->nr_tasks)) {\n\t\tif (ioc->aic && ioc->aic->exit)\n\t\t\tioc->aic->exit(ioc->aic);\n\t\tcfq_exit(ioc);\n\n\t}\n\tput_io_context(ioc);\n}",
        "func": "void exit_io_context(struct task_struct *task)\n{\n\tstruct io_context *ioc;\n\n\ttask_lock(task);\n\tioc = task->io_context;\n\ttask->io_context = NULL;\n\ttask_unlock(task);\n\n\tif (atomic_dec_and_test(&ioc->nr_tasks)) {\n\t\tif (ioc->aic && ioc->aic->exit)\n\t\t\tioc->aic->exit(ioc->aic);\n\t\tcfq_exit(ioc);\n\n\t}\n\tput_io_context(ioc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,11 @@\n-void exit_io_context(void)\n+void exit_io_context(struct task_struct *task)\n {\n \tstruct io_context *ioc;\n \n-\ttask_lock(current);\n-\tioc = current->io_context;\n-\tcurrent->io_context = NULL;\n-\ttask_unlock(current);\n+\ttask_lock(task);\n+\tioc = task->io_context;\n+\ttask->io_context = NULL;\n+\ttask_unlock(task);\n \n \tif (atomic_dec_and_test(&ioc->nr_tasks)) {\n \t\tif (ioc->aic && ioc->aic->exit)",
        "diff_line_info": {
            "deleted_lines": [
                "void exit_io_context(void)",
                "\ttask_lock(current);",
                "\tioc = current->io_context;",
                "\tcurrent->io_context = NULL;",
                "\ttask_unlock(current);"
            ],
            "added_lines": [
                "void exit_io_context(struct task_struct *task)",
                "\ttask_lock(task);",
                "\tioc = task->io_context;",
                "\ttask->io_context = NULL;",
                "\ttask_unlock(task);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0879",
        "func_name": "torvalds/linux/do_exit",
        "description": "The I/O implementation for block devices in the Linux kernel before 2.6.33 does not properly handle the CLONE_IO feature, which allows local users to cause a denial of service (I/O instability) by starting multiple processes that share an I/O context.",
        "git_url": "https://github.com/torvalds/linux/commit/b69f2292063d2caf37ca9aec7d63ded203701bf3",
        "commit_title": "block: Fix io_context leak after failure of clone with CLONE_IO",
        "commit_text": " With CLONE_IO, parent's io_context->nr_tasks is incremented, but never decremented whenever copy_process() fails afterwards, which prevents exit_io_context() from calling IO schedulers exit functions.  Give a task_struct to exit_io_context(), and call exit_io_context() instead of put_io_context() in copy_process() cleanup path. ",
        "func_before": "NORET_TYPE void do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\tprofile_task_exit(tsk);\n\n\tWARN_ON(atomic_read(&tsk->fs_excl));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\ttracehook_report_exit(&code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tprintk(KERN_ALERT\n\t\t\t\"Fixing recursive fault but reboot is needed!\\n\");\n\t\t/*\n\t\t * We can do this unlocked here. The futex code uses\n\t\t * this flag just to verify whether the pi state\n\t\t * cleanup has been done or not. In the worst case it\n\t\t * loops once more. We pretend that the cleanup was\n\t\t * done as there is no way to return. Either the\n\t\t * OWNER_DIED bit is set by now or we push the blocked\n\t\t * task into the wait for ever nirwana as well.\n\t\t */\n\t\ttsk->flags |= PF_EXITPIDONE;\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\texit_irq_thread();\n\n\texit_signals(tsk);  /* sets PF_EXITING */\n\t/*\n\t * tsk->flags are checked in the futex code to protect against\n\t * an exiting task cleaning up the robust pi futexes.\n\t */\n\tsmp_mb();\n\tspin_unlock_wait(&tsk->pi_lock);\n\n\tif (unlikely(in_atomic()))\n\t\tprintk(KERN_INFO \"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t\tpreempt_count());\n\n\tacct_update_integrals(tsk);\n\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk->signal);\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\tif (unlikely(tsk->audit_context))\n\t\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm(tsk);\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tcheck_stack_usage();\n\texit_thread();\n\tcgroup_exit(tsk, 1);\n\n\tif (group_dead && tsk->signal->leader)\n\t\tdisassociate_ctty(1);\n\n\tmodule_put(task_thread_info(tsk)->exec_domain->module);\n\n\tproc_exit_connector(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t */\n\tperf_event_exit_task(tsk);\n\n\texit_notify(tsk, group_dead);\n#ifdef CONFIG_NUMA\n\tmpol_put(tsk->mempolicy);\n\ttsk->mempolicy = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held(tsk);\n\t/*\n\t * We can do this unlocked here. The futex code uses this flag\n\t * just to verify whether the pi state cleanup has been done\n\t * or not. In the worst case it loops once more.\n\t */\n\ttsk->flags |= PF_EXITPIDONE;\n\n\tif (tsk->io_context)\n\t\texit_io_context();\n\n\tif (tsk->splice_pipe)\n\t\t__free_pipe_info(tsk->splice_pipe);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tpreempt_disable();\n\texit_rcu();\n\t/* causes final put_task_struct in finish_task_switch(). */\n\ttsk->state = TASK_DEAD;\n\tschedule();\n\tBUG();\n\t/* Avoid \"noreturn function does return\".  */\n\tfor (;;)\n\t\tcpu_relax();\t/* For when BUG is null */\n}",
        "func": "NORET_TYPE void do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\tprofile_task_exit(tsk);\n\n\tWARN_ON(atomic_read(&tsk->fs_excl));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\ttracehook_report_exit(&code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tprintk(KERN_ALERT\n\t\t\t\"Fixing recursive fault but reboot is needed!\\n\");\n\t\t/*\n\t\t * We can do this unlocked here. The futex code uses\n\t\t * this flag just to verify whether the pi state\n\t\t * cleanup has been done or not. In the worst case it\n\t\t * loops once more. We pretend that the cleanup was\n\t\t * done as there is no way to return. Either the\n\t\t * OWNER_DIED bit is set by now or we push the blocked\n\t\t * task into the wait for ever nirwana as well.\n\t\t */\n\t\ttsk->flags |= PF_EXITPIDONE;\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\texit_irq_thread();\n\n\texit_signals(tsk);  /* sets PF_EXITING */\n\t/*\n\t * tsk->flags are checked in the futex code to protect against\n\t * an exiting task cleaning up the robust pi futexes.\n\t */\n\tsmp_mb();\n\tspin_unlock_wait(&tsk->pi_lock);\n\n\tif (unlikely(in_atomic()))\n\t\tprintk(KERN_INFO \"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t\tpreempt_count());\n\n\tacct_update_integrals(tsk);\n\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk->signal);\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\tif (unlikely(tsk->audit_context))\n\t\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm(tsk);\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tcheck_stack_usage();\n\texit_thread();\n\tcgroup_exit(tsk, 1);\n\n\tif (group_dead && tsk->signal->leader)\n\t\tdisassociate_ctty(1);\n\n\tmodule_put(task_thread_info(tsk)->exec_domain->module);\n\n\tproc_exit_connector(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t */\n\tperf_event_exit_task(tsk);\n\n\texit_notify(tsk, group_dead);\n#ifdef CONFIG_NUMA\n\tmpol_put(tsk->mempolicy);\n\ttsk->mempolicy = NULL;\n#endif\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held(tsk);\n\t/*\n\t * We can do this unlocked here. The futex code uses this flag\n\t * just to verify whether the pi state cleanup has been done\n\t * or not. In the worst case it loops once more.\n\t */\n\ttsk->flags |= PF_EXITPIDONE;\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\t__free_pipe_info(tsk->splice_pipe);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tpreempt_disable();\n\texit_rcu();\n\t/* causes final put_task_struct in finish_task_switch(). */\n\ttsk->state = TASK_DEAD;\n\tschedule();\n\tBUG();\n\t/* Avoid \"noreturn function does return\".  */\n\tfor (;;)\n\t\tcpu_relax();\t/* For when BUG is null */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -117,7 +117,7 @@\n \ttsk->flags |= PF_EXITPIDONE;\n \n \tif (tsk->io_context)\n-\t\texit_io_context();\n+\t\texit_io_context(tsk);\n \n \tif (tsk->splice_pipe)\n \t\t__free_pipe_info(tsk->splice_pipe);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\texit_io_context();"
            ],
            "added_lines": [
                "\t\texit_io_context(tsk);"
            ]
        }
    },
    {
        "cve_id": "CVE-2012-0879",
        "func_name": "torvalds/linux/exit_io_context",
        "description": "The I/O implementation for block devices in the Linux kernel before 2.6.33 does not properly handle the CLONE_IO feature, which allows local users to cause a denial of service (I/O instability) by starting multiple processes that share an I/O context.",
        "git_url": "https://github.com/torvalds/linux/commit/61cc74fbb87af6aa551a06a370590c9bc07e29d9",
        "commit_title": "block: Fix io_context leak after clone with CLONE_IO",
        "commit_text": " With CLONE_IO, copy_io() increments both ioc->refcount and ioc->nr_tasks. However exit_io_context() only decrements ioc->refcount if ioc->nr_tasks reaches 0.  Always call put_io_context() in exit_io_context(). ",
        "func_before": "void exit_io_context(void)\n{\n\tstruct io_context *ioc;\n\n\ttask_lock(current);\n\tioc = current->io_context;\n\tcurrent->io_context = NULL;\n\ttask_unlock(current);\n\n\tif (atomic_dec_and_test(&ioc->nr_tasks)) {\n\t\tif (ioc->aic && ioc->aic->exit)\n\t\t\tioc->aic->exit(ioc->aic);\n\t\tcfq_exit(ioc);\n\n\t\tput_io_context(ioc);\n\t}\n}",
        "func": "void exit_io_context(void)\n{\n\tstruct io_context *ioc;\n\n\ttask_lock(current);\n\tioc = current->io_context;\n\tcurrent->io_context = NULL;\n\ttask_unlock(current);\n\n\tif (atomic_dec_and_test(&ioc->nr_tasks)) {\n\t\tif (ioc->aic && ioc->aic->exit)\n\t\t\tioc->aic->exit(ioc->aic);\n\t\tcfq_exit(ioc);\n\n\t}\n\tput_io_context(ioc);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,6 @@\n \t\t\tioc->aic->exit(ioc->aic);\n \t\tcfq_exit(ioc);\n \n-\t\tput_io_context(ioc);\n \t}\n+\tput_io_context(ioc);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tput_io_context(ioc);"
            ],
            "added_lines": [
                "\tput_io_context(ioc);"
            ]
        }
    }
]