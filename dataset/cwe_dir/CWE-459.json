[
    {
        "cve_id": "CVE-2019-17420",
        "func_name": "OISF/libhtp/htp_connp_RES_HEADERS",
        "description": "In OISF LibHTP before 0.5.31, as used in Suricata 4.1.4 and other products, an HTTP protocol parsing error causes the http_header signature to not alert on a response with a single \\r\\n ending.",
        "git_url": "https://github.com/OISF/libhtp/commit/a7729b41553fe02a13befd8d6152da5934b0ed31",
        "commit_title": "response: finalize when closing during headers",
        "commit_text": " Fixes #2969",
        "func_before": "htp_status_t htp_connp_RES_HEADERS(htp_connp_t *connp) {\n    for (;;) {\n        OUT_COPY_BYTE_OR_RETURN(connp);\n\n        // Have we reached the end of the line?\n        if (connp->out_next_byte == LF || connp->out_next_byte == CR) {\n\n            if (connp->out_next_byte == CR) {\n                OUT_PEEK_NEXT(connp);\n                if (connp->out_next_byte == -1) {\n                    return HTP_DATA_BUFFER;\n                } else if (connp->out_next_byte == LF) {\n                    OUT_COPY_BYTE_OR_RETURN(connp);\n                }\n            }\n\n            unsigned char *data;\n            size_t len;\n\n            if (htp_connp_res_consolidate_data(connp, &data, &len) != HTP_OK) {\n                return HTP_ERROR;\n            }\n\n            #ifdef HTP_DEBUG\n            fprint_raw_data(stderr, __func__, data, len);\n            #endif\n\n            // Should we terminate headers?\n            if (htp_connp_is_line_terminator(connp, data, len)) {\n                // Parse previous header, if any.\n                if (connp->out_header != NULL) {\n                    if (connp->cfg->process_response_header(connp, bstr_ptr(connp->out_header),\n                            bstr_len(connp->out_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->out_header);\n                    connp->out_header = NULL;\n                }\n\n                htp_connp_res_clear_buffer(connp);\n\n                // We've seen all response headers.\n                if (connp->out_tx->response_progress == HTP_RESPONSE_HEADERS) {\n                    // Response headers.\n\n                    // The next step is to determine if this response has a body.\n                    connp->out_state = htp_connp_RES_BODY_DETERMINE;\n                } else {\n                    // Response trailer.\n\n                    // Finalize sending raw trailer data.\n                    htp_status_t rc = htp_connp_res_receiver_finalize_clear(connp);\n                    if (rc != HTP_OK) return rc;\n\n                    // Run hook response_TRAILER.\n                    rc = htp_hook_run_all(connp->cfg->hook_response_trailer, connp->out_tx);\n                    if (rc != HTP_OK) return rc;\n\n                    // The next step is to finalize this response.\n                    connp->out_state = htp_connp_RES_FINALIZE;\n                }\n\n                return HTP_OK;\n            }\n\n            htp_chomp(data, &len);\n\n            // Check for header folding.\n            if (htp_connp_is_line_folded(data, len) == 0) {\n                // New header line.\n\n                // Parse previous header, if any.\n                if (connp->out_header != NULL) {\n                    if (connp->cfg->process_response_header(connp, bstr_ptr(connp->out_header),\n                            bstr_len(connp->out_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->out_header);\n                    connp->out_header = NULL;\n                }\n\n                OUT_PEEK_NEXT(connp);\n\n                if (htp_is_folding_char(connp->out_next_byte) == 0) {\n                    // Because we know this header is not folded, we can process the buffer straight away.\n                    if (connp->cfg->process_response_header(connp, data, len) != HTP_OK) return HTP_ERROR;\n                } else {\n                    // Keep the partial header data for parsing later.\n                    connp->out_header = bstr_dup_mem(data, len);\n                    if (connp->out_header == NULL) return HTP_ERROR;\n                }\n            } else {\n                // Folding; check that there's a previous header line to add to.\n                if (connp->out_header == NULL) {\n                    // Invalid folding.\n\n                    // Warn only once per transaction.\n                    if (!(connp->out_tx->flags & HTP_INVALID_FOLDING)) {\n                        connp->out_tx->flags |= HTP_INVALID_FOLDING;\n                        htp_log(connp, HTP_LOG_MARK, HTP_LOG_WARNING, 0, \"Invalid response field folding\");\n                    }\n\n                    // Keep the header data for parsing later.\n                    connp->out_header = bstr_dup_mem(data, len);\n                    if (connp->out_header == NULL) return HTP_ERROR;\n                } else {\n                    // Add to the existing header.                    \n                    bstr *new_out_header = bstr_add_mem(connp->out_header, data, len);\n                    if (new_out_header == NULL) return HTP_ERROR;\n                    connp->out_header = new_out_header;\n                }\n            }\n\n            htp_connp_res_clear_buffer(connp);\n        }\n    }\n\n    return HTP_ERROR;\n}",
        "func": "htp_status_t htp_connp_RES_HEADERS(htp_connp_t *connp) {\n    for (;;) {\n        if (connp->out_status == HTP_STREAM_CLOSED) {\n            // Finalize sending raw trailer data.\n            htp_status_t rc = htp_connp_res_receiver_finalize_clear(connp);\n            if (rc != HTP_OK) return rc;\n\n            // Run hook response_TRAILER.\n            rc = htp_hook_run_all(connp->cfg->hook_response_trailer, connp->out_tx);\n            if (rc != HTP_OK) return rc;\n\n            connp->out_state = htp_connp_RES_FINALIZE;\n            return HTP_OK;\n        }\n        OUT_COPY_BYTE_OR_RETURN(connp);\n\n        // Have we reached the end of the line?\n        if (connp->out_next_byte == LF || connp->out_next_byte == CR) {\n\n            if (connp->out_next_byte == CR) {\n                OUT_PEEK_NEXT(connp);\n                if (connp->out_next_byte == -1) {\n                    return HTP_DATA_BUFFER;\n                } else if (connp->out_next_byte == LF) {\n                    OUT_COPY_BYTE_OR_RETURN(connp);\n                }\n            }\n\n            unsigned char *data;\n            size_t len;\n\n            if (htp_connp_res_consolidate_data(connp, &data, &len) != HTP_OK) {\n                return HTP_ERROR;\n            }\n\n            #ifdef HTP_DEBUG\n            fprint_raw_data(stderr, __func__, data, len);\n            #endif\n\n            // Should we terminate headers?\n            if (htp_connp_is_line_terminator(connp, data, len)) {\n                // Parse previous header, if any.\n                if (connp->out_header != NULL) {\n                    if (connp->cfg->process_response_header(connp, bstr_ptr(connp->out_header),\n                            bstr_len(connp->out_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->out_header);\n                    connp->out_header = NULL;\n                }\n\n                htp_connp_res_clear_buffer(connp);\n\n                // We've seen all response headers.\n                if (connp->out_tx->response_progress == HTP_RESPONSE_HEADERS) {\n                    // Response headers.\n\n                    // The next step is to determine if this response has a body.\n                    connp->out_state = htp_connp_RES_BODY_DETERMINE;\n                } else {\n                    // Response trailer.\n\n                    // Finalize sending raw trailer data.\n                    htp_status_t rc = htp_connp_res_receiver_finalize_clear(connp);\n                    if (rc != HTP_OK) return rc;\n\n                    // Run hook response_TRAILER.\n                    rc = htp_hook_run_all(connp->cfg->hook_response_trailer, connp->out_tx);\n                    if (rc != HTP_OK) return rc;\n\n                    // The next step is to finalize this response.\n                    connp->out_state = htp_connp_RES_FINALIZE;\n                }\n\n                return HTP_OK;\n            }\n\n            htp_chomp(data, &len);\n\n            // Check for header folding.\n            if (htp_connp_is_line_folded(data, len) == 0) {\n                // New header line.\n\n                // Parse previous header, if any.\n                if (connp->out_header != NULL) {\n                    if (connp->cfg->process_response_header(connp, bstr_ptr(connp->out_header),\n                            bstr_len(connp->out_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->out_header);\n                    connp->out_header = NULL;\n                }\n\n                OUT_PEEK_NEXT(connp);\n\n                if (htp_is_folding_char(connp->out_next_byte) == 0) {\n                    // Because we know this header is not folded, we can process the buffer straight away.\n                    if (connp->cfg->process_response_header(connp, data, len) != HTP_OK) return HTP_ERROR;\n                } else {\n                    // Keep the partial header data for parsing later.\n                    connp->out_header = bstr_dup_mem(data, len);\n                    if (connp->out_header == NULL) return HTP_ERROR;\n                }\n            } else {\n                // Folding; check that there's a previous header line to add to.\n                if (connp->out_header == NULL) {\n                    // Invalid folding.\n\n                    // Warn only once per transaction.\n                    if (!(connp->out_tx->flags & HTP_INVALID_FOLDING)) {\n                        connp->out_tx->flags |= HTP_INVALID_FOLDING;\n                        htp_log(connp, HTP_LOG_MARK, HTP_LOG_WARNING, 0, \"Invalid response field folding\");\n                    }\n\n                    // Keep the header data for parsing later.\n                    connp->out_header = bstr_dup_mem(data, len);\n                    if (connp->out_header == NULL) return HTP_ERROR;\n                } else {\n                    // Add to the existing header.                    \n                    bstr *new_out_header = bstr_add_mem(connp->out_header, data, len);\n                    if (new_out_header == NULL) return HTP_ERROR;\n                    connp->out_header = new_out_header;\n                }\n            }\n\n            htp_connp_res_clear_buffer(connp);\n        }\n    }\n\n    return HTP_ERROR;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,17 @@\n htp_status_t htp_connp_RES_HEADERS(htp_connp_t *connp) {\n     for (;;) {\n+        if (connp->out_status == HTP_STREAM_CLOSED) {\n+            // Finalize sending raw trailer data.\n+            htp_status_t rc = htp_connp_res_receiver_finalize_clear(connp);\n+            if (rc != HTP_OK) return rc;\n+\n+            // Run hook response_TRAILER.\n+            rc = htp_hook_run_all(connp->cfg->hook_response_trailer, connp->out_tx);\n+            if (rc != HTP_OK) return rc;\n+\n+            connp->out_state = htp_connp_RES_FINALIZE;\n+            return HTP_OK;\n+        }\n         OUT_COPY_BYTE_OR_RETURN(connp);\n \n         // Have we reached the end of the line?",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        if (connp->out_status == HTP_STREAM_CLOSED) {",
                "            // Finalize sending raw trailer data.",
                "            htp_status_t rc = htp_connp_res_receiver_finalize_clear(connp);",
                "            if (rc != HTP_OK) return rc;",
                "",
                "            // Run hook response_TRAILER.",
                "            rc = htp_hook_run_all(connp->cfg->hook_response_trailer, connp->out_tx);",
                "            if (rc != HTP_OK) return rc;",
                "",
                "            connp->out_state = htp_connp_RES_FINALIZE;",
                "            return HTP_OK;",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17420",
        "func_name": "OISF/libhtp/htp_connp_REQ_HEADERS",
        "description": "In OISF LibHTP before 0.5.31, as used in Suricata 4.1.4 and other products, an HTTP protocol parsing error causes the http_header signature to not alert on a response with a single \\r\\n ending.",
        "git_url": "https://github.com/OISF/libhtp/commit/67df3c240e4a825847cf83f1dd7328f1739b90d9",
        "commit_title": "request: finalize when closing during headers",
        "commit_text": "",
        "func_before": "htp_status_t htp_connp_REQ_HEADERS(htp_connp_t *connp) {\n    for (;;) {\n        IN_COPY_BYTE_OR_RETURN(connp);\n\n        // Have we reached the end of the line?\n        if (connp->in_next_byte == LF) {\n            unsigned char *data;\n            size_t len;\n\n            if (htp_connp_req_consolidate_data(connp, &data, &len) != HTP_OK) {\n                return HTP_ERROR;\n            }\n\n            #ifdef HTP_DEBUG\n            fprint_raw_data(stderr, __func__, data, len);\n            #endif           \n\n            // Should we terminate headers?\n            if (htp_connp_is_line_terminator(connp, data, len)) {\n                // Parse previous header, if any.\n                if (connp->in_header != NULL) {\n                    if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n                            bstr_len(connp->in_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->in_header);\n                    connp->in_header = NULL;\n                }\n\n                htp_connp_req_clear_buffer(connp);\n\n                // We've seen all the request headers.\n                return htp_tx_state_request_headers(connp->in_tx);\n            }\n\n            htp_chomp(data, &len);\n\n            // Check for header folding.\n            if (htp_connp_is_line_folded(data, len) == 0) {\n                // New header line.\n\n                // Parse previous header, if any.\n                if (connp->in_header != NULL) {\n                    if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n                            bstr_len(connp->in_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->in_header);\n                    connp->in_header = NULL;\n                }\n\n                IN_PEEK_NEXT(connp);\n\n                if (connp->in_next_byte != -1 && htp_is_folding_char(connp->in_next_byte) == 0) {\n                    // Because we know this header is not folded, we can process the buffer straight away.\n                    if (connp->cfg->process_request_header(connp, data, len) != HTP_OK) return HTP_ERROR;\n                } else {\n                    // Keep the partial header data for parsing later.\n                    connp->in_header = bstr_dup_mem(data, len);\n                    if (connp->in_header == NULL) return HTP_ERROR;\n                }\n            } else {\n                // Folding; check that there's a previous header line to add to.\n                if (connp->in_header == NULL) {\n                    // Invalid folding.\n\n                    // Warn only once per transaction.\n                    if (!(connp->in_tx->flags & HTP_INVALID_FOLDING)) {\n                        connp->in_tx->flags |= HTP_INVALID_FOLDING;\n                        htp_log(connp, HTP_LOG_MARK, HTP_LOG_WARNING, 0, \"Invalid request field folding\");\n                    }\n\n                    // Keep the header data for parsing later.\n                    connp->in_header = bstr_dup_mem(data, len);\n                    if (connp->in_header == NULL) return HTP_ERROR;\n                } else {\n                    // Add to the existing header.                    \n                    bstr *new_in_header = bstr_add_mem(connp->in_header, data, len);\n                    if (new_in_header == NULL) return HTP_ERROR;\n                    connp->in_header = new_in_header;\n                }\n            }\n\n            htp_connp_req_clear_buffer(connp);\n        }\n    }\n\n    return HTP_ERROR;\n}",
        "func": "htp_status_t htp_connp_REQ_HEADERS(htp_connp_t *connp) {\n    for (;;) {\n        if (connp->in_status == HTP_STREAM_CLOSED) {\n            // Parse previous header, if any.\n            if (connp->in_header != NULL) {\n                if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n                                                       bstr_len(connp->in_header)) != HTP_OK)\n                    return HTP_ERROR;\n                bstr_free(connp->in_header);\n                connp->in_header = NULL;\n            }\n\n            htp_connp_req_clear_buffer(connp);\n\n            connp->in_tx->request_progress = HTP_REQUEST_TRAILER;\n\n            // We've seen all the request headers.\n            return htp_tx_state_request_headers(connp->in_tx);\n        }\n        IN_COPY_BYTE_OR_RETURN(connp);\n\n        // Have we reached the end of the line?\n        if (connp->in_next_byte == LF) {\n            unsigned char *data;\n            size_t len;\n\n            if (htp_connp_req_consolidate_data(connp, &data, &len) != HTP_OK) {\n                return HTP_ERROR;\n            }\n\n            #ifdef HTP_DEBUG\n            fprint_raw_data(stderr, __func__, data, len);\n            #endif           \n\n            // Should we terminate headers?\n            if (htp_connp_is_line_terminator(connp, data, len)) {\n                // Parse previous header, if any.\n                if (connp->in_header != NULL) {\n                    if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n                            bstr_len(connp->in_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->in_header);\n                    connp->in_header = NULL;\n                }\n\n                htp_connp_req_clear_buffer(connp);\n\n                // We've seen all the request headers.\n                return htp_tx_state_request_headers(connp->in_tx);\n            }\n\n            htp_chomp(data, &len);\n\n            // Check for header folding.\n            if (htp_connp_is_line_folded(data, len) == 0) {\n                // New header line.\n\n                // Parse previous header, if any.\n                if (connp->in_header != NULL) {\n                    if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n                            bstr_len(connp->in_header)) != HTP_OK) return HTP_ERROR;\n\n                    bstr_free(connp->in_header);\n                    connp->in_header = NULL;\n                }\n\n                IN_PEEK_NEXT(connp);\n\n                if (connp->in_next_byte != -1 && htp_is_folding_char(connp->in_next_byte) == 0) {\n                    // Because we know this header is not folded, we can process the buffer straight away.\n                    if (connp->cfg->process_request_header(connp, data, len) != HTP_OK) return HTP_ERROR;\n                } else {\n                    // Keep the partial header data for parsing later.\n                    connp->in_header = bstr_dup_mem(data, len);\n                    if (connp->in_header == NULL) return HTP_ERROR;\n                }\n            } else {\n                // Folding; check that there's a previous header line to add to.\n                if (connp->in_header == NULL) {\n                    // Invalid folding.\n\n                    // Warn only once per transaction.\n                    if (!(connp->in_tx->flags & HTP_INVALID_FOLDING)) {\n                        connp->in_tx->flags |= HTP_INVALID_FOLDING;\n                        htp_log(connp, HTP_LOG_MARK, HTP_LOG_WARNING, 0, \"Invalid request field folding\");\n                    }\n\n                    // Keep the header data for parsing later.\n                    connp->in_header = bstr_dup_mem(data, len);\n                    if (connp->in_header == NULL) return HTP_ERROR;\n                } else {\n                    // Add to the existing header.                    \n                    bstr *new_in_header = bstr_add_mem(connp->in_header, data, len);\n                    if (new_in_header == NULL) return HTP_ERROR;\n                    connp->in_header = new_in_header;\n                }\n            }\n\n            htp_connp_req_clear_buffer(connp);\n        }\n    }\n\n    return HTP_ERROR;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,22 @@\n htp_status_t htp_connp_REQ_HEADERS(htp_connp_t *connp) {\n     for (;;) {\n+        if (connp->in_status == HTP_STREAM_CLOSED) {\n+            // Parse previous header, if any.\n+            if (connp->in_header != NULL) {\n+                if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),\n+                                                       bstr_len(connp->in_header)) != HTP_OK)\n+                    return HTP_ERROR;\n+                bstr_free(connp->in_header);\n+                connp->in_header = NULL;\n+            }\n+\n+            htp_connp_req_clear_buffer(connp);\n+\n+            connp->in_tx->request_progress = HTP_REQUEST_TRAILER;\n+\n+            // We've seen all the request headers.\n+            return htp_tx_state_request_headers(connp->in_tx);\n+        }\n         IN_COPY_BYTE_OR_RETURN(connp);\n \n         // Have we reached the end of the line?",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        if (connp->in_status == HTP_STREAM_CLOSED) {",
                "            // Parse previous header, if any.",
                "            if (connp->in_header != NULL) {",
                "                if (connp->cfg->process_request_header(connp, bstr_ptr(connp->in_header),",
                "                                                       bstr_len(connp->in_header)) != HTP_OK)",
                "                    return HTP_ERROR;",
                "                bstr_free(connp->in_header);",
                "                connp->in_header = NULL;",
                "            }",
                "",
                "            htp_connp_req_clear_buffer(connp);",
                "",
                "            connp->in_tx->request_progress = HTP_REQUEST_TRAILER;",
                "",
                "            // We've seen all the request headers.",
                "            return htp_tx_state_request_headers(connp->in_tx);",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-42310",
        "func_name": "xen-project/xen/write_node",
        "description": "Xenstore: Guests can create orphaned Xenstore nodes By creating multiple nodes inside a transaction resulting in an error, a malicious guest can create orphaned nodes in the Xenstore data base, as the cleanup after the error will not remove all nodes already created. When the transaction is committed after this situation, nodes without a valid parent can be made permanent in the data base.",
        "git_url": "https://github.com/xen-project/xen/commit/5d71766bd1a4a3a8b2fe952ca2be80e02fe48f34",
        "commit_title": "tools/xenstore: Fail a transaction if it is not possible to create a node",
        "commit_text": " Commit f2bebf72c4d5 \"xenstore: rework of transaction handling\" moved out from copying the entire database everytime a new transaction is opened to track the list of nodes changed.  The content of all the nodes accessed during a transaction will be temporarily stored in TDB using a different key.  The function create_node() may write/update multiple nodes if the child doesn't exist. In case of a failure, the function will revert any changes (this include any update to TDB). Unfortunately, the function which reverts the changes (i.e. destroy_node()) will not use the correct key to delete any update or even request the transaction to fail.  This means that if a client decide to go ahead with committing the transaction, orphan nodes will be created because they were not linked to an existing node (create_node() will write the nodes backwards).  Once some nodes have been partially updated in a transaction, it is not easily possible to undo any changes. So rather than continuing and hit weird issue while committing, it is much saner to fail the transaction.  This will have an impact on any client that decides to commit even if it can't write a node. Although, it is not clear why a normal client would want to do that...  Lastly, update destroy_node() to use the correct key for deleting the node. Rather than recreating it (this will allocate memory and therefore fail), stash the key in the structure node.  This is XSA-415 / CVE-2022-42310. ",
        "func_before": "static int write_node(struct connection *conn, struct node *node,\n\t\t      bool no_quota_check)\n{\n\tTDB_DATA key;\n\n\tif (access_node(conn, node, NODE_ACCESS_WRITE, &key))\n\t\treturn errno;\n\n\treturn write_node_raw(conn, &key, node, no_quota_check);\n}",
        "func": "static int write_node(struct connection *conn, struct node *node,\n\t\t      bool no_quota_check)\n{\n\tif (access_node(conn, node, NODE_ACCESS_WRITE, &node->key))\n\t\treturn errno;\n\n\treturn write_node_raw(conn, &node->key, node, no_quota_check);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,8 @@\n static int write_node(struct connection *conn, struct node *node,\n \t\t      bool no_quota_check)\n {\n-\tTDB_DATA key;\n-\n-\tif (access_node(conn, node, NODE_ACCESS_WRITE, &key))\n+\tif (access_node(conn, node, NODE_ACCESS_WRITE, &node->key))\n \t\treturn errno;\n \n-\treturn write_node_raw(conn, &key, node, no_quota_check);\n+\treturn write_node_raw(conn, &node->key, node, no_quota_check);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tTDB_DATA key;",
                "",
                "\tif (access_node(conn, node, NODE_ACCESS_WRITE, &key))",
                "\treturn write_node_raw(conn, &key, node, no_quota_check);"
            ],
            "added_lines": [
                "\tif (access_node(conn, node, NODE_ACCESS_WRITE, &node->key))",
                "\treturn write_node_raw(conn, &node->key, node, no_quota_check);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-42310",
        "func_name": "xen-project/xen/destroy_node",
        "description": "Xenstore: Guests can create orphaned Xenstore nodes By creating multiple nodes inside a transaction resulting in an error, a malicious guest can create orphaned nodes in the Xenstore data base, as the cleanup after the error will not remove all nodes already created. When the transaction is committed after this situation, nodes without a valid parent can be made permanent in the data base.",
        "git_url": "https://github.com/xen-project/xen/commit/5d71766bd1a4a3a8b2fe952ca2be80e02fe48f34",
        "commit_title": "tools/xenstore: Fail a transaction if it is not possible to create a node",
        "commit_text": " Commit f2bebf72c4d5 \"xenstore: rework of transaction handling\" moved out from copying the entire database everytime a new transaction is opened to track the list of nodes changed.  The content of all the nodes accessed during a transaction will be temporarily stored in TDB using a different key.  The function create_node() may write/update multiple nodes if the child doesn't exist. In case of a failure, the function will revert any changes (this include any update to TDB). Unfortunately, the function which reverts the changes (i.e. destroy_node()) will not use the correct key to delete any update or even request the transaction to fail.  This means that if a client decide to go ahead with committing the transaction, orphan nodes will be created because they were not linked to an existing node (create_node() will write the nodes backwards).  Once some nodes have been partially updated in a transaction, it is not easily possible to undo any changes. So rather than continuing and hit weird issue while committing, it is much saner to fail the transaction.  This will have an impact on any client that decides to commit even if it can't write a node. Although, it is not clear why a normal client would want to do that...  Lastly, update destroy_node() to use the correct key for deleting the node. Rather than recreating it (this will allocate memory and therefore fail), stash the key in the structure node.  This is XSA-415 / CVE-2022-42310. ",
        "func_before": "static int destroy_node(struct connection *conn, struct node *node)\n{\n\tTDB_DATA key;\n\n\tif (streq(node->name, \"/\"))\n\t\tcorrupt(NULL, \"Destroying root node!\");\n\n\tset_tdb_key(node->name, &key);\n\ttdb_delete(tdb_ctx, key);\n\n\tdomain_entry_dec(conn, node);\n\n\treturn 0;\n}",
        "func": "static int destroy_node(struct connection *conn, struct node *node)\n{\n\tif (streq(node->name, \"/\"))\n\t\tcorrupt(NULL, \"Destroying root node!\");\n\n\ttdb_delete(tdb_ctx, node->key);\n\n\tdomain_entry_dec(conn, node);\n\n\t/*\n\t * It is not possible to easily revert the changes in a transaction.\n\t * So if the failure happens in a transaction, mark it as fail to\n\t * prevent any commit.\n\t */\n\tif ( conn->transaction )\n\t\tfail_transaction(conn->transaction);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,14 +1,19 @@\n static int destroy_node(struct connection *conn, struct node *node)\n {\n-\tTDB_DATA key;\n-\n \tif (streq(node->name, \"/\"))\n \t\tcorrupt(NULL, \"Destroying root node!\");\n \n-\tset_tdb_key(node->name, &key);\n-\ttdb_delete(tdb_ctx, key);\n+\ttdb_delete(tdb_ctx, node->key);\n \n \tdomain_entry_dec(conn, node);\n \n+\t/*\n+\t * It is not possible to easily revert the changes in a transaction.\n+\t * So if the failure happens in a transaction, mark it as fail to\n+\t * prevent any commit.\n+\t */\n+\tif ( conn->transaction )\n+\t\tfail_transaction(conn->transaction);\n+\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tTDB_DATA key;",
                "",
                "\tset_tdb_key(node->name, &key);",
                "\ttdb_delete(tdb_ctx, key);"
            ],
            "added_lines": [
                "\ttdb_delete(tdb_ctx, node->key);",
                "\t/*",
                "\t * It is not possible to easily revert the changes in a transaction.",
                "\t * So if the failure happens in a transaction, mark it as fail to",
                "\t * prevent any commit.",
                "\t */",
                "\tif ( conn->transaction )",
                "\t\tfail_transaction(conn->transaction);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-42320",
        "func_name": "xen-project/xen/domain_adjust_node_perms",
        "description": "Xenstore: Guests can get access to Xenstore nodes of deleted domains Access rights of Xenstore nodes are per domid. When a domain is gone, there might be Xenstore nodes left with access rights containing the domid of the removed domain. This is normally no problem, as those access right entries will be corrected when such a node is written later. There is a small time window when a new domain is created, where the access rights of a past domain with the same domid as the new one will be regarded to be still valid, leading to the new domain being able to get access to a node which was meant to be accessible by the removed domain. For this to happen another domain needs to write the node before the newly created domain is being introduced to Xenstore by dom0.",
        "git_url": "https://github.com/xen-project/xen/commit/ab128218225d3542596ca3a02aee80d55494bef8",
        "commit_title": "tools/xenstore: fix checking node permissions",
        "commit_text": " Today chk_domain_generation() is being used to check whether a node permission entry is still valid or whether it is referring to a domain no longer existing. This is done by comparing the node's and the domain's generation count.  In case no struct domain is existing for a checked domain, but the domain itself is valid, chk_domain_generation() assumes it is being called due to the first node created for a new domain and it will return success.  This might be wrong in case the checked permission is related to an old domain, which has just been replaced with a new domain using the same domid.  Fix that by letting chk_domain_generation() fail in case a struct domain isn't found. In order to cover the case of the first node for a new domain try to allocate the needed struct domain explicitly when processing the related SET_PERMS command. In case a referenced domain isn't existing, flag the related permission to be ignored right away.  This is XSA-417 / CVE-2022-42320. ",
        "func_before": "int domain_adjust_node_perms(struct connection *conn, struct node *node)\n{\n\tunsigned int i;\n\tint ret;\n\n\tret = chk_domain_generation(node->perms.p[0].id, node->generation);\n\tif (ret < 0)\n\t\treturn errno;\n\n\t/* If the owner doesn't exist any longer give it to priv domain. */\n\tif (!ret) {\n\t\t/*\n\t\t * In theory we'd need to update the number of dom0 nodes here,\n\t\t * but we could be called for a read of the node. So better\n\t\t * avoid the risk to overflow the node count of dom0.\n\t\t */\n\t\tnode->perms.p[0].id = priv_domid;\n\t}\n\n\tfor (i = 1; i < node->perms.num; i++) {\n\t\tif (node->perms.p[i].perms & XS_PERM_IGNORE)\n\t\t\tcontinue;\n\t\tret = chk_domain_generation(node->perms.p[i].id,\n\t\t\t\t\t    node->generation);\n\t\tif (ret < 0)\n\t\t\treturn errno;\n\t\tif (!ret)\n\t\t\tnode->perms.p[i].perms |= XS_PERM_IGNORE;\n\t}\n\n\treturn 0;\n}",
        "func": "int domain_adjust_node_perms(struct connection *conn, struct node *node)\n{\n\tunsigned int i;\n\tint ret;\n\n\tret = chk_domain_generation(node->perms.p[0].id, node->generation);\n\n\t/* If the owner doesn't exist any longer give it to priv domain. */\n\tif (!ret) {\n\t\t/*\n\t\t * In theory we'd need to update the number of dom0 nodes here,\n\t\t * but we could be called for a read of the node. So better\n\t\t * avoid the risk to overflow the node count of dom0.\n\t\t */\n\t\tnode->perms.p[0].id = priv_domid;\n\t}\n\n\tfor (i = 1; i < node->perms.num; i++) {\n\t\tif (node->perms.p[i].perms & XS_PERM_IGNORE)\n\t\t\tcontinue;\n\t\tret = chk_domain_generation(node->perms.p[i].id,\n\t\t\t\t\t    node->generation);\n\t\tif (!ret)\n\t\t\tnode->perms.p[i].perms |= XS_PERM_IGNORE;\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,8 +4,6 @@\n \tint ret;\n \n \tret = chk_domain_generation(node->perms.p[0].id, node->generation);\n-\tif (ret < 0)\n-\t\treturn errno;\n \n \t/* If the owner doesn't exist any longer give it to priv domain. */\n \tif (!ret) {\n@@ -22,8 +20,6 @@\n \t\t\tcontinue;\n \t\tret = chk_domain_generation(node->perms.p[i].id,\n \t\t\t\t\t    node->generation);\n-\t\tif (ret < 0)\n-\t\t\treturn errno;\n \t\tif (!ret)\n \t\t\tnode->perms.p[i].perms |= XS_PERM_IGNORE;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (ret < 0)",
                "\t\treturn errno;",
                "\t\tif (ret < 0)",
                "\t\t\treturn errno;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-42320",
        "func_name": "xen-project/xen/chk_domain_generation",
        "description": "Xenstore: Guests can get access to Xenstore nodes of deleted domains Access rights of Xenstore nodes are per domid. When a domain is gone, there might be Xenstore nodes left with access rights containing the domid of the removed domain. This is normally no problem, as those access right entries will be corrected when such a node is written later. There is a small time window when a new domain is created, where the access rights of a past domain with the same domid as the new one will be regarded to be still valid, leading to the new domain being able to get access to a node which was meant to be accessible by the removed domain. For this to happen another domain needs to write the node before the newly created domain is being introduced to Xenstore by dom0.",
        "git_url": "https://github.com/xen-project/xen/commit/ab128218225d3542596ca3a02aee80d55494bef8",
        "commit_title": "tools/xenstore: fix checking node permissions",
        "commit_text": " Today chk_domain_generation() is being used to check whether a node permission entry is still valid or whether it is referring to a domain no longer existing. This is done by comparing the node's and the domain's generation count.  In case no struct domain is existing for a checked domain, but the domain itself is valid, chk_domain_generation() assumes it is being called due to the first node created for a new domain and it will return success.  This might be wrong in case the checked permission is related to an old domain, which has just been replaced with a new domain using the same domid.  Fix that by letting chk_domain_generation() fail in case a struct domain isn't found. In order to cover the case of the first node for a new domain try to allocate the needed struct domain explicitly when processing the related SET_PERMS command. In case a referenced domain isn't existing, flag the related permission to be ignored right away.  This is XSA-417 / CVE-2022-42320. ",
        "func_before": "static int chk_domain_generation(unsigned int domid, uint64_t gen)\n{\n\tstruct domain *d;\n\txc_dominfo_t dominfo;\n\n\tif (!xc_handle && domid == 0)\n\t\treturn 1;\n\n\td = find_domain_struct(domid);\n\tif (d)\n\t\treturn (d->generation <= gen) ? 1 : 0;\n\n\tif (!get_domain_info(domid, &dominfo))\n\t\treturn 0;\n\n\td = alloc_domain(NULL, domid);\n\treturn d ? 1 : -1;\n}",
        "func": "static int chk_domain_generation(unsigned int domid, uint64_t gen)\n{\n\tstruct domain *d;\n\n\tif (!xc_handle && domid == 0)\n\t\treturn 1;\n\n\td = find_domain_struct(domid);\n\n\treturn (d && d->generation <= gen) ? 1 : 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,18 +1,11 @@\n static int chk_domain_generation(unsigned int domid, uint64_t gen)\n {\n \tstruct domain *d;\n-\txc_dominfo_t dominfo;\n \n \tif (!xc_handle && domid == 0)\n \t\treturn 1;\n \n \td = find_domain_struct(domid);\n-\tif (d)\n-\t\treturn (d->generation <= gen) ? 1 : 0;\n \n-\tif (!get_domain_info(domid, &dominfo))\n-\t\treturn 0;\n-\n-\td = alloc_domain(NULL, domid);\n-\treturn d ? 1 : -1;\n+\treturn (d && d->generation <= gen) ? 1 : 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\txc_dominfo_t dominfo;",
                "\tif (d)",
                "\t\treturn (d->generation <= gen) ? 1 : 0;",
                "\tif (!get_domain_info(domid, &dominfo))",
                "\t\treturn 0;",
                "",
                "\td = alloc_domain(NULL, domid);",
                "\treturn d ? 1 : -1;"
            ],
            "added_lines": [
                "\treturn (d && d->generation <= gen) ? 1 : 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-42320",
        "func_name": "xen-project/xen/do_set_perms",
        "description": "Xenstore: Guests can get access to Xenstore nodes of deleted domains Access rights of Xenstore nodes are per domid. When a domain is gone, there might be Xenstore nodes left with access rights containing the domid of the removed domain. This is normally no problem, as those access right entries will be corrected when such a node is written later. There is a small time window when a new domain is created, where the access rights of a past domain with the same domid as the new one will be regarded to be still valid, leading to the new domain being able to get access to a node which was meant to be accessible by the removed domain. For this to happen another domain needs to write the node before the newly created domain is being introduced to Xenstore by dom0.",
        "git_url": "https://github.com/xen-project/xen/commit/ab128218225d3542596ca3a02aee80d55494bef8",
        "commit_title": "tools/xenstore: fix checking node permissions",
        "commit_text": " Today chk_domain_generation() is being used to check whether a node permission entry is still valid or whether it is referring to a domain no longer existing. This is done by comparing the node's and the domain's generation count.  In case no struct domain is existing for a checked domain, but the domain itself is valid, chk_domain_generation() assumes it is being called due to the first node created for a new domain and it will return success.  This might be wrong in case the checked permission is related to an old domain, which has just been replaced with a new domain using the same domid.  Fix that by letting chk_domain_generation() fail in case a struct domain isn't found. In order to cover the case of the first node for a new domain try to allocate the needed struct domain explicitly when processing the related SET_PERMS command. In case a referenced domain isn't existing, flag the related permission to be ignored right away.  This is XSA-417 / CVE-2022-42320. ",
        "func_before": "static int do_set_perms(const void *ctx, struct connection *conn,\n\t\t\tstruct buffered_data *in)\n{\n\tstruct node_perms perms, old_perms;\n\tchar *name, *permstr;\n\tstruct node *node;\n\n\tperms.num = xs_count_strings(in->buffer, in->used);\n\tif (perms.num < 2)\n\t\treturn EINVAL;\n\n\tperms.num--;\n\tif (domain_is_unprivileged(conn) &&\n\t    perms.num > quota_nb_perms_per_node)\n\t\treturn ENOSPC;\n\n\tpermstr = in->buffer + strlen(in->buffer) + 1;\n\n\tperms.p = talloc_array(ctx, struct xs_permissions, perms.num);\n\tif (!perms.p)\n\t\treturn ENOMEM;\n\tif (!xs_strings_to_perms(perms.p, perms.num, permstr))\n\t\treturn errno;\n\n\t/* First arg is node name. */\n\tif (strstarts(in->buffer, \"@\")) {\n\t\tif (set_perms_special(conn, in->buffer, &perms))\n\t\t\treturn errno;\n\t\tsend_ack(conn, XS_SET_PERMS);\n\t\treturn 0;\n\t}\n\n\t/* We must own node to do this (tools can do this too). */\n\tnode = get_node_canonicalized(conn, ctx, in->buffer, &name,\n\t\t\t\t      XS_PERM_WRITE | XS_PERM_OWNER);\n\tif (!node)\n\t\treturn errno;\n\n\t/* Unprivileged domains may not change the owner. */\n\tif (domain_is_unprivileged(conn) &&\n\t    perms.p[0].id != node->perms.p[0].id)\n\t\treturn EPERM;\n\n\told_perms = node->perms;\n\tdomain_entry_dec(conn, node);\n\tnode->perms = perms;\n\tif (domain_entry_inc(conn, node)) {\n\t\tnode->perms = old_perms;\n\t\t/*\n\t\t * This should never fail because we had a reference on the\n\t\t * domain before and Xenstored is single-threaded.\n\t\t */\n\t\tdomain_entry_inc(conn, node);\n\t\treturn ENOMEM;\n\t}\n\n\tif (write_node(conn, node, false)) {\n\t\tint saved_errno = errno;\n\n\t\tdomain_entry_dec(conn, node);\n\t\tnode->perms = old_perms;\n\t\t/* No failure possible as above. */\n\t\tdomain_entry_inc(conn, node);\n\n\t\terrno = saved_errno;\n\t\treturn errno;\n\t}\n\n\tfire_watches(conn, ctx, name, node, false, &old_perms);\n\tsend_ack(conn, XS_SET_PERMS);\n\n\treturn 0;\n}",
        "func": "static int do_set_perms(const void *ctx, struct connection *conn,\n\t\t\tstruct buffered_data *in)\n{\n\tstruct node_perms perms, old_perms;\n\tchar *name, *permstr;\n\tstruct node *node;\n\n\tperms.num = xs_count_strings(in->buffer, in->used);\n\tif (perms.num < 2)\n\t\treturn EINVAL;\n\n\tperms.num--;\n\tif (domain_is_unprivileged(conn) &&\n\t    perms.num > quota_nb_perms_per_node)\n\t\treturn ENOSPC;\n\n\tpermstr = in->buffer + strlen(in->buffer) + 1;\n\n\tperms.p = talloc_array(ctx, struct xs_permissions, perms.num);\n\tif (!perms.p)\n\t\treturn ENOMEM;\n\tif (!xs_strings_to_perms(perms.p, perms.num, permstr))\n\t\treturn errno;\n\n\tif (domain_alloc_permrefs(&perms) < 0)\n\t\treturn ENOMEM;\n\tif (perms.p[0].perms & XS_PERM_IGNORE)\n\t\treturn ENOENT;\n\n\t/* First arg is node name. */\n\tif (strstarts(in->buffer, \"@\")) {\n\t\tif (set_perms_special(conn, in->buffer, &perms))\n\t\t\treturn errno;\n\t\tsend_ack(conn, XS_SET_PERMS);\n\t\treturn 0;\n\t}\n\n\t/* We must own node to do this (tools can do this too). */\n\tnode = get_node_canonicalized(conn, ctx, in->buffer, &name,\n\t\t\t\t      XS_PERM_WRITE | XS_PERM_OWNER);\n\tif (!node)\n\t\treturn errno;\n\n\t/* Unprivileged domains may not change the owner. */\n\tif (domain_is_unprivileged(conn) &&\n\t    perms.p[0].id != node->perms.p[0].id)\n\t\treturn EPERM;\n\n\told_perms = node->perms;\n\tdomain_entry_dec(conn, node);\n\tnode->perms = perms;\n\tif (domain_entry_inc(conn, node)) {\n\t\tnode->perms = old_perms;\n\t\t/*\n\t\t * This should never fail because we had a reference on the\n\t\t * domain before and Xenstored is single-threaded.\n\t\t */\n\t\tdomain_entry_inc(conn, node);\n\t\treturn ENOMEM;\n\t}\n\n\tif (write_node(conn, node, false)) {\n\t\tint saved_errno = errno;\n\n\t\tdomain_entry_dec(conn, node);\n\t\tnode->perms = old_perms;\n\t\t/* No failure possible as above. */\n\t\tdomain_entry_inc(conn, node);\n\n\t\terrno = saved_errno;\n\t\treturn errno;\n\t}\n\n\tfire_watches(conn, ctx, name, node, false, &old_perms);\n\tsend_ack(conn, XS_SET_PERMS);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,11 @@\n \t\treturn ENOMEM;\n \tif (!xs_strings_to_perms(perms.p, perms.num, permstr))\n \t\treturn errno;\n+\n+\tif (domain_alloc_permrefs(&perms) < 0)\n+\t\treturn ENOMEM;\n+\tif (perms.p[0].perms & XS_PERM_IGNORE)\n+\t\treturn ENOENT;\n \n \t/* First arg is node name. */\n \tif (strstarts(in->buffer, \"@\")) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (domain_alloc_permrefs(&perms) < 0)",
                "\t\treturn ENOMEM;",
                "\tif (perms.p[0].perms & XS_PERM_IGNORE)",
                "\t\treturn ENOENT;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/arch_domain_create",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/e06b95c1d44ab80da255219fc9f1e2fc423edcb6",
        "commit_title": "x86/spec-ctrl: Make VERW flushing runtime conditional",
        "commit_text": " Currently, VERW flushing to mitigate MDS is boot time conditional per domain type.  However, to provide mitigations for DRPW (CVE-2022-21166), we need to conditionally use VERW based on the trustworthiness of the guest, and the devices passed through.  Remove the PV/HVM alternatives and instead issue a VERW on the return-to-guest path depending on the SCF_verw bit in cpuinfo spec_ctrl_flags.  Introduce spec_ctrl_init_domain() and d->arch.verw to calculate the VERW disposition at domain creation time, and context switch the SCF_verw bit.  For now, VERW flushing is used and controlled exactly as before, but later patches will add per-domain cases too.  No change in behaviour.  This is part of XSA-404. ",
        "func_before": "int arch_domain_create(struct domain *d,\n                       struct xen_domctl_createdomain *config,\n                       unsigned int flags)\n{\n    bool paging_initialised = false;\n    uint32_t emflags;\n    int rc;\n\n    INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);\n\n    spin_lock_init(&d->arch.e820_lock);\n\n    /* Minimal initialisation for the idle domain. */\n    if ( unlikely(is_idle_domain(d)) )\n    {\n        static const struct arch_csw idle_csw = {\n            .from = paravirt_ctxt_switch_from,\n            .to   = paravirt_ctxt_switch_to,\n            .tail = idle_loop,\n        };\n\n        d->arch.ctxt_switch = &idle_csw;\n\n        d->arch.cpuid = ZERO_BLOCK_PTR; /* Catch stray misuses. */\n        d->arch.msr = ZERO_BLOCK_PTR;\n\n        return 0;\n    }\n\n    if ( !config )\n    {\n        /* Only IDLE is allowed with no config. */\n        ASSERT_UNREACHABLE();\n        return -EINVAL;\n    }\n\n    if ( d->domain_id && cpu_has_amd_erratum(&boot_cpu_data, AMD_ERRATUM_121) )\n    {\n        if ( !opt_allow_unsafe )\n        {\n            printk(XENLOG_G_ERR \"Xen does not allow DomU creation on this CPU\"\n                   \" for security reasons.\\n\");\n            return -EPERM;\n        }\n        printk(XENLOG_G_WARNING\n               \"Dom%d may compromise security on this CPU.\\n\",\n               d->domain_id);\n    }\n\n    emflags = config->arch.emulation_flags;\n\n    if ( is_hardware_domain(d) && is_pv_domain(d) )\n        emflags |= XEN_X86_EMU_PIT;\n\n    if ( emflags & ~XEN_X86_EMU_ALL )\n    {\n        printk(XENLOG_G_ERR \"d%d: Invalid emulation bitmap: %#x\\n\",\n               d->domain_id, emflags);\n        return -EINVAL;\n    }\n\n    if ( !emulation_flags_ok(d, emflags) )\n    {\n        printk(XENLOG_G_ERR \"d%d: Xen does not allow %s domain creation \"\n               \"with the current selection of emulators: %#x\\n\",\n               d->domain_id, is_hvm_domain(d) ? \"HVM\" : \"PV\", emflags);\n        return -EOPNOTSUPP;\n    }\n    d->arch.emulation_flags = emflags;\n\n#ifdef CONFIG_PV32\n    HYPERVISOR_COMPAT_VIRT_START(d) =\n        is_pv_domain(d) ? __HYPERVISOR_COMPAT_VIRT_START : ~0u;\n#endif\n\n    if ( (rc = paging_domain_init(d)) != 0 )\n        goto fail;\n    paging_initialised = true;\n\n    if ( (rc = init_domain_cpuid_policy(d)) )\n        goto fail;\n\n    if ( (rc = init_domain_msr_policy(d)) )\n        goto fail;\n\n    d->arch.ioport_caps =\n        rangeset_new(d, \"I/O Ports\", RANGESETF_prettyprint_hex);\n    rc = -ENOMEM;\n    if ( d->arch.ioport_caps == NULL )\n        goto fail;\n\n    /*\n     * The shared_info machine address must fit in a 32-bit field within a\n     * 32-bit guest's start_info structure. Hence we specify MEMF_bits(32).\n     */\n    if ( (d->shared_info = alloc_xenheap_pages(0, MEMF_bits(32))) == NULL )\n        goto fail;\n\n    clear_page(d->shared_info);\n    share_xen_page_with_guest(virt_to_page(d->shared_info), d, SHARE_rw);\n\n    if ( (rc = init_domain_irq_mapping(d)) != 0 )\n        goto fail;\n\n    if ( (rc = iommu_domain_init(d, config->iommu_opts)) != 0 )\n        goto fail;\n\n    psr_domain_init(d);\n\n    if ( is_hvm_domain(d) )\n    {\n        if ( (rc = hvm_domain_initialise(d)) != 0 )\n            goto fail;\n    }\n    else if ( is_pv_domain(d) )\n    {\n        mapcache_domain_init(d);\n\n        if ( (rc = pv_domain_initialise(d)) != 0 )\n            goto fail;\n    }\n    else\n        ASSERT_UNREACHABLE(); /* Not HVM and not PV? */\n\n    if ( (rc = tsc_set_info(d, TSC_MODE_DEFAULT, 0, 0, 0)) != 0 )\n    {\n        ASSERT_UNREACHABLE();\n        goto fail;\n    }\n\n    /* PV/PVH guests get an emulated PIT too for video BIOSes to use. */\n    pit_init(d, cpu_khz);\n\n    /*\n     * If the FPU does not save FCS/FDS then we can always\n     * save/restore the 64-bit FIP/FDP and ignore the selectors.\n     */\n    d->arch.x87_fip_width = cpu_has_fpu_sel ? 0 : 8;\n\n    domain_cpu_policy_changed(d);\n\n    d->arch.msr_relaxed = config->arch.misc_flags & XEN_X86_MSR_RELAXED;\n\n    return 0;\n\n fail:\n    d->is_dying = DOMDYING_dead;\n    psr_domain_free(d);\n    iommu_domain_destroy(d);\n    cleanup_domain_irq_mapping(d);\n    free_xenheap_page(d->shared_info);\n    xfree(d->arch.cpuid);\n    xfree(d->arch.msr);\n    if ( paging_initialised )\n        paging_final_teardown(d);\n    free_perdomain_mappings(d);\n\n    return rc;\n}",
        "func": "int arch_domain_create(struct domain *d,\n                       struct xen_domctl_createdomain *config,\n                       unsigned int flags)\n{\n    bool paging_initialised = false;\n    uint32_t emflags;\n    int rc;\n\n    INIT_PAGE_LIST_HEAD(&d->arch.relmem_list);\n\n    spin_lock_init(&d->arch.e820_lock);\n\n    /* Minimal initialisation for the idle domain. */\n    if ( unlikely(is_idle_domain(d)) )\n    {\n        static const struct arch_csw idle_csw = {\n            .from = paravirt_ctxt_switch_from,\n            .to   = paravirt_ctxt_switch_to,\n            .tail = idle_loop,\n        };\n\n        d->arch.ctxt_switch = &idle_csw;\n\n        d->arch.cpuid = ZERO_BLOCK_PTR; /* Catch stray misuses. */\n        d->arch.msr = ZERO_BLOCK_PTR;\n\n        return 0;\n    }\n\n    if ( !config )\n    {\n        /* Only IDLE is allowed with no config. */\n        ASSERT_UNREACHABLE();\n        return -EINVAL;\n    }\n\n    if ( d->domain_id && cpu_has_amd_erratum(&boot_cpu_data, AMD_ERRATUM_121) )\n    {\n        if ( !opt_allow_unsafe )\n        {\n            printk(XENLOG_G_ERR \"Xen does not allow DomU creation on this CPU\"\n                   \" for security reasons.\\n\");\n            return -EPERM;\n        }\n        printk(XENLOG_G_WARNING\n               \"Dom%d may compromise security on this CPU.\\n\",\n               d->domain_id);\n    }\n\n    emflags = config->arch.emulation_flags;\n\n    if ( is_hardware_domain(d) && is_pv_domain(d) )\n        emflags |= XEN_X86_EMU_PIT;\n\n    if ( emflags & ~XEN_X86_EMU_ALL )\n    {\n        printk(XENLOG_G_ERR \"d%d: Invalid emulation bitmap: %#x\\n\",\n               d->domain_id, emflags);\n        return -EINVAL;\n    }\n\n    if ( !emulation_flags_ok(d, emflags) )\n    {\n        printk(XENLOG_G_ERR \"d%d: Xen does not allow %s domain creation \"\n               \"with the current selection of emulators: %#x\\n\",\n               d->domain_id, is_hvm_domain(d) ? \"HVM\" : \"PV\", emflags);\n        return -EOPNOTSUPP;\n    }\n    d->arch.emulation_flags = emflags;\n\n#ifdef CONFIG_PV32\n    HYPERVISOR_COMPAT_VIRT_START(d) =\n        is_pv_domain(d) ? __HYPERVISOR_COMPAT_VIRT_START : ~0u;\n#endif\n\n    if ( (rc = paging_domain_init(d)) != 0 )\n        goto fail;\n    paging_initialised = true;\n\n    if ( (rc = init_domain_cpuid_policy(d)) )\n        goto fail;\n\n    if ( (rc = init_domain_msr_policy(d)) )\n        goto fail;\n\n    d->arch.ioport_caps =\n        rangeset_new(d, \"I/O Ports\", RANGESETF_prettyprint_hex);\n    rc = -ENOMEM;\n    if ( d->arch.ioport_caps == NULL )\n        goto fail;\n\n    /*\n     * The shared_info machine address must fit in a 32-bit field within a\n     * 32-bit guest's start_info structure. Hence we specify MEMF_bits(32).\n     */\n    if ( (d->shared_info = alloc_xenheap_pages(0, MEMF_bits(32))) == NULL )\n        goto fail;\n\n    clear_page(d->shared_info);\n    share_xen_page_with_guest(virt_to_page(d->shared_info), d, SHARE_rw);\n\n    if ( (rc = init_domain_irq_mapping(d)) != 0 )\n        goto fail;\n\n    if ( (rc = iommu_domain_init(d, config->iommu_opts)) != 0 )\n        goto fail;\n\n    psr_domain_init(d);\n\n    if ( is_hvm_domain(d) )\n    {\n        if ( (rc = hvm_domain_initialise(d)) != 0 )\n            goto fail;\n    }\n    else if ( is_pv_domain(d) )\n    {\n        mapcache_domain_init(d);\n\n        if ( (rc = pv_domain_initialise(d)) != 0 )\n            goto fail;\n    }\n    else\n        ASSERT_UNREACHABLE(); /* Not HVM and not PV? */\n\n    if ( (rc = tsc_set_info(d, TSC_MODE_DEFAULT, 0, 0, 0)) != 0 )\n    {\n        ASSERT_UNREACHABLE();\n        goto fail;\n    }\n\n    /* PV/PVH guests get an emulated PIT too for video BIOSes to use. */\n    pit_init(d, cpu_khz);\n\n    /*\n     * If the FPU does not save FCS/FDS then we can always\n     * save/restore the 64-bit FIP/FDP and ignore the selectors.\n     */\n    d->arch.x87_fip_width = cpu_has_fpu_sel ? 0 : 8;\n\n    domain_cpu_policy_changed(d);\n\n    d->arch.msr_relaxed = config->arch.misc_flags & XEN_X86_MSR_RELAXED;\n\n    spec_ctrl_init_domain(d);\n\n    return 0;\n\n fail:\n    d->is_dying = DOMDYING_dead;\n    psr_domain_free(d);\n    iommu_domain_destroy(d);\n    cleanup_domain_irq_mapping(d);\n    free_xenheap_page(d->shared_info);\n    xfree(d->arch.cpuid);\n    xfree(d->arch.msr);\n    if ( paging_initialised )\n        paging_final_teardown(d);\n    free_perdomain_mappings(d);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -141,6 +141,8 @@\n \n     d->arch.msr_relaxed = config->arch.misc_flags & XEN_X86_MSR_RELAXED;\n \n+    spec_ctrl_init_domain(d);\n+\n     return 0;\n \n  fail:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    spec_ctrl_init_domain(d);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/context_switch",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/e06b95c1d44ab80da255219fc9f1e2fc423edcb6",
        "commit_title": "x86/spec-ctrl: Make VERW flushing runtime conditional",
        "commit_text": " Currently, VERW flushing to mitigate MDS is boot time conditional per domain type.  However, to provide mitigations for DRPW (CVE-2022-21166), we need to conditionally use VERW based on the trustworthiness of the guest, and the devices passed through.  Remove the PV/HVM alternatives and instead issue a VERW on the return-to-guest path depending on the SCF_verw bit in cpuinfo spec_ctrl_flags.  Introduce spec_ctrl_init_domain() and d->arch.verw to calculate the VERW disposition at domain creation time, and context switch the SCF_verw bit.  For now, VERW flushing is used and controlled exactly as before, but later patches will add per-domain cases too.  No change in behaviour.  This is part of XSA-404. ",
        "func_before": "void context_switch(struct vcpu *prev, struct vcpu *next)\n{\n    unsigned int cpu = smp_processor_id();\n    const struct domain *prevd = prev->domain, *nextd = next->domain;\n    unsigned int dirty_cpu = read_atomic(&next->dirty_cpu);\n\n    ASSERT(prev != next);\n    ASSERT(local_irq_is_enabled());\n\n    get_cpu_info()->use_pv_cr3 = false;\n    get_cpu_info()->xen_cr3 = 0;\n\n    if ( unlikely(dirty_cpu != cpu) && dirty_cpu != VCPU_CPU_CLEAN )\n    {\n        /* Remote CPU calls __sync_local_execstate() from flush IPI handler. */\n        flush_mask(cpumask_of(dirty_cpu), FLUSH_VCPU_STATE);\n        ASSERT(!vcpu_cpu_dirty(next));\n    }\n\n    _update_runstate_area(prev);\n    vpmu_switch_from(prev);\n    np2m_schedule(NP2M_SCHEDLE_OUT);\n\n    if ( is_hvm_domain(prevd) && !list_empty(&prev->arch.hvm.tm_list) )\n        pt_save_timer(prev);\n\n    local_irq_disable();\n\n    set_current(next);\n\n    if ( (per_cpu(curr_vcpu, cpu) == next) ||\n         (is_idle_domain(nextd) && cpu_online(cpu)) )\n    {\n        local_irq_enable();\n    }\n    else\n    {\n        __context_switch();\n\n        /* Re-enable interrupts before restoring state which may fault. */\n        local_irq_enable();\n\n        if ( is_pv_domain(nextd) )\n            load_segments(next);\n\n        ctxt_switch_levelling(next);\n\n        if ( opt_ibpb && !is_idle_domain(nextd) )\n        {\n            static DEFINE_PER_CPU(unsigned int, last);\n            unsigned int *last_id = &this_cpu(last);\n\n            /*\n             * Squash the domid and vcpu id together for comparison\n             * efficiency.  We could in principle stash and compare the struct\n             * vcpu pointer, but this risks a false alias if a domain has died\n             * and the same 4k page gets reused for a new vcpu.\n             */\n            unsigned int next_id = (((unsigned int)nextd->domain_id << 16) |\n                                    (uint16_t)next->vcpu_id);\n            BUILD_BUG_ON(MAX_VIRT_CPUS > 0xffff);\n\n            /*\n             * When scheduling from a vcpu, to idle, and back to the same vcpu\n             * (which might be common in a lightly loaded system, or when\n             * using vcpu pinning), there is no need to issue IBPB, as we are\n             * returning to the same security context.\n             */\n            if ( *last_id != next_id )\n            {\n                wrmsrl(MSR_PRED_CMD, PRED_CMD_IBPB);\n                *last_id = next_id;\n            }\n        }\n    }\n\n    sched_context_switched(prev, next);\n\n    _update_runstate_area(next);\n    /* Must be done with interrupts enabled */\n    vpmu_switch_to(next);\n    np2m_schedule(NP2M_SCHEDLE_IN);\n\n    /* Ensure that the vcpu has an up-to-date time base. */\n    update_vcpu_system_time(next);\n\n    reset_stack_and_jump_ind(nextd->arch.ctxt_switch->tail);\n}",
        "func": "void context_switch(struct vcpu *prev, struct vcpu *next)\n{\n    unsigned int cpu = smp_processor_id();\n    struct cpu_info *info = get_cpu_info();\n    const struct domain *prevd = prev->domain, *nextd = next->domain;\n    unsigned int dirty_cpu = read_atomic(&next->dirty_cpu);\n\n    ASSERT(prev != next);\n    ASSERT(local_irq_is_enabled());\n\n    info->use_pv_cr3 = false;\n    info->xen_cr3 = 0;\n\n    if ( unlikely(dirty_cpu != cpu) && dirty_cpu != VCPU_CPU_CLEAN )\n    {\n        /* Remote CPU calls __sync_local_execstate() from flush IPI handler. */\n        flush_mask(cpumask_of(dirty_cpu), FLUSH_VCPU_STATE);\n        ASSERT(!vcpu_cpu_dirty(next));\n    }\n\n    _update_runstate_area(prev);\n    vpmu_switch_from(prev);\n    np2m_schedule(NP2M_SCHEDLE_OUT);\n\n    if ( is_hvm_domain(prevd) && !list_empty(&prev->arch.hvm.tm_list) )\n        pt_save_timer(prev);\n\n    local_irq_disable();\n\n    set_current(next);\n\n    if ( (per_cpu(curr_vcpu, cpu) == next) ||\n         (is_idle_domain(nextd) && cpu_online(cpu)) )\n    {\n        local_irq_enable();\n    }\n    else\n    {\n        __context_switch();\n\n        /* Re-enable interrupts before restoring state which may fault. */\n        local_irq_enable();\n\n        if ( is_pv_domain(nextd) )\n            load_segments(next);\n\n        ctxt_switch_levelling(next);\n\n        if ( opt_ibpb && !is_idle_domain(nextd) )\n        {\n            static DEFINE_PER_CPU(unsigned int, last);\n            unsigned int *last_id = &this_cpu(last);\n\n            /*\n             * Squash the domid and vcpu id together for comparison\n             * efficiency.  We could in principle stash and compare the struct\n             * vcpu pointer, but this risks a false alias if a domain has died\n             * and the same 4k page gets reused for a new vcpu.\n             */\n            unsigned int next_id = (((unsigned int)nextd->domain_id << 16) |\n                                    (uint16_t)next->vcpu_id);\n            BUILD_BUG_ON(MAX_VIRT_CPUS > 0xffff);\n\n            /*\n             * When scheduling from a vcpu, to idle, and back to the same vcpu\n             * (which might be common in a lightly loaded system, or when\n             * using vcpu pinning), there is no need to issue IBPB, as we are\n             * returning to the same security context.\n             */\n            if ( *last_id != next_id )\n            {\n                wrmsrl(MSR_PRED_CMD, PRED_CMD_IBPB);\n                *last_id = next_id;\n            }\n        }\n\n        /* Update the top-of-stack block with the VERW disposition. */\n        info->spec_ctrl_flags &= ~SCF_verw;\n        if ( nextd->arch.verw )\n            info->spec_ctrl_flags |= SCF_verw;\n    }\n\n    sched_context_switched(prev, next);\n\n    _update_runstate_area(next);\n    /* Must be done with interrupts enabled */\n    vpmu_switch_to(next);\n    np2m_schedule(NP2M_SCHEDLE_IN);\n\n    /* Ensure that the vcpu has an up-to-date time base. */\n    update_vcpu_system_time(next);\n\n    reset_stack_and_jump_ind(nextd->arch.ctxt_switch->tail);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,14 +1,15 @@\n void context_switch(struct vcpu *prev, struct vcpu *next)\n {\n     unsigned int cpu = smp_processor_id();\n+    struct cpu_info *info = get_cpu_info();\n     const struct domain *prevd = prev->domain, *nextd = next->domain;\n     unsigned int dirty_cpu = read_atomic(&next->dirty_cpu);\n \n     ASSERT(prev != next);\n     ASSERT(local_irq_is_enabled());\n \n-    get_cpu_info()->use_pv_cr3 = false;\n-    get_cpu_info()->xen_cr3 = 0;\n+    info->use_pv_cr3 = false;\n+    info->xen_cr3 = 0;\n \n     if ( unlikely(dirty_cpu != cpu) && dirty_cpu != VCPU_CPU_CLEAN )\n     {\n@@ -72,6 +73,11 @@\n                 *last_id = next_id;\n             }\n         }\n+\n+        /* Update the top-of-stack block with the VERW disposition. */\n+        info->spec_ctrl_flags &= ~SCF_verw;\n+        if ( nextd->arch.verw )\n+            info->spec_ctrl_flags |= SCF_verw;\n     }\n \n     sched_context_switched(prev, next);",
        "diff_line_info": {
            "deleted_lines": [
                "    get_cpu_info()->use_pv_cr3 = false;",
                "    get_cpu_info()->xen_cr3 = 0;"
            ],
            "added_lines": [
                "    struct cpu_info *info = get_cpu_info();",
                "    info->use_pv_cr3 = false;",
                "    info->xen_cr3 = 0;",
                "",
                "        /* Update the top-of-stack block with the VERW disposition. */",
                "        info->spec_ctrl_flags &= ~SCF_verw;",
                "        if ( nextd->arch.verw )",
                "            info->spec_ctrl_flags |= SCF_verw;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/init_speculation_mitigations",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/e06b95c1d44ab80da255219fc9f1e2fc423edcb6",
        "commit_title": "x86/spec-ctrl: Make VERW flushing runtime conditional",
        "commit_text": " Currently, VERW flushing to mitigate MDS is boot time conditional per domain type.  However, to provide mitigations for DRPW (CVE-2022-21166), we need to conditionally use VERW based on the trustworthiness of the guest, and the devices passed through.  Remove the PV/HVM alternatives and instead issue a VERW on the return-to-guest path depending on the SCF_verw bit in cpuinfo spec_ctrl_flags.  Introduce spec_ctrl_init_domain() and d->arch.verw to calculate the VERW disposition at domain creation time, and context switch the SCF_verw bit.  For now, VERW flushing is used and controlled exactly as before, but later patches will add per-domain cases too.  No change in behaviour.  This is part of XSA-404. ",
        "func_before": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa;\n    uint64_t caps = 0;\n\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n        else if ( opt_ibrs == -1 )\n        {\n            opt_ibrs = ibrs = true;\n            default_xen_spec_ctrl |= SPEC_CTRL_IBRS | SPEC_CTRL_STIBP;\n        }\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe(caps) )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_wrmsr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* If we have IBRS available, see whether we should use it. */\n    if ( has_spec_ctrl && ibrs )\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n\n    /* If we have SSBD available, see whether we should use it. */\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 && boot_cpu_has(X86_FEATURE_XEN_SMEP) &&\n         !opt_pv32 && rsb_is_full_width() )\n        opt_rsb_pv = 0;\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n        opt_ibpb = false;\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /* If Xen is using any MSR_SPEC_CTRL settings, adjust the idle path. */\n    if ( default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default(caps);\n\n    l1tf_calculations(caps);\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !(caps & ARCH_CAPS_SKIP_L1DFL);\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations(caps);\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS defences as applicable.  The PV blocks need using all the\n     * time, and the Idle blocks need using if either PV or HVM defences are\n     * used.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivelent semantics to avoid needing to perform both flushes on the\n     * HVM path.  The HVM blocks don't need activating if our hypervisor told\n     * us it was handling L1D_FLUSH, or we are using L1D_FLUSH ourselves.\n     */\n    if ( opt_md_clear_pv )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_PV);\n    if ( opt_md_clear_pv || opt_md_clear_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    if ( opt_md_clear_hvm && !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_HVM);\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || (caps & ARCH_CAPS_TSX_CTRL)) &&\n        (caps & (ARCH_CAPS_MDS_NO | ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && (caps & ARCH_CAPS_TSX_CTRL) &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * On parts which enumerate MDS_NO and not TAA_NO, TSX is the only known\n     * way to access the Fill Buffer.  If TSX isn't available (inc. SKU\n     * reasons on some models), or TSX is explicitly disabled, then there is\n     * no need for the extra overhead to protect RDRAND/RDSEED.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 &&\n             (caps & (ARCH_CAPS_MDS_NO|ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO &&\n             (!cpu_has_hle || ((caps & ARCH_CAPS_TSX_CTRL) && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk, caps);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "func": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa;\n    uint64_t caps = 0;\n\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n        else if ( opt_ibrs == -1 )\n        {\n            opt_ibrs = ibrs = true;\n            default_xen_spec_ctrl |= SPEC_CTRL_IBRS | SPEC_CTRL_STIBP;\n        }\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe(caps) )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_wrmsr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* If we have IBRS available, see whether we should use it. */\n    if ( has_spec_ctrl && ibrs )\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n\n    /* If we have SSBD available, see whether we should use it. */\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 && boot_cpu_has(X86_FEATURE_XEN_SMEP) &&\n         !opt_pv32 && rsb_is_full_width() )\n        opt_rsb_pv = 0;\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n        opt_ibpb = false;\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /* If Xen is using any MSR_SPEC_CTRL settings, adjust the idle path. */\n    if ( default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default(caps);\n\n    l1tf_calculations(caps);\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !(caps & ARCH_CAPS_SKIP_L1DFL);\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations(caps);\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS defences as applicable.  The Idle blocks need using if\n     * either PV or HVM defences are used.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || (caps & ARCH_CAPS_TSX_CTRL)) &&\n        (caps & (ARCH_CAPS_MDS_NO | ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && (caps & ARCH_CAPS_TSX_CTRL) &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * On parts which enumerate MDS_NO and not TAA_NO, TSX is the only known\n     * way to access the Fill Buffer.  If TSX isn't available (inc. SKU\n     * reasons on some models), or TSX is explicitly disabled, then there is\n     * no need for the extra overhead to protect RDRAND/RDSEED.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 &&\n             (caps & (ARCH_CAPS_MDS_NO|ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO &&\n             (!cpu_has_hle || ((caps & ARCH_CAPS_TSX_CTRL) && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk, caps);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -262,21 +262,20 @@\n                             boot_cpu_has(X86_FEATURE_MD_CLEAR));\n \n     /*\n-     * Enable MDS defences as applicable.  The PV blocks need using all the\n-     * time, and the Idle blocks need using if either PV or HVM defences are\n-     * used.\n+     * Enable MDS defences as applicable.  The Idle blocks need using if\n+     * either PV or HVM defences are used.\n      *\n      * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n-     * equivelent semantics to avoid needing to perform both flushes on the\n-     * HVM path.  The HVM blocks don't need activating if our hypervisor told\n-     * us it was handling L1D_FLUSH, or we are using L1D_FLUSH ourselves.\n-     */\n-    if ( opt_md_clear_pv )\n-        setup_force_cpu_cap(X86_FEATURE_SC_VERW_PV);\n+     * equivalent semantics to avoid needing to perform both flushes on the\n+     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.\n+     *\n+     * After calculating the appropriate idle setting, simplify\n+     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n+     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n+     */\n     if ( opt_md_clear_pv || opt_md_clear_hvm )\n         setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n-    if ( opt_md_clear_hvm && !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush )\n-        setup_force_cpu_cap(X86_FEATURE_SC_VERW_HVM);\n+    opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;\n \n     /*\n      * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT",
        "diff_line_info": {
            "deleted_lines": [
                "     * Enable MDS defences as applicable.  The PV blocks need using all the",
                "     * time, and the Idle blocks need using if either PV or HVM defences are",
                "     * used.",
                "     * equivelent semantics to avoid needing to perform both flushes on the",
                "     * HVM path.  The HVM blocks don't need activating if our hypervisor told",
                "     * us it was handling L1D_FLUSH, or we are using L1D_FLUSH ourselves.",
                "     */",
                "    if ( opt_md_clear_pv )",
                "        setup_force_cpu_cap(X86_FEATURE_SC_VERW_PV);",
                "    if ( opt_md_clear_hvm && !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush )",
                "        setup_force_cpu_cap(X86_FEATURE_SC_VERW_HVM);"
            ],
            "added_lines": [
                "     * Enable MDS defences as applicable.  The Idle blocks need using if",
                "     * either PV or HVM defences are used.",
                "     * equivalent semantics to avoid needing to perform both flushes on the",
                "     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.",
                "     *",
                "     * After calculating the appropriate idle setting, simplify",
                "     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM",
                "     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.",
                "     */",
                "    opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/parse_spec_ctrl",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/8c24b70fedcb52633b2370f834d8a2be3f7fa38e",
        "commit_title": "x86/spec-ctrl: Add spec-ctrl=unpriv-mmio",
        "commit_text": " Per Xen's support statement, PCI passthrough should be to trusted domains because the overall system security depends on factors outside of Xen's control.  As such, Xen, in a supported configuration, is not vulnerable to DRPW/SBDR.  However, users who have risk assessed their configuration may be happy with the risk of DoS, but unhappy with the risk of cross-domain data leakage.  Such users should enable this option.  On CPUs vulnerable to MDS, the existing mitigations are the best we can do to mitigate MMIO cross-domain data leakage.  On CPUs fixed to MDS but vulnerable MMIO stale data leakage, this option:   * On CPUs susceptible to FBSDP, mitigates cross-domain fill buffer leakage    using FB_CLEAR.  * On CPUs susceptible to SBDR, mitigates RNG data recovery by engaging the    srb-lock, previously used to mitigate SRBDS.  Both mitigations require microcode from IPU 2022.1, May 2022.  This is part of XSA-404. ",
        "func_before": "static int __init cf_check parse_spec_ctrl(const char *s)\n{\n    const char *ss;\n    int val, rc = 0;\n\n    do {\n        ss = strchr(s, ',');\n        if ( !ss )\n            ss = strchr(s, '\\0');\n\n        /* Global and Xen-wide disable. */\n        val = parse_bool(s, ss);\n        if ( !val )\n        {\n            opt_msr_sc_pv = false;\n            opt_msr_sc_hvm = false;\n\n            opt_eager_fpu = 0;\n\n            if ( opt_xpti_hwdom < 0 )\n                opt_xpti_hwdom = 0;\n            if ( opt_xpti_domu < 0 )\n                opt_xpti_domu = 0;\n\n            if ( opt_smt < 0 )\n                opt_smt = 1;\n\n            if ( opt_pv_l1tf_hwdom < 0 )\n                opt_pv_l1tf_hwdom = 0;\n            if ( opt_pv_l1tf_domu < 0 )\n                opt_pv_l1tf_domu = 0;\n\n            if ( opt_tsx == -1 )\n                opt_tsx = -3;\n\n        disable_common:\n            opt_rsb_pv = false;\n            opt_rsb_hvm = false;\n            opt_md_clear_pv = 0;\n            opt_md_clear_hvm = 0;\n\n            opt_thunk = THUNK_JMP;\n            opt_ibrs = 0;\n            opt_ibpb = false;\n            opt_ssbd = false;\n            opt_l1d_flush = 0;\n            opt_branch_harden = false;\n            opt_srb_lock = 0;\n        }\n        else if ( val > 0 )\n            rc = -EINVAL;\n        else if ( (val = parse_boolean(\"xen\", s, ss)) >= 0 )\n        {\n            if ( !val )\n                goto disable_common;\n\n            rc = -EINVAL;\n        }\n\n        /* Xen's alternative blocks. */\n        else if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_rsb_pv = val;\n            opt_md_clear_pv = val;\n        }\n        else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_hvm = val;\n            opt_rsb_hvm = val;\n            opt_md_clear_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"msr-sc\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_msr_sc_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"rsb\", s, ss)) >= 0 )\n        {\n            opt_rsb_pv = val;\n            opt_rsb_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"md-clear\", s, ss)) >= 0 )\n        {\n            opt_md_clear_pv = val;\n            opt_md_clear_hvm = val;\n        }\n\n        /* Xen's speculative sidechannel mitigation settings. */\n        else if ( !strncmp(s, \"bti-thunk=\", 10) )\n        {\n            s += 10;\n\n            if ( !cmdline_strcmp(s, \"retpoline\") )\n                opt_thunk = THUNK_RETPOLINE;\n            else if ( !cmdline_strcmp(s, \"lfence\") )\n                opt_thunk = THUNK_LFENCE;\n            else if ( !cmdline_strcmp(s, \"jmp\") )\n                opt_thunk = THUNK_JMP;\n            else\n                rc = -EINVAL;\n        }\n        else if ( (val = parse_boolean(\"ibrs\", s, ss)) >= 0 )\n            opt_ibrs = val;\n        else if ( (val = parse_boolean(\"ibpb\", s, ss)) >= 0 )\n            opt_ibpb = val;\n        else if ( (val = parse_boolean(\"ssbd\", s, ss)) >= 0 )\n            opt_ssbd = val;\n        else if ( (val = parse_boolean(\"eager-fpu\", s, ss)) >= 0 )\n            opt_eager_fpu = val;\n        else if ( (val = parse_boolean(\"l1d-flush\", s, ss)) >= 0 )\n            opt_l1d_flush = val;\n        else if ( (val = parse_boolean(\"branch-harden\", s, ss)) >= 0 )\n            opt_branch_harden = val;\n        else if ( (val = parse_boolean(\"srb-lock\", s, ss)) >= 0 )\n            opt_srb_lock = val;\n        else\n            rc = -EINVAL;\n\n        s = ss + 1;\n    } while ( *ss );\n\n    return rc;\n}",
        "func": "static int __init cf_check parse_spec_ctrl(const char *s)\n{\n    const char *ss;\n    int val, rc = 0;\n\n    do {\n        ss = strchr(s, ',');\n        if ( !ss )\n            ss = strchr(s, '\\0');\n\n        /* Global and Xen-wide disable. */\n        val = parse_bool(s, ss);\n        if ( !val )\n        {\n            opt_msr_sc_pv = false;\n            opt_msr_sc_hvm = false;\n\n            opt_eager_fpu = 0;\n\n            if ( opt_xpti_hwdom < 0 )\n                opt_xpti_hwdom = 0;\n            if ( opt_xpti_domu < 0 )\n                opt_xpti_domu = 0;\n\n            if ( opt_smt < 0 )\n                opt_smt = 1;\n\n            if ( opt_pv_l1tf_hwdom < 0 )\n                opt_pv_l1tf_hwdom = 0;\n            if ( opt_pv_l1tf_domu < 0 )\n                opt_pv_l1tf_domu = 0;\n\n            if ( opt_tsx == -1 )\n                opt_tsx = -3;\n\n        disable_common:\n            opt_rsb_pv = false;\n            opt_rsb_hvm = false;\n            opt_md_clear_pv = 0;\n            opt_md_clear_hvm = 0;\n\n            opt_thunk = THUNK_JMP;\n            opt_ibrs = 0;\n            opt_ibpb = false;\n            opt_ssbd = false;\n            opt_l1d_flush = 0;\n            opt_branch_harden = false;\n            opt_srb_lock = 0;\n        }\n        else if ( val > 0 )\n            rc = -EINVAL;\n        else if ( (val = parse_boolean(\"xen\", s, ss)) >= 0 )\n        {\n            if ( !val )\n                goto disable_common;\n\n            rc = -EINVAL;\n        }\n\n        /* Xen's alternative blocks. */\n        else if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_rsb_pv = val;\n            opt_md_clear_pv = val;\n        }\n        else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_hvm = val;\n            opt_rsb_hvm = val;\n            opt_md_clear_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"msr-sc\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_msr_sc_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"rsb\", s, ss)) >= 0 )\n        {\n            opt_rsb_pv = val;\n            opt_rsb_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"md-clear\", s, ss)) >= 0 )\n        {\n            opt_md_clear_pv = val;\n            opt_md_clear_hvm = val;\n        }\n\n        /* Xen's speculative sidechannel mitigation settings. */\n        else if ( !strncmp(s, \"bti-thunk=\", 10) )\n        {\n            s += 10;\n\n            if ( !cmdline_strcmp(s, \"retpoline\") )\n                opt_thunk = THUNK_RETPOLINE;\n            else if ( !cmdline_strcmp(s, \"lfence\") )\n                opt_thunk = THUNK_LFENCE;\n            else if ( !cmdline_strcmp(s, \"jmp\") )\n                opt_thunk = THUNK_JMP;\n            else\n                rc = -EINVAL;\n        }\n        else if ( (val = parse_boolean(\"ibrs\", s, ss)) >= 0 )\n            opt_ibrs = val;\n        else if ( (val = parse_boolean(\"ibpb\", s, ss)) >= 0 )\n            opt_ibpb = val;\n        else if ( (val = parse_boolean(\"ssbd\", s, ss)) >= 0 )\n            opt_ssbd = val;\n        else if ( (val = parse_boolean(\"eager-fpu\", s, ss)) >= 0 )\n            opt_eager_fpu = val;\n        else if ( (val = parse_boolean(\"l1d-flush\", s, ss)) >= 0 )\n            opt_l1d_flush = val;\n        else if ( (val = parse_boolean(\"branch-harden\", s, ss)) >= 0 )\n            opt_branch_harden = val;\n        else if ( (val = parse_boolean(\"srb-lock\", s, ss)) >= 0 )\n            opt_srb_lock = val;\n        else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )\n            opt_unpriv_mmio = val;\n        else\n            rc = -EINVAL;\n\n        s = ss + 1;\n    } while ( *ss );\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -114,6 +114,8 @@\n             opt_branch_harden = val;\n         else if ( (val = parse_boolean(\"srb-lock\", s, ss)) >= 0 )\n             opt_srb_lock = val;\n+        else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )\n+            opt_unpriv_mmio = val;\n         else\n             rc = -EINVAL;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )",
                "            opt_unpriv_mmio = val;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/spec_ctrl_init_domain",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/8c24b70fedcb52633b2370f834d8a2be3f7fa38e",
        "commit_title": "x86/spec-ctrl: Add spec-ctrl=unpriv-mmio",
        "commit_text": " Per Xen's support statement, PCI passthrough should be to trusted domains because the overall system security depends on factors outside of Xen's control.  As such, Xen, in a supported configuration, is not vulnerable to DRPW/SBDR.  However, users who have risk assessed their configuration may be happy with the risk of DoS, but unhappy with the risk of cross-domain data leakage.  Such users should enable this option.  On CPUs vulnerable to MDS, the existing mitigations are the best we can do to mitigate MMIO cross-domain data leakage.  On CPUs fixed to MDS but vulnerable MMIO stale data leakage, this option:   * On CPUs susceptible to FBSDP, mitigates cross-domain fill buffer leakage    using FB_CLEAR.  * On CPUs susceptible to SBDR, mitigates RNG data recovery by engaging the    srb-lock, previously used to mitigate SRBDS.  Both mitigations require microcode from IPU 2022.1, May 2022.  This is part of XSA-404. ",
        "func_before": "void spec_ctrl_init_domain(struct domain *d)\n{\n    bool pv = is_pv_domain(d);\n\n    d->arch.verw = pv ? opt_md_clear_pv : opt_md_clear_hvm;\n}",
        "func": "void spec_ctrl_init_domain(struct domain *d)\n{\n    bool pv = is_pv_domain(d);\n\n    d->arch.verw =\n        (pv ? opt_md_clear_pv : opt_md_clear_hvm) ||\n        (opt_fb_clear_mmio && is_iommu_enabled(d));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,5 +2,7 @@\n {\n     bool pv = is_pv_domain(d);\n \n-    d->arch.verw = pv ? opt_md_clear_pv : opt_md_clear_hvm;\n+    d->arch.verw =\n+        (pv ? opt_md_clear_pv : opt_md_clear_hvm) ||\n+        (opt_fb_clear_mmio && is_iommu_enabled(d));\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    d->arch.verw = pv ? opt_md_clear_pv : opt_md_clear_hvm;"
            ],
            "added_lines": [
                "    d->arch.verw =",
                "        (pv ? opt_md_clear_pv : opt_md_clear_hvm) ||",
                "        (opt_fb_clear_mmio && is_iommu_enabled(d));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/init_speculation_mitigations",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/8c24b70fedcb52633b2370f834d8a2be3f7fa38e",
        "commit_title": "x86/spec-ctrl: Add spec-ctrl=unpriv-mmio",
        "commit_text": " Per Xen's support statement, PCI passthrough should be to trusted domains because the overall system security depends on factors outside of Xen's control.  As such, Xen, in a supported configuration, is not vulnerable to DRPW/SBDR.  However, users who have risk assessed their configuration may be happy with the risk of DoS, but unhappy with the risk of cross-domain data leakage.  Such users should enable this option.  On CPUs vulnerable to MDS, the existing mitigations are the best we can do to mitigate MMIO cross-domain data leakage.  On CPUs fixed to MDS but vulnerable MMIO stale data leakage, this option:   * On CPUs susceptible to FBSDP, mitigates cross-domain fill buffer leakage    using FB_CLEAR.  * On CPUs susceptible to SBDR, mitigates RNG data recovery by engaging the    srb-lock, previously used to mitigate SRBDS.  Both mitigations require microcode from IPU 2022.1, May 2022.  This is part of XSA-404. ",
        "func_before": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa;\n    uint64_t caps = 0;\n\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n        else if ( opt_ibrs == -1 )\n        {\n            opt_ibrs = ibrs = true;\n            default_xen_spec_ctrl |= SPEC_CTRL_IBRS | SPEC_CTRL_STIBP;\n        }\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe(caps) )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_wrmsr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* If we have IBRS available, see whether we should use it. */\n    if ( has_spec_ctrl && ibrs )\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n\n    /* If we have SSBD available, see whether we should use it. */\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 && boot_cpu_has(X86_FEATURE_XEN_SMEP) &&\n         !opt_pv32 && rsb_is_full_width() )\n        opt_rsb_pv = 0;\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n        opt_ibpb = false;\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /* If Xen is using any MSR_SPEC_CTRL settings, adjust the idle path. */\n    if ( default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default(caps);\n\n    l1tf_calculations(caps);\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !(caps & ARCH_CAPS_SKIP_L1DFL);\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations(caps);\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS defences as applicable.  The Idle blocks need using if\n     * either PV or HVM defences are used.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || (caps & ARCH_CAPS_TSX_CTRL)) &&\n        (caps & (ARCH_CAPS_MDS_NO | ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && (caps & ARCH_CAPS_TSX_CTRL) &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * On parts which enumerate MDS_NO and not TAA_NO, TSX is the only known\n     * way to access the Fill Buffer.  If TSX isn't available (inc. SKU\n     * reasons on some models), or TSX is explicitly disabled, then there is\n     * no need for the extra overhead to protect RDRAND/RDSEED.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 &&\n             (caps & (ARCH_CAPS_MDS_NO|ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO &&\n             (!cpu_has_hle || ((caps & ARCH_CAPS_TSX_CTRL) && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk, caps);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "func": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa;\n    uint64_t caps = 0;\n\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n        else if ( opt_ibrs == -1 )\n        {\n            opt_ibrs = ibrs = true;\n            default_xen_spec_ctrl |= SPEC_CTRL_IBRS | SPEC_CTRL_STIBP;\n        }\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe(caps) )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_wrmsr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_wrmsr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* If we have IBRS available, see whether we should use it. */\n    if ( has_spec_ctrl && ibrs )\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n\n    /* If we have SSBD available, see whether we should use it. */\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 && boot_cpu_has(X86_FEATURE_XEN_SMEP) &&\n         !opt_pv32 && rsb_is_full_width() )\n        opt_rsb_pv = 0;\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n        opt_ibpb = false;\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /* If Xen is using any MSR_SPEC_CTRL settings, adjust the idle path. */\n    if ( default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default(caps);\n\n    l1tf_calculations(caps);\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !(caps & ARCH_CAPS_SKIP_L1DFL);\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations(caps);\n\n    /*\n     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n     * reintroduced the VERW fill buffer flushing side effect because of a\n     * susceptibility to FBSDP.\n     *\n     * If unprivileged guests have (or will have) MMIO mappings, we can\n     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n     * the return-to-guest path.\n     */\n    if ( opt_unpriv_mmio )\n        opt_fb_clear_mmio = caps & ARCH_CAPS_FB_CLEAR;\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n     * either the PV or HVM MDS defences are used, or if we may give MMIO\n     * access to untrusted guests.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || (caps & ARCH_CAPS_TSX_CTRL)) &&\n        (caps & (ARCH_CAPS_MDS_NO | ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && (caps & ARCH_CAPS_TSX_CTRL) &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n     * data becomes available to other contexts.  To recover the data, an\n     * attacker needs to use:\n     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n     *  - DRPW (Architecturally latch stale fill buffer data)\n     *\n     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n             (caps & (ARCH_CAPS_MDS_NO|ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO &&\n             (!cpu_has_hle || ((caps & ARCH_CAPS_TSX_CTRL) && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk, caps);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -250,6 +250,18 @@\n     mds_calculations(caps);\n \n     /*\n+     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n+     * reintroduced the VERW fill buffer flushing side effect because of a\n+     * susceptibility to FBSDP.\n+     *\n+     * If unprivileged guests have (or will have) MMIO mappings, we can\n+     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n+     * the return-to-guest path.\n+     */\n+    if ( opt_unpriv_mmio )\n+        opt_fb_clear_mmio = caps & ARCH_CAPS_FB_CLEAR;\n+\n+    /*\n      * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n      * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n      * but it is somewhat better than nothing.\n@@ -262,18 +274,20 @@\n                             boot_cpu_has(X86_FEATURE_MD_CLEAR));\n \n     /*\n-     * Enable MDS defences as applicable.  The Idle blocks need using if\n-     * either PV or HVM defences are used.\n+     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n+     * either the PV or HVM MDS defences are used, or if we may give MMIO\n+     * access to untrusted guests.\n      *\n      * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n      * equivalent semantics to avoid needing to perform both flushes on the\n-     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.\n+     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n+     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n      *\n      * After calculating the appropriate idle setting, simplify\n      * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n      * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n      */\n-    if ( opt_md_clear_pv || opt_md_clear_hvm )\n+    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n         setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n     opt_md_clear_hvm &= !(caps & ARCH_CAPS_SKIP_L1DFL) && !opt_l1d_flush;\n \n@@ -338,14 +352,19 @@\n      * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n      * default.\n      *\n-     * On parts which enumerate MDS_NO and not TAA_NO, TSX is the only known\n-     * way to access the Fill Buffer.  If TSX isn't available (inc. SKU\n-     * reasons on some models), or TSX is explicitly disabled, then there is\n-     * no need for the extra overhead to protect RDRAND/RDSEED.\n+     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n+     * data becomes available to other contexts.  To recover the data, an\n+     * attacker needs to use:\n+     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n+     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n+     *  - DRPW (Architecturally latch stale fill buffer data)\n+     *\n+     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n+     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n      */\n     if ( cpu_has_srbds_ctrl )\n     {\n-        if ( opt_srb_lock == -1 &&\n+        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n              (caps & (ARCH_CAPS_MDS_NO|ARCH_CAPS_TAA_NO)) == ARCH_CAPS_MDS_NO &&\n              (!cpu_has_hle || ((caps & ARCH_CAPS_TSX_CTRL) && rtm_disabled)) )\n             opt_srb_lock = 0;",
        "diff_line_info": {
            "deleted_lines": [
                "     * Enable MDS defences as applicable.  The Idle blocks need using if",
                "     * either PV or HVM defences are used.",
                "     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH.",
                "    if ( opt_md_clear_pv || opt_md_clear_hvm )",
                "     * On parts which enumerate MDS_NO and not TAA_NO, TSX is the only known",
                "     * way to access the Fill Buffer.  If TSX isn't available (inc. SKU",
                "     * reasons on some models), or TSX is explicitly disabled, then there is",
                "     * no need for the extra overhead to protect RDRAND/RDSEED.",
                "        if ( opt_srb_lock == -1 &&"
            ],
            "added_lines": [
                "     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have",
                "     * reintroduced the VERW fill buffer flushing side effect because of a",
                "     * susceptibility to FBSDP.",
                "     *",
                "     * If unprivileged guests have (or will have) MMIO mappings, we can",
                "     * mitigate cross-domain leakage of fill buffer data by issuing VERW on",
                "     * the return-to-guest path.",
                "     */",
                "    if ( opt_unpriv_mmio )",
                "        opt_fb_clear_mmio = caps & ARCH_CAPS_FB_CLEAR;",
                "",
                "    /*",
                "     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if",
                "     * either the PV or HVM MDS defences are used, or if we may give MMIO",
                "     * access to untrusted guests.",
                "     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for",
                "     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)",
                "    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )",
                "     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG",
                "     * data becomes available to other contexts.  To recover the data, an",
                "     * attacker needs to use:",
                "     *  - SBDS (MDS or TAA to sample the cores fill buffer)",
                "     *  - SBDR (Architecturally retrieve stale transaction buffer contents)",
                "     *  - DRPW (Architecturally latch stale fill buffer data)",
                "     *",
                "     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there",
                "     * is no unprivileged MMIO access, the RNG data doesn't need protecting.",
                "        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/print_details",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/8c24b70fedcb52633b2370f834d8a2be3f7fa38e",
        "commit_title": "x86/spec-ctrl: Add spec-ctrl=unpriv-mmio",
        "commit_text": " Per Xen's support statement, PCI passthrough should be to trusted domains because the overall system security depends on factors outside of Xen's control.  As such, Xen, in a supported configuration, is not vulnerable to DRPW/SBDR.  However, users who have risk assessed their configuration may be happy with the risk of DoS, but unhappy with the risk of cross-domain data leakage.  Such users should enable this option.  On CPUs vulnerable to MDS, the existing mitigations are the best we can do to mitigate MMIO cross-domain data leakage.  On CPUs fixed to MDS but vulnerable MMIO stale data leakage, this option:   * On CPUs susceptible to FBSDP, mitigates cross-domain fill buffer leakage    using FB_CLEAR.  * On CPUs susceptible to SBDR, mitigates RNG data recovery by engaging the    srb-lock, previously used to mitigate SRBDS.  Both mitigations require microcode from IPU 2022.1, May 2022.  This is part of XSA-404. ",
        "func_before": "static void __init print_details(enum ind_thunk thunk, uint64_t caps)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_IBRS_ALL)                       ? \" IBRS_ALL\"       : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb                                  ? \" IBPB\"  : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm       ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)   ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)  ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "func": "static void __init print_details(enum ind_thunk thunk, uint64_t caps)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_IBRS_ALL)                       ? \" IBRS_ALL\"       : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb                                  ? \" IBPB\"  : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm ||\n           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)   ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)  ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -85,7 +85,8 @@\n            opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n            opt_ibpb                                  ? \" IBPB\"  : \"\",\n            opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n-           opt_md_clear_pv || opt_md_clear_hvm       ? \" VERW\"  : \"\",\n+           opt_md_clear_pv || opt_md_clear_hvm ||\n+           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n            opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n \n     /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */",
        "diff_line_info": {
            "deleted_lines": [
                "           opt_md_clear_pv || opt_md_clear_hvm       ? \" VERW\"  : \"\","
            ],
            "added_lines": [
                "           opt_md_clear_pv || opt_md_clear_hvm ||",
                "           opt_fb_clear_mmio                         ? \" VERW\"  : \"\","
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21124",
        "func_name": "xen-project/xen/print_details",
        "description": "Out-of-bounds write vulnerability in CX-Programmer v9.76.1 and earlier which is a part of CX-One (v4.60) suite allows an attacker to cause information disclosure and/or arbitrary code execution by having a user to open a specially crafted CXP file. This vulnerability is different from CVE-2022-25234.",
        "git_url": "https://github.com/xen-project/xen/commit/2ebe8fe9b7e0d36e9ec3cfe4552b2b197ef0dcec",
        "commit_title": "x86/spec-ctrl: Enumeration for MMIO Stale Data controls",
        "commit_text": " The three *_NO bits indicate non-susceptibility to the SSDP, FBSDP and PSDP data movement primitives.  FB_CLEAR indicates that the VERW instruction has re-gained it's Fill Buffer flushing side effect.  This is only enumerated on parts where VERW had previously lost it's flushing side effect due to the MDS/TAA vulnerabilities being fixed in hardware.  FB_CLEAR_CTRL is available on a subset of FB_CLEAR parts where the Fill Buffer clearing side effect of VERW can be turned off for performance reasons.  This is part of XSA-404. ",
        "func_before": "static void __init print_details(enum ind_thunk thunk, uint64_t caps)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_IBRS_ALL)                       ? \" IBRS_ALL\"       : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb                                  ? \" IBPB\"  : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm       ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)   ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)  ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "func": "static void __init print_details(enum ind_thunk thunk, uint64_t caps)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_IBRS_ALL)                       ? \" IBRS_ALL\"       : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb                                  ? \" IBPB\"  : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm       ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)   ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_MD_CLEAR)  ||\n            opt_eager_fpu)                           ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           boot_cpu_has(X86_FEATURE_MD_CLEAR)        ? \" MD_CLEAR\"      : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,7 +16,7 @@\n      * Hardware read-only information, stating immunity to certain issues, or\n      * suggestions of which mitigation to use.\n      */\n-    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n            (caps & ARCH_CAPS_IBRS_ALL)                       ? \" IBRS_ALL\"       : \"\",\n            (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n@@ -25,13 +25,16 @@\n            (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n            (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n            (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n+           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n+           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n+           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\");\n \n     /* Hardware features which need driving to mitigate issues. */\n-    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n            (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n@@ -46,7 +49,9 @@\n            (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n            (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n-           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\");\n+           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n+           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n+           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n \n     /* Compiled-in support which pertains to mitigations. */\n     if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )",
        "diff_line_info": {
            "deleted_lines": [
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\");"
            ],
            "added_lines": [
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",",
                "           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",",
                "           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",",
                "           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",",
                "           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1473",
        "func_name": "openssl/OPENSSL_LH_flush",
        "description": "The OPENSSL_LH_flush() function, which empties a hash table, contains a bug that breaks reuse of the memory occuppied by the removed hash table entries. This function is used when decoding certificates or keys. If a long lived process periodically decodes certificates or keys its memory usage will expand without bounds and the process might be terminated by the operating system causing a denial of service. Also traversing the empty hash table entries will take increasingly more time. Typically such long lived processes might be TLS clients or TLS servers configured to accept client certificate authentication. The function was added in the OpenSSL 3.0 version thus older releases are not affected by the issue. Fixed in OpenSSL 3.0.3 (Affected 3.0.0,3.0.1,3.0.2).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=64c85430f95200b6b51fe9475bd5203f7c19daf1",
        "commit_title": "",
        "commit_text": "Fix bug in OPENSSL_LH_flush  Fixes #18139.  (Merged from https://github.com/openssl/openssl/pull/18141)  (cherry picked from commit e5da68183410c06f7b350a0721bc2bd6057e438e) ",
        "func_before": "void OPENSSL_LH_flush(OPENSSL_LHASH *lh)\n{\n    unsigned int i;\n    OPENSSL_LH_NODE *n, *nn;\n\n    if (lh == NULL)\n        return;\n\n    for (i = 0; i < lh->num_nodes; i++) {\n        n = lh->b[i];\n        while (n != NULL) {\n            nn = n->next;\n            OPENSSL_free(n);\n            n = nn;\n        }\n        lh->b[i] = NULL;\n    }\n}",
        "func": "void OPENSSL_LH_flush(OPENSSL_LHASH *lh)\n{\n    unsigned int i;\n    OPENSSL_LH_NODE *n, *nn;\n\n    if (lh == NULL)\n        return;\n\n    for (i = 0; i < lh->num_nodes; i++) {\n        n = lh->b[i];\n        while (n != NULL) {\n            nn = n->next;\n            OPENSSL_free(n);\n            n = nn;\n        }\n        lh->b[i] = NULL;\n    }\n\n    lh->num_items = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,4 +15,6 @@\n         }\n         lh->b[i] = NULL;\n     }\n+\n+    lh->num_items = 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    lh->num_items = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18281",
        "func_name": "torvalds/linux/move_huge_pmd",
        "description": "Since Linux kernel version 3.2, the mremap() syscall performs TLB flushes after dropping pagetable locks. If a syscall such as ftruncate() removes entries from the pagetables of a task that is in the middle of mremap(), a stale TLB entry can remain for a short time that permits access to a physical page after it has been released back to the page allocator and reused. This is fixed in the following kernel versions: 4.9.135, 4.14.78, 4.18.16, 4.19.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=eb66ae030829605d61fbef1909ce310e29f78821",
        "commit_title": "Jann Horn points out that our TLB flushing was subtly wrong for the",
        "commit_text": "mremap() case.  What makes mremap() special is that we don't follow the usual \"add page to list of pages to be freed, then flush tlb, and then free pages\".  No, mremap() obviously just _moves_ the page from one page table location to another.  That matters, because mremap() thus doesn't directly control the lifetime of the moved page with a freelist: instead, the lifetime of the page is controlled by the page table locking, that serializes access to the entry.  As a result, we need to flush the TLB not just before releasing the lock for the source location (to avoid any concurrent accesses to the entry), but also before we release the destination page table lock (to avoid the TLB being flushed after somebody else has already done something to that page).  This also makes the whole \"need_flush\" logic unnecessary, since we now always end up flushing the TLB for every valid entry.  Reported-and-tested-by: Jann Horn <jannh@google.com> ",
        "func_before": "bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t  unsigned long new_addr, unsigned long old_end,\n\t\t  pmd_t *old_pmd, pmd_t *new_pmd, bool *need_flush)\n{\n\tspinlock_t *old_ptl, *new_ptl;\n\tpmd_t pmd;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tbool force_flush = false;\n\n\tif ((old_addr & ~HPAGE_PMD_MASK) ||\n\t    (new_addr & ~HPAGE_PMD_MASK) ||\n\t    old_end - old_addr < HPAGE_PMD_SIZE)\n\t\treturn false;\n\n\t/*\n\t * The destination pmd shouldn't be established, free_pgtables()\n\t * should have release it.\n\t */\n\tif (WARN_ON(!pmd_none(*new_pmd))) {\n\t\tVM_BUG_ON(pmd_trans_huge(*new_pmd));\n\t\treturn false;\n\t}\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * ptlocks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_ptl = __pmd_trans_huge_lock(old_pmd, vma);\n\tif (old_ptl) {\n\t\tnew_ptl = pmd_lockptr(mm, new_pmd);\n\t\tif (new_ptl != old_ptl)\n\t\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\t\tpmd = pmdp_huge_get_and_clear(mm, old_addr, old_pmd);\n\t\tif (pmd_present(pmd) && pmd_dirty(pmd))\n\t\t\tforce_flush = true;\n\t\tVM_BUG_ON(!pmd_none(*new_pmd));\n\n\t\tif (pmd_move_must_withdraw(new_ptl, old_ptl, vma)) {\n\t\t\tpgtable_t pgtable;\n\t\t\tpgtable = pgtable_trans_huge_withdraw(mm, old_pmd);\n\t\t\tpgtable_trans_huge_deposit(mm, new_pmd, pgtable);\n\t\t}\n\t\tpmd = move_soft_dirty_pmd(pmd);\n\t\tset_pmd_at(mm, new_addr, new_pmd, pmd);\n\t\tif (new_ptl != old_ptl)\n\t\t\tspin_unlock(new_ptl);\n\t\tif (force_flush)\n\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);\n\t\telse\n\t\t\t*need_flush = true;\n\t\tspin_unlock(old_ptl);\n\t\treturn true;\n\t}\n\treturn false;\n}",
        "func": "bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t  unsigned long new_addr, unsigned long old_end,\n\t\t  pmd_t *old_pmd, pmd_t *new_pmd)\n{\n\tspinlock_t *old_ptl, *new_ptl;\n\tpmd_t pmd;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tbool force_flush = false;\n\n\tif ((old_addr & ~HPAGE_PMD_MASK) ||\n\t    (new_addr & ~HPAGE_PMD_MASK) ||\n\t    old_end - old_addr < HPAGE_PMD_SIZE)\n\t\treturn false;\n\n\t/*\n\t * The destination pmd shouldn't be established, free_pgtables()\n\t * should have release it.\n\t */\n\tif (WARN_ON(!pmd_none(*new_pmd))) {\n\t\tVM_BUG_ON(pmd_trans_huge(*new_pmd));\n\t\treturn false;\n\t}\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * ptlocks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_ptl = __pmd_trans_huge_lock(old_pmd, vma);\n\tif (old_ptl) {\n\t\tnew_ptl = pmd_lockptr(mm, new_pmd);\n\t\tif (new_ptl != old_ptl)\n\t\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\t\tpmd = pmdp_huge_get_and_clear(mm, old_addr, old_pmd);\n\t\tif (pmd_present(pmd))\n\t\t\tforce_flush = true;\n\t\tVM_BUG_ON(!pmd_none(*new_pmd));\n\n\t\tif (pmd_move_must_withdraw(new_ptl, old_ptl, vma)) {\n\t\t\tpgtable_t pgtable;\n\t\t\tpgtable = pgtable_trans_huge_withdraw(mm, old_pmd);\n\t\t\tpgtable_trans_huge_deposit(mm, new_pmd, pgtable);\n\t\t}\n\t\tpmd = move_soft_dirty_pmd(pmd);\n\t\tset_pmd_at(mm, new_addr, new_pmd, pmd);\n\t\tif (force_flush)\n\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);\n\t\tif (new_ptl != old_ptl)\n\t\t\tspin_unlock(new_ptl);\n\t\tspin_unlock(old_ptl);\n\t\treturn true;\n\t}\n\treturn false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,\n \t\t  unsigned long new_addr, unsigned long old_end,\n-\t\t  pmd_t *old_pmd, pmd_t *new_pmd, bool *need_flush)\n+\t\t  pmd_t *old_pmd, pmd_t *new_pmd)\n {\n \tspinlock_t *old_ptl, *new_ptl;\n \tpmd_t pmd;\n@@ -31,7 +31,7 @@\n \t\tif (new_ptl != old_ptl)\n \t\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n \t\tpmd = pmdp_huge_get_and_clear(mm, old_addr, old_pmd);\n-\t\tif (pmd_present(pmd) && pmd_dirty(pmd))\n+\t\tif (pmd_present(pmd))\n \t\t\tforce_flush = true;\n \t\tVM_BUG_ON(!pmd_none(*new_pmd));\n \n@@ -42,12 +42,10 @@\n \t\t}\n \t\tpmd = move_soft_dirty_pmd(pmd);\n \t\tset_pmd_at(mm, new_addr, new_pmd, pmd);\n+\t\tif (force_flush)\n+\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);\n \t\tif (new_ptl != old_ptl)\n \t\t\tspin_unlock(new_ptl);\n-\t\tif (force_flush)\n-\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);\n-\t\telse\n-\t\t\t*need_flush = true;\n \t\tspin_unlock(old_ptl);\n \t\treturn true;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t  pmd_t *old_pmd, pmd_t *new_pmd, bool *need_flush)",
                "\t\tif (pmd_present(pmd) && pmd_dirty(pmd))",
                "\t\tif (force_flush)",
                "\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);",
                "\t\telse",
                "\t\t\t*need_flush = true;"
            ],
            "added_lines": [
                "\t\t  pmd_t *old_pmd, pmd_t *new_pmd)",
                "\t\tif (pmd_present(pmd))",
                "\t\tif (force_flush)",
                "\t\t\tflush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18281",
        "func_name": "torvalds/linux/move_page_tables",
        "description": "Since Linux kernel version 3.2, the mremap() syscall performs TLB flushes after dropping pagetable locks. If a syscall such as ftruncate() removes entries from the pagetables of a task that is in the middle of mremap(), a stale TLB entry can remain for a short time that permits access to a physical page after it has been released back to the page allocator and reused. This is fixed in the following kernel versions: 4.9.135, 4.14.78, 4.18.16, 4.19.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=eb66ae030829605d61fbef1909ce310e29f78821",
        "commit_title": "Jann Horn points out that our TLB flushing was subtly wrong for the",
        "commit_text": "mremap() case.  What makes mremap() special is that we don't follow the usual \"add page to list of pages to be freed, then flush tlb, and then free pages\".  No, mremap() obviously just _moves_ the page from one page table location to another.  That matters, because mremap() thus doesn't directly control the lifetime of the moved page with a freelist: instead, the lifetime of the page is controlled by the page table locking, that serializes access to the entry.  As a result, we need to flush the TLB not just before releasing the lock for the source location (to avoid any concurrent accesses to the entry), but also before we release the destination page table lock (to avoid the TLB being flushed after somebody else has already done something to that page).  This also makes the whole \"need_flush\" logic unnecessary, since we now always end up flushing the TLB for every valid entry.  Reported-and-tested-by: Jann Horn <jannh@google.com> ",
        "func_before": "unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks)\n{\n\tunsigned long extent, next, old_end;\n\tpmd_t *old_pmd, *new_pmd;\n\tbool need_flush = false;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\n\tmmun_start = old_addr;\n\tmmun_end   = old_end;\n\tmmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);\n\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\tnext = (old_addr + PMD_SIZE) & PMD_MASK;\n\t\t/* even if next overflowed, extent below will be ok */\n\t\textent = next - old_addr;\n\t\tif (extent > old_end - old_addr)\n\t\t\textent = old_end - old_addr;\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE) {\n\t\t\t\tbool moved;\n\t\t\t\t/* See comment in move_ptes() */\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\ttake_rmap_locks(vma);\n\t\t\t\tmoved = move_huge_pmd(vma, old_addr, new_addr,\n\t\t\t\t\t\t    old_end, old_pmd, new_pmd,\n\t\t\t\t\t\t    &need_flush);\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\tdrop_rmap_locks(vma);\n\t\t\t\tif (moved)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);\n\t\t\tif (pmd_trans_unstable(old_pmd))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (pte_alloc(new_vma->vm_mm, new_pmd, new_addr))\n\t\t\tbreak;\n\t\tnext = (new_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (extent > next - new_addr)\n\t\t\textent = next - new_addr;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\n\t\t\t  new_pmd, new_addr, need_rmap_locks, &need_flush);\n\t}\n\tif (need_flush)\n\t\tflush_tlb_range(vma, old_end-len, old_addr);\n\n\tmmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);\n\n\treturn len + old_addr - old_end;\t/* how much done */\n}",
        "func": "unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks)\n{\n\tunsigned long extent, next, old_end;\n\tpmd_t *old_pmd, *new_pmd;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\n\tmmun_start = old_addr;\n\tmmun_end   = old_end;\n\tmmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);\n\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\tnext = (old_addr + PMD_SIZE) & PMD_MASK;\n\t\t/* even if next overflowed, extent below will be ok */\n\t\textent = next - old_addr;\n\t\tif (extent > old_end - old_addr)\n\t\t\textent = old_end - old_addr;\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE) {\n\t\t\t\tbool moved;\n\t\t\t\t/* See comment in move_ptes() */\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\ttake_rmap_locks(vma);\n\t\t\t\tmoved = move_huge_pmd(vma, old_addr, new_addr,\n\t\t\t\t\t\t    old_end, old_pmd, new_pmd);\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\tdrop_rmap_locks(vma);\n\t\t\t\tif (moved)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tsplit_huge_pmd(vma, old_pmd, old_addr);\n\t\t\tif (pmd_trans_unstable(old_pmd))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (pte_alloc(new_vma->vm_mm, new_pmd, new_addr))\n\t\t\tbreak;\n\t\tnext = (new_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (extent > next - new_addr)\n\t\t\textent = next - new_addr;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\n\t\t\t  new_pmd, new_addr, need_rmap_locks);\n\t}\n\n\tmmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);\n\n\treturn len + old_addr - old_end;\t/* how much done */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,6 @@\n {\n \tunsigned long extent, next, old_end;\n \tpmd_t *old_pmd, *new_pmd;\n-\tbool need_flush = false;\n \tunsigned long mmun_start;\t/* For mmu_notifiers */\n \tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n \n@@ -36,8 +35,7 @@\n \t\t\t\tif (need_rmap_locks)\n \t\t\t\t\ttake_rmap_locks(vma);\n \t\t\t\tmoved = move_huge_pmd(vma, old_addr, new_addr,\n-\t\t\t\t\t\t    old_end, old_pmd, new_pmd,\n-\t\t\t\t\t\t    &need_flush);\n+\t\t\t\t\t\t    old_end, old_pmd, new_pmd);\n \t\t\t\tif (need_rmap_locks)\n \t\t\t\t\tdrop_rmap_locks(vma);\n \t\t\t\tif (moved)\n@@ -53,10 +51,8 @@\n \t\tif (extent > next - new_addr)\n \t\t\textent = next - new_addr;\n \t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\n-\t\t\t  new_pmd, new_addr, need_rmap_locks, &need_flush);\n+\t\t\t  new_pmd, new_addr, need_rmap_locks);\n \t}\n-\tif (need_flush)\n-\t\tflush_tlb_range(vma, old_end-len, old_addr);\n \n \tmmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tbool need_flush = false;",
                "\t\t\t\t\t\t    old_end, old_pmd, new_pmd,",
                "\t\t\t\t\t\t    &need_flush);",
                "\t\t\t  new_pmd, new_addr, need_rmap_locks, &need_flush);",
                "\tif (need_flush)",
                "\t\tflush_tlb_range(vma, old_end-len, old_addr);"
            ],
            "added_lines": [
                "\t\t\t\t\t\t    old_end, old_pmd, new_pmd);",
                "\t\t\t  new_pmd, new_addr, need_rmap_locks);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-18281",
        "func_name": "torvalds/linux/move_ptes",
        "description": "Since Linux kernel version 3.2, the mremap() syscall performs TLB flushes after dropping pagetable locks. If a syscall such as ftruncate() removes entries from the pagetables of a task that is in the middle of mremap(), a stale TLB entry can remain for a short time that permits access to a physical page after it has been released back to the page allocator and reused. This is fixed in the following kernel versions: 4.9.135, 4.14.78, 4.18.16, 4.19.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=eb66ae030829605d61fbef1909ce310e29f78821",
        "commit_title": "Jann Horn points out that our TLB flushing was subtly wrong for the",
        "commit_text": "mremap() case.  What makes mremap() special is that we don't follow the usual \"add page to list of pages to be freed, then flush tlb, and then free pages\".  No, mremap() obviously just _moves_ the page from one page table location to another.  That matters, because mremap() thus doesn't directly control the lifetime of the moved page with a freelist: instead, the lifetime of the page is controlled by the page table locking, that serializes access to the entry.  As a result, we need to flush the TLB not just before releasing the lock for the source location (to avoid any concurrent accesses to the entry), but also before we release the destination page table lock (to avoid the TLB being flushed after somebody else has already done something to that page).  This also makes the whole \"need_flush\" logic unnecessary, since we now always end up flushing the TLB for every valid entry.  Reported-and-tested-by: Jann Horn <jannh@google.com> ",
        "func_before": "static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n\t\tunsigned long old_addr, unsigned long old_end,\n\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n\t\tunsigned long new_addr, bool need_rmap_locks, bool *need_flush)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *old_pte, *new_pte, pte;\n\tspinlock_t *old_ptl, *new_ptl;\n\tbool force_flush = false;\n\tunsigned long len = old_end - old_addr;\n\n\t/*\n\t * When need_rmap_locks is true, we take the i_mmap_rwsem and anon_vma\n\t * locks to ensure that rmap will always observe either the old or the\n\t * new ptes. This is the easiest way to avoid races with\n\t * truncate_pagecache(), page migration, etc...\n\t *\n\t * When need_rmap_locks is false, we use other ways to avoid\n\t * such races:\n\t *\n\t * - During exec() shift_arg_pages(), we use a specially tagged vma\n\t *   which rmap call sites look for using is_vma_temporary_stack().\n\t *\n\t * - During mremap(), new_vma is often known to be placed after vma\n\t *   in rmap traversal order. This ensures rmap will always observe\n\t *   either the old pte, or the new pte, or both (the page table locks\n\t *   serialize access to individual ptes, but only rmap traversal\n\t *   order guarantees that we won't miss both the old and new ptes).\n\t */\n\tif (need_rmap_locks)\n\t\ttake_rmap_locks(vma);\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * pte locks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\n\tnew_pte = pte_offset_map(new_pmd, new_addr);\n\tnew_ptl = pte_lockptr(mm, new_pmd);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\tflush_tlb_batched_pending(vma->vm_mm);\n\tarch_enter_lazy_mmu_mode();\n\n\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\n\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {\n\t\tif (pte_none(*old_pte))\n\t\t\tcontinue;\n\n\t\tpte = ptep_get_and_clear(mm, old_addr, old_pte);\n\t\t/*\n\t\t * If we are remapping a dirty PTE, make sure\n\t\t * to flush TLB before we drop the PTL for the\n\t\t * old PTE or we may race with page_mkclean().\n\t\t *\n\t\t * This check has to be done after we removed the\n\t\t * old PTE from page tables or another thread may\n\t\t * dirty it after the check and before the removal.\n\t\t */\n\t\tif (pte_present(pte) && pte_dirty(pte))\n\t\t\tforce_flush = true;\n\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n\t\tpte = move_soft_dirty_pte(pte);\n\t\tset_pte_at(mm, new_addr, new_pte, pte);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tpte_unmap(new_pte - 1);\n\tif (force_flush)\n\t\tflush_tlb_range(vma, old_end - len, old_end);\n\telse\n\t\t*need_flush = true;\n\tpte_unmap_unlock(old_pte - 1, old_ptl);\n\tif (need_rmap_locks)\n\t\tdrop_rmap_locks(vma);\n}",
        "func": "static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n\t\tunsigned long old_addr, unsigned long old_end,\n\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n\t\tunsigned long new_addr, bool need_rmap_locks)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *old_pte, *new_pte, pte;\n\tspinlock_t *old_ptl, *new_ptl;\n\tbool force_flush = false;\n\tunsigned long len = old_end - old_addr;\n\n\t/*\n\t * When need_rmap_locks is true, we take the i_mmap_rwsem and anon_vma\n\t * locks to ensure that rmap will always observe either the old or the\n\t * new ptes. This is the easiest way to avoid races with\n\t * truncate_pagecache(), page migration, etc...\n\t *\n\t * When need_rmap_locks is false, we use other ways to avoid\n\t * such races:\n\t *\n\t * - During exec() shift_arg_pages(), we use a specially tagged vma\n\t *   which rmap call sites look for using is_vma_temporary_stack().\n\t *\n\t * - During mremap(), new_vma is often known to be placed after vma\n\t *   in rmap traversal order. This ensures rmap will always observe\n\t *   either the old pte, or the new pte, or both (the page table locks\n\t *   serialize access to individual ptes, but only rmap traversal\n\t *   order guarantees that we won't miss both the old and new ptes).\n\t */\n\tif (need_rmap_locks)\n\t\ttake_rmap_locks(vma);\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * pte locks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\n\tnew_pte = pte_offset_map(new_pmd, new_addr);\n\tnew_ptl = pte_lockptr(mm, new_pmd);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\tflush_tlb_batched_pending(vma->vm_mm);\n\tarch_enter_lazy_mmu_mode();\n\n\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\n\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {\n\t\tif (pte_none(*old_pte))\n\t\t\tcontinue;\n\n\t\tpte = ptep_get_and_clear(mm, old_addr, old_pte);\n\t\t/*\n\t\t * If we are remapping a valid PTE, make sure\n\t\t * to flush TLB before we drop the PTL for the\n\t\t * PTE.\n\t\t *\n\t\t * NOTE! Both old and new PTL matter: the old one\n\t\t * for racing with page_mkclean(), the new one to\n\t\t * make sure the physical page stays valid until\n\t\t * the TLB entry for the old mapping has been\n\t\t * flushed.\n\t\t */\n\t\tif (pte_present(pte))\n\t\t\tforce_flush = true;\n\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n\t\tpte = move_soft_dirty_pte(pte);\n\t\tset_pte_at(mm, new_addr, new_pte, pte);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tif (force_flush)\n\t\tflush_tlb_range(vma, old_end - len, old_end);\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tpte_unmap(new_pte - 1);\n\tpte_unmap_unlock(old_pte - 1, old_ptl);\n\tif (need_rmap_locks)\n\t\tdrop_rmap_locks(vma);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n \t\tunsigned long old_addr, unsigned long old_end,\n \t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n-\t\tunsigned long new_addr, bool need_rmap_locks, bool *need_flush)\n+\t\tunsigned long new_addr, bool need_rmap_locks)\n {\n \tstruct mm_struct *mm = vma->vm_mm;\n \tpte_t *old_pte, *new_pte, pte;\n@@ -49,15 +49,17 @@\n \n \t\tpte = ptep_get_and_clear(mm, old_addr, old_pte);\n \t\t/*\n-\t\t * If we are remapping a dirty PTE, make sure\n+\t\t * If we are remapping a valid PTE, make sure\n \t\t * to flush TLB before we drop the PTL for the\n-\t\t * old PTE or we may race with page_mkclean().\n+\t\t * PTE.\n \t\t *\n-\t\t * This check has to be done after we removed the\n-\t\t * old PTE from page tables or another thread may\n-\t\t * dirty it after the check and before the removal.\n+\t\t * NOTE! Both old and new PTL matter: the old one\n+\t\t * for racing with page_mkclean(), the new one to\n+\t\t * make sure the physical page stays valid until\n+\t\t * the TLB entry for the old mapping has been\n+\t\t * flushed.\n \t\t */\n-\t\tif (pte_present(pte) && pte_dirty(pte))\n+\t\tif (pte_present(pte))\n \t\t\tforce_flush = true;\n \t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n \t\tpte = move_soft_dirty_pte(pte);\n@@ -65,13 +67,11 @@\n \t}\n \n \tarch_leave_lazy_mmu_mode();\n+\tif (force_flush)\n+\t\tflush_tlb_range(vma, old_end - len, old_end);\n \tif (new_ptl != old_ptl)\n \t\tspin_unlock(new_ptl);\n \tpte_unmap(new_pte - 1);\n-\tif (force_flush)\n-\t\tflush_tlb_range(vma, old_end - len, old_end);\n-\telse\n-\t\t*need_flush = true;\n \tpte_unmap_unlock(old_pte - 1, old_ptl);\n \tif (need_rmap_locks)\n \t\tdrop_rmap_locks(vma);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tunsigned long new_addr, bool need_rmap_locks, bool *need_flush)",
                "\t\t * If we are remapping a dirty PTE, make sure",
                "\t\t * old PTE or we may race with page_mkclean().",
                "\t\t * This check has to be done after we removed the",
                "\t\t * old PTE from page tables or another thread may",
                "\t\t * dirty it after the check and before the removal.",
                "\t\tif (pte_present(pte) && pte_dirty(pte))",
                "\tif (force_flush)",
                "\t\tflush_tlb_range(vma, old_end - len, old_end);",
                "\telse",
                "\t\t*need_flush = true;"
            ],
            "added_lines": [
                "\t\tunsigned long new_addr, bool need_rmap_locks)",
                "\t\t * If we are remapping a valid PTE, make sure",
                "\t\t * PTE.",
                "\t\t * NOTE! Both old and new PTL matter: the old one",
                "\t\t * for racing with page_mkclean(), the new one to",
                "\t\t * make sure the physical page stays valid until",
                "\t\t * the TLB entry for the old mapping has been",
                "\t\t * flushed.",
                "\t\tif (pte_present(pte))",
                "\tif (force_flush)",
                "\t\tflush_tlb_range(vma, old_end - len, old_end);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19961",
        "func_name": "xen-project/xen/amd_iommu_map_page",
        "description": "An issue was discovered in Xen through 4.11.x on AMD x86 platforms, possibly allowing guest OS users to gain host OS privileges because TLB flushes do not always occur after IOMMU mapping changes.",
        "git_url": "https://github.com/xen-project/xen/commit/1a7ffe466cd057daaef245b0a1ab6b82588e4c01",
        "commit_title": "amd/iommu: fix flush checks",
        "commit_text": " Flush checking for AMD IOMMU didn't check whether the previous entry was present, or whether the flags (writable/readable) changed in order to decide whether a flush should be executed.  Fix this by taking the writable/readable/next-level fields into account, together with the present bit.  Along these lines the flushing in amd_iommu_map_page() must not be omitted for PV domains. The comment there was simply wrong: Mappings may very well change, both their addresses and their permissions. Ultimately this should honor iommu_dont_flush_iotlb, but to achieve this amd_iommu_ops first needs to gain an .iotlb_flush hook.  Also make clear_iommu_pte_present() static, to demonstrate there's no caller omitting the (subsequent) flush.  This is part of XSA-275. ",
        "func_before": "int amd_iommu_map_page(struct domain *d, dfn_t dfn, mfn_t mfn,\n                       unsigned int flags)\n{\n    bool_t need_flush = 0;\n    struct domain_iommu *hd = dom_iommu(d);\n    int rc;\n    unsigned long pt_mfn[7];\n    unsigned int merge_level;\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    rc = amd_iommu_alloc_root(hd);\n    if ( rc )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Root table alloc failed, dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return rc;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for wider dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        if ( update_paging_mode(d, dfn_x(dfn)) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* Install 4k mapping first */\n    need_flush = set_iommu_pte_present(pt_mfn[1], dfn_x(dfn), mfn_x(mfn), 1,\n                                       !!(flags & IOMMUF_writable),\n                                       !!(flags & IOMMUF_readable));\n\n    /* Do not increase pde count if io mapping has not been changed */\n    if ( !need_flush )\n        goto out;\n\n    /* 4K mapping for PV guests never changes, \n     * no need to flush if we trust non-present bits */\n    if ( is_hvm_domain(d) )\n        amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n\n    for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n          merge_level++ )\n    {\n        if ( pt_mfn[merge_level] == 0 )\n            break;\n        if ( !iommu_update_pde_count(d, pt_mfn[merge_level],\n                                     dfn_x(dfn), mfn_x(mfn), merge_level) )\n            break;\n\n        if ( iommu_merge_pages(d, pt_mfn[merge_level], dfn_x(dfn),\n                               flags, merge_level) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Merge iommu page failed at level %d, \"\n                            \"dfn = %\"PRI_dfn\" mfn = %\"PRI_mfn\"\\n\",\n                            merge_level, dfn_x(dfn), mfn_x(mfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n\n        /* Deallocate lower level page table */\n        free_amd_iommu_pgtable(mfn_to_page(_mfn(pt_mfn[merge_level - 1])));\n    }\n\nout:\n    spin_unlock(&hd->arch.mapping_lock);\n    return 0;\n}",
        "func": "int amd_iommu_map_page(struct domain *d, dfn_t dfn, mfn_t mfn,\n                       unsigned int flags)\n{\n    bool_t need_flush = 0;\n    struct domain_iommu *hd = dom_iommu(d);\n    int rc;\n    unsigned long pt_mfn[7];\n    unsigned int merge_level;\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    rc = amd_iommu_alloc_root(hd);\n    if ( rc )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Root table alloc failed, dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return rc;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for wider dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        if ( update_paging_mode(d, dfn_x(dfn)) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* Install 4k mapping first */\n    need_flush = set_iommu_pte_present(pt_mfn[1], dfn_x(dfn), mfn_x(mfn), 1,\n                                       !!(flags & IOMMUF_writable),\n                                       !!(flags & IOMMUF_readable));\n\n    /* Do not increase pde count if io mapping has not been changed */\n    if ( !need_flush )\n        goto out;\n\n    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n\n    for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n          merge_level++ )\n    {\n        if ( pt_mfn[merge_level] == 0 )\n            break;\n        if ( !iommu_update_pde_count(d, pt_mfn[merge_level],\n                                     dfn_x(dfn), mfn_x(mfn), merge_level) )\n            break;\n\n        if ( iommu_merge_pages(d, pt_mfn[merge_level], dfn_x(dfn),\n                               flags, merge_level) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Merge iommu page failed at level %d, \"\n                            \"dfn = %\"PRI_dfn\" mfn = %\"PRI_mfn\"\\n\",\n                            merge_level, dfn_x(dfn), mfn_x(mfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n\n        /* Deallocate lower level page table */\n        free_amd_iommu_pgtable(mfn_to_page(_mfn(pt_mfn[merge_level - 1])));\n    }\n\nout:\n    spin_unlock(&hd->arch.mapping_lock);\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,10 +56,7 @@\n     if ( !need_flush )\n         goto out;\n \n-    /* 4K mapping for PV guests never changes, \n-     * no need to flush if we trust non-present bits */\n-    if ( is_hvm_domain(d) )\n-        amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n+    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n \n     for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n           merge_level++ )",
        "diff_line_info": {
            "deleted_lines": [
                "    /* 4K mapping for PV guests never changes, ",
                "     * no need to flush if we trust non-present bits */",
                "    if ( is_hvm_domain(d) )",
                "        amd_iommu_flush_pages(d, dfn_x(dfn), 0);"
            ],
            "added_lines": [
                "    amd_iommu_flush_pages(d, dfn_x(dfn), 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19961",
        "func_name": "xen-project/xen/set_iommu_pde_present",
        "description": "An issue was discovered in Xen through 4.11.x on AMD x86 platforms, possibly allowing guest OS users to gain host OS privileges because TLB flushes do not always occur after IOMMU mapping changes.",
        "git_url": "https://github.com/xen-project/xen/commit/1a7ffe466cd057daaef245b0a1ab6b82588e4c01",
        "commit_title": "amd/iommu: fix flush checks",
        "commit_text": " Flush checking for AMD IOMMU didn't check whether the previous entry was present, or whether the flags (writable/readable) changed in order to decide whether a flush should be executed.  Fix this by taking the writable/readable/next-level fields into account, together with the present bit.  Along these lines the flushing in amd_iommu_map_page() must not be omitted for PV domains. The comment there was simply wrong: Mappings may very well change, both their addresses and their permissions. Ultimately this should honor iommu_dont_flush_iotlb, but to achieve this amd_iommu_ops first needs to gain an .iotlb_flush hook.  Also make clear_iommu_pte_present() static, to demonstrate there's no caller omitting the (subsequent) flush.  This is part of XSA-275. ",
        "func_before": "static bool_t set_iommu_pde_present(u32 *pde, unsigned long next_mfn, \n                                    unsigned int next_level,\n                                    bool_t iw, bool_t ir)\n{\n    u64 addr_lo, addr_hi, maddr_old, maddr_next;\n    u32 entry;\n    bool_t need_flush = 0;\n\n    maddr_next = (u64)next_mfn << PAGE_SHIFT;\n\n    addr_hi = get_field_from_reg_u32(pde[1],\n                                     IOMMU_PTE_ADDR_HIGH_MASK,\n                                     IOMMU_PTE_ADDR_HIGH_SHIFT);\n    addr_lo = get_field_from_reg_u32(pde[0],\n                                     IOMMU_PTE_ADDR_LOW_MASK,\n                                     IOMMU_PTE_ADDR_LOW_SHIFT);\n\n    maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);\n\n    if ( maddr_old != maddr_next )\n        need_flush = 1;\n\n    addr_lo = maddr_next & DMA_32BIT_MASK;\n    addr_hi = maddr_next >> 32;\n\n    /* enable read/write permissions,which will be enforced at the PTE */\n    set_field_in_reg_u32((u32)addr_hi, 0,\n                         IOMMU_PDE_ADDR_HIGH_MASK,\n                         IOMMU_PDE_ADDR_HIGH_SHIFT, &entry);\n    set_field_in_reg_u32(iw, entry,\n                         IOMMU_PDE_IO_WRITE_PERMISSION_MASK,\n                         IOMMU_PDE_IO_WRITE_PERMISSION_SHIFT, &entry);\n    set_field_in_reg_u32(ir, entry,\n                         IOMMU_PDE_IO_READ_PERMISSION_MASK,\n                         IOMMU_PDE_IO_READ_PERMISSION_SHIFT, &entry);\n\n    /* FC bit should be enabled in PTE, this helps to solve potential\n     * issues with ATS devices\n     */\n    if ( next_level == 0 )\n        set_field_in_reg_u32(IOMMU_CONTROL_ENABLED, entry,\n                             IOMMU_PTE_FC_MASK, IOMMU_PTE_FC_SHIFT, &entry);\n    pde[1] = entry;\n\n    /* mark next level as 'present' */\n    set_field_in_reg_u32((u32)addr_lo >> PAGE_SHIFT, 0,\n                         IOMMU_PDE_ADDR_LOW_MASK,\n                         IOMMU_PDE_ADDR_LOW_SHIFT, &entry);\n    set_field_in_reg_u32(next_level, entry,\n                         IOMMU_PDE_NEXT_LEVEL_MASK,\n                         IOMMU_PDE_NEXT_LEVEL_SHIFT, &entry);\n    set_field_in_reg_u32(IOMMU_CONTROL_ENABLED, entry,\n                         IOMMU_PDE_PRESENT_MASK,\n                         IOMMU_PDE_PRESENT_SHIFT, &entry);\n    pde[0] = entry;\n\n    return need_flush;\n}",
        "func": "static bool_t set_iommu_pde_present(u32 *pde, unsigned long next_mfn, \n                                    unsigned int next_level,\n                                    bool_t iw, bool_t ir)\n{\n    uint64_t addr_lo, addr_hi, maddr_next;\n    u32 entry;\n    bool need_flush = false, old_present;\n\n    maddr_next = (u64)next_mfn << PAGE_SHIFT;\n\n    old_present = get_field_from_reg_u32(pde[0], IOMMU_PTE_PRESENT_MASK,\n                                         IOMMU_PTE_PRESENT_SHIFT);\n    if ( old_present )\n    {\n        bool old_r, old_w;\n        unsigned int old_level;\n        uint64_t maddr_old;\n\n        addr_hi = get_field_from_reg_u32(pde[1],\n                                         IOMMU_PTE_ADDR_HIGH_MASK,\n                                         IOMMU_PTE_ADDR_HIGH_SHIFT);\n        addr_lo = get_field_from_reg_u32(pde[0],\n                                         IOMMU_PTE_ADDR_LOW_MASK,\n                                         IOMMU_PTE_ADDR_LOW_SHIFT);\n        old_level = get_field_from_reg_u32(pde[0],\n                                           IOMMU_PDE_NEXT_LEVEL_MASK,\n                                           IOMMU_PDE_NEXT_LEVEL_SHIFT);\n        old_w = get_field_from_reg_u32(pde[1],\n                                       IOMMU_PTE_IO_WRITE_PERMISSION_MASK,\n                                       IOMMU_PTE_IO_WRITE_PERMISSION_SHIFT);\n        old_r = get_field_from_reg_u32(pde[1],\n                                       IOMMU_PTE_IO_READ_PERMISSION_MASK,\n                                       IOMMU_PTE_IO_READ_PERMISSION_SHIFT);\n\n        maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);\n\n        if ( maddr_old != maddr_next || iw != old_w || ir != old_r ||\n             old_level != next_level )\n            need_flush = true;\n    }\n\n    addr_lo = maddr_next & DMA_32BIT_MASK;\n    addr_hi = maddr_next >> 32;\n\n    /* enable read/write permissions,which will be enforced at the PTE */\n    set_field_in_reg_u32((u32)addr_hi, 0,\n                         IOMMU_PDE_ADDR_HIGH_MASK,\n                         IOMMU_PDE_ADDR_HIGH_SHIFT, &entry);\n    set_field_in_reg_u32(iw, entry,\n                         IOMMU_PDE_IO_WRITE_PERMISSION_MASK,\n                         IOMMU_PDE_IO_WRITE_PERMISSION_SHIFT, &entry);\n    set_field_in_reg_u32(ir, entry,\n                         IOMMU_PDE_IO_READ_PERMISSION_MASK,\n                         IOMMU_PDE_IO_READ_PERMISSION_SHIFT, &entry);\n\n    /* FC bit should be enabled in PTE, this helps to solve potential\n     * issues with ATS devices\n     */\n    if ( next_level == 0 )\n        set_field_in_reg_u32(IOMMU_CONTROL_ENABLED, entry,\n                             IOMMU_PTE_FC_MASK, IOMMU_PTE_FC_SHIFT, &entry);\n    pde[1] = entry;\n\n    /* mark next level as 'present' */\n    set_field_in_reg_u32((u32)addr_lo >> PAGE_SHIFT, 0,\n                         IOMMU_PDE_ADDR_LOW_MASK,\n                         IOMMU_PDE_ADDR_LOW_SHIFT, &entry);\n    set_field_in_reg_u32(next_level, entry,\n                         IOMMU_PDE_NEXT_LEVEL_MASK,\n                         IOMMU_PDE_NEXT_LEVEL_SHIFT, &entry);\n    set_field_in_reg_u32(IOMMU_CONTROL_ENABLED, entry,\n                         IOMMU_PDE_PRESENT_MASK,\n                         IOMMU_PDE_PRESENT_SHIFT, &entry);\n    pde[0] = entry;\n\n    return need_flush;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,23 +2,42 @@\n                                     unsigned int next_level,\n                                     bool_t iw, bool_t ir)\n {\n-    u64 addr_lo, addr_hi, maddr_old, maddr_next;\n+    uint64_t addr_lo, addr_hi, maddr_next;\n     u32 entry;\n-    bool_t need_flush = 0;\n+    bool need_flush = false, old_present;\n \n     maddr_next = (u64)next_mfn << PAGE_SHIFT;\n \n-    addr_hi = get_field_from_reg_u32(pde[1],\n-                                     IOMMU_PTE_ADDR_HIGH_MASK,\n-                                     IOMMU_PTE_ADDR_HIGH_SHIFT);\n-    addr_lo = get_field_from_reg_u32(pde[0],\n-                                     IOMMU_PTE_ADDR_LOW_MASK,\n-                                     IOMMU_PTE_ADDR_LOW_SHIFT);\n+    old_present = get_field_from_reg_u32(pde[0], IOMMU_PTE_PRESENT_MASK,\n+                                         IOMMU_PTE_PRESENT_SHIFT);\n+    if ( old_present )\n+    {\n+        bool old_r, old_w;\n+        unsigned int old_level;\n+        uint64_t maddr_old;\n \n-    maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);\n+        addr_hi = get_field_from_reg_u32(pde[1],\n+                                         IOMMU_PTE_ADDR_HIGH_MASK,\n+                                         IOMMU_PTE_ADDR_HIGH_SHIFT);\n+        addr_lo = get_field_from_reg_u32(pde[0],\n+                                         IOMMU_PTE_ADDR_LOW_MASK,\n+                                         IOMMU_PTE_ADDR_LOW_SHIFT);\n+        old_level = get_field_from_reg_u32(pde[0],\n+                                           IOMMU_PDE_NEXT_LEVEL_MASK,\n+                                           IOMMU_PDE_NEXT_LEVEL_SHIFT);\n+        old_w = get_field_from_reg_u32(pde[1],\n+                                       IOMMU_PTE_IO_WRITE_PERMISSION_MASK,\n+                                       IOMMU_PTE_IO_WRITE_PERMISSION_SHIFT);\n+        old_r = get_field_from_reg_u32(pde[1],\n+                                       IOMMU_PTE_IO_READ_PERMISSION_MASK,\n+                                       IOMMU_PTE_IO_READ_PERMISSION_SHIFT);\n \n-    if ( maddr_old != maddr_next )\n-        need_flush = 1;\n+        maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);\n+\n+        if ( maddr_old != maddr_next || iw != old_w || ir != old_r ||\n+             old_level != next_level )\n+            need_flush = true;\n+    }\n \n     addr_lo = maddr_next & DMA_32BIT_MASK;\n     addr_hi = maddr_next >> 32;",
        "diff_line_info": {
            "deleted_lines": [
                "    u64 addr_lo, addr_hi, maddr_old, maddr_next;",
                "    bool_t need_flush = 0;",
                "    addr_hi = get_field_from_reg_u32(pde[1],",
                "                                     IOMMU_PTE_ADDR_HIGH_MASK,",
                "                                     IOMMU_PTE_ADDR_HIGH_SHIFT);",
                "    addr_lo = get_field_from_reg_u32(pde[0],",
                "                                     IOMMU_PTE_ADDR_LOW_MASK,",
                "                                     IOMMU_PTE_ADDR_LOW_SHIFT);",
                "    maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);",
                "    if ( maddr_old != maddr_next )",
                "        need_flush = 1;"
            ],
            "added_lines": [
                "    uint64_t addr_lo, addr_hi, maddr_next;",
                "    bool need_flush = false, old_present;",
                "    old_present = get_field_from_reg_u32(pde[0], IOMMU_PTE_PRESENT_MASK,",
                "                                         IOMMU_PTE_PRESENT_SHIFT);",
                "    if ( old_present )",
                "    {",
                "        bool old_r, old_w;",
                "        unsigned int old_level;",
                "        uint64_t maddr_old;",
                "        addr_hi = get_field_from_reg_u32(pde[1],",
                "                                         IOMMU_PTE_ADDR_HIGH_MASK,",
                "                                         IOMMU_PTE_ADDR_HIGH_SHIFT);",
                "        addr_lo = get_field_from_reg_u32(pde[0],",
                "                                         IOMMU_PTE_ADDR_LOW_MASK,",
                "                                         IOMMU_PTE_ADDR_LOW_SHIFT);",
                "        old_level = get_field_from_reg_u32(pde[0],",
                "                                           IOMMU_PDE_NEXT_LEVEL_MASK,",
                "                                           IOMMU_PDE_NEXT_LEVEL_SHIFT);",
                "        old_w = get_field_from_reg_u32(pde[1],",
                "                                       IOMMU_PTE_IO_WRITE_PERMISSION_MASK,",
                "                                       IOMMU_PTE_IO_WRITE_PERMISSION_SHIFT);",
                "        old_r = get_field_from_reg_u32(pde[1],",
                "                                       IOMMU_PTE_IO_READ_PERMISSION_MASK,",
                "                                       IOMMU_PTE_IO_READ_PERMISSION_SHIFT);",
                "        maddr_old = (addr_hi << 32) | (addr_lo << PAGE_SHIFT);",
                "",
                "        if ( maddr_old != maddr_next || iw != old_w || ir != old_r ||",
                "             old_level != next_level )",
                "            need_flush = true;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19961",
        "func_name": "xen-project/xen/clear_iommu_pte_present",
        "description": "An issue was discovered in Xen through 4.11.x on AMD x86 platforms, possibly allowing guest OS users to gain host OS privileges because TLB flushes do not always occur after IOMMU mapping changes.",
        "git_url": "https://github.com/xen-project/xen/commit/1a7ffe466cd057daaef245b0a1ab6b82588e4c01",
        "commit_title": "amd/iommu: fix flush checks",
        "commit_text": " Flush checking for AMD IOMMU didn't check whether the previous entry was present, or whether the flags (writable/readable) changed in order to decide whether a flush should be executed.  Fix this by taking the writable/readable/next-level fields into account, together with the present bit.  Along these lines the flushing in amd_iommu_map_page() must not be omitted for PV domains. The comment there was simply wrong: Mappings may very well change, both their addresses and their permissions. Ultimately this should honor iommu_dont_flush_iotlb, but to achieve this amd_iommu_ops first needs to gain an .iotlb_flush hook.  Also make clear_iommu_pte_present() static, to demonstrate there's no caller omitting the (subsequent) flush.  This is part of XSA-275. ",
        "func_before": "void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)\n{\n    u64 *table, *pte;\n\n    table = map_domain_page(_mfn(l1_mfn));\n    pte = table + pfn_to_pde_idx(dfn, 1);\n    *pte = 0;\n    unmap_domain_page(table);\n}",
        "func": "static void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)\n{\n    u64 *table, *pte;\n\n    table = map_domain_page(_mfn(l1_mfn));\n    pte = table + pfn_to_pde_idx(dfn, 1);\n    *pte = 0;\n    unmap_domain_page(table);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)\n+static void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)\n {\n     u64 *table, *pte;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)"
            ],
            "added_lines": [
                "static void clear_iommu_pte_present(unsigned long l1_mfn, unsigned long dfn)"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19961",
        "func_name": "xen-project/xen/amd_iommu_unmap_page",
        "description": "An issue was discovered in Xen through 4.11.x on AMD x86 platforms, possibly allowing guest OS users to gain host OS privileges because TLB flushes do not always occur after IOMMU mapping changes.",
        "git_url": "https://github.com/xen-project/xen/commit/937ef32565fa3a81fdb37b9dd5aa99a1b87afa75",
        "commit_title": "AMD/IOMMU: suppress PTE merging after initial table creation",
        "commit_text": " The logic is not fit for this purpose, so simply disable its use until it can be fixed / replaced. Note that this re-enables merging for the table creation case, which was disabled as a (perhaps unintended) side effect of the earlier \"amd/iommu: fix flush checks\". It relies on no page getting mapped more than once (with different properties) in this process, as that would still be beyond what the merging logic can cope with. But arch_iommu_populate_page_table() guarantees this afaict.  This is part of XSA-275. ",
        "func_before": "int amd_iommu_unmap_page(struct domain *d, dfn_t dfn)\n{\n    unsigned long pt_mfn[7];\n    struct domain_iommu *hd = dom_iommu(d);\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    if ( !hd->arch.root_table )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        return 0;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for lager dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        int rc = update_paging_mode(d, dfn_x(dfn));\n\n        if ( rc )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            if ( rc != -EADDRNOTAVAIL )\n                domain_crash(d);\n            return rc;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* mark PTE as 'page not present' */\n    clear_iommu_pte_present(pt_mfn[1], dfn_x(dfn));\n    spin_unlock(&hd->arch.mapping_lock);\n\n    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n\n    return 0;\n}",
        "func": "int amd_iommu_unmap_page(struct domain *d, dfn_t dfn)\n{\n    unsigned long pt_mfn[7];\n    struct domain_iommu *hd = dom_iommu(d);\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    if ( !hd->arch.root_table )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        return 0;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for lager dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        int rc = update_paging_mode(d, dfn_x(dfn));\n\n        if ( rc )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            if ( rc != -EADDRNOTAVAIL )\n                domain_crash(d);\n            return rc;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* mark PTE as 'page not present' */\n    clear_iommu_pte_present(pt_mfn[1], dfn_x(dfn));\n\n    /* No further merging in amd_iommu_map_page(), as the logic doesn't cope. */\n    hd->arch.no_merge = true;\n\n    spin_unlock(&hd->arch.mapping_lock);\n\n    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -44,6 +44,10 @@\n \n     /* mark PTE as 'page not present' */\n     clear_iommu_pte_present(pt_mfn[1], dfn_x(dfn));\n+\n+    /* No further merging in amd_iommu_map_page(), as the logic doesn't cope. */\n+    hd->arch.no_merge = true;\n+\n     spin_unlock(&hd->arch.mapping_lock);\n \n     amd_iommu_flush_pages(d, dfn_x(dfn), 0);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    /* No further merging in amd_iommu_map_page(), as the logic doesn't cope. */",
                "    hd->arch.no_merge = true;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19961",
        "func_name": "xen-project/xen/amd_iommu_map_page",
        "description": "An issue was discovered in Xen through 4.11.x on AMD x86 platforms, possibly allowing guest OS users to gain host OS privileges because TLB flushes do not always occur after IOMMU mapping changes.",
        "git_url": "https://github.com/xen-project/xen/commit/937ef32565fa3a81fdb37b9dd5aa99a1b87afa75",
        "commit_title": "AMD/IOMMU: suppress PTE merging after initial table creation",
        "commit_text": " The logic is not fit for this purpose, so simply disable its use until it can be fixed / replaced. Note that this re-enables merging for the table creation case, which was disabled as a (perhaps unintended) side effect of the earlier \"amd/iommu: fix flush checks\". It relies on no page getting mapped more than once (with different properties) in this process, as that would still be beyond what the merging logic can cope with. But arch_iommu_populate_page_table() guarantees this afaict.  This is part of XSA-275. ",
        "func_before": "int amd_iommu_map_page(struct domain *d, dfn_t dfn, mfn_t mfn,\n                       unsigned int flags)\n{\n    bool_t need_flush = 0;\n    struct domain_iommu *hd = dom_iommu(d);\n    int rc;\n    unsigned long pt_mfn[7];\n    unsigned int merge_level;\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    rc = amd_iommu_alloc_root(hd);\n    if ( rc )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Root table alloc failed, dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return rc;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for wider dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        if ( update_paging_mode(d, dfn_x(dfn)) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* Install 4k mapping first */\n    need_flush = set_iommu_pte_present(pt_mfn[1], dfn_x(dfn), mfn_x(mfn), 1,\n                                       !!(flags & IOMMUF_writable),\n                                       !!(flags & IOMMUF_readable));\n\n    /* Do not increase pde count if io mapping has not been changed */\n    if ( !need_flush )\n        goto out;\n\n    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n\n    for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n          merge_level++ )\n    {\n        if ( pt_mfn[merge_level] == 0 )\n            break;\n        if ( !iommu_update_pde_count(d, pt_mfn[merge_level],\n                                     dfn_x(dfn), mfn_x(mfn), merge_level) )\n            break;\n\n        if ( iommu_merge_pages(d, pt_mfn[merge_level], dfn_x(dfn),\n                               flags, merge_level) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Merge iommu page failed at level %d, \"\n                            \"dfn = %\"PRI_dfn\" mfn = %\"PRI_mfn\"\\n\",\n                            merge_level, dfn_x(dfn), mfn_x(mfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n\n        /* Deallocate lower level page table */\n        free_amd_iommu_pgtable(mfn_to_page(_mfn(pt_mfn[merge_level - 1])));\n    }\n\nout:\n    spin_unlock(&hd->arch.mapping_lock);\n    return 0;\n}",
        "func": "int amd_iommu_map_page(struct domain *d, dfn_t dfn, mfn_t mfn,\n                       unsigned int flags)\n{\n    bool_t need_flush = 0;\n    struct domain_iommu *hd = dom_iommu(d);\n    int rc;\n    unsigned long pt_mfn[7];\n    unsigned int merge_level;\n\n    if ( iommu_use_hap_pt(d) )\n        return 0;\n\n    memset(pt_mfn, 0, sizeof(pt_mfn));\n\n    spin_lock(&hd->arch.mapping_lock);\n\n    rc = amd_iommu_alloc_root(hd);\n    if ( rc )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Root table alloc failed, dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return rc;\n    }\n\n    /* Since HVM domain is initialized with 2 level IO page table,\n     * we might need a deeper page table for wider dfn now */\n    if ( is_hvm_domain(d) )\n    {\n        if ( update_paging_mode(d, dfn_x(dfn)) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Update page mode failed dfn = %\"PRI_dfn\"\\n\",\n                            dfn_x(dfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n    }\n\n    if ( iommu_pde_from_dfn(d, dfn_x(dfn), pt_mfn) || (pt_mfn[1] == 0) )\n    {\n        spin_unlock(&hd->arch.mapping_lock);\n        AMD_IOMMU_DEBUG(\"Invalid IO pagetable entry dfn = %\"PRI_dfn\"\\n\",\n                        dfn_x(dfn));\n        domain_crash(d);\n        return -EFAULT;\n    }\n\n    /* Install 4k mapping first */\n    need_flush = set_iommu_pte_present(pt_mfn[1], dfn_x(dfn), mfn_x(mfn), 1,\n                                       !!(flags & IOMMUF_writable),\n                                       !!(flags & IOMMUF_readable));\n\n    if ( need_flush )\n    {\n        amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n        /* No further merging, as the logic doesn't cope. */\n        hd->arch.no_merge = true;\n    }\n\n    /*\n     * Suppress merging of non-R/W mappings or after initial table creation,\n     * as the merge logic does not cope with this.\n     */\n    if ( hd->arch.no_merge || flags != (IOMMUF_writable | IOMMUF_readable) )\n        goto out;\n    if ( d->creation_finished )\n    {\n        hd->arch.no_merge = true;\n        goto out;\n    }\n\n    for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n          merge_level++ )\n    {\n        if ( pt_mfn[merge_level] == 0 )\n            break;\n        if ( !iommu_update_pde_count(d, pt_mfn[merge_level],\n                                     dfn_x(dfn), mfn_x(mfn), merge_level) )\n            break;\n\n        if ( iommu_merge_pages(d, pt_mfn[merge_level], dfn_x(dfn),\n                               flags, merge_level) )\n        {\n            spin_unlock(&hd->arch.mapping_lock);\n            AMD_IOMMU_DEBUG(\"Merge iommu page failed at level %d, \"\n                            \"dfn = %\"PRI_dfn\" mfn = %\"PRI_mfn\"\\n\",\n                            merge_level, dfn_x(dfn), mfn_x(mfn));\n            domain_crash(d);\n            return -EFAULT;\n        }\n\n        /* Deallocate lower level page table */\n        free_amd_iommu_pgtable(mfn_to_page(_mfn(pt_mfn[merge_level - 1])));\n    }\n\nout:\n    spin_unlock(&hd->arch.mapping_lock);\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -52,11 +52,24 @@\n                                        !!(flags & IOMMUF_writable),\n                                        !!(flags & IOMMUF_readable));\n \n-    /* Do not increase pde count if io mapping has not been changed */\n-    if ( !need_flush )\n+    if ( need_flush )\n+    {\n+        amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n+        /* No further merging, as the logic doesn't cope. */\n+        hd->arch.no_merge = true;\n+    }\n+\n+    /*\n+     * Suppress merging of non-R/W mappings or after initial table creation,\n+     * as the merge logic does not cope with this.\n+     */\n+    if ( hd->arch.no_merge || flags != (IOMMUF_writable | IOMMUF_readable) )\n         goto out;\n-\n-    amd_iommu_flush_pages(d, dfn_x(dfn), 0);\n+    if ( d->creation_finished )\n+    {\n+        hd->arch.no_merge = true;\n+        goto out;\n+    }\n \n     for ( merge_level = 2; merge_level <= hd->arch.paging_mode;\n           merge_level++ )",
        "diff_line_info": {
            "deleted_lines": [
                "    /* Do not increase pde count if io mapping has not been changed */",
                "    if ( !need_flush )",
                "",
                "    amd_iommu_flush_pages(d, dfn_x(dfn), 0);"
            ],
            "added_lines": [
                "    if ( need_flush )",
                "    {",
                "        amd_iommu_flush_pages(d, dfn_x(dfn), 0);",
                "        /* No further merging, as the logic doesn't cope. */",
                "        hd->arch.no_merge = true;",
                "    }",
                "",
                "    /*",
                "     * Suppress merging of non-R/W mappings or after initial table creation,",
                "     * as the merge logic does not cope with this.",
                "     */",
                "    if ( hd->arch.no_merge || flags != (IOMMUF_writable | IOMMUF_readable) )",
                "    if ( d->creation_finished )",
                "    {",
                "        hd->arch.no_merge = true;",
                "        goto out;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4032",
        "func_name": "torvalds/linux/kvm_lapic_reset",
        "description": "A vulnerability was found in the Linux kernel's KVM subsystem in arch/x86/kvm/lapic.c kvm_free_lapic when a failure allocation was detected. In this flaw the KVM subsystem may crash the kernel due to mishandling of memory errors that happens during VCPU construction, which allows an attacker with special user privilege to cause a denial of service. This flaw affects kernel versions prior to 5.15 rc7.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f7d8a19f9a056a05c5c509fa65af472a322abfee",
        "commit_title": "Revert a change to open code bits of kvm_lapic_set_base() when emulating",
        "commit_text": "APIC RESET to fix an apic_hw_disabled underflow bug due to arch.apic_base and apic_hw_disabled being unsyncrhonized when the APIC is created.  If kvm_arch_vcpu_create() fails after creating the APIC, kvm_free_lapic() will see the initialized-to-zero vcpu->arch.apic_base and decrement apic_hw_disabled without KVM ever having incremented apic_hw_disabled.  Using kvm_lapic_set_base() in kvm_lapic_reset() is also desirable for a potential future where KVM supports RESET outside of vCPU creation, in which case all the side effects of kvm_lapic_set_base() are needed, e.g. to handle the transition from x2APIC => xAPIC.  Alternatively, KVM could temporarily increment apic_hw_disabled (and call kvm_lapic_set_base() at RESET), but that's a waste of cycles and would impact the performance of other vCPUs and VMs.  The other subtle side effect is that updating the xAPIC ID needs to be done at RESET regardless of whether the APIC was previously enabled, i.e. kvm_lapic_reset() needs an explicit call to kvm_apic_set_xapic_id() regardless of whether or not kvm_lapic_set_base() also performs the update.  That makes stuffing the enable bit at vCPU creation slightly more palatable, as doing so affects only the apic_hw_disabled key.  Opportunistically tweak the comment to explicitly call out the connection between vcpu->arch.apic_base and apic_hw_disabled, and add a comment to call out the need to always do kvm_apic_set_xapic_id() at RESET.  Underflow scenario:    kvm_vm_ioctl() {     kvm_vm_ioctl_create_vcpu() {       kvm_arch_vcpu_create() {         if (something_went_wrong)           goto fail_free_lapic;         /* vcpu->arch.apic_base is initialized when something_went_wrong is false. */         kvm_vcpu_reset() {           kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event) {             vcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;           }         }         return 0;       fail_free_lapic:         kvm_free_lapic() {           /* vcpu->arch.apic_base is not yet initialized when something_went_wrong is true. */           if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))             static_branch_slow_dec_deferred(&apic_hw_disabled); // <= underflow bug.         }         return r;       }     }   }  This (mostly) reverts commit 421221234ada41b4a9f0beeb08e30b07388bd4bd.  Debugged-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp> Message-Id: <20211013003554.47705-2-seanjc@google.com> ",
        "func_before": "void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tint i;\n\n\tif (!init_event) {\n\t\tvcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE |\n\t\t\t\t       MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tvcpu->arch.apic_base |= MSR_IA32_APICBASE_BSP;\n\t}\n\n\tif (!apic)\n\t\treturn;\n\n\t/* Stop the timer in case it's a reset to an active apic */\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\tif (!init_event) {\n\t\tapic->base_address = APIC_DEFAULT_PHYS_BASE;\n\n\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\t}\n\tkvm_apic_set_version(apic->vcpu);\n\n\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i, APIC_LVT_MASKED);\n\tapic_update_lvtt(apic);\n\tif (kvm_vcpu_is_reset_bsp(vcpu) &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))\n\t\tkvm_lapic_set_reg(apic, APIC_LVT0,\n\t\t\t     SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXTINT));\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\n\tkvm_apic_set_dfr(apic, 0xffffffffU);\n\tapic_set_spiv(apic, 0xff);\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, 0);\n\tif (!apic_x2apic_mode(apic))\n\t\tkvm_apic_set_ldr(apic, 0);\n\tkvm_lapic_set_reg(apic, APIC_ESR, 0);\n\tkvm_lapic_set_reg(apic, APIC_ICR, 0);\n\tkvm_lapic_set_reg(apic, APIC_ICR2, 0);\n\tkvm_lapic_set_reg(apic, APIC_TDCR, 0);\n\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\tfor (i = 0; i < 8; i++) {\n\t\tkvm_lapic_set_reg(apic, APIC_IRR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);\n\t}\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tupdate_divide_count(apic);\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tvcpu->arch.pv_eoi.msr_val = 0;\n\tapic_update_ppr(apic);\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call(kvm_x86_hwapic_irr_update)(vcpu, -1);\n\t\tstatic_call(kvm_x86_hwapic_isr_update)(vcpu, -1);\n\t}\n\n\tvcpu->arch.apic_arb_prio = 0;\n\tvcpu->arch.apic_attention = 0;\n\n\tkvm_recalculate_apic_map(vcpu->kvm);\n}",
        "func": "void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 msr_val;\n\tint i;\n\n\tif (!init_event) {\n\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;\n\t\tkvm_lapic_set_base(vcpu, msr_val);\n\t}\n\n\tif (!apic)\n\t\treturn;\n\n\t/* Stop the timer in case it's a reset to an active apic */\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\t/* The xAPIC ID is set at RESET even if the APIC was already enabled. */\n\tif (!init_event)\n\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\tkvm_apic_set_version(apic->vcpu);\n\n\tfor (i = 0; i < KVM_APIC_LVT_NUM; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i, APIC_LVT_MASKED);\n\tapic_update_lvtt(apic);\n\tif (kvm_vcpu_is_reset_bsp(vcpu) &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))\n\t\tkvm_lapic_set_reg(apic, APIC_LVT0,\n\t\t\t     SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXTINT));\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\n\tkvm_apic_set_dfr(apic, 0xffffffffU);\n\tapic_set_spiv(apic, 0xff);\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, 0);\n\tif (!apic_x2apic_mode(apic))\n\t\tkvm_apic_set_ldr(apic, 0);\n\tkvm_lapic_set_reg(apic, APIC_ESR, 0);\n\tkvm_lapic_set_reg(apic, APIC_ICR, 0);\n\tkvm_lapic_set_reg(apic, APIC_ICR2, 0);\n\tkvm_lapic_set_reg(apic, APIC_TDCR, 0);\n\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\tfor (i = 0; i < 8; i++) {\n\t\tkvm_lapic_set_reg(apic, APIC_IRR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);\n\t}\n\tkvm_apic_update_apicv(vcpu);\n\tapic->highest_isr_cache = -1;\n\tupdate_divide_count(apic);\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tvcpu->arch.pv_eoi.msr_val = 0;\n\tapic_update_ppr(apic);\n\tif (vcpu->arch.apicv_active) {\n\t\tstatic_call(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call(kvm_x86_hwapic_irr_update)(vcpu, -1);\n\t\tstatic_call(kvm_x86_hwapic_isr_update)(vcpu, -1);\n\t}\n\n\tvcpu->arch.apic_arb_prio = 0;\n\tvcpu->arch.apic_attention = 0;\n\n\tkvm_recalculate_apic_map(vcpu->kvm);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,13 +1,14 @@\n void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n {\n \tstruct kvm_lapic *apic = vcpu->arch.apic;\n+\tu64 msr_val;\n \tint i;\n \n \tif (!init_event) {\n-\t\tvcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE |\n-\t\t\t\t       MSR_IA32_APICBASE_ENABLE;\n+\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;\n \t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n-\t\t\tvcpu->arch.apic_base |= MSR_IA32_APICBASE_BSP;\n+\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;\n+\t\tkvm_lapic_set_base(vcpu, msr_val);\n \t}\n \n \tif (!apic)\n@@ -16,11 +17,9 @@\n \t/* Stop the timer in case it's a reset to an active apic */\n \thrtimer_cancel(&apic->lapic_timer.timer);\n \n-\tif (!init_event) {\n-\t\tapic->base_address = APIC_DEFAULT_PHYS_BASE;\n-\n+\t/* The xAPIC ID is set at RESET even if the APIC was already enabled. */\n+\tif (!init_event)\n \t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n-\t}\n \tkvm_apic_set_version(apic->vcpu);\n \n \tfor (i = 0; i < KVM_APIC_LVT_NUM; i++)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tvcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE |",
                "\t\t\t\t       MSR_IA32_APICBASE_ENABLE;",
                "\t\t\tvcpu->arch.apic_base |= MSR_IA32_APICBASE_BSP;",
                "\tif (!init_event) {",
                "\t\tapic->base_address = APIC_DEFAULT_PHYS_BASE;",
                "",
                "\t}"
            ],
            "added_lines": [
                "\tu64 msr_val;",
                "\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;",
                "\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;",
                "\t\tkvm_lapic_set_base(vcpu, msr_val);",
                "\t/* The xAPIC ID is set at RESET even if the APIC was already enabled. */",
                "\tif (!init_event)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4032",
        "func_name": "torvalds/linux/kvm_create_lapic",
        "description": "A vulnerability was found in the Linux kernel's KVM subsystem in arch/x86/kvm/lapic.c kvm_free_lapic when a failure allocation was detected. In this flaw the KVM subsystem may crash the kernel due to mishandling of memory errors that happens during VCPU construction, which allows an attacker with special user privilege to cause a denial of service. This flaw affects kernel versions prior to 5.15 rc7.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f7d8a19f9a056a05c5c509fa65af472a322abfee",
        "commit_title": "Revert a change to open code bits of kvm_lapic_set_base() when emulating",
        "commit_text": "APIC RESET to fix an apic_hw_disabled underflow bug due to arch.apic_base and apic_hw_disabled being unsyncrhonized when the APIC is created.  If kvm_arch_vcpu_create() fails after creating the APIC, kvm_free_lapic() will see the initialized-to-zero vcpu->arch.apic_base and decrement apic_hw_disabled without KVM ever having incremented apic_hw_disabled.  Using kvm_lapic_set_base() in kvm_lapic_reset() is also desirable for a potential future where KVM supports RESET outside of vCPU creation, in which case all the side effects of kvm_lapic_set_base() are needed, e.g. to handle the transition from x2APIC => xAPIC.  Alternatively, KVM could temporarily increment apic_hw_disabled (and call kvm_lapic_set_base() at RESET), but that's a waste of cycles and would impact the performance of other vCPUs and VMs.  The other subtle side effect is that updating the xAPIC ID needs to be done at RESET regardless of whether the APIC was previously enabled, i.e. kvm_lapic_reset() needs an explicit call to kvm_apic_set_xapic_id() regardless of whether or not kvm_lapic_set_base() also performs the update.  That makes stuffing the enable bit at vCPU creation slightly more palatable, as doing so affects only the apic_hw_disabled key.  Opportunistically tweak the comment to explicitly call out the connection between vcpu->arch.apic_base and apic_hw_disabled, and add a comment to call out the need to always do kvm_apic_set_xapic_id() at RESET.  Underflow scenario:    kvm_vm_ioctl() {     kvm_vm_ioctl_create_vcpu() {       kvm_arch_vcpu_create() {         if (something_went_wrong)           goto fail_free_lapic;         /* vcpu->arch.apic_base is initialized when something_went_wrong is false. */         kvm_vcpu_reset() {           kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event) {             vcpu->arch.apic_base = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;           }         }         return 0;       fail_free_lapic:         kvm_free_lapic() {           /* vcpu->arch.apic_base is not yet initialized when something_went_wrong is true. */           if (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))             static_branch_slow_dec_deferred(&apic_hw_disabled); // <= underflow bug.         }         return r;       }     }   }  This (mostly) reverts commit 421221234ada41b4a9f0beeb08e30b07388bd4bd.  Debugged-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp> Message-Id: <20211013003554.47705-2-seanjc@google.com> ",
        "func_before": "int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)\n{\n\tstruct kvm_lapic *apic;\n\n\tASSERT(vcpu != NULL);\n\n\tapic = kzalloc(sizeof(*apic), GFP_KERNEL_ACCOUNT);\n\tif (!apic)\n\t\tgoto nomem;\n\n\tvcpu->arch.apic = apic;\n\n\tapic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!apic->regs) {\n\t\tprintk(KERN_ERR \"malloc apic regs error for vcpu %x\\n\",\n\t\t       vcpu->vcpu_id);\n\t\tgoto nomem_free_apic;\n\t}\n\tapic->vcpu = vcpu;\n\n\thrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tapic->lapic_timer.timer.function = apic_timer_fn;\n\tif (timer_advance_ns == -1) {\n\t\tapic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\t\tlapic_timer_advance_dynamic = true;\n\t} else {\n\t\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n\t\tlapic_timer_advance_dynamic = false;\n\t}\n\n\tstatic_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */\n\tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n\n\treturn 0;\nnomem_free_apic:\n\tkfree(apic);\n\tvcpu->arch.apic = NULL;\nnomem:\n\treturn -ENOMEM;\n}",
        "func": "int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)\n{\n\tstruct kvm_lapic *apic;\n\n\tASSERT(vcpu != NULL);\n\n\tapic = kzalloc(sizeof(*apic), GFP_KERNEL_ACCOUNT);\n\tif (!apic)\n\t\tgoto nomem;\n\n\tvcpu->arch.apic = apic;\n\n\tapic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!apic->regs) {\n\t\tprintk(KERN_ERR \"malloc apic regs error for vcpu %x\\n\",\n\t\t       vcpu->vcpu_id);\n\t\tgoto nomem_free_apic;\n\t}\n\tapic->vcpu = vcpu;\n\n\thrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tapic->lapic_timer.timer.function = apic_timer_fn;\n\tif (timer_advance_ns == -1) {\n\t\tapic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\t\tlapic_timer_advance_dynamic = true;\n\t} else {\n\t\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n\t\tlapic_timer_advance_dynamic = false;\n\t}\n\n\t/*\n\t * Stuff the APIC ENABLE bit in lieu of temporarily incrementing\n\t * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().\n\t */\n\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;\n\tstatic_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */\n\tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n\n\treturn 0;\nnomem_free_apic:\n\tkfree(apic);\n\tvcpu->arch.apic = NULL;\nnomem:\n\treturn -ENOMEM;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,6 +29,11 @@\n \t\tlapic_timer_advance_dynamic = false;\n \t}\n \n+\t/*\n+\t * Stuff the APIC ENABLE bit in lieu of temporarily incrementing\n+\t * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().\n+\t */\n+\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;\n \tstatic_branch_inc(&apic_sw_disabled.key); /* sw disabled at reset */\n \tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t/*",
                "\t * Stuff the APIC ENABLE bit in lieu of temporarily incrementing",
                "\t * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().",
                "\t */",
                "\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25016",
        "func_name": "Duncaen/OpenDoas/main",
        "description": "In OpenDoas from 6.6 to 6.8 the users PATH variable was incorrectly inherited by authenticated executions if the authenticating rule allowed the user to execute any command. Rules that only allowed to authenticated user to execute specific commands were not affected by this issue.",
        "git_url": "https://github.com/Duncaen/OpenDoas/commit/01c658f8c45cb92a343be5f32aa6da70b2032168",
        "commit_title": "redo the environment inheritance to not inherit. it was intended to make life easier, but it can be surprising or even unsafe. instead, reset just about everything to the target user's values. ok deraadt martijn Thanks to Sander Bos in particular for pointing out some nasty edge cases.",
        "commit_text": "",
        "func_before": "int\nmain(int argc, char **argv)\n{\n\tconst char *safepath = \"/bin:/sbin:/usr/bin:/usr/sbin:\"\n\t    \"/usr/local/bin:/usr/local/sbin\";\n\tconst char *confpath = NULL;\n\tchar *shargv[] = { NULL, NULL };\n\tchar *sh;\n\tconst char *cmd;\n\tchar cmdline[LINE_MAX];\n#ifdef __OpenBSD__\n\tchar mypwbuf[_PW_BUF_LEN], targpwbuf[_PW_BUF_LEN];\n#else\n\tchar *mypwbuf = NULL, *targpwbuf = NULL;\n#endif\n\tstruct passwd mypwstore, targpwstore;\n\tstruct passwd *mypw, *targpw;\n\tconst struct rule *rule;\n\tuid_t uid;\n\tuid_t target = 0;\n\tgid_t groups[NGROUPS_MAX + 1];\n\tint ngroups;\n\tint i, ch, rv;\n\tint sflag = 0;\n\tint nflag = 0;\n\tchar cwdpath[PATH_MAX];\n\tconst char *cwd;\n\tchar **envp;\n#ifdef USE_BSD_AUTH\n\tchar *login_style = NULL;\n#endif\n\n\tsetprogname(\"doas\");\n\n\tclosefrom(STDERR_FILENO + 1);\n\n\tuid = getuid();\n\n#ifdef USE_BSD_AUTH\n# define OPTSTRING \"a:C:Lnsu:\"\n#else\n# define OPTSTRING \"+C:Lnsu:\"\n#endif\n\n\twhile ((ch = getopt(argc, argv, OPTSTRING)) != -1) {\n\t\tswitch (ch) {\n#ifdef USE_BSD_AUTH\n\t\tcase 'a':\n\t\t\tlogin_style = optarg;\n\t\t\tbreak;\n#endif\n\t\tcase 'C':\n\t\t\tconfpath = optarg;\n\t\t\tbreak;\n\t\tcase 'L':\n#if defined(USE_BSD_AUTH)\n\t\t\ti = open(\"/dev/tty\", O_RDWR);\n\t\t\tif (i != -1)\n\t\t\t\tioctl(i, TIOCCLRVERAUTH);\n\t\t\texit(i == -1);\n#elif defined(USE_TIMESTAMP)\n\t\t\texit(timestamp_clear() == -1);\n#else\n\t\t\texit(0);\n#endif\n\t\tcase 'u':\n\t\t\tif (parseuid(optarg, &target) != 0)\n\t\t\t\terrx(1, \"unknown user\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tnflag = 1;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tsflag = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\tbreak;\n\t\t}\n\t}\n\targv += optind;\n\targc -= optind;\n\n\tif (confpath) {\n\t\tif (sflag)\n\t\t\tusage();\n\t} else if ((!sflag && !argc) || (sflag && argc))\n\t\tusage();\n\n#ifdef __OpenBSD__\n\trv = getpwuid_r(uid, &mypwstore, mypwbuf, sizeof(mypwbuf), &mypw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#else\n\tfor (size_t sz = 1024; sz <= 16*1024; sz *= 2) {\n\t\tmypwbuf = reallocarray(mypwbuf, sz, sizeof (char));\n\t\tif (mypwbuf == NULL)\n\t\t\terrx(1, \"can't allocate mypwbuf\");\n\t\trv = getpwuid_r(uid, &mypwstore, mypwbuf, sz, &mypw);\n\t\tif (rv != ERANGE)\n\t\t\tbreak;\n\t}\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#endif\n\tif (mypw == NULL)\n\t\terrx(1, \"no passwd entry for self\");\n\tngroups = getgroups(NGROUPS_MAX, groups);\n\tif (ngroups == -1)\n\t\terr(1, \"can't get groups\");\n\tgroups[ngroups++] = getgid();\n\n\tif (sflag) {\n\t\tsh = getenv(\"SHELL\");\n\t\tif (sh == NULL || *sh == '\\0') {\n\t\t\tshargv[0] = mypw->pw_shell;\n\t\t} else\n\t\t\tshargv[0] = sh;\n\t\targv = shargv;\n\t\targc = 1;\n\t}\n\n\tif (confpath) {\n\t\tcheckconfig(confpath, argc, argv, uid, groups, ngroups,\n\t\t    target);\n\t\texit(1);\t/* fail safe */\n\t}\n\n\tif (geteuid())\n\t\terrx(1, \"not installed setuid\");\n\n\tparseconfig(\"/etc/doas.conf\", 1);\n\n\t/* cmdline is used only for logging, no need to abort on truncate */\n\t(void)strlcpy(cmdline, argv[0], sizeof(cmdline));\n\tfor (i = 1; i < argc; i++) {\n\t\tif (strlcat(cmdline, \" \", sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t\tif (strlcat(cmdline, argv[i], sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t}\n\n\tcmd = argv[0];\n\tif (!permit(uid, groups, ngroups, &rule, target, cmd,\n\t    (const char **)argv + 1)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_NOTICE,\n\t\t    \"failed command for %s: %s\", mypw->pw_name, cmdline);\n\t\terrc(1, EPERM, NULL);\n\t}\n\n#if defined(__OpenBSD__) || defined(USE_SHADOW)\n\tif (!(rule->options & NOPASS)) {\n\t\tif (nflag)\n\t\t\terrx(1, \"Authorization required\");\n\n# ifdef __OpenBSD__\n\t\tauthuser(mypw->pw_name, login_style, rule->options & PERSIST);\n# else\n\t\tshadowauth(mypw->pw_name, rule->options & PERSIST);\n# endif\n\t}\n\n# ifdef __OpenBSD__\n\tif (pledge(\"stdio rpath getpw exec id\", NULL) == -1)\n\t\terr(1, \"pledge\");\n# endif\n\n#elif !defined(USE_PAM)\n\t(void) nflag;\n\tif (!(rule->options & NOPASS)) {\n\t\terrx(1, \"Authorization required\");\n\t}\n#endif /* !(__OpenBSD__ || USE_SHADOW) && !USE_PAM */\n\n#ifdef __OpenBSD__\n\trv = getpwuid_r(target, &targpwstore, targpwbuf, sizeof(targpwbuf), &targpw);\n\tif (rv != 0)\n\t\terrx(1, \"no passwd entry for target\");\n#else\n\tfor (size_t sz = 1024; sz <= 16*1024; sz *= 2) {\n\t\ttargpwbuf = reallocarray(targpwbuf, sz, sizeof (char));\n\t\tif (targpwbuf == NULL)\n\t\t\terrx(1, \"can't allocate targpwbuf\");\n\t\trv = getpwuid_r(target, &targpwstore, targpwbuf, sz, &targpw);\n\t\tif (rv != ERANGE)\n\t\t\tbreak;\n\t}\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#endif\n\tif (targpw == NULL)\n\t\terr(1, \"getpwuid_r failed\");\n\n#if defined(USE_PAM)\n\tpamauth(targpw->pw_name, mypw->pw_name, !nflag, rule->options & NOPASS,\n\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_SETUSERCONTEXT\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n#endif\n\n#ifdef __OpenBSD__\n\tif (pledge(\"stdio rpath exec\", NULL) == -1)\n\t\terr(1, \"pledge\");\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse\n\t\tcwd = cwdpath;\n\n#ifdef __OpenBSD__\n\tif (pledge(\"stdio exec\", NULL) == -1)\n\t\terr(1, \"pledge\");\n#endif\n\n\tsyslog(LOG_AUTHPRIV | LOG_INFO, \"%s ran command %s as %s from %s\",\n\t    mypw->pw_name, cmdline, targpw->pw_name, cwd);\n\n\tenvp = prepenv(rule);\n\n\tif (rule->cmd) {\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t}\n\texecvpe(cmd, argv, envp);\n\tif (errno == ENOENT)\n\t\terrx(1, \"%s: command not found\", cmd);\n\terr(1, \"%s\", cmd);\n}",
        "func": "int\nmain(int argc, char **argv)\n{\n\tconst char *safepath = \"/bin:/sbin:/usr/bin:/usr/sbin:\"\n\t    \"/usr/local/bin:/usr/local/sbin\";\n\tconst char *confpath = NULL;\n\tchar *shargv[] = { NULL, NULL };\n\tchar *sh;\n\tconst char *cmd;\n\tchar cmdline[LINE_MAX];\n#ifdef __OpenBSD__\n\tchar mypwbuf[_PW_BUF_LEN], targpwbuf[_PW_BUF_LEN];\n#else\n\tchar *mypwbuf = NULL, *targpwbuf = NULL;\n#endif\n\tstruct passwd mypwstore, targpwstore;\n\tstruct passwd *mypw, *targpw;\n\tconst struct rule *rule;\n\tuid_t uid;\n\tuid_t target = 0;\n\tgid_t groups[NGROUPS_MAX + 1];\n\tint ngroups;\n\tint i, ch, rv;\n\tint sflag = 0;\n\tint nflag = 0;\n\tchar cwdpath[PATH_MAX];\n\tconst char *cwd;\n\tchar **envp;\n#ifdef USE_BSD_AUTH\n\tchar *login_style = NULL;\n#endif\n\n\tsetprogname(\"doas\");\n\n\tclosefrom(STDERR_FILENO + 1);\n\n\tuid = getuid();\n\n#ifdef USE_BSD_AUTH\n# define OPTSTRING \"a:C:Lnsu:\"\n#else\n# define OPTSTRING \"+C:Lnsu:\"\n#endif\n\n\twhile ((ch = getopt(argc, argv, OPTSTRING)) != -1) {\n\t\tswitch (ch) {\n#ifdef USE_BSD_AUTH\n\t\tcase 'a':\n\t\t\tlogin_style = optarg;\n\t\t\tbreak;\n#endif\n\t\tcase 'C':\n\t\t\tconfpath = optarg;\n\t\t\tbreak;\n\t\tcase 'L':\n#if defined(USE_BSD_AUTH)\n\t\t\ti = open(\"/dev/tty\", O_RDWR);\n\t\t\tif (i != -1)\n\t\t\t\tioctl(i, TIOCCLRVERAUTH);\n\t\t\texit(i == -1);\n#elif defined(USE_TIMESTAMP)\n\t\t\texit(timestamp_clear() == -1);\n#else\n\t\t\texit(0);\n#endif\n\t\tcase 'u':\n\t\t\tif (parseuid(optarg, &target) != 0)\n\t\t\t\terrx(1, \"unknown user\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tnflag = 1;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tsflag = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\tbreak;\n\t\t}\n\t}\n\targv += optind;\n\targc -= optind;\n\n\tif (confpath) {\n\t\tif (sflag)\n\t\t\tusage();\n\t} else if ((!sflag && !argc) || (sflag && argc))\n\t\tusage();\n\n#ifdef __OpenBSD__\n\trv = getpwuid_r(uid, &mypwstore, mypwbuf, sizeof(mypwbuf), &mypw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#else\n\tfor (size_t sz = 1024; sz <= 16*1024; sz *= 2) {\n\t\tmypwbuf = reallocarray(mypwbuf, sz, sizeof (char));\n\t\tif (mypwbuf == NULL)\n\t\t\terrx(1, \"can't allocate mypwbuf\");\n\t\trv = getpwuid_r(uid, &mypwstore, mypwbuf, sz, &mypw);\n\t\tif (rv != ERANGE)\n\t\t\tbreak;\n\t}\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#endif\n\tif (mypw == NULL)\n\t\terrx(1, \"no passwd entry for self\");\n\tngroups = getgroups(NGROUPS_MAX, groups);\n\tif (ngroups == -1)\n\t\terr(1, \"can't get groups\");\n\tgroups[ngroups++] = getgid();\n\n\tif (sflag) {\n\t\tsh = getenv(\"SHELL\");\n\t\tif (sh == NULL || *sh == '\\0') {\n\t\t\tshargv[0] = mypw->pw_shell;\n\t\t} else\n\t\t\tshargv[0] = sh;\n\t\targv = shargv;\n\t\targc = 1;\n\t}\n\n\tif (confpath) {\n\t\tcheckconfig(confpath, argc, argv, uid, groups, ngroups,\n\t\t    target);\n\t\texit(1);\t/* fail safe */\n\t}\n\n\tif (geteuid())\n\t\terrx(1, \"not installed setuid\");\n\n\tparseconfig(\"/etc/doas.conf\", 1);\n\n\t/* cmdline is used only for logging, no need to abort on truncate */\n\t(void)strlcpy(cmdline, argv[0], sizeof(cmdline));\n\tfor (i = 1; i < argc; i++) {\n\t\tif (strlcat(cmdline, \" \", sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t\tif (strlcat(cmdline, argv[i], sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t}\n\n\tcmd = argv[0];\n\tif (!permit(uid, groups, ngroups, &rule, target, cmd,\n\t    (const char **)argv + 1)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_NOTICE,\n\t\t    \"failed command for %s: %s\", mypw->pw_name, cmdline);\n\t\terrc(1, EPERM, NULL);\n\t}\n\n#if defined(__OpenBSD__) || defined(USE_SHADOW)\n\tif (!(rule->options & NOPASS)) {\n\t\tif (nflag)\n\t\t\terrx(1, \"Authorization required\");\n\n# ifdef __OpenBSD__\n\t\tauthuser(mypw->pw_name, login_style, rule->options & PERSIST);\n# else\n\t\tshadowauth(mypw->pw_name, rule->options & PERSIST);\n# endif\n\t}\n\n# ifdef __OpenBSD__\n\tif (pledge(\"stdio rpath getpw exec id\", NULL) == -1)\n\t\terr(1, \"pledge\");\n# endif\n\n#elif !defined(USE_PAM)\n\t(void) nflag;\n\tif (!(rule->options & NOPASS)) {\n\t\terrx(1, \"Authorization required\");\n\t}\n#endif /* !(__OpenBSD__ || USE_SHADOW) && !USE_PAM */\n\n#ifdef __OpenBSD__\n\trv = getpwuid_r(target, &targpwstore, targpwbuf, sizeof(targpwbuf), &targpw);\n\tif (rv != 0)\n\t\terrx(1, \"no passwd entry for target\");\n#else\n\tfor (size_t sz = 1024; sz <= 16*1024; sz *= 2) {\n\t\ttargpwbuf = reallocarray(targpwbuf, sz, sizeof (char));\n\t\tif (targpwbuf == NULL)\n\t\t\terrx(1, \"can't allocate targpwbuf\");\n\t\trv = getpwuid_r(target, &targpwstore, targpwbuf, sz, &targpw);\n\t\tif (rv != ERANGE)\n\t\t\tbreak;\n\t}\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n#endif\n\tif (targpw == NULL)\n\t\terr(1, \"getpwuid_r failed\");\n\n#if defined(USE_PAM)\n\tpamauth(targpw->pw_name, mypw->pw_name, !nflag, rule->options & NOPASS,\n\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_SETUSERCONTEXT\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPATH |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n#endif\n\n#ifdef __OpenBSD__\n\tif (pledge(\"stdio rpath exec\", NULL) == -1)\n\t\terr(1, \"pledge\");\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse\n\t\tcwd = cwdpath;\n\n#ifdef __OpenBSD__\n\tif (pledge(\"stdio exec\", NULL) == -1)\n\t\terr(1, \"pledge\");\n#endif\n\n\tsyslog(LOG_AUTHPRIV | LOG_INFO, \"%s ran command %s as %s from %s\",\n\t    mypw->pw_name, cmdline, targpw->pw_name, cwd);\n\n\tenvp = prepenv(rule, mypw, targpw);\n\n\tif (rule->cmd) {\n\t\t/* do this again after setusercontext reset it */\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t}\n\texecvpe(cmd, argv, envp);\n\tif (errno == ENOENT)\n\t\terrx(1, \"%s: command not found\", cmd);\n\terr(1, \"%s\", cmd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -198,6 +198,7 @@\n \n #ifdef HAVE_SETUSERCONTEXT\n \tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n+\t    LOGIN_SETPATH |\n \t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n \t    LOGIN_SETUSER) != 0)\n \t\terrx(1, \"failed to set user context for target\");\n@@ -228,9 +229,10 @@\n \tsyslog(LOG_AUTHPRIV | LOG_INFO, \"%s ran command %s as %s from %s\",\n \t    mypw->pw_name, cmdline, targpw->pw_name, cwd);\n \n-\tenvp = prepenv(rule);\n+\tenvp = prepenv(rule, mypw, targpw);\n \n \tif (rule->cmd) {\n+\t\t/* do this again after setusercontext reset it */\n \t\tif (setenv(\"PATH\", safepath, 1) == -1)\n \t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tenvp = prepenv(rule);"
            ],
            "added_lines": [
                "\t    LOGIN_SETPATH |",
                "\tenvp = prepenv(rule, mypw, targpw);",
                "\t\t/* do this again after setusercontext reset it */"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25016",
        "func_name": "Duncaen/OpenDoas/createenv",
        "description": "In OpenDoas from 6.6 to 6.8 the users PATH variable was incorrectly inherited by authenticated executions if the authenticating rule allowed the user to execute any command. Rules that only allowed to authenticated user to execute specific commands were not affected by this issue.",
        "git_url": "https://github.com/Duncaen/OpenDoas/commit/01c658f8c45cb92a343be5f32aa6da70b2032168",
        "commit_title": "redo the environment inheritance to not inherit. it was intended to make life easier, but it can be surprising or even unsafe. instead, reset just about everything to the target user's values. ok deraadt martijn Thanks to Sander Bos in particular for pointing out some nasty edge cases.",
        "commit_text": "",
        "func_before": "static struct env *\ncreateenv(const struct rule *rule)\n{\n\tstruct env *env;\n\tu_int i;\n\n\tenv = malloc(sizeof(*env));\n\tif (!env)\n\t\terr(1, NULL);\n\tRB_INIT(&env->root);\n\tenv->count = 0;\n\n\tif (rule->options & KEEPENV) {\n\t\textern char **environ;\n\n\t\tfor (i = 0; environ[i] != NULL; i++) {\n\t\t\tstruct envnode *node;\n\t\t\tconst char *e, *eq;\n\t\t\tsize_t len;\n\t\t\tchar keybuf[1024];\n\n\t\t\te = environ[i];\n\n\t\t\t/* ignore invalid or overlong names */\n\t\t\tif ((eq = strchr(e, '=')) == NULL || eq == e)\n\t\t\t\tcontinue;\n\t\t\tlen = eq - e;\n\t\t\tif (len > sizeof(keybuf) - 1)\n\t\t\t\tcontinue;\n\t\t\tmemcpy(keybuf, e, len);\n\t\t\tkeybuf[len] = '\\0';\n\n\t\t\tnode = createnode(keybuf, eq + 1);\n\t\t\tif (RB_INSERT(envtree, &env->root, node)) {\n\t\t\t\t/* ignore any later duplicates */\n\t\t\t\tfreenode(node);\n\t\t\t} else {\n\t\t\t\tenv->count++;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn env;\n}",
        "func": "static struct env *\ncreateenv(const struct rule *rule, const struct passwd *mypw,\n    const struct passwd *targpw)\n{\n\tstruct env *env;\n\tu_int i;\n\n\tenv = malloc(sizeof(*env));\n\tif (!env)\n\t\terr(1, NULL);\n\tRB_INIT(&env->root);\n\tenv->count = 0;\n\n\taddnode(env, \"DOAS_USER\", mypw->pw_name);\n\n\tif (rule->options & KEEPENV) {\n\t\textern char **environ;\n\n\t\tfor (i = 0; environ[i] != NULL; i++) {\n\t\t\tstruct envnode *node;\n\t\t\tconst char *e, *eq;\n\t\t\tsize_t len;\n\t\t\tchar keybuf[1024];\n\n\t\t\te = environ[i];\n\n\t\t\t/* ignore invalid or overlong names */\n\t\t\tif ((eq = strchr(e, '=')) == NULL || eq == e)\n\t\t\t\tcontinue;\n\t\t\tlen = eq - e;\n\t\t\tif (len > sizeof(keybuf) - 1)\n\t\t\t\tcontinue;\n\t\t\tmemcpy(keybuf, e, len);\n\t\t\tkeybuf[len] = '\\0';\n\n\t\t\tnode = createnode(keybuf, eq + 1);\n\t\t\tif (RB_INSERT(envtree, &env->root, node)) {\n\t\t\t\t/* ignore any later duplicates */\n\t\t\t\tfreenode(node);\n\t\t\t} else {\n\t\t\t\tenv->count++;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatic const char *copyset[] = {\n\t\t\t\"DISPLAY\", \"TERM\",\n\t\t\tNULL\n\t\t};\n\n\t\taddnode(env, \"HOME\", targpw->pw_dir);\n\t\taddnode(env, \"LOGNAME\", targpw->pw_name);\n\t\taddnode(env, \"PATH\", getenv(\"PATH\"));\n\t\taddnode(env, \"SHELL\", targpw->pw_shell);\n\t\taddnode(env, \"USER\", targpw->pw_name);\n\n\t\tfillenv(env, copyset);\n\t}\n\n\treturn env;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n static struct env *\n-createenv(const struct rule *rule)\n+createenv(const struct rule *rule, const struct passwd *mypw,\n+    const struct passwd *targpw)\n {\n \tstruct env *env;\n \tu_int i;\n@@ -9,6 +10,8 @@\n \t\terr(1, NULL);\n \tRB_INIT(&env->root);\n \tenv->count = 0;\n+\n+\taddnode(env, \"DOAS_USER\", mypw->pw_name);\n \n \tif (rule->options & KEEPENV) {\n \t\textern char **environ;\n@@ -38,6 +41,19 @@\n \t\t\t\tenv->count++;\n \t\t\t}\n \t\t}\n+\t} else {\n+\t\tstatic const char *copyset[] = {\n+\t\t\t\"DISPLAY\", \"TERM\",\n+\t\t\tNULL\n+\t\t};\n+\n+\t\taddnode(env, \"HOME\", targpw->pw_dir);\n+\t\taddnode(env, \"LOGNAME\", targpw->pw_name);\n+\t\taddnode(env, \"PATH\", getenv(\"PATH\"));\n+\t\taddnode(env, \"SHELL\", targpw->pw_shell);\n+\t\taddnode(env, \"USER\", targpw->pw_name);\n+\n+\t\tfillenv(env, copyset);\n \t}\n \n \treturn env;",
        "diff_line_info": {
            "deleted_lines": [
                "createenv(const struct rule *rule)"
            ],
            "added_lines": [
                "createenv(const struct rule *rule, const struct passwd *mypw,",
                "    const struct passwd *targpw)",
                "",
                "\taddnode(env, \"DOAS_USER\", mypw->pw_name);",
                "\t} else {",
                "\t\tstatic const char *copyset[] = {",
                "\t\t\t\"DISPLAY\", \"TERM\",",
                "\t\t\tNULL",
                "\t\t};",
                "",
                "\t\taddnode(env, \"HOME\", targpw->pw_dir);",
                "\t\taddnode(env, \"LOGNAME\", targpw->pw_name);",
                "\t\taddnode(env, \"PATH\", getenv(\"PATH\"));",
                "\t\taddnode(env, \"SHELL\", targpw->pw_shell);",
                "\t\taddnode(env, \"USER\", targpw->pw_name);",
                "",
                "\t\tfillenv(env, copyset);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25016",
        "func_name": "Duncaen/OpenDoas/prepenv",
        "description": "In OpenDoas from 6.6 to 6.8 the users PATH variable was incorrectly inherited by authenticated executions if the authenticating rule allowed the user to execute any command. Rules that only allowed to authenticated user to execute specific commands were not affected by this issue.",
        "git_url": "https://github.com/Duncaen/OpenDoas/commit/01c658f8c45cb92a343be5f32aa6da70b2032168",
        "commit_title": "redo the environment inheritance to not inherit. it was intended to make life easier, but it can be surprising or even unsafe. instead, reset just about everything to the target user's values. ok deraadt martijn Thanks to Sander Bos in particular for pointing out some nasty edge cases.",
        "commit_text": "",
        "func_before": "char **\nprepenv(const struct rule *rule)\n{\n\tstatic const char *safeset[] = {\n\t\t\"DISPLAY\", \"HOME\", \"LOGNAME\", \"MAIL\",\n\t\t\"PATH\", \"TERM\", \"USER\", \"USERNAME\",\n\t\tNULL\n\t};\n\tstruct env *env;\n\n\tenv = createenv(rule);\n\n\t/* if we started with blank, fill some defaults then apply rules */\n\tif (!(rule->options & KEEPENV))\n\t\tfillenv(env, safeset);\n\tif (rule->envlist)\n\t\tfillenv(env, rule->envlist);\n\n\treturn flattenenv(env);\n}",
        "func": "char **\nprepenv(const struct rule *rule, const struct passwd *mypw,\n    const struct passwd *targpw)\n{\n\tstruct env *env;\n\n\tenv = createenv(rule, mypw, targpw);\n\tif (rule->envlist)\n\t\tfillenv(env, rule->envlist);\n\n\treturn flattenenv(env);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,18 +1,10 @@\n char **\n-prepenv(const struct rule *rule)\n+prepenv(const struct rule *rule, const struct passwd *mypw,\n+    const struct passwd *targpw)\n {\n-\tstatic const char *safeset[] = {\n-\t\t\"DISPLAY\", \"HOME\", \"LOGNAME\", \"MAIL\",\n-\t\t\"PATH\", \"TERM\", \"USER\", \"USERNAME\",\n-\t\tNULL\n-\t};\n \tstruct env *env;\n \n-\tenv = createenv(rule);\n-\n-\t/* if we started with blank, fill some defaults then apply rules */\n-\tif (!(rule->options & KEEPENV))\n-\t\tfillenv(env, safeset);\n+\tenv = createenv(rule, mypw, targpw);\n \tif (rule->envlist)\n \t\tfillenv(env, rule->envlist);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "prepenv(const struct rule *rule)",
                "\tstatic const char *safeset[] = {",
                "\t\t\"DISPLAY\", \"HOME\", \"LOGNAME\", \"MAIL\",",
                "\t\t\"PATH\", \"TERM\", \"USER\", \"USERNAME\",",
                "\t\tNULL",
                "\t};",
                "\tenv = createenv(rule);",
                "",
                "\t/* if we started with blank, fill some defaults then apply rules */",
                "\tif (!(rule->options & KEEPENV))",
                "\t\tfillenv(env, safeset);"
            ],
            "added_lines": [
                "prepenv(const struct rule *rule, const struct passwd *mypw,",
                "    const struct passwd *targpw)",
                "\tenv = createenv(rule, mypw, targpw);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-25016",
        "func_name": "Duncaen/OpenDoas/main",
        "description": "In OpenDoas from 6.6 to 6.8 the users PATH variable was incorrectly inherited by authenticated executions if the authenticating rule allowed the user to execute any command. Rules that only allowed to authenticated user to execute specific commands were not affected by this issue.",
        "git_url": "https://github.com/Duncaen/OpenDoas/commit/d5acd52e2a15c36a8e06f9103d35622933aa422d",
        "commit_title": "correctly reset path for rules without specific command",
        "commit_text": " This is a fixup for commit 01c658f8c45cb92a343be5f32aa6da70b2032168 where the behaviour was changed to not inherit the PATH variable by default.",
        "func_before": "int\nmain(int argc, char **argv)\n{\n\tconst char *safepath = \"/bin:/sbin:/usr/bin:/usr/sbin:\"\n\t    \"/usr/local/bin:/usr/local/sbin\";\n\tconst char *confpath = NULL;\n\tchar *shargv[] = { NULL, NULL };\n\tchar *sh;\n\tconst char *p;\n\tconst char *cmd;\n\tchar cmdline[LINE_MAX];\n\tstruct passwd mypwstore, targpwstore;\n\tstruct passwd *mypw, *targpw;\n\tconst struct rule *rule;\n\tuid_t uid;\n\tuid_t target = 0;\n\tgid_t groups[NGROUPS_MAX + 1];\n\tint ngroups;\n\tint i, ch, rv;\n\tint sflag = 0;\n\tint nflag = 0;\n\tchar cwdpath[PATH_MAX];\n\tconst char *cwd;\n\tchar **envp;\n\n\tsetprogname(\"doas\");\n\n\tclosefrom(STDERR_FILENO + 1);\n\n\tuid = getuid();\n\n\twhile ((ch = getopt(argc, argv, \"+C:Lnsu:\")) != -1) {\n\t\tswitch (ch) {\n\t\tcase 'C':\n\t\t\tconfpath = optarg;\n\t\t\tbreak;\n\t\tcase 'L':\n#if defined(USE_TIMESTAMP)\n\t\t\texit(timestamp_clear() == -1);\n#else\n\t\t\texit(0);\n#endif\n\t\tcase 'u':\n\t\t\tif (parseuid(optarg, &target) != 0)\n\t\t\t\terrx(1, \"unknown user\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tnflag = 1;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tsflag = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\tbreak;\n\t\t}\n\t}\n\targv += optind;\n\targc -= optind;\n\n\tif (confpath) {\n\t\tif (sflag)\n\t\t\tusage();\n\t} else if ((!sflag && !argc) || (sflag && argc))\n\t\tusage();\n\n\trv = mygetpwuid_r(uid, &mypwstore, &mypw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n\tif (mypw == NULL)\n\t\terrx(1, \"no passwd entry for self\");\n\tngroups = getgroups(NGROUPS_MAX, groups);\n\tif (ngroups == -1)\n\t\terr(1, \"can't get groups\");\n\tgroups[ngroups++] = getgid();\n\n\tif (sflag) {\n\t\tsh = getenv(\"SHELL\");\n\t\tif (sh == NULL || *sh == '\\0') {\n\t\t\tshargv[0] = mypw->pw_shell;\n\t\t} else\n\t\t\tshargv[0] = sh;\n\t\targv = shargv;\n\t\targc = 1;\n\t}\n\n\tif (confpath) {\n\t\tcheckconfig(confpath, argc, argv, uid, groups, ngroups,\n\t\t    target);\n\t\texit(1);\t/* fail safe */\n\t}\n\n\tif (geteuid())\n\t\terrx(1, \"not installed setuid\");\n\n\tparseconfig(DOAS_CONF, 1);\n\n\t/* cmdline is used only for logging, no need to abort on truncate */\n\t(void)strlcpy(cmdline, argv[0], sizeof(cmdline));\n\tfor (i = 1; i < argc; i++) {\n\t\tif (strlcat(cmdline, \" \", sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t\tif (strlcat(cmdline, argv[i], sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t}\n\n\tcmd = argv[0];\n\tif (!permit(uid, groups, ngroups, &rule, target, cmd,\n\t    (const char **)argv + 1)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_NOTICE,\n\t\t    \"command not permitted for %s: %s\", mypw->pw_name, cmdline);\n\t\terrc(1, EPERM, NULL);\n\t}\n\n#if defined(USE_SHADOW)\n\tif (!(rule->options & NOPASS)) {\n\t\tif (nflag)\n\t\t\terrx(1, \"Authorization required\");\n\n\t\tshadowauth(mypw->pw_name, rule->options & PERSIST);\n\t}\n#elif !defined(USE_PAM)\n\t/* no authentication provider, only allow NOPASS rules */\n\t(void) nflag;\n\tif (!(rule->options & NOPASS))\n\t\terrx(1, \"Authorization required\");\n#endif\n\n\tif ((p = getenv(\"PATH\")) != NULL)\n\t\tformerpath = strdup(p);\n\tif (formerpath == NULL)\n\t\tformerpath = \"\";\n\n\tif (rule->cmd) {\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t}\n\n\trv = mygetpwuid_r(target, &targpwstore, &targpw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n\tif (targpw == NULL)\n\t\terrx(1, \"no passwd entry for target\");\n\n#if defined(USE_PAM)\n\tpamauth(targpw->pw_name, mypw->pw_name, !nflag, rule->options & NOPASS,\n\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_LOGIN_CAP_H\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse\n\t\tcwd = cwdpath;\n\n\tif (!(rule->options & NOLOG)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_INFO,\n\t\t    \"%s ran command %s as %s from %s\",\n\t\t    mypw->pw_name, cmdline, targpw->pw_name, cwd);\n\t}\n\n\tenvp = prepenv(rule, mypw, targpw);\n\n\t/* setusercontext set path for the next process, so reset it for us */\n\tif (rule->cmd) {\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t} else {\n\t\tif (setenv(\"PATH\", formerpath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", formerpath);\n\t}\n\texecvpe(cmd, argv, envp);\n\tif (errno == ENOENT)\n\t\terrx(1, \"%s: command not found\", cmd);\n\terr(1, \"%s\", cmd);\n}",
        "func": "int\nmain(int argc, char **argv)\n{\n\tconst char *safepath = \"/bin:/sbin:/usr/bin:/usr/sbin:\"\n\t    \"/usr/local/bin:/usr/local/sbin\";\n\tconst char *confpath = NULL;\n\tchar *shargv[] = { NULL, NULL };\n\tchar *sh;\n\tconst char *p;\n\tconst char *cmd;\n\tchar cmdline[LINE_MAX];\n\tstruct passwd mypwstore, targpwstore;\n\tstruct passwd *mypw, *targpw;\n\tconst struct rule *rule;\n\tuid_t uid;\n\tuid_t target = 0;\n\tgid_t groups[NGROUPS_MAX + 1];\n\tint ngroups;\n\tint i, ch, rv;\n\tint sflag = 0;\n\tint nflag = 0;\n\tchar cwdpath[PATH_MAX];\n\tconst char *cwd;\n\tchar **envp;\n\n\tsetprogname(\"doas\");\n\n\tclosefrom(STDERR_FILENO + 1);\n\n\tuid = getuid();\n\n\twhile ((ch = getopt(argc, argv, \"+C:Lnsu:\")) != -1) {\n\t\tswitch (ch) {\n\t\tcase 'C':\n\t\t\tconfpath = optarg;\n\t\t\tbreak;\n\t\tcase 'L':\n#if defined(USE_TIMESTAMP)\n\t\t\texit(timestamp_clear() == -1);\n#else\n\t\t\texit(0);\n#endif\n\t\tcase 'u':\n\t\t\tif (parseuid(optarg, &target) != 0)\n\t\t\t\terrx(1, \"unknown user\");\n\t\t\tbreak;\n\t\tcase 'n':\n\t\t\tnflag = 1;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\tsflag = 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t\tbreak;\n\t\t}\n\t}\n\targv += optind;\n\targc -= optind;\n\n\tif (confpath) {\n\t\tif (sflag)\n\t\t\tusage();\n\t} else if ((!sflag && !argc) || (sflag && argc))\n\t\tusage();\n\n\trv = mygetpwuid_r(uid, &mypwstore, &mypw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n\tif (mypw == NULL)\n\t\terrx(1, \"no passwd entry for self\");\n\tngroups = getgroups(NGROUPS_MAX, groups);\n\tif (ngroups == -1)\n\t\terr(1, \"can't get groups\");\n\tgroups[ngroups++] = getgid();\n\n\tif (sflag) {\n\t\tsh = getenv(\"SHELL\");\n\t\tif (sh == NULL || *sh == '\\0') {\n\t\t\tshargv[0] = mypw->pw_shell;\n\t\t} else\n\t\t\tshargv[0] = sh;\n\t\targv = shargv;\n\t\targc = 1;\n\t}\n\n\tif (confpath) {\n\t\tcheckconfig(confpath, argc, argv, uid, groups, ngroups,\n\t\t    target);\n\t\texit(1);\t/* fail safe */\n\t}\n\n\tif (geteuid())\n\t\terrx(1, \"not installed setuid\");\n\n\tparseconfig(DOAS_CONF, 1);\n\n\t/* cmdline is used only for logging, no need to abort on truncate */\n\t(void)strlcpy(cmdline, argv[0], sizeof(cmdline));\n\tfor (i = 1; i < argc; i++) {\n\t\tif (strlcat(cmdline, \" \", sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t\tif (strlcat(cmdline, argv[i], sizeof(cmdline)) >= sizeof(cmdline))\n\t\t\tbreak;\n\t}\n\n\tcmd = argv[0];\n\tif (!permit(uid, groups, ngroups, &rule, target, cmd,\n\t    (const char **)argv + 1)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_NOTICE,\n\t\t    \"command not permitted for %s: %s\", mypw->pw_name, cmdline);\n\t\terrc(1, EPERM, NULL);\n\t}\n\n#if defined(USE_SHADOW)\n\tif (!(rule->options & NOPASS)) {\n\t\tif (nflag)\n\t\t\terrx(1, \"Authorization required\");\n\n\t\tshadowauth(mypw->pw_name, rule->options & PERSIST);\n\t}\n#elif !defined(USE_PAM)\n\t/* no authentication provider, only allow NOPASS rules */\n\t(void) nflag;\n\tif (!(rule->options & NOPASS))\n\t\terrx(1, \"Authorization required\");\n#endif\n\n\tif ((p = getenv(\"PATH\")) != NULL)\n\t\tformerpath = strdup(p);\n\tif (formerpath == NULL)\n\t\tformerpath = \"\";\n\n\tif (rule->cmd) {\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t}\n\n\trv = mygetpwuid_r(target, &targpwstore, &targpw);\n\tif (rv != 0)\n\t\terr(1, \"getpwuid_r failed\");\n\tif (targpw == NULL)\n\t\terrx(1, \"no passwd entry for target\");\n\n#if defined(USE_PAM)\n\tpamauth(targpw->pw_name, mypw->pw_name, !nflag, rule->options & NOPASS,\n\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_LOGIN_CAP_H\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPATH |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\terr(1, \"failed to set PATH '%s'\", safepath);\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse\n\t\tcwd = cwdpath;\n\n\tif (!(rule->options & NOLOG)) {\n\t\tsyslog(LOG_AUTHPRIV | LOG_INFO,\n\t\t    \"%s ran command %s as %s from %s\",\n\t\t    mypw->pw_name, cmdline, targpw->pw_name, cwd);\n\t}\n\n\tenvp = prepenv(rule, mypw, targpw);\n\n\t/* setusercontext set path for the next process, so reset it for us */\n\tif (rule->cmd) {\n\t\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", safepath);\n\t} else {\n\t\tif (setenv(\"PATH\", formerpath, 1) == -1)\n\t\t\terr(1, \"failed to set PATH '%s'\", formerpath);\n\t}\n\texecvpe(cmd, argv, envp);\n\tif (errno == ENOENT)\n\t\terrx(1, \"%s: command not found\", cmd);\n\terr(1, \"%s\", cmd);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -149,6 +149,7 @@\n \n #ifdef HAVE_LOGIN_CAP_H\n \tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n+\t    LOGIN_SETPATH |\n \t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n \t    LOGIN_SETUSER) != 0)\n \t\terrx(1, \"failed to set user context for target\");\n@@ -159,6 +160,8 @@\n \t\terr(1, \"initgroups\");\n \tif (setresuid(target, target, target) != 0)\n \t\terr(1, \"setresuid\");\n+\tif (setenv(\"PATH\", safepath, 1) == -1)\n+\t\terr(1, \"failed to set PATH '%s'\", safepath);\n #endif\n \n \tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t    LOGIN_SETPATH |",
                "\tif (setenv(\"PATH\", safepath, 1) == -1)",
                "\t\terr(1, \"failed to set PATH '%s'\", safepath);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_xattr_set",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_xattr_set(const struct xattr_handler *handler,\n\t\t\t  struct dentry *dentry, struct inode *inode,\n\t\t\t  const char *name, const void *value, size_t size,\n\t\t\t  int flags)\n{\n\tif (!value)\n\t\treturn fuse_removexattr(inode, name);\n\n\treturn fuse_setxattr(inode, name, value, size, flags);\n}",
        "func": "static int fuse_xattr_set(const struct xattr_handler *handler,\n\t\t\t  struct dentry *dentry, struct inode *inode,\n\t\t\t  const char *name, const void *value, size_t size,\n\t\t\t  int flags)\n{\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!value)\n\t\treturn fuse_removexattr(inode, name);\n\n\treturn fuse_setxattr(inode, name, value, size, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n \t\t\t  const char *name, const void *value, size_t size,\n \t\t\t  int flags)\n {\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n+\n \tif (!value)\n \t\treturn fuse_removexattr(inode, name);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_listxattr",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "ssize_t fuse_listxattr(struct dentry *entry, char *list, size_t size)\n{\n\tstruct inode *inode = d_inode(entry);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tFUSE_ARGS(args);\n\tstruct fuse_getxattr_in inarg;\n\tstruct fuse_getxattr_out outarg;\n\tssize_t ret;\n\n\tif (!fuse_allow_current_process(fm->fc))\n\t\treturn -EACCES;\n\n\tif (fm->fc->no_listxattr)\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.size = size;\n\targs.opcode = FUSE_LISTXATTR;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\t/* This is really two different operations rolled into one */\n\targs.out_numargs = 1;\n\tif (size) {\n\t\targs.out_argvar = true;\n\t\targs.out_args[0].size = size;\n\t\targs.out_args[0].value = list;\n\t} else {\n\t\targs.out_args[0].size = sizeof(outarg);\n\t\targs.out_args[0].value = &outarg;\n\t}\n\tret = fuse_simple_request(fm, &args);\n\tif (!ret && !size)\n\t\tret = min_t(ssize_t, outarg.size, XATTR_LIST_MAX);\n\tif (ret > 0 && size)\n\t\tret = fuse_verify_xattr_list(list, ret);\n\tif (ret == -ENOSYS) {\n\t\tfm->fc->no_listxattr = 1;\n\t\tret = -EOPNOTSUPP;\n\t}\n\treturn ret;\n}",
        "func": "ssize_t fuse_listxattr(struct dentry *entry, char *list, size_t size)\n{\n\tstruct inode *inode = d_inode(entry);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tFUSE_ARGS(args);\n\tstruct fuse_getxattr_in inarg;\n\tstruct fuse_getxattr_out outarg;\n\tssize_t ret;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!fuse_allow_current_process(fm->fc))\n\t\treturn -EACCES;\n\n\tif (fm->fc->no_listxattr)\n\t\treturn -EOPNOTSUPP;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.size = size;\n\targs.opcode = FUSE_LISTXATTR;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\t/* This is really two different operations rolled into one */\n\targs.out_numargs = 1;\n\tif (size) {\n\t\targs.out_argvar = true;\n\t\targs.out_args[0].size = size;\n\t\targs.out_args[0].value = list;\n\t} else {\n\t\targs.out_args[0].size = sizeof(outarg);\n\t\targs.out_args[0].value = &outarg;\n\t}\n\tret = fuse_simple_request(fm, &args);\n\tif (!ret && !size)\n\t\tret = min_t(ssize_t, outarg.size, XATTR_LIST_MAX);\n\tif (ret > 0 && size)\n\t\tret = fuse_verify_xattr_list(list, ret);\n\tif (ret == -ENOSYS) {\n\t\tfm->fc->no_listxattr = 1;\n\t\tret = -EOPNOTSUPP;\n\t}\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,9 @@\n \tstruct fuse_getxattr_in inarg;\n \tstruct fuse_getxattr_out outarg;\n \tssize_t ret;\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \tif (!fuse_allow_current_process(fm->fc))\n \t\treturn -EACCES;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_xattr_get",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_xattr_get(const struct xattr_handler *handler,\n\t\t\t struct dentry *dentry, struct inode *inode,\n\t\t\t const char *name, void *value, size_t size)\n{\n\treturn fuse_getxattr(inode, name, value, size);\n}",
        "func": "static int fuse_xattr_get(const struct xattr_handler *handler,\n\t\t\t struct dentry *dentry, struct inode *inode,\n\t\t\t const char *name, void *value, size_t size)\n{\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\treturn fuse_getxattr(inode, name, value, size);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,5 +2,8 @@\n \t\t\t struct dentry *dentry, struct inode *inode,\n \t\t\t const char *name, void *value, size_t size)\n {\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n+\n \treturn fuse_getxattr(inode, name, value, size);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_direntplus_link",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_direntplus_link(struct file *file,\n\t\t\t\tstruct fuse_direntplus *direntplus,\n\t\t\t\tu64 attr_version)\n{\n\tstruct fuse_entry_out *o = &direntplus->entry_out;\n\tstruct fuse_dirent *dirent = &direntplus->dirent;\n\tstruct dentry *parent = file->f_path.dentry;\n\tstruct qstr name = QSTR_INIT(dirent->name, dirent->namelen);\n\tstruct dentry *dentry;\n\tstruct dentry *alias;\n\tstruct inode *dir = d_inode(parent);\n\tstruct fuse_conn *fc;\n\tstruct inode *inode;\n\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);\n\n\tif (!o->nodeid) {\n\t\t/*\n\t\t * Unlike in the case of fuse_lookup, zero nodeid does not mean\n\t\t * ENOENT. Instead, it only means the userspace filesystem did\n\t\t * not want to return attributes/handle for this entry.\n\t\t *\n\t\t * So do nothing.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (name.name[0] == '.') {\n\t\t/*\n\t\t * We could potentially refresh the attributes of the directory\n\t\t * and its parent?\n\t\t */\n\t\tif (name.len == 1)\n\t\t\treturn 0;\n\t\tif (name.name[1] == '.' && name.len == 2)\n\t\t\treturn 0;\n\t}\n\n\tif (invalid_nodeid(o->nodeid))\n\t\treturn -EIO;\n\tif (fuse_invalid_attr(&o->attr))\n\t\treturn -EIO;\n\n\tfc = get_fuse_conn(dir);\n\n\tname.hash = full_name_hash(parent, name.name, name.len);\n\tdentry = d_lookup(parent, &name);\n\tif (!dentry) {\nretry:\n\t\tdentry = d_alloc_parallel(parent, &name, &wq);\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\t}\n\tif (!d_in_lookup(dentry)) {\n\t\tstruct fuse_inode *fi;\n\t\tinode = d_inode(dentry);\n\t\tif (!inode ||\n\t\t    get_node_id(inode) != o->nodeid ||\n\t\t    ((o->attr.mode ^ inode->i_mode) & S_IFMT)) {\n\t\t\td_invalidate(dentry);\n\t\t\tdput(dentry);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (is_bad_inode(inode)) {\n\t\t\tdput(dentry);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tfi = get_fuse_inode(inode);\n\t\tspin_lock(&fi->lock);\n\t\tfi->nlookup++;\n\t\tspin_unlock(&fi->lock);\n\n\t\tforget_all_cached_acls(inode);\n\t\tfuse_change_attributes(inode, &o->attr,\n\t\t\t\t       entry_attr_timeout(o),\n\t\t\t\t       attr_version);\n\t\t/*\n\t\t * The other branch comes via fuse_iget()\n\t\t * which bumps nlookup inside\n\t\t */\n\t} else {\n\t\tinode = fuse_iget(dir->i_sb, o->nodeid, o->generation,\n\t\t\t\t  &o->attr, entry_attr_timeout(o),\n\t\t\t\t  attr_version);\n\t\tif (!inode)\n\t\t\tinode = ERR_PTR(-ENOMEM);\n\n\t\talias = d_splice_alias(inode, dentry);\n\t\td_lookup_done(dentry);\n\t\tif (alias) {\n\t\t\tdput(dentry);\n\t\t\tdentry = alias;\n\t\t}\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\t}\n\tif (fc->readdirplus_auto)\n\t\tset_bit(FUSE_I_INIT_RDPLUS, &get_fuse_inode(inode)->state);\n\tfuse_change_entry_timeout(dentry, o);\n\n\tdput(dentry);\n\treturn 0;\n}",
        "func": "static int fuse_direntplus_link(struct file *file,\n\t\t\t\tstruct fuse_direntplus *direntplus,\n\t\t\t\tu64 attr_version)\n{\n\tstruct fuse_entry_out *o = &direntplus->entry_out;\n\tstruct fuse_dirent *dirent = &direntplus->dirent;\n\tstruct dentry *parent = file->f_path.dentry;\n\tstruct qstr name = QSTR_INIT(dirent->name, dirent->namelen);\n\tstruct dentry *dentry;\n\tstruct dentry *alias;\n\tstruct inode *dir = d_inode(parent);\n\tstruct fuse_conn *fc;\n\tstruct inode *inode;\n\tDECLARE_WAIT_QUEUE_HEAD_ONSTACK(wq);\n\n\tif (!o->nodeid) {\n\t\t/*\n\t\t * Unlike in the case of fuse_lookup, zero nodeid does not mean\n\t\t * ENOENT. Instead, it only means the userspace filesystem did\n\t\t * not want to return attributes/handle for this entry.\n\t\t *\n\t\t * So do nothing.\n\t\t */\n\t\treturn 0;\n\t}\n\n\tif (name.name[0] == '.') {\n\t\t/*\n\t\t * We could potentially refresh the attributes of the directory\n\t\t * and its parent?\n\t\t */\n\t\tif (name.len == 1)\n\t\t\treturn 0;\n\t\tif (name.name[1] == '.' && name.len == 2)\n\t\t\treturn 0;\n\t}\n\n\tif (invalid_nodeid(o->nodeid))\n\t\treturn -EIO;\n\tif (fuse_invalid_attr(&o->attr))\n\t\treturn -EIO;\n\n\tfc = get_fuse_conn(dir);\n\n\tname.hash = full_name_hash(parent, name.name, name.len);\n\tdentry = d_lookup(parent, &name);\n\tif (!dentry) {\nretry:\n\t\tdentry = d_alloc_parallel(parent, &name, &wq);\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\t}\n\tif (!d_in_lookup(dentry)) {\n\t\tstruct fuse_inode *fi;\n\t\tinode = d_inode(dentry);\n\t\tif (!inode ||\n\t\t    get_node_id(inode) != o->nodeid ||\n\t\t    ((o->attr.mode ^ inode->i_mode) & S_IFMT)) {\n\t\t\td_invalidate(dentry);\n\t\t\tdput(dentry);\n\t\t\tgoto retry;\n\t\t}\n\t\tif (fuse_is_bad(inode)) {\n\t\t\tdput(dentry);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tfi = get_fuse_inode(inode);\n\t\tspin_lock(&fi->lock);\n\t\tfi->nlookup++;\n\t\tspin_unlock(&fi->lock);\n\n\t\tforget_all_cached_acls(inode);\n\t\tfuse_change_attributes(inode, &o->attr,\n\t\t\t\t       entry_attr_timeout(o),\n\t\t\t\t       attr_version);\n\t\t/*\n\t\t * The other branch comes via fuse_iget()\n\t\t * which bumps nlookup inside\n\t\t */\n\t} else {\n\t\tinode = fuse_iget(dir->i_sb, o->nodeid, o->generation,\n\t\t\t\t  &o->attr, entry_attr_timeout(o),\n\t\t\t\t  attr_version);\n\t\tif (!inode)\n\t\t\tinode = ERR_PTR(-ENOMEM);\n\n\t\talias = d_splice_alias(inode, dentry);\n\t\td_lookup_done(dentry);\n\t\tif (alias) {\n\t\t\tdput(dentry);\n\t\t\tdentry = alias;\n\t\t}\n\t\tif (IS_ERR(dentry))\n\t\t\treturn PTR_ERR(dentry);\n\t}\n\tif (fc->readdirplus_auto)\n\t\tset_bit(FUSE_I_INIT_RDPLUS, &get_fuse_inode(inode)->state);\n\tfuse_change_entry_timeout(dentry, o);\n\n\tdput(dentry);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -60,7 +60,7 @@\n \t\t\tdput(dentry);\n \t\t\tgoto retry;\n \t\t}\n-\t\tif (is_bad_inode(inode)) {\n+\t\tif (fuse_is_bad(inode)) {\n \t\t\tdput(dentry);\n \t\t\treturn -EIO;\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (is_bad_inode(inode)) {"
            ],
            "added_lines": [
                "\t\tif (fuse_is_bad(inode)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_readdir",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "int fuse_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\tint err;\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\tmutex_lock(&ff->readdir.lock);\n\n\terr = UNCACHED;\n\tif (ff->open_flags & FOPEN_CACHE_DIR)\n\t\terr = fuse_readdir_cached(file, ctx);\n\tif (err == UNCACHED)\n\t\terr = fuse_readdir_uncached(file, ctx);\n\n\tmutex_unlock(&ff->readdir.lock);\n\n\treturn err;\n}",
        "func": "int fuse_readdir(struct file *file, struct dir_context *ctx)\n{\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\tint err;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tmutex_lock(&ff->readdir.lock);\n\n\terr = UNCACHED;\n\tif (ff->open_flags & FOPEN_CACHE_DIR)\n\t\terr = fuse_readdir_cached(file, ctx);\n\tif (err == UNCACHED)\n\t\terr = fuse_readdir_uncached(file, ctx);\n\n\tmutex_unlock(&ff->readdir.lock);\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tstruct inode *inode = file_inode(file);\n \tint err;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \tmutex_lock(&ff->readdir.lock);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_rename2",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_rename2(struct inode *olddir, struct dentry *oldent,\n\t\t\tstruct inode *newdir, struct dentry *newent,\n\t\t\tunsigned int flags)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(olddir);\n\tint err;\n\n\tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n\t\treturn -EINVAL;\n\n\tif (flags) {\n\t\tif (fc->no_rename2 || fc->minor < 23)\n\t\t\treturn -EINVAL;\n\n\t\terr = fuse_rename_common(olddir, oldent, newdir, newent, flags,\n\t\t\t\t\t FUSE_RENAME2,\n\t\t\t\t\t sizeof(struct fuse_rename2_in));\n\t\tif (err == -ENOSYS) {\n\t\t\tfc->no_rename2 = 1;\n\t\t\terr = -EINVAL;\n\t\t}\n\t} else {\n\t\terr = fuse_rename_common(olddir, oldent, newdir, newent, 0,\n\t\t\t\t\t FUSE_RENAME,\n\t\t\t\t\t sizeof(struct fuse_rename_in));\n\t}\n\n\treturn err;\n}",
        "func": "static int fuse_rename2(struct inode *olddir, struct dentry *oldent,\n\t\t\tstruct inode *newdir, struct dentry *newent,\n\t\t\tunsigned int flags)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(olddir);\n\tint err;\n\n\tif (fuse_is_bad(olddir))\n\t\treturn -EIO;\n\n\tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n\t\treturn -EINVAL;\n\n\tif (flags) {\n\t\tif (fc->no_rename2 || fc->minor < 23)\n\t\t\treturn -EINVAL;\n\n\t\terr = fuse_rename_common(olddir, oldent, newdir, newent, flags,\n\t\t\t\t\t FUSE_RENAME2,\n\t\t\t\t\t sizeof(struct fuse_rename2_in));\n\t\tif (err == -ENOSYS) {\n\t\t\tfc->no_rename2 = 1;\n\t\t\terr = -EINVAL;\n\t\t}\n\t} else {\n\t\terr = fuse_rename_common(olddir, oldent, newdir, newent, 0,\n\t\t\t\t\t FUSE_RENAME,\n\t\t\t\t\t sizeof(struct fuse_rename_in));\n\t}\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,9 @@\n {\n \tstruct fuse_conn *fc = get_fuse_conn(olddir);\n \tint err;\n+\n+\tif (fuse_is_bad(olddir))\n+\t\treturn -EIO;\n \n \tif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\n \t\treturn -EINVAL;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(olddir))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_do_setattr",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "int fuse_do_setattr(struct dentry *dentry, struct iattr *attr,\n\t\t    struct file *file)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_conn *fc = fm->fc;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tFUSE_ARGS(args);\n\tstruct fuse_setattr_in inarg;\n\tstruct fuse_attr_out outarg;\n\tbool is_truncate = false;\n\tbool is_wb = fc->writeback_cache;\n\tloff_t oldsize;\n\tint err;\n\tbool trust_local_cmtime = is_wb && S_ISREG(inode->i_mode);\n\tbool fault_blocked = false;\n\n\tif (!fc->default_permissions)\n\t\tattr->ia_valid |= ATTR_FORCE;\n\n\terr = setattr_prepare(dentry, attr);\n\tif (err)\n\t\treturn err;\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\tif (WARN_ON(!S_ISREG(inode->i_mode)))\n\t\t\treturn -EIO;\n\t\tis_truncate = true;\n\t}\n\n\tif (FUSE_IS_DAX(inode) && is_truncate) {\n\t\tdown_write(&fi->i_mmap_sem);\n\t\tfault_blocked = true;\n\t\terr = fuse_dax_break_layouts(inode, 0, 0);\n\t\tif (err) {\n\t\t\tup_write(&fi->i_mmap_sem);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (attr->ia_valid & ATTR_OPEN) {\n\t\t/* This is coming from open(..., ... | O_TRUNC); */\n\t\tWARN_ON(!(attr->ia_valid & ATTR_SIZE));\n\t\tWARN_ON(attr->ia_size != 0);\n\t\tif (fc->atomic_o_trunc) {\n\t\t\t/*\n\t\t\t * No need to send request to userspace, since actual\n\t\t\t * truncation has already been done by OPEN.  But still\n\t\t\t * need to truncate page cache.\n\t\t\t */\n\t\t\ti_size_write(inode, 0);\n\t\t\ttruncate_pagecache(inode, 0);\n\t\t\tgoto out;\n\t\t}\n\t\tfile = NULL;\n\t}\n\n\t/* Flush dirty data/metadata before non-truncate SETATTR */\n\tif (is_wb && S_ISREG(inode->i_mode) &&\n\t    attr->ia_valid &\n\t\t\t(ATTR_MODE | ATTR_UID | ATTR_GID | ATTR_MTIME_SET |\n\t\t\t ATTR_TIMES_SET)) {\n\t\terr = write_inode_now(inode, true);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tfuse_set_nowrite(inode);\n\t\tfuse_release_nowrite(inode);\n\t}\n\n\tif (is_truncate) {\n\t\tfuse_set_nowrite(inode);\n\t\tset_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\n\t\tif (trust_local_cmtime && attr->ia_size != inode->i_size)\n\t\t\tattr->ia_valid |= ATTR_MTIME | ATTR_CTIME;\n\t}\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tmemset(&outarg, 0, sizeof(outarg));\n\tiattr_to_fattr(fc, attr, &inarg, trust_local_cmtime);\n\tif (file) {\n\t\tstruct fuse_file *ff = file->private_data;\n\t\tinarg.valid |= FATTR_FH;\n\t\tinarg.fh = ff->fh;\n\t}\n\n\t/* Kill suid/sgid for non-directory chown unconditionally */\n\tif (fc->handle_killpriv_v2 && !S_ISDIR(inode->i_mode) &&\n\t    attr->ia_valid & (ATTR_UID | ATTR_GID))\n\t\tinarg.valid |= FATTR_KILL_SUIDGID;\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\t/* For mandatory locking in truncate */\n\t\tinarg.valid |= FATTR_LOCKOWNER;\n\t\tinarg.lock_owner = fuse_lock_owner_id(fc, current->files);\n\n\t\t/* Kill suid/sgid for truncate only if no CAP_FSETID */\n\t\tif (fc->handle_killpriv_v2 && !capable(CAP_FSETID))\n\t\t\tinarg.valid |= FATTR_KILL_SUIDGID;\n\t}\n\tfuse_setattr_fill(fc, &args, inode, &inarg, &outarg);\n\terr = fuse_simple_request(fm, &args);\n\tif (err) {\n\t\tif (err == -EINTR)\n\t\t\tfuse_invalidate_attr(inode);\n\t\tgoto error;\n\t}\n\n\tif (fuse_invalid_attr(&outarg.attr) ||\n\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\tmake_bad_inode(inode);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tspin_lock(&fi->lock);\n\t/* the kernel maintains i_mtime locally */\n\tif (trust_local_cmtime) {\n\t\tif (attr->ia_valid & ATTR_MTIME)\n\t\t\tinode->i_mtime = attr->ia_mtime;\n\t\tif (attr->ia_valid & ATTR_CTIME)\n\t\t\tinode->i_ctime = attr->ia_ctime;\n\t\t/* FIXME: clear I_DIRTY_SYNC? */\n\t}\n\n\tfuse_change_attributes_common(inode, &outarg.attr,\n\t\t\t\t      attr_timeout(&outarg));\n\toldsize = inode->i_size;\n\t/* see the comment in fuse_change_attributes() */\n\tif (!is_wb || is_truncate || !S_ISREG(inode->i_mode))\n\t\ti_size_write(inode, outarg.attr.size);\n\n\tif (is_truncate) {\n\t\t/* NOTE: this may release/reacquire fi->lock */\n\t\t__fuse_release_nowrite(inode);\n\t}\n\tspin_unlock(&fi->lock);\n\n\t/*\n\t * Only call invalidate_inode_pages2() after removing\n\t * FUSE_NOWRITE, otherwise fuse_launder_page() would deadlock.\n\t */\n\tif ((is_truncate || !is_wb) &&\n\t    S_ISREG(inode->i_mode) && oldsize != outarg.attr.size) {\n\t\ttruncate_pagecache(inode, outarg.attr.size);\n\t\tinvalidate_inode_pages2(inode->i_mapping);\n\t}\n\n\tclear_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\nout:\n\tif (fault_blocked)\n\t\tup_write(&fi->i_mmap_sem);\n\n\treturn 0;\n\nerror:\n\tif (is_truncate)\n\t\tfuse_release_nowrite(inode);\n\n\tclear_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\n\n\tif (fault_blocked)\n\t\tup_write(&fi->i_mmap_sem);\n\treturn err;\n}",
        "func": "int fuse_do_setattr(struct dentry *dentry, struct iattr *attr,\n\t\t    struct file *file)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_conn *fc = fm->fc;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tFUSE_ARGS(args);\n\tstruct fuse_setattr_in inarg;\n\tstruct fuse_attr_out outarg;\n\tbool is_truncate = false;\n\tbool is_wb = fc->writeback_cache;\n\tloff_t oldsize;\n\tint err;\n\tbool trust_local_cmtime = is_wb && S_ISREG(inode->i_mode);\n\tbool fault_blocked = false;\n\n\tif (!fc->default_permissions)\n\t\tattr->ia_valid |= ATTR_FORCE;\n\n\terr = setattr_prepare(dentry, attr);\n\tif (err)\n\t\treturn err;\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\tif (WARN_ON(!S_ISREG(inode->i_mode)))\n\t\t\treturn -EIO;\n\t\tis_truncate = true;\n\t}\n\n\tif (FUSE_IS_DAX(inode) && is_truncate) {\n\t\tdown_write(&fi->i_mmap_sem);\n\t\tfault_blocked = true;\n\t\terr = fuse_dax_break_layouts(inode, 0, 0);\n\t\tif (err) {\n\t\t\tup_write(&fi->i_mmap_sem);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (attr->ia_valid & ATTR_OPEN) {\n\t\t/* This is coming from open(..., ... | O_TRUNC); */\n\t\tWARN_ON(!(attr->ia_valid & ATTR_SIZE));\n\t\tWARN_ON(attr->ia_size != 0);\n\t\tif (fc->atomic_o_trunc) {\n\t\t\t/*\n\t\t\t * No need to send request to userspace, since actual\n\t\t\t * truncation has already been done by OPEN.  But still\n\t\t\t * need to truncate page cache.\n\t\t\t */\n\t\t\ti_size_write(inode, 0);\n\t\t\ttruncate_pagecache(inode, 0);\n\t\t\tgoto out;\n\t\t}\n\t\tfile = NULL;\n\t}\n\n\t/* Flush dirty data/metadata before non-truncate SETATTR */\n\tif (is_wb && S_ISREG(inode->i_mode) &&\n\t    attr->ia_valid &\n\t\t\t(ATTR_MODE | ATTR_UID | ATTR_GID | ATTR_MTIME_SET |\n\t\t\t ATTR_TIMES_SET)) {\n\t\terr = write_inode_now(inode, true);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tfuse_set_nowrite(inode);\n\t\tfuse_release_nowrite(inode);\n\t}\n\n\tif (is_truncate) {\n\t\tfuse_set_nowrite(inode);\n\t\tset_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\n\t\tif (trust_local_cmtime && attr->ia_size != inode->i_size)\n\t\t\tattr->ia_valid |= ATTR_MTIME | ATTR_CTIME;\n\t}\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tmemset(&outarg, 0, sizeof(outarg));\n\tiattr_to_fattr(fc, attr, &inarg, trust_local_cmtime);\n\tif (file) {\n\t\tstruct fuse_file *ff = file->private_data;\n\t\tinarg.valid |= FATTR_FH;\n\t\tinarg.fh = ff->fh;\n\t}\n\n\t/* Kill suid/sgid for non-directory chown unconditionally */\n\tif (fc->handle_killpriv_v2 && !S_ISDIR(inode->i_mode) &&\n\t    attr->ia_valid & (ATTR_UID | ATTR_GID))\n\t\tinarg.valid |= FATTR_KILL_SUIDGID;\n\n\tif (attr->ia_valid & ATTR_SIZE) {\n\t\t/* For mandatory locking in truncate */\n\t\tinarg.valid |= FATTR_LOCKOWNER;\n\t\tinarg.lock_owner = fuse_lock_owner_id(fc, current->files);\n\n\t\t/* Kill suid/sgid for truncate only if no CAP_FSETID */\n\t\tif (fc->handle_killpriv_v2 && !capable(CAP_FSETID))\n\t\t\tinarg.valid |= FATTR_KILL_SUIDGID;\n\t}\n\tfuse_setattr_fill(fc, &args, inode, &inarg, &outarg);\n\terr = fuse_simple_request(fm, &args);\n\tif (err) {\n\t\tif (err == -EINTR)\n\t\t\tfuse_invalidate_attr(inode);\n\t\tgoto error;\n\t}\n\n\tif (fuse_invalid_attr(&outarg.attr) ||\n\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\tfuse_make_bad(inode);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tspin_lock(&fi->lock);\n\t/* the kernel maintains i_mtime locally */\n\tif (trust_local_cmtime) {\n\t\tif (attr->ia_valid & ATTR_MTIME)\n\t\t\tinode->i_mtime = attr->ia_mtime;\n\t\tif (attr->ia_valid & ATTR_CTIME)\n\t\t\tinode->i_ctime = attr->ia_ctime;\n\t\t/* FIXME: clear I_DIRTY_SYNC? */\n\t}\n\n\tfuse_change_attributes_common(inode, &outarg.attr,\n\t\t\t\t      attr_timeout(&outarg));\n\toldsize = inode->i_size;\n\t/* see the comment in fuse_change_attributes() */\n\tif (!is_wb || is_truncate || !S_ISREG(inode->i_mode))\n\t\ti_size_write(inode, outarg.attr.size);\n\n\tif (is_truncate) {\n\t\t/* NOTE: this may release/reacquire fi->lock */\n\t\t__fuse_release_nowrite(inode);\n\t}\n\tspin_unlock(&fi->lock);\n\n\t/*\n\t * Only call invalidate_inode_pages2() after removing\n\t * FUSE_NOWRITE, otherwise fuse_launder_page() would deadlock.\n\t */\n\tif ((is_truncate || !is_wb) &&\n\t    S_ISREG(inode->i_mode) && oldsize != outarg.attr.size) {\n\t\ttruncate_pagecache(inode, outarg.attr.size);\n\t\tinvalidate_inode_pages2(inode->i_mapping);\n\t}\n\n\tclear_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\nout:\n\tif (fault_blocked)\n\t\tup_write(&fi->i_mmap_sem);\n\n\treturn 0;\n\nerror:\n\tif (is_truncate)\n\t\tfuse_release_nowrite(inode);\n\n\tclear_bit(FUSE_I_SIZE_UNSTABLE, &fi->state);\n\n\tif (fault_blocked)\n\t\tup_write(&fi->i_mmap_sem);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -108,7 +108,7 @@\n \n \tif (fuse_invalid_attr(&outarg.attr) ||\n \t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n-\t\tmake_bad_inode(inode);\n+\t\tfuse_make_bad(inode);\n \t\terr = -EIO;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tmake_bad_inode(inode);"
            ],
            "added_lines": [
                "\t\tfuse_make_bad(inode);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_rmdir",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_rmdir(struct inode *dir, struct dentry *entry)\n{\n\tint err;\n\tstruct fuse_mount *fm = get_fuse_mount(dir);\n\tFUSE_ARGS(args);\n\n\targs.opcode = FUSE_RMDIR;\n\targs.nodeid = get_node_id(dir);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = entry->d_name.len + 1;\n\targs.in_args[0].value = entry->d_name.name;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tclear_nlink(d_inode(entry));\n\t\tfuse_dir_changed(dir);\n\t\tfuse_invalidate_entry_cache(entry);\n\t} else if (err == -EINTR)\n\t\tfuse_invalidate_entry(entry);\n\treturn err;\n}",
        "func": "static int fuse_rmdir(struct inode *dir, struct dentry *entry)\n{\n\tint err;\n\tstruct fuse_mount *fm = get_fuse_mount(dir);\n\tFUSE_ARGS(args);\n\n\tif (fuse_is_bad(dir))\n\t\treturn -EIO;\n\n\targs.opcode = FUSE_RMDIR;\n\targs.nodeid = get_node_id(dir);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = entry->d_name.len + 1;\n\targs.in_args[0].value = entry->d_name.name;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tclear_nlink(d_inode(entry));\n\t\tfuse_dir_changed(dir);\n\t\tfuse_invalidate_entry_cache(entry);\n\t} else if (err == -EINTR)\n\t\tfuse_invalidate_entry(entry);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n \tint err;\n \tstruct fuse_mount *fm = get_fuse_mount(dir);\n \tFUSE_ARGS(args);\n+\n+\tif (fuse_is_bad(dir))\n+\t\treturn -EIO;\n \n \targs.opcode = FUSE_RMDIR;\n \targs.nodeid = get_node_id(dir);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(dir))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_unlink",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_unlink(struct inode *dir, struct dentry *entry)\n{\n\tint err;\n\tstruct fuse_mount *fm = get_fuse_mount(dir);\n\tFUSE_ARGS(args);\n\n\targs.opcode = FUSE_UNLINK;\n\targs.nodeid = get_node_id(dir);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = entry->d_name.len + 1;\n\targs.in_args[0].value = entry->d_name.name;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tstruct inode *inode = d_inode(entry);\n\t\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\t\tspin_lock(&fi->lock);\n\t\tfi->attr_version = atomic64_inc_return(&fm->fc->attr_version);\n\t\t/*\n\t\t * If i_nlink == 0 then unlink doesn't make sense, yet this can\n\t\t * happen if userspace filesystem is careless.  It would be\n\t\t * difficult to enforce correct nlink usage so just ignore this\n\t\t * condition here\n\t\t */\n\t\tif (inode->i_nlink > 0)\n\t\t\tdrop_nlink(inode);\n\t\tspin_unlock(&fi->lock);\n\t\tfuse_invalidate_attr(inode);\n\t\tfuse_dir_changed(dir);\n\t\tfuse_invalidate_entry_cache(entry);\n\t\tfuse_update_ctime(inode);\n\t} else if (err == -EINTR)\n\t\tfuse_invalidate_entry(entry);\n\treturn err;\n}",
        "func": "static int fuse_unlink(struct inode *dir, struct dentry *entry)\n{\n\tint err;\n\tstruct fuse_mount *fm = get_fuse_mount(dir);\n\tFUSE_ARGS(args);\n\n\tif (fuse_is_bad(dir))\n\t\treturn -EIO;\n\n\targs.opcode = FUSE_UNLINK;\n\targs.nodeid = get_node_id(dir);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = entry->d_name.len + 1;\n\targs.in_args[0].value = entry->d_name.name;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tstruct inode *inode = d_inode(entry);\n\t\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\t\tspin_lock(&fi->lock);\n\t\tfi->attr_version = atomic64_inc_return(&fm->fc->attr_version);\n\t\t/*\n\t\t * If i_nlink == 0 then unlink doesn't make sense, yet this can\n\t\t * happen if userspace filesystem is careless.  It would be\n\t\t * difficult to enforce correct nlink usage so just ignore this\n\t\t * condition here\n\t\t */\n\t\tif (inode->i_nlink > 0)\n\t\t\tdrop_nlink(inode);\n\t\tspin_unlock(&fi->lock);\n\t\tfuse_invalidate_attr(inode);\n\t\tfuse_dir_changed(dir);\n\t\tfuse_invalidate_entry_cache(entry);\n\t\tfuse_update_ctime(inode);\n\t} else if (err == -EINTR)\n\t\tfuse_invalidate_entry(entry);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n \tint err;\n \tstruct fuse_mount *fm = get_fuse_mount(dir);\n \tFUSE_ARGS(args);\n+\n+\tif (fuse_is_bad(dir))\n+\t\treturn -EIO;\n \n \targs.opcode = FUSE_UNLINK;\n \targs.nodeid = get_node_id(dir);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(dir))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_do_getattr",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_do_getattr(struct inode *inode, struct kstat *stat,\n\t\t\t   struct file *file)\n{\n\tint err;\n\tstruct fuse_getattr_in inarg;\n\tstruct fuse_attr_out outarg;\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tFUSE_ARGS(args);\n\tu64 attr_version;\n\n\tattr_version = fuse_get_attr_version(fm->fc);\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tmemset(&outarg, 0, sizeof(outarg));\n\t/* Directories have separate file-handle space */\n\tif (file && S_ISREG(inode->i_mode)) {\n\t\tstruct fuse_file *ff = file->private_data;\n\n\t\tinarg.getattr_flags |= FUSE_GETATTR_FH;\n\t\tinarg.fh = ff->fh;\n\t}\n\targs.opcode = FUSE_GETATTR;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\targs.out_numargs = 1;\n\targs.out_args[0].size = sizeof(outarg);\n\targs.out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tif (fuse_invalid_attr(&outarg.attr) ||\n\t\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\t\tmake_bad_inode(inode);\n\t\t\terr = -EIO;\n\t\t} else {\n\t\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t\t       attr_timeout(&outarg),\n\t\t\t\t\t       attr_version);\n\t\t\tif (stat)\n\t\t\t\tfuse_fillattr(inode, &outarg.attr, stat);\n\t\t}\n\t}\n\treturn err;\n}",
        "func": "static int fuse_do_getattr(struct inode *inode, struct kstat *stat,\n\t\t\t   struct file *file)\n{\n\tint err;\n\tstruct fuse_getattr_in inarg;\n\tstruct fuse_attr_out outarg;\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tFUSE_ARGS(args);\n\tu64 attr_version;\n\n\tattr_version = fuse_get_attr_version(fm->fc);\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tmemset(&outarg, 0, sizeof(outarg));\n\t/* Directories have separate file-handle space */\n\tif (file && S_ISREG(inode->i_mode)) {\n\t\tstruct fuse_file *ff = file->private_data;\n\n\t\tinarg.getattr_flags |= FUSE_GETATTR_FH;\n\t\tinarg.fh = ff->fh;\n\t}\n\targs.opcode = FUSE_GETATTR;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\targs.out_numargs = 1;\n\targs.out_args[0].size = sizeof(outarg);\n\targs.out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tif (fuse_invalid_attr(&outarg.attr) ||\n\t\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\t\tfuse_make_bad(inode);\n\t\t\terr = -EIO;\n\t\t} else {\n\t\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t\t       attr_timeout(&outarg),\n\t\t\t\t\t       attr_version);\n\t\t\tif (stat)\n\t\t\t\tfuse_fillattr(inode, &outarg.attr, stat);\n\t\t}\n\t}\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,7 @@\n \tif (!err) {\n \t\tif (fuse_invalid_attr(&outarg.attr) ||\n \t\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n-\t\t\tmake_bad_inode(inode);\n+\t\t\tfuse_make_bad(inode);\n \t\t\terr = -EIO;\n \t\t} else {\n \t\t\tfuse_change_attributes(inode, &outarg.attr,",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tmake_bad_inode(inode);"
            ],
            "added_lines": [
                "\t\t\tfuse_make_bad(inode);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_dir_fsync",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_dir_fsync(struct file *file, loff_t start, loff_t end,\n\t\t\t  int datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint err;\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\tif (fc->no_fsyncdir)\n\t\treturn 0;\n\n\tinode_lock(inode);\n\terr = fuse_fsync_common(file, start, end, datasync, FUSE_FSYNCDIR);\n\tif (err == -ENOSYS) {\n\t\tfc->no_fsyncdir = 1;\n\t\terr = 0;\n\t}\n\tinode_unlock(inode);\n\n\treturn err;\n}",
        "func": "static int fuse_dir_fsync(struct file *file, loff_t start, loff_t end,\n\t\t\t  int datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint err;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (fc->no_fsyncdir)\n\t\treturn 0;\n\n\tinode_lock(inode);\n\terr = fuse_fsync_common(file, start, end, datasync, FUSE_FSYNCDIR);\n\tif (err == -ENOSYS) {\n\t\tfc->no_fsyncdir = 1;\n\t\terr = 0;\n\t}\n\tinode_unlock(inode);\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tint err;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \tif (fc->no_fsyncdir)",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/create_new_entry",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int create_new_entry(struct fuse_mount *fm, struct fuse_args *args,\n\t\t\t    struct inode *dir, struct dentry *entry,\n\t\t\t    umode_t mode)\n{\n\tstruct fuse_entry_out outarg;\n\tstruct inode *inode;\n\tstruct dentry *d;\n\tint err;\n\tstruct fuse_forget_link *forget;\n\n\tforget = fuse_alloc_forget();\n\tif (!forget)\n\t\treturn -ENOMEM;\n\n\tmemset(&outarg, 0, sizeof(outarg));\n\targs->nodeid = get_node_id(dir);\n\targs->out_numargs = 1;\n\targs->out_args[0].size = sizeof(outarg);\n\targs->out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, args);\n\tif (err)\n\t\tgoto out_put_forget_req;\n\n\terr = -EIO;\n\tif (invalid_nodeid(outarg.nodeid) || fuse_invalid_attr(&outarg.attr))\n\t\tgoto out_put_forget_req;\n\n\tif ((outarg.attr.mode ^ mode) & S_IFMT)\n\t\tgoto out_put_forget_req;\n\n\tinode = fuse_iget(dir->i_sb, outarg.nodeid, outarg.generation,\n\t\t\t  &outarg.attr, entry_attr_timeout(&outarg), 0);\n\tif (!inode) {\n\t\tfuse_queue_forget(fm->fc, forget, outarg.nodeid, 1);\n\t\treturn -ENOMEM;\n\t}\n\tkfree(forget);\n\n\td_drop(entry);\n\td = d_splice_alias(inode, entry);\n\tif (IS_ERR(d))\n\t\treturn PTR_ERR(d);\n\n\tif (d) {\n\t\tfuse_change_entry_timeout(d, &outarg);\n\t\tdput(d);\n\t} else {\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\t}\n\tfuse_dir_changed(dir);\n\treturn 0;\n\n out_put_forget_req:\n\tkfree(forget);\n\treturn err;\n}",
        "func": "static int create_new_entry(struct fuse_mount *fm, struct fuse_args *args,\n\t\t\t    struct inode *dir, struct dentry *entry,\n\t\t\t    umode_t mode)\n{\n\tstruct fuse_entry_out outarg;\n\tstruct inode *inode;\n\tstruct dentry *d;\n\tint err;\n\tstruct fuse_forget_link *forget;\n\n\tif (fuse_is_bad(dir))\n\t\treturn -EIO;\n\n\tforget = fuse_alloc_forget();\n\tif (!forget)\n\t\treturn -ENOMEM;\n\n\tmemset(&outarg, 0, sizeof(outarg));\n\targs->nodeid = get_node_id(dir);\n\targs->out_numargs = 1;\n\targs->out_args[0].size = sizeof(outarg);\n\targs->out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, args);\n\tif (err)\n\t\tgoto out_put_forget_req;\n\n\terr = -EIO;\n\tif (invalid_nodeid(outarg.nodeid) || fuse_invalid_attr(&outarg.attr))\n\t\tgoto out_put_forget_req;\n\n\tif ((outarg.attr.mode ^ mode) & S_IFMT)\n\t\tgoto out_put_forget_req;\n\n\tinode = fuse_iget(dir->i_sb, outarg.nodeid, outarg.generation,\n\t\t\t  &outarg.attr, entry_attr_timeout(&outarg), 0);\n\tif (!inode) {\n\t\tfuse_queue_forget(fm->fc, forget, outarg.nodeid, 1);\n\t\treturn -ENOMEM;\n\t}\n\tkfree(forget);\n\n\td_drop(entry);\n\td = d_splice_alias(inode, entry);\n\tif (IS_ERR(d))\n\t\treturn PTR_ERR(d);\n\n\tif (d) {\n\t\tfuse_change_entry_timeout(d, &outarg);\n\t\tdput(d);\n\t} else {\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\t}\n\tfuse_dir_changed(dir);\n\treturn 0;\n\n out_put_forget_req:\n\tkfree(forget);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,9 @@\n \tstruct dentry *d;\n \tint err;\n \tstruct fuse_forget_link *forget;\n+\n+\tif (fuse_is_bad(dir))\n+\t\treturn -EIO;\n \n \tforget = fuse_alloc_forget();\n \tif (!forget)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(dir))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_getattr",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\tif (!fuse_allow_current_process(fc)) {\n\t\tif (!request_mask) {\n\t\t\t/*\n\t\t\t * If user explicitly requested *nothing* then don't\n\t\t\t * error out, but return st_dev only.\n\t\t\t */\n\t\t\tstat->result_mask = 0;\n\t\t\tstat->dev = inode->i_sb->s_dev;\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EACCES;\n\t}\n\n\treturn fuse_update_get_attr(inode, NULL, stat, request_mask, flags);\n}",
        "func": "static int fuse_getattr(const struct path *path, struct kstat *stat,\n\t\t\tu32 request_mask, unsigned int flags)\n{\n\tstruct inode *inode = d_inode(path->dentry);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!fuse_allow_current_process(fc)) {\n\t\tif (!request_mask) {\n\t\t\t/*\n\t\t\t * If user explicitly requested *nothing* then don't\n\t\t\t * error out, but return st_dev only.\n\t\t\t */\n\t\t\tstat->result_mask = 0;\n\t\t\tstat->dev = inode->i_sb->s_dev;\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EACCES;\n\t}\n\n\treturn fuse_update_get_attr(inode, NULL, stat, request_mask, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n {\n \tstruct inode *inode = d_inode(path->dentry);\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \tif (!fuse_allow_current_process(fc)) {\n \t\tif (!request_mask) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_lookup",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static struct dentry *fuse_lookup(struct inode *dir, struct dentry *entry,\n\t\t\t\t  unsigned int flags)\n{\n\tint err;\n\tstruct fuse_entry_out outarg;\n\tstruct inode *inode;\n\tstruct dentry *newent;\n\tbool outarg_valid = true;\n\tbool locked;\n\n\tlocked = fuse_lock_inode(dir);\n\terr = fuse_lookup_name(dir->i_sb, get_node_id(dir), &entry->d_name,\n\t\t\t       &outarg, &inode);\n\tfuse_unlock_inode(dir, locked);\n\tif (err == -ENOENT) {\n\t\toutarg_valid = false;\n\t\terr = 0;\n\t}\n\tif (err)\n\t\tgoto out_err;\n\n\terr = -EIO;\n\tif (inode && get_node_id(inode) == FUSE_ROOT_ID)\n\t\tgoto out_iput;\n\n\tnewent = d_splice_alias(inode, entry);\n\terr = PTR_ERR(newent);\n\tif (IS_ERR(newent))\n\t\tgoto out_err;\n\n\tentry = newent ? newent : entry;\n\tif (outarg_valid)\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\telse\n\t\tfuse_invalidate_entry_cache(entry);\n\n\tif (inode)\n\t\tfuse_advise_use_readdirplus(dir);\n\treturn newent;\n\n out_iput:\n\tiput(inode);\n out_err:\n\treturn ERR_PTR(err);\n}",
        "func": "static struct dentry *fuse_lookup(struct inode *dir, struct dentry *entry,\n\t\t\t\t  unsigned int flags)\n{\n\tint err;\n\tstruct fuse_entry_out outarg;\n\tstruct inode *inode;\n\tstruct dentry *newent;\n\tbool outarg_valid = true;\n\tbool locked;\n\n\tif (fuse_is_bad(dir))\n\t\treturn ERR_PTR(-EIO);\n\n\tlocked = fuse_lock_inode(dir);\n\terr = fuse_lookup_name(dir->i_sb, get_node_id(dir), &entry->d_name,\n\t\t\t       &outarg, &inode);\n\tfuse_unlock_inode(dir, locked);\n\tif (err == -ENOENT) {\n\t\toutarg_valid = false;\n\t\terr = 0;\n\t}\n\tif (err)\n\t\tgoto out_err;\n\n\terr = -EIO;\n\tif (inode && get_node_id(inode) == FUSE_ROOT_ID)\n\t\tgoto out_iput;\n\n\tnewent = d_splice_alias(inode, entry);\n\terr = PTR_ERR(newent);\n\tif (IS_ERR(newent))\n\t\tgoto out_err;\n\n\tentry = newent ? newent : entry;\n\tif (outarg_valid)\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\telse\n\t\tfuse_invalidate_entry_cache(entry);\n\n\tif (inode)\n\t\tfuse_advise_use_readdirplus(dir);\n\treturn newent;\n\n out_iput:\n\tiput(inode);\n out_err:\n\treturn ERR_PTR(err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,9 @@\n \tstruct dentry *newent;\n \tbool outarg_valid = true;\n \tbool locked;\n+\n+\tif (fuse_is_bad(dir))\n+\t\treturn ERR_PTR(-EIO);\n \n \tlocked = fuse_lock_inode(dir);\n \terr = fuse_lookup_name(dir->i_sb, get_node_id(dir), &entry->d_name,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(dir))",
                "\t\treturn ERR_PTR(-EIO);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_permission",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_permission(struct inode *inode, int mask)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tbool refreshed = false;\n\tint err = 0;\n\n\tif (!fuse_allow_current_process(fc))\n\t\treturn -EACCES;\n\n\t/*\n\t * If attributes are needed, refresh them before proceeding\n\t */\n\tif (fc->default_permissions ||\n\t    ((mask & MAY_EXEC) && S_ISREG(inode->i_mode))) {\n\t\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\t\tu32 perm_mask = STATX_MODE | STATX_UID | STATX_GID;\n\n\t\tif (perm_mask & READ_ONCE(fi->inval_mask) ||\n\t\t    time_before64(fi->i_time, get_jiffies_64())) {\n\t\t\trefreshed = true;\n\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (fc->default_permissions) {\n\t\terr = generic_permission(inode, mask);\n\n\t\t/* If permission is denied, try to refresh file\n\t\t   attributes.  This is also needed, because the root\n\t\t   node will at first have no permissions */\n\t\tif (err == -EACCES && !refreshed) {\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (!err)\n\t\t\t\terr = generic_permission(inode, mask);\n\t\t}\n\n\t\t/* Note: the opposite of the above test does not\n\t\t   exist.  So if permissions are revoked this won't be\n\t\t   noticed immediately, only after the attribute\n\t\t   timeout has expired */\n\t} else if (mask & (MAY_ACCESS | MAY_CHDIR)) {\n\t\terr = fuse_access(inode, mask);\n\t} else if ((mask & MAY_EXEC) && S_ISREG(inode->i_mode)) {\n\t\tif (!(inode->i_mode & S_IXUGO)) {\n\t\t\tif (refreshed)\n\t\t\t\treturn -EACCES;\n\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (!err && !(inode->i_mode & S_IXUGO))\n\t\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn err;\n}",
        "func": "static int fuse_permission(struct inode *inode, int mask)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tbool refreshed = false;\n\tint err = 0;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!fuse_allow_current_process(fc))\n\t\treturn -EACCES;\n\n\t/*\n\t * If attributes are needed, refresh them before proceeding\n\t */\n\tif (fc->default_permissions ||\n\t    ((mask & MAY_EXEC) && S_ISREG(inode->i_mode))) {\n\t\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\t\tu32 perm_mask = STATX_MODE | STATX_UID | STATX_GID;\n\n\t\tif (perm_mask & READ_ONCE(fi->inval_mask) ||\n\t\t    time_before64(fi->i_time, get_jiffies_64())) {\n\t\t\trefreshed = true;\n\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (fc->default_permissions) {\n\t\terr = generic_permission(inode, mask);\n\n\t\t/* If permission is denied, try to refresh file\n\t\t   attributes.  This is also needed, because the root\n\t\t   node will at first have no permissions */\n\t\tif (err == -EACCES && !refreshed) {\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (!err)\n\t\t\t\terr = generic_permission(inode, mask);\n\t\t}\n\n\t\t/* Note: the opposite of the above test does not\n\t\t   exist.  So if permissions are revoked this won't be\n\t\t   noticed immediately, only after the attribute\n\t\t   timeout has expired */\n\t} else if (mask & (MAY_ACCESS | MAY_CHDIR)) {\n\t\terr = fuse_access(inode, mask);\n\t} else if ((mask & MAY_EXEC) && S_ISREG(inode->i_mode)) {\n\t\tif (!(inode->i_mode & S_IXUGO)) {\n\t\t\tif (refreshed)\n\t\t\t\treturn -EACCES;\n\n\t\t\terr = fuse_perm_getattr(inode, mask);\n\t\t\tif (!err && !(inode->i_mode & S_IXUGO))\n\t\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tbool refreshed = false;\n \tint err = 0;\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \tif (!fuse_allow_current_process(fc))\n \t\treturn -EACCES;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_get_link",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static const char *fuse_get_link(struct dentry *dentry, struct inode *inode,\n\t\t\t\t struct delayed_call *callback)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct page *page;\n\tint err;\n\n\terr = -EIO;\n\tif (is_bad_inode(inode))\n\t\tgoto out_err;\n\n\tif (fc->cache_symlinks)\n\t\treturn page_get_link(dentry, inode, callback);\n\n\terr = -ECHILD;\n\tif (!dentry)\n\t\tgoto out_err;\n\n\tpage = alloc_page(GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!page)\n\t\tgoto out_err;\n\n\terr = fuse_readlink_page(inode, page);\n\tif (err) {\n\t\t__free_page(page);\n\t\tgoto out_err;\n\t}\n\n\tset_delayed_call(callback, page_put_link, page);\n\n\treturn page_address(page);\n\nout_err:\n\treturn ERR_PTR(err);\n}",
        "func": "static const char *fuse_get_link(struct dentry *dentry, struct inode *inode,\n\t\t\t\t struct delayed_call *callback)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct page *page;\n\tint err;\n\n\terr = -EIO;\n\tif (fuse_is_bad(inode))\n\t\tgoto out_err;\n\n\tif (fc->cache_symlinks)\n\t\treturn page_get_link(dentry, inode, callback);\n\n\terr = -ECHILD;\n\tif (!dentry)\n\t\tgoto out_err;\n\n\tpage = alloc_page(GFP_KERNEL);\n\terr = -ENOMEM;\n\tif (!page)\n\t\tgoto out_err;\n\n\terr = fuse_readlink_page(inode, page);\n\tif (err) {\n\t\t__free_page(page);\n\t\tgoto out_err;\n\t}\n\n\tset_delayed_call(callback, page_put_link, page);\n\n\treturn page_address(page);\n\nout_err:\n\treturn ERR_PTR(err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \tint err;\n \n \terr = -EIO;\n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\tgoto out_err;\n \n \tif (fc->cache_symlinks)",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_setattr",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_setattr(struct dentry *entry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(entry);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct file *file = (attr->ia_valid & ATTR_FILE) ? attr->ia_file : NULL;\n\tint ret;\n\n\tif (!fuse_allow_current_process(get_fuse_conn(inode)))\n\t\treturn -EACCES;\n\n\tif (attr->ia_valid & (ATTR_KILL_SUID | ATTR_KILL_SGID)) {\n\t\tattr->ia_valid &= ~(ATTR_KILL_SUID | ATTR_KILL_SGID |\n\t\t\t\t    ATTR_MODE);\n\n\t\t/*\n\t\t * The only sane way to reliably kill suid/sgid is to do it in\n\t\t * the userspace filesystem\n\t\t *\n\t\t * This should be done on write(), truncate() and chown().\n\t\t */\n\t\tif (!fc->handle_killpriv && !fc->handle_killpriv_v2) {\n\t\t\t/*\n\t\t\t * ia_mode calculation may have used stale i_mode.\n\t\t\t * Refresh and recalculate.\n\t\t\t */\n\t\t\tret = fuse_do_getattr(inode, NULL, file);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tattr->ia_mode = inode->i_mode;\n\t\t\tif (inode->i_mode & S_ISUID) {\n\t\t\t\tattr->ia_valid |= ATTR_MODE;\n\t\t\t\tattr->ia_mode &= ~S_ISUID;\n\t\t\t}\n\t\t\tif ((inode->i_mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\t\tattr->ia_valid |= ATTR_MODE;\n\t\t\t\tattr->ia_mode &= ~S_ISGID;\n\t\t\t}\n\t\t}\n\t}\n\tif (!attr->ia_valid)\n\t\treturn 0;\n\n\tret = fuse_do_setattr(entry, attr, file);\n\tif (!ret) {\n\t\t/*\n\t\t * If filesystem supports acls it may have updated acl xattrs in\n\t\t * the filesystem, so forget cached acls for the inode.\n\t\t */\n\t\tif (fc->posix_acl)\n\t\t\tforget_all_cached_acls(inode);\n\n\t\t/* Directory mode changed, may need to revalidate access */\n\t\tif (d_is_dir(entry) && (attr->ia_valid & ATTR_MODE))\n\t\t\tfuse_invalidate_entry_cache(entry);\n\t}\n\treturn ret;\n}",
        "func": "static int fuse_setattr(struct dentry *entry, struct iattr *attr)\n{\n\tstruct inode *inode = d_inode(entry);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct file *file = (attr->ia_valid & ATTR_FILE) ? attr->ia_file : NULL;\n\tint ret;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!fuse_allow_current_process(get_fuse_conn(inode)))\n\t\treturn -EACCES;\n\n\tif (attr->ia_valid & (ATTR_KILL_SUID | ATTR_KILL_SGID)) {\n\t\tattr->ia_valid &= ~(ATTR_KILL_SUID | ATTR_KILL_SGID |\n\t\t\t\t    ATTR_MODE);\n\n\t\t/*\n\t\t * The only sane way to reliably kill suid/sgid is to do it in\n\t\t * the userspace filesystem\n\t\t *\n\t\t * This should be done on write(), truncate() and chown().\n\t\t */\n\t\tif (!fc->handle_killpriv && !fc->handle_killpriv_v2) {\n\t\t\t/*\n\t\t\t * ia_mode calculation may have used stale i_mode.\n\t\t\t * Refresh and recalculate.\n\t\t\t */\n\t\t\tret = fuse_do_getattr(inode, NULL, file);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tattr->ia_mode = inode->i_mode;\n\t\t\tif (inode->i_mode & S_ISUID) {\n\t\t\t\tattr->ia_valid |= ATTR_MODE;\n\t\t\t\tattr->ia_mode &= ~S_ISUID;\n\t\t\t}\n\t\t\tif ((inode->i_mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {\n\t\t\t\tattr->ia_valid |= ATTR_MODE;\n\t\t\t\tattr->ia_mode &= ~S_ISGID;\n\t\t\t}\n\t\t}\n\t}\n\tif (!attr->ia_valid)\n\t\treturn 0;\n\n\tret = fuse_do_setattr(entry, attr, file);\n\tif (!ret) {\n\t\t/*\n\t\t * If filesystem supports acls it may have updated acl xattrs in\n\t\t * the filesystem, so forget cached acls for the inode.\n\t\t */\n\t\tif (fc->posix_acl)\n\t\t\tforget_all_cached_acls(inode);\n\n\t\t/* Directory mode changed, may need to revalidate access */\n\t\tif (d_is_dir(entry) && (attr->ia_valid & ATTR_MODE))\n\t\t\tfuse_invalidate_entry_cache(entry);\n\t}\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,9 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tstruct file *file = (attr->ia_valid & ATTR_FILE) ? attr->ia_file : NULL;\n \tint ret;\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \tif (!fuse_allow_current_process(get_fuse_conn(inode)))\n \t\treturn -EACCES;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_dentry_revalidate",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_dentry_revalidate(struct dentry *entry, unsigned int flags)\n{\n\tstruct inode *inode;\n\tstruct dentry *parent;\n\tstruct fuse_mount *fm;\n\tstruct fuse_inode *fi;\n\tint ret;\n\n\tinode = d_inode_rcu(entry);\n\tif (inode && is_bad_inode(inode))\n\t\tgoto invalid;\n\telse if (time_before64(fuse_dentry_time(entry), get_jiffies_64()) ||\n\t\t (flags & (LOOKUP_EXCL | LOOKUP_REVAL))) {\n\t\tstruct fuse_entry_out outarg;\n\t\tFUSE_ARGS(args);\n\t\tstruct fuse_forget_link *forget;\n\t\tu64 attr_version;\n\n\t\t/* For negative dentries, always do a fresh lookup */\n\t\tif (!inode)\n\t\t\tgoto invalid;\n\n\t\tret = -ECHILD;\n\t\tif (flags & LOOKUP_RCU)\n\t\t\tgoto out;\n\n\t\tfm = get_fuse_mount(inode);\n\n\t\tforget = fuse_alloc_forget();\n\t\tret = -ENOMEM;\n\t\tif (!forget)\n\t\t\tgoto out;\n\n\t\tattr_version = fuse_get_attr_version(fm->fc);\n\n\t\tparent = dget_parent(entry);\n\t\tfuse_lookup_init(fm->fc, &args, get_node_id(d_inode(parent)),\n\t\t\t\t &entry->d_name, &outarg);\n\t\tret = fuse_simple_request(fm, &args);\n\t\tdput(parent);\n\t\t/* Zero nodeid is same as -ENOENT */\n\t\tif (!ret && !outarg.nodeid)\n\t\t\tret = -ENOENT;\n\t\tif (!ret) {\n\t\t\tfi = get_fuse_inode(inode);\n\t\t\tif (outarg.nodeid != get_node_id(inode) ||\n\t\t\t    (bool) IS_AUTOMOUNT(inode) != (bool) (outarg.attr.flags & FUSE_ATTR_SUBMOUNT)) {\n\t\t\t\tfuse_queue_forget(fm->fc, forget,\n\t\t\t\t\t\t  outarg.nodeid, 1);\n\t\t\t\tgoto invalid;\n\t\t\t}\n\t\t\tspin_lock(&fi->lock);\n\t\t\tfi->nlookup++;\n\t\t\tspin_unlock(&fi->lock);\n\t\t}\n\t\tkfree(forget);\n\t\tif (ret == -ENOMEM)\n\t\t\tgoto out;\n\t\tif (ret || fuse_invalid_attr(&outarg.attr) ||\n\t\t    (outarg.attr.mode ^ inode->i_mode) & S_IFMT)\n\t\t\tgoto invalid;\n\n\t\tforget_all_cached_acls(inode);\n\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t       entry_attr_timeout(&outarg),\n\t\t\t\t       attr_version);\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\t} else if (inode) {\n\t\tfi = get_fuse_inode(inode);\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tif (test_bit(FUSE_I_INIT_RDPLUS, &fi->state))\n\t\t\t\treturn -ECHILD;\n\t\t} else if (test_and_clear_bit(FUSE_I_INIT_RDPLUS, &fi->state)) {\n\t\t\tparent = dget_parent(entry);\n\t\t\tfuse_advise_use_readdirplus(d_inode(parent));\n\t\t\tdput(parent);\n\t\t}\n\t}\n\tret = 1;\nout:\n\treturn ret;\n\ninvalid:\n\tret = 0;\n\tgoto out;\n}",
        "func": "static int fuse_dentry_revalidate(struct dentry *entry, unsigned int flags)\n{\n\tstruct inode *inode;\n\tstruct dentry *parent;\n\tstruct fuse_mount *fm;\n\tstruct fuse_inode *fi;\n\tint ret;\n\n\tinode = d_inode_rcu(entry);\n\tif (inode && fuse_is_bad(inode))\n\t\tgoto invalid;\n\telse if (time_before64(fuse_dentry_time(entry), get_jiffies_64()) ||\n\t\t (flags & (LOOKUP_EXCL | LOOKUP_REVAL))) {\n\t\tstruct fuse_entry_out outarg;\n\t\tFUSE_ARGS(args);\n\t\tstruct fuse_forget_link *forget;\n\t\tu64 attr_version;\n\n\t\t/* For negative dentries, always do a fresh lookup */\n\t\tif (!inode)\n\t\t\tgoto invalid;\n\n\t\tret = -ECHILD;\n\t\tif (flags & LOOKUP_RCU)\n\t\t\tgoto out;\n\n\t\tfm = get_fuse_mount(inode);\n\n\t\tforget = fuse_alloc_forget();\n\t\tret = -ENOMEM;\n\t\tif (!forget)\n\t\t\tgoto out;\n\n\t\tattr_version = fuse_get_attr_version(fm->fc);\n\n\t\tparent = dget_parent(entry);\n\t\tfuse_lookup_init(fm->fc, &args, get_node_id(d_inode(parent)),\n\t\t\t\t &entry->d_name, &outarg);\n\t\tret = fuse_simple_request(fm, &args);\n\t\tdput(parent);\n\t\t/* Zero nodeid is same as -ENOENT */\n\t\tif (!ret && !outarg.nodeid)\n\t\t\tret = -ENOENT;\n\t\tif (!ret) {\n\t\t\tfi = get_fuse_inode(inode);\n\t\t\tif (outarg.nodeid != get_node_id(inode) ||\n\t\t\t    (bool) IS_AUTOMOUNT(inode) != (bool) (outarg.attr.flags & FUSE_ATTR_SUBMOUNT)) {\n\t\t\t\tfuse_queue_forget(fm->fc, forget,\n\t\t\t\t\t\t  outarg.nodeid, 1);\n\t\t\t\tgoto invalid;\n\t\t\t}\n\t\t\tspin_lock(&fi->lock);\n\t\t\tfi->nlookup++;\n\t\t\tspin_unlock(&fi->lock);\n\t\t}\n\t\tkfree(forget);\n\t\tif (ret == -ENOMEM)\n\t\t\tgoto out;\n\t\tif (ret || fuse_invalid_attr(&outarg.attr) ||\n\t\t    (outarg.attr.mode ^ inode->i_mode) & S_IFMT)\n\t\t\tgoto invalid;\n\n\t\tforget_all_cached_acls(inode);\n\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t       entry_attr_timeout(&outarg),\n\t\t\t\t       attr_version);\n\t\tfuse_change_entry_timeout(entry, &outarg);\n\t} else if (inode) {\n\t\tfi = get_fuse_inode(inode);\n\t\tif (flags & LOOKUP_RCU) {\n\t\t\tif (test_bit(FUSE_I_INIT_RDPLUS, &fi->state))\n\t\t\t\treturn -ECHILD;\n\t\t} else if (test_and_clear_bit(FUSE_I_INIT_RDPLUS, &fi->state)) {\n\t\t\tparent = dget_parent(entry);\n\t\t\tfuse_advise_use_readdirplus(d_inode(parent));\n\t\t\tdput(parent);\n\t\t}\n\t}\n\tret = 1;\nout:\n\treturn ret;\n\ninvalid:\n\tret = 0;\n\tgoto out;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \tint ret;\n \n \tinode = d_inode_rcu(entry);\n-\tif (inode && is_bad_inode(inode))\n+\tif (inode && fuse_is_bad(inode))\n \t\tgoto invalid;\n \telse if (time_before64(fuse_dentry_time(entry), get_jiffies_64()) ||\n \t\t (flags & (LOOKUP_EXCL | LOOKUP_REVAL))) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (inode && is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (inode && fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_atomic_open",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_atomic_open(struct inode *dir, struct dentry *entry,\n\t\t\t    struct file *file, unsigned flags,\n\t\t\t    umode_t mode)\n{\n\tint err;\n\tstruct fuse_conn *fc = get_fuse_conn(dir);\n\tstruct dentry *res = NULL;\n\n\tif (d_in_lookup(entry)) {\n\t\tres = fuse_lookup(dir, entry, 0);\n\t\tif (IS_ERR(res))\n\t\t\treturn PTR_ERR(res);\n\n\t\tif (res)\n\t\t\tentry = res;\n\t}\n\n\tif (!(flags & O_CREAT) || d_really_is_positive(entry))\n\t\tgoto no_open;\n\n\t/* Only creates */\n\tfile->f_mode |= FMODE_CREATED;\n\n\tif (fc->no_create)\n\t\tgoto mknod;\n\n\terr = fuse_create_open(dir, entry, file, flags, mode);\n\tif (err == -ENOSYS) {\n\t\tfc->no_create = 1;\n\t\tgoto mknod;\n\t}\nout_dput:\n\tdput(res);\n\treturn err;\n\nmknod:\n\terr = fuse_mknod(dir, entry, mode, 0);\n\tif (err)\n\t\tgoto out_dput;\nno_open:\n\treturn finish_no_open(file, res);\n}",
        "func": "static int fuse_atomic_open(struct inode *dir, struct dentry *entry,\n\t\t\t    struct file *file, unsigned flags,\n\t\t\t    umode_t mode)\n{\n\tint err;\n\tstruct fuse_conn *fc = get_fuse_conn(dir);\n\tstruct dentry *res = NULL;\n\n\tif (fuse_is_bad(dir))\n\t\treturn -EIO;\n\n\tif (d_in_lookup(entry)) {\n\t\tres = fuse_lookup(dir, entry, 0);\n\t\tif (IS_ERR(res))\n\t\t\treturn PTR_ERR(res);\n\n\t\tif (res)\n\t\t\tentry = res;\n\t}\n\n\tif (!(flags & O_CREAT) || d_really_is_positive(entry))\n\t\tgoto no_open;\n\n\t/* Only creates */\n\tfile->f_mode |= FMODE_CREATED;\n\n\tif (fc->no_create)\n\t\tgoto mknod;\n\n\terr = fuse_create_open(dir, entry, file, flags, mode);\n\tif (err == -ENOSYS) {\n\t\tfc->no_create = 1;\n\t\tgoto mknod;\n\t}\nout_dput:\n\tdput(res);\n\treturn err;\n\nmknod:\n\terr = fuse_mknod(dir, entry, mode, 0);\n\tif (err)\n\t\tgoto out_dput;\nno_open:\n\treturn finish_no_open(file, res);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,9 @@\n \tint err;\n \tstruct fuse_conn *fc = get_fuse_conn(dir);\n \tstruct dentry *res = NULL;\n+\n+\tif (fuse_is_bad(dir))\n+\t\treturn -EIO;\n \n \tif (d_in_lookup(entry)) {\n \t\tres = fuse_lookup(dir, entry, 0);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(dir))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_evict_inode",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static void fuse_evict_inode(struct inode *inode)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\ttruncate_inode_pages_final(&inode->i_data);\n\tclear_inode(inode);\n\tif (inode->i_sb->s_flags & SB_ACTIVE) {\n\t\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\t\tif (FUSE_IS_DAX(inode))\n\t\t\tfuse_dax_inode_cleanup(inode);\n\t\tif (fi->nlookup) {\n\t\t\tfuse_queue_forget(fc, fi->forget, fi->nodeid,\n\t\t\t\t\t  fi->nlookup);\n\t\t\tfi->forget = NULL;\n\t\t}\n\t}\n\tif (S_ISREG(inode->i_mode) && !is_bad_inode(inode)) {\n\t\tWARN_ON(!list_empty(&fi->write_files));\n\t\tWARN_ON(!list_empty(&fi->queued_writes));\n\t}\n}",
        "func": "static void fuse_evict_inode(struct inode *inode)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\ttruncate_inode_pages_final(&inode->i_data);\n\tclear_inode(inode);\n\tif (inode->i_sb->s_flags & SB_ACTIVE) {\n\t\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\t\tif (FUSE_IS_DAX(inode))\n\t\t\tfuse_dax_inode_cleanup(inode);\n\t\tif (fi->nlookup) {\n\t\t\tfuse_queue_forget(fc, fi->forget, fi->nodeid,\n\t\t\t\t\t  fi->nlookup);\n\t\t\tfi->forget = NULL;\n\t\t}\n\t}\n\tif (S_ISREG(inode->i_mode) && !fuse_is_bad(inode)) {\n\t\tWARN_ON(!list_empty(&fi->write_files));\n\t\tWARN_ON(!list_empty(&fi->queued_writes));\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \t\t\tfi->forget = NULL;\n \t\t}\n \t}\n-\tif (S_ISREG(inode->i_mode) && !is_bad_inode(inode)) {\n+\tif (S_ISREG(inode->i_mode) && !fuse_is_bad(inode)) {\n \t\tWARN_ON(!list_empty(&fi->write_files));\n \t\tWARN_ON(!list_empty(&fi->queued_writes));\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (S_ISREG(inode->i_mode) && !is_bad_inode(inode)) {"
            ],
            "added_lines": [
                "\tif (S_ISREG(inode->i_mode) && !fuse_is_bad(inode)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_iget",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "struct inode *fuse_iget(struct super_block *sb, u64 nodeid,\n\t\t\tint generation, struct fuse_attr *attr,\n\t\t\tu64 attr_valid, u64 attr_version)\n{\n\tstruct inode *inode;\n\tstruct fuse_inode *fi;\n\tstruct fuse_conn *fc = get_fuse_conn_super(sb);\n\n\t/*\n\t * Auto mount points get their node id from the submount root, which is\n\t * not a unique identifier within this filesystem.\n\t *\n\t * To avoid conflicts, do not place submount points into the inode hash\n\t * table.\n\t */\n\tif (fc->auto_submounts && (attr->flags & FUSE_ATTR_SUBMOUNT) &&\n\t    S_ISDIR(attr->mode)) {\n\t\tinode = new_inode(sb);\n\t\tif (!inode)\n\t\t\treturn NULL;\n\n\t\tfuse_init_inode(inode, attr);\n\t\tget_fuse_inode(inode)->nodeid = nodeid;\n\t\tinode->i_flags |= S_AUTOMOUNT;\n\t\tgoto done;\n\t}\n\nretry:\n\tinode = iget5_locked(sb, nodeid, fuse_inode_eq, fuse_inode_set, &nodeid);\n\tif (!inode)\n\t\treturn NULL;\n\n\tif ((inode->i_state & I_NEW)) {\n\t\tinode->i_flags |= S_NOATIME;\n\t\tif (!fc->writeback_cache || !S_ISREG(attr->mode))\n\t\t\tinode->i_flags |= S_NOCMTIME;\n\t\tinode->i_generation = generation;\n\t\tfuse_init_inode(inode, attr);\n\t\tunlock_new_inode(inode);\n\t} else if ((inode->i_mode ^ attr->mode) & S_IFMT) {\n\t\t/* Inode has changed type, any I/O on the old should fail */\n\t\tmake_bad_inode(inode);\n\t\tiput(inode);\n\t\tgoto retry;\n\t}\ndone:\n\tfi = get_fuse_inode(inode);\n\tspin_lock(&fi->lock);\n\tfi->nlookup++;\n\tspin_unlock(&fi->lock);\n\tfuse_change_attributes(inode, attr, attr_valid, attr_version);\n\n\treturn inode;\n}",
        "func": "struct inode *fuse_iget(struct super_block *sb, u64 nodeid,\n\t\t\tint generation, struct fuse_attr *attr,\n\t\t\tu64 attr_valid, u64 attr_version)\n{\n\tstruct inode *inode;\n\tstruct fuse_inode *fi;\n\tstruct fuse_conn *fc = get_fuse_conn_super(sb);\n\n\t/*\n\t * Auto mount points get their node id from the submount root, which is\n\t * not a unique identifier within this filesystem.\n\t *\n\t * To avoid conflicts, do not place submount points into the inode hash\n\t * table.\n\t */\n\tif (fc->auto_submounts && (attr->flags & FUSE_ATTR_SUBMOUNT) &&\n\t    S_ISDIR(attr->mode)) {\n\t\tinode = new_inode(sb);\n\t\tif (!inode)\n\t\t\treturn NULL;\n\n\t\tfuse_init_inode(inode, attr);\n\t\tget_fuse_inode(inode)->nodeid = nodeid;\n\t\tinode->i_flags |= S_AUTOMOUNT;\n\t\tgoto done;\n\t}\n\nretry:\n\tinode = iget5_locked(sb, nodeid, fuse_inode_eq, fuse_inode_set, &nodeid);\n\tif (!inode)\n\t\treturn NULL;\n\n\tif ((inode->i_state & I_NEW)) {\n\t\tinode->i_flags |= S_NOATIME;\n\t\tif (!fc->writeback_cache || !S_ISREG(attr->mode))\n\t\t\tinode->i_flags |= S_NOCMTIME;\n\t\tinode->i_generation = generation;\n\t\tfuse_init_inode(inode, attr);\n\t\tunlock_new_inode(inode);\n\t} else if ((inode->i_mode ^ attr->mode) & S_IFMT) {\n\t\t/* Inode has changed type, any I/O on the old should fail */\n\t\tfuse_make_bad(inode);\n\t\tiput(inode);\n\t\tgoto retry;\n\t}\ndone:\n\tfi = get_fuse_inode(inode);\n\tspin_lock(&fi->lock);\n\tfi->nlookup++;\n\tspin_unlock(&fi->lock);\n\tfuse_change_attributes(inode, attr, attr_valid, attr_version);\n\n\treturn inode;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,7 +39,7 @@\n \t\tunlock_new_inode(inode);\n \t} else if ((inode->i_mode ^ attr->mode) & S_IFMT) {\n \t\t/* Inode has changed type, any I/O on the old should fail */\n-\t\tmake_bad_inode(inode);\n+\t\tfuse_make_bad(inode);\n \t\tiput(inode);\n \t\tgoto retry;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tmake_bad_inode(inode);"
            ],
            "added_lines": [
                "\t\tfuse_make_bad(inode);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_set_acl",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "int fuse_set_acl(struct inode *inode, struct posix_acl *acl, int type)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tconst char *name;\n\tint ret;\n\n\tif (!fc->posix_acl || fc->no_setxattr)\n\t\treturn -EOPNOTSUPP;\n\n\tif (type == ACL_TYPE_ACCESS)\n\t\tname = XATTR_NAME_POSIX_ACL_ACCESS;\n\telse if (type == ACL_TYPE_DEFAULT)\n\t\tname = XATTR_NAME_POSIX_ACL_DEFAULT;\n\telse\n\t\treturn -EINVAL;\n\n\tif (acl) {\n\t\t/*\n\t\t * Fuse userspace is responsible for updating access\n\t\t * permissions in the inode, if needed. fuse_setxattr\n\t\t * invalidates the inode attributes, which will force\n\t\t * them to be refreshed the next time they are used,\n\t\t * and it also updates i_ctime.\n\t\t */\n\t\tsize_t size = posix_acl_xattr_size(acl->a_count);\n\t\tvoid *value;\n\n\t\tif (size > PAGE_SIZE)\n\t\t\treturn -E2BIG;\n\n\t\tvalue = kmalloc(size, GFP_KERNEL);\n\t\tif (!value)\n\t\t\treturn -ENOMEM;\n\n\t\tret = posix_acl_to_xattr(fc->user_ns, acl, value, size);\n\t\tif (ret < 0) {\n\t\t\tkfree(value);\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = fuse_setxattr(inode, name, value, size, 0);\n\t\tkfree(value);\n\t} else {\n\t\tret = fuse_removexattr(inode, name);\n\t}\n\tforget_all_cached_acls(inode);\n\tfuse_invalidate_attr(inode);\n\n\treturn ret;\n}",
        "func": "int fuse_set_acl(struct inode *inode, struct posix_acl *acl, int type)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tconst char *name;\n\tint ret;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (!fc->posix_acl || fc->no_setxattr)\n\t\treturn -EOPNOTSUPP;\n\n\tif (type == ACL_TYPE_ACCESS)\n\t\tname = XATTR_NAME_POSIX_ACL_ACCESS;\n\telse if (type == ACL_TYPE_DEFAULT)\n\t\tname = XATTR_NAME_POSIX_ACL_DEFAULT;\n\telse\n\t\treturn -EINVAL;\n\n\tif (acl) {\n\t\t/*\n\t\t * Fuse userspace is responsible for updating access\n\t\t * permissions in the inode, if needed. fuse_setxattr\n\t\t * invalidates the inode attributes, which will force\n\t\t * them to be refreshed the next time they are used,\n\t\t * and it also updates i_ctime.\n\t\t */\n\t\tsize_t size = posix_acl_xattr_size(acl->a_count);\n\t\tvoid *value;\n\n\t\tif (size > PAGE_SIZE)\n\t\t\treturn -E2BIG;\n\n\t\tvalue = kmalloc(size, GFP_KERNEL);\n\t\tif (!value)\n\t\t\treturn -ENOMEM;\n\n\t\tret = posix_acl_to_xattr(fc->user_ns, acl, value, size);\n\t\tif (ret < 0) {\n\t\t\tkfree(value);\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = fuse_setxattr(inode, name, value, size, 0);\n\t\tkfree(value);\n\t} else {\n\t\tret = fuse_removexattr(inode, name);\n\t}\n\tforget_all_cached_acls(inode);\n\tfuse_invalidate_attr(inode);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,9 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tconst char *name;\n \tint ret;\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \tif (!fc->posix_acl || fc->no_setxattr)\n \t\treturn -EOPNOTSUPP;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_get_acl",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "struct posix_acl *fuse_get_acl(struct inode *inode, int type)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint size;\n\tconst char *name;\n\tvoid *value = NULL;\n\tstruct posix_acl *acl;\n\n\tif (!fc->posix_acl || fc->no_getxattr)\n\t\treturn NULL;\n\n\tif (type == ACL_TYPE_ACCESS)\n\t\tname = XATTR_NAME_POSIX_ACL_ACCESS;\n\telse if (type == ACL_TYPE_DEFAULT)\n\t\tname = XATTR_NAME_POSIX_ACL_DEFAULT;\n\telse\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tvalue = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!value)\n\t\treturn ERR_PTR(-ENOMEM);\n\tsize = fuse_getxattr(inode, name, value, PAGE_SIZE);\n\tif (size > 0)\n\t\tacl = posix_acl_from_xattr(fc->user_ns, value, size);\n\telse if ((size == 0) || (size == -ENODATA) ||\n\t\t (size == -EOPNOTSUPP && fc->no_getxattr))\n\t\tacl = NULL;\n\telse if (size == -ERANGE)\n\t\tacl = ERR_PTR(-E2BIG);\n\telse\n\t\tacl = ERR_PTR(size);\n\n\tkfree(value);\n\treturn acl;\n}",
        "func": "struct posix_acl *fuse_get_acl(struct inode *inode, int type)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint size;\n\tconst char *name;\n\tvoid *value = NULL;\n\tstruct posix_acl *acl;\n\n\tif (fuse_is_bad(inode))\n\t\treturn ERR_PTR(-EIO);\n\n\tif (!fc->posix_acl || fc->no_getxattr)\n\t\treturn NULL;\n\n\tif (type == ACL_TYPE_ACCESS)\n\t\tname = XATTR_NAME_POSIX_ACL_ACCESS;\n\telse if (type == ACL_TYPE_DEFAULT)\n\t\tname = XATTR_NAME_POSIX_ACL_DEFAULT;\n\telse\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tvalue = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!value)\n\t\treturn ERR_PTR(-ENOMEM);\n\tsize = fuse_getxattr(inode, name, value, PAGE_SIZE);\n\tif (size > 0)\n\t\tacl = posix_acl_from_xattr(fc->user_ns, value, size);\n\telse if ((size == 0) || (size == -ENODATA) ||\n\t\t (size == -EOPNOTSUPP && fc->no_getxattr))\n\t\tacl = NULL;\n\telse if (size == -ERANGE)\n\t\tacl = ERR_PTR(-E2BIG);\n\telse\n\t\tacl = ERR_PTR(size);\n\n\tkfree(value);\n\treturn acl;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,9 @@\n \tconst char *name;\n \tvoid *value = NULL;\n \tstruct posix_acl *acl;\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn ERR_PTR(-EIO);\n \n \tif (!fc->posix_acl || fc->no_getxattr)\n \t\treturn NULL;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn ERR_PTR(-EIO);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_fsync",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_fsync(struct file *file, loff_t start, loff_t end,\n\t\t      int datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint err;\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\tinode_lock(inode);\n\n\t/*\n\t * Start writeback against all dirty pages of the inode, then\n\t * wait for all outstanding writes, before sending the FSYNC\n\t * request.\n\t */\n\terr = file_write_and_wait_range(file, start, end);\n\tif (err)\n\t\tgoto out;\n\n\tfuse_sync_writes(inode);\n\n\t/*\n\t * Due to implementation of fuse writeback\n\t * file_write_and_wait_range() does not catch errors.\n\t * We have to do this directly after fuse_sync_writes()\n\t */\n\terr = file_check_and_advance_wb_err(file);\n\tif (err)\n\t\tgoto out;\n\n\terr = sync_inode_metadata(inode, 1);\n\tif (err)\n\t\tgoto out;\n\n\tif (fc->no_fsync)\n\t\tgoto out;\n\n\terr = fuse_fsync_common(file, start, end, datasync, FUSE_FSYNC);\n\tif (err == -ENOSYS) {\n\t\tfc->no_fsync = 1;\n\t\terr = 0;\n\t}\nout:\n\tinode_unlock(inode);\n\n\treturn err;\n}",
        "func": "static int fuse_fsync(struct file *file, loff_t start, loff_t end,\n\t\t      int datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tint err;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tinode_lock(inode);\n\n\t/*\n\t * Start writeback against all dirty pages of the inode, then\n\t * wait for all outstanding writes, before sending the FSYNC\n\t * request.\n\t */\n\terr = file_write_and_wait_range(file, start, end);\n\tif (err)\n\t\tgoto out;\n\n\tfuse_sync_writes(inode);\n\n\t/*\n\t * Due to implementation of fuse writeback\n\t * file_write_and_wait_range() does not catch errors.\n\t * We have to do this directly after fuse_sync_writes()\n\t */\n\terr = file_check_and_advance_wb_err(file);\n\tif (err)\n\t\tgoto out;\n\n\terr = sync_inode_metadata(inode, 1);\n\tif (err)\n\t\tgoto out;\n\n\tif (fc->no_fsync)\n\t\tgoto out;\n\n\terr = fuse_fsync_common(file, start, end, datasync, FUSE_FSYNC);\n\tif (err == -ENOSYS) {\n\t\tfc->no_fsync = 1;\n\t\terr = 0;\n\t}\nout:\n\tinode_unlock(inode);\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tint err;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \tinode_lock(inode);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_flush",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_flush(struct file *file, fl_owner_t id)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_file *ff = file->private_data;\n\tstruct fuse_flush_in inarg;\n\tFUSE_ARGS(args);\n\tint err;\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\terr = write_inode_now(inode, 1);\n\tif (err)\n\t\treturn err;\n\n\tinode_lock(inode);\n\tfuse_sync_writes(inode);\n\tinode_unlock(inode);\n\n\terr = filemap_check_errors(file->f_mapping);\n\tif (err)\n\t\treturn err;\n\n\terr = 0;\n\tif (fm->fc->no_flush)\n\t\tgoto inval_attr_out;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.fh = ff->fh;\n\tinarg.lock_owner = fuse_lock_owner_id(fm->fc, id);\n\targs.opcode = FUSE_FLUSH;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\targs.force = true;\n\n\terr = fuse_simple_request(fm, &args);\n\tif (err == -ENOSYS) {\n\t\tfm->fc->no_flush = 1;\n\t\terr = 0;\n\t}\n\ninval_attr_out:\n\t/*\n\t * In memory i_blocks is not maintained by fuse, if writeback cache is\n\t * enabled, i_blocks from cached attr may not be accurate.\n\t */\n\tif (!err && fm->fc->writeback_cache)\n\t\tfuse_invalidate_attr(inode);\n\treturn err;\n}",
        "func": "static int fuse_flush(struct file *file, fl_owner_t id)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_file *ff = file->private_data;\n\tstruct fuse_flush_in inarg;\n\tFUSE_ARGS(args);\n\tint err;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\terr = write_inode_now(inode, 1);\n\tif (err)\n\t\treturn err;\n\n\tinode_lock(inode);\n\tfuse_sync_writes(inode);\n\tinode_unlock(inode);\n\n\terr = filemap_check_errors(file->f_mapping);\n\tif (err)\n\t\treturn err;\n\n\terr = 0;\n\tif (fm->fc->no_flush)\n\t\tgoto inval_attr_out;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.fh = ff->fh;\n\tinarg.lock_owner = fuse_lock_owner_id(fm->fc, id);\n\targs.opcode = FUSE_FLUSH;\n\targs.nodeid = get_node_id(inode);\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\targs.force = true;\n\n\terr = fuse_simple_request(fm, &args);\n\tif (err == -ENOSYS) {\n\t\tfm->fc->no_flush = 1;\n\t\terr = 0;\n\t}\n\ninval_attr_out:\n\t/*\n\t * In memory i_blocks is not maintained by fuse, if writeback cache is\n\t * enabled, i_blocks from cached attr may not be accurate.\n\t */\n\tif (!err && fm->fc->writeback_cache)\n\t\tfuse_invalidate_attr(inode);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \tFUSE_ARGS(args);\n \tint err;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \terr = write_inode_now(inode, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_readahead",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static void fuse_readahead(struct readahead_control *rac)\n{\n\tstruct inode *inode = rac->mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tunsigned int i, max_pages, nr_pages = 0;\n\n\tif (is_bad_inode(inode))\n\t\treturn;\n\n\tmax_pages = min_t(unsigned int, fc->max_pages,\n\t\t\tfc->max_read / PAGE_SIZE);\n\n\tfor (;;) {\n\t\tstruct fuse_io_args *ia;\n\t\tstruct fuse_args_pages *ap;\n\n\t\tnr_pages = readahead_count(rac) - nr_pages;\n\t\tif (nr_pages > max_pages)\n\t\t\tnr_pages = max_pages;\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tia = fuse_io_alloc(NULL, nr_pages);\n\t\tif (!ia)\n\t\t\treturn;\n\t\tap = &ia->ap;\n\t\tnr_pages = __readahead_batch(rac, ap->pages, nr_pages);\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tfuse_wait_on_page_writeback(inode,\n\t\t\t\t\t\t    readahead_index(rac) + i);\n\t\t\tap->descs[i].length = PAGE_SIZE;\n\t\t}\n\t\tap->num_pages = nr_pages;\n\t\tfuse_send_readpages(ia, rac->file);\n\t}\n}",
        "func": "static void fuse_readahead(struct readahead_control *rac)\n{\n\tstruct inode *inode = rac->mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tunsigned int i, max_pages, nr_pages = 0;\n\n\tif (fuse_is_bad(inode))\n\t\treturn;\n\n\tmax_pages = min_t(unsigned int, fc->max_pages,\n\t\t\tfc->max_read / PAGE_SIZE);\n\n\tfor (;;) {\n\t\tstruct fuse_io_args *ia;\n\t\tstruct fuse_args_pages *ap;\n\n\t\tnr_pages = readahead_count(rac) - nr_pages;\n\t\tif (nr_pages > max_pages)\n\t\t\tnr_pages = max_pages;\n\t\tif (nr_pages == 0)\n\t\t\tbreak;\n\t\tia = fuse_io_alloc(NULL, nr_pages);\n\t\tif (!ia)\n\t\t\treturn;\n\t\tap = &ia->ap;\n\t\tnr_pages = __readahead_batch(rac, ap->pages, nr_pages);\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tfuse_wait_on_page_writeback(inode,\n\t\t\t\t\t\t    readahead_index(rac) + i);\n\t\t\tap->descs[i].length = PAGE_SIZE;\n\t\t}\n\t\tap->num_pages = nr_pages;\n\t\tfuse_send_readpages(ia, rac->file);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n \tunsigned int i, max_pages, nr_pages = 0;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn;\n \n \tmax_pages = min_t(unsigned int, fc->max_pages,",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_writepages",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_writepages(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_fill_wb_data data;\n\tint err;\n\n\terr = -EIO;\n\tif (is_bad_inode(inode))\n\t\tgoto out;\n\n\tdata.inode = inode;\n\tdata.wpa = NULL;\n\tdata.ff = NULL;\n\n\terr = -ENOMEM;\n\tdata.orig_pages = kcalloc(fc->max_pages,\n\t\t\t\t  sizeof(struct page *),\n\t\t\t\t  GFP_NOFS);\n\tif (!data.orig_pages)\n\t\tgoto out;\n\n\terr = write_cache_pages(mapping, wbc, fuse_writepages_fill, &data);\n\tif (data.wpa) {\n\t\tWARN_ON(!data.wpa->ia.ap.num_pages);\n\t\tfuse_writepages_send(&data);\n\t}\n\tif (data.ff)\n\t\tfuse_file_put(data.ff, false, false);\n\n\tkfree(data.orig_pages);\nout:\n\treturn err;\n}",
        "func": "static int fuse_writepages(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_fill_wb_data data;\n\tint err;\n\n\terr = -EIO;\n\tif (fuse_is_bad(inode))\n\t\tgoto out;\n\n\tdata.inode = inode;\n\tdata.wpa = NULL;\n\tdata.ff = NULL;\n\n\terr = -ENOMEM;\n\tdata.orig_pages = kcalloc(fc->max_pages,\n\t\t\t\t  sizeof(struct page *),\n\t\t\t\t  GFP_NOFS);\n\tif (!data.orig_pages)\n\t\tgoto out;\n\n\terr = write_cache_pages(mapping, wbc, fuse_writepages_fill, &data);\n\tif (data.wpa) {\n\t\tWARN_ON(!data.wpa->ia.ap.num_pages);\n\t\tfuse_writepages_send(&data);\n\t}\n\tif (data.ff)\n\t\tfuse_file_put(data.ff, false, false);\n\n\tkfree(data.orig_pages);\nout:\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \tint err;\n \n \terr = -EIO;\n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\tgoto out;\n \n \tdata.inode = inode;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_open_common",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "int fuse_open_common(struct inode *inode, struct file *file, bool isdir)\n{\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_conn *fc = fm->fc;\n\tint err;\n\tbool is_wb_truncate = (file->f_flags & O_TRUNC) &&\n\t\t\t  fc->atomic_o_trunc &&\n\t\t\t  fc->writeback_cache;\n\tbool dax_truncate = (file->f_flags & O_TRUNC) &&\n\t\t\t  fc->atomic_o_trunc && FUSE_IS_DAX(inode);\n\n\terr = generic_file_open(inode, file);\n\tif (err)\n\t\treturn err;\n\n\tif (is_wb_truncate || dax_truncate) {\n\t\tinode_lock(inode);\n\t\tfuse_set_nowrite(inode);\n\t}\n\n\tif (dax_truncate) {\n\t\tdown_write(&get_fuse_inode(inode)->i_mmap_sem);\n\t\terr = fuse_dax_break_layouts(inode, 0, 0);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = fuse_do_open(fm, get_node_id(inode), file, isdir);\n\tif (!err)\n\t\tfuse_finish_open(inode, file);\n\nout:\n\tif (dax_truncate)\n\t\tup_write(&get_fuse_inode(inode)->i_mmap_sem);\n\n\tif (is_wb_truncate | dax_truncate) {\n\t\tfuse_release_nowrite(inode);\n\t\tinode_unlock(inode);\n\t}\n\n\treturn err;\n}",
        "func": "int fuse_open_common(struct inode *inode, struct file *file, bool isdir)\n{\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_conn *fc = fm->fc;\n\tint err;\n\tbool is_wb_truncate = (file->f_flags & O_TRUNC) &&\n\t\t\t  fc->atomic_o_trunc &&\n\t\t\t  fc->writeback_cache;\n\tbool dax_truncate = (file->f_flags & O_TRUNC) &&\n\t\t\t  fc->atomic_o_trunc && FUSE_IS_DAX(inode);\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\terr = generic_file_open(inode, file);\n\tif (err)\n\t\treturn err;\n\n\tif (is_wb_truncate || dax_truncate) {\n\t\tinode_lock(inode);\n\t\tfuse_set_nowrite(inode);\n\t}\n\n\tif (dax_truncate) {\n\t\tdown_write(&get_fuse_inode(inode)->i_mmap_sem);\n\t\terr = fuse_dax_break_layouts(inode, 0, 0);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = fuse_do_open(fm, get_node_id(inode), file, isdir);\n\tif (!err)\n\t\tfuse_finish_open(inode, file);\n\nout:\n\tif (dax_truncate)\n\t\tup_write(&get_fuse_inode(inode)->i_mmap_sem);\n\n\tif (is_wb_truncate | dax_truncate) {\n\t\tfuse_release_nowrite(inode);\n\t\tinode_unlock(inode);\n\t}\n\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,9 @@\n \t\t\t  fc->writeback_cache;\n \tbool dax_truncate = (file->f_flags & O_TRUNC) &&\n \t\t\t  fc->atomic_o_trunc && FUSE_IS_DAX(inode);\n+\n+\tif (fuse_is_bad(inode))\n+\t\treturn -EIO;\n \n \terr = generic_file_open(inode, file);\n \tif (err)",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (fuse_is_bad(inode))",
                "\t\treturn -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_file_read_iter",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static ssize_t fuse_file_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\tif (FUSE_IS_DAX(inode))\n\t\treturn fuse_dax_read_iter(iocb, to);\n\n\tif (!(ff->open_flags & FOPEN_DIRECT_IO))\n\t\treturn fuse_cache_read_iter(iocb, to);\n\telse\n\t\treturn fuse_direct_read_iter(iocb, to);\n}",
        "func": "static ssize_t fuse_file_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (FUSE_IS_DAX(inode))\n\t\treturn fuse_dax_read_iter(iocb, to);\n\n\tif (!(ff->open_flags & FOPEN_DIRECT_IO))\n\t\treturn fuse_cache_read_iter(iocb, to);\n\telse\n\t\treturn fuse_direct_read_iter(iocb, to);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tstruct fuse_file *ff = file->private_data;\n \tstruct inode *inode = file_inode(file);\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \tif (FUSE_IS_DAX(inode))",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_readpage",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static int fuse_readpage(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint err;\n\n\terr = -EIO;\n\tif (is_bad_inode(inode))\n\t\tgoto out;\n\n\terr = fuse_do_readpage(file, page);\n\tfuse_invalidate_atime(inode);\n out:\n\tunlock_page(page);\n\treturn err;\n}",
        "func": "static int fuse_readpage(struct file *file, struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tint err;\n\n\terr = -EIO;\n\tif (fuse_is_bad(inode))\n\t\tgoto out;\n\n\terr = fuse_do_readpage(file, page);\n\tfuse_invalidate_atime(inode);\n out:\n\tunlock_page(page);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tint err;\n \n \terr = -EIO;\n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\tgoto out;\n \n \terr = fuse_do_readpage(file, page);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_file_write_iter",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "static ssize_t fuse_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\tif (FUSE_IS_DAX(inode))\n\t\treturn fuse_dax_write_iter(iocb, from);\n\n\tif (!(ff->open_flags & FOPEN_DIRECT_IO))\n\t\treturn fuse_cache_write_iter(iocb, from);\n\telse\n\t\treturn fuse_direct_write_iter(iocb, from);\n}",
        "func": "static ssize_t fuse_file_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_file *ff = file->private_data;\n\tstruct inode *inode = file_inode(file);\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\tif (FUSE_IS_DAX(inode))\n\t\treturn fuse_dax_write_iter(iocb, from);\n\n\tif (!(ff->open_flags & FOPEN_DIRECT_IO))\n\t\treturn fuse_cache_write_iter(iocb, from);\n\telse\n\t\treturn fuse_direct_write_iter(iocb, from);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n \tstruct fuse_file *ff = file->private_data;\n \tstruct inode *inode = file_inode(file);\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \tif (FUSE_IS_DAX(inode))",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36322",
        "func_name": "torvalds/linux/fuse_ioctl_common",
        "description": "An issue was discovered in the FUSE filesystem implementation in the Linux kernel before 5.10.6, aka CID-5d069dbe8aaf. fuse_do_getattr() calls make_bad_inode() in inappropriate situations, causing a system crash. NOTE: the original fix for this vulnerability was incomplete, and its incompleteness is tracked as CVE-2021-28950.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=5d069dbe8aaf2a197142558b6fb2978189ba3454",
        "commit_title": "Jan Kara's analysis of the syzbot report (edited):",
        "commit_text": "   The reproducer opens a directory on FUSE filesystem, it then attaches   dnotify mark to the open directory.  After that a fuse_do_getattr() call   finds that attributes returned by the server are inconsistent, and calls   make_bad_inode() which, among other things does:            inode->i_mode = S_IFREG;    This then confuses dnotify which doesn't tear down its structures   properly and eventually crashes.  Avoid calling make_bad_inode() on a live inode: switch to a private flag on the fuse inode.  Also add the test to ops which the bad_inode_ops would have caught.  This bug goes back to the initial merge of fuse in 2.6.14...  Cc: <stable@vger.kernel.org> ",
        "func_before": "long fuse_ioctl_common(struct file *file, unsigned int cmd,\n\t\t       unsigned long arg, unsigned int flags)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\tif (!fuse_allow_current_process(fc))\n\t\treturn -EACCES;\n\n\tif (is_bad_inode(inode))\n\t\treturn -EIO;\n\n\treturn fuse_do_ioctl(file, cmd, arg, flags);\n}",
        "func": "long fuse_ioctl_common(struct file *file, unsigned int cmd,\n\t\t       unsigned long arg, unsigned int flags)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\tif (!fuse_allow_current_process(fc))\n\t\treturn -EACCES;\n\n\tif (fuse_is_bad(inode))\n\t\treturn -EIO;\n\n\treturn fuse_do_ioctl(file, cmd, arg, flags);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \tif (!fuse_allow_current_process(fc))\n \t\treturn -EACCES;\n \n-\tif (is_bad_inode(inode))\n+\tif (fuse_is_bad(inode))\n \t\treturn -EIO;\n \n \treturn fuse_do_ioctl(file, cmd, arg, flags);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (is_bad_inode(inode))"
            ],
            "added_lines": [
                "\tif (fuse_is_bad(inode))"
            ]
        }
    }
]