[
    {
        "cve_id": "CVE-2019-12380",
        "func_name": "kernel/git/tip/tip/phys_efi_set_virtual_address_map",
        "description": "**DISPUTED** An issue was discovered in the efi subsystem in the Linux kernel through 5.1.5. phys_efi_set_virtual_address_map in arch/x86/platform/efi/efi.c and efi_call_phys_prolog in arch/x86/platform/efi/efi_64.c mishandle memory allocation failures. NOTE: This id is disputed as not being an issue because “All the code touched by the referenced commit runs only at boot, before any user processes are started. Therefore, there is no possibility for an unprivileged user to control it.”.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?h=4e78921ba4dd0aca1cc89168f45039add4183f8e",
        "commit_title": "The old_memmap flow in efi_call_phys_prolog() performs numerous memory",
        "commit_text": "allocations, and either does not check for failure at all, or it does but fails to propagate it back to the caller, which may end up calling into the firmware with an incomplete 1:1 mapping.  So let's fix this by returning NULL from efi_call_phys_prolog() on memory allocation failures only, and by handling this condition in the caller. Also, clean up any half baked sets of page tables that we may have created before returning with a NULL return value.  Note that any failure at this level will trigger a panic() two levels up, so none of this makes a huge difference, but it is a nice cleanup nonetheless.  [ardb: update commit log, add efi_call_phys_epilog() call on error path]  Cc: Linus Torvalds <torvalds@linux-foundation.org> Cc: Peter Zijlstra <peterz@infradead.org> Cc: Rob Bradford <robert.bradford@intel.com> Cc: Thomas Gleixner <tglx@linutronix.de> Cc: linux-efi@vger.kernel.org Link: http://lkml.kernel.org/r/20190525112559.7917-2-ard.biesheuvel@linaro.org ",
        "func_before": "static efi_status_t __init phys_efi_set_virtual_address_map(\n\tunsigned long memory_map_size,\n\tunsigned long descriptor_size,\n\tu32 descriptor_version,\n\tefi_memory_desc_t *virtual_map)\n{\n\tefi_status_t status;\n\tunsigned long flags;\n\tpgd_t *save_pgd;\n\n\tsave_pgd = efi_call_phys_prolog();\n\n\t/* Disable interrupts around EFI calls: */\n\tlocal_irq_save(flags);\n\tstatus = efi_call_phys(efi_phys.set_virtual_address_map,\n\t\t\t       memory_map_size, descriptor_size,\n\t\t\t       descriptor_version, virtual_map);\n\tlocal_irq_restore(flags);\n\n\tefi_call_phys_epilog(save_pgd);\n\n\treturn status;\n}",
        "func": "static efi_status_t __init phys_efi_set_virtual_address_map(\n\tunsigned long memory_map_size,\n\tunsigned long descriptor_size,\n\tu32 descriptor_version,\n\tefi_memory_desc_t *virtual_map)\n{\n\tefi_status_t status;\n\tunsigned long flags;\n\tpgd_t *save_pgd;\n\n\tsave_pgd = efi_call_phys_prolog();\n\tif (!save_pgd)\n\t\treturn EFI_ABORTED;\n\n\t/* Disable interrupts around EFI calls: */\n\tlocal_irq_save(flags);\n\tstatus = efi_call_phys(efi_phys.set_virtual_address_map,\n\t\t\t       memory_map_size, descriptor_size,\n\t\t\t       descriptor_version, virtual_map);\n\tlocal_irq_restore(flags);\n\n\tefi_call_phys_epilog(save_pgd);\n\n\treturn status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,6 +9,8 @@\n \tpgd_t *save_pgd;\n \n \tsave_pgd = efi_call_phys_prolog();\n+\tif (!save_pgd)\n+\t\treturn EFI_ABORTED;\n \n \t/* Disable interrupts around EFI calls: */\n \tlocal_irq_save(flags);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!save_pgd)",
                "\t\treturn EFI_ABORTED;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-6346",
        "func_name": "facebook/proxygen/HTTP2Codec::parseHeadersDecodeFrames",
        "description": "A potential denial-of-service issue in the Proxygen handling of invalid HTTP2 priority settings (specifically a circular dependency). This affects Proxygen prior to v2018.12.31.00.",
        "git_url": "https://github.com/facebook/proxygen/commit/52cf331743ebd74194d6343a6c2ec52bb917c982",
        "commit_title": "Fix h2 codec state after bad priority header.",
        "commit_text": " Summary: It's possible for the http2 codec to enter an invalid state after processing a http2 header with invalid priorities. CVE-2018-6346  Reviewed By: maxgeorg  Differential Revision: D13510025  fbshipit-source-id: 7c4e42daf1cd2b912454d13a66ab8488d1863263",
        "func_before": "folly::Optional<ErrorCode> HTTP2Codec::parseHeadersDecodeFrames(\n    const folly::Optional<http2::PriorityUpdate>& priority,\n    const folly::Optional<uint32_t>& promisedStream,\n    const folly::Optional<ExAttributes>& exAttributes,\n    std::unique_ptr<HTTPMessage>& msg) {\n  // decompress headers\n  Cursor headerCursor(curHeaderBlock_.front());\n  bool isReq = false;\n  if (promisedStream) {\n    isReq = true;\n  } else if (exAttributes) {\n    isReq = isRequest(curHeader_.stream);\n  } else {\n    isReq = transportDirection_ == TransportDirection::DOWNSTREAM;\n  }\n\n  decodeInfo_.init(isReq, parsingDownstreamTrailers_);\n  if (priority) {\n    if (curHeader_.stream == priority->streamDependency) {\n      streamError(folly::to<string>(\"Circular dependency for txn=\",\n                                    curHeader_.stream),\n                  ErrorCode::PROTOCOL_ERROR,\n                  curHeader_.type == http2::FrameType::HEADERS);\n      return ErrorCode::NO_ERROR;\n    }\n\n    decodeInfo_.msg->setHTTP2Priority(\n        std::make_tuple(priority->streamDependency,\n                        priority->exclusive,\n                        priority->weight));\n  }\n  headerCodec_.decodeStreaming(\n      headerCursor, curHeaderBlock_.chainLength(), this);\n  msg = std::move(decodeInfo_.msg);\n  // Saving this in case we need to log it on error\n  auto g = folly::makeGuard([this] { curHeaderBlock_.move(); });\n  // Check decoding error\n  if (decodeInfo_.decodeError != HPACK::DecodeError::NONE) {\n    static const std::string decodeErrorMessage =\n        \"Failed decoding header block for stream=\";\n    // Avoid logging header blocks that have failed decoding due to being\n    // excessively large.\n    if (decodeInfo_.decodeError != HPACK::DecodeError::HEADERS_TOO_LARGE) {\n      LOG(ERROR) << decodeErrorMessage << curHeader_.stream\n                 << \" header block=\";\n      VLOG(3) << IOBufPrinter::printHexFolly(curHeaderBlock_.front(), true);\n    } else {\n      LOG(ERROR) << decodeErrorMessage << curHeader_.stream;\n    }\n\n    if (msg) {\n      // print the partial message\n      msg->dumpMessage(3);\n    }\n    return ErrorCode::COMPRESSION_ERROR;\n  }\n\n  // Check parsing error\n  if (decodeInfo_.parsingError != \"\") {\n    LOG(ERROR) << \"Failed parsing header list for stream=\" << curHeader_.stream\n               << \", error=\" << decodeInfo_.parsingError << \", header block=\";\n    VLOG(3) << IOBufPrinter::printHexFolly(curHeaderBlock_.front(), true);\n    HTTPException err(HTTPException::Direction::INGRESS,\n                      folly::to<std::string>(\"HTTP2Codec stream error: \",\n                                             \"stream=\",\n                                             curHeader_.stream,\n                                             \" status=\",\n                                             400,\n                                             \" error: \",\n                                             decodeInfo_.parsingError));\n    err.setHttpStatusCode(400);\n    callback_->onError(curHeader_.stream, err, true);\n    return ErrorCode::NO_ERROR;\n  }\n\n  return folly::Optional<ErrorCode>();\n}",
        "func": "folly::Optional<ErrorCode> HTTP2Codec::parseHeadersDecodeFrames(\n    const folly::Optional<http2::PriorityUpdate>& priority,\n    const folly::Optional<uint32_t>& promisedStream,\n    const folly::Optional<ExAttributes>& exAttributes,\n    std::unique_ptr<HTTPMessage>& msg) {\n  // decompress headers\n  Cursor headerCursor(curHeaderBlock_.front());\n  bool isReq = false;\n  if (promisedStream) {\n    isReq = true;\n  } else if (exAttributes) {\n    isReq = isRequest(curHeader_.stream);\n  } else {\n    isReq = transportDirection_ == TransportDirection::DOWNSTREAM;\n  }\n\n  // Validate circular dependencies.\n  if (priority && (curHeader_.stream == priority->streamDependency)) {\n    streamError(\n        folly::to<string>(\"Circular dependency for txn=\", curHeader_.stream),\n        ErrorCode::PROTOCOL_ERROR,\n        curHeader_.type == http2::FrameType::HEADERS);\n    return ErrorCode::NO_ERROR;\n  }\n\n  decodeInfo_.init(isReq, parsingDownstreamTrailers_);\n  if (priority) {\n    decodeInfo_.msg->setHTTP2Priority(\n        std::make_tuple(priority->streamDependency,\n                        priority->exclusive,\n                        priority->weight));\n  }\n\n  headerCodec_.decodeStreaming(\n      headerCursor, curHeaderBlock_.chainLength(), this);\n  msg = std::move(decodeInfo_.msg);\n  // Saving this in case we need to log it on error\n  auto g = folly::makeGuard([this] { curHeaderBlock_.move(); });\n  // Check decoding error\n  if (decodeInfo_.decodeError != HPACK::DecodeError::NONE) {\n    static const std::string decodeErrorMessage =\n        \"Failed decoding header block for stream=\";\n    // Avoid logging header blocks that have failed decoding due to being\n    // excessively large.\n    if (decodeInfo_.decodeError != HPACK::DecodeError::HEADERS_TOO_LARGE) {\n      LOG(ERROR) << decodeErrorMessage << curHeader_.stream\n                 << \" header block=\";\n      VLOG(3) << IOBufPrinter::printHexFolly(curHeaderBlock_.front(), true);\n    } else {\n      LOG(ERROR) << decodeErrorMessage << curHeader_.stream;\n    }\n\n    if (msg) {\n      // print the partial message\n      msg->dumpMessage(3);\n    }\n    return ErrorCode::COMPRESSION_ERROR;\n  }\n\n  // Check parsing error\n  if (decodeInfo_.parsingError != \"\") {\n    LOG(ERROR) << \"Failed parsing header list for stream=\" << curHeader_.stream\n               << \", error=\" << decodeInfo_.parsingError << \", header block=\";\n    VLOG(3) << IOBufPrinter::printHexFolly(curHeaderBlock_.front(), true);\n    HTTPException err(HTTPException::Direction::INGRESS,\n                      folly::to<std::string>(\"HTTP2Codec stream error: \",\n                                             \"stream=\",\n                                             curHeader_.stream,\n                                             \" status=\",\n                                             400,\n                                             \" error: \",\n                                             decodeInfo_.parsingError));\n    err.setHttpStatusCode(400);\n    callback_->onError(curHeader_.stream, err, true);\n    return ErrorCode::NO_ERROR;\n  }\n\n  return folly::Optional<ErrorCode>();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,21 +14,23 @@\n     isReq = transportDirection_ == TransportDirection::DOWNSTREAM;\n   }\n \n+  // Validate circular dependencies.\n+  if (priority && (curHeader_.stream == priority->streamDependency)) {\n+    streamError(\n+        folly::to<string>(\"Circular dependency for txn=\", curHeader_.stream),\n+        ErrorCode::PROTOCOL_ERROR,\n+        curHeader_.type == http2::FrameType::HEADERS);\n+    return ErrorCode::NO_ERROR;\n+  }\n+\n   decodeInfo_.init(isReq, parsingDownstreamTrailers_);\n   if (priority) {\n-    if (curHeader_.stream == priority->streamDependency) {\n-      streamError(folly::to<string>(\"Circular dependency for txn=\",\n-                                    curHeader_.stream),\n-                  ErrorCode::PROTOCOL_ERROR,\n-                  curHeader_.type == http2::FrameType::HEADERS);\n-      return ErrorCode::NO_ERROR;\n-    }\n-\n     decodeInfo_.msg->setHTTP2Priority(\n         std::make_tuple(priority->streamDependency,\n                         priority->exclusive,\n                         priority->weight));\n   }\n+\n   headerCodec_.decodeStreaming(\n       headerCursor, curHeaderBlock_.chainLength(), this);\n   msg = std::move(decodeInfo_.msg);",
        "diff_line_info": {
            "deleted_lines": [
                "    if (curHeader_.stream == priority->streamDependency) {",
                "      streamError(folly::to<string>(\"Circular dependency for txn=\",",
                "                                    curHeader_.stream),",
                "                  ErrorCode::PROTOCOL_ERROR,",
                "                  curHeader_.type == http2::FrameType::HEADERS);",
                "      return ErrorCode::NO_ERROR;",
                "    }",
                ""
            ],
            "added_lines": [
                "  // Validate circular dependencies.",
                "  if (priority && (curHeader_.stream == priority->streamDependency)) {",
                "    streamError(",
                "        folly::to<string>(\"Circular dependency for txn=\", curHeader_.stream),",
                "        ErrorCode::PROTOCOL_ERROR,",
                "        curHeader_.type == http2::FrameType::HEADERS);",
                "    return ErrorCode::NO_ERROR;",
                "  }",
                "",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17564",
        "func_name": "xen-project/xen/shadow_set_l4e",
        "description": "An issue was discovered in Xen through 4.9.x allowing guest OS users to cause a denial of service (host OS crash) or gain host OS privileges by leveraging incorrect error handling for reference counting in shadow mode.",
        "git_url": "https://github.com/xen-project/xen/commit/10be8001de7d87be1f0ccdda75cc70e922e56d03",
        "commit_title": "x86/shadow: fix ref-counting error handling",
        "commit_text": " The old-Linux handling in shadow_set_l4e() mistakenly ORed together the results of sh_get_ref() and sh_pin(). As the latter failing is not a correctness problem, simply ignore its return value.  In sh_set_toplevel_shadow() a failing sh_get_ref() must not be accompanied by installing the entry, despite the domain being crashed.  This is XSA-250. ",
        "func_before": "static int shadow_set_l4e(struct domain *d,\n                          shadow_l4e_t *sl4e,\n                          shadow_l4e_t new_sl4e,\n                          mfn_t sl4mfn)\n{\n    int flags = 0, ok;\n    shadow_l4e_t old_sl4e;\n    paddr_t paddr;\n    ASSERT(sl4e != NULL);\n    old_sl4e = *sl4e;\n\n    if ( old_sl4e.l4 == new_sl4e.l4 ) return 0; /* Nothing to do */\n\n    paddr = ((((paddr_t)mfn_x(sl4mfn)) << PAGE_SHIFT)\n             | (((unsigned long)sl4e) & ~PAGE_MASK));\n\n    if ( shadow_l4e_get_flags(new_sl4e) & _PAGE_PRESENT )\n    {\n        /* About to install a new reference */\n        mfn_t sl3mfn = shadow_l4e_get_mfn(new_sl4e);\n        ok = sh_get_ref(d, sl3mfn, paddr);\n        /* Are we pinning l3 shadows to handle wierd linux behaviour? */\n        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )\n            ok |= sh_pin(d, sl3mfn);\n        if ( !ok )\n        {\n            domain_crash(d);\n            return SHADOW_SET_ERROR;\n        }\n    }\n\n    /* Write the new entry */\n    shadow_write_entries(sl4e, &new_sl4e, 1, sl4mfn);\n    flags |= SHADOW_SET_CHANGED;\n\n    if ( shadow_l4e_get_flags(old_sl4e) & _PAGE_PRESENT )\n    {\n        /* We lost a reference to an old mfn. */\n        mfn_t osl3mfn = shadow_l4e_get_mfn(old_sl4e);\n        if ( (mfn_x(osl3mfn) != mfn_x(shadow_l4e_get_mfn(new_sl4e)))\n             || !perms_strictly_increased(shadow_l4e_get_flags(old_sl4e),\n                                          shadow_l4e_get_flags(new_sl4e)) )\n        {\n            flags |= SHADOW_SET_FLUSH;\n        }\n        sh_put_ref(d, osl3mfn, paddr);\n    }\n    return flags;\n}",
        "func": "static int shadow_set_l4e(struct domain *d,\n                          shadow_l4e_t *sl4e,\n                          shadow_l4e_t new_sl4e,\n                          mfn_t sl4mfn)\n{\n    int flags = 0;\n    shadow_l4e_t old_sl4e;\n    paddr_t paddr;\n    ASSERT(sl4e != NULL);\n    old_sl4e = *sl4e;\n\n    if ( old_sl4e.l4 == new_sl4e.l4 ) return 0; /* Nothing to do */\n\n    paddr = ((((paddr_t)mfn_x(sl4mfn)) << PAGE_SHIFT)\n             | (((unsigned long)sl4e) & ~PAGE_MASK));\n\n    if ( shadow_l4e_get_flags(new_sl4e) & _PAGE_PRESENT )\n    {\n        /* About to install a new reference */\n        mfn_t sl3mfn = shadow_l4e_get_mfn(new_sl4e);\n\n        if ( !sh_get_ref(d, sl3mfn, paddr) )\n        {\n            domain_crash(d);\n            return SHADOW_SET_ERROR;\n        }\n\n        /* Are we pinning l3 shadows to handle weird Linux behaviour? */\n        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )\n            sh_pin(d, sl3mfn);\n    }\n\n    /* Write the new entry */\n    shadow_write_entries(sl4e, &new_sl4e, 1, sl4mfn);\n    flags |= SHADOW_SET_CHANGED;\n\n    if ( shadow_l4e_get_flags(old_sl4e) & _PAGE_PRESENT )\n    {\n        /* We lost a reference to an old mfn. */\n        mfn_t osl3mfn = shadow_l4e_get_mfn(old_sl4e);\n        if ( (mfn_x(osl3mfn) != mfn_x(shadow_l4e_get_mfn(new_sl4e)))\n             || !perms_strictly_increased(shadow_l4e_get_flags(old_sl4e),\n                                          shadow_l4e_get_flags(new_sl4e)) )\n        {\n            flags |= SHADOW_SET_FLUSH;\n        }\n        sh_put_ref(d, osl3mfn, paddr);\n    }\n    return flags;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,7 +3,7 @@\n                           shadow_l4e_t new_sl4e,\n                           mfn_t sl4mfn)\n {\n-    int flags = 0, ok;\n+    int flags = 0;\n     shadow_l4e_t old_sl4e;\n     paddr_t paddr;\n     ASSERT(sl4e != NULL);\n@@ -18,15 +18,16 @@\n     {\n         /* About to install a new reference */\n         mfn_t sl3mfn = shadow_l4e_get_mfn(new_sl4e);\n-        ok = sh_get_ref(d, sl3mfn, paddr);\n-        /* Are we pinning l3 shadows to handle wierd linux behaviour? */\n-        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )\n-            ok |= sh_pin(d, sl3mfn);\n-        if ( !ok )\n+\n+        if ( !sh_get_ref(d, sl3mfn, paddr) )\n         {\n             domain_crash(d);\n             return SHADOW_SET_ERROR;\n         }\n+\n+        /* Are we pinning l3 shadows to handle weird Linux behaviour? */\n+        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )\n+            sh_pin(d, sl3mfn);\n     }\n \n     /* Write the new entry */",
        "diff_line_info": {
            "deleted_lines": [
                "    int flags = 0, ok;",
                "        ok = sh_get_ref(d, sl3mfn, paddr);",
                "        /* Are we pinning l3 shadows to handle wierd linux behaviour? */",
                "        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )",
                "            ok |= sh_pin(d, sl3mfn);",
                "        if ( !ok )"
            ],
            "added_lines": [
                "    int flags = 0;",
                "",
                "        if ( !sh_get_ref(d, sl3mfn, paddr) )",
                "",
                "        /* Are we pinning l3 shadows to handle weird Linux behaviour? */",
                "        if ( sh_type_is_pinnable(d, SH_type_l3_64_shadow) )",
                "            sh_pin(d, sl3mfn);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-17564",
        "func_name": "xen-project/xen/sh_set_toplevel_shadow",
        "description": "An issue was discovered in Xen through 4.9.x allowing guest OS users to cause a denial of service (host OS crash) or gain host OS privileges by leveraging incorrect error handling for reference counting in shadow mode.",
        "git_url": "https://github.com/xen-project/xen/commit/10be8001de7d87be1f0ccdda75cc70e922e56d03",
        "commit_title": "x86/shadow: fix ref-counting error handling",
        "commit_text": " The old-Linux handling in shadow_set_l4e() mistakenly ORed together the results of sh_get_ref() and sh_pin(). As the latter failing is not a correctness problem, simply ignore its return value.  In sh_set_toplevel_shadow() a failing sh_get_ref() must not be accompanied by installing the entry, despite the domain being crashed.  This is XSA-250. ",
        "func_before": "static void\nsh_set_toplevel_shadow(struct vcpu *v,\n                       int slot,\n                       mfn_t gmfn,\n                       unsigned int root_type)\n{\n    mfn_t smfn;\n    pagetable_t old_entry, new_entry;\n\n    struct domain *d = v->domain;\n\n    /* Remember the old contents of this slot */\n    old_entry = v->arch.shadow_table[slot];\n\n    /* Now figure out the new contents: is this a valid guest MFN? */\n    if ( !mfn_valid(gmfn) )\n    {\n        new_entry = pagetable_null();\n        goto install_new_entry;\n    }\n\n    /* Guest mfn is valid: shadow it and install the shadow */\n    smfn = get_shadow_status(d, gmfn, root_type);\n    if ( !mfn_valid(smfn) )\n    {\n        /* Make sure there's enough free shadow memory. */\n        shadow_prealloc(d, root_type, 1);\n        /* Shadow the page. */\n        smfn = sh_make_shadow(v, gmfn, root_type);\n    }\n    ASSERT(mfn_valid(smfn));\n\n    /* Pin the shadow and put it (back) on the list of pinned shadows */\n    if ( sh_pin(d, smfn) == 0 )\n    {\n        SHADOW_ERROR(\"can't pin %#lx as toplevel shadow\\n\", mfn_x(smfn));\n        domain_crash(d);\n    }\n\n    /* Take a ref to this page: it will be released in sh_detach_old_tables()\n     * or the next call to set_toplevel_shadow() */\n    if ( !sh_get_ref(d, smfn, 0) )\n    {\n        SHADOW_ERROR(\"can't install %#lx as toplevel shadow\\n\", mfn_x(smfn));\n        domain_crash(d);\n    }\n\n    new_entry = pagetable_from_mfn(smfn);\n\n install_new_entry:\n    /* Done.  Install it */\n    SHADOW_PRINTK(\"%u/%u [%u] gmfn %#\"PRI_mfn\" smfn %#\"PRI_mfn\"\\n\",\n                  GUEST_PAGING_LEVELS, SHADOW_PAGING_LEVELS, slot,\n                  mfn_x(gmfn), mfn_x(pagetable_get_mfn(new_entry)));\n    v->arch.shadow_table[slot] = new_entry;\n\n    /* Decrement the refcount of the old contents of this slot */\n    if ( !pagetable_is_null(old_entry) ) {\n        mfn_t old_smfn = pagetable_get_mfn(old_entry);\n        /* Need to repin the old toplevel shadow if it's been unpinned\n         * by shadow_prealloc(): in PV mode we're still running on this\n         * shadow and it's not safe to free it yet. */\n        if ( !mfn_to_page(old_smfn)->u.sh.pinned && !sh_pin(d, old_smfn) )\n        {\n            SHADOW_ERROR(\"can't re-pin %#lx\\n\", mfn_x(old_smfn));\n            domain_crash(d);\n        }\n        sh_put_ref(d, old_smfn, 0);\n    }\n}",
        "func": "static void\nsh_set_toplevel_shadow(struct vcpu *v,\n                       int slot,\n                       mfn_t gmfn,\n                       unsigned int root_type)\n{\n    mfn_t smfn;\n    pagetable_t old_entry, new_entry;\n\n    struct domain *d = v->domain;\n\n    /* Remember the old contents of this slot */\n    old_entry = v->arch.shadow_table[slot];\n\n    /* Now figure out the new contents: is this a valid guest MFN? */\n    if ( !mfn_valid(gmfn) )\n    {\n        new_entry = pagetable_null();\n        goto install_new_entry;\n    }\n\n    /* Guest mfn is valid: shadow it and install the shadow */\n    smfn = get_shadow_status(d, gmfn, root_type);\n    if ( !mfn_valid(smfn) )\n    {\n        /* Make sure there's enough free shadow memory. */\n        shadow_prealloc(d, root_type, 1);\n        /* Shadow the page. */\n        smfn = sh_make_shadow(v, gmfn, root_type);\n    }\n    ASSERT(mfn_valid(smfn));\n\n    /* Pin the shadow and put it (back) on the list of pinned shadows */\n    if ( sh_pin(d, smfn) == 0 )\n    {\n        SHADOW_ERROR(\"can't pin %#lx as toplevel shadow\\n\", mfn_x(smfn));\n        domain_crash(d);\n    }\n\n    /* Take a ref to this page: it will be released in sh_detach_old_tables()\n     * or the next call to set_toplevel_shadow() */\n    if ( sh_get_ref(d, smfn, 0) )\n        new_entry = pagetable_from_mfn(smfn);\n    else\n    {\n        SHADOW_ERROR(\"can't install %#lx as toplevel shadow\\n\", mfn_x(smfn));\n        domain_crash(d);\n        new_entry = pagetable_null();\n    }\n\n install_new_entry:\n    /* Done.  Install it */\n    SHADOW_PRINTK(\"%u/%u [%u] gmfn %#\"PRI_mfn\" smfn %#\"PRI_mfn\"\\n\",\n                  GUEST_PAGING_LEVELS, SHADOW_PAGING_LEVELS, slot,\n                  mfn_x(gmfn), mfn_x(pagetable_get_mfn(new_entry)));\n    v->arch.shadow_table[slot] = new_entry;\n\n    /* Decrement the refcount of the old contents of this slot */\n    if ( !pagetable_is_null(old_entry) ) {\n        mfn_t old_smfn = pagetable_get_mfn(old_entry);\n        /* Need to repin the old toplevel shadow if it's been unpinned\n         * by shadow_prealloc(): in PV mode we're still running on this\n         * shadow and it's not safe to free it yet. */\n        if ( !mfn_to_page(old_smfn)->u.sh.pinned && !sh_pin(d, old_smfn) )\n        {\n            SHADOW_ERROR(\"can't re-pin %#lx\\n\", mfn_x(old_smfn));\n            domain_crash(d);\n        }\n        sh_put_ref(d, old_smfn, 0);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -39,13 +39,14 @@\n \n     /* Take a ref to this page: it will be released in sh_detach_old_tables()\n      * or the next call to set_toplevel_shadow() */\n-    if ( !sh_get_ref(d, smfn, 0) )\n+    if ( sh_get_ref(d, smfn, 0) )\n+        new_entry = pagetable_from_mfn(smfn);\n+    else\n     {\n         SHADOW_ERROR(\"can't install %#lx as toplevel shadow\\n\", mfn_x(smfn));\n         domain_crash(d);\n+        new_entry = pagetable_null();\n     }\n-\n-    new_entry = pagetable_from_mfn(smfn);\n \n  install_new_entry:\n     /* Done.  Install it */",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( !sh_get_ref(d, smfn, 0) )",
                "",
                "    new_entry = pagetable_from_mfn(smfn);"
            ],
            "added_lines": [
                "    if ( sh_get_ref(d, smfn, 0) )",
                "        new_entry = pagetable_from_mfn(smfn);",
                "    else",
                "        new_entry = pagetable_null();"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9588",
        "func_name": "torvalds/linux/vmx_complete_atomic_exit",
        "description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.9 mismanages the #BP and #OF exceptions, which allows guest OS users to cause a denial of service (guest OS crash) by declining to handle an exception thrown by an L2 guest.",
        "git_url": "https://github.com/torvalds/linux/commit/ef85b67385436ddc1998f45f1d6a210f935b3388",
        "commit_title": "kvm: nVMX: Allow L1 to intercept software exceptions (#BP and #OF)",
        "commit_text": " When L2 exits to L0 due to \"exception or NMI\", software exceptions (#BP and #OF) for which L1 has requested an intercept should be handled by L1 rather than L0. Previously, only hardware exceptions were forwarded to L1.  Cc: stable@vger.kernel.org",
        "func_before": "static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info;\n\n\tif (!(vmx->exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || vmx->exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tvmx->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\texit_intr_info = vmx->exit_intr_info;\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif ((exit_intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR &&\n\t    (exit_intr_info & INTR_INFO_VALID_MASK)) {\n\t\tkvm_before_handle_nmi(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_handle_nmi(&vmx->vcpu);\n\t}\n}",
        "func": "static void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info;\n\n\tif (!(vmx->exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || vmx->exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tvmx->exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\texit_intr_info = vmx->exit_intr_info;\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif (is_nmi(exit_intr_info)) {\n\t\tkvm_before_handle_nmi(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_handle_nmi(&vmx->vcpu);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,8 +14,7 @@\n \t\tkvm_machine_check();\n \n \t/* We need to handle NMIs before interrupts are enabled */\n-\tif ((exit_intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR &&\n-\t    (exit_intr_info & INTR_INFO_VALID_MASK)) {\n+\tif (is_nmi(exit_intr_info)) {\n \t\tkvm_before_handle_nmi(&vmx->vcpu);\n \t\tasm(\"int $2\");\n \t\tkvm_after_handle_nmi(&vmx->vcpu);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif ((exit_intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR &&",
                "\t    (exit_intr_info & INTR_INFO_VALID_MASK)) {"
            ],
            "added_lines": [
                "\tif (is_nmi(exit_intr_info)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9588",
        "func_name": "torvalds/linux/nested_vmx_exit_handled",
        "description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.9 mismanages the #BP and #OF exceptions, which allows guest OS users to cause a denial of service (guest OS crash) by declining to handle an exception thrown by an L2 guest.",
        "git_url": "https://github.com/torvalds/linux/commit/ef85b67385436ddc1998f45f1d6a210f935b3388",
        "commit_title": "kvm: nVMX: Allow L1 to intercept software exceptions (#BP and #OF)",
        "commit_text": " When L2 exits to L0 due to \"exception or NMI\", software exceptions (#BP and #OF) for which L1 has requested an intercept should be handled by L1 rather than L0. Previously, only hardware exceptions were forwarded to L1.  Cc: stable@vger.kernel.org",
        "func_before": "static bool nested_vmx_exit_handled(struct kvm_vcpu *vcpu)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\n\ttrace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,\n\t\t\t\tvmcs_readl(EXIT_QUALIFICATION),\n\t\t\t\tvmx->idt_vectoring_info,\n\t\t\t\tintr_info,\n\t\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t\t\t\tKVM_ISA_VMX);\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn false;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn true;\n\t}\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (!is_exception(intr_info))\n\t\t\treturn false;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn enable_ept;\n\t\telse if (is_no_device(intr_info) &&\n\t\t\t !(vmcs12->guest_cr0 & X86_CR0_TS))\n\t\t\treturn false;\n\t\telse if (is_debug(intr_info) &&\n\t\t\t vcpu->guest_debug &\n\t\t\t (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\telse if (is_breakpoint(intr_info) &&\n\t\t\t vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn false;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn true;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn true;\n\tcase EXIT_REASON_CPUID:\n\t\tif (kvm_register_read(vcpu, VCPU_REGS_RAX) == 0xa)\n\t\t\treturn false;\n\t\treturn true;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn true;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn true;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_TRAP_FLAG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn false;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);\n\tcase EXIT_REASON_APIC_ACCESS:\n\t\treturn nested_cpu_has2(vmcs12,\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\tcase EXIT_REASON_APIC_WRITE:\n\tcase EXIT_REASON_EOI_INDUCED:\n\t\t/* apic_write and eoi_induced should exit unconditionally. */\n\t\treturn true;\n\tcase EXIT_REASON_EPT_VIOLATION:\n\t\t/*\n\t\t * L0 always deals with the EPT violation. If nested EPT is\n\t\t * used, and the nested mmu code discovers that the address is\n\t\t * missing in the guest EPT table (EPT12), the EPT violation\n\t\t * will be injected with nested_ept_inject_page_fault()\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\t/*\n\t\t * L2 never uses directly L1's EPT, but rather L0's own EPT\n\t\t * table (shadow on EPT) or a merged EPT table that L0 built\n\t\t * (EPT on EPT). So any problems with the structure of the\n\t\t * table is L0's fault.\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn true;\n\tcase EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:\n\t\t/*\n\t\t * This should never happen, since it is not possible to\n\t\t * set XSS to a non-zero value---neither in L1 nor in L2.\n\t\t * If if it were, XSS would have to be checked against\n\t\t * the XSS exit bitmap in vmcs12.\n\t\t */\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
        "func": "static bool nested_vmx_exit_handled(struct kvm_vcpu *vcpu)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\n\ttrace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,\n\t\t\t\tvmcs_readl(EXIT_QUALIFICATION),\n\t\t\t\tvmx->idt_vectoring_info,\n\t\t\t\tintr_info,\n\t\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t\t\t\tKVM_ISA_VMX);\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn false;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn true;\n\t}\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (is_nmi(intr_info))\n\t\t\treturn false;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn enable_ept;\n\t\telse if (is_no_device(intr_info) &&\n\t\t\t !(vmcs12->guest_cr0 & X86_CR0_TS))\n\t\t\treturn false;\n\t\telse if (is_debug(intr_info) &&\n\t\t\t vcpu->guest_debug &\n\t\t\t (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\telse if (is_breakpoint(intr_info) &&\n\t\t\t vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn false;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn true;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn true;\n\tcase EXIT_REASON_CPUID:\n\t\tif (kvm_register_read(vcpu, VCPU_REGS_RAX) == 0xa)\n\t\t\treturn false;\n\t\treturn true;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn true;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn true;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_TRAP_FLAG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn false;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);\n\tcase EXIT_REASON_APIC_ACCESS:\n\t\treturn nested_cpu_has2(vmcs12,\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\tcase EXIT_REASON_APIC_WRITE:\n\tcase EXIT_REASON_EOI_INDUCED:\n\t\t/* apic_write and eoi_induced should exit unconditionally. */\n\t\treturn true;\n\tcase EXIT_REASON_EPT_VIOLATION:\n\t\t/*\n\t\t * L0 always deals with the EPT violation. If nested EPT is\n\t\t * used, and the nested mmu code discovers that the address is\n\t\t * missing in the guest EPT table (EPT12), the EPT violation\n\t\t * will be injected with nested_ept_inject_page_fault()\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\t/*\n\t\t * L2 never uses directly L1's EPT, but rather L0's own EPT\n\t\t * table (shadow on EPT) or a merged EPT table that L0 built\n\t\t * (EPT on EPT). So any problems with the structure of the\n\t\t * table is L0's fault.\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn true;\n\tcase EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:\n\t\t/*\n\t\t * This should never happen, since it is not possible to\n\t\t * set XSS to a non-zero value---neither in L1 nor in L2.\n\t\t * If if it were, XSS would have to be checked against\n\t\t * the XSS exit bitmap in vmcs12.\n\t\t */\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,7 +23,7 @@\n \n \tswitch (exit_reason) {\n \tcase EXIT_REASON_EXCEPTION_NMI:\n-\t\tif (!is_exception(intr_info))\n+\t\tif (is_nmi(intr_info))\n \t\t\treturn false;\n \t\telse if (is_page_fault(intr_info))\n \t\t\treturn enable_ept;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (!is_exception(intr_info))"
            ],
            "added_lines": [
                "\t\tif (is_nmi(intr_info))"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-9588",
        "func_name": "torvalds/linux/handle_exception",
        "description": "arch/x86/kvm/vmx.c in the Linux kernel through 4.9 mismanages the #BP and #OF exceptions, which allows guest OS users to cause a denial of service (guest OS crash) by declining to handle an exception thrown by an L2 guest.",
        "git_url": "https://github.com/torvalds/linux/commit/ef85b67385436ddc1998f45f1d6a210f935b3388",
        "commit_title": "kvm: nVMX: Allow L1 to intercept software exceptions (#BP and #OF)",
        "commit_text": " When L2 exits to L0 due to \"exception or NMI\", software exceptions (#BP and #OF) for which L1 has requested an intercept should be handled by L1 rather than L0. Previously, only hardware exceptions were forwarded to L1.  Cc: stable@vger.kernel.org",
        "func_before": "static int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif ((intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR)\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_no_device(intr_info)) {\n\t\tvmx_fpu_activate(vcpu);\n\t\treturn 1;\n\t}\n\n\tif (is_invalid_opcode(intr_info)) {\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\ter = emulate_instruction(vcpu, EMULTYPE_TRAP_UD);\n\t\tif (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\t/* EPT won't cause page fault directly */\n\t\tBUG_ON(enable_ept);\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\ttrace_kvm_page_fault(cr2, error_code);\n\n\t\tif (kvm_event_needs_reinjection(vcpu))\n\t\t\tkvm_mmu_unprotect_page_virt(vcpu, cr2);\n\t\treturn kvm_mmu_page_fault(vcpu, cr2, error_code, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (!(dr6 & ~DR6_RESERVED)) /* icebp */\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "func": "static int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif (is_nmi(intr_info))\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_no_device(intr_info)) {\n\t\tvmx_fpu_activate(vcpu);\n\t\treturn 1;\n\t}\n\n\tif (is_invalid_opcode(intr_info)) {\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\ter = emulate_instruction(vcpu, EMULTYPE_TRAP_UD);\n\t\tif (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\t/* EPT won't cause page fault directly */\n\t\tBUG_ON(enable_ept);\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\ttrace_kvm_page_fault(cr2, error_code);\n\n\t\tif (kvm_event_needs_reinjection(vcpu))\n\t\t\tkvm_mmu_unprotect_page_virt(vcpu, cr2);\n\t\treturn kvm_mmu_page_fault(vcpu, cr2, error_code, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (!(dr6 & ~DR6_RESERVED)) /* icebp */\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,7 +13,7 @@\n \tif (is_machine_check(intr_info))\n \t\treturn handle_machine_check(vcpu);\n \n-\tif ((intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR)\n+\tif (is_nmi(intr_info))\n \t\treturn 1;  /* already handled by vmx_vcpu_run() */\n \n \tif (is_no_device(intr_info)) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif ((intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR)"
            ],
            "added_lines": [
                "\tif (is_nmi(intr_info))"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-8072",
        "func_name": "torvalds/linux/cp2112_gpio_direction_input",
        "description": "The cp2112_gpio_direction_input function in drivers/hid/hid-cp2112.c in the Linux kernel 4.9.x before 4.9.9 does not have the expected EIO error status for a zero-length report, which allows local users to have an unspecified impact via unknown vectors.",
        "git_url": "https://github.com/torvalds/linux/commit/8e9faa15469ed7c7467423db4c62aeed3ff4cae3",
        "commit_title": "HID: cp2112: fix gpio-callback error handling",
        "commit_text": " In case of a zero-length report, the gpio direction_input callback would currently return success instead of an errno.  Cc: stable <stable@vger.kernel.org>     # 4.9",
        "func_before": "static int cp2112_gpio_direction_input(struct gpio_chip *chip, unsigned offset)\n{\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tbuf[1] &= ~(1 << offset);\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tret = 0;\n\nexit:\n\tmutex_unlock(&dev->lock);\n\treturn ret <= 0 ? ret : -EIO;\n}",
        "func": "static int cp2112_gpio_direction_input(struct gpio_chip *chip, unsigned offset)\n{\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tbuf[1] &= ~(1 << offset);\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tret = 0;\n\nexit:\n\tmutex_unlock(&dev->lock);\n\treturn ret < 0 ? ret : -EIO;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -30,5 +30,5 @@\n \n exit:\n \tmutex_unlock(&dev->lock);\n-\treturn ret <= 0 ? ret : -EIO;\n+\treturn ret < 0 ? ret : -EIO;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn ret <= 0 ? ret : -EIO;"
            ],
            "added_lines": [
                "\treturn ret < 0 ? ret : -EIO;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-5577",
        "func_name": "torvalds/linux/vc4_get_bcl",
        "description": "The vc4_get_bcl function in drivers/gpu/drm/vc4/vc4_gem.c in the VideoCore DRM driver in the Linux kernel before 4.9.7 does not set an errno value upon certain overflow detections, which allows local users to cause a denial of service (incorrect pointer dereference and OOPS) via inconsistent size values in a VC4_SUBMIT_CL ioctl call.",
        "git_url": "https://github.com/torvalds/linux/commit/6b8ac63847bc2f958dd93c09edc941a0118992d9",
        "commit_title": "drm/vc4: Return -EINVAL on the overflow checks failing.",
        "commit_text": " By failing to set the errno, we'd continue on to trying to set up the RCL, and then oops on trying to dereference the tile_bo that binning validation should have set up. ",
        "func_before": "static int\nvc4_get_bcl(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tvoid *temp = NULL;\n\tvoid *bin;\n\tint ret = 0;\n\tuint32_t bin_offset = 0;\n\tuint32_t shader_rec_offset = roundup(bin_offset + args->bin_cl_size,\n\t\t\t\t\t     16);\n\tuint32_t uniforms_offset = shader_rec_offset + args->shader_rec_size;\n\tuint32_t exec_size = uniforms_offset + args->uniforms_size;\n\tuint32_t temp_size = exec_size + (sizeof(struct vc4_shader_state) *\n\t\t\t\t\t  args->shader_rec_count);\n\tstruct vc4_bo *bo;\n\n\tif (shader_rec_offset < args->bin_cl_size ||\n\t    uniforms_offset < shader_rec_offset ||\n\t    exec_size < uniforms_offset ||\n\t    args->shader_rec_count >= (UINT_MAX /\n\t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n\t    temp_size < exec_size) {\n\t\tDRM_ERROR(\"overflow in exec arguments\\n\");\n\t\tgoto fail;\n\t}\n\n\t/* Allocate space where we'll store the copied in user command lists\n\t * and shader records.\n\t *\n\t * We don't just copy directly into the BOs because we need to\n\t * read the contents back for validation, and I think the\n\t * bo->vaddr is uncached access.\n\t */\n\ttemp = drm_malloc_ab(temp_size, 1);\n\tif (!temp) {\n\t\tDRM_ERROR(\"Failed to allocate storage for copying \"\n\t\t\t  \"in bin/render CLs.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tbin = temp + bin_offset;\n\texec->shader_rec_u = temp + shader_rec_offset;\n\texec->uniforms_u = temp + uniforms_offset;\n\texec->shader_state = temp + exec_size;\n\texec->shader_state_size = args->shader_rec_count;\n\n\tif (copy_from_user(bin,\n\t\t\t   (void __user *)(uintptr_t)args->bin_cl,\n\t\t\t   args->bin_cl_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->shader_rec_u,\n\t\t\t   (void __user *)(uintptr_t)args->shader_rec,\n\t\t\t   args->shader_rec_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->uniforms_u,\n\t\t\t   (void __user *)(uintptr_t)args->uniforms,\n\t\t\t   args->uniforms_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tbo = vc4_bo_create(dev, exec_size, true);\n\tif (IS_ERR(bo)) {\n\t\tDRM_ERROR(\"Couldn't allocate BO for binning\\n\");\n\t\tret = PTR_ERR(bo);\n\t\tgoto fail;\n\t}\n\texec->exec_bo = &bo->base;\n\n\tlist_add_tail(&to_vc4_bo(&exec->exec_bo->base)->unref_head,\n\t\t      &exec->unref_list);\n\n\texec->ct0ca = exec->exec_bo->paddr + bin_offset;\n\n\texec->bin_u = bin;\n\n\texec->shader_rec_v = exec->exec_bo->vaddr + shader_rec_offset;\n\texec->shader_rec_p = exec->exec_bo->paddr + shader_rec_offset;\n\texec->shader_rec_size = args->shader_rec_size;\n\n\texec->uniforms_v = exec->exec_bo->vaddr + uniforms_offset;\n\texec->uniforms_p = exec->exec_bo->paddr + uniforms_offset;\n\texec->uniforms_size = args->uniforms_size;\n\n\tret = vc4_validate_bin_cl(dev,\n\t\t\t\t  exec->exec_bo->vaddr + bin_offset,\n\t\t\t\t  bin,\n\t\t\t\t  exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_validate_shader_recs(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Block waiting on any previous rendering into the CS's VBO,\n\t * IB, or textures, so that pixels are actually written by the\n\t * time we try to read them.\n\t */\n\tret = vc4_wait_for_seqno(dev, exec->bin_dep_seqno, ~0ull, true);\n\nfail:\n\tdrm_free_large(temp);\n\treturn ret;\n}",
        "func": "static int\nvc4_get_bcl(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tvoid *temp = NULL;\n\tvoid *bin;\n\tint ret = 0;\n\tuint32_t bin_offset = 0;\n\tuint32_t shader_rec_offset = roundup(bin_offset + args->bin_cl_size,\n\t\t\t\t\t     16);\n\tuint32_t uniforms_offset = shader_rec_offset + args->shader_rec_size;\n\tuint32_t exec_size = uniforms_offset + args->uniforms_size;\n\tuint32_t temp_size = exec_size + (sizeof(struct vc4_shader_state) *\n\t\t\t\t\t  args->shader_rec_count);\n\tstruct vc4_bo *bo;\n\n\tif (shader_rec_offset < args->bin_cl_size ||\n\t    uniforms_offset < shader_rec_offset ||\n\t    exec_size < uniforms_offset ||\n\t    args->shader_rec_count >= (UINT_MAX /\n\t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n\t    temp_size < exec_size) {\n\t\tDRM_ERROR(\"overflow in exec arguments\\n\");\n\t\tret = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\t/* Allocate space where we'll store the copied in user command lists\n\t * and shader records.\n\t *\n\t * We don't just copy directly into the BOs because we need to\n\t * read the contents back for validation, and I think the\n\t * bo->vaddr is uncached access.\n\t */\n\ttemp = drm_malloc_ab(temp_size, 1);\n\tif (!temp) {\n\t\tDRM_ERROR(\"Failed to allocate storage for copying \"\n\t\t\t  \"in bin/render CLs.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tbin = temp + bin_offset;\n\texec->shader_rec_u = temp + shader_rec_offset;\n\texec->uniforms_u = temp + uniforms_offset;\n\texec->shader_state = temp + exec_size;\n\texec->shader_state_size = args->shader_rec_count;\n\n\tif (copy_from_user(bin,\n\t\t\t   (void __user *)(uintptr_t)args->bin_cl,\n\t\t\t   args->bin_cl_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->shader_rec_u,\n\t\t\t   (void __user *)(uintptr_t)args->shader_rec,\n\t\t\t   args->shader_rec_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->uniforms_u,\n\t\t\t   (void __user *)(uintptr_t)args->uniforms,\n\t\t\t   args->uniforms_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tbo = vc4_bo_create(dev, exec_size, true);\n\tif (IS_ERR(bo)) {\n\t\tDRM_ERROR(\"Couldn't allocate BO for binning\\n\");\n\t\tret = PTR_ERR(bo);\n\t\tgoto fail;\n\t}\n\texec->exec_bo = &bo->base;\n\n\tlist_add_tail(&to_vc4_bo(&exec->exec_bo->base)->unref_head,\n\t\t      &exec->unref_list);\n\n\texec->ct0ca = exec->exec_bo->paddr + bin_offset;\n\n\texec->bin_u = bin;\n\n\texec->shader_rec_v = exec->exec_bo->vaddr + shader_rec_offset;\n\texec->shader_rec_p = exec->exec_bo->paddr + shader_rec_offset;\n\texec->shader_rec_size = args->shader_rec_size;\n\n\texec->uniforms_v = exec->exec_bo->vaddr + uniforms_offset;\n\texec->uniforms_p = exec->exec_bo->paddr + uniforms_offset;\n\texec->uniforms_size = args->uniforms_size;\n\n\tret = vc4_validate_bin_cl(dev,\n\t\t\t\t  exec->exec_bo->vaddr + bin_offset,\n\t\t\t\t  bin,\n\t\t\t\t  exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_validate_shader_recs(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\t/* Block waiting on any previous rendering into the CS's VBO,\n\t * IB, or textures, so that pixels are actually written by the\n\t * time we try to read them.\n\t */\n\tret = vc4_wait_for_seqno(dev, exec->bin_dep_seqno, ~0ull, true);\n\nfail:\n\tdrm_free_large(temp);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,7 @@\n \t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n \t    temp_size < exec_size) {\n \t\tDRM_ERROR(\"overflow in exec arguments\\n\");\n+\t\tret = -EINVAL;\n \t\tgoto fail;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tret = -EINVAL;"
            ]
        }
    }
]