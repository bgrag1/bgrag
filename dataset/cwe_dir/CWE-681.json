[
    {
        "cve_id": "CVE-2022-2639",
        "func_name": "torvalds/linux/reserve_sfa_size",
        "description": "An integer coercion error was found in the openvswitch kernel module. Given a sufficiently large number of actions, while copying and reserving memory for a new action of a new flow, the reserve_sfa_size() function does not return -EMSGSIZE as expected, potentially leading to an out-of-bounds write access. This flaw allows a local user to crash or potentially escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/cefa91b2332d7009bc0be5d951d6cbbf349f90f8",
        "commit_title": "openvswitch: fix OOB access in reserve_sfa_size()",
        "commit_text": " Given a sufficiently large number of actions, while copying and reserving memory for a new action of a new flow, if next_offset is greater than MAX_ACTIONS_BUFSIZE, the function reserve_sfa_size() does not return -EMSGSIZE as expected, but it allocates MAX_ACTIONS_BUFSIZE bytes increasing actions_len by req_size. This can then lead to an OOB write access, especially when further actions need to be copied.  Fix it by rearranging the flow action size check.  KASAN splat below:  ================================================================== BUG: KASAN: slab-out-of-bounds in reserve_sfa_size+0x1ba/0x380 [openvswitch] Write of size 65360 at addr ffff888147e4001c by task handler15/836  CPU: 1 PID: 836 Comm: handler15 Not tainted 5.18.0-rc1+ #27 ... Call Trace:  <TASK>  dump_stack_lvl+0x45/0x5a  print_report.cold+0x5e/0x5db  ? __lock_text_start+0x8/0x8  ? reserve_sfa_size+0x1ba/0x380 [openvswitch]  kasan_report+0xb5/0x130  ? reserve_sfa_size+0x1ba/0x380 [openvswitch]  kasan_check_range+0xf5/0x1d0  memcpy+0x39/0x60  reserve_sfa_size+0x1ba/0x380 [openvswitch]  __add_action+0x24/0x120 [openvswitch]  ovs_nla_add_action+0xe/0x20 [openvswitch]  ovs_ct_copy_action+0x29d/0x1130 [openvswitch]  ? __kernel_text_address+0xe/0x30  ? unwind_get_return_address+0x56/0xa0  ? create_prof_cpu_mask+0x20/0x20  ? ovs_ct_verify+0xf0/0xf0 [openvswitch]  ? prep_compound_page+0x198/0x2a0  ? __kasan_check_byte+0x10/0x40  ? kasan_unpoison+0x40/0x70  ? ksize+0x44/0x60  ? reserve_sfa_size+0x75/0x380 [openvswitch]  __ovs_nla_copy_actions+0xc26/0x2070 [openvswitch]  ? __zone_watermark_ok+0x420/0x420  ? validate_set.constprop.0+0xc90/0xc90 [openvswitch]  ? __alloc_pages+0x1a9/0x3e0  ? __alloc_pages_slowpath.constprop.0+0x1da0/0x1da0  ? unwind_next_frame+0x991/0x1e40  ? __mod_node_page_state+0x99/0x120  ? __mod_lruvec_page_state+0x2e3/0x470  ? __kasan_kmalloc_large+0x90/0xe0  ovs_nla_copy_actions+0x1b4/0x2c0 [openvswitch]  ovs_flow_cmd_new+0x3cd/0xb10 [openvswitch]  ...  Cc: stable@vger.kernel.org",
        "func_before": "static struct nlattr *reserve_sfa_size(struct sw_flow_actions **sfa,\n\t\t\t\t       int attr_len, bool log)\n{\n\n\tstruct sw_flow_actions *acts;\n\tint new_acts_size;\n\tsize_t req_size = NLA_ALIGN(attr_len);\n\tint next_offset = offsetof(struct sw_flow_actions, actions) +\n\t\t\t\t\t(*sfa)->actions_len;\n\n\tif (req_size <= (ksize(*sfa) - next_offset))\n\t\tgoto out;\n\n\tnew_acts_size = max(next_offset + req_size, ksize(*sfa) * 2);\n\n\tif (new_acts_size > MAX_ACTIONS_BUFSIZE) {\n\t\tif ((MAX_ACTIONS_BUFSIZE - next_offset) < req_size) {\n\t\t\tOVS_NLERR(log, \"Flow action size exceeds max %u\",\n\t\t\t\t  MAX_ACTIONS_BUFSIZE);\n\t\t\treturn ERR_PTR(-EMSGSIZE);\n\t\t}\n\t\tnew_acts_size = MAX_ACTIONS_BUFSIZE;\n\t}\n\n\tacts = nla_alloc_flow_actions(new_acts_size);\n\tif (IS_ERR(acts))\n\t\treturn (void *)acts;\n\n\tmemcpy(acts->actions, (*sfa)->actions, (*sfa)->actions_len);\n\tacts->actions_len = (*sfa)->actions_len;\n\tacts->orig_len = (*sfa)->orig_len;\n\tkfree(*sfa);\n\t*sfa = acts;\n\nout:\n\t(*sfa)->actions_len += req_size;\n\treturn  (struct nlattr *) ((unsigned char *)(*sfa) + next_offset);\n}",
        "func": "static struct nlattr *reserve_sfa_size(struct sw_flow_actions **sfa,\n\t\t\t\t       int attr_len, bool log)\n{\n\n\tstruct sw_flow_actions *acts;\n\tint new_acts_size;\n\tsize_t req_size = NLA_ALIGN(attr_len);\n\tint next_offset = offsetof(struct sw_flow_actions, actions) +\n\t\t\t\t\t(*sfa)->actions_len;\n\n\tif (req_size <= (ksize(*sfa) - next_offset))\n\t\tgoto out;\n\n\tnew_acts_size = max(next_offset + req_size, ksize(*sfa) * 2);\n\n\tif (new_acts_size > MAX_ACTIONS_BUFSIZE) {\n\t\tif ((next_offset + req_size) > MAX_ACTIONS_BUFSIZE) {\n\t\t\tOVS_NLERR(log, \"Flow action size exceeds max %u\",\n\t\t\t\t  MAX_ACTIONS_BUFSIZE);\n\t\t\treturn ERR_PTR(-EMSGSIZE);\n\t\t}\n\t\tnew_acts_size = MAX_ACTIONS_BUFSIZE;\n\t}\n\n\tacts = nla_alloc_flow_actions(new_acts_size);\n\tif (IS_ERR(acts))\n\t\treturn (void *)acts;\n\n\tmemcpy(acts->actions, (*sfa)->actions, (*sfa)->actions_len);\n\tacts->actions_len = (*sfa)->actions_len;\n\tacts->orig_len = (*sfa)->orig_len;\n\tkfree(*sfa);\n\t*sfa = acts;\n\nout:\n\t(*sfa)->actions_len += req_size;\n\treturn  (struct nlattr *) ((unsigned char *)(*sfa) + next_offset);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n \tnew_acts_size = max(next_offset + req_size, ksize(*sfa) * 2);\n \n \tif (new_acts_size > MAX_ACTIONS_BUFSIZE) {\n-\t\tif ((MAX_ACTIONS_BUFSIZE - next_offset) < req_size) {\n+\t\tif ((next_offset + req_size) > MAX_ACTIONS_BUFSIZE) {\n \t\t\tOVS_NLERR(log, \"Flow action size exceeds max %u\",\n \t\t\t\t  MAX_ACTIONS_BUFSIZE);\n \t\t\treturn ERR_PTR(-EMSGSIZE);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif ((MAX_ACTIONS_BUFSIZE - next_offset) < req_size) {"
            ],
            "added_lines": [
                "\t\tif ((next_offset + req_size) > MAX_ACTIONS_BUFSIZE) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/hbc::generateBytecode",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "std::unique_ptr<BytecodeModule> hbc::generateBytecode(\n    Module *M,\n    raw_ostream &OS,\n    const BytecodeGenerationOptions &options,\n    const SHA1 &sourceHash,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  auto BM = generateBytecodeModule(\n      M,\n      M->getTopLevelFunction(),\n      options,\n      segment,\n      sourceMapGen,\n      std::move(baseBCProvider));\n\n  if (options.format == OutputFormatKind::EmitBundle) {\n    assert(BM != nullptr);\n    BytecodeSerializer BS{OS, options};\n    BS.serialize(*BM, sourceHash);\n  }\n  // Now that the BytecodeFunctions know their offsets into the stream, we can\n  // populate the source map.\n  if (sourceMapGen)\n    BM->populateSourceMap(sourceMapGen);\n  return BM;\n}",
        "func": "std::unique_ptr<BytecodeModule> hbc::generateBytecode(\n    Module *M,\n    raw_ostream &OS,\n    const BytecodeGenerationOptions &options,\n    const SHA1 &sourceHash,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  auto BM = generateBytecodeModule(\n      M,\n      M->getTopLevelFunction(),\n      options,\n      segment,\n      sourceMapGen,\n      std::move(baseBCProvider));\n\n  if (!BM) {\n    return {};\n  }\n\n  if (options.format == OutputFormatKind::EmitBundle) {\n    assert(BM != nullptr);\n    BytecodeSerializer BS{OS, options};\n    BS.serialize(*BM, sourceHash);\n  }\n  // Now that the BytecodeFunctions know their offsets into the stream, we can\n  // populate the source map.\n  if (sourceMapGen)\n    BM->populateSourceMap(sourceMapGen);\n  return BM;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,10 @@\n       sourceMapGen,\n       std::move(baseBCProvider));\n \n+  if (!BM) {\n+    return {};\n+  }\n+\n   if (options.format == OutputFormatKind::EmitBundle) {\n     assert(BM != nullptr);\n     BytecodeSerializer BS{OS, options};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (!BM) {",
                "    return {};",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/hbc::generateBytecodeModule",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *M,\n    Function *lexicalTopLevel,\n    Function *entryPoint,\n    const BytecodeGenerationOptions &options,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  PerfSection perf(\"Bytecode Generation\");\n  lowerIR(M, options);\n\n  if (options.format == DumpLIR)\n    M->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (segment) {\n    BMGen.setSegmentID(*segment);\n  }\n  // Empty if all functions should be generated (i.e. bundle splitting was not\n  // requested).\n  llvh::DenseSet<Function *> functionsToGenerate = segment\n      ? M->getFunctionsInSegment(*segment)\n      : llvh::DenseSet<Function *>{};\n\n  /// \\return true if we should generate function \\p f.\n  std::function<bool(const Function *)> shouldGenerate;\n  if (segment) {\n    shouldGenerate = [entryPoint, &functionsToGenerate](const Function *f) {\n      return f == entryPoint || functionsToGenerate.count(f) > 0;\n    };\n  } else {\n    shouldGenerate = [](const Function *) { return true; };\n  }\n\n  { // Collect all the strings in the bytecode module into a storage.\n    // If we are in delta optimizing mode, start with the string storage from\n    // our base bytecode provider.\n    auto strings = baseBCProvider\n        ? stringAccumulatorFromBCProvider(*baseBCProvider)\n        : UniquingStringLiteralAccumulator{};\n\n    auto addStringOrIdent = [&strings](llvh::StringRef str, bool isIdentifier) {\n      strings.addString(str, isIdentifier);\n    };\n\n    auto addString = [&strings](llvh::StringRef str) {\n      strings.addString(str, /* isIdentifier */ false);\n    };\n\n    traverseLiteralStrings(M, shouldGenerate, addStringOrIdent);\n\n    if (options.stripFunctionNames) {\n      addString(kStrippedFunctionName);\n    }\n    traverseFunctions(M, shouldGenerate, addString, options.stripFunctionNames);\n\n    if (!M->getCJSModulesResolved()) {\n      traverseCJSModuleNames(M, shouldGenerate, addString);\n    }\n\n    BMGen.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(strings), options.optimizationEnabled));\n  }\n\n  // Add each function to BMGen so that each function has a unique ID.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    unsigned index = BMGen.addFunction(&F);\n    if (&F == entryPoint) {\n      BMGen.setEntryPointIndex(index);\n    }\n\n    auto *cjsModule = M->findCJSModule(&F);\n    if (cjsModule) {\n      if (M->getCJSModulesResolved()) {\n        BMGen.addCJSModuleStatic(cjsModule->id, index);\n      } else {\n        BMGen.addCJSModule(index, BMGen.getStringID(cjsModule->filename.str()));\n      }\n    }\n\n    // Add entries to function source table for non-default source.\n    if (!F.isGlobalScope()) {\n      if (auto source = F.getSourceRepresentationStr()) {\n        BMGen.addFunctionSource(index, BMGen.getStringID(*source));\n      }\n    }\n  }\n  assert(BMGen.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  // Construct the relative function scope depth map.\n  FunctionScopeAnalysis scopeAnalysis{lexicalTopLevel};\n\n  // Allow reusing the debug cache between functions\n  HBCISelDebugCache debugCache;\n\n  // Bytecode generation for each function.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> funcGen;\n\n    if (F.isLazy()) {\n      funcGen = BytecodeFunctionGenerator::create(BMGen, 0);\n    } else {\n      HVMRegisterAllocator RA(&F);\n      if (!options.optimizationEnabled) {\n        RA.setFastPassThreshold(kFastRegisterAllocationThreshold);\n        RA.setMemoryLimit(kRegisterAllocationMemoryLimit);\n      }\n      PostOrderAnalysis PO(&F);\n      /// The order of the blocks is reverse-post-order, which is a simply\n      /// topological sort.\n      llvh::SmallVector<BasicBlock *, 16> order(PO.rbegin(), PO.rend());\n      RA.allocate(order);\n\n      if (options.format == DumpRA) {\n        RA.dump();\n      }\n\n      PassManager PM;\n      PM.addPass(new LowerStoreInstrs(RA));\n      PM.addPass(new LowerCalls(RA));\n      if (options.optimizationEnabled) {\n        PM.addPass(new MovElimination(RA));\n        PM.addPass(new RecreateCheapValues(RA));\n        PM.addPass(new LoadConstantValueNumbering(RA));\n      }\n      PM.addPass(new SpillRegisters(RA));\n      if (options.basicBlockProfiling) {\n        // Insert after all other passes so that it sees final basic block\n        // list.\n        PM.addPass(new InsertProfilePoint());\n      }\n      PM.run(&F);\n\n      if (options.format == DumpLRA)\n        RA.dump();\n\n      if (options.format == DumpPostRA)\n        F.dump();\n\n      funcGen =\n          BytecodeFunctionGenerator::create(BMGen, RA.getMaxRegisterUsage());\n      HBCISel hbciSel(&F, funcGen.get(), RA, scopeAnalysis, options);\n      hbciSel.populateDebugCache(debugCache);\n      hbciSel.generate(sourceMapGen);\n      debugCache = hbciSel.getDebugCache();\n    }\n\n    BMGen.setFunctionGenerator(&F, std::move(funcGen));\n  }\n\n  return BMGen.generate();\n}",
        "func": "std::unique_ptr<BytecodeModule> hbc::generateBytecodeModule(\n    Module *M,\n    Function *lexicalTopLevel,\n    Function *entryPoint,\n    const BytecodeGenerationOptions &options,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGen,\n    std::unique_ptr<BCProviderBase> baseBCProvider) {\n  PerfSection perf(\"Bytecode Generation\");\n  lowerIR(M, options);\n\n  if (options.format == DumpLIR)\n    M->dump();\n\n  BytecodeModuleGenerator BMGen(options);\n\n  if (segment) {\n    BMGen.setSegmentID(*segment);\n  }\n  // Empty if all functions should be generated (i.e. bundle splitting was not\n  // requested).\n  llvh::DenseSet<Function *> functionsToGenerate = segment\n      ? M->getFunctionsInSegment(*segment)\n      : llvh::DenseSet<Function *>{};\n\n  /// \\return true if we should generate function \\p f.\n  std::function<bool(const Function *)> shouldGenerate;\n  if (segment) {\n    shouldGenerate = [entryPoint, &functionsToGenerate](const Function *f) {\n      return f == entryPoint || functionsToGenerate.count(f) > 0;\n    };\n  } else {\n    shouldGenerate = [](const Function *) { return true; };\n  }\n\n  { // Collect all the strings in the bytecode module into a storage.\n    // If we are in delta optimizing mode, start with the string storage from\n    // our base bytecode provider.\n    auto strings = baseBCProvider\n        ? stringAccumulatorFromBCProvider(*baseBCProvider)\n        : UniquingStringLiteralAccumulator{};\n\n    auto addStringOrIdent = [&strings](llvh::StringRef str, bool isIdentifier) {\n      strings.addString(str, isIdentifier);\n    };\n\n    auto addString = [&strings](llvh::StringRef str) {\n      strings.addString(str, /* isIdentifier */ false);\n    };\n\n    traverseLiteralStrings(M, shouldGenerate, addStringOrIdent);\n\n    if (options.stripFunctionNames) {\n      addString(kStrippedFunctionName);\n    }\n    traverseFunctions(M, shouldGenerate, addString, options.stripFunctionNames);\n\n    if (!M->getCJSModulesResolved()) {\n      traverseCJSModuleNames(M, shouldGenerate, addString);\n    }\n\n    BMGen.initializeStringTable(UniquingStringLiteralAccumulator::toTable(\n        std::move(strings), options.optimizationEnabled));\n  }\n\n  // Add each function to BMGen so that each function has a unique ID.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    unsigned index = BMGen.addFunction(&F);\n    if (&F == entryPoint) {\n      BMGen.setEntryPointIndex(index);\n    }\n\n    auto *cjsModule = M->findCJSModule(&F);\n    if (cjsModule) {\n      if (M->getCJSModulesResolved()) {\n        BMGen.addCJSModuleStatic(cjsModule->id, index);\n      } else {\n        BMGen.addCJSModule(index, BMGen.getStringID(cjsModule->filename.str()));\n      }\n    }\n\n    // Add entries to function source table for non-default source.\n    if (!F.isGlobalScope()) {\n      if (auto source = F.getSourceRepresentationStr()) {\n        BMGen.addFunctionSource(index, BMGen.getStringID(*source));\n      }\n    }\n  }\n  assert(BMGen.getEntryPointIndex() != -1 && \"Entry point not added\");\n\n  // Construct the relative function scope depth map.\n  FunctionScopeAnalysis scopeAnalysis{lexicalTopLevel};\n\n  // Allow reusing the debug cache between functions\n  HBCISelDebugCache debugCache;\n\n  // Bytecode generation for each function.\n  for (auto &F : *M) {\n    if (!shouldGenerate(&F)) {\n      continue;\n    }\n\n    std::unique_ptr<BytecodeFunctionGenerator> funcGen;\n\n    if (F.isLazy()) {\n      funcGen = BytecodeFunctionGenerator::create(BMGen, 0);\n    } else {\n      HVMRegisterAllocator RA(&F);\n      if (!options.optimizationEnabled) {\n        RA.setFastPassThreshold(kFastRegisterAllocationThreshold);\n        RA.setMemoryLimit(kRegisterAllocationMemoryLimit);\n      }\n      PostOrderAnalysis PO(&F);\n      /// The order of the blocks is reverse-post-order, which is a simply\n      /// topological sort.\n      llvh::SmallVector<BasicBlock *, 16> order(PO.rbegin(), PO.rend());\n      RA.allocate(order);\n\n      if (options.format == DumpRA) {\n        RA.dump();\n      }\n\n      PassManager PM;\n      PM.addPass(new LowerStoreInstrs(RA));\n      PM.addPass(new LowerCalls(RA));\n      if (options.optimizationEnabled) {\n        PM.addPass(new MovElimination(RA));\n        PM.addPass(new RecreateCheapValues(RA));\n        PM.addPass(new LoadConstantValueNumbering(RA));\n      }\n      PM.addPass(new SpillRegisters(RA));\n      if (options.basicBlockProfiling) {\n        // Insert after all other passes so that it sees final basic block\n        // list.\n        PM.addPass(new InsertProfilePoint());\n      }\n      PM.run(&F);\n\n      if (options.format == DumpLRA)\n        RA.dump();\n\n      if (options.format == DumpPostRA)\n        F.dump();\n\n      funcGen =\n          BytecodeFunctionGenerator::create(BMGen, RA.getMaxRegisterUsage());\n      HBCISel hbciSel(&F, funcGen.get(), RA, scopeAnalysis, options);\n      hbciSel.populateDebugCache(debugCache);\n      hbciSel.generate(sourceMapGen);\n      debugCache = hbciSel.getDebugCache();\n    }\n\n    if (funcGen->hasEncodingError()) {\n      M->getContext().getSourceErrorManager().error(\n          F.getSourceRange().Start, \"Error encoding bytecode\");\n      return nullptr;\n    }\n    BMGen.setFunctionGenerator(&F, std::move(funcGen));\n  }\n\n  return BMGen.generate();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -154,6 +154,11 @@\n       debugCache = hbciSel.getDebugCache();\n     }\n \n+    if (funcGen->hasEncodingError()) {\n+      M->getContext().getSourceErrorManager().error(\n+          F.getSourceRange().Start, \"Error encoding bytecode\");\n+      return nullptr;\n+    }\n     BMGen.setFunctionGenerator(&F, std::move(funcGen));\n   }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (funcGen->hasEncodingError()) {",
                "      M->getContext().getSourceErrorManager().error(",
                "          F.getSourceRange().Start, \"Error encoding bytecode\");",
                "      return nullptr;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/HBCISel::generateHBCResolveEnvironment",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "void HBCISel::generateHBCResolveEnvironment(\n    HBCResolveEnvironment *Inst,\n    BasicBlock *next) {\n  // We statically determine the relative depth delta of the current scope\n  // and the scope that the variable belongs to. Such delta is used as\n  // the operand to get_scope instruction.\n  VariableScope *instScope = Inst->getScope();\n  Optional<int32_t> instScopeDepth = scopeAnalysis_.getScopeDepth(instScope);\n  Optional<int32_t> curScopeDepth =\n      scopeAnalysis_.getScopeDepth(F_->getFunctionScope());\n  if (!instScopeDepth || !curScopeDepth) {\n    // the function did not have any CreateFunctionInst, this function is dead.\n    emitUnreachableIfDebug();\n    return;\n  }\n  assert(\n      curScopeDepth && curScopeDepth.getValue() >= instScopeDepth.getValue() &&\n      \"Cannot access variables in inner scopes\");\n  int32_t delta = curScopeDepth.getValue() - instScopeDepth.getValue();\n  assert(delta > 0 && \"HBCResolveEnvironment for current scope\");\n  BCFGen_->emitGetEnvironment(encodeValue(Inst), delta - 1);\n}",
        "func": "void HBCISel::generateHBCResolveEnvironment(\n    HBCResolveEnvironment *Inst,\n    BasicBlock *next) {\n  // We statically determine the relative depth delta of the current scope\n  // and the scope that the variable belongs to. Such delta is used as\n  // the operand to get_scope instruction.\n  VariableScope *instScope = Inst->getScope();\n  Optional<int32_t> instScopeDepth = scopeAnalysis_.getScopeDepth(instScope);\n  Optional<int32_t> curScopeDepth =\n      scopeAnalysis_.getScopeDepth(F_->getFunctionScope());\n  if (!instScopeDepth || !curScopeDepth) {\n    // the function did not have any CreateFunctionInst, this function is dead.\n    emitUnreachableIfDebug();\n    return;\n  }\n  assert(\n      curScopeDepth && curScopeDepth.getValue() >= instScopeDepth.getValue() &&\n      \"Cannot access variables in inner scopes\");\n  int32_t delta = curScopeDepth.getValue() - instScopeDepth.getValue();\n  assert(delta > 0 && \"HBCResolveEnvironment for current scope\");\n  if (std::numeric_limits<uint8_t>::max() < delta) {\n    F_->getContext().getSourceErrorManager().error(\n        Inst->getLocation(), \"Variable environment is out-of-reach\");\n  }\n  BCFGen_->emitGetEnvironment(encodeValue(Inst), delta - 1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,5 +18,9 @@\n       \"Cannot access variables in inner scopes\");\n   int32_t delta = curScopeDepth.getValue() - instScopeDepth.getValue();\n   assert(delta > 0 && \"HBCResolveEnvironment for current scope\");\n+  if (std::numeric_limits<uint8_t>::max() < delta) {\n+    F_->getContext().getSourceErrorManager().error(\n+        Inst->getLocation(), \"Variable environment is out-of-reach\");\n+  }\n   BCFGen_->emitGetEnvironment(encodeValue(Inst), delta - 1);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (std::numeric_limits<uint8_t>::max() < delta) {",
                "    F_->getContext().getSourceErrorManager().error(",
                "        Inst->getLocation(), \"Variable environment is out-of-reach\");",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/BytecodeModuleGenerator::setFunctionGenerator",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "void BytecodeModuleGenerator::setFunctionGenerator(\n    Function *F,\n    unique_ptr<BytecodeFunctionGenerator> BFG) {\n  assert(\n      functionGenerators_.find(F) == functionGenerators_.end() &&\n      \"Adding same function twice.\");\n  functionGenerators_[F] = std::move(BFG);\n}",
        "func": "void BytecodeModuleGenerator::setFunctionGenerator(\n    Function *F,\n    unique_ptr<BytecodeFunctionGenerator> BFG) {\n  assert(\n      functionGenerators_.find(F) == functionGenerators_.end() &&\n      \"Adding same function twice.\");\n  assert(\n      !BFG->hasEncodingError() && \"Error should have been reported already.\");\n  functionGenerators_[F] = std::move(BFG);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,5 +4,7 @@\n   assert(\n       functionGenerators_.find(F) == functionGenerators_.end() &&\n       \"Adding same function twice.\");\n+  assert(\n+      !BFG->hasEncodingError() && \"Error should have been reported already.\");\n   functionGenerators_[F] = std::move(BFG);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  assert(",
                "      !BFG->hasEncodingError() && \"Error should have been reported already.\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/GeneratorInnerFunction::callInnerFunction",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "CallResult<PseudoHandle<>> GeneratorInnerFunction::callInnerFunction(\n    Handle<GeneratorInnerFunction> selfHandle,\n    Runtime &runtime,\n    Handle<> arg,\n    Action action) {\n  auto self = Handle<GeneratorInnerFunction>::vmcast(selfHandle);\n\n  SmallHermesValue shv =\n      SmallHermesValue::encodeHermesValue(arg.getHermesValue(), runtime);\n  self->result_.set(shv, runtime.getHeap());\n  self->action_ = action;\n\n  auto ctx = runtime.makeMutableHandle(selfHandle->savedContext_);\n  // Account for the `this` argument stored as the first element of ctx.\n  const uint32_t argCount = self->argCount_;\n  // Generators cannot be used as constructors, so newTarget is always\n  // undefined.\n  HermesValue newTarget = HermesValue::encodeUndefinedValue();\n  ScopedNativeCallFrame frame{\n      runtime,\n      argCount, // Account for `this`.\n      selfHandle.getHermesValue(),\n      newTarget,\n      ctx->at(0)};\n  if (LLVM_UNLIKELY(frame.overflowed()))\n    return runtime.raiseStackOverflow(Runtime::StackOverflowKind::NativeStack);\n  for (ArrayStorage::size_type i = 0, e = argCount; i < e; ++i) {\n    frame->getArgRef(i) = ctx->at(i + 1);\n  }\n\n  // Force lazy compilation immediately in order to size the context properly.\n  // We're about to call the function anyway, so this doesn't reduce laziness.\n  // Note that this will do nothing after the very first time a lazy function\n  // is called, so we only resize before we save any registers at all.\n  if (LLVM_UNLIKELY(selfHandle->getCodeBlock(runtime)->isLazy())) {\n    selfHandle->getCodeBlock(runtime)->lazyCompile(runtime);\n    if (LLVM_UNLIKELY(\n            ArrayStorage::resize(\n                ctx,\n                runtime,\n                getContextSize(\n                    selfHandle->getCodeBlock(runtime),\n                    selfHandle->argCount_)) == ExecutionStatus::EXCEPTION)) {\n      return ExecutionStatus::EXCEPTION;\n    }\n    selfHandle->savedContext_.set(runtime, ctx.get(), runtime.getHeap());\n  }\n\n  return JSFunction::_callImpl(selfHandle, runtime);\n}",
        "func": "CallResult<PseudoHandle<>> GeneratorInnerFunction::callInnerFunction(\n    Handle<GeneratorInnerFunction> selfHandle,\n    Runtime &runtime,\n    Handle<> arg,\n    Action action) {\n  auto self = Handle<GeneratorInnerFunction>::vmcast(selfHandle);\n\n  SmallHermesValue shv =\n      SmallHermesValue::encodeHermesValue(arg.getHermesValue(), runtime);\n  self->result_.set(shv, runtime.getHeap());\n  self->action_ = action;\n\n  auto ctx = runtime.makeMutableHandle(selfHandle->savedContext_);\n  // Account for the `this` argument stored as the first element of ctx.\n  const uint32_t argCount = self->argCount_;\n  // Generators cannot be used as constructors, so newTarget is always\n  // undefined.\n  HermesValue newTarget = HermesValue::encodeUndefinedValue();\n  ScopedNativeCallFrame frame{\n      runtime,\n      argCount, // Account for `this`.\n      selfHandle.getHermesValue(),\n      newTarget,\n      ctx->at(0)};\n  if (LLVM_UNLIKELY(frame.overflowed()))\n    return runtime.raiseStackOverflow(Runtime::StackOverflowKind::NativeStack);\n  for (ArrayStorage::size_type i = 0, e = argCount; i < e; ++i) {\n    frame->getArgRef(i) = ctx->at(i + 1);\n  }\n\n  // Force lazy compilation immediately in order to size the context properly.\n  // We're about to call the function anyway, so this doesn't reduce laziness.\n  // Note that this will do nothing after the very first time a lazy function\n  // is called, so we only resize before we save any registers at all.\n  if (LLVM_UNLIKELY(selfHandle->getCodeBlock(runtime)->isLazy())) {\n    if (LLVM_UNLIKELY(\n            selfHandle->getCodeBlock(runtime)->lazyCompile(runtime) ==\n            ExecutionStatus::EXCEPTION)) {\n      return ExecutionStatus::EXCEPTION;\n    }\n    if (LLVM_UNLIKELY(\n            ArrayStorage::resize(\n                ctx,\n                runtime,\n                getContextSize(\n                    selfHandle->getCodeBlock(runtime),\n                    selfHandle->argCount_)) == ExecutionStatus::EXCEPTION)) {\n      return ExecutionStatus::EXCEPTION;\n    }\n    selfHandle->savedContext_.set(runtime, ctx.get(), runtime.getHeap());\n  }\n\n  return JSFunction::_callImpl(selfHandle, runtime);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,7 +33,11 @@\n   // Note that this will do nothing after the very first time a lazy function\n   // is called, so we only resize before we save any registers at all.\n   if (LLVM_UNLIKELY(selfHandle->getCodeBlock(runtime)->isLazy())) {\n-    selfHandle->getCodeBlock(runtime)->lazyCompile(runtime);\n+    if (LLVM_UNLIKELY(\n+            selfHandle->getCodeBlock(runtime)->lazyCompile(runtime) ==\n+            ExecutionStatus::EXCEPTION)) {\n+      return ExecutionStatus::EXCEPTION;\n+    }\n     if (LLVM_UNLIKELY(\n             ArrayStorage::resize(\n                 ctx,",
        "diff_line_info": {
            "deleted_lines": [
                "    selfHandle->getCodeBlock(runtime)->lazyCompile(runtime);"
            ],
            "added_lines": [
                "    if (LLVM_UNLIKELY(",
                "            selfHandle->getCodeBlock(runtime)->lazyCompile(runtime) ==",
                "            ExecutionStatus::EXCEPTION)) {",
                "      return ExecutionStatus::EXCEPTION;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/BCProviderFromSrc::createBCProviderFromSrcImpl",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "std::pair<std::unique_ptr<BCProviderFromSrc>, std::string>\nBCProviderFromSrc::createBCProviderFromSrcImpl(\n    std::unique_ptr<Buffer> buffer,\n    llvh::StringRef sourceURL,\n    std::unique_ptr<SourceMap> sourceMap,\n    const CompileFlags &compileFlags,\n    const ScopeChain &scopeChain,\n    SourceErrorManager::DiagHandlerTy diagHandler,\n    void *diagContext,\n    const std::function<void(Module &)> &runOptimizationPasses) {\n  assert(\n      buffer->data()[buffer->size()] == 0 &&\n      \"The input buffer must be null terminated\");\n\n  CodeGenerationSettings codeGenOpts{};\n  codeGenOpts.unlimitedRegisters = false;\n  codeGenOpts.instrumentIR = compileFlags.instrumentIR;\n\n  OptimizationSettings optSettings;\n  // If the optional value is not set, the parser will automatically detect\n  // the 'use static builtin' directive and we will set it correctly.\n  optSettings.staticBuiltins = compileFlags.staticBuiltins.hasValue()\n      ? compileFlags.staticBuiltins.getValue()\n      : false;\n\n  auto context = std::make_shared<Context>(codeGenOpts, optSettings);\n  std::unique_ptr<SimpleDiagHandlerRAII> outputManager;\n  if (diagHandler) {\n    context->getSourceErrorManager().setDiagHandler(diagHandler, diagContext);\n  } else {\n    outputManager.reset(\n        new SimpleDiagHandlerRAII(context->getSourceErrorManager()));\n  }\n  // If a custom diagHandler was provided, it will receive the details and we\n  // just return the string \"error\" on failure.\n  auto getErrorString = [&outputManager]() {\n    return outputManager ? outputManager->getErrorString()\n                         : std::string(\"error\");\n  };\n\n  // To avoid frequent source buffer rescans, avoid emitting warnings about\n  // undefined variables.\n  context->getSourceErrorManager().setWarningStatus(\n      Warning::UndefinedVariable, false);\n\n  context->setStrictMode(compileFlags.strict);\n  context->setEnableEval(true);\n  context->setPreemptiveFunctionCompilationThreshold(\n      compileFlags.preemptiveFunctionCompilationThreshold);\n  context->setPreemptiveFileCompilationThreshold(\n      compileFlags.preemptiveFileCompilationThreshold);\n\n  if (compileFlags.lazy && !runOptimizationPasses) {\n    context->setLazyCompilation(true);\n  }\n\n  context->setGeneratorEnabled(compileFlags.enableGenerator);\n  context->setDebugInfoSetting(\n      compileFlags.debug ? DebugInfoSetting::ALL : DebugInfoSetting::THROWING);\n  context->setEmitAsyncBreakCheck(compileFlags.emitAsyncBreakCheck);\n\n  // Populate the declFileList.\n  DeclarationFileListTy declFileList;\n  if (compileFlags.includeLibHermes) {\n    auto libBuffer = llvh::MemoryBuffer::getMemBuffer(libhermes);\n    parser::JSParser libParser(*context, std::move(libBuffer));\n    auto libParsed = libParser.parse();\n    assert(libParsed && \"Libhermes failed to parse\");\n    declFileList.push_back(libParsed.getValue());\n  }\n\n  bool isLargeFile =\n      buffer->size() >= context->getPreemptiveFileCompilationThreshold();\n  int fileBufId = context->getSourceErrorManager().addNewSourceBuffer(\n      std::make_unique<HermesLLVMMemoryBuffer>(std::move(buffer), sourceURL));\n  if (sourceMap != nullptr) {\n    auto sourceMapTranslator =\n        std::make_shared<SourceMapTranslator>(context->getSourceErrorManager());\n    context->getSourceErrorManager().setTranslator(sourceMapTranslator);\n    sourceMapTranslator->addSourceMap(fileBufId, std::move(sourceMap));\n  }\n\n  auto parserMode = parser::FullParse;\n  bool useStaticBuiltinDetected = false;\n  if (context->isLazyCompilation() && isLargeFile) {\n    if (!parser::JSParser::preParseBuffer(\n            *context, fileBufId, useStaticBuiltinDetected)) {\n      return {nullptr, getErrorString()};\n    }\n    parserMode = parser::LazyParse;\n  }\n\n  sem::SemContext semCtx{};\n  parser::JSParser parser(*context, fileBufId, parserMode);\n  auto parsed = parser.parse();\n  if (!parsed || !hermes::sem::validateAST(*context, semCtx, *parsed)) {\n    return {nullptr, getErrorString()};\n  }\n  // If we are using lazy parse mode, we should have already detected the 'use\n  // static builtin' directive in the pre-parsing stage.\n  if (parserMode != parser::LazyParse) {\n    useStaticBuiltinDetected = parser.getUseStaticBuiltin();\n  }\n  // The compiler flag is not set, automatically detect 'use static builtin'\n  // from the source.\n  if (!compileFlags.staticBuiltins) {\n    context->setStaticBuiltinOptimization(useStaticBuiltinDetected);\n  }\n\n  Module M(context);\n  hermes::generateIRFromESTree(parsed.getValue(), &M, declFileList, scopeChain);\n  if (context->getSourceErrorManager().getErrorCount() > 0) {\n    return {nullptr, getErrorString()};\n  }\n\n  if (runOptimizationPasses)\n    runOptimizationPasses(M);\n\n  BytecodeGenerationOptions opts{compileFlags.format};\n  opts.optimizationEnabled = !!runOptimizationPasses;\n  opts.staticBuiltinsEnabled =\n      context->getOptimizationSettings().staticBuiltins;\n  opts.verifyIR = compileFlags.verifyIR;\n  auto bytecode = createBCProviderFromSrc(\n      hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts));\n  bytecode->singleFunction_ = isSingleFunctionExpression(parsed.getValue());\n  return {std::move(bytecode), std::string{}};\n}",
        "func": "std::pair<std::unique_ptr<BCProviderFromSrc>, std::string>\nBCProviderFromSrc::createBCProviderFromSrcImpl(\n    std::unique_ptr<Buffer> buffer,\n    llvh::StringRef sourceURL,\n    std::unique_ptr<SourceMap> sourceMap,\n    const CompileFlags &compileFlags,\n    const ScopeChain &scopeChain,\n    SourceErrorManager::DiagHandlerTy diagHandler,\n    void *diagContext,\n    const std::function<void(Module &)> &runOptimizationPasses) {\n  assert(\n      buffer->data()[buffer->size()] == 0 &&\n      \"The input buffer must be null terminated\");\n\n  CodeGenerationSettings codeGenOpts{};\n  codeGenOpts.unlimitedRegisters = false;\n  codeGenOpts.instrumentIR = compileFlags.instrumentIR;\n\n  OptimizationSettings optSettings;\n  // If the optional value is not set, the parser will automatically detect\n  // the 'use static builtin' directive and we will set it correctly.\n  optSettings.staticBuiltins = compileFlags.staticBuiltins.hasValue()\n      ? compileFlags.staticBuiltins.getValue()\n      : false;\n\n  auto context = std::make_shared<Context>(codeGenOpts, optSettings);\n  std::unique_ptr<SimpleDiagHandlerRAII> outputManager;\n  if (diagHandler) {\n    context->getSourceErrorManager().setDiagHandler(diagHandler, diagContext);\n  } else {\n    outputManager.reset(\n        new SimpleDiagHandlerRAII(context->getSourceErrorManager()));\n  }\n  // If a custom diagHandler was provided, it will receive the details and we\n  // just return the string \"error\" on failure.\n  auto getErrorString = [&outputManager]() {\n    return outputManager ? outputManager->getErrorString()\n                         : std::string(\"error\");\n  };\n\n  // To avoid frequent source buffer rescans, avoid emitting warnings about\n  // undefined variables.\n  context->getSourceErrorManager().setWarningStatus(\n      Warning::UndefinedVariable, false);\n\n  context->setStrictMode(compileFlags.strict);\n  context->setEnableEval(true);\n  context->setPreemptiveFunctionCompilationThreshold(\n      compileFlags.preemptiveFunctionCompilationThreshold);\n  context->setPreemptiveFileCompilationThreshold(\n      compileFlags.preemptiveFileCompilationThreshold);\n\n  if (compileFlags.lazy && !runOptimizationPasses) {\n    context->setLazyCompilation(true);\n  }\n\n  context->setGeneratorEnabled(compileFlags.enableGenerator);\n  context->setDebugInfoSetting(\n      compileFlags.debug ? DebugInfoSetting::ALL : DebugInfoSetting::THROWING);\n  context->setEmitAsyncBreakCheck(compileFlags.emitAsyncBreakCheck);\n\n  // Populate the declFileList.\n  DeclarationFileListTy declFileList;\n  if (compileFlags.includeLibHermes) {\n    auto libBuffer = llvh::MemoryBuffer::getMemBuffer(libhermes);\n    parser::JSParser libParser(*context, std::move(libBuffer));\n    auto libParsed = libParser.parse();\n    assert(libParsed && \"Libhermes failed to parse\");\n    declFileList.push_back(libParsed.getValue());\n  }\n\n  bool isLargeFile =\n      buffer->size() >= context->getPreemptiveFileCompilationThreshold();\n  int fileBufId = context->getSourceErrorManager().addNewSourceBuffer(\n      std::make_unique<HermesLLVMMemoryBuffer>(std::move(buffer), sourceURL));\n  if (sourceMap != nullptr) {\n    auto sourceMapTranslator =\n        std::make_shared<SourceMapTranslator>(context->getSourceErrorManager());\n    context->getSourceErrorManager().setTranslator(sourceMapTranslator);\n    sourceMapTranslator->addSourceMap(fileBufId, std::move(sourceMap));\n  }\n\n  auto parserMode = parser::FullParse;\n  bool useStaticBuiltinDetected = false;\n  if (context->isLazyCompilation() && isLargeFile) {\n    if (!parser::JSParser::preParseBuffer(\n            *context, fileBufId, useStaticBuiltinDetected)) {\n      return {nullptr, getErrorString()};\n    }\n    parserMode = parser::LazyParse;\n  }\n\n  sem::SemContext semCtx{};\n  parser::JSParser parser(*context, fileBufId, parserMode);\n  auto parsed = parser.parse();\n  if (!parsed || !hermes::sem::validateAST(*context, semCtx, *parsed)) {\n    return {nullptr, getErrorString()};\n  }\n  // If we are using lazy parse mode, we should have already detected the 'use\n  // static builtin' directive in the pre-parsing stage.\n  if (parserMode != parser::LazyParse) {\n    useStaticBuiltinDetected = parser.getUseStaticBuiltin();\n  }\n  // The compiler flag is not set, automatically detect 'use static builtin'\n  // from the source.\n  if (!compileFlags.staticBuiltins) {\n    context->setStaticBuiltinOptimization(useStaticBuiltinDetected);\n  }\n\n  Module M(context);\n  hermes::generateIRFromESTree(parsed.getValue(), &M, declFileList, scopeChain);\n  if (context->getSourceErrorManager().getErrorCount() > 0) {\n    return {nullptr, getErrorString()};\n  }\n\n  if (runOptimizationPasses)\n    runOptimizationPasses(M);\n\n  BytecodeGenerationOptions opts{compileFlags.format};\n  opts.optimizationEnabled = !!runOptimizationPasses;\n  opts.staticBuiltinsEnabled =\n      context->getOptimizationSettings().staticBuiltins;\n  opts.verifyIR = compileFlags.verifyIR;\n  auto BM = hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts);\n  if (context->getSourceErrorManager().getErrorCount() > 0) {\n    return {nullptr, getErrorString()};\n  }\n  auto bytecode = createBCProviderFromSrc(std::move(BM));\n  bytecode->singleFunction_ = isSingleFunctionExpression(parsed.getValue());\n  return {std::move(bytecode), std::string{}};\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -121,8 +121,11 @@\n   opts.staticBuiltinsEnabled =\n       context->getOptimizationSettings().staticBuiltins;\n   opts.verifyIR = compileFlags.verifyIR;\n-  auto bytecode = createBCProviderFromSrc(\n-      hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts));\n+  auto BM = hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts);\n+  if (context->getSourceErrorManager().getErrorCount() > 0) {\n+    return {nullptr, getErrorString()};\n+  }\n+  auto bytecode = createBCProviderFromSrc(std::move(BM));\n   bytecode->singleFunction_ = isSingleFunctionExpression(parsed.getValue());\n   return {std::move(bytecode), std::string{}};\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  auto bytecode = createBCProviderFromSrc(",
                "      hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts));"
            ],
            "added_lines": [
                "  auto BM = hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), opts);",
                "  if (context->getSourceErrorManager().getErrorCount() > 0) {",
                "    return {nullptr, getErrorString()};",
                "  }",
                "  auto bytecode = createBCProviderFromSrc(std::move(BM));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/lazyCompile",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "void lazyCompile(Runtime &runtime) {\n    if (LLVM_UNLIKELY(isLazy())) {\n      lazyCompileImpl(runtime);\n    }\n  }",
        "func": "ExecutionStatus lazyCompile(Runtime &runtime) {\n    if (LLVM_UNLIKELY(isLazy())) {\n      return lazyCompileImpl(runtime);\n    }\n    return ExecutionStatus::RETURNED;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n-void lazyCompile(Runtime &runtime) {\n+ExecutionStatus lazyCompile(Runtime &runtime) {\n     if (LLVM_UNLIKELY(isLazy())) {\n-      lazyCompileImpl(runtime);\n+      return lazyCompileImpl(runtime);\n     }\n+    return ExecutionStatus::RETURNED;\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "void lazyCompile(Runtime &runtime) {",
                "      lazyCompileImpl(runtime);"
            ],
            "added_lines": [
                "ExecutionStatus lazyCompile(Runtime &runtime) {",
                "      return lazyCompileImpl(runtime);",
                "    return ExecutionStatus::RETURNED;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/CodeBlock::lazyCompileImpl",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "void CodeBlock::lazyCompileImpl(Runtime &runtime) {\n  assert(isLazy() && \"Laziness has not been checked\");\n  PerfSection perf(\"Lazy function compilation\");\n  auto *provider = (hbc::BCProviderLazy *)runtimeModule_->getBytecode();\n  auto *func = provider->getBytecodeFunction();\n  auto *lazyData = func->getLazyCompilationData();\n  auto bcModule = compileLazyFunction(lazyData);\n\n  runtimeModule_->initializeLazyMayAllocate(\n      hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(bcModule)));\n  // Reset all meta lazyData of the CodeBlock to point to the newly\n  // generated bytecode module.\n  functionID_ = runtimeModule_->getBytecode()->getGlobalFunctionIndex();\n  functionHeader_ =\n      runtimeModule_->getBytecode()->getFunctionHeader(functionID_);\n  bytecode_ = runtimeModule_->getBytecode()->getBytecode(functionID_);\n}",
        "func": "ExecutionStatus CodeBlock::lazyCompileImpl(Runtime &runtime) {\n  assert(isLazy() && \"Laziness has not been checked\");\n  PerfSection perf(\"Lazy function compilation\");\n  auto *provider = (hbc::BCProviderLazy *)runtimeModule_->getBytecode();\n  auto *func = provider->getBytecodeFunction();\n  auto *lazyData = func->getLazyCompilationData();\n  SourceErrorManager &manager = lazyData->context->getSourceErrorManager();\n  SimpleDiagHandlerRAII outputManager{manager};\n  auto bcModule = compileLazyFunction(lazyData);\n\n  if (manager.getErrorCount()) {\n    // Raise a SyntaxError to be consistent with eval().\n    return runtime.raiseSyntaxError(\n        llvh::StringRef{outputManager.getErrorString()});\n  }\n\n  assert(bcModule && \"No errors, yet no bcModule\");\n\n  runtimeModule_->initializeLazyMayAllocate(\n      hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(bcModule)));\n  // Reset all meta lazyData of the CodeBlock to point to the newly\n  // generated bytecode module.\n  functionID_ = runtimeModule_->getBytecode()->getGlobalFunctionIndex();\n  functionHeader_ =\n      runtimeModule_->getBytecode()->getFunctionHeader(functionID_);\n  bytecode_ = runtimeModule_->getBytecode()->getBytecode(functionID_);\n\n  return ExecutionStatus::RETURNED;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,10 +1,20 @@\n-void CodeBlock::lazyCompileImpl(Runtime &runtime) {\n+ExecutionStatus CodeBlock::lazyCompileImpl(Runtime &runtime) {\n   assert(isLazy() && \"Laziness has not been checked\");\n   PerfSection perf(\"Lazy function compilation\");\n   auto *provider = (hbc::BCProviderLazy *)runtimeModule_->getBytecode();\n   auto *func = provider->getBytecodeFunction();\n   auto *lazyData = func->getLazyCompilationData();\n+  SourceErrorManager &manager = lazyData->context->getSourceErrorManager();\n+  SimpleDiagHandlerRAII outputManager{manager};\n   auto bcModule = compileLazyFunction(lazyData);\n+\n+  if (manager.getErrorCount()) {\n+    // Raise a SyntaxError to be consistent with eval().\n+    return runtime.raiseSyntaxError(\n+        llvh::StringRef{outputManager.getErrorString()});\n+  }\n+\n+  assert(bcModule && \"No errors, yet no bcModule\");\n \n   runtimeModule_->initializeLazyMayAllocate(\n       hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(bcModule)));\n@@ -14,4 +24,6 @@\n   functionHeader_ =\n       runtimeModule_->getBytecode()->getFunctionHeader(functionID_);\n   bytecode_ = runtimeModule_->getBytecode()->getBytecode(functionID_);\n+\n+  return ExecutionStatus::RETURNED;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "void CodeBlock::lazyCompileImpl(Runtime &runtime) {"
            ],
            "added_lines": [
                "ExecutionStatus CodeBlock::lazyCompileImpl(Runtime &runtime) {",
                "  SourceErrorManager &manager = lazyData->context->getSourceErrorManager();",
                "  SimpleDiagHandlerRAII outputManager{manager};",
                "",
                "  if (manager.getErrorCount()) {",
                "    // Raise a SyntaxError to be consistent with eval().",
                "    return runtime.raiseSyntaxError(",
                "        llvh::StringRef{outputManager.getErrorString()});",
                "  }",
                "",
                "  assert(bcModule && \"No errors, yet no bcModule\");",
                "",
                "  return ExecutionStatus::RETURNED;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/isConstructor",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "CallResult<bool> isConstructor(Runtime &runtime, Callable *callable) {\n  // This is not a complete definition, since ES6 and later define member\n  // functions of objects to not be constructors; however, Hermes does not have\n  // ES6 classes implemented yet, so we cannot check for that case.\n  if (!callable) {\n    return false;\n  }\n\n  // We traverse the BoundFunction target chain to find the eventual target.\n  while (BoundFunction *b = dyn_vmcast<BoundFunction>(callable)) {\n    callable = b->getTarget(runtime);\n  }\n\n  // If it is a bytecode function, check the flags.\n  if (auto *func = dyn_vmcast<JSFunction>(callable)) {\n    auto *cb = func->getCodeBlock(runtime);\n    // Even though it doesn't make sense logically, we need to compile the\n    // function in order to access it flags.\n    cb->lazyCompile(runtime);\n    return !func->getCodeBlock(runtime)->getHeaderFlags().isCallProhibited(\n        true);\n  }\n\n  // We check for NativeFunction since those are defined to not be\n  // constructible, with the exception of NativeConstructor.\n  if (!vmisa<NativeFunction>(callable) || vmisa<NativeConstructor>(callable)) {\n    return true;\n  }\n\n  // JSCallableProxy is a NativeFunction, but may or may not be a\n  // constructor, so we ask it.\n  if (auto *cproxy = dyn_vmcast<JSCallableProxy>(callable)) {\n    return cproxy->isConstructor(runtime);\n  }\n\n  return false;\n}",
        "func": "CallResult<bool> isConstructor(Runtime &runtime, Callable *callable) {\n  // This is not a complete definition, since ES6 and later define member\n  // functions of objects to not be constructors; however, Hermes does not have\n  // ES6 classes implemented yet, so we cannot check for that case.\n  if (!callable) {\n    return false;\n  }\n\n  // We traverse the BoundFunction target chain to find the eventual target.\n  while (BoundFunction *b = dyn_vmcast<BoundFunction>(callable)) {\n    callable = b->getTarget(runtime);\n  }\n\n  // If it is a bytecode function, check the flags.\n  if (auto *func = dyn_vmcast<JSFunction>(callable)) {\n    auto *cb = func->getCodeBlock(runtime);\n    // Even though it doesn't make sense logically, we need to compile the\n    // function in order to access it flags.\n    if (LLVM_UNLIKELY(cb->lazyCompile(runtime) == ExecutionStatus::EXCEPTION)) {\n      return ExecutionStatus::EXCEPTION;\n    }\n    return !func->getCodeBlock(runtime)->getHeaderFlags().isCallProhibited(\n        true);\n  }\n\n  // We check for NativeFunction since those are defined to not be\n  // constructible, with the exception of NativeConstructor.\n  if (!vmisa<NativeFunction>(callable) || vmisa<NativeConstructor>(callable)) {\n    return true;\n  }\n\n  // JSCallableProxy is a NativeFunction, but may or may not be a\n  // constructor, so we ask it.\n  if (auto *cproxy = dyn_vmcast<JSCallableProxy>(callable)) {\n    return cproxy->isConstructor(runtime);\n  }\n\n  return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,7 +16,9 @@\n     auto *cb = func->getCodeBlock(runtime);\n     // Even though it doesn't make sense logically, we need to compile the\n     // function in order to access it flags.\n-    cb->lazyCompile(runtime);\n+    if (LLVM_UNLIKELY(cb->lazyCompile(runtime) == ExecutionStatus::EXCEPTION)) {\n+      return ExecutionStatus::EXCEPTION;\n+    }\n     return !func->getCodeBlock(runtime)->getHeaderFlags().isCallProhibited(\n         true);\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "    cb->lazyCompile(runtime);"
            ],
            "added_lines": [
                "    if (LLVM_UNLIKELY(cb->lazyCompile(runtime) == ExecutionStatus::EXCEPTION)) {",
                "      return ExecutionStatus::EXCEPTION;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/generateBytecodeForSerialization",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "CompileResult generateBytecodeForSerialization(\n    raw_ostream &OS,\n    Module &M,\n    const BytecodeGenerationOptions &genOptions,\n    const SHA1 &sourceHash,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGenOrNull,\n    BaseBytecodeMap &baseBytecodeMap) {\n  // Serialize the bytecode to the file.\n  if (cl::BytecodeFormat == cl::BytecodeFormatKind::HBC) {\n    std::unique_ptr<hbc::BCProviderFromBuffer> baseBCProvider = nullptr;\n    auto itr = baseBytecodeMap.find(segment ? *segment : 0);\n    if (itr != baseBytecodeMap.end()) {\n      baseBCProvider = std::move(itr->second);\n      // We want to erase it from the map because unique_ptr can only\n      // have one owner.\n      baseBytecodeMap.erase(itr);\n    }\n    auto bytecodeModule = hbc::generateBytecode(\n        &M,\n        OS,\n        genOptions,\n        sourceHash,\n        segment,\n        sourceMapGenOrNull,\n        std::move(baseBCProvider));\n\n    if (cl::DumpTarget == DumpBytecode) {\n      disassembleBytecode(hbc::BCProviderFromSrc::createBCProviderFromSrc(\n          std::move(bytecodeModule)));\n    }\n  } else {\n    llvm_unreachable(\"Invalid bytecode kind\");\n  }\n  return Success;\n}",
        "func": "CompileResult generateBytecodeForSerialization(\n    raw_ostream &OS,\n    Module &M,\n    const BytecodeGenerationOptions &genOptions,\n    const SHA1 &sourceHash,\n    hermes::OptValue<uint32_t> segment,\n    SourceMapGenerator *sourceMapGenOrNull,\n    BaseBytecodeMap &baseBytecodeMap) {\n  // Serialize the bytecode to the file.\n  if (cl::BytecodeFormat == cl::BytecodeFormatKind::HBC) {\n    std::unique_ptr<hbc::BCProviderFromBuffer> baseBCProvider = nullptr;\n    auto itr = baseBytecodeMap.find(segment ? *segment : 0);\n    if (itr != baseBytecodeMap.end()) {\n      baseBCProvider = std::move(itr->second);\n      // We want to erase it from the map because unique_ptr can only\n      // have one owner.\n      baseBytecodeMap.erase(itr);\n    }\n    auto bytecodeModule = hbc::generateBytecode(\n        &M,\n        OS,\n        genOptions,\n        sourceHash,\n        segment,\n        sourceMapGenOrNull,\n        std::move(baseBCProvider));\n\n    if (auto N = M.getContext().getSourceErrorManager().getErrorCount()) {\n      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";\n      return BackendError;\n    }\n\n    if (cl::DumpTarget == DumpBytecode) {\n      disassembleBytecode(hbc::BCProviderFromSrc::createBCProviderFromSrc(\n          std::move(bytecodeModule)));\n    }\n  } else {\n    llvm_unreachable(\"Invalid bytecode kind\");\n  }\n  return Success;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,11 @@\n         sourceMapGenOrNull,\n         std::move(baseBCProvider));\n \n+    if (auto N = M.getContext().getSourceErrorManager().getErrorCount()) {\n+      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";\n+      return BackendError;\n+    }\n+\n     if (cl::DumpTarget == DumpBytecode) {\n       disassembleBytecode(hbc::BCProviderFromSrc::createBCProviderFromSrc(\n           std::move(bytecodeModule)));",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (auto N = M.getContext().getSourceErrorManager().getErrorCount()) {",
                "      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";",
                "      return BackendError;",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/generateBytecodeForExecution",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "CompileResult generateBytecodeForExecution(\n    Module &M,\n    const BytecodeGenerationOptions &genOptions) {\n  std::shared_ptr<Context> context = M.shareContext();\n  CompileResult result{Success};\n  if (cl::BytecodeFormat == cl::BytecodeFormatKind::HBC) {\n    result.bytecodeProvider = hbc::BCProviderFromSrc::createBCProviderFromSrc(\n        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions));\n\n  } else {\n    llvm_unreachable(\"Invalid bytecode kind for execution\");\n    result = InvalidFlags;\n  }\n  return result;\n}",
        "func": "CompileResult generateBytecodeForExecution(\n    Module &M,\n    const BytecodeGenerationOptions &genOptions) {\n  std::shared_ptr<Context> context = M.shareContext();\n  CompileResult result{Success};\n  if (cl::BytecodeFormat == cl::BytecodeFormatKind::HBC) {\n    auto BM =\n        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions);\n    if (auto N = context->getSourceErrorManager().getErrorCount()) {\n      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";\n      return BackendError;\n    }\n\n    result.bytecodeProvider =\n        hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(BM));\n  } else {\n    llvm_unreachable(\"Invalid bytecode kind for execution\");\n    result = InvalidFlags;\n  }\n  return result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,9 +4,15 @@\n   std::shared_ptr<Context> context = M.shareContext();\n   CompileResult result{Success};\n   if (cl::BytecodeFormat == cl::BytecodeFormatKind::HBC) {\n-    result.bytecodeProvider = hbc::BCProviderFromSrc::createBCProviderFromSrc(\n-        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions));\n+    auto BM =\n+        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions);\n+    if (auto N = context->getSourceErrorManager().getErrorCount()) {\n+      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";\n+      return BackendError;\n+    }\n \n+    result.bytecodeProvider =\n+        hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(BM));\n   } else {\n     llvm_unreachable(\"Invalid bytecode kind for execution\");\n     result = InvalidFlags;",
        "diff_line_info": {
            "deleted_lines": [
                "    result.bytecodeProvider = hbc::BCProviderFromSrc::createBCProviderFromSrc(",
                "        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions));"
            ],
            "added_lines": [
                "    auto BM =",
                "        hbc::generateBytecodeModule(&M, M.getTopLevelFunction(), genOptions);",
                "    if (auto N = context->getSourceErrorManager().getErrorCount()) {",
                "      llvh::errs() << \"Emitted \" << N << \" errors in the backend. exiting.\\n\";",
                "      return BackendError;",
                "    }",
                "    result.bytecodeProvider =",
                "        hbc::BCProviderFromSrc::createBCProviderFromSrc(std::move(BM));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/Debugger::resolveBreakpointLocation",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "bool Debugger::resolveBreakpointLocation(Breakpoint &breakpoint) const {\n  using fhd::kInvalidLocation;\n  assert(!breakpoint.isResolved() && \"breakpoint already resolved\");\n\n  OptValue<hbc::DebugSearchResult> locationOpt{};\n\n#ifndef HERMESVM_LEAN\n  // If we could have lazy code blocks, compile them before we try to resolve.\n  // Eagerly compile code blocks that may contain the location.\n  // This is done using a search in which we enumerate all CodeBlocks in the\n  // runtime module, and we visit any code blocks which are lazy and check\n  // their ASTs to see if the breakpoint location is in them.\n  // Note that this works because we have the start and end locations\n  // exactly when a CodeBlock is lazy, because that's only when the AST exists.\n  // If it is, we compile the CodeBlock and start over,\n  // skipping any CodeBlocks we've seen before.\n  GCScope gcScope{runtime_};\n  for (auto &runtimeModule : runtime_.getRuntimeModules()) {\n    llvh::DenseSet<CodeBlock *> visited{};\n    std::vector<CodeBlock *> toVisit{};\n    for (uint32_t i = 0, e = runtimeModule.getNumCodeBlocks(); i < e; ++i) {\n      GCScopeMarkerRAII marker{gcScope};\n      // Use getCodeBlock to ensure they get initialized (but not compiled).\n      toVisit.push_back(runtimeModule.getCodeBlockMayAllocate(i));\n    }\n\n    while (!toVisit.empty()) {\n      GCScopeMarkerRAII marker{gcScope};\n      CodeBlock *codeBlock = toVisit.back();\n      toVisit.pop_back();\n\n      if (!codeBlock || !codeBlock->isLazy()) {\n        // When looking for a lazy code block to expand,\n        // there's no point looking at the non-lazy ones.\n        continue;\n      }\n\n      if (visited.count(codeBlock) > 0) {\n        // We've already been here.\n        continue;\n      }\n\n      visited.insert(codeBlock);\n      auto start = codeBlock->getLazyFunctionStartLoc();\n      auto end = codeBlock->getLazyFunctionEndLoc();\n\n      const auto &request = breakpoint.requestedLocation;\n      if ((start.line < request.line && request.line < end.line) ||\n          ((start.line == request.line || request.line == end.line) &&\n           (start.col <= request.column && request.column <= end.col))) {\n        // The code block probably contains the breakpoint we want to set.\n        // First, we compile it.\n        codeBlock->lazyCompile(runtime_);\n\n        // We've found the codeBlock at this level and expanded it,\n        // so there's no point continuing the search.\n        // Abandon the current toVisit queue and repopulate it.\n        toVisit.clear();\n\n        // Compiling the function will add more functions to the runtimeModule.\n        // Re-add them all so we can continue the search.\n        for (uint32_t i = 0, e = runtimeModule.getNumCodeBlocks(); i < e; ++i) {\n          GCScopeMarkerRAII marker2{gcScope};\n          // Use getCodeBlock to ensure they get initialized (but not compiled).\n          toVisit.push_back(runtimeModule.getCodeBlockMayAllocate(i));\n        }\n      }\n    }\n  }\n#endif\n\n  // Iterate backwards through runtime modules, under the assumption that\n  // modules at the end of the list were added more recently, and are more\n  // likely to match the user's intention.\n  // Specifically, this will check any user source before runtime modules loaded\n  // by the VM.\n  for (auto it = runtime_.getRuntimeModules().rbegin();\n       it != runtime_.getRuntimeModules().rend();\n       ++it) {\n    auto &runtimeModule = *it;\n    GCScope gcScope{runtime_};\n\n    if (!runtimeModule.isInitialized()) {\n      // Uninitialized module.\n      continue;\n    }\n    if (!runtimeModule.getBytecode()->getDebugInfo()) {\n      // No debug info in this module, keep going.\n      continue;\n    }\n\n    const auto *debugInfo = runtimeModule.getBytecode()->getDebugInfo();\n    const auto &fileRegions = debugInfo->viewFiles();\n    if (fileRegions.empty()) {\n      continue;\n    }\n\n    uint32_t resolvedFileId = kInvalidLocation;\n    std::string resolvedFileName{};\n\n    if (!breakpoint.requestedLocation.fileName.empty()) {\n      for (const auto &region : fileRegions) {\n        std::string storage =\n            getFileNameAsUTF8(runtime_, &runtimeModule, region.filenameId);\n        llvh::StringRef storageRef{storage};\n        if (storageRef.consume_back(breakpoint.requestedLocation.fileName)) {\n          resolvedFileId = region.filenameId;\n          resolvedFileName = std::move(storage);\n          break;\n        }\n      }\n    } else if (breakpoint.requestedLocation.fileId != kInvalidLocation) {\n      for (const auto &region : fileRegions) {\n        // We don't yet have a convincing story for debugging CommonJS, so for\n        // now just assert that we're still living in the one-file-per-RM world.\n        // TODO(T84976604): Properly handle setting breakpoints when there are\n        // multiple JS files per HBC file.\n        assert(\n            region.filenameId == 0 && \"Unexpected multiple filenames per RM\");\n        if (resolveScriptId(&runtimeModule, region.filenameId) ==\n            breakpoint.requestedLocation.fileId) {\n          resolvedFileId = region.filenameId;\n          resolvedFileName =\n              getFileNameAsUTF8(runtime_, &runtimeModule, resolvedFileId);\n          break;\n        }\n      }\n    } else {\n      // No requested file, just pick the first one.\n      resolvedFileId = fileRegions.front().filenameId;\n      resolvedFileName =\n          getFileNameAsUTF8(runtime_, &runtimeModule, resolvedFileId);\n    }\n\n    if (resolvedFileId == kInvalidLocation) {\n      // Unable to find the file here.\n      continue;\n    }\n\n    locationOpt = debugInfo->getAddressForLocation(\n        resolvedFileId,\n        breakpoint.requestedLocation.line,\n        breakpoint.requestedLocation.column == kInvalidLocation\n            ? llvh::None\n            : OptValue<uint32_t>{breakpoint.requestedLocation.column});\n\n    if (locationOpt.hasValue()) {\n      breakpoint.codeBlock =\n          runtimeModule.getCodeBlockMayAllocate(locationOpt->functionIndex);\n      breakpoint.offset = locationOpt->bytecodeOffset;\n\n      SourceLocation resolvedLocation;\n      resolvedLocation.line = locationOpt->line;\n      resolvedLocation.column = locationOpt->column;\n      resolvedLocation.fileId = resolveScriptId(&runtimeModule, resolvedFileId);\n      resolvedLocation.fileName = std::move(resolvedFileName);\n      breakpoint.resolvedLocation = resolvedLocation;\n      return true;\n    }\n  }\n\n  return false;\n}",
        "func": "bool Debugger::resolveBreakpointLocation(Breakpoint &breakpoint) const {\n  using fhd::kInvalidLocation;\n  assert(!breakpoint.isResolved() && \"breakpoint already resolved\");\n\n  OptValue<hbc::DebugSearchResult> locationOpt{};\n\n#ifndef HERMESVM_LEAN\n  // If we could have lazy code blocks, compile them before we try to resolve.\n  // Eagerly compile code blocks that may contain the location.\n  // This is done using a search in which we enumerate all CodeBlocks in the\n  // runtime module, and we visit any code blocks which are lazy and check\n  // their ASTs to see if the breakpoint location is in them.\n  // Note that this works because we have the start and end locations\n  // exactly when a CodeBlock is lazy, because that's only when the AST exists.\n  // If it is, we compile the CodeBlock and start over,\n  // skipping any CodeBlocks we've seen before.\n  GCScope gcScope{runtime_};\n  for (auto &runtimeModule : runtime_.getRuntimeModules()) {\n    llvh::DenseSet<CodeBlock *> visited{};\n    std::vector<CodeBlock *> toVisit{};\n    for (uint32_t i = 0, e = runtimeModule.getNumCodeBlocks(); i < e; ++i) {\n      GCScopeMarkerRAII marker{gcScope};\n      // Use getCodeBlock to ensure they get initialized (but not compiled).\n      toVisit.push_back(runtimeModule.getCodeBlockMayAllocate(i));\n    }\n\n    while (!toVisit.empty()) {\n      GCScopeMarkerRAII marker{gcScope};\n      CodeBlock *codeBlock = toVisit.back();\n      toVisit.pop_back();\n\n      if (!codeBlock || !codeBlock->isLazy()) {\n        // When looking for a lazy code block to expand,\n        // there's no point looking at the non-lazy ones.\n        continue;\n      }\n\n      if (visited.count(codeBlock) > 0) {\n        // We've already been here.\n        continue;\n      }\n\n      visited.insert(codeBlock);\n      auto start = codeBlock->getLazyFunctionStartLoc();\n      auto end = codeBlock->getLazyFunctionEndLoc();\n\n      const auto &request = breakpoint.requestedLocation;\n      if ((start.line < request.line && request.line < end.line) ||\n          ((start.line == request.line || request.line == end.line) &&\n           (start.col <= request.column && request.column <= end.col))) {\n        // The code block probably contains the breakpoint we want to set.\n        // First, we compile it.\n        if (LLVM_UNLIKELY(\n                codeBlock->lazyCompile(runtime_) ==\n                ExecutionStatus::EXCEPTION)) {\n          // TODO: how to better handle this?\n          runtime_.clearThrownValue();\n        }\n\n        // We've found the codeBlock at this level and expanded it,\n        // so there's no point continuing the search.\n        // Abandon the current toVisit queue and repopulate it.\n        toVisit.clear();\n\n        // Compiling the function will add more functions to the runtimeModule.\n        // Re-add them all so we can continue the search.\n        for (uint32_t i = 0, e = runtimeModule.getNumCodeBlocks(); i < e; ++i) {\n          GCScopeMarkerRAII marker2{gcScope};\n          // Use getCodeBlock to ensure they get initialized (but not compiled).\n          toVisit.push_back(runtimeModule.getCodeBlockMayAllocate(i));\n        }\n      }\n    }\n  }\n#endif\n\n  // Iterate backwards through runtime modules, under the assumption that\n  // modules at the end of the list were added more recently, and are more\n  // likely to match the user's intention.\n  // Specifically, this will check any user source before runtime modules loaded\n  // by the VM.\n  for (auto it = runtime_.getRuntimeModules().rbegin();\n       it != runtime_.getRuntimeModules().rend();\n       ++it) {\n    auto &runtimeModule = *it;\n    GCScope gcScope{runtime_};\n\n    if (!runtimeModule.isInitialized()) {\n      // Uninitialized module.\n      continue;\n    }\n    if (!runtimeModule.getBytecode()->getDebugInfo()) {\n      // No debug info in this module, keep going.\n      continue;\n    }\n\n    const auto *debugInfo = runtimeModule.getBytecode()->getDebugInfo();\n    const auto &fileRegions = debugInfo->viewFiles();\n    if (fileRegions.empty()) {\n      continue;\n    }\n\n    uint32_t resolvedFileId = kInvalidLocation;\n    std::string resolvedFileName{};\n\n    if (!breakpoint.requestedLocation.fileName.empty()) {\n      for (const auto &region : fileRegions) {\n        std::string storage =\n            getFileNameAsUTF8(runtime_, &runtimeModule, region.filenameId);\n        llvh::StringRef storageRef{storage};\n        if (storageRef.consume_back(breakpoint.requestedLocation.fileName)) {\n          resolvedFileId = region.filenameId;\n          resolvedFileName = std::move(storage);\n          break;\n        }\n      }\n    } else if (breakpoint.requestedLocation.fileId != kInvalidLocation) {\n      for (const auto &region : fileRegions) {\n        // We don't yet have a convincing story for debugging CommonJS, so for\n        // now just assert that we're still living in the one-file-per-RM world.\n        // TODO(T84976604): Properly handle setting breakpoints when there are\n        // multiple JS files per HBC file.\n        assert(\n            region.filenameId == 0 && \"Unexpected multiple filenames per RM\");\n        if (resolveScriptId(&runtimeModule, region.filenameId) ==\n            breakpoint.requestedLocation.fileId) {\n          resolvedFileId = region.filenameId;\n          resolvedFileName =\n              getFileNameAsUTF8(runtime_, &runtimeModule, resolvedFileId);\n          break;\n        }\n      }\n    } else {\n      // No requested file, just pick the first one.\n      resolvedFileId = fileRegions.front().filenameId;\n      resolvedFileName =\n          getFileNameAsUTF8(runtime_, &runtimeModule, resolvedFileId);\n    }\n\n    if (resolvedFileId == kInvalidLocation) {\n      // Unable to find the file here.\n      continue;\n    }\n\n    locationOpt = debugInfo->getAddressForLocation(\n        resolvedFileId,\n        breakpoint.requestedLocation.line,\n        breakpoint.requestedLocation.column == kInvalidLocation\n            ? llvh::None\n            : OptValue<uint32_t>{breakpoint.requestedLocation.column});\n\n    if (locationOpt.hasValue()) {\n      breakpoint.codeBlock =\n          runtimeModule.getCodeBlockMayAllocate(locationOpt->functionIndex);\n      breakpoint.offset = locationOpt->bytecodeOffset;\n\n      SourceLocation resolvedLocation;\n      resolvedLocation.line = locationOpt->line;\n      resolvedLocation.column = locationOpt->column;\n      resolvedLocation.fileId = resolveScriptId(&runtimeModule, resolvedFileId);\n      resolvedLocation.fileName = std::move(resolvedFileName);\n      breakpoint.resolvedLocation = resolvedLocation;\n      return true;\n    }\n  }\n\n  return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,7 +50,12 @@\n            (start.col <= request.column && request.column <= end.col))) {\n         // The code block probably contains the breakpoint we want to set.\n         // First, we compile it.\n-        codeBlock->lazyCompile(runtime_);\n+        if (LLVM_UNLIKELY(\n+                codeBlock->lazyCompile(runtime_) ==\n+                ExecutionStatus::EXCEPTION)) {\n+          // TODO: how to better handle this?\n+          runtime_.clearThrownValue();\n+        }\n \n         // We've found the codeBlock at this level and expanded it,\n         // so there's no point continuing the search.",
        "diff_line_info": {
            "deleted_lines": [
                "        codeBlock->lazyCompile(runtime_);"
            ],
            "added_lines": [
                "        if (LLVM_UNLIKELY(",
                "                codeBlock->lazyCompile(runtime_) ==",
                "                ExecutionStatus::EXCEPTION)) {",
                "          // TODO: how to better handle this?",
                "          runtime_.clearThrownValue();",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40138",
        "func_name": "facebook/hermes/Runtime::interpretFunctionImpl",
        "description": "An integer conversion error in Hermes bytecode generation, prior to commit 6aa825e480d48127b480b08d13adf70033237097, could have been used to perform Out-Of-Bounds operations and subsequently execute arbitrary code. Note that this is only exploitable in cases where Hermes is used to execute untrusted JavaScript. Hence, most React Native applications are not affected.",
        "git_url": "https://github.com/facebook/hermes/commit/6aa825e480d48127b480b08d13adf70033237097",
        "commit_title": "Re-sync with internal repository (#822)",
        "commit_text": " Co-authored-by: Facebook Community Bot <6422482+facebook-github-bot@users.noreply.github.com>",
        "func_before": "CallResult<HermesValue> Runtime::interpretFunctionImpl(\n    CodeBlock *newCodeBlock) {\n  newCodeBlock->lazyCompile(*this);\n\n#if defined(HERMES_MEMORY_INSTRUMENTATION) || !defined(NDEBUG)\n  // We always call getCurrentIP() in a debug build as this has the effect\n  // of asserting the IP is correctly set (not invalidated) at this point.\n  // This allows us to leverage our whole test-suite to find missing cases\n  // of CAPTURE_IP* macros in the interpreter loop.\n  const inst::Inst *ip = getCurrentIP();\n  (void)ip;\n#endif\n#ifdef HERMES_MEMORY_INSTRUMENTATION\n  if (ip) {\n    const CodeBlock *codeBlock;\n    std::tie(codeBlock, ip) = getCurrentInterpreterLocation(ip);\n    // All functions end in a Ret so we must match this with a pushCallStack()\n    // before executing.\n    if (codeBlock) {\n      // Push a call entry at the last location we were executing bytecode.\n      // This will correctly attribute things like eval().\n      pushCallStack(codeBlock, ip);\n    } else {\n      // Push a call entry at the entry at the top of interpreted code.\n      pushCallStack(newCodeBlock, (const Inst *)newCodeBlock->begin());\n    }\n  } else {\n    // Push a call entry at the entry at the top of interpreted code.\n    pushCallStack(newCodeBlock, (const Inst *)newCodeBlock->begin());\n  }\n#endif\n\n  InterpreterState state{newCodeBlock, 0};\n  if (HERMESVM_CRASH_TRACE &&\n      (getVMExperimentFlags() & experiments::CrashTrace)) {\n    return Interpreter::interpretFunction<false, true>(*this, state);\n  } else {\n    return Interpreter::interpretFunction<false, false>(*this, state);\n  }\n}",
        "func": "CallResult<HermesValue> Runtime::interpretFunctionImpl(\n    CodeBlock *newCodeBlock) {\n  if (LLVM_UNLIKELY(\n          newCodeBlock->lazyCompile(*this) == ExecutionStatus::EXCEPTION)) {\n    return ExecutionStatus::EXCEPTION;\n  }\n\n#if defined(HERMES_MEMORY_INSTRUMENTATION) || !defined(NDEBUG)\n  // We always call getCurrentIP() in a debug build as this has the effect\n  // of asserting the IP is correctly set (not invalidated) at this point.\n  // This allows us to leverage our whole test-suite to find missing cases\n  // of CAPTURE_IP* macros in the interpreter loop.\n  const inst::Inst *ip = getCurrentIP();\n  (void)ip;\n#endif\n#ifdef HERMES_MEMORY_INSTRUMENTATION\n  if (ip) {\n    const CodeBlock *codeBlock;\n    std::tie(codeBlock, ip) = getCurrentInterpreterLocation(ip);\n    // All functions end in a Ret so we must match this with a pushCallStack()\n    // before executing.\n    if (codeBlock) {\n      // Push a call entry at the last location we were executing bytecode.\n      // This will correctly attribute things like eval().\n      pushCallStack(codeBlock, ip);\n    } else {\n      // Push a call entry at the entry at the top of interpreted code.\n      pushCallStack(newCodeBlock, (const Inst *)newCodeBlock->begin());\n    }\n  } else {\n    // Push a call entry at the entry at the top of interpreted code.\n    pushCallStack(newCodeBlock, (const Inst *)newCodeBlock->begin());\n  }\n#endif\n\n  InterpreterState state{newCodeBlock, 0};\n  if (HERMESVM_CRASH_TRACE &&\n      (getVMExperimentFlags() & experiments::CrashTrace)) {\n    return Interpreter::interpretFunction<false, true>(*this, state);\n  } else {\n    return Interpreter::interpretFunction<false, false>(*this, state);\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,9 @@\n CallResult<HermesValue> Runtime::interpretFunctionImpl(\n     CodeBlock *newCodeBlock) {\n-  newCodeBlock->lazyCompile(*this);\n+  if (LLVM_UNLIKELY(\n+          newCodeBlock->lazyCompile(*this) == ExecutionStatus::EXCEPTION)) {\n+    return ExecutionStatus::EXCEPTION;\n+  }\n \n #if defined(HERMES_MEMORY_INSTRUMENTATION) || !defined(NDEBUG)\n   // We always call getCurrentIP() in a debug build as this has the effect",
        "diff_line_info": {
            "deleted_lines": [
                "  newCodeBlock->lazyCompile(*this);"
            ],
            "added_lines": [
                "  if (LLVM_UNLIKELY(",
                "          newCodeBlock->lazyCompile(*this) == ExecutionStatus::EXCEPTION)) {",
                "    return ExecutionStatus::EXCEPTION;",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2016-3074",
        "func_name": "libgd/_gd2GetHeader",
        "description": "Integer signedness error in GD Graphics Library 2.1.1 (aka libgd or libgd2) allows remote attackers to cause a denial of service (crash) or potentially execute arbitrary code via crafted compressed gd2 data, which triggers a heap-based buffer overflow.",
        "git_url": "https://github.com/libgd/libgd/commit/2bb97f407c1145c850416a3bfbcc8cf124e68a19",
        "commit_title": "gd2: handle corrupt images better (CVE-2016-3074)",
        "commit_text": " Make sure we do some range checking on corrupted chunks.  Thanks to Hans Jerry Illikainen <hji@dyntopia.com> for indepth report and reproducer information.  Made for easy test case writing :).",
        "func_before": "static int\n_gd2GetHeader (gdIOCtxPtr in, int *sx, int *sy,\n               int *cs, int *vers, int *fmt, int *ncx, int *ncy,\n               t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG (printf (\"Reading gd2 header info\\n\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC (in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t};\n\t\tid[i] = ch;\n\t};\n\tid[4] = 0;\n\n\tGD2_DBG (printf (\"Got file code: %s\\n\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp (id, GD2_ID) != 0) {\n\t\tGD2_DBG (printf (\"Not a valid gd2 file\\n\"));\n\t\tgoto fail1;\n\t};\n\n\t/* Version */\n\tif (gdGetWord (vers, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"Version: %d\\n\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG (printf (\"Bad version: %d\\n\", *vers));\n\t\tgoto fail1;\n\t};\n\n\t/* Image Size */\n\tif (!gdGetWord (sx, in)) {\n\t\tGD2_DBG (printf (\"Could not get x-size\\n\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord (sy, in)) {\n\t\tGD2_DBG (printf (\"Could not get y-size\\n\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG (printf (\"Image is %dx%d\\n\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord (cs, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"ChunkSize: %d\\n\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG (printf (\"Bad chunk size: %d\\n\", *cs));\n\t\tgoto fail1;\n\t};\n\n\t/* Data Format */\n\tif (gdGetWord (fmt, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"Format: %d\\n\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) &&\n\t        (*fmt != GD2_FMT_TRUECOLOR_RAW) &&\n\t        (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG (printf (\"Bad data format: %d\\n\", *fmt));\n\t\tgoto fail1;\n\t};\n\n\n\t/* # of chunks wide */\n\tif (gdGetWord (ncx, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"%d Chunks Wide\\n\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord (ncy, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"%d Chunks vertically\\n\", *ncy));\n\n\tif (gd2_compressed (*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG (printf (\"Reading %d chunk index entries\\n\", nc));\n\t\tsidx = sizeof (t_chunk_info) * nc;\n\t\tcidx = gdCalloc (sidx, 1);\n\t\tif (!cidx) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt (&cidx[i].offset, in) != 1) {\n\t\t\t\tgoto fail2;\n\t\t\t};\n\t\t\tif (gdGetInt (&cidx[i].size, in) != 1) {\n\t\t\t\tgoto fail2;\n\t\t\t};\n\t\t};\n\t\t*chunkIdx = cidx;\n\t};\n\n\tGD2_DBG (printf (\"gd2 header complete\\n\"));\n\n\treturn 1;\nfail2:\n\tgdFree(cidx);\nfail1:\n\treturn 0;\n}",
        "func": "static int\n_gd2GetHeader (gdIOCtxPtr in, int *sx, int *sy,\n               int *cs, int *vers, int *fmt, int *ncx, int *ncy,\n               t_chunk_info ** chunkIdx)\n{\n\tint i;\n\tint ch;\n\tchar id[5];\n\tt_chunk_info *cidx;\n\tint sidx;\n\tint nc;\n\n\tGD2_DBG (printf (\"Reading gd2 header info\\n\"));\n\n\tfor (i = 0; i < 4; i++) {\n\t\tch = gdGetC (in);\n\t\tif (ch == EOF) {\n\t\t\tgoto fail1;\n\t\t};\n\t\tid[i] = ch;\n\t};\n\tid[4] = 0;\n\n\tGD2_DBG (printf (\"Got file code: %s\\n\", id));\n\n\t/* Equiv. of 'magick'.  */\n\tif (strcmp (id, GD2_ID) != 0) {\n\t\tGD2_DBG (printf (\"Not a valid gd2 file\\n\"));\n\t\tgoto fail1;\n\t};\n\n\t/* Version */\n\tif (gdGetWord (vers, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"Version: %d\\n\", *vers));\n\n\tif ((*vers != 1) && (*vers != 2)) {\n\t\tGD2_DBG (printf (\"Bad version: %d\\n\", *vers));\n\t\tgoto fail1;\n\t};\n\n\t/* Image Size */\n\tif (!gdGetWord (sx, in)) {\n\t\tGD2_DBG (printf (\"Could not get x-size\\n\"));\n\t\tgoto fail1;\n\t}\n\tif (!gdGetWord (sy, in)) {\n\t\tGD2_DBG (printf (\"Could not get y-size\\n\"));\n\t\tgoto fail1;\n\t}\n\tGD2_DBG (printf (\"Image is %dx%d\\n\", *sx, *sy));\n\n\t/* Chunk Size (pixels, not bytes!) */\n\tif (gdGetWord (cs, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"ChunkSize: %d\\n\", *cs));\n\n\tif ((*cs < GD2_CHUNKSIZE_MIN) || (*cs > GD2_CHUNKSIZE_MAX)) {\n\t\tGD2_DBG (printf (\"Bad chunk size: %d\\n\", *cs));\n\t\tgoto fail1;\n\t};\n\n\t/* Data Format */\n\tif (gdGetWord (fmt, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"Format: %d\\n\", *fmt));\n\n\tif ((*fmt != GD2_FMT_RAW) && (*fmt != GD2_FMT_COMPRESSED) &&\n\t        (*fmt != GD2_FMT_TRUECOLOR_RAW) &&\n\t        (*fmt != GD2_FMT_TRUECOLOR_COMPRESSED)) {\n\t\tGD2_DBG (printf (\"Bad data format: %d\\n\", *fmt));\n\t\tgoto fail1;\n\t};\n\n\n\t/* # of chunks wide */\n\tif (gdGetWord (ncx, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"%d Chunks Wide\\n\", *ncx));\n\n\t/* # of chunks high */\n\tif (gdGetWord (ncy, in) != 1) {\n\t\tgoto fail1;\n\t};\n\tGD2_DBG (printf (\"%d Chunks vertically\\n\", *ncy));\n\n\tif (gd2_compressed (*fmt)) {\n\t\tnc = (*ncx) * (*ncy);\n\t\tGD2_DBG (printf (\"Reading %d chunk index entries\\n\", nc));\n\t\tsidx = sizeof (t_chunk_info) * nc;\n\t\tcidx = gdCalloc (sidx, 1);\n\t\tif (!cidx) {\n\t\t\tgoto fail1;\n\t\t}\n\t\tfor (i = 0; i < nc; i++) {\n\t\t\tif (gdGetInt (&cidx[i].offset, in) != 1) {\n\t\t\t\tgoto fail2;\n\t\t\t};\n\t\t\tif (gdGetInt (&cidx[i].size, in) != 1) {\n\t\t\t\tgoto fail2;\n\t\t\t};\n\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0)\n\t\t\t\tgoto fail2;\n\t\t};\n\t\t*chunkIdx = cidx;\n\t};\n\n\tGD2_DBG (printf (\"gd2 header complete\\n\"));\n\n\treturn 1;\nfail2:\n\tgdFree(cidx);\nfail1:\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -103,6 +103,8 @@\n \t\t\tif (gdGetInt (&cidx[i].size, in) != 1) {\n \t\t\t\tgoto fail2;\n \t\t\t};\n+\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0)\n+\t\t\t\tgoto fail2;\n \t\t};\n \t\t*chunkIdx = cidx;\n \t};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tif (cidx[i].offset < 0 || cidx[i].size < 0)",
                "\t\t\t\tgoto fail2;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-7310",
        "func_name": "poppler/XRef::getEntry",
        "description": "In Poppler 0.73.0, a heap-based buffer over-read (due to an integer signedness error in the XRef::getEntry function in XRef.cc) allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact via a crafted PDF document, as demonstrated by pdftocairo.",
        "git_url": "https://cgit.freedesktop.org/poppler/poppler/commit/?id=b54e1fc3e0d2600621a28d50f9f085b9e38619c2",
        "commit_title": "",
        "commit_text": "",
        "func_before": "XRefEntry *XRef::getEntry(int i, bool complainIfMissing)\n{\n  if (i >= size || entries[i].type == xrefEntryNone) {\n\n    if ((!xRefStream) && mainXRefEntriesOffset) {\n      if (unlikely(i >= capacity)) {\n\terror(errInternal, -1, \"Request for out-of-bounds XRef entry [{0:d}]\", i);\n\treturn &dummyXRefEntry;\n      }\n\n      if (!parseEntry(mainXRefEntriesOffset + 20*i, &entries[i])) {\n        error(errSyntaxError, -1, \"Failed to parse XRef entry [{0:d}].\", i);\n      }\n    } else {\n      // Read XRef tables until the entry we're looking for is found\n      readXRefUntil(i);\n      \n      // We might have reconstructed the xref\n      // Check again i is in bounds\n      if (unlikely(i >= size)) {\n\treturn &dummyXRefEntry;\n      }\n\n      if (entries[i].type == xrefEntryNone) {\n        if (complainIfMissing) {\n          error(errSyntaxError, -1, \"Invalid XRef entry {0:d}\", i);\n        }\n        entries[i].type = xrefEntryFree;\n      }\n    }\n  }\n\n  return &entries[i];\n}",
        "func": "XRefEntry *XRef::getEntry(int i, bool complainIfMissing)\n{\n  if (unlikely(i < 0)) {\n    error(errInternal, -1, \"Request for invalid XRef entry [{0:d}]\", i);\n    return &dummyXRefEntry;\n  }\n\n  if (i >= size || entries[i].type == xrefEntryNone) {\n\n    if ((!xRefStream) && mainXRefEntriesOffset) {\n      if (unlikely(i >= capacity)) {\n\terror(errInternal, -1, \"Request for out-of-bounds XRef entry [{0:d}]\", i);\n\treturn &dummyXRefEntry;\n      }\n\n      if (!parseEntry(mainXRefEntriesOffset + 20*i, &entries[i])) {\n        error(errSyntaxError, -1, \"Failed to parse XRef entry [{0:d}].\", i);\n      }\n    } else {\n      // Read XRef tables until the entry we're looking for is found\n      readXRefUntil(i);\n      \n      // We might have reconstructed the xref\n      // Check again i is in bounds\n      if (unlikely(i >= size)) {\n\treturn &dummyXRefEntry;\n      }\n\n      if (entries[i].type == xrefEntryNone) {\n        if (complainIfMissing) {\n          error(errSyntaxError, -1, \"Invalid XRef entry {0:d}\", i);\n        }\n        entries[i].type = xrefEntryFree;\n      }\n    }\n  }\n\n  return &entries[i];\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,10 @@\n XRefEntry *XRef::getEntry(int i, bool complainIfMissing)\n {\n+  if (unlikely(i < 0)) {\n+    error(errInternal, -1, \"Request for invalid XRef entry [{0:d}]\", i);\n+    return &dummyXRefEntry;\n+  }\n+\n   if (i >= size || entries[i].type == xrefEntryNone) {\n \n     if ((!xRefStream) && mainXRefEntriesOffset) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (unlikely(i < 0)) {",
                "    error(errInternal, -1, \"Request for invalid XRef entry [{0:d}]\", i);",
                "    return &dummyXRefEntry;",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/mqtt_packet_drop",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "static inline int mqtt_packet_drop(struct mqtt_conn *conn)\n{\n    int move_bytes;\n\n    move_bytes = conn->buf_pos + 1;\n    memmove(conn->buf,\n            conn->buf + move_bytes,\n            conn->buf_len - move_bytes);\n\n    conn->buf_frame_end = 0;\n    conn->buf_len -= move_bytes;\n    conn->buf_pos  = 0;\n\n    return 0;\n}",
        "func": "static inline int mqtt_packet_drop(struct mqtt_conn *conn)\n{\n    int move_bytes;\n\n    if (conn->buf_pos == conn->buf_len) {\n        conn->buf_frame_end = 0;\n        conn->buf_len = 0;\n        conn->buf_pos = 0;\n        return 0;\n    }\n\n    move_bytes = conn->buf_pos + 1;\n    memmove(conn->buf,\n            conn->buf + move_bytes,\n            conn->buf_len - move_bytes);\n\n    conn->buf_frame_end = 0;\n    conn->buf_len -= move_bytes;\n    conn->buf_pos  = 0;\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,13 @@\n static inline int mqtt_packet_drop(struct mqtt_conn *conn)\n {\n     int move_bytes;\n+\n+    if (conn->buf_pos == conn->buf_len) {\n+        conn->buf_frame_end = 0;\n+        conn->buf_len = 0;\n+        conn->buf_pos = 0;\n+        return 0;\n+    }\n \n     move_bytes = conn->buf_pos + 1;\n     memmove(conn->buf,",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "    if (conn->buf_pos == conn->buf_len) {",
                "        conn->buf_frame_end = 0;",
                "        conn->buf_len = 0;",
                "        conn->buf_pos = 0;",
                "        return 0;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/mqtt_prot_parser",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "int mqtt_prot_parser(struct mqtt_conn *conn)\n{\n    int bytes = 0;\n    int length = 0;\n    int pos = conn->buf_pos;\n    int mult;\n\n    for (; conn->buf_pos < conn->buf_len; conn->buf_pos++) {\n        if (conn->status & (MQTT_NEW | MQTT_NEXT)) {\n            /*\n             * Do we have at least the Control Packet fixed header\n             * and the remaining length byte field ?\n             */\n            if (BUF_AVAIL() < 2) {\n                conn->buf_pos = pos;\n                flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                          conn->event.fd, __FILENAME__, __LINE__);\n                return MQTT_MORE;\n            }\n\n            /* As the connection is new we expect a MQTT_CONNECT request */\n            conn->packet_type = BUFC() >> 4;\n            if (conn->status == MQTT_NEW && conn->packet_type != MQTT_CONNECT) {\n                flb_trace(\"[in_mqtt] [fd=%i] error, expecting MQTT_CONNECT\",\n                          conn->event.fd);\n                return MQTT_ERROR;\n            }\n            conn->packet_length = conn->buf_pos;\n            conn->buf_pos++;\n\n            /* Get the remaining length */\n            mult   = 1;\n            length = 0;\n            bytes  = 0;\n            do {\n                if (conn->buf_pos + 1 > conn->buf_len) {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n\n                bytes++;\n                length += (BUFC() & 127) * mult;\n                mult *= 128;\n                if (mult > 128*128*128) {\n                    return MQTT_ERROR;\n                }\n\n                if (length + 2 > (conn->buf_len - pos)) {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n\n                if ((BUFC() & 128) == 0) {\n                    if (conn->buf_len - 2 < length) {\n                        conn->buf_pos = pos;\n                        flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                                  conn->event.fd, __FILENAME__, __LINE__);\n                        return MQTT_MORE;\n                    }\n                    else {\n                        conn->buf_frame_end = conn->buf_pos + length;\n                        break;\n                    }\n                }\n\n                if (conn->buf_pos + 1 < conn->buf_len) {\n                    conn->buf_pos++;\n                }\n                else {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n            } while (1);\n\n            conn->buf_pos += bytes - 1;\n            conn->packet_length = length;\n\n            /* At this point we have a full control packet in place */\n            if (conn->packet_type == MQTT_CONNECT) {\n                mqtt_handle_connect(conn);\n            }\n            else if (conn->packet_type == MQTT_PUBLISH) {\n                mqtt_handle_publish(conn);\n            }\n            else if (conn->packet_type == MQTT_PINGREQ) {\n                mqtt_handle_ping(conn);\n            }\n            else if (conn->packet_type == MQTT_DISCONNECT) {\n                flb_trace(\"[in_mqtt] [fd=%i] CMD DISCONNECT\",\n                          conn->event.fd);\n                return MQTT_HANGUP;\n            }\n            else {\n            }\n\n            /* Prepare for next round */\n            conn->status = MQTT_NEXT;\n            conn->buf_pos = conn->buf_frame_end;\n            mqtt_packet_drop(conn);\n\n            if (conn->buf_len > 0) {\n                conn->buf_pos = -1;\n            }\n        }\n    }\n    conn->buf_pos--;\n    return 0;\n}",
        "func": "int mqtt_prot_parser(struct mqtt_conn *conn)\n{\n    int bytes = 0;\n    int length = 0;\n    int pos = conn->buf_pos;\n    int mult;\n\n    for (; conn->buf_pos < conn->buf_len; conn->buf_pos++) {\n        if (conn->status & (MQTT_NEW | MQTT_NEXT)) {\n            /*\n             * Do we have at least the Control Packet fixed header\n             * and the remaining length byte field ?\n             */\n            if (BUF_AVAIL() < 2) {\n                conn->buf_pos = pos;\n                flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                          conn->event.fd, __FILENAME__, __LINE__);\n                return MQTT_MORE;\n            }\n\n            /* As the connection is new we expect a MQTT_CONNECT request */\n            conn->packet_type = BUFC() >> 4;\n            if (conn->status == MQTT_NEW && conn->packet_type != MQTT_CONNECT) {\n                flb_trace(\"[in_mqtt] [fd=%i] error, expecting MQTT_CONNECT\",\n                          conn->event.fd);\n                return MQTT_ERROR;\n            }\n            conn->packet_length = conn->buf_pos;\n            conn->buf_pos++;\n\n            /* Get the remaining length */\n            mult   = 1;\n            length = 0;\n            bytes  = 0;\n            do {\n                if (conn->buf_pos + 1 > conn->buf_len) {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n\n                bytes++;\n                length += (BUFC() & 127) * mult;\n                mult *= 128;\n                if (mult > 128*128*128) {\n                    return MQTT_ERROR;\n                }\n\n                if (length + 2 > (conn->buf_len - pos)) {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n\n                if ((BUFC() & 128) == 0) {\n                    if (conn->buf_len - 2 < length) {\n                        conn->buf_pos = pos;\n                        flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                                  conn->event.fd, __FILENAME__, __LINE__);\n                        return MQTT_MORE;\n                    }\n                    else {\n                        conn->buf_frame_end = conn->buf_pos + length;\n                        break;\n                    }\n                }\n\n                if (conn->buf_pos + 1 < conn->buf_len) {\n                    conn->buf_pos++;\n                }\n                else {\n                    conn->buf_pos = pos;\n                    flb_trace(\"[in_mqtt] [fd=%i] Need more data at %s:%i\",\n                              conn->event.fd, __FILENAME__, __LINE__);\n                    return MQTT_MORE;\n                }\n            } while (1);\n\n            conn->buf_pos += bytes - 1;\n            conn->packet_length = length;\n\n            /* At this point we have a full control packet in place */\n            if (conn->packet_type == MQTT_CONNECT) {\n                mqtt_handle_connect(conn);\n            }\n            else if (conn->packet_type == MQTT_PUBLISH) {\n                mqtt_handle_publish(conn);\n            }\n            else if (conn->packet_type == MQTT_PINGREQ) {\n                mqtt_handle_ping(conn);\n            }\n            else if (conn->packet_type == MQTT_DISCONNECT) {\n                flb_trace(\"[in_mqtt] [fd=%i] CMD DISCONNECT\",\n                          conn->event.fd);\n                return MQTT_HANGUP;\n            }\n            else {\n            }\n\n            /* Prepare for next round */\n            conn->status = MQTT_NEXT;\n            conn->buf_pos = conn->buf_frame_end;\n\n            mqtt_packet_drop(conn);\n\n            if (conn->buf_len > 0) {\n                conn->buf_pos = -1;\n            }\n        }\n    }\n    conn->buf_pos--;\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -102,6 +102,7 @@\n             /* Prepare for next round */\n             conn->status = MQTT_NEXT;\n             conn->buf_pos = conn->buf_frame_end;\n+\n             mqtt_packet_drop(conn);\n \n             if (conn->buf_len > 0) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/mqtt_conn_del",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "int mqtt_conn_del(struct mqtt_conn *conn)\n{\n    /* Unregister the file descriptior from the event-loop */\n    mk_event_del(conn->ctx->evl, &conn->event);\n\n    /* Release resources */\n    close(conn->fd);\n    flb_free(conn);\n\n    return 0;\n}",
        "func": "int mqtt_conn_del(struct mqtt_conn *conn)\n{\n    /* Unregister the file descriptior from the event-loop */\n    mk_event_del(conn->ctx->evl, &conn->event);\n\n    /* Release resources */\n    close(conn->fd);\n    mk_list_del(&conn->_head);\n    flb_free(conn);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n \n     /* Release resources */\n     close(conn->fd);\n+    mk_list_del(&conn->_head);\n     flb_free(conn);\n \n     return 0;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    mk_list_del(&conn->_head);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/mqtt_conn_add",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "struct mqtt_conn *mqtt_conn_add(int fd, struct flb_in_mqtt_config *ctx)\n{\n    int ret;\n    struct mqtt_conn *conn;\n    struct mk_event *event;\n\n    conn = flb_malloc(sizeof(struct mqtt_conn));\n    if (!conn) {\n        return NULL;\n    }\n\n    /* Set data for the event-loop */\n    event = &conn->event;\n    event->fd           = fd;\n    event->type         = FLB_ENGINE_EV_CUSTOM;\n    event->mask         = MK_EVENT_EMPTY;\n    event->handler      = mqtt_conn_event;\n    event->status       = MK_EVENT_NONE;\n\n    /* Connection info */\n    conn->fd      = fd;\n    conn->ctx     = ctx;\n    conn->buf_pos = 0;\n    conn->buf_len = 0;\n    conn->buf_frame_end = 0;\n    conn->status  = MQTT_NEW;\n\n    /* Register instance into the event loop */\n    ret = mk_event_add(ctx->evl, fd, FLB_ENGINE_EV_CUSTOM, MK_EVENT_READ, conn);\n    if (ret == -1) {\n        flb_error(\"[mqtt] could not register new connection\");\n        close(fd);\n        flb_free(conn);\n        return NULL;\n    }\n\n    return conn;\n}",
        "func": "struct mqtt_conn *mqtt_conn_add(int fd, struct flb_in_mqtt_config *ctx)\n{\n    int ret;\n    struct mqtt_conn *conn;\n    struct mk_event *event;\n\n    conn = flb_malloc(sizeof(struct mqtt_conn));\n    if (!conn) {\n        flb_errno();\n        return NULL;\n    }\n\n    /* Set data for the event-loop */\n    event = &conn->event;\n    event->fd           = fd;\n    event->type         = FLB_ENGINE_EV_CUSTOM;\n    event->mask         = MK_EVENT_EMPTY;\n    event->handler      = mqtt_conn_event;\n    event->status       = MK_EVENT_NONE;\n\n    /* Connection info */\n    conn->fd      = fd;\n    conn->ctx     = ctx;\n    conn->buf_pos = 0;\n    conn->buf_len = 0;\n    conn->buf_frame_end = 0;\n    conn->status  = MQTT_NEW;\n\n    /* Register instance into the event loop */\n    ret = mk_event_add(ctx->evl, fd, FLB_ENGINE_EV_CUSTOM, MK_EVENT_READ, conn);\n    if (ret == -1) {\n        flb_error(\"[mqtt] could not register new connection\");\n        close(fd);\n        flb_free(conn);\n        return NULL;\n    }\n\n    mk_list_add(&conn->_head, &ctx->conns);\n    return conn;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n \n     conn = flb_malloc(sizeof(struct mqtt_conn));\n     if (!conn) {\n+        flb_errno();\n         return NULL;\n     }\n \n@@ -34,5 +35,6 @@\n         return NULL;\n     }\n \n+    mk_list_add(&conn->_head, &ctx->conns);\n     return conn;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        flb_errno();",
                "    mk_list_add(&conn->_head, &ctx->conns);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/in_mqtt_exit",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "static int in_mqtt_exit(void *data, struct flb_config *config)\n{\n    (void) *config;\n    struct flb_in_mqtt_config *ctx = data;\n\n    mqtt_config_free(ctx);\n\n    return 0;\n}",
        "func": "static int in_mqtt_exit(void *data, struct flb_config *config)\n{\n    (void) *config;\n    struct flb_in_mqtt_config *ctx = data;\n\n    mqtt_conn_destroy_all(ctx);\n    mqtt_config_free(ctx);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n     (void) *config;\n     struct flb_in_mqtt_config *ctx = data;\n \n+    mqtt_conn_destroy_all(ctx);\n     mqtt_config_free(ctx);\n \n     return 0;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    mqtt_conn_destroy_all(ctx);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-9749",
        "func_name": "fluent/fluent-bit/mqtt_config_init",
        "description": "An issue was discovered in the MQTT input plugin in Fluent Bit through 1.0.4. When this plugin acts as an MQTT broker (server), it mishandles incoming network messages. After processing a crafted packet, the plugin's mqtt_packet_drop function (in /plugins/in_mqtt/mqtt_prot.c) executes the memmove() function with a negative size parameter. That leads to a crash of the whole Fluent Bit server via a SIGSEGV signal.",
        "git_url": "https://github.com/fluent/fluent-bit/commit/d978659be437b6f6c47997ab748ce8d253627b16",
        "commit_title": "in_mqtt: fix memory corruption on dropping packet (#1135)",
        "commit_text": " This patch adds an extra verification to the buffer counters to avoid corruption when memmove() an extra byte.  In addition this patch implement a linked list for the active connections so when closing Fluent Bit we have a clean exit. ",
        "func_before": "struct flb_in_mqtt_config *mqtt_config_init(struct flb_input_instance *i_ins)\n{\n    char tmp[16];\n    char *listen;\n    struct flb_in_mqtt_config *config;\n\n    config = flb_malloc(sizeof(struct flb_in_mqtt_config));\n    memset(config, '\\0', sizeof(struct flb_in_mqtt_config));\n\n    /* Listen interface (if not set, defaults to 0.0.0.0) */\n    if (!i_ins->host.listen) {\n        listen = flb_input_get_property(\"listen\", i_ins);\n        if (listen) {\n            config->listen = flb_strdup(listen);\n        }\n        else {\n            config->listen = flb_strdup(\"0.0.0.0\");\n        }\n    }\n    else {\n        config->listen = i_ins->host.listen;\n    }\n\n    /* Listener TCP Port */\n    if (i_ins->host.port == 0) {\n        config->tcp_port = flb_strdup(\"1883\");\n    }\n    else {\n        snprintf(tmp, sizeof(tmp) - 1, \"%d\", i_ins->host.port);\n        config->tcp_port = flb_strdup(tmp);\n    }\n\n    flb_debug(\"[in_mqtt] Listen='%s' TCP_Port=%s\",\n              config->listen, config->tcp_port);\n\n    return config;\n}",
        "func": "struct flb_in_mqtt_config *mqtt_config_init(struct flb_input_instance *i_ins)\n{\n    char tmp[16];\n    char *listen;\n    struct flb_in_mqtt_config *config;\n\n    config = flb_calloc(1, sizeof(struct flb_in_mqtt_config));\n    if (!config) {\n        flb_errno();\n        return NULL;\n    }\n\n    /* Listen interface (if not set, defaults to 0.0.0.0) */\n    if (!i_ins->host.listen) {\n        listen = flb_input_get_property(\"listen\", i_ins);\n        if (listen) {\n            config->listen = flb_strdup(listen);\n        }\n        else {\n            config->listen = flb_strdup(\"0.0.0.0\");\n        }\n    }\n    else {\n        config->listen = i_ins->host.listen;\n    }\n\n    /* Listener TCP Port */\n    if (i_ins->host.port == 0) {\n        config->tcp_port = flb_strdup(\"1883\");\n    }\n    else {\n        snprintf(tmp, sizeof(tmp) - 1, \"%d\", i_ins->host.port);\n        config->tcp_port = flb_strdup(tmp);\n    }\n\n    flb_debug(\"[in_mqtt] Listen='%s' TCP_Port=%s\",\n              config->listen, config->tcp_port);\n\n    mk_list_init(&config->conns);\n    return config;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,8 +4,11 @@\n     char *listen;\n     struct flb_in_mqtt_config *config;\n \n-    config = flb_malloc(sizeof(struct flb_in_mqtt_config));\n-    memset(config, '\\0', sizeof(struct flb_in_mqtt_config));\n+    config = flb_calloc(1, sizeof(struct flb_in_mqtt_config));\n+    if (!config) {\n+        flb_errno();\n+        return NULL;\n+    }\n \n     /* Listen interface (if not set, defaults to 0.0.0.0) */\n     if (!i_ins->host.listen) {\n@@ -33,5 +36,6 @@\n     flb_debug(\"[in_mqtt] Listen='%s' TCP_Port=%s\",\n               config->listen, config->tcp_port);\n \n+    mk_list_init(&config->conns);\n     return config;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    config = flb_malloc(sizeof(struct flb_in_mqtt_config));",
                "    memset(config, '\\0', sizeof(struct flb_in_mqtt_config));"
            ],
            "added_lines": [
                "    config = flb_calloc(1, sizeof(struct flb_in_mqtt_config));",
                "    if (!config) {",
                "        flb_errno();",
                "        return NULL;",
                "    }",
                "    mk_list_init(&config->conns);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-37646",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of `tf.raw_ops.StringNGrams` is vulnerable to an integer overflow issue caused by converting a signed integer value to an unsigned one and then allocating memory based on this value. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/string_ngrams_op.cc#L184) calls `reserve` on a `tstring` with a value that sometimes can be negative if user supplies negative `ngram_widths`. The `reserve` method calls `TF_TString_Reserve` which has an `unsigned long` argument for the size of the buffer. Hence, the implicit conversion transforms the negative value to a large integer. We have patched the issue in GitHub commit c283e542a3f422420cfdb332414543b62fc4e4a5. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/c283e542a3f422420cfdb332414543b62fc4e4a5",
        "commit_title": "Disallow negative ngram_widths values in tf.raw_ops.StringNGrams",
        "commit_text": " PiperOrigin-RevId: 387148179",
        "func_before": "void Compute(tensorflow::OpKernelContext* context) override {\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate that the splits are valid indices into data, only if there are\n    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size > 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split == 0,\n                  errors::InvalidArgument(\"First split value must be 0, got \",\n                                          prev_split));\n      for (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i) >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n                        prev_split, \", \", input_data_size, \"]\"));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context, prev_split == input_data_size,\n                  errors::InvalidArgument(\n                      \"Last split value must be data size. Expected \",\n                      input_data_size, \", got \", prev_split));\n    }\n\n    int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(), &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size() == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, data->shape(), &empty));\n      for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i] = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) - splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width : ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}), &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n      // If we're preserving short sequences, check to see if no sequence was\n      // generated by comparing the current output start idx to the original\n      // one (ngram_splits_data). If no ngrams were generated, then they will\n      // be equal (since we increment output_start_idx by num_ngrams every\n      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n        // One legitimate reason to not have any ngrams when preserve_short_\n        // is true is if the sequence itself is empty. In that case, move on.\n        if (data_length == 0) {\n          continue;\n        }\n        // We don't have to worry about dynamic padding sizes here: if padding\n        // was dynamic, every sequence would have had sufficient padding to\n        // generate at least one ngram.\n        int ngram_width = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n      }\n    }\n  }",
        "func": "void Compute(tensorflow::OpKernelContext* context) override {\n    for (int ngram_width : ngram_widths_) {\n      OP_REQUIRES(\n          context, ngram_width > 0,\n          errors::InvalidArgument(\"ngram_widths must contain positive values\"));\n    }\n\n    const tensorflow::Tensor* data;\n    OP_REQUIRES_OK(context, context->input(\"data\", &data));\n    const auto& input_data = data->flat<tstring>().data();\n\n    const tensorflow::Tensor* splits;\n    OP_REQUIRES_OK(context, context->input(\"data_splits\", &splits));\n    const auto& splits_vec = splits->flat<SPLITS_TYPE>();\n\n    // Validate that the splits are valid indices into data, only if there are\n    // splits specified.\n    const int input_data_size = data->flat<tstring>().size();\n    const int splits_vec_size = splits_vec.size();\n    if (splits_vec_size > 0) {\n      int prev_split = splits_vec(0);\n      OP_REQUIRES(context, prev_split == 0,\n                  errors::InvalidArgument(\"First split value must be 0, got \",\n                                          prev_split));\n      for (int i = 1; i < splits_vec_size; ++i) {\n        bool valid_splits = splits_vec(i) >= prev_split;\n        valid_splits = valid_splits && (splits_vec(i) <= input_data_size);\n        OP_REQUIRES(context, valid_splits,\n                    errors::InvalidArgument(\n                        \"Invalid split value \", splits_vec(i), \", must be in [\",\n                        prev_split, \", \", input_data_size, \"]\"));\n        prev_split = splits_vec(i);\n      }\n      OP_REQUIRES(context, prev_split == input_data_size,\n                  errors::InvalidArgument(\n                      \"Last split value must be data size. Expected \",\n                      input_data_size, \", got \", prev_split));\n    }\n\n    int num_batch_items = splits_vec.size() - 1;\n    tensorflow::Tensor* ngrams_splits;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(1, splits->shape(), &ngrams_splits));\n    auto ngrams_splits_data = ngrams_splits->flat<SPLITS_TYPE>().data();\n\n    // If there is no data or size, return an empty RT.\n    if (data->flat<tstring>().size() == 0 || splits_vec.size() == 0) {\n      tensorflow::Tensor* empty;\n      OP_REQUIRES_OK(context,\n                     context->allocate_output(0, data->shape(), &empty));\n      for (int i = 0; i <= num_batch_items; ++i) {\n        ngrams_splits_data[i] = 0;\n      }\n      return;\n    }\n\n    ngrams_splits_data[0] = 0;\n    for (int i = 1; i <= num_batch_items; ++i) {\n      int length = splits_vec(i) - splits_vec(i - 1);\n      int num_ngrams = 0;\n      for (int ngram_width : ngram_widths_)\n        num_ngrams += get_num_ngrams(length, ngram_width);\n      if (preserve_short_ && length > 0 && num_ngrams == 0) {\n        num_ngrams = 1;\n      }\n      ngrams_splits_data[i] = ngrams_splits_data[i - 1] + num_ngrams;\n    }\n\n    tensorflow::Tensor* ngrams;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            0, TensorShape({ngrams_splits_data[num_batch_items]}), &ngrams));\n    auto ngrams_data = ngrams->flat<tstring>().data();\n\n    for (int i = 0; i < num_batch_items; ++i) {\n      auto data_start = &input_data[splits_vec(i)];\n      int output_start_idx = ngrams_splits_data[i];\n      for (int ngram_width : ngram_widths_) {\n        auto output_start = &ngrams_data[output_start_idx];\n        int length = splits_vec(i + 1) - splits_vec(i);\n        int num_ngrams = get_num_ngrams(length, ngram_width);\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n        output_start_idx += num_ngrams;\n      }\n      // If we're preserving short sequences, check to see if no sequence was\n      // generated by comparing the current output start idx to the original\n      // one (ngram_splits_data). If no ngrams were generated, then they will\n      // be equal (since we increment output_start_idx by num_ngrams every\n      // time we create a set of ngrams.)\n      if (preserve_short_ && output_start_idx == ngrams_splits_data[i]) {\n        int data_length = splits_vec(i + 1) - splits_vec(i);\n        // One legitimate reason to not have any ngrams when preserve_short_\n        // is true is if the sequence itself is empty. In that case, move on.\n        if (data_length == 0) {\n          continue;\n        }\n        // We don't have to worry about dynamic padding sizes here: if padding\n        // was dynamic, every sequence would have had sufficient padding to\n        // generate at least one ngram.\n        int ngram_width = data_length + 2 * pad_width_;\n        auto output_start = &ngrams_data[output_start_idx];\n        int num_ngrams = 1;\n        CreateNgrams(data_start, output_start, num_ngrams, ngram_width);\n      }\n    }\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,10 @@\n void Compute(tensorflow::OpKernelContext* context) override {\n+    for (int ngram_width : ngram_widths_) {\n+      OP_REQUIRES(\n+          context, ngram_width > 0,\n+          errors::InvalidArgument(\"ngram_widths must contain positive values\"));\n+    }\n+\n     const tensorflow::Tensor* data;\n     OP_REQUIRES_OK(context, context->input(\"data\", &data));\n     const auto& input_data = data->flat<tstring>().data();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    for (int ngram_width : ngram_widths_) {",
                "      OP_REQUIRES(",
                "          context, ngram_width > 0,",
                "          errors::InvalidArgument(\"ngram_widths must contain positive values\"));",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2021-37669",
        "func_name": "tensorflow/Compute",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/image/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`. However, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to unsigned. If the attacker supplies a negative value, this conversion results in a crash. A similar issue occurs in `CombinedNonMaxSuppression`. We have patched the issue in GitHub commit 3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58",
        "commit_title": "Prevent overflow due to integer conversion to unsigned.",
        "commit_text": " PiperOrigin-RevId: 387738045",
        "func_before": "void Compute(OpKernelContext* context) override {\n    // boxes: [batch_size, num_anchors, q, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [batch_size, num_anchors, num_classes]\n    const Tensor& scores = context->input(1);\n    OP_REQUIRES(\n        context, (boxes.dim_size(0) == scores.dim_size(0)),\n        errors::InvalidArgument(\"boxes and scores must have same batch size\"));\n\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    const int max_size_per_class = max_output_size.scalar<int>()();\n    // max_total_size: scalar\n    const Tensor& max_total_size = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_total_size.shape()),\n        errors::InvalidArgument(\"max_total_size must be 0-D, got shape \",\n                                max_total_size.shape().DebugString()));\n    const int max_total_size_per_batch = max_total_size.scalar<int>()();\n    OP_REQUIRES(context, max_total_size_per_batch > 0,\n                errors::InvalidArgument(\"max_total_size must be > 0\"));\n    // Throw warning when `max_total_size` is too large as it may cause OOM.\n    if (max_total_size_per_batch > pow(10, 6)) {\n      LOG(WARNING) << \"Detected a large value for `max_total_size`. This may \"\n                   << \"cause OOM error. (max_total_size: \"\n                   << max_total_size.scalar<int>()() << \")\";\n    }\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(4);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const float iou_threshold_val = iou_threshold.scalar<float>()();\n\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(5);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const float score_threshold_val = score_threshold.scalar<float>()();\n\n    OP_REQUIRES(context, iou_threshold_val >= 0 && iou_threshold_val <= 1,\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    int num_boxes = 0;\n    const int num_classes = scores.dim_size(2);\n    ParseAndCheckCombinedNMSBoxSizes(context, boxes, &num_boxes, num_classes);\n    CheckCombinedNMSScoreSizes(context, num_boxes, scores);\n\n    if (!context->status().ok()) {\n      return;\n    }\n    BatchedNonMaxSuppressionOp(context, boxes, scores, num_boxes,\n                               max_size_per_class, max_total_size_per_batch,\n                               score_threshold_val, iou_threshold_val,\n                               pad_per_class_, clip_boxes_);\n  }",
        "func": "void Compute(OpKernelContext* context) override {\n    // boxes: [batch_size, num_anchors, q, 4]\n    const Tensor& boxes = context->input(0);\n    // scores: [batch_size, num_anchors, num_classes]\n    const Tensor& scores = context->input(1);\n    OP_REQUIRES(\n        context, (boxes.dim_size(0) == scores.dim_size(0)),\n        errors::InvalidArgument(\"boxes and scores must have same batch size\"));\n\n    // max_output_size: scalar\n    const Tensor& max_output_size = context->input(2);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_output_size.shape()),\n        errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                max_output_size.shape().DebugString()));\n    const int max_size_per_class = max_output_size.scalar<int>()();\n    OP_REQUIRES(context, max_size_per_class > 0,\n                errors::InvalidArgument(\"max_size_per_class must be positive\"));\n    // max_total_size: scalar\n    const Tensor& max_total_size = context->input(3);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(max_total_size.shape()),\n        errors::InvalidArgument(\"max_total_size must be 0-D, got shape \",\n                                max_total_size.shape().DebugString()));\n    const int max_total_size_per_batch = max_total_size.scalar<int>()();\n    OP_REQUIRES(context, max_total_size_per_batch > 0,\n                errors::InvalidArgument(\"max_total_size must be > 0\"));\n    // Throw warning when `max_total_size` is too large as it may cause OOM.\n    if (max_total_size_per_batch > pow(10, 6)) {\n      LOG(WARNING) << \"Detected a large value for `max_total_size`. This may \"\n                   << \"cause OOM error. (max_total_size: \"\n                   << max_total_size.scalar<int>()() << \")\";\n    }\n    // iou_threshold: scalar\n    const Tensor& iou_threshold = context->input(4);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(iou_threshold.shape()),\n                errors::InvalidArgument(\"iou_threshold must be 0-D, got shape \",\n                                        iou_threshold.shape().DebugString()));\n    const float iou_threshold_val = iou_threshold.scalar<float>()();\n\n    // score_threshold: scalar\n    const Tensor& score_threshold = context->input(5);\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(score_threshold.shape()),\n        errors::InvalidArgument(\"score_threshold must be 0-D, got shape \",\n                                score_threshold.shape().DebugString()));\n    const float score_threshold_val = score_threshold.scalar<float>()();\n\n    OP_REQUIRES(context, iou_threshold_val >= 0 && iou_threshold_val <= 1,\n                errors::InvalidArgument(\"iou_threshold must be in [0, 1]\"));\n    int num_boxes = 0;\n    const int num_classes = scores.dim_size(2);\n    ParseAndCheckCombinedNMSBoxSizes(context, boxes, &num_boxes, num_classes);\n    CheckCombinedNMSScoreSizes(context, num_boxes, scores);\n\n    if (!context->status().ok()) {\n      return;\n    }\n    BatchedNonMaxSuppressionOp(context, boxes, scores, num_boxes,\n                               max_size_per_class, max_total_size_per_batch,\n                               score_threshold_val, iou_threshold_val,\n                               pad_per_class_, clip_boxes_);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,8 @@\n         errors::InvalidArgument(\"max_size_per_class must be 0-D, got shape \",\n                                 max_output_size.shape().DebugString()));\n     const int max_size_per_class = max_output_size.scalar<int>()();\n+    OP_REQUIRES(context, max_size_per_class > 0,\n+                errors::InvalidArgument(\"max_size_per_class must be positive\"));\n     // max_total_size: scalar\n     const Tensor& max_total_size = context->input(3);\n     OP_REQUIRES(",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES(context, max_size_per_class > 0,",
                "                errors::InvalidArgument(\"max_size_per_class must be positive\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-37669",
        "func_name": "tensorflow/DoNonMaxSuppressionOp",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can cause denial of service in applications serving models using `tf.raw_ops.NonMaxSuppressionV5` by triggering a division by 0. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/image/non_max_suppression_op.cc#L170-L271) uses a user controlled argument to resize a `std::vector`. However, as `std::vector::resize` takes the size argument as a `size_t` and `output_size` is an `int`, there is an implicit conversion to unsigned. If the attacker supplies a negative value, this conversion results in a crash. A similar issue occurs in `CombinedNonMaxSuppression`. We have patched the issue in GitHub commit 3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d and commit [b5cdbf12ffcaaffecf98f22a6be5a64bb96e4f58. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/3a7362750d5c372420aa8f0caf7bf5b5c3d0f52d",
        "commit_title": "Prevent crash/heap OOB due to integer conversion to unsigned in NMS kernels",
        "commit_text": " PiperOrigin-RevId: 387938262",
        "func_before": "void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\n                           int num_boxes, const Tensor& max_output_size,\n                           const T similarity_threshold,\n                           const T score_threshold, const T soft_nms_sigma,\n                           const std::function<float(int, int)>& similarity_fn,\n                           bool return_scores_tensor = false,\n                           bool pad_to_max_output_size = false,\n                           int* ptr_num_valid_outputs = nullptr) {\n  const int output_size = max_output_size.scalar<int>()();\n\n  std::vector<T> scores_data(num_boxes);\n  std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\n\n  // Data structure for a selection candidate in NMS.\n  struct Candidate {\n    int box_index;\n    T score;\n    int suppress_begin_index;\n  };\n\n  auto cmp = [](const Candidate bs_i, const Candidate bs_j) {\n    return ((bs_i.score == bs_j.score) && (bs_i.box_index > bs_j.box_index)) ||\n           bs_i.score < bs_j.score;\n  };\n  std::priority_queue<Candidate, std::deque<Candidate>, decltype(cmp)>\n      candidate_priority_queue(cmp);\n  for (int i = 0; i < scores_data.size(); ++i) {\n    if (scores_data[i] > score_threshold) {\n      candidate_priority_queue.emplace(Candidate({i, scores_data[i], 0}));\n    }\n  }\n\n  T scale = static_cast<T>(0.0);\n  bool is_soft_nms = soft_nms_sigma > static_cast<T>(0.0);\n  if (is_soft_nms) {\n    scale = static_cast<T>(-0.5) / soft_nms_sigma;\n  }\n\n  auto suppress_weight = [similarity_threshold, scale,\n                          is_soft_nms](const T sim) {\n    const T weight = Eigen::numext::exp<T>(scale * sim * sim);\n    return is_soft_nms || sim <= similarity_threshold ? weight\n                                                      : static_cast<T>(0.0);\n  };\n\n  std::vector<int> selected;\n  std::vector<T> selected_scores;\n  float similarity;\n  T original_score;\n  Candidate next_candidate;\n\n  while (selected.size() < output_size && !candidate_priority_queue.empty()) {\n    next_candidate = candidate_priority_queue.top();\n    original_score = next_candidate.score;\n    candidate_priority_queue.pop();\n\n    // Overlapping boxes are likely to have similar scores, therefore we\n    // iterate through the previously selected boxes backwards in order to\n    // see if `next_candidate` should be suppressed. We also enforce a property\n    // that a candidate can be suppressed by another candidate no more than\n    // once via `suppress_begin_index` which tracks which previously selected\n    // boxes have already been compared against next_candidate prior to a given\n    // iteration.  These previous selected boxes are then skipped over in the\n    // following loop.\n    bool should_hard_suppress = false;\n    for (int j = static_cast<int>(selected.size()) - 1;\n         j >= next_candidate.suppress_begin_index; --j) {\n      similarity = similarity_fn(next_candidate.box_index, selected[j]);\n\n      next_candidate.score *= suppress_weight(static_cast<T>(similarity));\n\n      // First decide whether to perform hard suppression\n      if (!is_soft_nms && static_cast<T>(similarity) > similarity_threshold) {\n        should_hard_suppress = true;\n        break;\n      }\n\n      // If next_candidate survives hard suppression, apply soft suppression\n      if (next_candidate.score <= score_threshold) break;\n    }\n    // If `next_candidate.score` has not dropped below `score_threshold`\n    // by this point, then we know that we went through all of the previous\n    // selections and can safely update `suppress_begin_index` to\n    // `selected.size()`. If on the other hand `next_candidate.score`\n    // *has* dropped below the score threshold, then since `suppress_weight`\n    // always returns values in [0, 1], further suppression by items that were\n    // not covered in the above for loop would not have caused the algorithm\n    // to select this item. We thus do the same update to\n    // `suppress_begin_index`, but really, this element will not be added back\n    // into the priority queue in the following.\n    next_candidate.suppress_begin_index = selected.size();\n\n    if (!should_hard_suppress) {\n      if (next_candidate.score == original_score) {\n        // Suppression has not occurred, so select next_candidate\n        selected.push_back(next_candidate.box_index);\n        selected_scores.push_back(next_candidate.score);\n        continue;\n      }\n      if (next_candidate.score > score_threshold) {\n        // Soft suppression has occurred and current score is still greater than\n        // score_threshold; add next_candidate back onto priority queue.\n        candidate_priority_queue.push(next_candidate);\n      }\n    }\n  }\n\n  int num_valid_outputs = selected.size();\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    selected_scores.resize(output_size, static_cast<T>(0));\n  }\n  if (ptr_num_valid_outputs) {\n    *ptr_num_valid_outputs = num_valid_outputs;\n  }\n\n  // Allocate output tensors\n  Tensor* output_indices = nullptr;\n  TensorShape output_shape({static_cast<int>(selected.size())});\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, output_shape, &output_indices));\n  TTypes<int, 1>::Tensor output_indices_data = output_indices->tensor<int, 1>();\n  std::copy_n(selected.begin(), selected.size(), output_indices_data.data());\n\n  if (return_scores_tensor) {\n    Tensor* output_scores = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, output_shape, &output_scores));\n    typename TTypes<T, 1>::Tensor output_scores_data =\n        output_scores->tensor<T, 1>();\n    std::copy_n(selected_scores.begin(), selected_scores.size(),\n                output_scores_data.data());\n  }\n}",
        "func": "void DoNonMaxSuppressionOp(OpKernelContext* context, const Tensor& scores,\n                           int num_boxes, const Tensor& max_output_size,\n                           const T similarity_threshold,\n                           const T score_threshold, const T soft_nms_sigma,\n                           const std::function<float(int, int)>& similarity_fn,\n                           bool return_scores_tensor = false,\n                           bool pad_to_max_output_size = false,\n                           int* ptr_num_valid_outputs = nullptr) {\n  const int output_size = max_output_size.scalar<int>()();\n  OP_REQUIRES(context, output_size >= 0,\n              errors::InvalidArgument(\"output size must be non-negative\"));\n\n  std::vector<T> scores_data(num_boxes);\n  std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());\n\n  // Data structure for a selection candidate in NMS.\n  struct Candidate {\n    int box_index;\n    T score;\n    int suppress_begin_index;\n  };\n\n  auto cmp = [](const Candidate bs_i, const Candidate bs_j) {\n    return ((bs_i.score == bs_j.score) && (bs_i.box_index > bs_j.box_index)) ||\n           bs_i.score < bs_j.score;\n  };\n  std::priority_queue<Candidate, std::deque<Candidate>, decltype(cmp)>\n      candidate_priority_queue(cmp);\n  for (int i = 0; i < scores_data.size(); ++i) {\n    if (scores_data[i] > score_threshold) {\n      candidate_priority_queue.emplace(Candidate({i, scores_data[i], 0}));\n    }\n  }\n\n  T scale = static_cast<T>(0.0);\n  bool is_soft_nms = soft_nms_sigma > static_cast<T>(0.0);\n  if (is_soft_nms) {\n    scale = static_cast<T>(-0.5) / soft_nms_sigma;\n  }\n\n  auto suppress_weight = [similarity_threshold, scale,\n                          is_soft_nms](const T sim) {\n    const T weight = Eigen::numext::exp<T>(scale * sim * sim);\n    return is_soft_nms || sim <= similarity_threshold ? weight\n                                                      : static_cast<T>(0.0);\n  };\n\n  std::vector<int> selected;\n  std::vector<T> selected_scores;\n  float similarity;\n  T original_score;\n  Candidate next_candidate;\n\n  while (selected.size() < output_size && !candidate_priority_queue.empty()) {\n    next_candidate = candidate_priority_queue.top();\n    original_score = next_candidate.score;\n    candidate_priority_queue.pop();\n\n    // Overlapping boxes are likely to have similar scores, therefore we\n    // iterate through the previously selected boxes backwards in order to\n    // see if `next_candidate` should be suppressed. We also enforce a property\n    // that a candidate can be suppressed by another candidate no more than\n    // once via `suppress_begin_index` which tracks which previously selected\n    // boxes have already been compared against next_candidate prior to a given\n    // iteration.  These previous selected boxes are then skipped over in the\n    // following loop.\n    bool should_hard_suppress = false;\n    for (int j = static_cast<int>(selected.size()) - 1;\n         j >= next_candidate.suppress_begin_index; --j) {\n      similarity = similarity_fn(next_candidate.box_index, selected[j]);\n\n      next_candidate.score *= suppress_weight(static_cast<T>(similarity));\n\n      // First decide whether to perform hard suppression\n      if (!is_soft_nms && static_cast<T>(similarity) > similarity_threshold) {\n        should_hard_suppress = true;\n        break;\n      }\n\n      // If next_candidate survives hard suppression, apply soft suppression\n      if (next_candidate.score <= score_threshold) break;\n    }\n    // If `next_candidate.score` has not dropped below `score_threshold`\n    // by this point, then we know that we went through all of the previous\n    // selections and can safely update `suppress_begin_index` to\n    // `selected.size()`. If on the other hand `next_candidate.score`\n    // *has* dropped below the score threshold, then since `suppress_weight`\n    // always returns values in [0, 1], further suppression by items that were\n    // not covered in the above for loop would not have caused the algorithm\n    // to select this item. We thus do the same update to\n    // `suppress_begin_index`, but really, this element will not be added back\n    // into the priority queue in the following.\n    next_candidate.suppress_begin_index = selected.size();\n\n    if (!should_hard_suppress) {\n      if (next_candidate.score == original_score) {\n        // Suppression has not occurred, so select next_candidate\n        selected.push_back(next_candidate.box_index);\n        selected_scores.push_back(next_candidate.score);\n        continue;\n      }\n      if (next_candidate.score > score_threshold) {\n        // Soft suppression has occurred and current score is still greater than\n        // score_threshold; add next_candidate back onto priority queue.\n        candidate_priority_queue.push(next_candidate);\n      }\n    }\n  }\n\n  int num_valid_outputs = selected.size();\n  if (pad_to_max_output_size) {\n    selected.resize(output_size, 0);\n    selected_scores.resize(output_size, static_cast<T>(0));\n  }\n  if (ptr_num_valid_outputs) {\n    *ptr_num_valid_outputs = num_valid_outputs;\n  }\n\n  // Allocate output tensors\n  Tensor* output_indices = nullptr;\n  TensorShape output_shape({static_cast<int>(selected.size())});\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(0, output_shape, &output_indices));\n  TTypes<int, 1>::Tensor output_indices_data = output_indices->tensor<int, 1>();\n  std::copy_n(selected.begin(), selected.size(), output_indices_data.data());\n\n  if (return_scores_tensor) {\n    Tensor* output_scores = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(1, output_shape, &output_scores));\n    typename TTypes<T, 1>::Tensor output_scores_data =\n        output_scores->tensor<T, 1>();\n    std::copy_n(selected_scores.begin(), selected_scores.size(),\n                output_scores_data.data());\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,6 +7,8 @@\n                            bool pad_to_max_output_size = false,\n                            int* ptr_num_valid_outputs = nullptr) {\n   const int output_size = max_output_size.scalar<int>()();\n+  OP_REQUIRES(context, output_size >= 0,\n+              errors::InvalidArgument(\"output size must be non-negative\"));\n \n   std::vector<T> scores_data(num_boxes);\n   std::copy_n(scores.flat<T>().data(), num_boxes, scores_data.begin());",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  OP_REQUIRES(context, output_size >= 0,",
                "              errors::InvalidArgument(\"output size must be non-negative\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-37679",
        "func_name": "tensorflow/NestedStackRaggedTensors",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions it is possible to nest a `tf.map_fn` within another `tf.map_fn` call. However, if the input tensor is a `RaggedTensor` and there is no function signature provided, code assumes the output is a fully specified tensor and fills output buffer with uninitialized contents from the heap. The `t` and `z` outputs should be identical, however this is not the case. The last row of `t` contains data from the heap which can be used to leak other memory information. The bug lies in the conversion from a `Variant` tensor to a `RaggedTensor`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/ragged_tensor_from_variant_op.cc#L177-L190) does not check that all inner shapes match and this results in the additional dimensions. The same implementation can result in data loss, if input tensor is tweaked. We have patched the issue in GitHub commit 4e2565483d0ffcadc719bd44893fb7f609bb5f12. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/4e2565483d0ffcadc719bd44893fb7f609bb5f12",
        "commit_title": "Fix bug that could cause map_fn to produce incorrect results (rather than an error)",
        "commit_text": "when mapping over a ragged tensor with an inappropriate fn_output_signature.  (Note: there are cases where the default value for fn_output_signature is not appropriate, so the user needs to explicitly specify the correct output signature.)  PiperOrigin-RevId: 387606546",
        "func_before": "Status NestedStackRaggedTensors(\n    const std::vector<RaggedTensorVariant>& ragged_components,\n    const std::vector<int>& nested_dim_sizes, const int input_ragged_rank,\n    const int output_ragged_rank, RaggedTensorVariant* output_ragged) {\n  output_ragged->mutable_nested_splits()->reserve(output_ragged_rank);\n  const int dims = nested_dim_sizes.size();\n\n  // Populate first `dims - 1` splits.\n  for (int i = 0; i < dims - 1; i++) {\n    int dims_splits_size = nested_dim_sizes[i] + 1;\n    output_ragged->append_splits(Tensor(DataTypeToEnum<SPLIT_TYPE>::value,\n                                        TensorShape({dims_splits_size})));\n    auto splits_vec = output_ragged->mutable_splits(i)->vec<SPLIT_TYPE>();\n    int split_diff = nested_dim_sizes[i + 1];\n    for (int j = 0; j < dims_splits_size; j++) {\n      splits_vec(j) = j * split_diff;\n    }\n  }\n\n  // Populate `dims`-th split.\n  int splits_size = ragged_components.size() + 1;\n  output_ragged->append_splits(\n      Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({splits_size})));\n  auto dims_splits_vec =\n      output_ragged->mutable_splits(dims - 1)->vec<SPLIT_TYPE>();\n  dims_splits_vec(0) = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    int split_val = ragged_components[i].values().shape().dim_size(0);\n    if (input_ragged_rank != 0 && ragged_components[i].ragged_rank() > 0) {\n      split_val = ragged_components[i].splits(0).NumElements() - 1;\n    }\n    dims_splits_vec(i + 1) = dims_splits_vec(i) + split_val;\n  }\n\n  // Populate last `input_ragged_rank` splits.\n  for (int i = 0; i < input_ragged_rank; i++) {\n    int split_index = dims + i;\n    int split_size = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (!ragged_components[j].nested_splits().empty()) {\n        split_size += ragged_components[j].splits(i).NumElements() - 1;\n      }\n    }\n    output_ragged->append_splits(\n        Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n    auto splits_vec =\n        output_ragged->mutable_splits(split_index)->vec<SPLIT_TYPE>();\n    splits_vec(0) = 0;\n    SPLIT_TYPE last_split_value = 0;\n    int index = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (ragged_components[j].nested_splits().empty()) {\n        // Corner case: empty row. e.g [ [[x], [x]], [] ]\n        continue;\n      }\n      auto component_splits_vec =\n          ragged_components[j].splits(i).vec<SPLIT_TYPE>();\n      for (int k = 1; k < component_splits_vec.size(); k++, index++) {\n        splits_vec(index) = component_splits_vec(k) + last_split_value;\n      }\n      last_split_value = splits_vec(index - 1);\n    }\n  }\n\n  // If the variant tensor input is empty, then we have no way to determine\n  // the correct shape for the dense_values.  (It must have rank>=1, and its\n  // outer dimension must be 0, but we don't know its shape beyond that.)\n  // For now, we just use a shape of `[0]` in this case.\n  // TODO(edloper): Update this op with an attribute containing information\n  // about dense_values shape.  If it's `None`, then we'll probably still have\n  // to use shape=[0] here, but if we have more info, then we can use it.\n  // E.g., in map_fn, we may have shape info from the RaggedTensorSpec.\n  TensorShape component_values_shape;\n  if (ragged_components.empty()) {\n    component_values_shape = TensorShape({0});\n  } else {\n    component_values_shape = ragged_components[0].values().shape();\n  }\n\n  // Populate values.\n  int values_size = component_values_shape.dim_size(0);\n  for (int i = 1; i < ragged_components.size(); i++) {\n    if (ragged_components[i].values().dims() != component_values_shape.dims()) {\n      return errors::InvalidArgument(\n          \"Rank of values must match for all \"\n          \"components; values shape at index 0: \",\n          component_values_shape.DebugString(), \", values shape at index \", i,\n          \": \", ragged_components[i].values().shape().DebugString());\n    }\n    values_size += ragged_components[i].values().shape().dim_size(0);\n  }\n  component_values_shape.set_dim(0, values_size);\n  output_ragged->set_values(\n      Tensor(DataTypeToEnum<VALUE_TYPE>::value, component_values_shape));\n  auto output_values_flat =\n      output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n  int values_index = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    auto component_values_flat =\n        ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n    int num_inner_elements = ragged_components[i].values().NumElements();\n    if (ragged_components[i].values().dim_size(0) > 0) {\n      num_inner_elements /= ragged_components[i].values().dim_size(0);\n    }\n    for (int j = 0; j < ragged_components[i].values().dim_size(0);\n         j++, values_index++) {\n      for (int k = 0; k < num_inner_elements; k++) {\n        output_values_flat(values_index, k) = component_values_flat(j, k);\n      }\n    }\n  }\n  return Status::OK();\n}",
        "func": "Status NestedStackRaggedTensors(\n    const std::vector<RaggedTensorVariant>& ragged_components,\n    const std::vector<int>& nested_dim_sizes, const int input_ragged_rank,\n    const int output_ragged_rank, RaggedTensorVariant* output_ragged) {\n  output_ragged->mutable_nested_splits()->reserve(output_ragged_rank);\n  const int dims = nested_dim_sizes.size();\n\n  // Populate first `dims - 1` splits.\n  for (int i = 0; i < dims - 1; i++) {\n    int dims_splits_size = nested_dim_sizes[i] + 1;\n    output_ragged->append_splits(Tensor(DataTypeToEnum<SPLIT_TYPE>::value,\n                                        TensorShape({dims_splits_size})));\n    auto splits_vec = output_ragged->mutable_splits(i)->vec<SPLIT_TYPE>();\n    int split_diff = nested_dim_sizes[i + 1];\n    for (int j = 0; j < dims_splits_size; j++) {\n      splits_vec(j) = j * split_diff;\n    }\n  }\n\n  // Populate `dims`-th split.\n  int splits_size = ragged_components.size() + 1;\n  output_ragged->append_splits(\n      Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({splits_size})));\n  auto dims_splits_vec =\n      output_ragged->mutable_splits(dims - 1)->vec<SPLIT_TYPE>();\n  dims_splits_vec(0) = 0;\n  for (int i = 0; i < ragged_components.size(); i++) {\n    int split_val = ragged_components[i].values().shape().dim_size(0);\n    if (input_ragged_rank != 0 && ragged_components[i].ragged_rank() > 0) {\n      split_val = ragged_components[i].splits(0).NumElements() - 1;\n    }\n    dims_splits_vec(i + 1) = dims_splits_vec(i) + split_val;\n  }\n\n  // Populate last `input_ragged_rank` splits.\n  for (int i = 0; i < input_ragged_rank; i++) {\n    int split_index = dims + i;\n    int split_size = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (!ragged_components[j].nested_splits().empty()) {\n        split_size += ragged_components[j].splits(i).NumElements() - 1;\n      }\n    }\n    output_ragged->append_splits(\n        Tensor(DataTypeToEnum<SPLIT_TYPE>::value, TensorShape({split_size})));\n    auto splits_vec =\n        output_ragged->mutable_splits(split_index)->vec<SPLIT_TYPE>();\n    splits_vec(0) = 0;\n    SPLIT_TYPE last_split_value = 0;\n    int index = 1;\n    for (int j = 0; j < ragged_components.size(); j++) {\n      if (ragged_components[j].nested_splits().empty()) {\n        // Corner case: empty row. e.g [ [[x], [x]], [] ]\n        continue;\n      }\n      auto component_splits_vec =\n          ragged_components[j].splits(i).vec<SPLIT_TYPE>();\n      for (int k = 1; k < component_splits_vec.size(); k++, index++) {\n        splits_vec(index) = component_splits_vec(k) + last_split_value;\n      }\n      last_split_value = splits_vec(index - 1);\n    }\n  }\n\n  // If the variant tensor input is empty, then we have no way to determine\n  // the correct shape for the dense_values.  (It must have rank>=1, and its\n  // outer dimension must be 0, but we don't know its shape beyond that.)\n  // For now, we just use a shape of `[0]` in this case.\n  // TODO(edloper): Update this op with an attribute containing information\n  // about dense_values shape.  If it's `None`, then we'll probably still have\n  // to use shape=[0] here, but if we have more info, then we can use it.\n  // E.g., in map_fn, we may have shape info from the RaggedTensorSpec.\n  TensorShape component_values_shape;\n  if (ragged_components.empty()) {\n    component_values_shape = TensorShape({0});\n  } else {\n    component_values_shape = ragged_components[0].values().shape();\n  }\n\n  // Populate values.\n  int values_size = component_values_shape.dim_size(0);\n  for (int i = 1; i < ragged_components.size(); i++) {\n    if (ragged_components[i].values().dims() != component_values_shape.dims()) {\n      return errors::InvalidArgument(\n          \"Rank of values must match for all \"\n          \"components; values shape at index 0: \",\n          component_values_shape.DebugString(), \", values shape at index \", i,\n          \": \", ragged_components[i].values().shape().DebugString());\n    }\n    values_size += ragged_components[i].values().shape().dim_size(0);\n  }\n  component_values_shape.set_dim(0, values_size);\n  output_ragged->set_values(\n      Tensor(DataTypeToEnum<VALUE_TYPE>::value, component_values_shape));\n  auto output_values_flat =\n      output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n  int values_index = 0;\n\n  TensorShape expected_value_shape = component_values_shape;\n  expected_value_shape.RemoveDim(0);\n\n  for (int i = 0; i < ragged_components.size(); i++) {\n    // Check that the flat_values tensor shape is compatible.\n    TensorShape value_shape = ragged_components[i].values().shape();\n    value_shape.RemoveDim(0);\n    if (value_shape != expected_value_shape) {\n      return errors::InvalidArgument(\n          \"All flat_values must have compatible shapes.  Shape at index 0: \",\n          expected_value_shape, \".  Shape at index \", i, \": \", value_shape,\n          \".  If you are using tf.map_fn, then you may need to specify an \"\n          \"explicit fn_output_signature with appropriate ragged_rank, and/or \"\n          \"convert output tensors to RaggedTensors.\");\n    }\n\n    auto component_values_flat =\n        ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n    int num_inner_elements = ragged_components[i].values().NumElements();\n    if (ragged_components[i].values().dim_size(0) > 0) {\n      num_inner_elements /= ragged_components[i].values().dim_size(0);\n    }\n    for (int j = 0; j < ragged_components[i].values().dim_size(0);\n         j++, values_index++) {\n      for (int k = 0; k < num_inner_elements; k++) {\n        output_values_flat(values_index, k) = component_values_flat(j, k);\n      }\n    }\n  }\n  return Status::OK();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -95,7 +95,23 @@\n   auto output_values_flat =\n       output_ragged->mutable_values()->flat_outer_dims<VALUE_TYPE, 2>();\n   int values_index = 0;\n+\n+  TensorShape expected_value_shape = component_values_shape;\n+  expected_value_shape.RemoveDim(0);\n+\n   for (int i = 0; i < ragged_components.size(); i++) {\n+    // Check that the flat_values tensor shape is compatible.\n+    TensorShape value_shape = ragged_components[i].values().shape();\n+    value_shape.RemoveDim(0);\n+    if (value_shape != expected_value_shape) {\n+      return errors::InvalidArgument(\n+          \"All flat_values must have compatible shapes.  Shape at index 0: \",\n+          expected_value_shape, \".  Shape at index \", i, \": \", value_shape,\n+          \".  If you are using tf.map_fn, then you may need to specify an \"\n+          \"explicit fn_output_signature with appropriate ragged_rank, and/or \"\n+          \"convert output tensors to RaggedTensors.\");\n+    }\n+\n     auto component_values_flat =\n         ragged_components[i].values().flat_outer_dims<VALUE_TYPE, 2>();\n     int num_inner_elements = ragged_components[i].values().NumElements();",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  TensorShape expected_value_shape = component_values_shape;",
                "  expected_value_shape.RemoveDim(0);",
                "",
                "    // Check that the flat_values tensor shape is compatible.",
                "    TensorShape value_shape = ragged_components[i].values().shape();",
                "    value_shape.RemoveDim(0);",
                "    if (value_shape != expected_value_shape) {",
                "      return errors::InvalidArgument(",
                "          \"All flat_values must have compatible shapes.  Shape at index 0: \",",
                "          expected_value_shape, \".  Shape at index \", i, \": \", value_shape,",
                "          \".  If you are using tf.map_fn, then you may need to specify an \"",
                "          \"explicit fn_output_signature with appropriate ragged_rank, and/or \"",
                "          \"convert output tensors to RaggedTensors.\");",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-5251",
        "func_name": "libming/readSBits",
        "description": "In libming 0.4.8, there is an integer signedness error vulnerability (left shift of a negative value) in the readSBits function (util/read.c). Remote attackers can leverage this vulnerability to cause a denial of service via a crafted swf file.",
        "git_url": "https://github.com/libming/libming/commit/9141f1df0d6ecb84f298633ba03569bbf5c842d0",
        "commit_title": "Fix left shift of a negative value in readSBits.",
        "commit_text": " Check for !number before left-shifting by (number-1).  This commit fixes CVE-2018-5294 (fixes #97).",
        "func_before": "int readSBits(FILE *f, int number)\n{\n  int num = readBits(f, number);\n\n  if(num & (1<<(number-1)))\n    return num - (1<<number);\n  else\n    return num;\n}",
        "func": "int readSBits(FILE *f, int number)\n{\n  int num = readBits(f, number);\n\n  if(number && num & (1<<(number-1)))\n    return num - (1<<number);\n  else\n    return num;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,7 @@\n {\n   int num = readBits(f, number);\n \n-  if(num & (1<<(number-1)))\n+  if(number && num & (1<<(number-1)))\n     return num - (1<<number);\n   else\n     return num;",
        "diff_line_info": {
            "deleted_lines": [
                "  if(num & (1<<(number-1)))"
            ],
            "added_lines": [
                "  if(number && num & (1<<(number-1)))"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_MD",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_MD( INS_ARG )\n  {\n    FT_UShort   K, L;\n    FT_F26Dot6  D;\n\n\n    K = (FT_UShort)args[1];\n    L = (FT_UShort)args[0];\n\n    if( BOUNDS( L, CUR.zp0.n_points ) ||\n        BOUNDS( K, CUR.zp1.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n      {\n        CUR.error = TT_Err_Invalid_Reference;\n        return;\n      }\n      D = 0;\n    }\n    else\n    {\n      if ( CUR.opcode & 1 )\n        D = CUR_Func_project( CUR.zp0.cur + L, CUR.zp1.cur + K );\n      else\n      {\n        FT_Vector*  vec1 = CUR.zp0.orus + L;\n        FT_Vector*  vec2 = CUR.zp1.orus + K;\n\n\n        if ( CUR.metrics.x_scale == CUR.metrics.y_scale )\n        {\n          /* this should be faster */\n          D = CUR_Func_dualproj( vec1, vec2 );\n          D = TT_MULFIX( D, CUR.metrics.x_scale );\n        }\n        else\n        {\n          FT_Vector  vec;\n\n\n          vec.x = TT_MULFIX( vec1->x - vec2->x, CUR.metrics.x_scale );\n          vec.y = TT_MULFIX( vec1->y - vec2->y, CUR.metrics.y_scale );\n\n          D = CUR_fast_dualproj( &vec );\n        }\n      }\n    }\n\n    args[0] = D;\n  }",
        "func": "static void\n  Ins_MD( INS_ARG )\n  {\n    FT_UShort   K, L;\n    FT_F26Dot6  D;\n\n\n    K = (FT_UShort)args[1];\n    L = (FT_UShort)args[0];\n\n    if ( BOUNDS( L, CUR.zp0.n_points ) ||\n         BOUNDS( K, CUR.zp1.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n      {\n        CUR.error = TT_Err_Invalid_Reference;\n        return;\n      }\n      D = 0;\n    }\n    else\n    {\n      if ( CUR.opcode & 1 )\n        D = CUR_Func_project( CUR.zp0.cur + L, CUR.zp1.cur + K );\n      else\n      {\n        FT_Vector*  vec1 = CUR.zp0.orus + L;\n        FT_Vector*  vec2 = CUR.zp1.orus + K;\n\n\n        if ( CUR.metrics.x_scale == CUR.metrics.y_scale )\n        {\n          /* this should be faster */\n          D = CUR_Func_dualproj( vec1, vec2 );\n          D = TT_MULFIX( D, CUR.metrics.x_scale );\n        }\n        else\n        {\n          FT_Vector  vec;\n\n\n          vec.x = TT_MULFIX( vec1->x - vec2->x, CUR.metrics.x_scale );\n          vec.y = TT_MULFIX( vec1->y - vec2->y, CUR.metrics.y_scale );\n\n          D = CUR_fast_dualproj( &vec );\n        }\n      }\n    }\n\n    args[0] = D;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,8 +8,8 @@\n     K = (FT_UShort)args[1];\n     L = (FT_UShort)args[0];\n \n-    if( BOUNDS( L, CUR.zp0.n_points ) ||\n-        BOUNDS( K, CUR.zp1.n_points ) )\n+    if ( BOUNDS( L, CUR.zp0.n_points ) ||\n+         BOUNDS( K, CUR.zp1.n_points ) )\n     {\n       if ( CUR.pedantic_hinting )\n       {",
        "diff_line_info": {
            "deleted_lines": [
                "    if( BOUNDS( L, CUR.zp0.n_points ) ||",
                "        BOUNDS( K, CUR.zp1.n_points ) )"
            ],
            "added_lines": [
                "    if ( BOUNDS( L, CUR.zp0.n_points ) ||",
                "         BOUNDS( K, CUR.zp1.n_points ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_GC",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_GC( INS_ARG )\n  {\n    FT_ULong    L;\n    FT_F26Dot6  R;\n\n\n    L = (FT_ULong)args[0];\n\n    if ( BOUNDS( L, CUR.zp2.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n      {\n        CUR.error = TT_Err_Invalid_Reference;\n        return;\n      }\n      else\n        R = 0;\n    }\n    else\n    {\n      if ( CUR.opcode & 1 )\n        R = CUR_fast_dualproj( &CUR.zp2.org[L] );\n      else\n        R = CUR_fast_project( &CUR.zp2.cur[L] );\n    }\n\n    args[0] = R;\n  }",
        "func": "static void\n  Ins_GC( INS_ARG )\n  {\n    FT_ULong    L;\n    FT_F26Dot6  R;\n\n\n    L = (FT_ULong)args[0];\n\n    if ( BOUNDSL( L, CUR.zp2.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n      {\n        CUR.error = TT_Err_Invalid_Reference;\n        return;\n      }\n      else\n        R = 0;\n    }\n    else\n    {\n      if ( CUR.opcode & 1 )\n        R = CUR_fast_dualproj( &CUR.zp2.org[L] );\n      else\n        R = CUR_fast_project( &CUR.zp2.cur[L] );\n    }\n\n    args[0] = R;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \n     L = (FT_ULong)args[0];\n \n-    if ( BOUNDS( L, CUR.zp2.n_points ) )\n+    if ( BOUNDSL( L, CUR.zp2.n_points ) )\n     {\n       if ( CUR.pedantic_hinting )\n       {",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( BOUNDS( L, CUR.zp2.n_points ) )"
            ],
            "added_lines": [
                "    if ( BOUNDSL( L, CUR.zp2.n_points ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_DELTAC",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_DELTAC( INS_ARG )\n  {\n    FT_ULong  nump, k;\n    FT_ULong  A, C;\n    FT_Long   B;\n\n\n#ifdef TT_CONFIG_OPTION_UNPATENTED_HINTING\n    /* Delta hinting is covered by US Patent 5159668. */\n    if ( CUR.face->unpatented_hinting )\n    {\n      FT_Long  n = args[0] * 2;\n\n\n      if ( CUR.args < n )\n      {\n        CUR.error = TT_Err_Too_Few_Arguments;\n        return;\n      }\n\n      CUR.args -= n;\n      CUR.new_top = CUR.args;\n      return;\n    }\n#endif\n\n    nump = (FT_ULong)args[0];\n\n    for ( k = 1; k <= nump; k++ )\n    {\n      if ( CUR.args < 2 )\n      {\n        CUR.error = TT_Err_Too_Few_Arguments;\n        return;\n      }\n\n      CUR.args -= 2;\n\n      A = (FT_ULong)CUR.stack[CUR.args + 1];\n      B = CUR.stack[CUR.args];\n\n      if ( BOUNDS( A, CUR.cvtSize ) )\n      {\n        if ( CUR.pedantic_hinting )\n        {\n          CUR.error = TT_Err_Invalid_Reference;\n          return;\n        }\n      }\n      else\n      {\n        C = ( (FT_ULong)B & 0xF0 ) >> 4;\n\n        switch ( CUR.opcode )\n        {\n        case 0x73:\n          break;\n\n        case 0x74:\n          C += 16;\n          break;\n\n        case 0x75:\n          C += 32;\n          break;\n        }\n\n        C += CUR.GS.delta_base;\n\n        if ( CURRENT_Ppem() == (FT_Long)C )\n        {\n          B = ( (FT_ULong)B & 0xF ) - 8;\n          if ( B >= 0 )\n            B++;\n          B = B * 64 / ( 1L << CUR.GS.delta_shift );\n\n          CUR_Func_move_cvt( A, B );\n        }\n      }\n    }\n\n    CUR.new_top = CUR.args;\n  }",
        "func": "static void\n  Ins_DELTAC( INS_ARG )\n  {\n    FT_ULong  nump, k;\n    FT_ULong  A, C;\n    FT_Long   B;\n\n\n#ifdef TT_CONFIG_OPTION_UNPATENTED_HINTING\n    /* Delta hinting is covered by US Patent 5159668. */\n    if ( CUR.face->unpatented_hinting )\n    {\n      FT_Long  n = args[0] * 2;\n\n\n      if ( CUR.args < n )\n      {\n        CUR.error = TT_Err_Too_Few_Arguments;\n        return;\n      }\n\n      CUR.args -= n;\n      CUR.new_top = CUR.args;\n      return;\n    }\n#endif\n\n    nump = (FT_ULong)args[0];\n\n    for ( k = 1; k <= nump; k++ )\n    {\n      if ( CUR.args < 2 )\n      {\n        CUR.error = TT_Err_Too_Few_Arguments;\n        return;\n      }\n\n      CUR.args -= 2;\n\n      A = (FT_ULong)CUR.stack[CUR.args + 1];\n      B = CUR.stack[CUR.args];\n\n      if ( BOUNDSL( A, CUR.cvtSize ) )\n      {\n        if ( CUR.pedantic_hinting )\n        {\n          CUR.error = TT_Err_Invalid_Reference;\n          return;\n        }\n      }\n      else\n      {\n        C = ( (FT_ULong)B & 0xF0 ) >> 4;\n\n        switch ( CUR.opcode )\n        {\n        case 0x73:\n          break;\n\n        case 0x74:\n          C += 16;\n          break;\n\n        case 0x75:\n          C += 32;\n          break;\n        }\n\n        C += CUR.GS.delta_base;\n\n        if ( CURRENT_Ppem() == (FT_Long)C )\n        {\n          B = ( (FT_ULong)B & 0xF ) - 8;\n          if ( B >= 0 )\n            B++;\n          B = B * 64 / ( 1L << CUR.GS.delta_shift );\n\n          CUR_Func_move_cvt( A, B );\n        }\n      }\n    }\n\n    CUR.new_top = CUR.args;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,7 +40,7 @@\n       A = (FT_ULong)CUR.stack[CUR.args + 1];\n       B = CUR.stack[CUR.args];\n \n-      if ( BOUNDS( A, CUR.cvtSize ) )\n+      if ( BOUNDSL( A, CUR.cvtSize ) )\n       {\n         if ( CUR.pedantic_hinting )\n         {",
        "diff_line_info": {
            "deleted_lines": [
                "      if ( BOUNDS( A, CUR.cvtSize ) )"
            ],
            "added_lines": [
                "      if ( BOUNDSL( A, CUR.cvtSize ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_LOOPCALL",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_LOOPCALL( INS_ARG )\n  {\n    FT_ULong       F;\n    TT_CallRec*    pCrec;\n    TT_DefRecord*  def;\n\n\n    /* first of all, check the index */\n    F = args[1];\n    if ( BOUNDS( F, CUR.maxFunc + 1 ) )\n      goto Fail;\n\n    /* Except for some old Apple fonts, all functions in a TrueType */\n    /* font are defined in increasing order, starting from 0.  This */\n    /* means that we normally have                                  */\n    /*                                                              */\n    /*    CUR.maxFunc+1 == CUR.numFDefs                             */\n    /*    CUR.FDefs[n].opc == n for n in 0..CUR.maxFunc             */\n    /*                                                              */\n    /* If this isn't true, we need to look up the function table.   */\n\n    def = CUR.FDefs + F;\n    if ( CUR.maxFunc + 1 != CUR.numFDefs || def->opc != F )\n    {\n      /* look up the FDefs table */\n      TT_DefRecord*  limit;\n\n\n      def   = CUR.FDefs;\n      limit = def + CUR.numFDefs;\n\n      while ( def < limit && def->opc != F )\n        def++;\n\n      if ( def == limit )\n        goto Fail;\n    }\n\n    /* check that the function is active */\n    if ( !def->active )\n      goto Fail;\n\n    /* check stack */\n    if ( CUR.callTop >= CUR.callSize )\n    {\n      CUR.error = TT_Err_Stack_Overflow;\n      return;\n    }\n\n    if ( args[0] > 0 )\n    {\n      pCrec = CUR.callStack + CUR.callTop;\n\n      pCrec->Caller_Range = CUR.curRange;\n      pCrec->Caller_IP    = CUR.IP + 1;\n      pCrec->Cur_Count    = (FT_Int)args[0];\n      pCrec->Cur_Restart  = def->start;\n\n      CUR.callTop++;\n\n      INS_Goto_CodeRange( def->range, def->start );\n\n      CUR.step_ins = FALSE;\n    }\n    return;\n\n  Fail:\n    CUR.error = TT_Err_Invalid_Reference;\n  }",
        "func": "static void\n  Ins_LOOPCALL( INS_ARG )\n  {\n    FT_ULong       F;\n    TT_CallRec*    pCrec;\n    TT_DefRecord*  def;\n\n\n    /* first of all, check the index */\n    F = args[1];\n    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )\n      goto Fail;\n\n    /* Except for some old Apple fonts, all functions in a TrueType */\n    /* font are defined in increasing order, starting from 0.  This */\n    /* means that we normally have                                  */\n    /*                                                              */\n    /*    CUR.maxFunc+1 == CUR.numFDefs                             */\n    /*    CUR.FDefs[n].opc == n for n in 0..CUR.maxFunc             */\n    /*                                                              */\n    /* If this isn't true, we need to look up the function table.   */\n\n    def = CUR.FDefs + F;\n    if ( CUR.maxFunc + 1 != CUR.numFDefs || def->opc != F )\n    {\n      /* look up the FDefs table */\n      TT_DefRecord*  limit;\n\n\n      def   = CUR.FDefs;\n      limit = def + CUR.numFDefs;\n\n      while ( def < limit && def->opc != F )\n        def++;\n\n      if ( def == limit )\n        goto Fail;\n    }\n\n    /* check that the function is active */\n    if ( !def->active )\n      goto Fail;\n\n    /* check stack */\n    if ( CUR.callTop >= CUR.callSize )\n    {\n      CUR.error = TT_Err_Stack_Overflow;\n      return;\n    }\n\n    if ( args[0] > 0 )\n    {\n      pCrec = CUR.callStack + CUR.callTop;\n\n      pCrec->Caller_Range = CUR.curRange;\n      pCrec->Caller_IP    = CUR.IP + 1;\n      pCrec->Cur_Count    = (FT_Int)args[0];\n      pCrec->Cur_Restart  = def->start;\n\n      CUR.callTop++;\n\n      INS_Goto_CodeRange( def->range, def->start );\n\n      CUR.step_ins = FALSE;\n    }\n    return;\n\n  Fail:\n    CUR.error = TT_Err_Invalid_Reference;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n \n     /* first of all, check the index */\n     F = args[1];\n-    if ( BOUNDS( F, CUR.maxFunc + 1 ) )\n+    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )\n       goto Fail;\n \n     /* Except for some old Apple fonts, all functions in a TrueType */",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( BOUNDS( F, CUR.maxFunc + 1 ) )"
            ],
            "added_lines": [
                "    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_ALIGNPTS",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_ALIGNPTS( INS_ARG )\n  {\n    FT_UShort   p1, p2;\n    FT_F26Dot6  distance;\n\n\n    p1 = (FT_UShort)args[0];\n    p2 = (FT_UShort)args[1];\n\n    if ( BOUNDS( args[0], CUR.zp1.n_points ) ||\n         BOUNDS( args[1], CUR.zp0.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    distance = CUR_Func_project( CUR.zp0.cur + p2,\n                                 CUR.zp1.cur + p1 ) / 2;\n\n    CUR_Func_move( &CUR.zp1, p1, distance );\n    CUR_Func_move( &CUR.zp0, p2, -distance );\n  }",
        "func": "static void\n  Ins_ALIGNPTS( INS_ARG )\n  {\n    FT_UShort   p1, p2;\n    FT_F26Dot6  distance;\n\n\n    p1 = (FT_UShort)args[0];\n    p2 = (FT_UShort)args[1];\n\n    if ( BOUNDS( p1, CUR.zp1.n_points ) ||\n         BOUNDS( p2, CUR.zp0.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    distance = CUR_Func_project( CUR.zp0.cur + p2,\n                                 CUR.zp1.cur + p1 ) / 2;\n\n    CUR_Func_move( &CUR.zp1, p1, distance );\n    CUR_Func_move( &CUR.zp0, p2, -distance );\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,8 +8,8 @@\n     p1 = (FT_UShort)args[0];\n     p2 = (FT_UShort)args[1];\n \n-    if ( BOUNDS( args[0], CUR.zp1.n_points ) ||\n-         BOUNDS( args[1], CUR.zp0.n_points ) )\n+    if ( BOUNDS( p1, CUR.zp1.n_points ) ||\n+         BOUNDS( p2, CUR.zp0.n_points ) )\n     {\n       if ( CUR.pedantic_hinting )\n         CUR.error = TT_Err_Invalid_Reference;",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( BOUNDS( args[0], CUR.zp1.n_points ) ||",
                "         BOUNDS( args[1], CUR.zp0.n_points ) )"
            ],
            "added_lines": [
                "    if ( BOUNDS( p1, CUR.zp1.n_points ) ||",
                "         BOUNDS( p2, CUR.zp0.n_points ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_MIAP",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_MIAP( INS_ARG )\n  {\n    FT_ULong    cvtEntry;\n    FT_UShort   point;\n    FT_F26Dot6  distance,\n                org_dist;\n\n\n    cvtEntry = (FT_ULong)args[1];\n    point    = (FT_UShort)args[0];\n\n    if ( BOUNDS( point,    CUR.zp0.n_points ) ||\n         BOUNDS( cvtEntry, CUR.cvtSize )      )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    /* XXX: UNDOCUMENTED!                                */\n    /*                                                   */\n    /* The behaviour of an MIAP instruction is quite     */\n    /* different when used in the twilight zone.         */\n    /*                                                   */\n    /* First, no control value cut-in test is performed  */\n    /* as it would fail anyway.  Second, the original    */\n    /* point, i.e. (org_x,org_y) of zp0.point, is set    */\n    /* to the absolute, unrounded distance found in      */\n    /* the CVT.                                          */\n    /*                                                   */\n    /* This is used in the CVT programs of the Microsoft */\n    /* fonts Arial, Times, etc., in order to re-adjust   */\n    /* some key font heights.  It allows the use of the  */\n    /* IP instruction in the twilight zone, which        */\n    /* otherwise would be `illegal' according to the     */\n    /* specification.                                    */\n    /*                                                   */\n    /* We implement it with a special sequence for the   */\n    /* twilight zone.  This is a bad hack, but it seems  */\n    /* to work.                                          */\n\n    distance = CUR_Func_read_cvt( cvtEntry );\n\n    if ( CUR.GS.gep0 == 0 )   /* If in twilight zone */\n    {\n      CUR.zp0.org[point].x = TT_MulFix14( (FT_UInt32)distance, CUR.GS.freeVector.x );\n      CUR.zp0.org[point].y = TT_MulFix14( (FT_UInt32)distance, CUR.GS.freeVector.y ),\n      CUR.zp0.cur[point]   = CUR.zp0.org[point];\n    }\n\n    org_dist = CUR_fast_project( &CUR.zp0.cur[point] );\n\n    if ( ( CUR.opcode & 1 ) != 0 )   /* rounding and control cutin flag */\n    {\n      if ( FT_ABS( distance - org_dist ) > CUR.GS.control_value_cutin )\n        distance = org_dist;\n\n      distance = CUR_Func_round( distance, CUR.tt_metrics.compensations[0] );\n    }\n\n    CUR_Func_move( &CUR.zp0, point, distance - org_dist );\n\n    CUR.GS.rp0 = point;\n    CUR.GS.rp1 = point;\n  }",
        "func": "static void\n  Ins_MIAP( INS_ARG )\n  {\n    FT_ULong    cvtEntry;\n    FT_UShort   point;\n    FT_F26Dot6  distance,\n                org_dist;\n\n\n    cvtEntry = (FT_ULong)args[1];\n    point    = (FT_UShort)args[0];\n\n    if ( BOUNDS( point,     CUR.zp0.n_points ) ||\n         BOUNDSL( cvtEntry, CUR.cvtSize )      )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    /* XXX: UNDOCUMENTED!                                */\n    /*                                                   */\n    /* The behaviour of an MIAP instruction is quite     */\n    /* different when used in the twilight zone.         */\n    /*                                                   */\n    /* First, no control value cut-in test is performed  */\n    /* as it would fail anyway.  Second, the original    */\n    /* point, i.e. (org_x,org_y) of zp0.point, is set    */\n    /* to the absolute, unrounded distance found in      */\n    /* the CVT.                                          */\n    /*                                                   */\n    /* This is used in the CVT programs of the Microsoft */\n    /* fonts Arial, Times, etc., in order to re-adjust   */\n    /* some key font heights.  It allows the use of the  */\n    /* IP instruction in the twilight zone, which        */\n    /* otherwise would be `illegal' according to the     */\n    /* specification.                                    */\n    /*                                                   */\n    /* We implement it with a special sequence for the   */\n    /* twilight zone.  This is a bad hack, but it seems  */\n    /* to work.                                          */\n\n    distance = CUR_Func_read_cvt( cvtEntry );\n\n    if ( CUR.GS.gep0 == 0 )   /* If in twilight zone */\n    {\n      CUR.zp0.org[point].x = TT_MulFix14( (FT_UInt32)distance, CUR.GS.freeVector.x );\n      CUR.zp0.org[point].y = TT_MulFix14( (FT_UInt32)distance, CUR.GS.freeVector.y ),\n      CUR.zp0.cur[point]   = CUR.zp0.org[point];\n    }\n\n    org_dist = CUR_fast_project( &CUR.zp0.cur[point] );\n\n    if ( ( CUR.opcode & 1 ) != 0 )   /* rounding and control cutin flag */\n    {\n      if ( FT_ABS( distance - org_dist ) > CUR.GS.control_value_cutin )\n        distance = org_dist;\n\n      distance = CUR_Func_round( distance, CUR.tt_metrics.compensations[0] );\n    }\n\n    CUR_Func_move( &CUR.zp0, point, distance - org_dist );\n\n    CUR.GS.rp0 = point;\n    CUR.GS.rp1 = point;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,8 @@\n     cvtEntry = (FT_ULong)args[1];\n     point    = (FT_UShort)args[0];\n \n-    if ( BOUNDS( point,    CUR.zp0.n_points ) ||\n-         BOUNDS( cvtEntry, CUR.cvtSize )      )\n+    if ( BOUNDS( point,     CUR.zp0.n_points ) ||\n+         BOUNDSL( cvtEntry, CUR.cvtSize )      )\n     {\n       if ( CUR.pedantic_hinting )\n         CUR.error = TT_Err_Invalid_Reference;",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( BOUNDS( point,    CUR.zp0.n_points ) ||",
                "         BOUNDS( cvtEntry, CUR.cvtSize )      )"
            ],
            "added_lines": [
                "    if ( BOUNDS( point,     CUR.zp0.n_points ) ||",
                "         BOUNDSL( cvtEntry, CUR.cvtSize )      )"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_SHZ",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_SHZ( INS_ARG )\n  {\n    TT_GlyphZoneRec zp;\n    FT_UShort       refp;\n    FT_F26Dot6      dx,\n                    dy;\n\n    FT_UShort       last_point, i;\n\n\n    if ( BOUNDS( args[0], 2 ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    if ( COMPUTE_Point_Displacement( &dx, &dy, &zp, &refp ) )\n      return;\n\n    /* XXX: UNDOCUMENTED! SHZ doesn't move the phantom points.  */\n    /*      Twilight zone has no contours, so use `n_points'.   */\n    /*      Normal zone's `n_points' includes phantoms, so must */\n    /*      use end of last contour.                            */\n    if ( CUR.GS.gep2 == 0 && CUR.zp2.n_points > 0 )\n      last_point = (FT_UShort)( CUR.zp2.n_points - 1 );\n    else if ( CUR.GS.gep2 == 1 && CUR.zp2.n_contours > 0 )\n      last_point = (FT_UShort)( CUR.zp2.contours[CUR.zp2.n_contours - 1] );\n    else\n      last_point = 0;\n\n    /* XXX: UNDOCUMENTED! SHZ doesn't touch the points */\n    for ( i = 0; i <= last_point; i++ )\n    {\n      if ( zp.cur != CUR.zp2.cur || refp != i )\n        MOVE_Zp2_Point( i, dx, dy, FALSE );\n    }\n  }",
        "func": "static void\n  Ins_SHZ( INS_ARG )\n  {\n    TT_GlyphZoneRec  zp;\n    FT_UShort        refp;\n    FT_F26Dot6       dx,\n                     dy;\n\n    FT_UShort        last_point, i;\n\n\n    if ( BOUNDS( args[0], 2 ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    if ( COMPUTE_Point_Displacement( &dx, &dy, &zp, &refp ) )\n      return;\n\n    /* XXX: UNDOCUMENTED! SHZ doesn't move the phantom points.  */\n    /*      Twilight zone has no contours, so use `n_points'.   */\n    /*      Normal zone's `n_points' includes phantoms, so must */\n    /*      use end of last contour.                            */\n    if ( CUR.GS.gep2 == 0 && CUR.zp2.n_points > 0 )\n      last_point = (FT_UShort)( CUR.zp2.n_points - 1 );\n    else if ( CUR.GS.gep2 == 1 && CUR.zp2.n_contours > 0 )\n      last_point = (FT_UShort)( CUR.zp2.contours[CUR.zp2.n_contours - 1] );\n    else\n      last_point = 0;\n\n    /* XXX: UNDOCUMENTED! SHZ doesn't touch the points */\n    for ( i = 0; i <= last_point; i++ )\n    {\n      if ( zp.cur != CUR.zp2.cur || refp != i )\n        MOVE_Zp2_Point( i, dx, dy, FALSE );\n    }\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,12 @@\n static void\n   Ins_SHZ( INS_ARG )\n   {\n-    TT_GlyphZoneRec zp;\n-    FT_UShort       refp;\n-    FT_F26Dot6      dx,\n-                    dy;\n+    TT_GlyphZoneRec  zp;\n+    FT_UShort        refp;\n+    FT_F26Dot6       dx,\n+                     dy;\n \n-    FT_UShort       last_point, i;\n+    FT_UShort        last_point, i;\n \n \n     if ( BOUNDS( args[0], 2 ) )",
        "diff_line_info": {
            "deleted_lines": [
                "    TT_GlyphZoneRec zp;",
                "    FT_UShort       refp;",
                "    FT_F26Dot6      dx,",
                "                    dy;",
                "    FT_UShort       last_point, i;"
            ],
            "added_lines": [
                "    TT_GlyphZoneRec  zp;",
                "    FT_UShort        refp;",
                "    FT_F26Dot6       dx,",
                "                     dy;",
                "    FT_UShort        last_point, i;"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_MIRP",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_MIRP( INS_ARG )\n  {\n    FT_UShort   point;\n    FT_ULong    cvtEntry;\n\n    FT_F26Dot6  cvt_dist,\n                distance,\n                cur_dist,\n                org_dist;\n\n\n    point    = (FT_UShort)args[0];\n    cvtEntry = (FT_ULong)( args[1] + 1 );\n\n    /* XXX: UNDOCUMENTED! cvt[-1] = 0 always */\n\n    if ( BOUNDS( point,      CUR.zp1.n_points ) ||\n         BOUNDS( cvtEntry,   CUR.cvtSize + 1 )  ||\n         BOUNDS( CUR.GS.rp0, CUR.zp0.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    if ( !cvtEntry )\n      cvt_dist = 0;\n    else\n      cvt_dist = CUR_Func_read_cvt( cvtEntry - 1 );\n\n    /* single width test */\n\n    if ( FT_ABS( cvt_dist - CUR.GS.single_width_value ) <\n         CUR.GS.single_width_cutin )\n    {\n      if ( cvt_dist >= 0 )\n        cvt_dist =  CUR.GS.single_width_value;\n      else\n        cvt_dist = -CUR.GS.single_width_value;\n    }\n\n    /* XXX: UNDOCUMENTED! -- twilight zone */\n\n    if ( CUR.GS.gep1 == 0 )\n    {\n      CUR.zp1.org[point].x = CUR.zp0.org[CUR.GS.rp0].x +\n                             TT_MulFix14( (FT_UInt32)cvt_dist,\n                                          CUR.GS.freeVector.x );\n\n      CUR.zp1.org[point].y = CUR.zp0.org[CUR.GS.rp0].y +\n                             TT_MulFix14( (FT_UInt32)cvt_dist,\n                                          CUR.GS.freeVector.y );\n\n      CUR.zp1.cur[point] = CUR.zp0.cur[point];\n    }\n\n    org_dist = CUR_Func_dualproj( &CUR.zp1.org[point],\n                                  &CUR.zp0.org[CUR.GS.rp0] );\n    cur_dist = CUR_Func_project ( &CUR.zp1.cur[point],\n                                  &CUR.zp0.cur[CUR.GS.rp0] );\n\n    /* auto-flip test */\n\n    if ( CUR.GS.auto_flip )\n    {\n      if ( ( org_dist ^ cvt_dist ) < 0 )\n        cvt_dist = -cvt_dist;\n    }\n\n    /* control value cutin and round */\n\n    if ( ( CUR.opcode & 4 ) != 0 )\n    {\n      /* XXX: UNDOCUMENTED!  Only perform cut-in test when both points */\n      /*      refer to the same zone.                                  */\n\n      if ( CUR.GS.gep0 == CUR.GS.gep1 )\n        if ( FT_ABS( cvt_dist - org_dist ) >= CUR.GS.control_value_cutin )\n          cvt_dist = org_dist;\n\n      distance = CUR_Func_round(\n                   cvt_dist,\n                   CUR.tt_metrics.compensations[CUR.opcode & 3] );\n    }\n    else\n      distance = ROUND_None(\n                   cvt_dist,\n                   CUR.tt_metrics.compensations[CUR.opcode & 3] );\n\n    /* minimum distance test */\n\n    if ( ( CUR.opcode & 8 ) != 0 )\n    {\n      if ( org_dist >= 0 )\n      {\n        if ( distance < CUR.GS.minimum_distance )\n          distance = CUR.GS.minimum_distance;\n      }\n      else\n      {\n        if ( distance > -CUR.GS.minimum_distance )\n          distance = -CUR.GS.minimum_distance;\n      }\n    }\n\n    CUR_Func_move( &CUR.zp1, point, distance - cur_dist );\n\n    CUR.GS.rp1 = CUR.GS.rp0;\n\n    if ( ( CUR.opcode & 16 ) != 0 )\n      CUR.GS.rp0 = point;\n\n    /* XXX: UNDOCUMENTED! */\n    CUR.GS.rp2 = point;\n  }",
        "func": "static void\n  Ins_MIRP( INS_ARG )\n  {\n    FT_UShort   point;\n    FT_ULong    cvtEntry;\n\n    FT_F26Dot6  cvt_dist,\n                distance,\n                cur_dist,\n                org_dist;\n\n\n    point    = (FT_UShort)args[0];\n    cvtEntry = (FT_ULong)( args[1] + 1 );\n\n    /* XXX: UNDOCUMENTED! cvt[-1] = 0 always */\n\n    if ( BOUNDS( point,      CUR.zp1.n_points ) ||\n         BOUNDSL( cvtEntry,  CUR.cvtSize + 1 )  ||\n         BOUNDS( CUR.GS.rp0, CUR.zp0.n_points ) )\n    {\n      if ( CUR.pedantic_hinting )\n        CUR.error = TT_Err_Invalid_Reference;\n      return;\n    }\n\n    if ( !cvtEntry )\n      cvt_dist = 0;\n    else\n      cvt_dist = CUR_Func_read_cvt( cvtEntry - 1 );\n\n    /* single width test */\n\n    if ( FT_ABS( cvt_dist - CUR.GS.single_width_value ) <\n         CUR.GS.single_width_cutin )\n    {\n      if ( cvt_dist >= 0 )\n        cvt_dist =  CUR.GS.single_width_value;\n      else\n        cvt_dist = -CUR.GS.single_width_value;\n    }\n\n    /* XXX: UNDOCUMENTED! -- twilight zone */\n\n    if ( CUR.GS.gep1 == 0 )\n    {\n      CUR.zp1.org[point].x = CUR.zp0.org[CUR.GS.rp0].x +\n                             TT_MulFix14( (FT_UInt32)cvt_dist,\n                                          CUR.GS.freeVector.x );\n\n      CUR.zp1.org[point].y = CUR.zp0.org[CUR.GS.rp0].y +\n                             TT_MulFix14( (FT_UInt32)cvt_dist,\n                                          CUR.GS.freeVector.y );\n\n      CUR.zp1.cur[point] = CUR.zp0.cur[point];\n    }\n\n    org_dist = CUR_Func_dualproj( &CUR.zp1.org[point],\n                                  &CUR.zp0.org[CUR.GS.rp0] );\n    cur_dist = CUR_Func_project ( &CUR.zp1.cur[point],\n                                  &CUR.zp0.cur[CUR.GS.rp0] );\n\n    /* auto-flip test */\n\n    if ( CUR.GS.auto_flip )\n    {\n      if ( ( org_dist ^ cvt_dist ) < 0 )\n        cvt_dist = -cvt_dist;\n    }\n\n    /* control value cutin and round */\n\n    if ( ( CUR.opcode & 4 ) != 0 )\n    {\n      /* XXX: UNDOCUMENTED!  Only perform cut-in test when both points */\n      /*      refer to the same zone.                                  */\n\n      if ( CUR.GS.gep0 == CUR.GS.gep1 )\n        if ( FT_ABS( cvt_dist - org_dist ) >= CUR.GS.control_value_cutin )\n          cvt_dist = org_dist;\n\n      distance = CUR_Func_round(\n                   cvt_dist,\n                   CUR.tt_metrics.compensations[CUR.opcode & 3] );\n    }\n    else\n      distance = ROUND_None(\n                   cvt_dist,\n                   CUR.tt_metrics.compensations[CUR.opcode & 3] );\n\n    /* minimum distance test */\n\n    if ( ( CUR.opcode & 8 ) != 0 )\n    {\n      if ( org_dist >= 0 )\n      {\n        if ( distance < CUR.GS.minimum_distance )\n          distance = CUR.GS.minimum_distance;\n      }\n      else\n      {\n        if ( distance > -CUR.GS.minimum_distance )\n          distance = -CUR.GS.minimum_distance;\n      }\n    }\n\n    CUR_Func_move( &CUR.zp1, point, distance - cur_dist );\n\n    CUR.GS.rp1 = CUR.GS.rp0;\n\n    if ( ( CUR.opcode & 16 ) != 0 )\n      CUR.GS.rp0 = point;\n\n    /* XXX: UNDOCUMENTED! */\n    CUR.GS.rp2 = point;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,7 +16,7 @@\n     /* XXX: UNDOCUMENTED! cvt[-1] = 0 always */\n \n     if ( BOUNDS( point,      CUR.zp1.n_points ) ||\n-         BOUNDS( cvtEntry,   CUR.cvtSize + 1 )  ||\n+         BOUNDSL( cvtEntry,  CUR.cvtSize + 1 )  ||\n          BOUNDS( CUR.GS.rp0, CUR.zp0.n_points ) )\n     {\n       if ( CUR.pedantic_hinting )",
        "diff_line_info": {
            "deleted_lines": [
                "         BOUNDS( cvtEntry,   CUR.cvtSize + 1 )  ||"
            ],
            "added_lines": [
                "         BOUNDSL( cvtEntry,  CUR.cvtSize + 1 )  ||"
            ]
        }
    },
    {
        "cve_id": "CVE-2010-2807",
        "func_name": "freetype/freetype2/Ins_CALL",
        "description": "FreeType before 2.4.2 uses incorrect integer data types during bounds checking, which allows remote attackers to cause a denial of service (application crash) or possibly execute arbitrary code via a crafted font file.",
        "git_url": "http://git.savannah.gnu.org/cgit/freetype/freetype2.git/commit/?id=346f1867fd32dae8f56e5b482d1af98f626804ac",
        "commit_title": "* src/truetype/ttinterp.c (BOUNDSL): New macro.",
        "commit_text": "Change `BOUNDS' to `BOUNDSL' where appropriate.  * src/truetype/ttinterp.h (TT_ExecContextRec): Fix type of `cvtSize'. ",
        "func_before": "static void\n  Ins_CALL( INS_ARG )\n  {\n    FT_ULong       F;\n    TT_CallRec*    pCrec;\n    TT_DefRecord*  def;\n\n\n    /* first of all, check the index */\n\n    F = args[0];\n    if ( BOUNDS( F, CUR.maxFunc + 1 ) )\n      goto Fail;\n\n    /* Except for some old Apple fonts, all functions in a TrueType */\n    /* font are defined in increasing order, starting from 0.  This */\n    /* means that we normally have                                  */\n    /*                                                              */\n    /*    CUR.maxFunc+1 == CUR.numFDefs                             */\n    /*    CUR.FDefs[n].opc == n for n in 0..CUR.maxFunc             */\n    /*                                                              */\n    /* If this isn't true, we need to look up the function table.   */\n\n    def = CUR.FDefs + F;\n    if ( CUR.maxFunc + 1 != CUR.numFDefs || def->opc != F )\n    {\n      /* look up the FDefs table */\n      TT_DefRecord*  limit;\n\n\n      def   = CUR.FDefs;\n      limit = def + CUR.numFDefs;\n\n      while ( def < limit && def->opc != F )\n        def++;\n\n      if ( def == limit )\n        goto Fail;\n    }\n\n    /* check that the function is active */\n    if ( !def->active )\n      goto Fail;\n\n    /* check the call stack */\n    if ( CUR.callTop >= CUR.callSize )\n    {\n      CUR.error = TT_Err_Stack_Overflow;\n      return;\n    }\n\n    pCrec = CUR.callStack + CUR.callTop;\n\n    pCrec->Caller_Range = CUR.curRange;\n    pCrec->Caller_IP    = CUR.IP + 1;\n    pCrec->Cur_Count    = 1;\n    pCrec->Cur_Restart  = def->start;\n\n    CUR.callTop++;\n\n    INS_Goto_CodeRange( def->range,\n                        def->start );\n\n    CUR.step_ins = FALSE;\n    return;\n\n  Fail:\n    CUR.error = TT_Err_Invalid_Reference;\n  }",
        "func": "static void\n  Ins_CALL( INS_ARG )\n  {\n    FT_ULong       F;\n    TT_CallRec*    pCrec;\n    TT_DefRecord*  def;\n\n\n    /* first of all, check the index */\n\n    F = args[0];\n    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )\n      goto Fail;\n\n    /* Except for some old Apple fonts, all functions in a TrueType */\n    /* font are defined in increasing order, starting from 0.  This */\n    /* means that we normally have                                  */\n    /*                                                              */\n    /*    CUR.maxFunc+1 == CUR.numFDefs                             */\n    /*    CUR.FDefs[n].opc == n for n in 0..CUR.maxFunc             */\n    /*                                                              */\n    /* If this isn't true, we need to look up the function table.   */\n\n    def = CUR.FDefs + F;\n    if ( CUR.maxFunc + 1 != CUR.numFDefs || def->opc != F )\n    {\n      /* look up the FDefs table */\n      TT_DefRecord*  limit;\n\n\n      def   = CUR.FDefs;\n      limit = def + CUR.numFDefs;\n\n      while ( def < limit && def->opc != F )\n        def++;\n\n      if ( def == limit )\n        goto Fail;\n    }\n\n    /* check that the function is active */\n    if ( !def->active )\n      goto Fail;\n\n    /* check the call stack */\n    if ( CUR.callTop >= CUR.callSize )\n    {\n      CUR.error = TT_Err_Stack_Overflow;\n      return;\n    }\n\n    pCrec = CUR.callStack + CUR.callTop;\n\n    pCrec->Caller_Range = CUR.curRange;\n    pCrec->Caller_IP    = CUR.IP + 1;\n    pCrec->Cur_Count    = 1;\n    pCrec->Cur_Restart  = def->start;\n\n    CUR.callTop++;\n\n    INS_Goto_CodeRange( def->range,\n                        def->start );\n\n    CUR.step_ins = FALSE;\n    return;\n\n  Fail:\n    CUR.error = TT_Err_Invalid_Reference;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n     /* first of all, check the index */\n \n     F = args[0];\n-    if ( BOUNDS( F, CUR.maxFunc + 1 ) )\n+    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )\n       goto Fail;\n \n     /* Except for some old Apple fonts, all functions in a TrueType */",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( BOUNDS( F, CUR.maxFunc + 1 ) )"
            ],
            "added_lines": [
                "    if ( BOUNDSL( F, CUR.maxFunc + 1 ) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-16778",
        "func_name": "tensorflow/operator()",
        "description": "In TensorFlow before 1.15, a heap buffer overflow in UnsortedSegmentSum can be produced when the Index template argument is int32. In this case data_size and num_segments fields are truncated from int64 to int32 and can produce negative numbers, resulting in accessing out of bounds heap memory. This is unlikely to be exploitable and was detected and fixed internally in TensorFlow 1.15 and 2.0.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/db4f9717c41bccc3ce10099ab61996b246099892",
        "commit_title": "Fix heap buffer overflow in UnsortedSegmentSum.",
        "commit_text": " When Index=int32, data_size and num_segments were truncated from int64 to int32. This truncation can produce negative numbers, which causes UnsortedSegmentFunctor to access out of bounds memory.  Also: - Switches some indexing calculations to int64 to avoid signed integer overflow when either the input or output tensors have more than 2**31 - 1 elements. - Fixes a range check error in the GPU kernel. The segment ID was checked against an upper bound measured in elements, not segments. PiperOrigin-RevId: 256451663",
        "func_before": "void operator()(OpKernelContext* ctx, const Index num_segments,\n                  const TensorShape& segment_ids_shape,\n                  typename TTypes<Index>::ConstFlat segment_ids,\n                  const Index data_size, const T* data,\n                  typename TTypes<T, 2>::Tensor output) {\n    if (output.size() == 0) {\n      return;\n    }\n    // Set 'output' to initial value.\n    GPUDevice d = ctx->template eigen_device<GPUDevice>();\n    GpuLaunchConfig config = GetGpuLaunchConfig(output.size(), d);\n    TF_CHECK_OK(GpuLaunchKernel(\n        SetToValue<T>, config.block_count, config.thread_per_block, 0,\n        d.stream(), output.size(), output.data(), InitialValueF()()));\n    if (data_size == 0 || segment_ids_shape.num_elements() == 0) {\n      return;\n    }\n    // Launch kernel to compute unsorted segment reduction.\n    // Notes:\n    // *) 'data_size' is the total number of elements to process.\n    // *) 'segment_ids.shape' is a prefix of data's shape.\n    // *) 'input_outer_dim_size' is the total number of segments to process.\n    const Index input_outer_dim_size = segment_ids.dimension(0);\n    const Index input_inner_dim_size = data_size / input_outer_dim_size;\n    config = GetGpuLaunchConfig(data_size, d);\n\n    TF_CHECK_OK(\n        GpuLaunchKernel(UnsortedSegmentCustomKernel<T, Index, ReductionF>,\n                        config.block_count, config.thread_per_block, 0,\n                        d.stream(), input_outer_dim_size, input_inner_dim_size,\n                        num_segments, segment_ids.data(), data, output.data()));\n  }",
        "func": "void operator()(OpKernelContext* ctx, const TensorShape& segment_ids_shape,\n                  typename TTypes<Index>::ConstFlat segment_ids,\n                  typename TTypes<T, 2>::ConstTensor data,\n                  typename TTypes<T, 2>::Tensor output) {\n    if (output.size() == 0) {\n      return;\n    }\n    // Set 'output' to initial value.\n    GPUDevice d = ctx->template eigen_device<GPUDevice>();\n    GpuLaunchConfig config = GetGpuLaunchConfig(output.size(), d);\n    TF_CHECK_OK(GpuLaunchKernel(\n        SetToValue<T>, config.block_count, config.thread_per_block, 0,\n        d.stream(), output.size(), output.data(), InitialValueF()()));\n    const int64 data_size = data.size();\n    if (data_size == 0 || segment_ids_shape.num_elements() == 0) {\n      return;\n    }\n    // Launch kernel to compute unsorted segment reduction.\n    // Notes:\n    // *) 'data_size' is the total number of elements to process.\n    // *) 'segment_ids.shape' is a prefix of data's shape.\n    // *) 'input_outer_dim_size' is the total number of segments to process.\n    const int64 input_outer_dim_size = segment_ids.dimension(0);\n    const int64 input_inner_dim_size = data.dimension(1);\n    const int64 output_outer_dim_size = output.dimension(0);\n    config = GetGpuLaunchConfig(data_size, d);\n\n    TF_CHECK_OK(GpuLaunchKernel(\n        UnsortedSegmentCustomKernel<T, Index, ReductionF>, config.block_count,\n        config.thread_per_block, 0, d.stream(), input_outer_dim_size,\n        input_inner_dim_size, output_outer_dim_size, segment_ids.data(),\n        data.data(), output.data()));\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,6 @@\n-void operator()(OpKernelContext* ctx, const Index num_segments,\n-                  const TensorShape& segment_ids_shape,\n+void operator()(OpKernelContext* ctx, const TensorShape& segment_ids_shape,\n                   typename TTypes<Index>::ConstFlat segment_ids,\n-                  const Index data_size, const T* data,\n+                  typename TTypes<T, 2>::ConstTensor data,\n                   typename TTypes<T, 2>::Tensor output) {\n     if (output.size() == 0) {\n       return;\n@@ -12,6 +11,7 @@\n     TF_CHECK_OK(GpuLaunchKernel(\n         SetToValue<T>, config.block_count, config.thread_per_block, 0,\n         d.stream(), output.size(), output.data(), InitialValueF()()));\n+    const int64 data_size = data.size();\n     if (data_size == 0 || segment_ids_shape.num_elements() == 0) {\n       return;\n     }\n@@ -20,13 +20,14 @@\n     // *) 'data_size' is the total number of elements to process.\n     // *) 'segment_ids.shape' is a prefix of data's shape.\n     // *) 'input_outer_dim_size' is the total number of segments to process.\n-    const Index input_outer_dim_size = segment_ids.dimension(0);\n-    const Index input_inner_dim_size = data_size / input_outer_dim_size;\n+    const int64 input_outer_dim_size = segment_ids.dimension(0);\n+    const int64 input_inner_dim_size = data.dimension(1);\n+    const int64 output_outer_dim_size = output.dimension(0);\n     config = GetGpuLaunchConfig(data_size, d);\n \n-    TF_CHECK_OK(\n-        GpuLaunchKernel(UnsortedSegmentCustomKernel<T, Index, ReductionF>,\n-                        config.block_count, config.thread_per_block, 0,\n-                        d.stream(), input_outer_dim_size, input_inner_dim_size,\n-                        num_segments, segment_ids.data(), data, output.data()));\n+    TF_CHECK_OK(GpuLaunchKernel(\n+        UnsortedSegmentCustomKernel<T, Index, ReductionF>, config.block_count,\n+        config.thread_per_block, 0, d.stream(), input_outer_dim_size,\n+        input_inner_dim_size, output_outer_dim_size, segment_ids.data(),\n+        data.data(), output.data()));\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "void operator()(OpKernelContext* ctx, const Index num_segments,",
                "                  const TensorShape& segment_ids_shape,",
                "                  const Index data_size, const T* data,",
                "    const Index input_outer_dim_size = segment_ids.dimension(0);",
                "    const Index input_inner_dim_size = data_size / input_outer_dim_size;",
                "    TF_CHECK_OK(",
                "        GpuLaunchKernel(UnsortedSegmentCustomKernel<T, Index, ReductionF>,",
                "                        config.block_count, config.thread_per_block, 0,",
                "                        d.stream(), input_outer_dim_size, input_inner_dim_size,",
                "                        num_segments, segment_ids.data(), data, output.data()));"
            ],
            "added_lines": [
                "void operator()(OpKernelContext* ctx, const TensorShape& segment_ids_shape,",
                "                  typename TTypes<T, 2>::ConstTensor data,",
                "    const int64 data_size = data.size();",
                "    const int64 input_outer_dim_size = segment_ids.dimension(0);",
                "    const int64 input_inner_dim_size = data.dimension(1);",
                "    const int64 output_outer_dim_size = output.dimension(0);",
                "    TF_CHECK_OK(GpuLaunchKernel(",
                "        UnsortedSegmentCustomKernel<T, Index, ReductionF>, config.block_count,",
                "        config.thread_per_block, 0, d.stream(), input_outer_dim_size,",
                "        input_inner_dim_size, output_outer_dim_size, segment_ids.data(),",
                "        data.data(), output.data()));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-16778",
        "func_name": "tensorflow/UnsortedSegmentCustomKernel",
        "description": "In TensorFlow before 1.15, a heap buffer overflow in UnsortedSegmentSum can be produced when the Index template argument is int32. In this case data_size and num_segments fields are truncated from int64 to int32 and can produce negative numbers, resulting in accessing out of bounds heap memory. This is unlikely to be exploitable and was detected and fixed internally in TensorFlow 1.15 and 2.0.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/db4f9717c41bccc3ce10099ab61996b246099892",
        "commit_title": "Fix heap buffer overflow in UnsortedSegmentSum.",
        "commit_text": " When Index=int32, data_size and num_segments were truncated from int64 to int32. This truncation can produce negative numbers, which causes UnsortedSegmentFunctor to access out of bounds memory.  Also: - Switches some indexing calculations to int64 to avoid signed integer overflow when either the input or output tensors have more than 2**31 - 1 elements. - Fixes a range check error in the GPU kernel. The segment ID was checked against an upper bound measured in elements, not segments. PiperOrigin-RevId: 256451663",
        "func_before": "__global__ void UnsortedSegmentCustomKernel(const Index input_outer_dim_size,\n                                            const Index inner_dim_size,\n                                            const Index output_outer_dim_size,\n                                            const Index* segment_ids,\n                                            const T* input, T* output) {\n  const Index input_total_size = input_outer_dim_size * inner_dim_size;\n  const Index output_total_size = output_outer_dim_size * inner_dim_size;\n  for (int input_index : GpuGridRangeX(input_total_size)) {\n    const Index input_segment_index = input_index / inner_dim_size;\n    const Index segment_offset = input_index % inner_dim_size;\n    const Index output_segment_index = segment_ids[input_segment_index];\n    if (output_segment_index < 0 || output_segment_index >= output_total_size) {\n      continue;\n    }\n    const Index output_index =\n        output_segment_index * inner_dim_size + segment_offset;\n    KernelReductionFunctor()(output + output_index, ldg(input + input_index));\n  }\n}",
        "func": "__global__ void UnsortedSegmentCustomKernel(const int64 input_outer_dim_size,\n                                            const int64 inner_dim_size,\n                                            const int64 output_outer_dim_size,\n                                            const Index* segment_ids,\n                                            const T* input, T* output) {\n  const int64 input_total_size = input_outer_dim_size * inner_dim_size;\n  for (int64 input_index : GpuGridRangeX(input_total_size)) {\n    const int64 input_segment_index = input_index / inner_dim_size;\n    const int64 segment_offset = input_index % inner_dim_size;\n    const Index output_segment_index = segment_ids[input_segment_index];\n    if (output_segment_index < 0 ||\n        output_segment_index >= output_outer_dim_size) {\n      continue;\n    }\n    const int64 output_index =\n        output_segment_index * inner_dim_size + segment_offset;\n    KernelReductionFunctor()(output + output_index, ldg(input + input_index));\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,18 +1,18 @@\n-__global__ void UnsortedSegmentCustomKernel(const Index input_outer_dim_size,\n-                                            const Index inner_dim_size,\n-                                            const Index output_outer_dim_size,\n+__global__ void UnsortedSegmentCustomKernel(const int64 input_outer_dim_size,\n+                                            const int64 inner_dim_size,\n+                                            const int64 output_outer_dim_size,\n                                             const Index* segment_ids,\n                                             const T* input, T* output) {\n-  const Index input_total_size = input_outer_dim_size * inner_dim_size;\n-  const Index output_total_size = output_outer_dim_size * inner_dim_size;\n-  for (int input_index : GpuGridRangeX(input_total_size)) {\n-    const Index input_segment_index = input_index / inner_dim_size;\n-    const Index segment_offset = input_index % inner_dim_size;\n+  const int64 input_total_size = input_outer_dim_size * inner_dim_size;\n+  for (int64 input_index : GpuGridRangeX(input_total_size)) {\n+    const int64 input_segment_index = input_index / inner_dim_size;\n+    const int64 segment_offset = input_index % inner_dim_size;\n     const Index output_segment_index = segment_ids[input_segment_index];\n-    if (output_segment_index < 0 || output_segment_index >= output_total_size) {\n+    if (output_segment_index < 0 ||\n+        output_segment_index >= output_outer_dim_size) {\n       continue;\n     }\n-    const Index output_index =\n+    const int64 output_index =\n         output_segment_index * inner_dim_size + segment_offset;\n     KernelReductionFunctor()(output + output_index, ldg(input + input_index));\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "__global__ void UnsortedSegmentCustomKernel(const Index input_outer_dim_size,",
                "                                            const Index inner_dim_size,",
                "                                            const Index output_outer_dim_size,",
                "  const Index input_total_size = input_outer_dim_size * inner_dim_size;",
                "  const Index output_total_size = output_outer_dim_size * inner_dim_size;",
                "  for (int input_index : GpuGridRangeX(input_total_size)) {",
                "    const Index input_segment_index = input_index / inner_dim_size;",
                "    const Index segment_offset = input_index % inner_dim_size;",
                "    if (output_segment_index < 0 || output_segment_index >= output_total_size) {",
                "    const Index output_index ="
            ],
            "added_lines": [
                "__global__ void UnsortedSegmentCustomKernel(const int64 input_outer_dim_size,",
                "                                            const int64 inner_dim_size,",
                "                                            const int64 output_outer_dim_size,",
                "  const int64 input_total_size = input_outer_dim_size * inner_dim_size;",
                "  for (int64 input_index : GpuGridRangeX(input_total_size)) {",
                "    const int64 input_segment_index = input_index / inner_dim_size;",
                "    const int64 segment_offset = input_index % inner_dim_size;",
                "    if (output_segment_index < 0 ||",
                "        output_segment_index >= output_outer_dim_size) {",
                "    const int64 output_index ="
            ]
        }
    },
    {
        "cve_id": "CVE-2018-10887",
        "func_name": "libgit2/git_delta_apply",
        "description": "A flaw was found in libgit2 before version 0.27.3. It has been discovered that an unexpected sign extension in git_delta_apply function in delta.c file may lead to an integer overflow which in turn leads to an out of bound read, allowing to read before the base object. An attacker may use this flaw to leak memory addresses or cause a Denial of Service.",
        "git_url": "https://github.com/libgit2/libgit2/commit/3f461902dc1072acb8b7607ee65d0a0458ffac2a",
        "commit_title": "delta: fix sign-extension of big left-shift",
        "commit_text": " Our delta code was originally adapted from JGit, which itself adapted it from git itself. Due to this heritage, we inherited a bug from git.git in how we compute the delta offset, which was fixed upstream in 48fb7deb5 (Fix big left-shifts of unsigned char, 2009-06-17). As explained by Linus:      Shifting 'unsigned char' or 'unsigned short' left can result in sign     extension errors, since the C integer promotion rules means that the     unsigned char/short will get implicitly promoted to a signed 'int' due to     the shift (or due to other operations).      This normally doesn't matter, but if you shift things up sufficiently, it     will now set the sign bit in 'int', and a subsequent cast to a bigger type     (eg 'long' or 'unsigned long') will now sign-extend the value despite the     original expression being unsigned.      One example of this would be something like              unsigned long size;             unsigned char c;              size += c << 24;      where despite all the variables being unsigned, 'c << 24' ends up being a     signed entity, and will get sign-extended when then doing the addition in     an 'unsigned long' type.      Since git uses 'unsigned char' pointers extensively, we actually have this     bug in a couple of places.  In our delta code, we inherited such a bogus shift when computing the offset at which the delta base is to be found. Due to the sign extension we can end up with an offset where all the bits are set. This can allow an arbitrary memory read, as the addition in `base_len < off + len` can now overflow if `off` has all its bits set.  Fix the issue by casting the result of `*delta++ << 24UL` to an unsigned integer again. Add a test with a crafted delta that would actually succeed with an out-of-bounds read in case where the cast wouldn't exist.  Test-provided-by: Riccardo Schirone <rschiron@redhat.com>",
        "func_before": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/* Check that the base size matches the data we were given;\n\t* if not we would underflow while accessing data from the\n\t* base object, resulting in data corruption or segfault.\n\t*/\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base.\n\t\t\t*/\n\t\t\tsize_t off = 0, len = 0;\n\n\t\t\tif (cmd & 0x01) off = *delta++;\n\t\t\tif (cmd & 0x02) off |= *delta++ << 8UL;\n\t\t\tif (cmd & 0x04) off |= *delta++ << 16UL;\n\t\t\tif (cmd & 0x08) off |= *delta++ << 24UL;\n\n\t\t\tif (cmd & 0x10) len = *delta++;\n\t\t\tif (cmd & 0x20) len |= *delta++ << 8UL;\n\t\t\tif (cmd & 0x40) len |= *delta++ << 16UL;\n\t\t\tif (!len)\t\tlen = 0x10000;\n\n\t\t\tif (base_len < off + len || res_sz < len)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t}\n\t\telse if (cmd) {\n\t\t\t/* cmd is a literal insert instruction; copy from\n\t\t\t* the delta stream itself.\n\t\t\t*/\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t}\n\t\telse {\n\t\t\t/* cmd == 0 is reserved for future encodings.\n\t\t\t*/\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}",
        "func": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/*\n\t * Check that the base size matches the data we were given;\n\t * if not we would underflow while accessing data from the\n\t * base object, resulting in data corruption or segfault.\n\t */\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base. */\n\t\t\tsize_t off = 0, len = 0;\n\n\t\t\tif (cmd & 0x01) off = *delta++;\n\t\t\tif (cmd & 0x02) off |= *delta++ << 8UL;\n\t\t\tif (cmd & 0x04) off |= *delta++ << 16UL;\n\t\t\tif (cmd & 0x08) off |= ((unsigned) *delta++ << 24UL);\n\n\t\t\tif (cmd & 0x10) len = *delta++;\n\t\t\tif (cmd & 0x20) len |= *delta++ << 8UL;\n\t\t\tif (cmd & 0x40) len |= *delta++ << 16UL;\n\t\t\tif (!len)       len = 0x10000;\n\n\t\t\tif (base_len < off + len || res_sz < len)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t} else if (cmd) {\n\t\t\t/*\n\t\t\t * cmd is a literal insert instruction; copy from\n\t\t\t * the delta stream itself.\n\t\t\t */\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t} else {\n\t\t\t/* cmd == 0 is reserved for future encodings. */\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,10 +13,11 @@\n \t*out = NULL;\n \t*out_len = 0;\n \n-\t/* Check that the base size matches the data we were given;\n-\t* if not we would underflow while accessing data from the\n-\t* base object, resulting in data corruption or segfault.\n-\t*/\n+\t/*\n+\t * Check that the base size matches the data we were given;\n+\t * if not we would underflow while accessing data from the\n+\t * base object, resulting in data corruption or segfault.\n+\t */\n \tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n \t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n \t\treturn -1;\n@@ -38,19 +39,18 @@\n \twhile (delta < delta_end) {\n \t\tunsigned char cmd = *delta++;\n \t\tif (cmd & 0x80) {\n-\t\t\t/* cmd is a copy instruction; copy from the base.\n-\t\t\t*/\n+\t\t\t/* cmd is a copy instruction; copy from the base. */\n \t\t\tsize_t off = 0, len = 0;\n \n \t\t\tif (cmd & 0x01) off = *delta++;\n \t\t\tif (cmd & 0x02) off |= *delta++ << 8UL;\n \t\t\tif (cmd & 0x04) off |= *delta++ << 16UL;\n-\t\t\tif (cmd & 0x08) off |= *delta++ << 24UL;\n+\t\t\tif (cmd & 0x08) off |= ((unsigned) *delta++ << 24UL);\n \n \t\t\tif (cmd & 0x10) len = *delta++;\n \t\t\tif (cmd & 0x20) len |= *delta++ << 8UL;\n \t\t\tif (cmd & 0x40) len |= *delta++ << 16UL;\n-\t\t\tif (!len)\t\tlen = 0x10000;\n+\t\t\tif (!len)       len = 0x10000;\n \n \t\t\tif (base_len < off + len || res_sz < len)\n \t\t\t\tgoto fail;\n@@ -58,11 +58,11 @@\n \t\t\tres_dp += len;\n \t\t\tres_sz -= len;\n \n-\t\t}\n-\t\telse if (cmd) {\n-\t\t\t/* cmd is a literal insert instruction; copy from\n-\t\t\t* the delta stream itself.\n-\t\t\t*/\n+\t\t} else if (cmd) {\n+\t\t\t/*\n+\t\t\t * cmd is a literal insert instruction; copy from\n+\t\t\t * the delta stream itself.\n+\t\t\t */\n \t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n \t\t\t\tgoto fail;\n \t\t\tmemcpy(res_dp, delta, cmd);\n@@ -70,10 +70,8 @@\n \t\t\tres_dp += cmd;\n \t\t\tres_sz -= cmd;\n \n-\t\t}\n-\t\telse {\n-\t\t\t/* cmd == 0 is reserved for future encodings.\n-\t\t\t*/\n+\t\t} else {\n+\t\t\t/* cmd == 0 is reserved for future encodings. */\n \t\t\tgoto fail;\n \t\t}\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* Check that the base size matches the data we were given;",
                "\t* if not we would underflow while accessing data from the",
                "\t* base object, resulting in data corruption or segfault.",
                "\t*/",
                "\t\t\t/* cmd is a copy instruction; copy from the base.",
                "\t\t\t*/",
                "\t\t\tif (cmd & 0x08) off |= *delta++ << 24UL;",
                "\t\t\tif (!len)\t\tlen = 0x10000;",
                "\t\t}",
                "\t\telse if (cmd) {",
                "\t\t\t/* cmd is a literal insert instruction; copy from",
                "\t\t\t* the delta stream itself.",
                "\t\t\t*/",
                "\t\t}",
                "\t\telse {",
                "\t\t\t/* cmd == 0 is reserved for future encodings.",
                "\t\t\t*/"
            ],
            "added_lines": [
                "\t/*",
                "\t * Check that the base size matches the data we were given;",
                "\t * if not we would underflow while accessing data from the",
                "\t * base object, resulting in data corruption or segfault.",
                "\t */",
                "\t\t\t/* cmd is a copy instruction; copy from the base. */",
                "\t\t\tif (cmd & 0x08) off |= ((unsigned) *delta++ << 24UL);",
                "\t\t\tif (!len)       len = 0x10000;",
                "\t\t} else if (cmd) {",
                "\t\t\t/*",
                "\t\t\t * cmd is a literal insert instruction; copy from",
                "\t\t\t * the delta stream itself.",
                "\t\t\t */",
                "\t\t} else {",
                "\t\t\t/* cmd == 0 is reserved for future encodings. */"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-10887",
        "func_name": "libgit2/git_delta_apply",
        "description": "A flaw was found in libgit2 before version 0.27.3. It has been discovered that an unexpected sign extension in git_delta_apply function in delta.c file may lead to an integer overflow which in turn leads to an out of bound read, allowing to read before the base object. An attacker may use this flaw to leak memory addresses or cause a Denial of Service.",
        "git_url": "https://github.com/libgit2/libgit2/commit/c1577110467b701dcbcf9439ac225ea851b47d22",
        "commit_title": "delta: fix overflow when computing limit",
        "commit_text": " When checking whether a delta base offset and length fit into the base we have in memory already, we can trigger an overflow which breaks the check. This would subsequently result in us reading memory from out of bounds of the base.  The issue is easily fixed by checking for overflow when adding `off` and `len`, thus guaranteeting that we are never indexing beyond `base_len`. This corresponds to the git patch 8960844a7 (check patch_delta bounds more carefully, 2006-04-07), which adds these overflow checks. ",
        "func_before": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/*\n\t * Check that the base size matches the data we were given;\n\t * if not we would underflow while accessing data from the\n\t * base object, resulting in data corruption or segfault.\n\t */\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base. */\n\t\t\tsize_t off = 0, len = 0;\n\n#define ADD_DELTA(o, shift) { if (delta < delta_end) (o) |= ((unsigned) *delta++ << shift); else goto fail; }\n\t\t\tif (cmd & 0x01) ADD_DELTA(off, 0UL);\n\t\t\tif (cmd & 0x02) ADD_DELTA(off, 8UL);\n\t\t\tif (cmd & 0x04) ADD_DELTA(off, 16UL);\n\t\t\tif (cmd & 0x08) ADD_DELTA(off, 24UL);\n\n\t\t\tif (cmd & 0x10) ADD_DELTA(len, 0UL);\n\t\t\tif (cmd & 0x20) ADD_DELTA(len, 8UL);\n\t\t\tif (cmd & 0x40) ADD_DELTA(len, 16UL);\n\t\t\tif (!len)       len = 0x10000;\n#undef ADD_DELTA\n\n\t\t\tif (base_len < off + len || res_sz < len)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t} else if (cmd) {\n\t\t\t/*\n\t\t\t * cmd is a literal insert instruction; copy from\n\t\t\t * the delta stream itself.\n\t\t\t */\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t} else {\n\t\t\t/* cmd == 0 is reserved for future encodings. */\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}",
        "func": "int git_delta_apply(\n\tvoid **out,\n\tsize_t *out_len,\n\tconst unsigned char *base,\n\tsize_t base_len,\n\tconst unsigned char *delta,\n\tsize_t delta_len)\n{\n\tconst unsigned char *delta_end = delta + delta_len;\n\tsize_t base_sz, res_sz, alloc_sz;\n\tunsigned char *res_dp;\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\t/*\n\t * Check that the base size matches the data we were given;\n\t * if not we would underflow while accessing data from the\n\t * base object, resulting in data corruption or segfault.\n\t */\n\tif ((hdr_sz(&base_sz, &delta, delta_end) < 0) || (base_sz != base_len)) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tif (hdr_sz(&res_sz, &delta, delta_end) < 0) {\n\t\tgiterr_set(GITERR_INVALID, \"failed to apply delta: base size does not match given data\");\n\t\treturn -1;\n\t}\n\n\tGITERR_CHECK_ALLOC_ADD(&alloc_sz, res_sz, 1);\n\tres_dp = git__malloc(alloc_sz);\n\tGITERR_CHECK_ALLOC(res_dp);\n\n\tres_dp[res_sz] = '\\0';\n\t*out = res_dp;\n\t*out_len = res_sz;\n\n\twhile (delta < delta_end) {\n\t\tunsigned char cmd = *delta++;\n\t\tif (cmd & 0x80) {\n\t\t\t/* cmd is a copy instruction; copy from the base. */\n\t\t\tsize_t off = 0, len = 0, end;\n\n#define ADD_DELTA(o, shift) { if (delta < delta_end) (o) |= ((unsigned) *delta++ << shift); else goto fail; }\n\t\t\tif (cmd & 0x01) ADD_DELTA(off, 0UL);\n\t\t\tif (cmd & 0x02) ADD_DELTA(off, 8UL);\n\t\t\tif (cmd & 0x04) ADD_DELTA(off, 16UL);\n\t\t\tif (cmd & 0x08) ADD_DELTA(off, 24UL);\n\n\t\t\tif (cmd & 0x10) ADD_DELTA(len, 0UL);\n\t\t\tif (cmd & 0x20) ADD_DELTA(len, 8UL);\n\t\t\tif (cmd & 0x40) ADD_DELTA(len, 16UL);\n\t\t\tif (!len)       len = 0x10000;\n#undef ADD_DELTA\n\n\t\t\tif (GIT_ADD_SIZET_OVERFLOW(&end, off, len) ||\n\t\t\t    base_len < end || res_sz < len)\n\t\t\t\tgoto fail;\n\n\t\t\tmemcpy(res_dp, base + off, len);\n\t\t\tres_dp += len;\n\t\t\tres_sz -= len;\n\n\t\t} else if (cmd) {\n\t\t\t/*\n\t\t\t * cmd is a literal insert instruction; copy from\n\t\t\t * the delta stream itself.\n\t\t\t */\n\t\t\tif (delta_end - delta < cmd || res_sz < cmd)\n\t\t\t\tgoto fail;\n\t\t\tmemcpy(res_dp, delta, cmd);\n\t\t\tdelta += cmd;\n\t\t\tres_dp += cmd;\n\t\t\tres_sz -= cmd;\n\n\t\t} else {\n\t\t\t/* cmd == 0 is reserved for future encodings. */\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (delta != delta_end || res_sz)\n\t\tgoto fail;\n\treturn 0;\n\nfail:\n\tgit__free(*out);\n\n\t*out = NULL;\n\t*out_len = 0;\n\n\tgiterr_set(GITERR_INVALID, \"failed to apply delta\");\n\treturn -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -40,7 +40,7 @@\n \t\tunsigned char cmd = *delta++;\n \t\tif (cmd & 0x80) {\n \t\t\t/* cmd is a copy instruction; copy from the base. */\n-\t\t\tsize_t off = 0, len = 0;\n+\t\t\tsize_t off = 0, len = 0, end;\n \n #define ADD_DELTA(o, shift) { if (delta < delta_end) (o) |= ((unsigned) *delta++ << shift); else goto fail; }\n \t\t\tif (cmd & 0x01) ADD_DELTA(off, 0UL);\n@@ -54,8 +54,10 @@\n \t\t\tif (!len)       len = 0x10000;\n #undef ADD_DELTA\n \n-\t\t\tif (base_len < off + len || res_sz < len)\n+\t\t\tif (GIT_ADD_SIZET_OVERFLOW(&end, off, len) ||\n+\t\t\t    base_len < end || res_sz < len)\n \t\t\t\tgoto fail;\n+\n \t\t\tmemcpy(res_dp, base + off, len);\n \t\t\tres_dp += len;\n \t\t\tres_sz -= len;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tsize_t off = 0, len = 0;",
                "\t\t\tif (base_len < off + len || res_sz < len)"
            ],
            "added_lines": [
                "\t\t\tsize_t off = 0, len = 0, end;",
                "\t\t\tif (GIT_ADD_SIZET_OVERFLOW(&end, off, len) ||",
                "\t\t\t    base_len < end || res_sz < len)",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-8786",
        "func_name": "FreeRDP/update_read_bitmap_update",
        "description": "FreeRDP prior to version 2.0.0-rc4 contains an Integer Truncation that leads to a Heap-Based Buffer Overflow in function update_read_bitmap_update() and results in a memory corruption and probably even a remote code execution.",
        "git_url": "https://github.com/FreeRDP/FreeRDP/commit/445a5a42c500ceb80f8fa7f2c11f3682538033f3",
        "commit_title": "Fixed CVE-2018-8786",
        "commit_text": " Thanks to Eyal Itkin from Check Point Software Technologies.",
        "func_before": "BITMAP_UPDATE* update_read_bitmap_update(rdpUpdate* update, wStream* s)\n{\n\tUINT32 i;\n\tBITMAP_UPDATE* bitmapUpdate = calloc(1, sizeof(BITMAP_UPDATE));\n\n\tif (!bitmapUpdate)\n\t\tgoto fail;\n\n\tif (Stream_GetRemainingLength(s) < 2)\n\t\tgoto fail;\n\n\tStream_Read_UINT16(s, bitmapUpdate->number); /* numberRectangles (2 bytes) */\n\tWLog_Print(update->log, WLOG_TRACE, \"BitmapUpdate: %\"PRIu32\"\", bitmapUpdate->number);\n\n\tif (bitmapUpdate->number > bitmapUpdate->count)\n\t{\n\t\tUINT16 count;\n\t\tBITMAP_DATA* newdata;\n\t\tcount = bitmapUpdate->number * 2;\n\t\tnewdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,\n\t\t                                 sizeof(BITMAP_DATA) * count);\n\n\t\tif (!newdata)\n\t\t\tgoto fail;\n\n\t\tbitmapUpdate->rectangles = newdata;\n\t\tZeroMemory(&bitmapUpdate->rectangles[bitmapUpdate->count],\n\t\t           sizeof(BITMAP_DATA) * (count - bitmapUpdate->count));\n\t\tbitmapUpdate->count = count;\n\t}\n\n\t/* rectangles */\n\tfor (i = 0; i < bitmapUpdate->number; i++)\n\t{\n\t\tif (!update_read_bitmap_data(update, s, &bitmapUpdate->rectangles[i]))\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapUpdate;\nfail:\n\tfree_bitmap_update(update->context, bitmapUpdate);\n\treturn NULL;\n}",
        "func": "BITMAP_UPDATE* update_read_bitmap_update(rdpUpdate* update, wStream* s)\n{\n\tUINT32 i;\n\tBITMAP_UPDATE* bitmapUpdate = calloc(1, sizeof(BITMAP_UPDATE));\n\n\tif (!bitmapUpdate)\n\t\tgoto fail;\n\n\tif (Stream_GetRemainingLength(s) < 2)\n\t\tgoto fail;\n\n\tStream_Read_UINT16(s, bitmapUpdate->number); /* numberRectangles (2 bytes) */\n\tWLog_Print(update->log, WLOG_TRACE, \"BitmapUpdate: %\"PRIu32\"\", bitmapUpdate->number);\n\n\tif (bitmapUpdate->number > bitmapUpdate->count)\n\t{\n\t\tUINT32 count = bitmapUpdate->number * 2;\n\t\tBITMAP_DATA* newdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,\n\t\t                       sizeof(BITMAP_DATA) * count);\n\n\t\tif (!newdata)\n\t\t\tgoto fail;\n\n\t\tbitmapUpdate->rectangles = newdata;\n\t\tZeroMemory(&bitmapUpdate->rectangles[bitmapUpdate->count],\n\t\t           sizeof(BITMAP_DATA) * (count - bitmapUpdate->count));\n\t\tbitmapUpdate->count = count;\n\t}\n\n\t/* rectangles */\n\tfor (i = 0; i < bitmapUpdate->number; i++)\n\t{\n\t\tif (!update_read_bitmap_data(update, s, &bitmapUpdate->rectangles[i]))\n\t\t\tgoto fail;\n\t}\n\n\treturn bitmapUpdate;\nfail:\n\tfree_bitmap_update(update->context, bitmapUpdate);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,11 +14,9 @@\n \n \tif (bitmapUpdate->number > bitmapUpdate->count)\n \t{\n-\t\tUINT16 count;\n-\t\tBITMAP_DATA* newdata;\n-\t\tcount = bitmapUpdate->number * 2;\n-\t\tnewdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,\n-\t\t                                 sizeof(BITMAP_DATA) * count);\n+\t\tUINT32 count = bitmapUpdate->number * 2;\n+\t\tBITMAP_DATA* newdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,\n+\t\t                       sizeof(BITMAP_DATA) * count);\n \n \t\tif (!newdata)\n \t\t\tgoto fail;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tUINT16 count;",
                "\t\tBITMAP_DATA* newdata;",
                "\t\tcount = bitmapUpdate->number * 2;",
                "\t\tnewdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,",
                "\t\t                                 sizeof(BITMAP_DATA) * count);"
            ],
            "added_lines": [
                "\t\tUINT32 count = bitmapUpdate->number * 2;",
                "\t\tBITMAP_DATA* newdata = (BITMAP_DATA*) realloc(bitmapUpdate->rectangles,",
                "\t\t                       sizeof(BITMAP_DATA) * count);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29539",
        "func_name": "tensorflow/ImmutableConstantOp::ImmutableConstantOp",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. Calling `tf.raw_ops.ImmutableConst`(https://www.tensorflow.org/api_docs/python/tf/raw_ops/ImmutableConst) with a `dtype` of `tf.resource` or `tf.variant` results in a segfault in the implementation as code assumes that the tensor contents are pure scalars. We have patched the issue in 4f663d4b8f0bec1b48da6fa091a7d29609980fa4 and will release TensorFlow 2.5.0 containing the patch. TensorFlow nightly packages after this commit will also have the issue resolved. If using `tf.raw_ops.ImmutableConst` in code, you can prevent the segfault by inserting a filter for the `dtype` argument.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/4f663d4b8f0bec1b48da6fa091a7d29609980fa4",
        "commit_title": "Allowlist certain data types to avoid a seg fault.",
        "commit_text": " PiperOrigin-RevId: 356326671",
        "func_before": "ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\n    : OpKernel(context) {\n  OP_REQUIRES_OK(context,\n                 context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n  OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n  OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n}",
        "func": "ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\n    : OpKernel(context) {\n  OP_REQUIRES_OK(context,\n                 context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n  OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,\n              errors::InvalidArgument(\n                  \"Resource and variant dtypes are invalid for this op.\"));\n  OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,5 +3,8 @@\n   OP_REQUIRES_OK(context,\n                  context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n   OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n+  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,\n+              errors::InvalidArgument(\n+                  \"Resource and variant dtypes are invalid for this op.\"));\n   OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,",
                "              errors::InvalidArgument(",
                "                  \"Resource and variant dtypes are invalid for this op.\"));"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-125011",
        "func_name": "ffmpeg/decode_frame",
        "description": "A vulnerability was found in FFmpeg 2.0. It has been declared as problematic. Affected by this vulnerability is the function decode_frame of the file libavcodec/ansi.c. The manipulation leads to integer coercion error. The attack can be launched remotely. It is recommended to apply a patch to fix this issue.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=d42ec8433c687fcbccefa51a7716d81920218e4f",
        "commit_title": "",
        "commit_text": "avcodec/ansi: fix integer overflow  Fixes out of array read ",
        "func_before": "static int decode_frame(AVCodecContext *avctx,\n                            void *data, int *got_frame,\n                            AVPacket *avpkt)\n{\n    AnsiContext *s = avctx->priv_data;\n    uint8_t *buf = avpkt->data;\n    int buf_size = avpkt->size;\n    const uint8_t *buf_end   = buf+buf_size;\n    int ret, i, count;\n\n    if ((ret = ff_reget_buffer(avctx, s->frame)) < 0)\n        return ret;\n    if (!avctx->frame_number) {\n        for (i=0; i<avctx->height; i++)\n            memset(s->frame->data[0]+ i*s->frame->linesize[0], 0, avctx->width);\n        memset(s->frame->data[1], 0, AVPALETTE_SIZE);\n    }\n\n    s->frame->pict_type           = AV_PICTURE_TYPE_I;\n    s->frame->palette_has_changed = 1;\n    set_palette((uint32_t *)s->frame->data[1]);\n    if (!s->first_frame) {\n        erase_screen(avctx);\n        s->first_frame = 1;\n    }\n\n    while(buf < buf_end) {\n        switch(s->state) {\n        case STATE_NORMAL:\n            switch (buf[0]) {\n            case 0x00: //NUL\n            case 0x07: //BEL\n            case 0x1A: //SUB\n                /* ignore */\n                break;\n            case 0x08: //BS\n                s->x = FFMAX(s->x - 1, 0);\n                break;\n            case 0x09: //HT\n                i = s->x / FONT_WIDTH;\n                count = ((i + 8) & ~7) - i;\n                for (i = 0; i < count; i++)\n                    draw_char(avctx, ' ');\n                break;\n            case 0x0A: //LF\n                hscroll(avctx);\n            case 0x0D: //CR\n                s->x = 0;\n                break;\n            case 0x0C: //FF\n                erase_screen(avctx);\n                break;\n            case 0x1B: //ESC\n                s->state = STATE_ESCAPE;\n                break;\n            default:\n                draw_char(avctx, buf[0]);\n            }\n            break;\n        case STATE_ESCAPE:\n            if (buf[0] == '[') {\n                s->state   = STATE_CODE;\n                s->nb_args = 0;\n                s->args[0] = -1;\n            } else {\n                s->state = STATE_NORMAL;\n                draw_char(avctx, 0x1B);\n                continue;\n            }\n            break;\n        case STATE_CODE:\n            switch(buf[0]) {\n            case '0': case '1': case '2': case '3': case '4':\n            case '5': case '6': case '7': case '8': case '9':\n                if (s->nb_args < MAX_NB_ARGS)\n                    s->args[s->nb_args] = FFMAX(s->args[s->nb_args], 0) * 10 + buf[0] - '0';\n                break;\n            case ';':\n                s->nb_args++;\n                if (s->nb_args < MAX_NB_ARGS)\n                    s->args[s->nb_args] = 0;\n                break;\n            case 'M':\n                s->state = STATE_MUSIC_PREAMBLE;\n                break;\n            case '=': case '?':\n                /* ignore */\n                break;\n            default:\n                if (s->nb_args > MAX_NB_ARGS)\n                    av_log(avctx, AV_LOG_WARNING, \"args overflow (%i)\\n\", s->nb_args);\n                if (s->nb_args < MAX_NB_ARGS && s->args[s->nb_args] >= 0)\n                    s->nb_args++;\n                if ((ret = execute_code(avctx, buf[0])) < 0)\n                    return ret;\n                s->state = STATE_NORMAL;\n            }\n            break;\n        case STATE_MUSIC_PREAMBLE:\n            if (buf[0] == 0x0E || buf[0] == 0x1B)\n                s->state = STATE_NORMAL;\n            /* ignore music data */\n            break;\n        }\n        buf++;\n    }\n\n    *got_frame = 1;\n    if ((ret = av_frame_ref(data, s->frame)) < 0)\n        return ret;\n    return buf_size;\n}",
        "func": "static int decode_frame(AVCodecContext *avctx,\n                            void *data, int *got_frame,\n                            AVPacket *avpkt)\n{\n    AnsiContext *s = avctx->priv_data;\n    uint8_t *buf = avpkt->data;\n    int buf_size = avpkt->size;\n    const uint8_t *buf_end   = buf+buf_size;\n    int ret, i, count;\n\n    if ((ret = ff_reget_buffer(avctx, s->frame)) < 0)\n        return ret;\n    if (!avctx->frame_number) {\n        for (i=0; i<avctx->height; i++)\n            memset(s->frame->data[0]+ i*s->frame->linesize[0], 0, avctx->width);\n        memset(s->frame->data[1], 0, AVPALETTE_SIZE);\n    }\n\n    s->frame->pict_type           = AV_PICTURE_TYPE_I;\n    s->frame->palette_has_changed = 1;\n    set_palette((uint32_t *)s->frame->data[1]);\n    if (!s->first_frame) {\n        erase_screen(avctx);\n        s->first_frame = 1;\n    }\n\n    while(buf < buf_end) {\n        switch(s->state) {\n        case STATE_NORMAL:\n            switch (buf[0]) {\n            case 0x00: //NUL\n            case 0x07: //BEL\n            case 0x1A: //SUB\n                /* ignore */\n                break;\n            case 0x08: //BS\n                s->x = FFMAX(s->x - 1, 0);\n                break;\n            case 0x09: //HT\n                i = s->x / FONT_WIDTH;\n                count = ((i + 8) & ~7) - i;\n                for (i = 0; i < count; i++)\n                    draw_char(avctx, ' ');\n                break;\n            case 0x0A: //LF\n                hscroll(avctx);\n            case 0x0D: //CR\n                s->x = 0;\n                break;\n            case 0x0C: //FF\n                erase_screen(avctx);\n                break;\n            case 0x1B: //ESC\n                s->state = STATE_ESCAPE;\n                break;\n            default:\n                draw_char(avctx, buf[0]);\n            }\n            break;\n        case STATE_ESCAPE:\n            if (buf[0] == '[') {\n                s->state   = STATE_CODE;\n                s->nb_args = 0;\n                s->args[0] = -1;\n            } else {\n                s->state = STATE_NORMAL;\n                draw_char(avctx, 0x1B);\n                continue;\n            }\n            break;\n        case STATE_CODE:\n            switch(buf[0]) {\n            case '0': case '1': case '2': case '3': case '4':\n            case '5': case '6': case '7': case '8': case '9':\n                if (s->nb_args < MAX_NB_ARGS && s->args[s->nb_args] < 6553)\n                    s->args[s->nb_args] = FFMAX(s->args[s->nb_args], 0) * 10 + buf[0] - '0';\n                break;\n            case ';':\n                s->nb_args++;\n                if (s->nb_args < MAX_NB_ARGS)\n                    s->args[s->nb_args] = 0;\n                break;\n            case 'M':\n                s->state = STATE_MUSIC_PREAMBLE;\n                break;\n            case '=': case '?':\n                /* ignore */\n                break;\n            default:\n                if (s->nb_args > MAX_NB_ARGS)\n                    av_log(avctx, AV_LOG_WARNING, \"args overflow (%i)\\n\", s->nb_args);\n                if (s->nb_args < MAX_NB_ARGS && s->args[s->nb_args] >= 0)\n                    s->nb_args++;\n                if ((ret = execute_code(avctx, buf[0])) < 0)\n                    return ret;\n                s->state = STATE_NORMAL;\n            }\n            break;\n        case STATE_MUSIC_PREAMBLE:\n            if (buf[0] == 0x0E || buf[0] == 0x1B)\n                s->state = STATE_NORMAL;\n            /* ignore music data */\n            break;\n        }\n        buf++;\n    }\n\n    *got_frame = 1;\n    if ((ret = av_frame_ref(data, s->frame)) < 0)\n        return ret;\n    return buf_size;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -72,7 +72,7 @@\n             switch(buf[0]) {\n             case '0': case '1': case '2': case '3': case '4':\n             case '5': case '6': case '7': case '8': case '9':\n-                if (s->nb_args < MAX_NB_ARGS)\n+                if (s->nb_args < MAX_NB_ARGS && s->args[s->nb_args] < 6553)\n                     s->args[s->nb_args] = FFMAX(s->args[s->nb_args], 0) * 10 + buf[0] - '0';\n                 break;\n             case ';':",
        "diff_line_info": {
            "deleted_lines": [
                "                if (s->nb_args < MAX_NB_ARGS)"
            ],
            "added_lines": [
                "                if (s->nb_args < MAX_NB_ARGS && s->args[s->nb_args] < 6553)"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-125012",
        "func_name": "ffmpeg/dxtory_decode_v1_420",
        "description": "A vulnerability was found in FFmpeg 2.0. It has been classified as problematic. Affected is an unknown function of the file libavcodec/dxtroy.c. The manipulation leads to integer coercion error. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=a392bf657015c9a79a5a13adfbfb15086c1943b9",
        "commit_title": "",
        "commit_text": "avcodec/dxtory: fix src size checks  Fixes integer overflow Fixes out of array read ",
        "func_before": "static int dxtory_decode_v1_420(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y1, *Y2, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 3 / 2) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV420P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y1 = pic->data[0];\n    Y2 = pic->data[0] + pic->linesize[0];\n    U  = pic->data[1];\n    V  = pic->data[2];\n    for (h = 0; h < avctx->height; h += 2) {\n        for (w = 0; w < avctx->width; w += 2) {\n            AV_COPY16(Y1 + w, src);\n            AV_COPY16(Y2 + w, src + 2);\n            U[w >> 1] = src[4] + 0x80;\n            V[w >> 1] = src[5] + 0x80;\n            src += 6;\n        }\n        Y1 += pic->linesize[0] << 1;\n        Y2 += pic->linesize[0] << 1;\n        U  += pic->linesize[1];\n        V  += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "func": "static int dxtory_decode_v1_420(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y1, *Y2, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 3L / 2) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV420P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y1 = pic->data[0];\n    Y2 = pic->data[0] + pic->linesize[0];\n    U  = pic->data[1];\n    V  = pic->data[2];\n    for (h = 0; h < avctx->height; h += 2) {\n        for (w = 0; w < avctx->width; w += 2) {\n            AV_COPY16(Y1 + w, src);\n            AV_COPY16(Y2 + w, src + 2);\n            U[w >> 1] = src[4] + 0x80;\n            V[w >> 1] = src[5] + 0x80;\n            src += 6;\n        }\n        Y1 += pic->linesize[0] << 1;\n        Y2 += pic->linesize[0] << 1;\n        U  += pic->linesize[1];\n        V  += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n     uint8_t *Y1, *Y2, *U, *V;\n     int ret;\n \n-    if (src_size < avctx->width * avctx->height * 3 / 2) {\n+    if (src_size < avctx->width * avctx->height * 3L / 2) {\n         av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n         return AVERROR_INVALIDDATA;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (src_size < avctx->width * avctx->height * 3 / 2) {"
            ],
            "added_lines": [
                "    if (src_size < avctx->width * avctx->height * 3L / 2) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-125012",
        "func_name": "ffmpeg/dxtory_decode_v1_rgb",
        "description": "A vulnerability was found in FFmpeg 2.0. It has been classified as problematic. Affected is an unknown function of the file libavcodec/dxtroy.c. The manipulation leads to integer coercion error. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=a392bf657015c9a79a5a13adfbfb15086c1943b9",
        "commit_title": "",
        "commit_text": "avcodec/dxtory: fix src size checks  Fixes integer overflow Fixes out of array read ",
        "func_before": "static int dxtory_decode_v1_rgb(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size,\n                                int id, int bpp)\n{\n    int h;\n    uint8_t *dst;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * bpp) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = id;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    dst = pic->data[0];\n    for (h = 0; h < avctx->height; h++) {\n        memcpy(dst, src, avctx->width * bpp);\n        src += avctx->width * bpp;\n        dst += pic->linesize[0];\n    }\n\n    return 0;\n}",
        "func": "static int dxtory_decode_v1_rgb(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size,\n                                int id, int bpp)\n{\n    int h;\n    uint8_t *dst;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * (int64_t)bpp) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = id;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    dst = pic->data[0];\n    for (h = 0; h < avctx->height; h++) {\n        memcpy(dst, src, avctx->width * bpp);\n        src += avctx->width * bpp;\n        dst += pic->linesize[0];\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n     uint8_t *dst;\n     int ret;\n \n-    if (src_size < avctx->width * avctx->height * bpp) {\n+    if (src_size < avctx->width * avctx->height * (int64_t)bpp) {\n         av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n         return AVERROR_INVALIDDATA;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (src_size < avctx->width * avctx->height * bpp) {"
            ],
            "added_lines": [
                "    if (src_size < avctx->width * avctx->height * (int64_t)bpp) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-125012",
        "func_name": "ffmpeg/dxtory_decode_v1_444",
        "description": "A vulnerability was found in FFmpeg 2.0. It has been classified as problematic. Affected is an unknown function of the file libavcodec/dxtroy.c. The manipulation leads to integer coercion error. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=a392bf657015c9a79a5a13adfbfb15086c1943b9",
        "commit_title": "",
        "commit_text": "avcodec/dxtory: fix src size checks  Fixes integer overflow Fixes out of array read ",
        "func_before": "static int dxtory_decode_v1_444(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 3) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV444P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y = pic->data[0];\n    U = pic->data[1];\n    V = pic->data[2];\n    for (h = 0; h < avctx->height; h++) {\n        for (w = 0; w < avctx->width; w++) {\n            Y[w] = *src++;\n            U[w] = *src++ ^ 0x80;\n            V[w] = *src++ ^ 0x80;\n        }\n        Y += pic->linesize[0];\n        U += pic->linesize[1];\n        V += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "func": "static int dxtory_decode_v1_444(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 3L) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV444P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y = pic->data[0];\n    U = pic->data[1];\n    V = pic->data[2];\n    for (h = 0; h < avctx->height; h++) {\n        for (w = 0; w < avctx->width; w++) {\n            Y[w] = *src++;\n            U[w] = *src++ ^ 0x80;\n            V[w] = *src++ ^ 0x80;\n        }\n        Y += pic->linesize[0];\n        U += pic->linesize[1];\n        V += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n     uint8_t *Y, *U, *V;\n     int ret;\n \n-    if (src_size < avctx->width * avctx->height * 3) {\n+    if (src_size < avctx->width * avctx->height * 3L) {\n         av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n         return AVERROR_INVALIDDATA;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (src_size < avctx->width * avctx->height * 3) {"
            ],
            "added_lines": [
                "    if (src_size < avctx->width * avctx->height * 3L) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2014-125012",
        "func_name": "ffmpeg/dxtory_decode_v1_410",
        "description": "A vulnerability was found in FFmpeg 2.0. It has been classified as problematic. Affected is an unknown function of the file libavcodec/dxtroy.c. The manipulation leads to integer coercion error. It is possible to launch the attack remotely. It is recommended to apply a patch to fix this issue.",
        "git_url": "http://git.videolan.org/?p=ffmpeg.git;a=commit;h=a392bf657015c9a79a5a13adfbfb15086c1943b9",
        "commit_title": "",
        "commit_text": "avcodec/dxtory: fix src size checks  Fixes integer overflow Fixes out of array read ",
        "func_before": "static int dxtory_decode_v1_410(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y1, *Y2, *Y3, *Y4, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 18 / 16) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV410P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y1 = pic->data[0];\n    Y2 = pic->data[0] + pic->linesize[0];\n    Y3 = pic->data[0] + pic->linesize[0] * 2;\n    Y4 = pic->data[0] + pic->linesize[0] * 3;\n    U  = pic->data[1];\n    V  = pic->data[2];\n    for (h = 0; h < avctx->height; h += 4) {\n        for (w = 0; w < avctx->width; w += 4) {\n            AV_COPY32(Y1 + w, src);\n            AV_COPY32(Y2 + w, src + 4);\n            AV_COPY32(Y3 + w, src + 8);\n            AV_COPY32(Y4 + w, src + 12);\n            U[w >> 2] = src[16] + 0x80;\n            V[w >> 2] = src[17] + 0x80;\n            src += 18;\n        }\n        Y1 += pic->linesize[0] << 2;\n        Y2 += pic->linesize[0] << 2;\n        Y3 += pic->linesize[0] << 2;\n        Y4 += pic->linesize[0] << 2;\n        U  += pic->linesize[1];\n        V  += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "func": "static int dxtory_decode_v1_410(AVCodecContext *avctx, AVFrame *pic,\n                                const uint8_t *src, int src_size)\n{\n    int h, w;\n    uint8_t *Y1, *Y2, *Y3, *Y4, *U, *V;\n    int ret;\n\n    if (src_size < avctx->width * avctx->height * 9L / 8) {\n        av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n        return AVERROR_INVALIDDATA;\n    }\n\n    avctx->pix_fmt = AV_PIX_FMT_YUV410P;\n    if ((ret = ff_get_buffer(avctx, pic, 0)) < 0)\n        return ret;\n\n    Y1 = pic->data[0];\n    Y2 = pic->data[0] + pic->linesize[0];\n    Y3 = pic->data[0] + pic->linesize[0] * 2;\n    Y4 = pic->data[0] + pic->linesize[0] * 3;\n    U  = pic->data[1];\n    V  = pic->data[2];\n    for (h = 0; h < avctx->height; h += 4) {\n        for (w = 0; w < avctx->width; w += 4) {\n            AV_COPY32(Y1 + w, src);\n            AV_COPY32(Y2 + w, src + 4);\n            AV_COPY32(Y3 + w, src + 8);\n            AV_COPY32(Y4 + w, src + 12);\n            U[w >> 2] = src[16] + 0x80;\n            V[w >> 2] = src[17] + 0x80;\n            src += 18;\n        }\n        Y1 += pic->linesize[0] << 2;\n        Y2 += pic->linesize[0] << 2;\n        Y3 += pic->linesize[0] << 2;\n        Y4 += pic->linesize[0] << 2;\n        U  += pic->linesize[1];\n        V  += pic->linesize[2];\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,7 +5,7 @@\n     uint8_t *Y1, *Y2, *Y3, *Y4, *U, *V;\n     int ret;\n \n-    if (src_size < avctx->width * avctx->height * 18 / 16) {\n+    if (src_size < avctx->width * avctx->height * 9L / 8) {\n         av_log(avctx, AV_LOG_ERROR, \"packet too small\\n\");\n         return AVERROR_INVALIDDATA;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (src_size < avctx->width * avctx->height * 18 / 16) {"
            ],
            "added_lines": [
                "    if (src_size < avctx->width * avctx->height * 9L / 8) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-19317",
        "func_name": "sqlite/lookupName",
        "description": "lookupName in resolve.c in SQLite 3.30.1 omits bits from the colUsed bitmask in the case of a generated column, which allows attackers to cause a denial of service or possibly have unspecified other impact.",
        "git_url": "https://github.com/sqlite/sqlite/commit/522ebfa7cee96fb325a22ea3a2464a63485886a8",
        "commit_title": "Whenever a generated column is used, assume that all columns are used.",
        "commit_text": " FossilOrigin-Name: 6601da58032d18ae00b466c0f2077fb2b1ecd84225b56e1787724bea478eedc9",
        "func_before": "static int lookupName(\n  Parse *pParse,       /* The parsing context */\n  const char *zDb,     /* Name of the database containing table, or NULL */\n  const char *zTab,    /* Name of table containing column, or NULL */\n  const char *zCol,    /* Name of the column. */\n  NameContext *pNC,    /* The name context used to resolve the name */\n  Expr *pExpr          /* Make this EXPR node point to the selected column */\n){\n  int i, j;                         /* Loop counters */\n  int cnt = 0;                      /* Number of matching column names */\n  int cntTab = 0;                   /* Number of matching table names */\n  int nSubquery = 0;                /* How many levels of subquery */\n  sqlite3 *db = pParse->db;         /* The database connection */\n  struct SrcList_item *pItem;       /* Use for looping over pSrcList items */\n  struct SrcList_item *pMatch = 0;  /* The matching pSrcList item */\n  NameContext *pTopNC = pNC;        /* First namecontext in the list */\n  Schema *pSchema = 0;              /* Schema of the expression */\n  int eNewExprOp = TK_COLUMN;       /* New value for pExpr->op on success */\n  Table *pTab = 0;                  /* Table hold the row */\n  Column *pCol;                     /* A column of pTab */\n\n  assert( pNC );     /* the name context cannot be NULL. */\n  assert( zCol );    /* The Z in X.Y.Z cannot be NULL */\n  assert( !ExprHasProperty(pExpr, EP_TokenOnly|EP_Reduced) );\n\n  /* Initialize the node to no-match */\n  pExpr->iTable = -1;\n  ExprSetVVAProperty(pExpr, EP_NoReduce);\n\n  /* Translate the schema name in zDb into a pointer to the corresponding\n  ** schema.  If not found, pSchema will remain NULL and nothing will match\n  ** resulting in an appropriate error message toward the end of this routine\n  */\n  if( zDb ){\n    testcase( pNC->ncFlags & NC_PartIdx );\n    testcase( pNC->ncFlags & NC_IsCheck );\n    if( (pNC->ncFlags & (NC_PartIdx|NC_IsCheck))!=0 ){\n      /* Silently ignore database qualifiers inside CHECK constraints and\n      ** partial indices.  Do not raise errors because that might break\n      ** legacy and because it does not hurt anything to just ignore the\n      ** database name. */\n      zDb = 0;\n    }else{\n      for(i=0; i<db->nDb; i++){\n        assert( db->aDb[i].zDbSName );\n        if( sqlite3StrICmp(db->aDb[i].zDbSName,zDb)==0 ){\n          pSchema = db->aDb[i].pSchema;\n          break;\n        }\n      }\n    }\n  }\n\n  /* Start at the inner-most context and move outward until a match is found */\n  assert( pNC && cnt==0 );\n  do{\n    ExprList *pEList;\n    SrcList *pSrcList = pNC->pSrcList;\n\n    if( pSrcList ){\n      for(i=0, pItem=pSrcList->a; i<pSrcList->nSrc; i++, pItem++){\n        pTab = pItem->pTab;\n        assert( pTab!=0 && pTab->zName!=0 );\n        assert( pTab->nCol>0 );\n        if( pItem->pSelect && (pItem->pSelect->selFlags & SF_NestedFrom)!=0 ){\n          int hit = 0;\n          pEList = pItem->pSelect->pEList;\n          for(j=0; j<pEList->nExpr; j++){\n            if( sqlite3MatchSpanName(pEList->a[j].zSpan, zCol, zTab, zDb) ){\n              cnt++;\n              cntTab = 2;\n              pMatch = pItem;\n              pExpr->iColumn = j;\n              hit = 1;\n            }\n          }\n          if( hit || zTab==0 ) continue;\n        }\n        if( zDb && pTab->pSchema!=pSchema ){\n          continue;\n        }\n        if( zTab ){\n          const char *zTabName = pItem->zAlias ? pItem->zAlias : pTab->zName;\n          assert( zTabName!=0 );\n          if( sqlite3StrICmp(zTabName, zTab)!=0 ){\n            continue;\n          }\n          if( IN_RENAME_OBJECT && pItem->zAlias ){\n            sqlite3RenameTokenRemap(pParse, 0, (void*)&pExpr->y.pTab);\n          }\n        }\n        if( 0==(cntTab++) ){\n          pMatch = pItem;\n        }\n        for(j=0, pCol=pTab->aCol; j<pTab->nCol; j++, pCol++){\n          if( sqlite3StrICmp(pCol->zName, zCol)==0 ){\n            /* If there has been exactly one prior match and this match\n            ** is for the right-hand table of a NATURAL JOIN or is in a \n            ** USING clause, then skip this match.\n            */\n            if( cnt==1 ){\n              if( pItem->fg.jointype & JT_NATURAL ) continue;\n              if( nameInUsingClause(pItem->pUsing, zCol) ) continue;\n            }\n            cnt++;\n            pMatch = pItem;\n            /* Substitute the rowid (column -1) for the INTEGER PRIMARY KEY */\n            pExpr->iColumn = j==pTab->iPKey ? -1 : (i16)j;\n            break;\n          }\n        }\n      }\n      if( pMatch ){\n        pExpr->iTable = pMatch->iCursor;\n        pExpr->y.pTab = pMatch->pTab;\n        /* RIGHT JOIN not (yet) supported */\n        assert( (pMatch->fg.jointype & JT_RIGHT)==0 );\n        if( (pMatch->fg.jointype & JT_LEFT)!=0 ){\n          ExprSetProperty(pExpr, EP_CanBeNull);\n        }\n        pSchema = pExpr->y.pTab->pSchema;\n      }\n    } /* if( pSrcList ) */\n\n#if !defined(SQLITE_OMIT_TRIGGER) || !defined(SQLITE_OMIT_UPSERT)\n    /* If we have not already resolved the name, then maybe \n    ** it is a new.* or old.* trigger argument reference.  Or\n    ** maybe it is an excluded.* from an upsert.\n    */\n    if( zDb==0 && zTab!=0 && cntTab==0 ){\n      pTab = 0;\n#ifndef SQLITE_OMIT_TRIGGER\n      if( pParse->pTriggerTab!=0 ){\n        int op = pParse->eTriggerOp;\n        assert( op==TK_DELETE || op==TK_UPDATE || op==TK_INSERT );\n        if( op!=TK_DELETE && sqlite3StrICmp(\"new\",zTab) == 0 ){\n          pExpr->iTable = 1;\n          pTab = pParse->pTriggerTab;\n        }else if( op!=TK_INSERT && sqlite3StrICmp(\"old\",zTab)==0 ){\n          pExpr->iTable = 0;\n          pTab = pParse->pTriggerTab;\n        }\n      }\n#endif /* SQLITE_OMIT_TRIGGER */\n#ifndef SQLITE_OMIT_UPSERT\n      if( (pNC->ncFlags & NC_UUpsert)!=0 ){\n        Upsert *pUpsert = pNC->uNC.pUpsert;\n        if( pUpsert && sqlite3StrICmp(\"excluded\",zTab)==0 ){\n          pTab = pUpsert->pUpsertSrc->a[0].pTab;\n          pExpr->iTable = 2;\n        }\n      }\n#endif /* SQLITE_OMIT_UPSERT */\n\n      if( pTab ){ \n        int iCol;\n        pSchema = pTab->pSchema;\n        cntTab++;\n        for(iCol=0, pCol=pTab->aCol; iCol<pTab->nCol; iCol++, pCol++){\n          if( sqlite3StrICmp(pCol->zName, zCol)==0 ){\n            if( iCol==pTab->iPKey ){\n              iCol = -1;\n            }\n            break;\n          }\n        }\n        if( iCol>=pTab->nCol && sqlite3IsRowid(zCol) && VisibleRowid(pTab) ){\n          /* IMP: R-51414-32910 */\n          iCol = -1;\n        }\n        if( iCol<pTab->nCol ){\n          cnt++;\n#ifndef SQLITE_OMIT_UPSERT\n          if( pExpr->iTable==2 ){\n            testcase( iCol==(-1) );\n            if( IN_RENAME_OBJECT ){\n              pExpr->iColumn = iCol;\n              pExpr->y.pTab = pTab;\n              eNewExprOp = TK_COLUMN;\n            }else{\n              pExpr->iTable = pNC->uNC.pUpsert->regData + iCol;\n              eNewExprOp = TK_REGISTER;\n              ExprSetProperty(pExpr, EP_Alias);\n            }\n          }else\n#endif /* SQLITE_OMIT_UPSERT */\n          {\n#ifndef SQLITE_OMIT_TRIGGER\n            if( iCol<0 ){\n              pExpr->affExpr = SQLITE_AFF_INTEGER;\n            }else if( pExpr->iTable==0 ){\n              testcase( iCol==31 );\n              testcase( iCol==32 );\n              pParse->oldmask |= (iCol>=32 ? 0xffffffff : (((u32)1)<<iCol));\n            }else{\n              testcase( iCol==31 );\n              testcase( iCol==32 );\n              pParse->newmask |= (iCol>=32 ? 0xffffffff : (((u32)1)<<iCol));\n            }\n            pExpr->y.pTab = pTab;\n            pExpr->iColumn = (i16)iCol;\n            eNewExprOp = TK_TRIGGER;\n#endif /* SQLITE_OMIT_TRIGGER */\n          }\n        }\n      }\n    }\n#endif /* !defined(SQLITE_OMIT_TRIGGER) || !defined(SQLITE_OMIT_UPSERT) */\n\n    /*\n    ** Perhaps the name is a reference to the ROWID\n    */\n    if( cnt==0\n     && cntTab==1\n     && pMatch\n     && (pNC->ncFlags & (NC_IdxExpr|NC_GenCol))==0\n     && sqlite3IsRowid(zCol)\n     && VisibleRowid(pMatch->pTab)\n    ){\n      cnt = 1;\n      pExpr->iColumn = -1;\n      pExpr->affExpr = SQLITE_AFF_INTEGER;\n    }\n\n    /*\n    ** If the input is of the form Z (not Y.Z or X.Y.Z) then the name Z\n    ** might refer to an result-set alias.  This happens, for example, when\n    ** we are resolving names in the WHERE clause of the following command:\n    **\n    **     SELECT a+b AS x FROM table WHERE x<10;\n    **\n    ** In cases like this, replace pExpr with a copy of the expression that\n    ** forms the result set entry (\"a+b\" in the example) and return immediately.\n    ** Note that the expression in the result set should have already been\n    ** resolved by the time the WHERE clause is resolved.\n    **\n    ** The ability to use an output result-set column in the WHERE, GROUP BY,\n    ** or HAVING clauses, or as part of a larger expression in the ORDER BY\n    ** clause is not standard SQL.  This is a (goofy) SQLite extension, that\n    ** is supported for backwards compatibility only. Hence, we issue a warning\n    ** on sqlite3_log() whenever the capability is used.\n    */\n    if( (pNC->ncFlags & NC_UEList)!=0\n     && cnt==0\n     && zTab==0\n    ){\n      pEList = pNC->uNC.pEList;\n      assert( pEList!=0 );\n      for(j=0; j<pEList->nExpr; j++){\n        char *zAs = pEList->a[j].zName;\n        if( zAs!=0 && sqlite3StrICmp(zAs, zCol)==0 ){\n          Expr *pOrig;\n          assert( pExpr->pLeft==0 && pExpr->pRight==0 );\n          assert( pExpr->x.pList==0 );\n          assert( pExpr->x.pSelect==0 );\n          pOrig = pEList->a[j].pExpr;\n          if( (pNC->ncFlags&NC_AllowAgg)==0 && ExprHasProperty(pOrig, EP_Agg) ){\n            sqlite3ErrorMsg(pParse, \"misuse of aliased aggregate %s\", zAs);\n            return WRC_Abort;\n          }\n          if( (pNC->ncFlags&NC_AllowWin)==0 && ExprHasProperty(pOrig, EP_Win) ){\n            sqlite3ErrorMsg(pParse, \"misuse of aliased window function %s\",zAs);\n            return WRC_Abort;\n          }\n          if( sqlite3ExprVectorSize(pOrig)!=1 ){\n            sqlite3ErrorMsg(pParse, \"row value misused\");\n            return WRC_Abort;\n          }\n          resolveAlias(pParse, pEList, j, pExpr, \"\", nSubquery);\n          cnt = 1;\n          pMatch = 0;\n          assert( zTab==0 && zDb==0 );\n          if( IN_RENAME_OBJECT ){\n            sqlite3RenameTokenRemap(pParse, 0, (void*)pExpr);\n          }\n          goto lookupname_end;\n        }\n      } \n    }\n\n    /* Advance to the next name context.  The loop will exit when either\n    ** we have a match (cnt>0) or when we run out of name contexts.\n    */\n    if( cnt ) break;\n    pNC = pNC->pNext;\n    nSubquery++;\n  }while( pNC );\n\n\n  /*\n  ** If X and Y are NULL (in other words if only the column name Z is\n  ** supplied) and the value of Z is enclosed in double-quotes, then\n  ** Z is a string literal if it doesn't match any column names.  In that\n  ** case, we need to return right away and not make any changes to\n  ** pExpr.\n  **\n  ** Because no reference was made to outer contexts, the pNC->nRef\n  ** fields are not changed in any context.\n  */\n  if( cnt==0 && zTab==0 ){\n    assert( pExpr->op==TK_ID );\n    if( ExprHasProperty(pExpr,EP_DblQuoted)\n     && areDoubleQuotedStringsEnabled(db, pTopNC)\n    ){\n      /* If a double-quoted identifier does not match any known column name,\n      ** then treat it as a string.\n      **\n      ** This hack was added in the early days of SQLite in a misguided attempt\n      ** to be compatible with MySQL 3.x, which used double-quotes for strings.\n      ** I now sorely regret putting in this hack. The effect of this hack is\n      ** that misspelled identifier names are silently converted into strings\n      ** rather than causing an error, to the frustration of countless\n      ** programmers. To all those frustrated programmers, my apologies.\n      **\n      ** Someday, I hope to get rid of this hack. Unfortunately there is\n      ** a huge amount of legacy SQL that uses it. So for now, we just\n      ** issue a warning.\n      */\n      sqlite3_log(SQLITE_WARNING,\n        \"double-quoted string literal: \\\"%w\\\"\", zCol);\n#ifdef SQLITE_ENABLE_NORMALIZE\n      sqlite3VdbeAddDblquoteStr(db, pParse->pVdbe, zCol);\n#endif\n      pExpr->op = TK_STRING;\n      pExpr->y.pTab = 0;\n      return WRC_Prune;\n    }\n    if( sqlite3ExprIdToTrueFalse(pExpr) ){\n      return WRC_Prune;\n    }\n  }\n\n  /*\n  ** cnt==0 means there was not match.  cnt>1 means there were two or\n  ** more matches.  Either way, we have an error.\n  */\n  if( cnt!=1 ){\n    const char *zErr;\n    zErr = cnt==0 ? \"no such column\" : \"ambiguous column name\";\n    if( zDb ){\n      sqlite3ErrorMsg(pParse, \"%s: %s.%s.%s\", zErr, zDb, zTab, zCol);\n    }else if( zTab ){\n      sqlite3ErrorMsg(pParse, \"%s: %s.%s\", zErr, zTab, zCol);\n    }else{\n      sqlite3ErrorMsg(pParse, \"%s: %s\", zErr, zCol);\n    }\n    pParse->checkSchema = 1;\n    pTopNC->nErr++;\n  }\n\n  /* If a column from a table in pSrcList is referenced, then record\n  ** this fact in the pSrcList.a[].colUsed bitmask.  Column 0 causes\n  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.  If the\n  ** column number is greater than the number of bits in the bitmask\n  ** then set the high-order bit of the bitmask.\n  */\n  if( pExpr->iColumn>=0 && pMatch!=0 ){\n    int n = pExpr->iColumn;\n    testcase( n==BMS-1 );\n    if( n>=BMS ){\n      n = BMS-1;\n    }\n    assert( pMatch->iCursor==pExpr->iTable );\n    pMatch->colUsed |= ((Bitmask)1)<<n;\n  }\n\n  /* Clean up and return\n  */\n  sqlite3ExprDelete(db, pExpr->pLeft);\n  pExpr->pLeft = 0;\n  sqlite3ExprDelete(db, pExpr->pRight);\n  pExpr->pRight = 0;\n  pExpr->op = eNewExprOp;\n  ExprSetProperty(pExpr, EP_Leaf);\nlookupname_end:\n  if( cnt==1 ){\n    assert( pNC!=0 );\n    if( !ExprHasProperty(pExpr, EP_Alias) ){\n      sqlite3AuthRead(pParse, pExpr, pSchema, pNC->pSrcList);\n    }\n    /* Increment the nRef value on all name contexts from TopNC up to\n    ** the point where the name matched. */\n    for(;;){\n      assert( pTopNC!=0 );\n      pTopNC->nRef++;\n      if( pTopNC==pNC ) break;\n      pTopNC = pTopNC->pNext;\n    }\n    return WRC_Prune;\n  } else {\n    return WRC_Abort;\n  }\n}",
        "func": "static int lookupName(\n  Parse *pParse,       /* The parsing context */\n  const char *zDb,     /* Name of the database containing table, or NULL */\n  const char *zTab,    /* Name of table containing column, or NULL */\n  const char *zCol,    /* Name of the column. */\n  NameContext *pNC,    /* The name context used to resolve the name */\n  Expr *pExpr          /* Make this EXPR node point to the selected column */\n){\n  int i, j;                         /* Loop counters */\n  int cnt = 0;                      /* Number of matching column names */\n  int cntTab = 0;                   /* Number of matching table names */\n  int nSubquery = 0;                /* How many levels of subquery */\n  sqlite3 *db = pParse->db;         /* The database connection */\n  struct SrcList_item *pItem;       /* Use for looping over pSrcList items */\n  struct SrcList_item *pMatch = 0;  /* The matching pSrcList item */\n  NameContext *pTopNC = pNC;        /* First namecontext in the list */\n  Schema *pSchema = 0;              /* Schema of the expression */\n  int eNewExprOp = TK_COLUMN;       /* New value for pExpr->op on success */\n  Table *pTab = 0;                  /* Table hold the row */\n  Column *pCol;                     /* A column of pTab */\n\n  assert( pNC );     /* the name context cannot be NULL. */\n  assert( zCol );    /* The Z in X.Y.Z cannot be NULL */\n  assert( !ExprHasProperty(pExpr, EP_TokenOnly|EP_Reduced) );\n\n  /* Initialize the node to no-match */\n  pExpr->iTable = -1;\n  ExprSetVVAProperty(pExpr, EP_NoReduce);\n\n  /* Translate the schema name in zDb into a pointer to the corresponding\n  ** schema.  If not found, pSchema will remain NULL and nothing will match\n  ** resulting in an appropriate error message toward the end of this routine\n  */\n  if( zDb ){\n    testcase( pNC->ncFlags & NC_PartIdx );\n    testcase( pNC->ncFlags & NC_IsCheck );\n    if( (pNC->ncFlags & (NC_PartIdx|NC_IsCheck))!=0 ){\n      /* Silently ignore database qualifiers inside CHECK constraints and\n      ** partial indices.  Do not raise errors because that might break\n      ** legacy and because it does not hurt anything to just ignore the\n      ** database name. */\n      zDb = 0;\n    }else{\n      for(i=0; i<db->nDb; i++){\n        assert( db->aDb[i].zDbSName );\n        if( sqlite3StrICmp(db->aDb[i].zDbSName,zDb)==0 ){\n          pSchema = db->aDb[i].pSchema;\n          break;\n        }\n      }\n    }\n  }\n\n  /* Start at the inner-most context and move outward until a match is found */\n  assert( pNC && cnt==0 );\n  do{\n    ExprList *pEList;\n    SrcList *pSrcList = pNC->pSrcList;\n\n    if( pSrcList ){\n      for(i=0, pItem=pSrcList->a; i<pSrcList->nSrc; i++, pItem++){\n        pTab = pItem->pTab;\n        assert( pTab!=0 && pTab->zName!=0 );\n        assert( pTab->nCol>0 );\n        if( pItem->pSelect && (pItem->pSelect->selFlags & SF_NestedFrom)!=0 ){\n          int hit = 0;\n          pEList = pItem->pSelect->pEList;\n          for(j=0; j<pEList->nExpr; j++){\n            if( sqlite3MatchSpanName(pEList->a[j].zSpan, zCol, zTab, zDb) ){\n              cnt++;\n              cntTab = 2;\n              pMatch = pItem;\n              pExpr->iColumn = j;\n              hit = 1;\n            }\n          }\n          if( hit || zTab==0 ) continue;\n        }\n        if( zDb && pTab->pSchema!=pSchema ){\n          continue;\n        }\n        if( zTab ){\n          const char *zTabName = pItem->zAlias ? pItem->zAlias : pTab->zName;\n          assert( zTabName!=0 );\n          if( sqlite3StrICmp(zTabName, zTab)!=0 ){\n            continue;\n          }\n          if( IN_RENAME_OBJECT && pItem->zAlias ){\n            sqlite3RenameTokenRemap(pParse, 0, (void*)&pExpr->y.pTab);\n          }\n        }\n        if( 0==(cntTab++) ){\n          pMatch = pItem;\n        }\n        for(j=0, pCol=pTab->aCol; j<pTab->nCol; j++, pCol++){\n          if( sqlite3StrICmp(pCol->zName, zCol)==0 ){\n            /* If there has been exactly one prior match and this match\n            ** is for the right-hand table of a NATURAL JOIN or is in a \n            ** USING clause, then skip this match.\n            */\n            if( cnt==1 ){\n              if( pItem->fg.jointype & JT_NATURAL ) continue;\n              if( nameInUsingClause(pItem->pUsing, zCol) ) continue;\n            }\n            cnt++;\n            pMatch = pItem;\n            /* Substitute the rowid (column -1) for the INTEGER PRIMARY KEY */\n            pExpr->iColumn = j==pTab->iPKey ? -1 : (i16)j;\n            break;\n          }\n        }\n      }\n      if( pMatch ){\n        pExpr->iTable = pMatch->iCursor;\n        pExpr->y.pTab = pMatch->pTab;\n        /* RIGHT JOIN not (yet) supported */\n        assert( (pMatch->fg.jointype & JT_RIGHT)==0 );\n        if( (pMatch->fg.jointype & JT_LEFT)!=0 ){\n          ExprSetProperty(pExpr, EP_CanBeNull);\n        }\n        pSchema = pExpr->y.pTab->pSchema;\n      }\n    } /* if( pSrcList ) */\n\n#if !defined(SQLITE_OMIT_TRIGGER) || !defined(SQLITE_OMIT_UPSERT)\n    /* If we have not already resolved the name, then maybe \n    ** it is a new.* or old.* trigger argument reference.  Or\n    ** maybe it is an excluded.* from an upsert.\n    */\n    if( zDb==0 && zTab!=0 && cntTab==0 ){\n      pTab = 0;\n#ifndef SQLITE_OMIT_TRIGGER\n      if( pParse->pTriggerTab!=0 ){\n        int op = pParse->eTriggerOp;\n        assert( op==TK_DELETE || op==TK_UPDATE || op==TK_INSERT );\n        if( op!=TK_DELETE && sqlite3StrICmp(\"new\",zTab) == 0 ){\n          pExpr->iTable = 1;\n          pTab = pParse->pTriggerTab;\n        }else if( op!=TK_INSERT && sqlite3StrICmp(\"old\",zTab)==0 ){\n          pExpr->iTable = 0;\n          pTab = pParse->pTriggerTab;\n        }\n      }\n#endif /* SQLITE_OMIT_TRIGGER */\n#ifndef SQLITE_OMIT_UPSERT\n      if( (pNC->ncFlags & NC_UUpsert)!=0 ){\n        Upsert *pUpsert = pNC->uNC.pUpsert;\n        if( pUpsert && sqlite3StrICmp(\"excluded\",zTab)==0 ){\n          pTab = pUpsert->pUpsertSrc->a[0].pTab;\n          pExpr->iTable = 2;\n        }\n      }\n#endif /* SQLITE_OMIT_UPSERT */\n\n      if( pTab ){ \n        int iCol;\n        pSchema = pTab->pSchema;\n        cntTab++;\n        for(iCol=0, pCol=pTab->aCol; iCol<pTab->nCol; iCol++, pCol++){\n          if( sqlite3StrICmp(pCol->zName, zCol)==0 ){\n            if( iCol==pTab->iPKey ){\n              iCol = -1;\n            }\n            break;\n          }\n        }\n        if( iCol>=pTab->nCol && sqlite3IsRowid(zCol) && VisibleRowid(pTab) ){\n          /* IMP: R-51414-32910 */\n          iCol = -1;\n        }\n        if( iCol<pTab->nCol ){\n          cnt++;\n#ifndef SQLITE_OMIT_UPSERT\n          if( pExpr->iTable==2 ){\n            testcase( iCol==(-1) );\n            if( IN_RENAME_OBJECT ){\n              pExpr->iColumn = iCol;\n              pExpr->y.pTab = pTab;\n              eNewExprOp = TK_COLUMN;\n            }else{\n              pExpr->iTable = pNC->uNC.pUpsert->regData + iCol;\n              eNewExprOp = TK_REGISTER;\n              ExprSetProperty(pExpr, EP_Alias);\n            }\n          }else\n#endif /* SQLITE_OMIT_UPSERT */\n          {\n#ifndef SQLITE_OMIT_TRIGGER\n            if( iCol<0 ){\n              pExpr->affExpr = SQLITE_AFF_INTEGER;\n            }else if( pExpr->iTable==0 ){\n              testcase( iCol==31 );\n              testcase( iCol==32 );\n              pParse->oldmask |= (iCol>=32 ? 0xffffffff : (((u32)1)<<iCol));\n            }else{\n              testcase( iCol==31 );\n              testcase( iCol==32 );\n              pParse->newmask |= (iCol>=32 ? 0xffffffff : (((u32)1)<<iCol));\n            }\n            pExpr->y.pTab = pTab;\n            pExpr->iColumn = (i16)iCol;\n            eNewExprOp = TK_TRIGGER;\n#endif /* SQLITE_OMIT_TRIGGER */\n          }\n        }\n      }\n    }\n#endif /* !defined(SQLITE_OMIT_TRIGGER) || !defined(SQLITE_OMIT_UPSERT) */\n\n    /*\n    ** Perhaps the name is a reference to the ROWID\n    */\n    if( cnt==0\n     && cntTab==1\n     && pMatch\n     && (pNC->ncFlags & (NC_IdxExpr|NC_GenCol))==0\n     && sqlite3IsRowid(zCol)\n     && VisibleRowid(pMatch->pTab)\n    ){\n      cnt = 1;\n      pExpr->iColumn = -1;\n      pExpr->affExpr = SQLITE_AFF_INTEGER;\n    }\n\n    /*\n    ** If the input is of the form Z (not Y.Z or X.Y.Z) then the name Z\n    ** might refer to an result-set alias.  This happens, for example, when\n    ** we are resolving names in the WHERE clause of the following command:\n    **\n    **     SELECT a+b AS x FROM table WHERE x<10;\n    **\n    ** In cases like this, replace pExpr with a copy of the expression that\n    ** forms the result set entry (\"a+b\" in the example) and return immediately.\n    ** Note that the expression in the result set should have already been\n    ** resolved by the time the WHERE clause is resolved.\n    **\n    ** The ability to use an output result-set column in the WHERE, GROUP BY,\n    ** or HAVING clauses, or as part of a larger expression in the ORDER BY\n    ** clause is not standard SQL.  This is a (goofy) SQLite extension, that\n    ** is supported for backwards compatibility only. Hence, we issue a warning\n    ** on sqlite3_log() whenever the capability is used.\n    */\n    if( (pNC->ncFlags & NC_UEList)!=0\n     && cnt==0\n     && zTab==0\n    ){\n      pEList = pNC->uNC.pEList;\n      assert( pEList!=0 );\n      for(j=0; j<pEList->nExpr; j++){\n        char *zAs = pEList->a[j].zName;\n        if( zAs!=0 && sqlite3StrICmp(zAs, zCol)==0 ){\n          Expr *pOrig;\n          assert( pExpr->pLeft==0 && pExpr->pRight==0 );\n          assert( pExpr->x.pList==0 );\n          assert( pExpr->x.pSelect==0 );\n          pOrig = pEList->a[j].pExpr;\n          if( (pNC->ncFlags&NC_AllowAgg)==0 && ExprHasProperty(pOrig, EP_Agg) ){\n            sqlite3ErrorMsg(pParse, \"misuse of aliased aggregate %s\", zAs);\n            return WRC_Abort;\n          }\n          if( (pNC->ncFlags&NC_AllowWin)==0 && ExprHasProperty(pOrig, EP_Win) ){\n            sqlite3ErrorMsg(pParse, \"misuse of aliased window function %s\",zAs);\n            return WRC_Abort;\n          }\n          if( sqlite3ExprVectorSize(pOrig)!=1 ){\n            sqlite3ErrorMsg(pParse, \"row value misused\");\n            return WRC_Abort;\n          }\n          resolveAlias(pParse, pEList, j, pExpr, \"\", nSubquery);\n          cnt = 1;\n          pMatch = 0;\n          assert( zTab==0 && zDb==0 );\n          if( IN_RENAME_OBJECT ){\n            sqlite3RenameTokenRemap(pParse, 0, (void*)pExpr);\n          }\n          goto lookupname_end;\n        }\n      } \n    }\n\n    /* Advance to the next name context.  The loop will exit when either\n    ** we have a match (cnt>0) or when we run out of name contexts.\n    */\n    if( cnt ) break;\n    pNC = pNC->pNext;\n    nSubquery++;\n  }while( pNC );\n\n\n  /*\n  ** If X and Y are NULL (in other words if only the column name Z is\n  ** supplied) and the value of Z is enclosed in double-quotes, then\n  ** Z is a string literal if it doesn't match any column names.  In that\n  ** case, we need to return right away and not make any changes to\n  ** pExpr.\n  **\n  ** Because no reference was made to outer contexts, the pNC->nRef\n  ** fields are not changed in any context.\n  */\n  if( cnt==0 && zTab==0 ){\n    assert( pExpr->op==TK_ID );\n    if( ExprHasProperty(pExpr,EP_DblQuoted)\n     && areDoubleQuotedStringsEnabled(db, pTopNC)\n    ){\n      /* If a double-quoted identifier does not match any known column name,\n      ** then treat it as a string.\n      **\n      ** This hack was added in the early days of SQLite in a misguided attempt\n      ** to be compatible with MySQL 3.x, which used double-quotes for strings.\n      ** I now sorely regret putting in this hack. The effect of this hack is\n      ** that misspelled identifier names are silently converted into strings\n      ** rather than causing an error, to the frustration of countless\n      ** programmers. To all those frustrated programmers, my apologies.\n      **\n      ** Someday, I hope to get rid of this hack. Unfortunately there is\n      ** a huge amount of legacy SQL that uses it. So for now, we just\n      ** issue a warning.\n      */\n      sqlite3_log(SQLITE_WARNING,\n        \"double-quoted string literal: \\\"%w\\\"\", zCol);\n#ifdef SQLITE_ENABLE_NORMALIZE\n      sqlite3VdbeAddDblquoteStr(db, pParse->pVdbe, zCol);\n#endif\n      pExpr->op = TK_STRING;\n      pExpr->y.pTab = 0;\n      return WRC_Prune;\n    }\n    if( sqlite3ExprIdToTrueFalse(pExpr) ){\n      return WRC_Prune;\n    }\n  }\n\n  /*\n  ** cnt==0 means there was not match.  cnt>1 means there were two or\n  ** more matches.  Either way, we have an error.\n  */\n  if( cnt!=1 ){\n    const char *zErr;\n    zErr = cnt==0 ? \"no such column\" : \"ambiguous column name\";\n    if( zDb ){\n      sqlite3ErrorMsg(pParse, \"%s: %s.%s.%s\", zErr, zDb, zTab, zCol);\n    }else if( zTab ){\n      sqlite3ErrorMsg(pParse, \"%s: %s.%s\", zErr, zTab, zCol);\n    }else{\n      sqlite3ErrorMsg(pParse, \"%s: %s\", zErr, zCol);\n    }\n    pParse->checkSchema = 1;\n    pTopNC->nErr++;\n  }\n\n  /* If a column from a table in pSrcList is referenced, then record\n  ** this fact in the pSrcList.a[].colUsed bitmask.  Column 0 causes\n  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.\n  **\n  ** The colUsed mask is an optimization used to help determine if an\n  ** index is a covering index.  The correct answer is still obtained\n  ** if the mask contains extra bits.  But omitting bits from the mask\n  ** might result in an incorrect answer.\n  **\n  ** The high-order bit of the mask is a \"we-use-them-all\" bit.\n  ** If the column number is greater than the number of bits in the bitmask\n  ** then set the high-order bit of the bitmask.  Also set the high-order\n  ** bit if the column is a generated column, as that adds dependencies\n  ** that are difficult to track, so we assume that all columns are used.\n  */\n  if( pExpr->iColumn>=0 && pMatch!=0 ){\n    int n = pExpr->iColumn;\n    testcase( n==BMS-1 );\n    if( n>=BMS ){\n      n = BMS-1;\n    }\n    assert( pExpr->y.pTab!=0 );\n    assert( pMatch->iCursor==pExpr->iTable );\n    if( pExpr->y.pTab->tabFlags & TF_HasGenerated ){\n      Column *pCol = pExpr->y.pTab->aCol + pExpr->iColumn;\n      if( pCol->colFlags & COLFLAG_GENERATED ) n = BMS-1;\n    }\n    pMatch->colUsed |= ((Bitmask)1)<<n;\n  }\n\n  /* Clean up and return\n  */\n  sqlite3ExprDelete(db, pExpr->pLeft);\n  pExpr->pLeft = 0;\n  sqlite3ExprDelete(db, pExpr->pRight);\n  pExpr->pRight = 0;\n  pExpr->op = eNewExprOp;\n  ExprSetProperty(pExpr, EP_Leaf);\nlookupname_end:\n  if( cnt==1 ){\n    assert( pNC!=0 );\n    if( !ExprHasProperty(pExpr, EP_Alias) ){\n      sqlite3AuthRead(pParse, pExpr, pSchema, pNC->pSrcList);\n    }\n    /* Increment the nRef value on all name contexts from TopNC up to\n    ** the point where the name matched. */\n    for(;;){\n      assert( pTopNC!=0 );\n      pTopNC->nRef++;\n      if( pTopNC==pNC ) break;\n      pTopNC = pTopNC->pNext;\n    }\n    return WRC_Prune;\n  } else {\n    return WRC_Abort;\n  }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -350,9 +350,18 @@\n \n   /* If a column from a table in pSrcList is referenced, then record\n   ** this fact in the pSrcList.a[].colUsed bitmask.  Column 0 causes\n-  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.  If the\n-  ** column number is greater than the number of bits in the bitmask\n-  ** then set the high-order bit of the bitmask.\n+  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.\n+  **\n+  ** The colUsed mask is an optimization used to help determine if an\n+  ** index is a covering index.  The correct answer is still obtained\n+  ** if the mask contains extra bits.  But omitting bits from the mask\n+  ** might result in an incorrect answer.\n+  **\n+  ** The high-order bit of the mask is a \"we-use-them-all\" bit.\n+  ** If the column number is greater than the number of bits in the bitmask\n+  ** then set the high-order bit of the bitmask.  Also set the high-order\n+  ** bit if the column is a generated column, as that adds dependencies\n+  ** that are difficult to track, so we assume that all columns are used.\n   */\n   if( pExpr->iColumn>=0 && pMatch!=0 ){\n     int n = pExpr->iColumn;\n@@ -360,7 +369,12 @@\n     if( n>=BMS ){\n       n = BMS-1;\n     }\n+    assert( pExpr->y.pTab!=0 );\n     assert( pMatch->iCursor==pExpr->iTable );\n+    if( pExpr->y.pTab->tabFlags & TF_HasGenerated ){\n+      Column *pCol = pExpr->y.pTab->aCol + pExpr->iColumn;\n+      if( pCol->colFlags & COLFLAG_GENERATED ) n = BMS-1;\n+    }\n     pMatch->colUsed |= ((Bitmask)1)<<n;\n   }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.  If the",
                "  ** column number is greater than the number of bits in the bitmask",
                "  ** then set the high-order bit of the bitmask."
            ],
            "added_lines": [
                "  ** bit 0 to be set.  Column 1 sets bit 1.  And so forth.",
                "  **",
                "  ** The colUsed mask is an optimization used to help determine if an",
                "  ** index is a covering index.  The correct answer is still obtained",
                "  ** if the mask contains extra bits.  But omitting bits from the mask",
                "  ** might result in an incorrect answer.",
                "  **",
                "  ** The high-order bit of the mask is a \"we-use-them-all\" bit.",
                "  ** If the column number is greater than the number of bits in the bitmask",
                "  ** then set the high-order bit of the bitmask.  Also set the high-order",
                "  ** bit if the column is a generated column, as that adds dependencies",
                "  ** that are difficult to track, so we assume that all columns are used.",
                "    assert( pExpr->y.pTab!=0 );",
                "    if( pExpr->y.pTab->tabFlags & TF_HasGenerated ){",
                "      Column *pCol = pExpr->y.pTab->aCol + pExpr->iColumn;",
                "      if( pCol->colFlags & COLFLAG_GENERATED ) n = BMS-1;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-36357",
        "func_name": "open-power/skiboot/unpack_timestamp",
        "description": "An issue was discovered in OpenPOWER 2.6 firmware. unpack_timestamp() calls le32_to_cpu() for endian conversion of a uint16_t \"year\" value, resulting in a type mismatch that can truncate a higher integer value to a smaller one, and bypass a timestamp check. The fix is to use the right endian conversion function.",
        "git_url": "https://github.com/open-power/skiboot/commit/5be38b672c1410e2f10acd3ad2eecfdc81d5daf7",
        "commit_title": "secvar: fix endian conversion",
        "commit_text": " unpack_timestamp() calls le32_to_cpu() for endian conversion of uint16_t \"year\" value. This patch fixes the code to use le16_to_cpu(). ",
        "func_before": "static uint64_t unpack_timestamp(const struct efi_time *timestamp)\n{\n\tuint64_t val = 0;\n\tuint16_t year = le32_to_cpu(timestamp->year);\n\n\t/* pad1, nanosecond, timezone, daylight and pad2 are meant to be zero */\n\tval |= ((uint64_t) timestamp->pad1 & 0xFF) << 0;\n\tval |= ((uint64_t) timestamp->second & 0xFF) << (1*8);\n\tval |= ((uint64_t) timestamp->minute & 0xFF) << (2*8);\n\tval |= ((uint64_t) timestamp->hour & 0xFF) << (3*8);\n\tval |= ((uint64_t) timestamp->day & 0xFF) << (4*8);\n\tval |= ((uint64_t) timestamp->month & 0xFF) << (5*8);\n\tval |= ((uint64_t) year) << (6*8);\n\n\treturn val;\n}",
        "func": "static uint64_t unpack_timestamp(const struct efi_time *timestamp)\n{\n\tuint64_t val = 0;\n\tuint16_t year = le16_to_cpu(timestamp->year);\n\n\t/* pad1, nanosecond, timezone, daylight and pad2 are meant to be zero */\n\tval |= ((uint64_t) timestamp->pad1 & 0xFF) << 0;\n\tval |= ((uint64_t) timestamp->second & 0xFF) << (1*8);\n\tval |= ((uint64_t) timestamp->minute & 0xFF) << (2*8);\n\tval |= ((uint64_t) timestamp->hour & 0xFF) << (3*8);\n\tval |= ((uint64_t) timestamp->day & 0xFF) << (4*8);\n\tval |= ((uint64_t) timestamp->month & 0xFF) << (5*8);\n\tval |= ((uint64_t) year) << (6*8);\n\n\treturn val;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n static uint64_t unpack_timestamp(const struct efi_time *timestamp)\n {\n \tuint64_t val = 0;\n-\tuint16_t year = le32_to_cpu(timestamp->year);\n+\tuint16_t year = le16_to_cpu(timestamp->year);\n \n \t/* pad1, nanosecond, timezone, daylight and pad2 are meant to be zero */\n \tval |= ((uint64_t) timestamp->pad1 & 0xFF) << 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\tuint16_t year = le32_to_cpu(timestamp->year);"
            ],
            "added_lines": [
                "\tuint16_t year = le16_to_cpu(timestamp->year);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27194",
        "func_name": "torvalds/linux/scalar32_min_max_or",
        "description": "An issue was discovered in the Linux kernel before 5.8.15. scalar32_min_max_or in kernel/bpf/verifier.c mishandles bounds tracking during use of 64-bit values, aka CID-5b9fbeb75b6a.",
        "git_url": "https://github.com/torvalds/linux/commit/5b9fbeb75b6a98955f628e205ac26689bcb1383e",
        "commit_title": "bpf: Fix scalar32_min_max_or bounds tracking",
        "commit_text": " Simon reported an issue with the current scalar32_min_max_or() implementation. That is, compared to the other 32 bit subreg tracking functions, the code in scalar32_min_max_or() stands out that it's using the 64 bit registers instead of 32 bit ones. This leads to bounds tracking issues, for example:    [...]   8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm   8: (79) r1 = *(u64 *)(r0 +0)    R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm   9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm   9: (b7) r0 = 1   10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm   10: (18) r2 = 0x600000002   12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   12: (ad) if r1 < r2 goto pc+1    R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   13: (95) exit   14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   14: (25) if r1 > 0x0 goto pc+1    R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   15: (95) exit   16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   16: (47) r1 |= 0   17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x1; 0x700000000),s32_max_value=1,u32_max_value=1) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   [...]  The bound tests on the map value force the upper unsigned bound to be 25769803777 in 64 bit (0b11000000000000000000000000000000001) and then lower one to be 1. By using OR they are truncated and thus result in the range [1,1] for the 32 bit reg tracker. This is incorrect given the only thing we know is that the value must be positive and thus 2147483647 (0b1111111111111111111111111111111) at max for the subregs. Fix it by using the {u,s}32_{min,max}_value vars instead. This also makes sense, for example, for the case where we update dst_reg->s32_{min,max}_value in the else branch we need to use the newly computed dst_reg->u32_{min,max}_value as we know that these are positive. Previously, in the else branch the 64 bit values of umin_value=1 and umax_value=32212254719 were used and latter got truncated to be 1 as upper bound there. After the fix the subreg range is now correct:    [...]   8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm   8: (79) r1 = *(u64 *)(r0 +0)    R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm   9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm   9: (b7) r0 = 1   10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm   10: (18) r2 = 0x600000002   12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   12: (ad) if r1 < r2 goto pc+1    R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   13: (95) exit   14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   14: (25) if r1 > 0x0 goto pc+1    R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   15: (95) exit   16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   16: (47) r1 |= 0   17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm   [...] ",
        "func_before": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->smin_value;\n\tu32 umin_val = src_reg->umin_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->umin_value;\n\t\tdst_reg->s32_max_value = dst_reg->umax_value;\n\t}\n}",
        "func": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,8 +4,8 @@\n \tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n \tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n \tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n-\ts32 smin_val = src_reg->smin_value;\n-\tu32 umin_val = src_reg->umin_value;\n+\ts32 smin_val = src_reg->s32_min_value;\n+\tu32 umin_val = src_reg->u32_min_value;\n \n \t/* Assuming scalar64_min_max_or will be called so it is safe\n \t * to skip updating register for known case.\n@@ -28,7 +28,7 @@\n \t\t/* ORing two positives gives a positive, so safe to\n \t\t * cast result into s64.\n \t\t */\n-\t\tdst_reg->s32_min_value = dst_reg->umin_value;\n-\t\tdst_reg->s32_max_value = dst_reg->umax_value;\n+\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n+\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\ts32 smin_val = src_reg->smin_value;",
                "\tu32 umin_val = src_reg->umin_value;",
                "\t\tdst_reg->s32_min_value = dst_reg->umin_value;",
                "\t\tdst_reg->s32_max_value = dst_reg->umax_value;"
            ],
            "added_lines": [
                "\ts32 smin_val = src_reg->s32_min_value;",
                "\tu32 umin_val = src_reg->u32_min_value;",
                "\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;",
                "\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-27218",
        "func_name": "GNOME/glib/g_byte_array_new_take",
        "description": "An issue was discovered in GNOME GLib before 2.66.7 and 2.67.x before 2.67.4. If g_byte_array_new_take() was called with a buffer of 4GB or more on a 64-bit platform, the length would be truncated modulo 2**32, causing unintended length truncation.",
        "git_url": "https://github.com/GNOME/glib/commit/0f384c88a241bbbd884487b1c40b7b75f1e638d3",
        "commit_title": "gbytearray: Do not accept too large byte arrays",
        "commit_text": " GByteArray uses guint for storing the length of the byte array, but it also has a constructor (g_byte_array_new_take) that takes length as a gsize. gsize may be larger than guint (64 bits for gsize vs 32 bits for guint). It is possible to call the function with a value greater than G_MAXUINT, which will result in silent length truncation. This may happen as a result of unreffing GBytes into GByteArray, so rather be loud about it.  (Test case tweaked by Philip Withnall.)  (Backport 2.66: Add #include gstrfuncsprivate.h in the test case for `g_memdup2()`.)",
        "func_before": "GByteArray*\ng_byte_array_new_take (guint8 *data,\n                       gsize   len)\n{\n  GByteArray *array;\n  GRealArray *real;\n\n  array = g_byte_array_new ();\n  real = (GRealArray *)array;\n  g_assert (real->data == NULL);\n  g_assert (real->len == 0);\n\n  real->data = data;\n  real->len = len;\n  real->alloc = len;\n\n  return array;\n}",
        "func": "GByteArray*\ng_byte_array_new_take (guint8 *data,\n                       gsize   len)\n{\n  GByteArray *array;\n  GRealArray *real;\n\n  g_return_val_if_fail (len <= G_MAXUINT, NULL);\n\n  array = g_byte_array_new ();\n  real = (GRealArray *)array;\n  g_assert (real->data == NULL);\n  g_assert (real->len == 0);\n\n  real->data = data;\n  real->len = len;\n  real->alloc = len;\n\n  return array;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,6 +4,8 @@\n {\n   GByteArray *array;\n   GRealArray *real;\n+\n+  g_return_val_if_fail (len <= G_MAXUINT, NULL);\n \n   array = g_byte_array_new ();\n   real = (GRealArray *)array;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  g_return_val_if_fail (len <= G_MAXUINT, NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3444",
        "func_name": "torvalds/linux/fixup_bpf_calls",
        "description": "The bpf verifier in the Linux kernel did not properly handle mod32 destination register truncation when the source register was known to be 0. A local attacker with the ability to load bpf programs could use this gain out-of-bounds reads in kernel memory leading to information disclosure (kernel memory), and possibly out-of-bounds writes that could potentially lead to code execution. This issue was addressed in the upstream kernel in commit 9b00f1b78809 (\"bpf: Fix truncation handling for mod32 dst reg wrt zero\") and in Linux stable kernels 5.11.2, 5.10.19, and 5.4.101.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=9b00f1b78809",
        "commit_title": "Recently noticed that when mod32 with a known src reg of 0 is performed,",
        "commit_text": "then the dst register is 32-bit truncated in verifier:    0: R1=ctx(id=0,off=0,imm=0) R10=fp0   0: (b7) r0 = 0   1: R0_w=inv0 R1=ctx(id=0,off=0,imm=0) R10=fp0   1: (b7) r1 = -1   2: R0_w=inv0 R1_w=inv-1 R10=fp0   2: (b4) w2 = -1   3: R0_w=inv0 R1_w=inv-1 R2_w=inv4294967295 R10=fp0   3: (9c) w1 %= w0   4: R0_w=inv0 R1_w=inv(id=0,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2_w=inv4294967295 R10=fp0   4: (b7) r0 = 1   5: R0_w=inv1 R1_w=inv(id=0,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2_w=inv4294967295 R10=fp0   5: (1d) if r1 == r2 goto pc+1    R0_w=inv1 R1_w=inv(id=0,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2_w=inv4294967295 R10=fp0   6: R0_w=inv1 R1_w=inv(id=0,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2_w=inv4294967295 R10=fp0   6: (b7) r0 = 2   7: R0_w=inv2 R1_w=inv(id=0,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2_w=inv4294967295 R10=fp0   7: (95) exit   7: R0=inv1 R1=inv(id=0,umin_value=4294967295,umax_value=4294967295,var_off=(0x0; 0xffffffff)) R2=inv4294967295 R10=fp0   7: (95) exit  However, as a runtime result, we get 2 instead of 1, meaning the dst register does not contain (u32)-1 in this case. The reason is fairly straight forward given the 0 test leaves the dst register as-is:    # ./bpftool p d x i 23    0: (b7) r0 = 0    1: (b7) r1 = -1    2: (b4) w2 = -1    3: (16) if w0 == 0x0 goto pc+1    4: (9c) w1 %= w0    5: (b7) r0 = 1    6: (1d) if r1 == r2 goto pc+1    7: (b7) r0 = 2    8: (95) exit  This was originally not an issue given the dst register was marked as completely unknown (aka 64 bit unknown). However, after 468f6eafa6c4 (\"bpf: fix 32-bit ALU op verification\") the verifier casts the register output to 32 bit, and hence it becomes 32 bit unknown. Note that for the case where the src register is unknown, the dst register is marked 64 bit unknown. After the fix, the register is truncated by the runtime and the test passes:    # ./bpftool p d x i 23    0: (b7) r0 = 0    1: (b7) r1 = -1    2: (b4) w2 = -1    3: (16) if w0 == 0x0 goto pc+2    4: (9c) w1 %= w0    5: (05) goto pc+1    6: (bc) w1 = w1    7: (b7) r0 = 1    8: (1d) if r1 == r2 goto pc+1    9: (b7) r0 = 2   10: (95) exit  Semantics also match with {R,W}x mod{64,32} 0 -> {R,W}x. Invalid div has always been {R,W}x div{64,32} 0 -> 0. Rewrites are as follows:    mod32:                            mod64:    (16) if w0 == 0x0 goto pc+2       (15) if r0 == 0x0 goto pc+1   (9c) w1 %= w0                     (9f) r1 %= r0   (05) goto pc+1   (bc) w1 = w1  ",
        "func_before": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n\t\t\tstruct bpf_insn *patchlet;\n\t\t\tstruct bpf_insn chk_and_div[] = {\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 2, 0),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn chk_and_mod[] = {\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 1, 0),\n\t\t\t\t*insn,\n\t\t\t};\n\n\t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n\t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n\t\t\t\t      ARRAY_SIZE(chk_and_mod);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn insn_buf[16];\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\tif (issrc) {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,\n\t\t\t\t\t\t\t off_reg);\n\t\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\t} else {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,\n\t\t\t\t\t\t\t BPF_REG_AX);\n\t\t\t}\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tif (!allow_tail_call_in_subprogs(env))\n\t\t\t\tprog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tprog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t\t.insn_idx = i + delta,\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == -EOPNOTSUPP)\n\t\t\t\t\tgoto patch_map_ops_generic;\n\t\t\t\tif (cnt <= 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\npatch_map_ops_generic:\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func": "static int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n\t\t\tstruct bpf_insn *patchlet;\n\t\t\tstruct bpf_insn chk_and_div[] = {\n\t\t\t\t/* [R,W]x div 0 -> 0 */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 2, 0),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn chk_and_mod[] = {\n\t\t\t\t/* [R,W]x mod 0 -> [R,W]x */\n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),\n\t\t\t\t*insn,\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),\n\t\t\t};\n\n\t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n\t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn insn_buf[16];\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\tif (issrc) {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,\n\t\t\t\t\t\t\t off_reg);\n\t\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\t} else {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,\n\t\t\t\t\t\t\t BPF_REG_AX);\n\t\t\t}\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tif (!allow_tail_call_in_subprogs(env))\n\t\t\t\tprog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tprog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t\t.insn_idx = i + delta,\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == -EOPNOTSUPP)\n\t\t\t\t\tgoto patch_map_ops_generic;\n\t\t\t\tif (cnt <= 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\npatch_map_ops_generic:\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,7 @@\n \t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n \t\t\tstruct bpf_insn *patchlet;\n \t\t\tstruct bpf_insn chk_and_div[] = {\n-\t\t\t\t/* Rx div 0 -> 0 */\n+\t\t\t\t/* [R,W]x div 0 -> 0 */\n \t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n \t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n \t\t\t\t\t     0, 2, 0),\n@@ -30,16 +30,18 @@\n \t\t\t\t*insn,\n \t\t\t};\n \t\t\tstruct bpf_insn chk_and_mod[] = {\n-\t\t\t\t/* Rx mod 0 -> Rx */\n+\t\t\t\t/* [R,W]x mod 0 -> [R,W]x */\n \t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n \t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n-\t\t\t\t\t     0, 1, 0),\n+\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),\n \t\t\t\t*insn,\n+\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n+\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),\n \t\t\t};\n \n \t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n \t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n-\t\t\t\t      ARRAY_SIZE(chk_and_mod);\n+\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);\n \n \t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n \t\t\tif (!new_prog)",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\t/* Rx div 0 -> 0 */",
                "\t\t\t\t/* Rx mod 0 -> Rx */",
                "\t\t\t\t\t     0, 1, 0),",
                "\t\t\t\t      ARRAY_SIZE(chk_and_mod);"
            ],
            "added_lines": [
                "\t\t\t\t/* [R,W]x div 0 -> 0 */",
                "\t\t\t\t/* [R,W]x mod 0 -> [R,W]x */",
                "\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),",
                "\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),",
                "\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),",
                "\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-4032",
        "func_name": "FreeRDP/update_recv_secondary_order",
        "description": "In FreeRDP before version 2.1.2, there is an integer casting vulnerability in update_recv_secondary_order. All clients with +glyph-cache /relax-order-checks are affected. This is fixed in version 2.1.2.",
        "git_url": "https://github.com/FreeRDP/FreeRDP/commit/e7bffa64ef5ed70bac94f823e2b95262642f5296",
        "commit_title": "Fixed OOB read in update_recv_secondary_order",
        "commit_text": " CVE-2020-4032 thanks to @antonio-morales for finding this.",
        "func_before": "static BOOL update_recv_secondary_order(rdpUpdate* update, wStream* s, BYTE flags)\n{\n\tBOOL rc = FALSE;\n\tsize_t start, end, diff;\n\tBYTE orderType;\n\tUINT16 extraFlags;\n\tUINT16 orderLength;\n\trdpContext* context = update->context;\n\trdpSettings* settings = context->settings;\n\trdpSecondaryUpdate* secondary = update->secondary;\n\tconst char* name;\n\n\tif (Stream_GetRemainingLength(s) < 5)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) < 5\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, orderLength); /* orderLength (2 bytes) */\n\tStream_Read_UINT16(s, extraFlags);  /* extraFlags (2 bytes) */\n\tStream_Read_UINT8(s, orderType);    /* orderType (1 byte) */\n\tif (Stream_GetRemainingLength(s) < orderLength + 7U)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) %\" PRIuz \" < %\" PRIu16,\n\t\t           Stream_GetRemainingLength(s), orderLength + 7);\n\t\treturn FALSE;\n\t}\n\n\tstart = Stream_GetPosition(s);\n\tname = secondary_order_string(orderType);\n\tWLog_Print(update->log, WLOG_DEBUG, \"Secondary Drawing Order %s\", name);\n\n\tif (!check_secondary_order_supported(update->log, settings, orderType, name))\n\t\treturn FALSE;\n\n\tswitch (orderType)\n\t{\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED:\n\t\tcase ORDER_TYPE_CACHE_BITMAP_COMPRESSED:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_CACHE_BITMAP_COMPRESSED);\n\t\t\tCACHE_BITMAP_ORDER* order =\n\t\t\t    update_read_cache_bitmap_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmap, context, order);\n\t\t\t\tfree_cache_bitmap_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED_V2:\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V2:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_BITMAP_COMPRESSED_V2);\n\t\t\tCACHE_BITMAP_V2_ORDER* order =\n\t\t\t    update_read_cache_bitmap_v2_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV2, context, order);\n\t\t\t\tfree_cache_bitmap_v2_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V3:\n\t\t{\n\t\t\tCACHE_BITMAP_V3_ORDER* order = update_read_cache_bitmap_v3_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV3, context, order);\n\t\t\t\tfree_cache_bitmap_v3_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_COLOR_TABLE:\n\t\t{\n\t\t\tCACHE_COLOR_TABLE_ORDER* order =\n\t\t\t    update_read_cache_color_table_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheColorTable, context, order);\n\t\t\t\tfree_cache_color_table_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_GLYPH:\n\t\t{\n\t\t\tswitch (settings->GlyphSupportLevel)\n\t\t\t{\n\t\t\t\tcase GLYPH_SUPPORT_PARTIAL:\n\t\t\t\tcase GLYPH_SUPPORT_FULL:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_ORDER* order = update_read_cache_glyph_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyph, context, order);\n\t\t\t\t\t\tfree_cache_glyph_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_ENCODE:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_V2_ORDER* order =\n\t\t\t\t\t    update_read_cache_glyph_v2_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyphV2, context, order);\n\t\t\t\t\t\tfree_cache_glyph_v2_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_NONE:\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_BRUSH:\n\t\t\t/* [MS-RDPEGDI] 2.2.2.2.1.2.7 Cache Brush (CACHE_BRUSH_ORDER) */\n\t\t\t{\n\t\t\t\tCACHE_BRUSH_ORDER* order = update_read_cache_brush_order(update, s, extraFlags);\n\n\t\t\t\tif (order)\n\t\t\t\t{\n\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBrush, context, order);\n\t\t\t\t\tfree_cache_brush_order(context, order);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY ORDER %s not supported\", name);\n\t\t\tbreak;\n\t}\n\n\tif (!rc)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"SECONDARY ORDER %s failed\", name);\n\t}\n\n\tstart += orderLength + 7;\n\tend = Stream_GetPosition(s);\n\tif (start > end)\n\t{\n\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes too much\",\n\t\t           name, end - start);\n\t\treturn FALSE;\n\t}\n\tdiff = start - end;\n\tif (diff > 0)\n\t{\n\t\tWLog_Print(update->log, WLOG_DEBUG,\n\t\t           \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes short, skipping\", name, diff);\n\t\tStream_Seek(s, diff);\n\t}\n\treturn rc;\n}",
        "func": "static BOOL update_recv_secondary_order(rdpUpdate* update, wStream* s, BYTE flags)\n{\n\tBOOL rc = FALSE;\n\tsize_t start, end, diff;\n\tBYTE orderType;\n\tUINT16 extraFlags;\n\tUINT16 orderLength;\n\trdpContext* context = update->context;\n\trdpSettings* settings = context->settings;\n\trdpSecondaryUpdate* secondary = update->secondary;\n\tconst char* name;\n\n\tif (Stream_GetRemainingLength(s) < 5)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) < 5\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, orderLength); /* orderLength (2 bytes) */\n\tStream_Read_UINT16(s, extraFlags);  /* extraFlags (2 bytes) */\n\tStream_Read_UINT8(s, orderType);    /* orderType (1 byte) */\n\tif (Stream_GetRemainingLength(s) < orderLength + 7U)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"Stream_GetRemainingLength(s) %\" PRIuz \" < %\" PRIu16,\n\t\t           Stream_GetRemainingLength(s), orderLength + 7);\n\t\treturn FALSE;\n\t}\n\n\tstart = Stream_GetPosition(s);\n\tname = secondary_order_string(orderType);\n\tWLog_Print(update->log, WLOG_DEBUG, \"Secondary Drawing Order %s\", name);\n\n\tif (!check_secondary_order_supported(update->log, settings, orderType, name))\n\t\treturn FALSE;\n\n\tswitch (orderType)\n\t{\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED:\n\t\tcase ORDER_TYPE_CACHE_BITMAP_COMPRESSED:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_CACHE_BITMAP_COMPRESSED);\n\t\t\tCACHE_BITMAP_ORDER* order =\n\t\t\t    update_read_cache_bitmap_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmap, context, order);\n\t\t\t\tfree_cache_bitmap_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_UNCOMPRESSED_V2:\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V2:\n\t\t{\n\t\t\tconst BOOL compressed = (orderType == ORDER_TYPE_BITMAP_COMPRESSED_V2);\n\t\t\tCACHE_BITMAP_V2_ORDER* order =\n\t\t\t    update_read_cache_bitmap_v2_order(update, s, compressed, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV2, context, order);\n\t\t\t\tfree_cache_bitmap_v2_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_BITMAP_COMPRESSED_V3:\n\t\t{\n\t\t\tCACHE_BITMAP_V3_ORDER* order = update_read_cache_bitmap_v3_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBitmapV3, context, order);\n\t\t\t\tfree_cache_bitmap_v3_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_COLOR_TABLE:\n\t\t{\n\t\t\tCACHE_COLOR_TABLE_ORDER* order =\n\t\t\t    update_read_cache_color_table_order(update, s, extraFlags);\n\n\t\t\tif (order)\n\t\t\t{\n\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheColorTable, context, order);\n\t\t\t\tfree_cache_color_table_order(context, order);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_GLYPH:\n\t\t{\n\t\t\tswitch (settings->GlyphSupportLevel)\n\t\t\t{\n\t\t\t\tcase GLYPH_SUPPORT_PARTIAL:\n\t\t\t\tcase GLYPH_SUPPORT_FULL:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_ORDER* order = update_read_cache_glyph_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyph, context, order);\n\t\t\t\t\t\tfree_cache_glyph_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_ENCODE:\n\t\t\t\t{\n\t\t\t\t\tCACHE_GLYPH_V2_ORDER* order =\n\t\t\t\t\t    update_read_cache_glyph_v2_order(update, s, extraFlags);\n\n\t\t\t\t\tif (order)\n\t\t\t\t\t{\n\t\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheGlyphV2, context, order);\n\t\t\t\t\t\tfree_cache_glyph_v2_order(context, order);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\t\tcase GLYPH_SUPPORT_NONE:\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\t\tcase ORDER_TYPE_CACHE_BRUSH:\n\t\t\t/* [MS-RDPEGDI] 2.2.2.2.1.2.7 Cache Brush (CACHE_BRUSH_ORDER) */\n\t\t\t{\n\t\t\t\tCACHE_BRUSH_ORDER* order = update_read_cache_brush_order(update, s, extraFlags);\n\n\t\t\t\tif (order)\n\t\t\t\t{\n\t\t\t\t\trc = IFCALLRESULT(FALSE, secondary->CacheBrush, context, order);\n\t\t\t\t\tfree_cache_brush_order(context, order);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY ORDER %s not supported\", name);\n\t\t\tbreak;\n\t}\n\n\tif (!rc)\n\t{\n\t\tWLog_Print(update->log, WLOG_ERROR, \"SECONDARY ORDER %s failed\", name);\n\t}\n\n\tstart += orderLength + 7;\n\tend = Stream_GetPosition(s);\n\tif (start > end)\n\t{\n\t\tWLog_Print(update->log, WLOG_WARN, \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes too much\",\n\t\t           name, end - start);\n\t\treturn FALSE;\n\t}\n\tdiff = end - start;\n\tif (diff > 0)\n\t{\n\t\tWLog_Print(update->log, WLOG_DEBUG,\n\t\t           \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes short, skipping\", name, diff);\n\t\tif (!Stream_SafeSeek(s, diff))\n\t\t\treturn FALSE;\n\t}\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -158,12 +158,13 @@\n \t\t           name, end - start);\n \t\treturn FALSE;\n \t}\n-\tdiff = start - end;\n+\tdiff = end - start;\n \tif (diff > 0)\n \t{\n \t\tWLog_Print(update->log, WLOG_DEBUG,\n \t\t           \"SECONDARY_ORDER %s: read %\" PRIuz \"bytes short, skipping\", name, diff);\n-\t\tStream_Seek(s, diff);\n+\t\tif (!Stream_SafeSeek(s, diff))\n+\t\t\treturn FALSE;\n \t}\n \treturn rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tdiff = start - end;",
                "\t\tStream_Seek(s, diff);"
            ],
            "added_lines": [
                "\tdiff = end - start;",
                "\t\tif (!Stream_SafeSeek(s, diff))",
                "\t\t\treturn FALSE;"
            ]
        }
    }
]