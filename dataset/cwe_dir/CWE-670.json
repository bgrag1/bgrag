[
    {
        "cve_id": "CVE-2019-11412",
        "func_name": "ccxvii/mujs/ctrycatchfinally",
        "description": "An issue was discovered in Artifex MuJS 1.0.5. jscompile.c can cause a denial of service (invalid stack-frame jump) because it lacks an ENDTRY opcode call.",
        "git_url": "https://github.com/ccxvii/mujs/commit/1e5479084bc9852854feb1ba9bf68b52cd127e02",
        "commit_title": "Bug 700947: Add missing ENDTRY opcode in try/catch/finally byte code.",
        "commit_text": " In one of the code branches in handling exceptions in the catch block we forgot to call the ENDTRY opcode to pop the inner hidden try. This leads to an unbalanced exception stack which can cause a crash due to us jumping to a stack frame that has already been exited.",
        "func_before": "static void ctrycatchfinally(JF, js_Ast *trystm, js_Ast *catchvar, js_Ast *catchstm, js_Ast *finallystm)\n{\n\tint L1, L2, L3;\n\tL1 = emitjump(J, F, OP_TRY);\n\t{\n\t\t/* if we get here, we have caught an exception in the try block */\n\t\tL2 = emitjump(J, F, OP_TRY);\n\t\t{\n\t\t\t/* if we get here, we have caught an exception in the catch block */\n\t\t\tcstm(J, F, finallystm); /* inline finally block */\n\t\t\temit(J, F, OP_THROW); /* rethrow exception */\n\t\t}\n\t\tlabel(J, F, L2);\n\t\tif (F->strict) {\n\t\t\tcheckfutureword(J, F, catchvar);\n\t\t\tif (!strcmp(catchvar->string, \"arguments\"))\n\t\t\t\tjsC_error(J, catchvar, \"redefining 'arguments' is not allowed in strict mode\");\n\t\t\tif (!strcmp(catchvar->string, \"eval\"))\n\t\t\t\tjsC_error(J, catchvar, \"redefining 'eval' is not allowed in strict mode\");\n\t\t}\n\t\temitline(J, F, catchvar);\n\t\temitstring(J, F, OP_CATCH, catchvar->string);\n\t\tcstm(J, F, catchstm);\n\t\temit(J, F, OP_ENDCATCH);\n\t\tL3 = emitjump(J, F, OP_JUMP); /* skip past the try block to the finally block */\n\t}\n\tlabel(J, F, L1);\n\tcstm(J, F, trystm);\n\temit(J, F, OP_ENDTRY);\n\tlabel(J, F, L3);\n\tcstm(J, F, finallystm);\n}",
        "func": "static void ctrycatchfinally(JF, js_Ast *trystm, js_Ast *catchvar, js_Ast *catchstm, js_Ast *finallystm)\n{\n\tint L1, L2, L3;\n\tL1 = emitjump(J, F, OP_TRY);\n\t{\n\t\t/* if we get here, we have caught an exception in the try block */\n\t\tL2 = emitjump(J, F, OP_TRY);\n\t\t{\n\t\t\t/* if we get here, we have caught an exception in the catch block */\n\t\t\tcstm(J, F, finallystm); /* inline finally block */\n\t\t\temit(J, F, OP_THROW); /* rethrow exception */\n\t\t}\n\t\tlabel(J, F, L2);\n\t\tif (F->strict) {\n\t\t\tcheckfutureword(J, F, catchvar);\n\t\t\tif (!strcmp(catchvar->string, \"arguments\"))\n\t\t\t\tjsC_error(J, catchvar, \"redefining 'arguments' is not allowed in strict mode\");\n\t\t\tif (!strcmp(catchvar->string, \"eval\"))\n\t\t\t\tjsC_error(J, catchvar, \"redefining 'eval' is not allowed in strict mode\");\n\t\t}\n\t\temitline(J, F, catchvar);\n\t\temitstring(J, F, OP_CATCH, catchvar->string);\n\t\tcstm(J, F, catchstm);\n\t\temit(J, F, OP_ENDCATCH);\n\t\temit(J, F, OP_ENDTRY);\n\t\tL3 = emitjump(J, F, OP_JUMP); /* skip past the try block to the finally block */\n\t}\n\tlabel(J, F, L1);\n\tcstm(J, F, trystm);\n\temit(J, F, OP_ENDTRY);\n\tlabel(J, F, L3);\n\tcstm(J, F, finallystm);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,6 +22,7 @@\n \t\temitstring(J, F, OP_CATCH, catchvar->string);\n \t\tcstm(J, F, catchstm);\n \t\temit(J, F, OP_ENDCATCH);\n+\t\temit(J, F, OP_ENDTRY);\n \t\tL3 = emitjump(J, F, OP_JUMP); /* skip past the try block to the finally block */\n \t}\n \tlabel(J, F, L1);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\temit(J, F, OP_ENDTRY);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-21655",
        "func_name": "envoyproxy/envoy/Filter::convertRequestHeadersForInternalRedirect",
        "description": "Envoy is an open source edge and service proxy, designed for cloud-native applications. The envoy common router will segfault if an internal redirect selects a route configured with direct response or redirect actions. This will result in a denial of service. As a workaround turn off internal redirects if direct response entries are configured on the same listener.",
        "git_url": "https://github.com/envoyproxy/envoy/commit/177d608155ba8b11598b9bbf8240e90d8c350682",
        "commit_title": "CVE-2022-21655",
        "commit_text": " Crash with direct_response ",
        "func_before": "bool Filter::convertRequestHeadersForInternalRedirect(Http::RequestHeaderMap& downstream_headers,\n                                                      const Http::HeaderEntry& internal_redirect,\n                                                      uint64_t status_code) {\n  if (!downstream_headers.Path()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no path in downstream_headers\", *callbacks_);\n    return false;\n  }\n\n  absl::string_view redirect_url = internal_redirect.value().getStringView();\n  // Make sure the redirect response contains a URL to redirect to.\n  if (redirect_url.empty()) {\n    config_.stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: empty location\", *callbacks_);\n    return false;\n  }\n  Http::Utility::Url absolute_url;\n  if (!absolute_url.initialize(redirect_url, false)) {\n    config_.stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: invalid location {}\", *callbacks_,\n                     redirect_url);\n    return false;\n  }\n\n  const auto& policy = route_entry_->internalRedirectPolicy();\n  // Don't change the scheme from the original request\n  const bool scheme_is_http = schemeIsHttp(downstream_headers, *callbacks_->connection());\n  const bool target_is_http = absolute_url.scheme() == Http::Headers::get().SchemeValues.Http;\n  if (!policy.isCrossSchemeRedirectAllowed() && scheme_is_http != target_is_http) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: incorrect scheme for {}\", *callbacks_,\n                     redirect_url);\n    config_.stats_.passthrough_internal_redirect_unsafe_scheme_.inc();\n    return false;\n  }\n\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  // Make sure that performing the redirect won't result in exceeding the configured number of\n  // redirects allowed for this route.\n  StreamInfo::UInt32Accessor* num_internal_redirect{};\n\n  if (num_internal_redirect = filter_state->getDataMutable<StreamInfo::UInt32Accessor>(\n          NumInternalRedirectsFilterStateName);\n      num_internal_redirect == nullptr) {\n    auto state = std::make_shared<StreamInfo::UInt32AccessorImpl>(0);\n    num_internal_redirect = state.get();\n\n    filter_state->setData(NumInternalRedirectsFilterStateName, std::move(state),\n                          StreamInfo::FilterState::StateType::Mutable,\n                          StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (num_internal_redirect->value() >= policy.maxInternalRedirects()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: redirect limits exceeded.\", *callbacks_);\n    config_.stats_.passthrough_internal_redirect_too_many_redirects_.inc();\n    return false;\n  }\n  // Copy the old values, so they can be restored if the redirect fails.\n  const std::string original_host(downstream_headers.getHostValue());\n  const std::string original_path(downstream_headers.getPathValue());\n  const bool scheme_is_set = (downstream_headers.Scheme() != nullptr);\n  Cleanup restore_original_headers(\n      [&downstream_headers, original_host, original_path, scheme_is_set, scheme_is_http]() {\n        downstream_headers.setHost(original_host);\n        downstream_headers.setPath(original_path);\n        if (scheme_is_set) {\n          downstream_headers.setScheme(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                                      : Http::Headers::get().SchemeValues.Https);\n        }\n      });\n\n  // Replace the original host, scheme and path.\n  downstream_headers.setScheme(absolute_url.scheme());\n  downstream_headers.setHost(absolute_url.hostAndPort());\n\n  auto path_and_query = absolute_url.pathAndQueryParams();\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.http_reject_path_with_fragment\")) {\n    // Envoy treats internal redirect as a new request and will reject it if URI path\n    // contains #fragment. However the Location header is allowed to have #fragment in URI path. To\n    // prevent Envoy from rejecting internal redirect, strip the #fragment from Location URI if it\n    // is present.\n    auto fragment_pos = path_and_query.find('#');\n    path_and_query = path_and_query.substr(0, fragment_pos);\n  }\n  downstream_headers.setPath(path_and_query);\n\n  callbacks_->clearRouteCache();\n  const auto route = callbacks_->route();\n  // Don't allow a redirect to a non existing route.\n  if (!route) {\n    config_.stats_.passthrough_internal_redirect_no_route_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no route found\", *callbacks_);\n    return false;\n  }\n\n  const auto& route_name = route->routeEntry()->routeName();\n  for (const auto& predicate : policy.predicates()) {\n    if (!predicate->acceptTargetRoute(*filter_state, route_name, !scheme_is_http,\n                                      !target_is_http)) {\n      config_.stats_.passthrough_internal_redirect_predicate_.inc();\n      ENVOY_STREAM_LOG(trace,\n                       \"Internal redirect failed: rejecting redirect targeting {}, by {} predicate\",\n                       *callbacks_, route_name, predicate->name());\n      return false;\n    }\n  }\n\n  // See https://tools.ietf.org/html/rfc7231#section-6.4.4.\n  if (status_code == enumToInt(Http::Code::SeeOther) &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Get &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Head) {\n    downstream_headers.setMethod(Http::Headers::get().MethodValues.Get);\n    downstream_headers.remove(Http::Headers::get().ContentLength);\n    callbacks_->modifyDecodingBuffer([](Buffer::Instance& data) { data.drain(data.length()); });\n  }\n\n  num_internal_redirect->increment();\n  restore_original_headers.cancel();\n  // Preserve the original request URL for the second pass.\n  downstream_headers.setEnvoyOriginalUrl(absl::StrCat(scheme_is_http\n                                                          ? Http::Headers::get().SchemeValues.Http\n                                                          : Http::Headers::get().SchemeValues.Https,\n                                                      \"://\", original_host, original_path));\n  return true;\n}",
        "func": "bool Filter::convertRequestHeadersForInternalRedirect(Http::RequestHeaderMap& downstream_headers,\n                                                      const Http::HeaderEntry& internal_redirect,\n                                                      uint64_t status_code) {\n  if (!downstream_headers.Path()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no path in downstream_headers\", *callbacks_);\n    return false;\n  }\n\n  absl::string_view redirect_url = internal_redirect.value().getStringView();\n  // Make sure the redirect response contains a URL to redirect to.\n  if (redirect_url.empty()) {\n    config_.stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: empty location\", *callbacks_);\n    return false;\n  }\n  Http::Utility::Url absolute_url;\n  if (!absolute_url.initialize(redirect_url, false)) {\n    config_.stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: invalid location {}\", *callbacks_,\n                     redirect_url);\n    return false;\n  }\n\n  const auto& policy = route_entry_->internalRedirectPolicy();\n  // Don't change the scheme from the original request\n  const bool scheme_is_http = schemeIsHttp(downstream_headers, *callbacks_->connection());\n  const bool target_is_http = absolute_url.scheme() == Http::Headers::get().SchemeValues.Http;\n  if (!policy.isCrossSchemeRedirectAllowed() && scheme_is_http != target_is_http) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: incorrect scheme for {}\", *callbacks_,\n                     redirect_url);\n    config_.stats_.passthrough_internal_redirect_unsafe_scheme_.inc();\n    return false;\n  }\n\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  // Make sure that performing the redirect won't result in exceeding the configured number of\n  // redirects allowed for this route.\n  StreamInfo::UInt32Accessor* num_internal_redirect{};\n\n  if (num_internal_redirect = filter_state->getDataMutable<StreamInfo::UInt32Accessor>(\n          NumInternalRedirectsFilterStateName);\n      num_internal_redirect == nullptr) {\n    auto state = std::make_shared<StreamInfo::UInt32AccessorImpl>(0);\n    num_internal_redirect = state.get();\n\n    filter_state->setData(NumInternalRedirectsFilterStateName, std::move(state),\n                          StreamInfo::FilterState::StateType::Mutable,\n                          StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (num_internal_redirect->value() >= policy.maxInternalRedirects()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: redirect limits exceeded.\", *callbacks_);\n    config_.stats_.passthrough_internal_redirect_too_many_redirects_.inc();\n    return false;\n  }\n  // Copy the old values, so they can be restored if the redirect fails.\n  const std::string original_host(downstream_headers.getHostValue());\n  const std::string original_path(downstream_headers.getPathValue());\n  const bool scheme_is_set = (downstream_headers.Scheme() != nullptr);\n  Cleanup restore_original_headers(\n      [&downstream_headers, original_host, original_path, scheme_is_set, scheme_is_http]() {\n        downstream_headers.setHost(original_host);\n        downstream_headers.setPath(original_path);\n        if (scheme_is_set) {\n          downstream_headers.setScheme(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                                      : Http::Headers::get().SchemeValues.Https);\n        }\n      });\n\n  // Replace the original host, scheme and path.\n  downstream_headers.setScheme(absolute_url.scheme());\n  downstream_headers.setHost(absolute_url.hostAndPort());\n\n  auto path_and_query = absolute_url.pathAndQueryParams();\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.http_reject_path_with_fragment\")) {\n    // Envoy treats internal redirect as a new request and will reject it if URI path\n    // contains #fragment. However the Location header is allowed to have #fragment in URI path. To\n    // prevent Envoy from rejecting internal redirect, strip the #fragment from Location URI if it\n    // is present.\n    auto fragment_pos = path_and_query.find('#');\n    path_and_query = path_and_query.substr(0, fragment_pos);\n  }\n  downstream_headers.setPath(path_and_query);\n\n  callbacks_->clearRouteCache();\n  const auto route = callbacks_->route();\n  // Don't allow a redirect to a non existing route.\n  if (!route) {\n    config_.stats_.passthrough_internal_redirect_no_route_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no route found\", *callbacks_);\n    return false;\n  }\n\n  const auto& route_name = route->directResponseEntry() ? route->directResponseEntry()->routeName()\n                                                        : route->routeEntry()->routeName();\n  for (const auto& predicate : policy.predicates()) {\n    if (!predicate->acceptTargetRoute(*filter_state, route_name, !scheme_is_http,\n                                      !target_is_http)) {\n      config_.stats_.passthrough_internal_redirect_predicate_.inc();\n      ENVOY_STREAM_LOG(trace,\n                       \"Internal redirect failed: rejecting redirect targeting {}, by {} predicate\",\n                       *callbacks_, route_name, predicate->name());\n      return false;\n    }\n  }\n\n  // See https://tools.ietf.org/html/rfc7231#section-6.4.4.\n  if (status_code == enumToInt(Http::Code::SeeOther) &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Get &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Head) {\n    downstream_headers.setMethod(Http::Headers::get().MethodValues.Get);\n    downstream_headers.remove(Http::Headers::get().ContentLength);\n    callbacks_->modifyDecodingBuffer([](Buffer::Instance& data) { data.drain(data.length()); });\n  }\n\n  num_internal_redirect->increment();\n  restore_original_headers.cancel();\n  // Preserve the original request URL for the second pass.\n  downstream_headers.setEnvoyOriginalUrl(absl::StrCat(scheme_is_http\n                                                          ? Http::Headers::get().SchemeValues.Http\n                                                          : Http::Headers::get().SchemeValues.Https,\n                                                      \"://\", original_host, original_path));\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -91,7 +91,8 @@\n     return false;\n   }\n \n-  const auto& route_name = route->routeEntry()->routeName();\n+  const auto& route_name = route->directResponseEntry() ? route->directResponseEntry()->routeName()\n+                                                        : route->routeEntry()->routeName();\n   for (const auto& predicate : policy.predicates()) {\n     if (!predicate->acceptTargetRoute(*filter_state, route_name, !scheme_is_http,\n                                       !target_is_http)) {",
        "diff_line_info": {
            "deleted_lines": [
                "  const auto& route_name = route->routeEntry()->routeName();"
            ],
            "added_lines": [
                "  const auto& route_name = route->directResponseEntry() ? route->directResponseEntry()->routeName()",
                "                                                        : route->routeEntry()->routeName();"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19058",
        "func_name": "poppler/EmbFile::save2",
        "description": "An issue was discovered in Poppler 0.71.0. There is a reachable abort in Object.h, will lead to denial of service because EmbFile::save2 in FileSpec.cc lacks a stream check before saving an embedded file.",
        "git_url": "https://cgit.freedesktop.org/poppler/poppler/commit/?id=6912e06d9ab19ba28991b5cab3319d61d856bd6d",
        "commit_title": "Closes #659",
        "commit_text": "",
        "func_before": "bool EmbFile::save2(FILE *f) {\n  int c;\n\n  m_objStr.streamReset();\n  while ((c = m_objStr.streamGetChar()) != EOF) {\n    fputc(c, f);\n  }\n  return true;\n}",
        "func": "bool EmbFile::save2(FILE *f) {\n  int c;\n\n  if (unlikely(!m_objStr.isStream()))\n    return false;\n\n  m_objStr.streamReset();\n  while ((c = m_objStr.streamGetChar()) != EOF) {\n    fputc(c, f);\n  }\n  return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,8 @@\n bool EmbFile::save2(FILE *f) {\n   int c;\n+\n+  if (unlikely(!m_objStr.isStream()))\n+    return false;\n \n   m_objStr.streamReset();\n   while ((c = m_objStr.streamGetChar()) != EOF) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "  if (unlikely(!m_objStr.isStream()))",
                "    return false;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-41884",
        "func_name": "tensorflow/ArrayFromMemory",
        "description": "TensorFlow is an open source platform for machine learning. If a numpy array is created with a shape such that one element is zero and the others sum to a large number, an error will be raised. We have patched the issue in GitHub commit 2b56169c16e375c521a3bc8ea658811cc0793784. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/2b56169c16e375c521a3bc8ea658811cc0793784",
        "commit_title": "Fix segfault when using unusally shaped tensors.",
        "commit_text": " If a numpy array is created with a shape such that one element is zero and the others sum to a large number, an error will be raised. E.g. the following raises an error:  np.ones((0, 2**31, 2**31))  TF previously would not check the return value of PyArray_SimpleNewFromData, which returns null on such shapes. Now the return value is checked.  PiperOrigin-RevId: 477751929",
        "func_before": "Status ArrayFromMemory(int dim_size, npy_intp* dims, void* data, DataType dtype,\n                       std::function<void()> destructor, PyObject** result) {\n  if (dtype == DT_STRING || dtype == DT_RESOURCE) {\n    return errors::FailedPrecondition(\n        \"Cannot convert string or resource Tensors.\");\n  }\n\n  int type_num = -1;\n  Status s =\n      TF_DataType_to_PyArray_TYPE(static_cast<TF_DataType>(dtype), &type_num);\n  if (!s.ok()) {\n    return s;\n  }\n\n  if (dim_size > NPY_MAXDIMS) {\n    return errors::InvalidArgument(\n        \"Cannot convert tensor with \", dim_size,\n        \" dimensions to NumPy array. NumPy arrays can have at most \",\n        NPY_MAXDIMS, \" dimensions\");\n  }\n  auto* np_array = reinterpret_cast<PyArrayObject*>(\n      PyArray_SimpleNewFromData(dim_size, dims, type_num, data));\n  PyArray_CLEARFLAGS(np_array, NPY_ARRAY_OWNDATA);\n  if (PyType_Ready(&TensorReleaserType) == -1) {\n    return errors::Unknown(\"Python type initialization failed.\");\n  }\n  auto* releaser = reinterpret_cast<TensorReleaser*>(\n      TensorReleaserType.tp_alloc(&TensorReleaserType, 0));\n  releaser->destructor = new std::function<void()>(std::move(destructor));\n  if (PyArray_SetBaseObject(np_array, reinterpret_cast<PyObject*>(releaser)) ==\n      -1) {\n    Py_DECREF(releaser);\n    return errors::Unknown(\"Python array refused to use memory.\");\n  }\n  *result = reinterpret_cast<PyObject*>(np_array);\n  return OkStatus();\n}",
        "func": "Status ArrayFromMemory(int dim_size, npy_intp* dims, void* data, DataType dtype,\n                       std::function<void()> destructor, PyObject** result) {\n  if (dtype == DT_STRING || dtype == DT_RESOURCE) {\n    return errors::FailedPrecondition(\n        \"Cannot convert string or resource Tensors.\");\n  }\n\n  int type_num = -1;\n  Status s =\n      TF_DataType_to_PyArray_TYPE(static_cast<TF_DataType>(dtype), &type_num);\n  if (!s.ok()) {\n    return s;\n  }\n\n  if (dim_size > NPY_MAXDIMS) {\n    return errors::InvalidArgument(\n        \"Cannot convert tensor with \", dim_size,\n        \" dimensions to NumPy array. NumPy arrays can have at most \",\n        NPY_MAXDIMS, \" dimensions\");\n  }\n  auto* np_array = reinterpret_cast<PyArrayObject*>(\n      PyArray_SimpleNewFromData(dim_size, dims, type_num, data));\n  if (np_array == nullptr) {\n    string shape_str = absl::StrJoin(\n        absl::Span<npy_intp>{dims, static_cast<size_t>(dim_size)}, \", \");\n    if (PyErr_Occurred()) {\n      string exception_str = PyExceptionFetch();\n      PyErr_Clear();\n      return errors::InvalidArgument(\n          \"Failed to create numpy array from tensor of shape [\", shape_str,\n          \"]. Numpy error: \", exception_str);\n    }\n    return errors::Internal(\n        \"Failed to create numpy array from tensor of shape [\", shape_str, \"]\");\n  }\n\n  PyArray_CLEARFLAGS(np_array, NPY_ARRAY_OWNDATA);\n  if (PyType_Ready(&TensorReleaserType) == -1) {\n    return errors::Unknown(\"Python type initialization failed.\");\n  }\n  auto* releaser = reinterpret_cast<TensorReleaser*>(\n      TensorReleaserType.tp_alloc(&TensorReleaserType, 0));\n  releaser->destructor = new std::function<void()>(std::move(destructor));\n  if (PyArray_SetBaseObject(np_array, reinterpret_cast<PyObject*>(releaser)) ==\n      -1) {\n    Py_DECREF(releaser);\n    return errors::Unknown(\"Python array refused to use memory.\");\n  }\n  *result = reinterpret_cast<PyObject*>(np_array);\n  return OkStatus();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,6 +20,20 @@\n   }\n   auto* np_array = reinterpret_cast<PyArrayObject*>(\n       PyArray_SimpleNewFromData(dim_size, dims, type_num, data));\n+  if (np_array == nullptr) {\n+    string shape_str = absl::StrJoin(\n+        absl::Span<npy_intp>{dims, static_cast<size_t>(dim_size)}, \", \");\n+    if (PyErr_Occurred()) {\n+      string exception_str = PyExceptionFetch();\n+      PyErr_Clear();\n+      return errors::InvalidArgument(\n+          \"Failed to create numpy array from tensor of shape [\", shape_str,\n+          \"]. Numpy error: \", exception_str);\n+    }\n+    return errors::Internal(\n+        \"Failed to create numpy array from tensor of shape [\", shape_str, \"]\");\n+  }\n+\n   PyArray_CLEARFLAGS(np_array, NPY_ARRAY_OWNDATA);\n   if (PyType_Ready(&TensorReleaserType) == -1) {\n     return errors::Unknown(\"Python type initialization failed.\");",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (np_array == nullptr) {",
                "    string shape_str = absl::StrJoin(",
                "        absl::Span<npy_intp>{dims, static_cast<size_t>(dim_size)}, \", \");",
                "    if (PyErr_Occurred()) {",
                "      string exception_str = PyExceptionFetch();",
                "      PyErr_Clear();",
                "      return errors::InvalidArgument(",
                "          \"Failed to create numpy array from tensor of shape [\", shape_str,",
                "          \"]. Numpy error: \", exception_str);",
                "    }",
                "    return errors::Internal(",
                "        \"Failed to create numpy array from tensor of shape [\", shape_str, \"]\");",
                "  }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25598",
        "func_name": "xen-project/xen/acquire_resource",
        "description": "An issue was discovered in Xen 4.14.x. There is a missing unlock in the XENMEM_acquire_resource error path. The RCU (Read, Copy, Update) mechanism is a synchronisation primitive. A buggy error path in the XENMEM_acquire_resource exits without releasing an RCU reference, which is conceptually similar to forgetting to unlock a spinlock. A buggy or malicious HVM stubdomain can cause an RCU reference to be leaked. This causes subsequent administration operations, (e.g., CPU offline) to livelock, resulting in a host Denial of Service. The buggy codepath has been present since Xen 4.12. Xen 4.14 and later are vulnerable to the DoS. The side effects are believed to be benign on Xen 4.12 and 4.13, but patches are provided nevertheless. The vulnerability can generally only be exploited by x86 HVM VMs, as these are generally the only type of VM that have a Qemu stubdomain. x86 PV and PVH domains, as well as ARM VMs, typically don't use a stubdomain. Only VMs using HVM stubdomains can exploit the vulnerability. VMs using PV stubdomains, or with emulators running in dom0, cannot exploit the vulnerability.",
        "git_url": "https://github.com/xen-project/xen/commit/42317dede5be4fe5be27c873f7a3f94d3bba98e0",
        "commit_title": "xen/memory: Don't skip the RCU unlock path in acquire_resource()",
        "commit_text": " In the case that an HVM Stubdomain makes an XENMEM_acquire_resource hypercall, the FIXME path will bypass rcu_unlock_domain() on the way out of the function.  Move the check to the start of the function.  This does change the behaviour of the get-size path for HVM Stubdomains, but that functionality is currently broken and unused anyway, as well as being quite useless to entities which can't actually map the resource anyway.  This is XSA-334. ",
        "func_before": "static int acquire_resource(\n    XEN_GUEST_HANDLE_PARAM(xen_mem_acquire_resource_t) arg)\n{\n    struct domain *d, *currd = current->domain;\n    xen_mem_acquire_resource_t xmar;\n    /*\n     * The mfn_list and gfn_list (below) arrays are ok on stack for the\n     * moment since they are small, but if they need to grow in future\n     * use-cases then per-CPU arrays or heap allocations may be required.\n     */\n    xen_pfn_t mfn_list[32];\n    int rc;\n\n    if ( copy_from_guest(&xmar, arg, 1) )\n        return -EFAULT;\n\n    if ( xmar.pad != 0 )\n        return -EINVAL;\n\n    if ( guest_handle_is_null(xmar.frame_list) )\n    {\n        if ( xmar.nr_frames )\n            return -EINVAL;\n\n        xmar.nr_frames = ARRAY_SIZE(mfn_list);\n\n        if ( __copy_field_to_guest(arg, &xmar, nr_frames) )\n            return -EFAULT;\n\n        return 0;\n    }\n\n    if ( xmar.nr_frames > ARRAY_SIZE(mfn_list) )\n        return -E2BIG;\n\n    rc = rcu_lock_remote_domain_by_id(xmar.domid, &d);\n    if ( rc )\n        return rc;\n\n    rc = xsm_domain_resource_map(XSM_DM_PRIV, d);\n    if ( rc )\n        goto out;\n\n    switch ( xmar.type )\n    {\n    case XENMEM_resource_grant_table:\n        rc = acquire_grant_table(d, xmar.id, xmar.frame, xmar.nr_frames,\n                                 mfn_list);\n        break;\n\n    default:\n        rc = arch_acquire_resource(d, xmar.type, xmar.id, xmar.frame,\n                                   xmar.nr_frames, mfn_list);\n        break;\n    }\n\n    if ( rc )\n        goto out;\n\n    if ( !paging_mode_translate(currd) )\n    {\n        if ( copy_to_guest(xmar.frame_list, mfn_list, xmar.nr_frames) )\n            rc = -EFAULT;\n    }\n    else\n    {\n        xen_pfn_t gfn_list[ARRAY_SIZE(mfn_list)];\n        unsigned int i;\n\n        /*\n         * FIXME: Until foreign pages inserted into the P2M are properly\n         *        reference counted, it is unsafe to allow mapping of\n         *        resource pages unless the caller is the hardware domain.\n         */\n        if ( !is_hardware_domain(currd) )\n            return -EACCES;\n\n        if ( copy_from_guest(gfn_list, xmar.frame_list, xmar.nr_frames) )\n            rc = -EFAULT;\n\n        for ( i = 0; !rc && i < xmar.nr_frames; i++ )\n        {\n            rc = set_foreign_p2m_entry(currd, gfn_list[i],\n                                       _mfn(mfn_list[i]));\n            /* rc should be -EIO for any iteration other than the first */\n            if ( rc && i )\n                rc = -EIO;\n        }\n    }\n\n out:\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "func": "static int acquire_resource(\n    XEN_GUEST_HANDLE_PARAM(xen_mem_acquire_resource_t) arg)\n{\n    struct domain *d, *currd = current->domain;\n    xen_mem_acquire_resource_t xmar;\n    /*\n     * The mfn_list and gfn_list (below) arrays are ok on stack for the\n     * moment since they are small, but if they need to grow in future\n     * use-cases then per-CPU arrays or heap allocations may be required.\n     */\n    xen_pfn_t mfn_list[32];\n    int rc;\n\n    /*\n     * FIXME: Until foreign pages inserted into the P2M are properly\n     *        reference counted, it is unsafe to allow mapping of\n     *        resource pages unless the caller is the hardware domain.\n     */\n    if ( paging_mode_translate(currd) && !is_hardware_domain(currd) )\n        return -EACCES;\n\n    if ( copy_from_guest(&xmar, arg, 1) )\n        return -EFAULT;\n\n    if ( xmar.pad != 0 )\n        return -EINVAL;\n\n    if ( guest_handle_is_null(xmar.frame_list) )\n    {\n        if ( xmar.nr_frames )\n            return -EINVAL;\n\n        xmar.nr_frames = ARRAY_SIZE(mfn_list);\n\n        if ( __copy_field_to_guest(arg, &xmar, nr_frames) )\n            return -EFAULT;\n\n        return 0;\n    }\n\n    if ( xmar.nr_frames > ARRAY_SIZE(mfn_list) )\n        return -E2BIG;\n\n    rc = rcu_lock_remote_domain_by_id(xmar.domid, &d);\n    if ( rc )\n        return rc;\n\n    rc = xsm_domain_resource_map(XSM_DM_PRIV, d);\n    if ( rc )\n        goto out;\n\n    switch ( xmar.type )\n    {\n    case XENMEM_resource_grant_table:\n        rc = acquire_grant_table(d, xmar.id, xmar.frame, xmar.nr_frames,\n                                 mfn_list);\n        break;\n\n    default:\n        rc = arch_acquire_resource(d, xmar.type, xmar.id, xmar.frame,\n                                   xmar.nr_frames, mfn_list);\n        break;\n    }\n\n    if ( rc )\n        goto out;\n\n    if ( !paging_mode_translate(currd) )\n    {\n        if ( copy_to_guest(xmar.frame_list, mfn_list, xmar.nr_frames) )\n            rc = -EFAULT;\n    }\n    else\n    {\n        xen_pfn_t gfn_list[ARRAY_SIZE(mfn_list)];\n        unsigned int i;\n\n        if ( copy_from_guest(gfn_list, xmar.frame_list, xmar.nr_frames) )\n            rc = -EFAULT;\n\n        for ( i = 0; !rc && i < xmar.nr_frames; i++ )\n        {\n            rc = set_foreign_p2m_entry(currd, gfn_list[i],\n                                       _mfn(mfn_list[i]));\n            /* rc should be -EIO for any iteration other than the first */\n            if ( rc && i )\n                rc = -EIO;\n        }\n    }\n\n out:\n    rcu_unlock_domain(d);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,14 @@\n      */\n     xen_pfn_t mfn_list[32];\n     int rc;\n+\n+    /*\n+     * FIXME: Until foreign pages inserted into the P2M are properly\n+     *        reference counted, it is unsafe to allow mapping of\n+     *        resource pages unless the caller is the hardware domain.\n+     */\n+    if ( paging_mode_translate(currd) && !is_hardware_domain(currd) )\n+        return -EACCES;\n \n     if ( copy_from_guest(&xmar, arg, 1) )\n         return -EFAULT;\n@@ -67,14 +75,6 @@\n         xen_pfn_t gfn_list[ARRAY_SIZE(mfn_list)];\n         unsigned int i;\n \n-        /*\n-         * FIXME: Until foreign pages inserted into the P2M are properly\n-         *        reference counted, it is unsafe to allow mapping of\n-         *        resource pages unless the caller is the hardware domain.\n-         */\n-        if ( !is_hardware_domain(currd) )\n-            return -EACCES;\n-\n         if ( copy_from_guest(gfn_list, xmar.frame_list, xmar.nr_frames) )\n             rc = -EFAULT;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "        /*",
                "         * FIXME: Until foreign pages inserted into the P2M are properly",
                "         *        reference counted, it is unsafe to allow mapping of",
                "         *        resource pages unless the caller is the hardware domain.",
                "         */",
                "        if ( !is_hardware_domain(currd) )",
                "            return -EACCES;",
                ""
            ],
            "added_lines": [
                "",
                "    /*",
                "     * FIXME: Until foreign pages inserted into the P2M are properly",
                "     *        reference counted, it is unsafe to allow mapping of",
                "     *        resource pages unless the caller is the hardware domain.",
                "     */",
                "    if ( paging_mode_translate(currd) && !is_hardware_domain(currd) )",
                "        return -EACCES;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25603",
        "func_name": "xen-project/xen/port_is_valid",
        "description": "An issue was discovered in Xen through 4.14.x. There are missing memory barriers when accessing/allocating an event channel. Event channels control structures can be accessed lockless as long as the port is considered to be valid. Such a sequence is missing an appropriate memory barrier (e.g., smp_*mb()) to prevent both the compiler and CPU from re-ordering access. A malicious guest may be able to cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded. Systems running all versions of Xen are affected. Whether a system is vulnerable will depend on the CPU and compiler used to build Xen. For all systems, the presence and the scope of the vulnerability depend on the precise re-ordering performed by the compiler used to build Xen. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code generation options). GCC documentation clearly suggests that re-ordering is possible. Arm systems will also be vulnerable if the CPU is able to re-order memory access. Please consult your CPU vendor. x86 systems are only vulnerable if a compiler performs re-ordering.",
        "git_url": "https://github.com/xen-project/xen/commit/112992b05b2d2ca63f3c78eefe1cf8d192d7303a",
        "commit_title": "xen/evtchn: Add missing barriers when accessing/allocating an event channel",
        "commit_text": " While the allocation of a bucket is always performed with the per-domain lock, the bucket may be accessed without the lock taken (for instance, see evtchn_send()).  Instead such sites relies on port_is_valid() to return a non-zero value when the port has a struct evtchn associated to it. The function will mostly check whether the port is less than d->valid_evtchns as all the buckets/event channels should be allocated up to that point.  Unfortunately a compiler is free to re-order the assignment in evtchn_allocate_port() so it would be possible to have d->valid_evtchns updated before the new bucket has finish to allocate.  Additionally on Arm, even if this was compiled \"correctly\", the processor can still re-order the memory access.  Add a write memory barrier in the allocation side and a read memory barrier when the port is valid to prevent any re-ordering issue.  This is XSA-340. ",
        "func_before": "static inline bool_t port_is_valid(struct domain *d, unsigned int p)\n{\n    return p < read_atomic(&d->valid_evtchns);\n}",
        "func": "static inline bool_t port_is_valid(struct domain *d, unsigned int p)\n{\n    if ( p >= read_atomic(&d->valid_evtchns) )\n        return false;\n\n    /*\n     * The caller will usually access the event channel afterwards and\n     * may be done without taking the per-domain lock. The barrier is\n     * going in pair the smp_wmb() barrier in evtchn_allocate_port().\n     */\n    smp_rmb();\n\n    return true;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,14 @@\n static inline bool_t port_is_valid(struct domain *d, unsigned int p)\n {\n-    return p < read_atomic(&d->valid_evtchns);\n+    if ( p >= read_atomic(&d->valid_evtchns) )\n+        return false;\n+\n+    /*\n+     * The caller will usually access the event channel afterwards and\n+     * may be done without taking the per-domain lock. The barrier is\n+     * going in pair the smp_wmb() barrier in evtchn_allocate_port().\n+     */\n+    smp_rmb();\n+\n+    return true;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    return p < read_atomic(&d->valid_evtchns);"
            ],
            "added_lines": [
                "    if ( p >= read_atomic(&d->valid_evtchns) )",
                "        return false;",
                "",
                "    /*",
                "     * The caller will usually access the event channel afterwards and",
                "     * may be done without taking the per-domain lock. The barrier is",
                "     * going in pair the smp_wmb() barrier in evtchn_allocate_port().",
                "     */",
                "    smp_rmb();",
                "",
                "    return true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25603",
        "func_name": "xen-project/xen/evtchn_allocate_port",
        "description": "An issue was discovered in Xen through 4.14.x. There are missing memory barriers when accessing/allocating an event channel. Event channels control structures can be accessed lockless as long as the port is considered to be valid. Such a sequence is missing an appropriate memory barrier (e.g., smp_*mb()) to prevent both the compiler and CPU from re-ordering access. A malicious guest may be able to cause a hypervisor crash resulting in a Denial of Service (DoS). Information leak and privilege escalation cannot be excluded. Systems running all versions of Xen are affected. Whether a system is vulnerable will depend on the CPU and compiler used to build Xen. For all systems, the presence and the scope of the vulnerability depend on the precise re-ordering performed by the compiler used to build Xen. We have not been able to survey compilers; consequently we cannot say which compiler(s) might produce vulnerable code (with which code generation options). GCC documentation clearly suggests that re-ordering is possible. Arm systems will also be vulnerable if the CPU is able to re-order memory access. Please consult your CPU vendor. x86 systems are only vulnerable if a compiler performs re-ordering.",
        "git_url": "https://github.com/xen-project/xen/commit/112992b05b2d2ca63f3c78eefe1cf8d192d7303a",
        "commit_title": "xen/evtchn: Add missing barriers when accessing/allocating an event channel",
        "commit_text": " While the allocation of a bucket is always performed with the per-domain lock, the bucket may be accessed without the lock taken (for instance, see evtchn_send()).  Instead such sites relies on port_is_valid() to return a non-zero value when the port has a struct evtchn associated to it. The function will mostly check whether the port is less than d->valid_evtchns as all the buckets/event channels should be allocated up to that point.  Unfortunately a compiler is free to re-order the assignment in evtchn_allocate_port() so it would be possible to have d->valid_evtchns updated before the new bucket has finish to allocate.  Additionally on Arm, even if this was compiled \"correctly\", the processor can still re-order the memory access.  Add a write memory barrier in the allocation side and a read memory barrier when the port is valid to prevent any re-ordering issue.  This is XSA-340. ",
        "func_before": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= d->max_evtchns )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n             evtchn_port_is_busy(d, port) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    return 0;\n}",
        "func": "int evtchn_allocate_port(struct domain *d, evtchn_port_t port)\n{\n    if ( port > d->max_evtchn_port || port >= d->max_evtchns )\n        return -ENOSPC;\n\n    if ( port_is_valid(d, port) )\n    {\n        if ( evtchn_from_port(d, port)->state != ECS_FREE ||\n             evtchn_port_is_busy(d, port) )\n            return -EBUSY;\n    }\n    else\n    {\n        struct evtchn *chn;\n        struct evtchn **grp;\n\n        if ( !group_from_port(d, port) )\n        {\n            grp = xzalloc_array(struct evtchn *, BUCKETS_PER_GROUP);\n            if ( !grp )\n                return -ENOMEM;\n            group_from_port(d, port) = grp;\n        }\n\n        chn = alloc_evtchn_bucket(d, port);\n        if ( !chn )\n            return -ENOMEM;\n        bucket_from_port(d, port) = chn;\n\n        /*\n         * d->valid_evtchns is used to check whether the bucket can be\n         * accessed without the per-domain lock. Therefore,\n         * d->valid_evtchns should be seen *after* the new bucket has\n         * been setup.\n         */\n        smp_wmb();\n        write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n    }\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,13 @@\n             return -ENOMEM;\n         bucket_from_port(d, port) = chn;\n \n+        /*\n+         * d->valid_evtchns is used to check whether the bucket can be\n+         * accessed without the per-domain lock. Therefore,\n+         * d->valid_evtchns should be seen *after* the new bucket has\n+         * been setup.\n+         */\n+        smp_wmb();\n         write_atomic(&d->valid_evtchns, d->valid_evtchns + EVTCHNS_PER_BUCKET);\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        /*",
                "         * d->valid_evtchns is used to check whether the bucket can be",
                "         * accessed without the per-domain lock. Therefore,",
                "         * d->valid_evtchns should be seen *after* the new bucket has",
                "         * been setup.",
                "         */",
                "        smp_wmb();"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-36277",
        "func_name": "DanBloomberg/leptonica/selReadStream",
        "description": "Leptonica before 1.80.0 allows a denial of service (application crash) via an incorrect left shift in pixConvert2To8 in pixconv.c.",
        "git_url": "https://github.com/DanBloomberg/leptonica/commit/3ddd9a35cebc28f2b1a5693c27dcfff58218674a",
        "commit_title": "Fix heap buffer overflow in selReadStream",
        "commit_text": " selio_reg triggers a heap buffer overflow when sscanf tries to write 201 bytes into a 24 byte string. It can be detected when the code is compiled with the address sanitizer:      ==19856==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x603000001288 at pc 0x00000044462b bp 0x7fffffffddf0 sp 0x7fffffffd5a0     WRITE of size 201 at 0x603000001288 thread T0     0x603000001288 is located 0 bytes to the right of 24-byte region [0x603000001270,0x603000001288) ",
        "func_before": "SEL  *\nselReadStream(FILE  *fp)\n{\nchar    *selname;\nchar     linebuf[256];\nl_int32  sy, sx, cy, cx, i, j, version, ignore;\nSEL     *sel;\n\n    PROCNAME(\"selReadStream\");\n\n    if (!fp)\n        return (SEL *)ERROR_PTR(\"stream not defined\", procName, NULL);\n\n    if (fscanf(fp, \"  Sel Version %d\\n\", &version) != 1)\n        return (SEL *)ERROR_PTR(\"not a sel file\", procName, NULL);\n    if (version != SEL_VERSION_NUMBER)\n        return (SEL *)ERROR_PTR(\"invalid sel version\", procName, NULL);\n\n    if (fgets(linebuf, sizeof(linebuf), fp) == NULL)\n        return (SEL *)ERROR_PTR(\"error reading into linebuf\", procName, NULL);\n    selname = stringNew(linebuf);\n    sscanf(linebuf, \"  ------  %200s  ------\", selname);\n\n    if (fscanf(fp, \"  sy = %d, sx = %d, cy = %d, cx = %d\\n\",\n            &sy, &sx, &cy, &cx) != 4) {\n        LEPT_FREE(selname);\n        return (SEL *)ERROR_PTR(\"dimensions not read\", procName, NULL);\n    }\n\n    if ((sel = selCreate(sy, sx, selname)) == NULL) {\n        LEPT_FREE(selname);\n        return (SEL *)ERROR_PTR(\"sel not made\", procName, NULL);\n    }\n    selSetOrigin(sel, cy, cx);\n\n    for (i = 0; i < sy; i++) {\n        ignore = fscanf(fp, \"    \");\n        for (j = 0; j < sx; j++)\n            ignore = fscanf(fp, \"%1d\", &sel->data[i][j]);\n        ignore = fscanf(fp, \"\\n\");\n    }\n    ignore = fscanf(fp, \"\\n\");\n\n    LEPT_FREE(selname);\n    return sel;\n}",
        "func": "SEL  *\nselReadStream(FILE  *fp)\n{\nchar     selname[256];\nchar     linebuf[256];\nl_int32  sy, sx, cy, cx, i, j, version, ignore;\nSEL     *sel;\n\n    PROCNAME(\"selReadStream\");\n\n    if (!fp)\n        return (SEL *)ERROR_PTR(\"stream not defined\", procName, NULL);\n\n    if (fscanf(fp, \"  Sel Version %d\\n\", &version) != 1)\n        return (SEL *)ERROR_PTR(\"not a sel file\", procName, NULL);\n    if (version != SEL_VERSION_NUMBER)\n        return (SEL *)ERROR_PTR(\"invalid sel version\", procName, NULL);\n\n    if (fgets(linebuf, sizeof(linebuf), fp) == NULL)\n        return (SEL *)ERROR_PTR(\"error reading into linebuf\", procName, NULL);\n    sscanf(linebuf, \"  ------  %200s  ------\", selname);\n\n    if (fscanf(fp, \"  sy = %d, sx = %d, cy = %d, cx = %d\\n\",\n            &sy, &sx, &cy, &cx) != 4) {\n        return (SEL *)ERROR_PTR(\"dimensions not read\", procName, NULL);\n    }\n\n    if ((sel = selCreate(sy, sx, selname)) == NULL) {\n        return (SEL *)ERROR_PTR(\"sel not made\", procName, NULL);\n    }\n    selSetOrigin(sel, cy, cx);\n\n    for (i = 0; i < sy; i++) {\n        ignore = fscanf(fp, \"    \");\n        for (j = 0; j < sx; j++)\n            ignore = fscanf(fp, \"%1d\", &sel->data[i][j]);\n        ignore = fscanf(fp, \"\\n\");\n    }\n    ignore = fscanf(fp, \"\\n\");\n\n    return sel;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,7 @@\n SEL  *\n selReadStream(FILE  *fp)\n {\n-char    *selname;\n+char     selname[256];\n char     linebuf[256];\n l_int32  sy, sx, cy, cx, i, j, version, ignore;\n SEL     *sel;\n@@ -18,17 +18,14 @@\n \n     if (fgets(linebuf, sizeof(linebuf), fp) == NULL)\n         return (SEL *)ERROR_PTR(\"error reading into linebuf\", procName, NULL);\n-    selname = stringNew(linebuf);\n     sscanf(linebuf, \"  ------  %200s  ------\", selname);\n \n     if (fscanf(fp, \"  sy = %d, sx = %d, cy = %d, cx = %d\\n\",\n             &sy, &sx, &cy, &cx) != 4) {\n-        LEPT_FREE(selname);\n         return (SEL *)ERROR_PTR(\"dimensions not read\", procName, NULL);\n     }\n \n     if ((sel = selCreate(sy, sx, selname)) == NULL) {\n-        LEPT_FREE(selname);\n         return (SEL *)ERROR_PTR(\"sel not made\", procName, NULL);\n     }\n     selSetOrigin(sel, cy, cx);\n@@ -41,6 +38,5 @@\n     }\n     ignore = fscanf(fp, \"\\n\");\n \n-    LEPT_FREE(selname);\n     return sel;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "char    *selname;",
                "    selname = stringNew(linebuf);",
                "        LEPT_FREE(selname);",
                "        LEPT_FREE(selname);",
                "    LEPT_FREE(selname);"
            ],
            "added_lines": [
                "char     selname[256];"
            ]
        }
    }
]