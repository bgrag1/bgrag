[
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_dep_link_up",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_dep_link_up(struct nfc_dev *dev, int target_index, u8 comm_mode)\n{\n\tint rc = 0;\n\tu8 *gb;\n\tsize_t gb_len;\n\tstruct nfc_target *target;\n\n\tpr_debug(\"dev_name=%s comm %d\\n\", dev_name(&dev->dev), comm_mode);\n\n\tif (!dev->ops->dep_link_up)\n\t\treturn -EOPNOTSUPP;\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dep_link_up == true) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tgb = nfc_llcp_general_bytes(dev, &gb_len);\n\tif (gb_len > NFC_MAX_GT_LEN) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\ttarget = nfc_find_target(dev, target_index);\n\tif (target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->dep_link_up(dev, target, comm_mode, gb, gb_len);\n\tif (!rc) {\n\t\tdev->active_target = target;\n\t\tdev->rf_mode = NFC_RF_INITIATOR;\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_dep_link_up(struct nfc_dev *dev, int target_index, u8 comm_mode)\n{\n\tint rc = 0;\n\tu8 *gb;\n\tsize_t gb_len;\n\tstruct nfc_target *target;\n\n\tpr_debug(\"dev_name=%s comm %d\\n\", dev_name(&dev->dev), comm_mode);\n\n\tif (!dev->ops->dep_link_up)\n\t\treturn -EOPNOTSUPP;\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dep_link_up == true) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tgb = nfc_llcp_general_bytes(dev, &gb_len);\n\tif (gb_len > NFC_MAX_GT_LEN) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\ttarget = nfc_find_target(dev, target_index);\n\tif (target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->dep_link_up(dev, target, comm_mode, gb, gb_len);\n\tif (!rc) {\n\t\tdev->active_target = target;\n\t\tdev->rf_mode = NFC_RF_INITIATOR;\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_deactivate_target",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_deactivate_target(struct nfc_dev *dev, u32 target_idx, u8 mode)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s target_idx=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target->idx != target_idx) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->check_presence)\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\n\tdev->ops->deactivate_target(dev, dev->active_target, mode);\n\tdev->active_target = NULL;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_deactivate_target(struct nfc_dev *dev, u32 target_idx, u8 mode)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s target_idx=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target->idx != target_idx) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->check_presence)\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\n\tdev->ops->deactivate_target(dev, dev->active_target, mode);\n\tdev->active_target = NULL;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_disable_se",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_disable_se(struct nfc_dev *dev, u32 se_idx)\n{\n\tstruct nfc_se *se;\n\tint rc;\n\n\tpr_debug(\"%s se index %d\\n\", dev_name(&dev->dev), se_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->enable_se || !dev->ops->disable_se) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tse = nfc_find_se(dev, se_idx);\n\tif (!se) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (se->state == NFC_SE_DISABLED) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->disable_se(dev, se_idx);\n\tif (rc >= 0)\n\t\tse->state = NFC_SE_DISABLED;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_disable_se(struct nfc_dev *dev, u32 se_idx)\n{\n\tstruct nfc_se *se;\n\tint rc;\n\n\tpr_debug(\"%s se index %d\\n\", dev_name(&dev->dev), se_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->enable_se || !dev->ops->disable_se) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tse = nfc_find_se(dev, se_idx);\n\tif (!se) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (se->state == NFC_SE_DISABLED) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->disable_se(dev, se_idx);\n\tif (rc >= 0)\n\t\tse->state = NFC_SE_DISABLED;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_register_device",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_register_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\trc = device_add(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = nfc_llcp_register_device(dev);\n\tif (rc)\n\t\tpr_err(\"Could not register llcp device\\n\");\n\n\tdevice_lock(&dev->dev);\n\tdev->rfkill = rfkill_alloc(dev_name(&dev->dev), &dev->dev,\n\t\t\t\t   RFKILL_TYPE_NFC, &nfc_rfkill_ops, dev);\n\tif (dev->rfkill) {\n\t\tif (rfkill_register(dev->rfkill) < 0) {\n\t\t\trfkill_destroy(dev->rfkill);\n\t\t\tdev->rfkill = NULL;\n\t\t}\n\t}\n\tdevice_unlock(&dev->dev);\n\n\trc = nfc_genl_device_added(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n\t\t\t dev_name(&dev->dev));\n\n\treturn 0;\n}",
        "func": "int nfc_register_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\trc = device_add(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = nfc_llcp_register_device(dev);\n\tif (rc)\n\t\tpr_err(\"Could not register llcp device\\n\");\n\n\tdevice_lock(&dev->dev);\n\tdev->rfkill = rfkill_alloc(dev_name(&dev->dev), &dev->dev,\n\t\t\t\t   RFKILL_TYPE_NFC, &nfc_rfkill_ops, dev);\n\tif (dev->rfkill) {\n\t\tif (rfkill_register(dev->rfkill) < 0) {\n\t\t\trfkill_destroy(dev->rfkill);\n\t\t\tdev->rfkill = NULL;\n\t\t}\n\t}\n\tdev->shutting_down = false;\n\tdevice_unlock(&dev->dev);\n\n\trc = nfc_genl_device_added(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s was added\\n\",\n\t\t\t dev_name(&dev->dev));\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,7 @@\n \t\t\tdev->rfkill = NULL;\n \t\t}\n \t}\n+\tdev->shutting_down = false;\n \tdevice_unlock(&dev->dev);\n \n \trc = nfc_genl_device_added(dev);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tdev->shutting_down = false;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_data_exchange",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_data_exchange(struct nfc_dev *dev, u32 target_idx, struct sk_buff *skb,\n\t\t      data_exchange_cb_t cb, void *cb_context)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s target_idx=%u skb->len=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx, skb->len);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (dev->rf_mode == NFC_RF_INITIATOR && dev->active_target != NULL) {\n\t\tif (dev->active_target->idx != target_idx) {\n\t\t\trc = -EADDRNOTAVAIL;\n\t\t\tkfree_skb(skb);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (dev->ops->check_presence)\n\t\t\tdel_timer_sync(&dev->check_pres_timer);\n\n\t\trc = dev->ops->im_transceive(dev, dev->active_target, skb, cb,\n\t\t\t\t\t     cb_context);\n\n\t\tif (!rc && dev->ops->check_presence && !dev->shutting_down)\n\t\t\tmod_timer(&dev->check_pres_timer, jiffies +\n\t\t\t\t  msecs_to_jiffies(NFC_CHECK_PRES_FREQ_MS));\n\t} else if (dev->rf_mode == NFC_RF_TARGET && dev->ops->tm_send != NULL) {\n\t\trc = dev->ops->tm_send(dev, skb);\n\t} else {\n\t\trc = -ENOTCONN;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_data_exchange(struct nfc_dev *dev, u32 target_idx, struct sk_buff *skb,\n\t\t      data_exchange_cb_t cb, void *cb_context)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s target_idx=%u skb->len=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx, skb->len);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\tif (dev->rf_mode == NFC_RF_INITIATOR && dev->active_target != NULL) {\n\t\tif (dev->active_target->idx != target_idx) {\n\t\t\trc = -EADDRNOTAVAIL;\n\t\t\tkfree_skb(skb);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (dev->ops->check_presence)\n\t\t\tdel_timer_sync(&dev->check_pres_timer);\n\n\t\trc = dev->ops->im_transceive(dev, dev->active_target, skb, cb,\n\t\t\t\t\t     cb_context);\n\n\t\tif (!rc && dev->ops->check_presence && !dev->shutting_down)\n\t\t\tmod_timer(&dev->check_pres_timer, jiffies +\n\t\t\t\t  msecs_to_jiffies(NFC_CHECK_PRES_FREQ_MS));\n\t} else if (dev->rf_mode == NFC_RF_TARGET && dev->ops->tm_send != NULL) {\n\t\trc = dev->ops->tm_send(dev, skb);\n\t} else {\n\t\trc = -ENOTCONN;\n\t\tkfree_skb(skb);\n\t\tgoto error;\n\t}\n\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tkfree_skb(skb);\n \t\tgoto error;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_enable_se",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_enable_se(struct nfc_dev *dev, u32 se_idx)\n{\n\tstruct nfc_se *se;\n\tint rc;\n\n\tpr_debug(\"%s se index %d\\n\", dev_name(&dev->dev), se_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->enable_se || !dev->ops->disable_se) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tse = nfc_find_se(dev, se_idx);\n\tif (!se) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (se->state == NFC_SE_ENABLED) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->enable_se(dev, se_idx);\n\tif (rc >= 0)\n\t\tse->state = NFC_SE_ENABLED;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_enable_se(struct nfc_dev *dev, u32 se_idx)\n{\n\tstruct nfc_se *se;\n\tint rc;\n\n\tpr_debug(\"%s se index %d\\n\", dev_name(&dev->dev), se_idx);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->enable_se || !dev->ops->disable_se) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tse = nfc_find_se(dev, se_idx);\n\tif (!se) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tif (se->state == NFC_SE_ENABLED) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->enable_se(dev, se_idx);\n\tif (rc >= 0)\n\t\tse->state = NFC_SE_ENABLED;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_activate_target",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_activate_target(struct nfc_dev *dev, u32 target_idx, u32 protocol)\n{\n\tint rc;\n\tstruct nfc_target *target;\n\n\tpr_debug(\"dev_name=%s target_idx=%u protocol=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx, protocol);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\ttarget = nfc_find_target(dev, target_idx);\n\tif (target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->activate_target(dev, target, protocol);\n\tif (!rc) {\n\t\tdev->active_target = target;\n\t\tdev->rf_mode = NFC_RF_INITIATOR;\n\n\t\tif (dev->ops->check_presence && !dev->shutting_down)\n\t\t\tmod_timer(&dev->check_pres_timer, jiffies +\n\t\t\t\t  msecs_to_jiffies(NFC_CHECK_PRES_FREQ_MS));\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_activate_target(struct nfc_dev *dev, u32 target_idx, u32 protocol)\n{\n\tint rc;\n\tstruct nfc_target *target;\n\n\tpr_debug(\"dev_name=%s target_idx=%u protocol=%u\\n\",\n\t\t dev_name(&dev->dev), target_idx, protocol);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->active_target) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\ttarget = nfc_find_target(dev, target_idx);\n\tif (target == NULL) {\n\t\trc = -ENOTCONN;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->activate_target(dev, target, protocol);\n\tif (!rc) {\n\t\tdev->active_target = target;\n\t\tdev->rf_mode = NFC_RF_INITIATOR;\n\n\t\tif (dev->ops->check_presence && !dev->shutting_down)\n\t\t\tmod_timer(&dev->check_pres_timer, jiffies +\n\t\t\t\t  msecs_to_jiffies(NFC_CHECK_PRES_FREQ_MS));\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_fw_download",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_fw_download(struct nfc_dev *dev, const char *firmware_name)\n{\n\tint rc = 0;\n\n\tpr_debug(\"%s do firmware %s\\n\", dev_name(&dev->dev), firmware_name);\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->fw_download) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tdev->fw_download_in_progress = true;\n\trc = dev->ops->fw_download(dev, firmware_name);\n\tif (rc)\n\t\tdev->fw_download_in_progress = false;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_fw_download(struct nfc_dev *dev, const char *firmware_name)\n{\n\tint rc = 0;\n\n\tpr_debug(\"%s do firmware %s\\n\", dev_name(&dev->dev), firmware_name);\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (!dev->ops->fw_download) {\n\t\trc = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tdev->fw_download_in_progress = true;\n\trc = dev->ops->fw_download(dev, firmware_name);\n\tif (rc)\n\t\tdev->fw_download_in_progress = false;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_unregister_device",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "void nfc_unregister_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\trc = nfc_genl_device_removed(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\tif (dev->rfkill) {\n\t\trfkill_unregister(dev->rfkill);\n\t\trfkill_destroy(dev->rfkill);\n\t}\n\tdevice_unlock(&dev->dev);\n\n\tif (dev->ops->check_presence) {\n\t\tdevice_lock(&dev->dev);\n\t\tdev->shutting_down = true;\n\t\tdevice_unlock(&dev->dev);\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\t\tcancel_work_sync(&dev->check_pres_work);\n\t}\n\n\tnfc_llcp_unregister_device(dev);\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\tdevice_del(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n}",
        "func": "void nfc_unregister_device(struct nfc_dev *dev)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\trc = nfc_genl_device_removed(dev);\n\tif (rc)\n\t\tpr_debug(\"The userspace won't be notified that the device %s \"\n\t\t\t \"was removed\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\tif (dev->rfkill) {\n\t\trfkill_unregister(dev->rfkill);\n\t\trfkill_destroy(dev->rfkill);\n\t}\n\tdev->shutting_down = true;\n\tdevice_unlock(&dev->dev);\n\n\tif (dev->ops->check_presence) {\n\t\tdel_timer_sync(&dev->check_pres_timer);\n\t\tcancel_work_sync(&dev->check_pres_work);\n\t}\n\n\tnfc_llcp_unregister_device(dev);\n\n\tmutex_lock(&nfc_devlist_mutex);\n\tnfc_devlist_generation++;\n\tdevice_del(&dev->dev);\n\tmutex_unlock(&nfc_devlist_mutex);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,12 +14,10 @@\n \t\trfkill_unregister(dev->rfkill);\n \t\trfkill_destroy(dev->rfkill);\n \t}\n+\tdev->shutting_down = true;\n \tdevice_unlock(&dev->dev);\n \n \tif (dev->ops->check_presence) {\n-\t\tdevice_lock(&dev->dev);\n-\t\tdev->shutting_down = true;\n-\t\tdevice_unlock(&dev->dev);\n \t\tdel_timer_sync(&dev->check_pres_timer);\n \t\tcancel_work_sync(&dev->check_pres_work);\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tdevice_lock(&dev->dev);",
                "\t\tdev->shutting_down = true;",
                "\t\tdevice_unlock(&dev->dev);"
            ],
            "added_lines": [
                "\tdev->shutting_down = true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_dep_link_down",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_dep_link_down(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tif (!dev->ops->dep_link_down)\n\t\treturn -EOPNOTSUPP;\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dep_link_up == false) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->dep_link_down(dev);\n\tif (!rc) {\n\t\tdev->dep_link_up = false;\n\t\tdev->active_target = NULL;\n\t\tdev->rf_mode = NFC_RF_NONE;\n\t\tnfc_llcp_mac_is_down(dev);\n\t\tnfc_genl_dep_link_down_event(dev);\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\n\treturn rc;\n}",
        "func": "int nfc_dep_link_down(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tif (!dev->ops->dep_link_down)\n\t\treturn -EOPNOTSUPP;\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->dep_link_up == false) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->dep_link_down(dev);\n\tif (!rc) {\n\t\tdev->dep_link_up = false;\n\t\tdev->active_target = NULL;\n\t\tdev->rf_mode = NFC_RF_NONE;\n\t\tnfc_llcp_mac_is_down(dev);\n\t\tnfc_genl_dep_link_down_event(dev);\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_stop_poll",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_stop_poll(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->polling) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tdev->ops->stop_poll(dev);\n\tdev->polling = false;\n\tdev->rf_mode = NFC_RF_NONE;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_stop_poll(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->polling) {\n\t\trc = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tdev->ops->stop_poll(dev);\n\tdev->polling = false;\n\tdev->rf_mode = NFC_RF_NONE;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_start_poll",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_start_poll(struct nfc_dev *dev, u32 im_protocols, u32 tm_protocols)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name %s initiator protocols 0x%x target protocols 0x%x\\n\",\n\t\t dev_name(&dev->dev), im_protocols, tm_protocols);\n\n\tif (!im_protocols && !tm_protocols)\n\t\treturn -EINVAL;\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->start_poll(dev, im_protocols, tm_protocols);\n\tif (!rc) {\n\t\tdev->polling = true;\n\t\tdev->rf_mode = NFC_RF_NONE;\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_start_poll(struct nfc_dev *dev, u32 im_protocols, u32 tm_protocols)\n{\n\tint rc;\n\n\tpr_debug(\"dev_name %s initiator protocols 0x%x target protocols 0x%x\\n\",\n\t\t dev_name(&dev->dev), im_protocols, tm_protocols);\n\n\tif (!im_protocols && !tm_protocols)\n\t\treturn -EINVAL;\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\trc = dev->ops->start_poll(dev, im_protocols, tm_protocols);\n\tif (!rc) {\n\t\tdev->polling = true;\n\t\tdev->rf_mode = NFC_RF_NONE;\n\t}\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,7 +10,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_dev_down",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_dev_down(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling || dev->active_target) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_down)\n\t\tdev->ops->dev_down(dev);\n\n\tdev->dev_up = false;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_dev_down(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (!dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->polling || dev->active_target) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_down)\n\t\tdev->ops->dev_down(dev);\n\n\tdev->dev_up = false;\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-1974",
        "func_name": "torvalds/linux/nfc_dev_up",
        "description": "A use-after-free flaw was found in the Linux kernel's NFC core functionality due to a race condition between kobject creation and delete. This vulnerability allows a local attacker with CAP_NET_ADMIN privilege to leak kernel information.",
        "git_url": "https://github.com/torvalds/linux/commit/da5c0f119203ad9728920456a0f52a6d850c01cd",
        "commit_title": "nfc: replace improper check device_is_registered() in netlink related functions",
        "commit_text": " The device_is_registered() in nfc core is used to check whether nfc device is registered in netlink related functions such as nfc_fw_download(), nfc_dev_up() and so on. Although device_is_registered() is protected by device_lock, there is still a race condition between device_del() and device_is_registered(). The root cause is that kobject_del() in device_del() is not protected by device_lock.     (cleanup task)         |     (netlink task)                           | nfc_unregister_device     | nfc_fw_download  device_del               |  device_lock   ...                     |   if (!device_is_registered)//(1)   kobject_del//(2)        |   ...  ...                      |  device_unlock  The device_is_registered() returns the value of state_in_sysfs and the state_in_sysfs is set to zero in kobject_del(). If we pass check in position (1), then set zero in position (2). As a result, the check in position (1) is useless.  This patch uses bool variable instead of device_is_registered() to judge whether the nfc device is registered, which is well synchronized. ",
        "func_before": "int nfc_dev_up(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (!device_is_registered(&dev->dev)) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n\t\trc = -ERFKILL;\n\t\tgoto error;\n\t}\n\n\tif (dev->fw_download_in_progress) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_up)\n\t\trc = dev->ops->dev_up(dev);\n\n\tif (!rc)\n\t\tdev->dev_up = true;\n\n\t/* We have to enable the device before discovering SEs */\n\tif (dev->ops->discover_se && dev->ops->discover_se(dev))\n\t\tpr_err(\"SE discovery failed\\n\");\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "func": "int nfc_dev_up(struct nfc_dev *dev)\n{\n\tint rc = 0;\n\n\tpr_debug(\"dev_name=%s\\n\", dev_name(&dev->dev));\n\n\tdevice_lock(&dev->dev);\n\n\tif (dev->shutting_down) {\n\t\trc = -ENODEV;\n\t\tgoto error;\n\t}\n\n\tif (dev->rfkill && rfkill_blocked(dev->rfkill)) {\n\t\trc = -ERFKILL;\n\t\tgoto error;\n\t}\n\n\tif (dev->fw_download_in_progress) {\n\t\trc = -EBUSY;\n\t\tgoto error;\n\t}\n\n\tif (dev->dev_up) {\n\t\trc = -EALREADY;\n\t\tgoto error;\n\t}\n\n\tif (dev->ops->dev_up)\n\t\trc = dev->ops->dev_up(dev);\n\n\tif (!rc)\n\t\tdev->dev_up = true;\n\n\t/* We have to enable the device before discovering SEs */\n\tif (dev->ops->discover_se && dev->ops->discover_se(dev))\n\t\tpr_err(\"SE discovery failed\\n\");\n\nerror:\n\tdevice_unlock(&dev->dev);\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,7 +6,7 @@\n \n \tdevice_lock(&dev->dev);\n \n-\tif (!device_is_registered(&dev->dev)) {\n+\tif (dev->shutting_down) {\n \t\trc = -ENODEV;\n \t\tgoto error;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!device_is_registered(&dev->dev)) {"
            ],
            "added_lines": [
                "\tif (dev->shutting_down) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25212",
        "func_name": "torvalds/linux/decode_attr_security_label",
        "description": "A TOCTOU mismatch in the NFS client code in the Linux kernel before 5.8.3 could be used by local attackers to corrupt memory or possibly have unspecified other impact because a size check is in fs/nfs/nfs4proc.c instead of fs/nfs/nfs4xdr.c, aka CID-b4487b935452.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b4487b93545214a9db8cbf32e86411677b0cca21",
        "commit_title": "Move the buffer size check to decode_attr_security_label() before memcpy()",
        "commit_text": "Only call memcpy() if the buffer is large enough  [Trond: clean up duplicate test of label->len != 0] ",
        "func_before": "static int decode_attr_security_label(struct xdr_stream *xdr, uint32_t *bitmap,\n\t\t\t\t\tstruct nfs4_label *label)\n{\n\tuint32_t pi = 0;\n\tuint32_t lfs = 0;\n\t__u32 len;\n\t__be32 *p;\n\tint status = 0;\n\n\tif (unlikely(bitmap[2] & (FATTR4_WORD2_SECURITY_LABEL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[2] & FATTR4_WORD2_SECURITY_LABEL)) {\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tlfs = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tpi = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tlen = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, len);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tif (len < NFS4_MAXLABELLEN) {\n\t\t\tif (label) {\n\t\t\t\tmemcpy(label->label, p, len);\n\t\t\t\tlabel->len = len;\n\t\t\t\tlabel->pi = pi;\n\t\t\t\tlabel->lfs = lfs;\n\t\t\t\tstatus = NFS_ATTR_FATTR_V4_SECURITY_LABEL;\n\t\t\t}\n\t\t\tbitmap[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\t} else\n\t\t\tprintk(KERN_WARNING \"%s: label too long (%u)!\\n\",\n\t\t\t\t\t__func__, len);\n\t}\n\tif (label && label->label)\n\t\tdprintk(\"%s: label=%s, len=%d, PI=%d, LFS=%d\\n\", __func__,\n\t\t\t(char *)label->label, label->len, label->pi, label->lfs);\n\treturn status;\n}",
        "func": "static int decode_attr_security_label(struct xdr_stream *xdr, uint32_t *bitmap,\n\t\t\t\t\tstruct nfs4_label *label)\n{\n\tuint32_t pi = 0;\n\tuint32_t lfs = 0;\n\t__u32 len;\n\t__be32 *p;\n\tint status = 0;\n\n\tif (unlikely(bitmap[2] & (FATTR4_WORD2_SECURITY_LABEL - 1U)))\n\t\treturn -EIO;\n\tif (likely(bitmap[2] & FATTR4_WORD2_SECURITY_LABEL)) {\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tlfs = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tpi = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, 4);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tlen = be32_to_cpup(p++);\n\t\tp = xdr_inline_decode(xdr, len);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tif (len < NFS4_MAXLABELLEN) {\n\t\t\tif (label) {\n\t\t\t\tif (label->len) {\n\t\t\t\t\tif (label->len < len)\n\t\t\t\t\t\treturn -ERANGE;\n\t\t\t\t\tmemcpy(label->label, p, len);\n\t\t\t\t}\n\t\t\t\tlabel->len = len;\n\t\t\t\tlabel->pi = pi;\n\t\t\t\tlabel->lfs = lfs;\n\t\t\t\tstatus = NFS_ATTR_FATTR_V4_SECURITY_LABEL;\n\t\t\t}\n\t\t\tbitmap[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\t} else\n\t\t\tprintk(KERN_WARNING \"%s: label too long (%u)!\\n\",\n\t\t\t\t\t__func__, len);\n\t}\n\tif (label && label->label)\n\t\tdprintk(\"%s: label=%s, len=%d, PI=%d, LFS=%d\\n\", __func__,\n\t\t\t(char *)label->label, label->len, label->pi, label->lfs);\n\treturn status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,7 +27,11 @@\n \t\t\treturn -EIO;\n \t\tif (len < NFS4_MAXLABELLEN) {\n \t\t\tif (label) {\n-\t\t\t\tmemcpy(label->label, p, len);\n+\t\t\t\tif (label->len) {\n+\t\t\t\t\tif (label->len < len)\n+\t\t\t\t\t\treturn -ERANGE;\n+\t\t\t\t\tmemcpy(label->label, p, len);\n+\t\t\t\t}\n \t\t\t\tlabel->len = len;\n \t\t\t\tlabel->pi = pi;\n \t\t\t\tlabel->lfs = lfs;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tmemcpy(label->label, p, len);"
            ],
            "added_lines": [
                "\t\t\t\tif (label->len) {",
                "\t\t\t\t\tif (label->len < len)",
                "\t\t\t\t\t\treturn -ERANGE;",
                "\t\t\t\t\tmemcpy(label->label, p, len);",
                "\t\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-25212",
        "func_name": "torvalds/linux/_nfs4_get_security_label",
        "description": "A TOCTOU mismatch in the NFS client code in the Linux kernel before 5.8.3 could be used by local attackers to corrupt memory or possibly have unspecified other impact because a size check is in fs/nfs/nfs4proc.c instead of fs/nfs/nfs4xdr.c, aka CID-b4487b935452.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b4487b93545214a9db8cbf32e86411677b0cca21",
        "commit_title": "Move the buffer size check to decode_attr_security_label() before memcpy()",
        "commit_text": "Only call memcpy() if the buffer is large enough  [Trond: clean up duplicate test of label->len != 0] ",
        "func_before": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\n\tnfs_fattr_init(&fattr);\n\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\tif (buflen < label.len)\n\t\treturn -ERANGE;\n\treturn 0;\n}",
        "func": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\n\tnfs_fattr_init(&fattr);\n\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,5 @@\n \t\treturn ret;\n \tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n \t\treturn -ENOENT;\n-\tif (buflen < label.len)\n-\t\treturn -ERANGE;\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (buflen < label.len)",
                "\t\treturn -ERANGE;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-1295",
        "func_name": "torvalds/linux/io_close_prep",
        "description": "A time-of-check to time-of-use issue exists in io_uring subsystem's IORING_OP_CLOSE operation in the Linux kernel's versions 5.6 - 5.11 (inclusive), which allows a local user to elevate their privileges to root. Introduced in b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb, patched in 9eac1904d3364254d622bf2c771c4f85cd435fc2, backported to stable in 788d0824269bef539fe31a785b1517882eafed93.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=9eac1904d3364254d622bf2c771c4f85cd435fc2",
        "commit_title": "We currently split the close into two, in case we have a ->flush op",
        "commit_text": "that we can't safely handle from non-blocking context. This requires us to flag the op as uncancelable if we do need to punt it async, and that means special handling for just this op type.  Use __close_fd_get_file() and grab the files lock so we can get the file and check if we need to go async in one atomic operation. That gets rid of the need for splitting this into two steps, and hence the need for IO_WQ_WORK_NO_CANCEL.  ",
        "func_before": "static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\t/*\n\t * If we queue this for async, it must not be cancellable. That would\n\t * leave the 'file' in an undeterminate state, and here need to modify\n\t * io_wq_work.flags, so initialize io_wq_work firstly.\n\t */\n\tio_req_init_async(req);\n\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\tif ((req->file && req->file->f_op == &io_uring_fops))\n\t\treturn -EBADF;\n\n\treq->close.put_file = NULL;\n\treturn 0;\n}",
        "func": "static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n\t    sqe->rw_flags || sqe->buf_index)\n\t\treturn -EINVAL;\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treturn -EBADF;\n\n\treq->close.fd = READ_ONCE(sqe->fd);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,5 @@\n static int io_close_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n {\n-\t/*\n-\t * If we queue this for async, it must not be cancellable. That would\n-\t * leave the 'file' in an undeterminate state, and here need to modify\n-\t * io_wq_work.flags, so initialize io_wq_work firstly.\n-\t */\n-\tio_req_init_async(req);\n-\n \tif (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))\n \t\treturn -EINVAL;\n \tif (sqe->ioprio || sqe->off || sqe->addr || sqe->len ||\n@@ -16,9 +9,5 @@\n \t\treturn -EBADF;\n \n \treq->close.fd = READ_ONCE(sqe->fd);\n-\tif ((req->file && req->file->f_op == &io_uring_fops))\n-\t\treturn -EBADF;\n-\n-\treq->close.put_file = NULL;\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t/*",
                "\t * If we queue this for async, it must not be cancellable. That would",
                "\t * leave the 'file' in an undeterminate state, and here need to modify",
                "\t * io_wq_work.flags, so initialize io_wq_work firstly.",
                "\t */",
                "\tio_req_init_async(req);",
                "",
                "\tif ((req->file && req->file->f_op == &io_uring_fops))",
                "\t\treturn -EBADF;",
                "",
                "\treq->close.put_file = NULL;"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-1295",
        "func_name": "torvalds/linux/io_close",
        "description": "A time-of-check to time-of-use issue exists in io_uring subsystem's IORING_OP_CLOSE operation in the Linux kernel's versions 5.6 - 5.11 (inclusive), which allows a local user to elevate their privileges to root. Introduced in b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb, patched in 9eac1904d3364254d622bf2c771c4f85cd435fc2, backported to stable in 788d0824269bef539fe31a785b1517882eafed93.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=9eac1904d3364254d622bf2c771c4f85cd435fc2",
        "commit_title": "We currently split the close into two, in case we have a ->flush op",
        "commit_text": "that we can't safely handle from non-blocking context. This requires us to flag the op as uncancelable if we do need to punt it async, and that means special handling for just this op type.  Use __close_fd_get_file() and grab the files lock so we can get the file and check if we need to go async in one atomic operation. That gets rid of the need for splitting this into two steps, and hence the need for IO_WQ_WORK_NO_CANCEL.  ",
        "func_before": "static int io_close(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct io_close *close = &req->close;\n\tint ret;\n\n\t/* might be already done during nonblock submission */\n\tif (!close->put_file) {\n\t\tret = close_fd_get_file(close->fd, &close->put_file);\n\t\tif (ret < 0)\n\t\t\treturn (ret == -ENOENT) ? -EBADF : ret;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (close->put_file->f_op->flush && force_nonblock) {\n\t\t/* not safe to cancel at this point */\n\t\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n\t\t/* was never set, but play safe */\n\t\treq->flags &= ~REQ_F_NOWAIT;\n\t\t/* avoid grabbing files - we don't need the files */\n\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n\t\treturn -EAGAIN;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(close->put_file, req->work.identity->files);\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tfput(close->put_file);\n\tclose->put_file = NULL;\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}",
        "func": "static int io_close(struct io_kiocb *req, bool force_nonblock,\n\t\t    struct io_comp_state *cs)\n{\n\tstruct files_struct *files = current->files;\n\tstruct io_close *close = &req->close;\n\tstruct fdtable *fdt;\n\tstruct file *file;\n\tint ret;\n\n\tfile = NULL;\n\tret = -EBADF;\n\tspin_lock(&files->file_lock);\n\tfdt = files_fdtable(files);\n\tif (close->fd >= fdt->max_fds) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\tfile = fdt->fd[close->fd];\n\tif (!file) {\n\t\tspin_unlock(&files->file_lock);\n\t\tgoto err;\n\t}\n\n\tif (file->f_op == &io_uring_fops) {\n\t\tspin_unlock(&files->file_lock);\n\t\tfile = NULL;\n\t\tgoto err;\n\t}\n\n\t/* if the file has a flush method, be safe and punt to async */\n\tif (file->f_op->flush && force_nonblock) {\n\t\tspin_unlock(&files->file_lock);\n\t\treturn -EAGAIN;\n\t}\n\n\tret = __close_fd_get_file(close->fd, &file);\n\tspin_unlock(&files->file_lock);\n\tif (ret < 0) {\n\t\tif (ret == -ENOENT)\n\t\t\tret = -EBADF;\n\t\tgoto err;\n\t}\n\n\t/* No ->flush() or already async, safely close from here */\n\tret = filp_close(file, current->files);\nerr:\n\tif (ret < 0)\n\t\treq_set_fail_links(req);\n\tif (file)\n\t\tfput(file);\n\t__io_req_complete(req, ret, 0, cs);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,33 +1,53 @@\n static int io_close(struct io_kiocb *req, bool force_nonblock,\n \t\t    struct io_comp_state *cs)\n {\n+\tstruct files_struct *files = current->files;\n \tstruct io_close *close = &req->close;\n+\tstruct fdtable *fdt;\n+\tstruct file *file;\n \tint ret;\n \n-\t/* might be already done during nonblock submission */\n-\tif (!close->put_file) {\n-\t\tret = close_fd_get_file(close->fd, &close->put_file);\n-\t\tif (ret < 0)\n-\t\t\treturn (ret == -ENOENT) ? -EBADF : ret;\n+\tfile = NULL;\n+\tret = -EBADF;\n+\tspin_lock(&files->file_lock);\n+\tfdt = files_fdtable(files);\n+\tif (close->fd >= fdt->max_fds) {\n+\t\tspin_unlock(&files->file_lock);\n+\t\tgoto err;\n+\t}\n+\tfile = fdt->fd[close->fd];\n+\tif (!file) {\n+\t\tspin_unlock(&files->file_lock);\n+\t\tgoto err;\n+\t}\n+\n+\tif (file->f_op == &io_uring_fops) {\n+\t\tspin_unlock(&files->file_lock);\n+\t\tfile = NULL;\n+\t\tgoto err;\n \t}\n \n \t/* if the file has a flush method, be safe and punt to async */\n-\tif (close->put_file->f_op->flush && force_nonblock) {\n-\t\t/* not safe to cancel at this point */\n-\t\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;\n-\t\t/* was never set, but play safe */\n-\t\treq->flags &= ~REQ_F_NOWAIT;\n-\t\t/* avoid grabbing files - we don't need the files */\n-\t\treq->flags |= REQ_F_NO_FILE_TABLE;\n+\tif (file->f_op->flush && force_nonblock) {\n+\t\tspin_unlock(&files->file_lock);\n \t\treturn -EAGAIN;\n \t}\n \n+\tret = __close_fd_get_file(close->fd, &file);\n+\tspin_unlock(&files->file_lock);\n+\tif (ret < 0) {\n+\t\tif (ret == -ENOENT)\n+\t\t\tret = -EBADF;\n+\t\tgoto err;\n+\t}\n+\n \t/* No ->flush() or already async, safely close from here */\n-\tret = filp_close(close->put_file, req->work.identity->files);\n+\tret = filp_close(file, current->files);\n+err:\n \tif (ret < 0)\n \t\treq_set_fail_links(req);\n-\tfput(close->put_file);\n-\tclose->put_file = NULL;\n+\tif (file)\n+\t\tfput(file);\n \t__io_req_complete(req, ret, 0, cs);\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* might be already done during nonblock submission */",
                "\tif (!close->put_file) {",
                "\t\tret = close_fd_get_file(close->fd, &close->put_file);",
                "\t\tif (ret < 0)",
                "\t\t\treturn (ret == -ENOENT) ? -EBADF : ret;",
                "\tif (close->put_file->f_op->flush && force_nonblock) {",
                "\t\t/* not safe to cancel at this point */",
                "\t\treq->work.flags |= IO_WQ_WORK_NO_CANCEL;",
                "\t\t/* was never set, but play safe */",
                "\t\treq->flags &= ~REQ_F_NOWAIT;",
                "\t\t/* avoid grabbing files - we don't need the files */",
                "\t\treq->flags |= REQ_F_NO_FILE_TABLE;",
                "\tret = filp_close(close->put_file, req->work.identity->files);",
                "\tfput(close->put_file);",
                "\tclose->put_file = NULL;"
            ],
            "added_lines": [
                "\tstruct files_struct *files = current->files;",
                "\tstruct fdtable *fdt;",
                "\tstruct file *file;",
                "\tfile = NULL;",
                "\tret = -EBADF;",
                "\tspin_lock(&files->file_lock);",
                "\tfdt = files_fdtable(files);",
                "\tif (close->fd >= fdt->max_fds) {",
                "\t\tspin_unlock(&files->file_lock);",
                "\t\tgoto err;",
                "\t}",
                "\tfile = fdt->fd[close->fd];",
                "\tif (!file) {",
                "\t\tspin_unlock(&files->file_lock);",
                "\t\tgoto err;",
                "\t}",
                "",
                "\tif (file->f_op == &io_uring_fops) {",
                "\t\tspin_unlock(&files->file_lock);",
                "\t\tfile = NULL;",
                "\t\tgoto err;",
                "\tif (file->f_op->flush && force_nonblock) {",
                "\t\tspin_unlock(&files->file_lock);",
                "\tret = __close_fd_get_file(close->fd, &file);",
                "\tspin_unlock(&files->file_lock);",
                "\tif (ret < 0) {",
                "\t\tif (ret == -ENOENT)",
                "\t\t\tret = -EBADF;",
                "\t\tgoto err;",
                "\t}",
                "",
                "\tret = filp_close(file, current->files);",
                "err:",
                "\tif (file)",
                "\t\tfput(file);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1295",
        "func_name": "torvalds/linux/io_grab_files",
        "description": "A time-of-check to time-of-use issue exists in io_uring subsystem's IORING_OP_CLOSE operation in the Linux kernel's versions 5.6 - 5.11 (inclusive), which allows a local user to elevate their privileges to root. Introduced in b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb, patched in 9eac1904d3364254d622bf2c771c4f85cd435fc2, backported to stable in 788d0824269bef539fe31a785b1517882eafed93.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb",
        "commit_title": "This works just like close(2), unsurprisingly. We remove the file",
        "commit_text": "descriptor and post the completion inline, then offload the actual (potential) last file put to async context.  Mark the async part of this work as uncancellable, as we really must guarantee that the latter part of the close is run.  ",
        "func_before": "static int io_grab_files(struct io_kiocb *req)\n{\n\tint ret = -EBADF;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\trcu_read_lock();\n\tspin_lock_irq(&ctx->inflight_lock);\n\t/*\n\t * We use the f_ops->flush() handler to ensure that we can flush\n\t * out work accessing these files if the fd is closed. Check if\n\t * the fd has changed since we started down this path, and disallow\n\t * this operation if it has.\n\t */\n\tif (fcheck(req->ring_fd) == req->ring_file) {\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\treq->work.files = current->files;\n\t\tret = 0;\n\t}\n\tspin_unlock_irq(&ctx->inflight_lock);\n\trcu_read_unlock();\n\n\treturn ret;\n}",
        "func": "static int io_grab_files(struct io_kiocb *req)\n{\n\tint ret = -EBADF;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!req->ring_file)\n\t\treturn -EBADF;\n\n\trcu_read_lock();\n\tspin_lock_irq(&ctx->inflight_lock);\n\t/*\n\t * We use the f_ops->flush() handler to ensure that we can flush\n\t * out work accessing these files if the fd is closed. Check if\n\t * the fd has changed since we started down this path, and disallow\n\t * this operation if it has.\n\t */\n\tif (fcheck(req->ring_fd) == req->ring_file) {\n\t\tlist_add(&req->inflight_entry, &ctx->inflight_list);\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\treq->work.files = current->files;\n\t\tret = 0;\n\t}\n\tspin_unlock_irq(&ctx->inflight_lock);\n\trcu_read_unlock();\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,9 @@\n {\n \tint ret = -EBADF;\n \tstruct io_ring_ctx *ctx = req->ctx;\n+\n+\tif (!req->ring_file)\n+\t\treturn -EBADF;\n \n \trcu_read_lock();\n \tspin_lock_irq(&ctx->inflight_lock);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\tif (!req->ring_file)",
                "\t\treturn -EBADF;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1295",
        "func_name": "torvalds/linux/io_req_defer_prep",
        "description": "A time-of-check to time-of-use issue exists in io_uring subsystem's IORING_OP_CLOSE operation in the Linux kernel's versions 5.6 - 5.11 (inclusive), which allows a local user to elevate their privileges to root. Introduced in b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb, patched in 9eac1904d3364254d622bf2c771c4f85cd435fc2, backported to stable in 788d0824269bef539fe31a785b1517882eafed93.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb",
        "commit_title": "This works just like close(2), unsurprisingly. We remove the file",
        "commit_text": "descriptor and post the completion inline, then offload the actual (potential) last file put to async context.  Mark the async part of this work as uncancellable, as we really must guarantee that the latter part of the close is run.  ",
        "func_before": "static int io_req_defer_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tssize_t ret = 0;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\t\tret = io_read_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\t\tret = io_write_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_prep_fsync(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_prep_sfr(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout_prep(req, sqe, false);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\tret = io_timeout_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat_prep(req, sqe);\n\t\tbreak;\n\tdefault:\n\t\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\t\treq->opcode);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "func": "static int io_req_defer_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe)\n{\n\tssize_t ret = 0;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\t\tret = io_read_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\t\tret = io_write_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tret = io_poll_add_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tret = io_poll_remove_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tret = io_prep_fsync(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tret = io_prep_sfr(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tret = io_sendmsg_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tret = io_recvmsg_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tret = io_connect_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tret = io_timeout_prep(req, sqe, false);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tret = io_timeout_remove_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tret = io_async_cancel_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_LINK_TIMEOUT:\n\t\tret = io_timeout_prep(req, sqe, true);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tret = io_accept_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tret = io_fallocate_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tret = io_openat_prep(req, sqe);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tret = io_close_prep(req, sqe);\n\t\tbreak;\n\tdefault:\n\t\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n\t\t\t\treq->opcode);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,6 +56,9 @@\n \tcase IORING_OP_OPENAT:\n \t\tret = io_openat_prep(req, sqe);\n \t\tbreak;\n+\tcase IORING_OP_CLOSE:\n+\t\tret = io_close_prep(req, sqe);\n+\t\tbreak;\n \tdefault:\n \t\tprintk_once(KERN_WARNING \"io_uring: unhandled opcode %d\\n\",\n \t\t\t\treq->opcode);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcase IORING_OP_CLOSE:",
                "\t\tret = io_close_prep(req, sqe);",
                "\t\tbreak;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1295",
        "func_name": "torvalds/linux/io_issue_sqe",
        "description": "A time-of-check to time-of-use issue exists in io_uring subsystem's IORING_OP_CLOSE operation in the Linux kernel's versions 5.6 - 5.11 (inclusive), which allows a local user to elevate their privileges to root. Introduced in b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb, patched in 9eac1904d3364254d622bf2c771c4f85cd435fc2, backported to stable in 788d0824269bef539fe31a785b1517882eafed93.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=b5dba59e0cf7e2cc4d3b3b1ac5fe81ddf21959eb",
        "commit_title": "This works just like close(2), unsurprisingly. We remove the file",
        "commit_text": "descriptor and post the completion inline, then offload the actual (potential) last file put to async context.  Mark the async part of this work as uncancellable, as we really must guarantee that the latter part of the close is run.  ",
        "func_before": "static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\tstruct io_kiocb **nxt, bool force_nonblock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\t\tif (sqe) {\n\t\t\tret = io_read_prep(req, sqe, force_nonblock);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_read(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\t\tif (sqe) {\n\t\t\tret = io_write_prep(req, sqe, force_nonblock);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_write(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tif (sqe) {\n\t\t\tret = io_prep_fsync(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_fsync(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tif (sqe) {\n\t\t\tret = io_poll_add_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_poll_add(req, nxt);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tif (sqe) {\n\t\t\tret = io_poll_remove_prep(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_poll_remove(req);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tif (sqe) {\n\t\t\tret = io_prep_sfr(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_sync_file_range(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tif (sqe) {\n\t\t\tret = io_sendmsg_prep(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_sendmsg(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tif (sqe) {\n\t\t\tret = io_recvmsg_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_recvmsg(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tif (sqe) {\n\t\t\tret = io_timeout_prep(req, sqe, false);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_timeout(req);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tif (sqe) {\n\t\t\tret = io_timeout_remove_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_timeout_remove(req);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tif (sqe) {\n\t\t\tret = io_accept_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_accept(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tif (sqe) {\n\t\t\tret = io_connect_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_connect(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tif (sqe) {\n\t\t\tret = io_async_cancel_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_async_cancel(req, nxt);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tif (sqe) {\n\t\t\tret = io_fallocate_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_fallocate(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tif (sqe) {\n\t\t\tret = io_openat_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_openat(req, nxt, force_nonblock);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\tif (req->result == -EAGAIN)\n\t\t\treturn -EAGAIN;\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}",
        "func": "static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,\n\t\t\tstruct io_kiocb **nxt, bool force_nonblock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tswitch (req->opcode) {\n\tcase IORING_OP_NOP:\n\t\tret = io_nop(req);\n\t\tbreak;\n\tcase IORING_OP_READV:\n\tcase IORING_OP_READ_FIXED:\n\t\tif (sqe) {\n\t\t\tret = io_read_prep(req, sqe, force_nonblock);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_read(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_WRITEV:\n\tcase IORING_OP_WRITE_FIXED:\n\t\tif (sqe) {\n\t\t\tret = io_write_prep(req, sqe, force_nonblock);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_write(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_FSYNC:\n\t\tif (sqe) {\n\t\t\tret = io_prep_fsync(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_fsync(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_POLL_ADD:\n\t\tif (sqe) {\n\t\t\tret = io_poll_add_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_poll_add(req, nxt);\n\t\tbreak;\n\tcase IORING_OP_POLL_REMOVE:\n\t\tif (sqe) {\n\t\t\tret = io_poll_remove_prep(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_poll_remove(req);\n\t\tbreak;\n\tcase IORING_OP_SYNC_FILE_RANGE:\n\t\tif (sqe) {\n\t\t\tret = io_prep_sfr(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_sync_file_range(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_SENDMSG:\n\t\tif (sqe) {\n\t\t\tret = io_sendmsg_prep(req, sqe);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_sendmsg(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_RECVMSG:\n\t\tif (sqe) {\n\t\t\tret = io_recvmsg_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_recvmsg(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT:\n\t\tif (sqe) {\n\t\t\tret = io_timeout_prep(req, sqe, false);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_timeout(req);\n\t\tbreak;\n\tcase IORING_OP_TIMEOUT_REMOVE:\n\t\tif (sqe) {\n\t\t\tret = io_timeout_remove_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_timeout_remove(req);\n\t\tbreak;\n\tcase IORING_OP_ACCEPT:\n\t\tif (sqe) {\n\t\t\tret = io_accept_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_accept(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_CONNECT:\n\t\tif (sqe) {\n\t\t\tret = io_connect_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_connect(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_ASYNC_CANCEL:\n\t\tif (sqe) {\n\t\t\tret = io_async_cancel_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_async_cancel(req, nxt);\n\t\tbreak;\n\tcase IORING_OP_FALLOCATE:\n\t\tif (sqe) {\n\t\t\tret = io_fallocate_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_fallocate(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_OPENAT:\n\t\tif (sqe) {\n\t\t\tret = io_openat_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_openat(req, nxt, force_nonblock);\n\t\tbreak;\n\tcase IORING_OP_CLOSE:\n\t\tif (sqe) {\n\t\t\tret = io_close_prep(req, sqe);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_close(req, nxt, force_nonblock);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tconst bool in_async = io_wq_current_is_worker();\n\n\t\tif (req->result == -EAGAIN)\n\t\t\treturn -EAGAIN;\n\n\t\t/* workqueue context doesn't hold uring_lock, grab it now */\n\t\tif (in_async)\n\t\t\tmutex_lock(&ctx->uring_lock);\n\n\t\tio_iopoll_req_issued(req);\n\n\t\tif (in_async)\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -130,6 +130,14 @@\n \t\t}\n \t\tret = io_openat(req, nxt, force_nonblock);\n \t\tbreak;\n+\tcase IORING_OP_CLOSE:\n+\t\tif (sqe) {\n+\t\t\tret = io_close_prep(req, sqe);\n+\t\t\tif (ret)\n+\t\t\t\tbreak;\n+\t\t}\n+\t\tret = io_close(req, nxt, force_nonblock);\n+\t\tbreak;\n \tdefault:\n \t\tret = -EINVAL;\n \t\tbreak;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tcase IORING_OP_CLOSE:",
                "\t\tif (sqe) {",
                "\t\t\tret = io_close_prep(req, sqe);",
                "\t\t\tif (ret)",
                "\t\t\t\tbreak;",
                "\t\t}",
                "\t\tret = io_close(req, nxt, force_nonblock);",
                "\t\tbreak;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29657",
        "func_name": "torvalds/linux/nested_svm_vmrun",
        "description": "arch/x86/kvm/svm/nested.c in the Linux kernel before 5.11.12 has a use-after-free in which an AMD KVM guest can bypass access control on host OS MSRs when there are nested guests, aka CID-a58d9166a756. This occurs because of a TOCTOU race condition associated with a VMCB12 double fetch in nested_svm_vmrun.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a58d9166a756a0f4a6618e4f593232593d6df134",
        "commit_title": "Avoid races between check and use of the nested VMCB controls.  This",
        "commit_text": "for example ensures that the VMRUN intercept is always reflected to the nested hypervisor, instead of being processed by the host.  Without this patch, it is possible to end up with svm->nested.hsave pointing to the MSR permission bitmap for nested guests.  This bug is CVE-2021-29657.  Cc: stable@vger.kernel.org ",
        "func_before": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tif (!nested_vmcb_checks(svm, vmcb12)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "func": "int nested_svm_vmrun(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct vmcb *vmcb12;\n\tstruct vmcb *hsave = svm->nested.hsave;\n\tstruct vmcb *vmcb = svm->vmcb;\n\tstruct kvm_host_map map;\n\tu64 vmcb12_gpa;\n\n\tif (is_smm(&svm->vcpu)) {\n\t\tkvm_queue_exception(&svm->vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcb12_gpa = svm->vmcb->save.rax;\n\tret = kvm_vcpu_map(&svm->vcpu, gpa_to_gfn(vmcb12_gpa), &map);\n\tif (ret == -EINVAL) {\n\t\tkvm_inject_gp(&svm->vcpu, 0);\n\t\treturn 1;\n\t} else if (ret) {\n\t\treturn kvm_skip_emulated_instruction(&svm->vcpu);\n\t}\n\n\tret = kvm_skip_emulated_instruction(&svm->vcpu);\n\n\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_kvm_nested_vmrun(svm->vmcb->save.rip, vmcb12_gpa,\n\t\t\t       vmcb12->save.rip,\n\t\t\t       vmcb12->control.int_ctl,\n\t\t\t       vmcb12->control.event_inj,\n\t\t\t       vmcb12->control.nested_ctl);\n\n\ttrace_kvm_nested_intercepts(vmcb12->control.intercepts[INTERCEPT_CR] & 0xffff,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_CR] >> 16,\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_EXCEPTION],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD3],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD4],\n\t\t\t\t    vmcb12->control.intercepts[INTERCEPT_WORD5]);\n\n\t/* Clear internal status */\n\tkvm_clear_exception_queue(&svm->vcpu);\n\tkvm_clear_interrupt_queue(&svm->vcpu);\n\n\t/*\n\t * Save the old vmcb, so we don't need to pick what we save, but can\n\t * restore everything when a VMEXIT occurs\n\t */\n\thsave->save.es     = vmcb->save.es;\n\thsave->save.cs     = vmcb->save.cs;\n\thsave->save.ss     = vmcb->save.ss;\n\thsave->save.ds     = vmcb->save.ds;\n\thsave->save.gdtr   = vmcb->save.gdtr;\n\thsave->save.idtr   = vmcb->save.idtr;\n\thsave->save.efer   = svm->vcpu.arch.efer;\n\thsave->save.cr0    = kvm_read_cr0(&svm->vcpu);\n\thsave->save.cr4    = svm->vcpu.arch.cr4;\n\thsave->save.rflags = kvm_get_rflags(&svm->vcpu);\n\thsave->save.rip    = kvm_rip_read(&svm->vcpu);\n\thsave->save.rsp    = vmcb->save.rsp;\n\thsave->save.rax    = vmcb->save.rax;\n\tif (npt_enabled)\n\t\thsave->save.cr3    = vmcb->save.cr3;\n\telse\n\t\thsave->save.cr3    = kvm_read_cr3(&svm->vcpu);\n\n\tcopy_vmcb_control_area(&hsave->control, &vmcb->control);\n\n\tsvm->nested.nested_run_pending = 1;\n\n\tif (enter_svm_guest_mode(svm, vmcb12_gpa, vmcb12))\n\t\tgoto out_exit_err;\n\n\tif (nested_svm_vmrun_msrpm(svm))\n\t\tgoto out;\n\nout_exit_err:\n\tsvm->nested.nested_run_pending = 0;\n\n\tsvm->vmcb->control.exit_code    = SVM_EXIT_ERR;\n\tsvm->vmcb->control.exit_code_hi = 0;\n\tsvm->vmcb->control.exit_info_1  = 0;\n\tsvm->vmcb->control.exit_info_2  = 0;\n\n\tnested_svm_vmexit(svm);\n\nout:\n\tkvm_vcpu_unmap(&svm->vcpu, &map, true);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,7 +28,10 @@\n \tif (WARN_ON_ONCE(!svm->nested.initialized))\n \t\treturn -EINVAL;\n \n-\tif (!nested_vmcb_checks(svm, vmcb12)) {\n+\tload_nested_vmcb_control(svm, &vmcb12->control);\n+\n+\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n+\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n \t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n \t\tvmcb12->control.exit_code_hi = 0;\n \t\tvmcb12->control.exit_info_1  = 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!nested_vmcb_checks(svm, vmcb12)) {"
            ],
            "added_lines": [
                "\tload_nested_vmcb_control(svm, &vmcb12->control);",
                "",
                "\tif (!nested_vmcb_check_save(svm, vmcb12) ||",
                "\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-29657",
        "func_name": "torvalds/linux/enter_svm_guest_mode",
        "description": "arch/x86/kvm/svm/nested.c in the Linux kernel before 5.11.12 has a use-after-free in which an AMD KVM guest can bypass access control on host OS MSRs when there are nested guests, aka CID-a58d9166a756. This occurs because of a TOCTOU race condition associated with a VMCB12 double fetch in nested_svm_vmrun.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=a58d9166a756a0f4a6618e4f593232593d6df134",
        "commit_title": "Avoid races between check and use of the nested VMCB controls.  This",
        "commit_text": "for example ensures that the VMRUN intercept is always reflected to the nested hypervisor, instead of being processed by the host.  Without this patch, it is possible to end up with svm->nested.hsave pointing to the MSR permission bitmap for nested guests.  This bug is CVE-2021-29657.  Cc: stable@vger.kernel.org ",
        "func_before": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "func": "int enter_svm_guest_mode(struct vcpu_svm *svm, u64 vmcb12_gpa,\n\t\t\t struct vmcb *vmcb12)\n{\n\tint ret;\n\n\tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n\tnested_prepare_vmcb_save(svm, vmcb12);\n\tnested_prepare_vmcb_control(svm);\n\n\tret = nested_svm_load_cr3(&svm->vcpu, vmcb12->save.cr3,\n\t\t\t\t  nested_npt_enabled(svm));\n\tif (ret)\n\t\treturn ret;\n\n\tsvm_set_gif(svm, true);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,6 @@\n \tint ret;\n \n \tsvm->nested.vmcb12_gpa = vmcb12_gpa;\n-\tload_nested_vmcb_control(svm, &vmcb12->control);\n \tnested_prepare_vmcb_save(svm, vmcb12);\n \tnested_prepare_vmcb_control(svm);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tload_nested_vmcb_control(svm, &vmcb12->control);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_files_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_files_cancel(struct files_struct *files)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_files_cancel(files);\n}",
        "func": "static inline void io_uring_files_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n-static inline void io_uring_files_cancel(struct files_struct *files)\n+static inline void io_uring_files_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_files_cancel(files);\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(false);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static inline void io_uring_files_cancel(struct files_struct *files)",
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_files_cancel(files);"
            ],
            "added_lines": [
                "static inline void io_uring_files_cancel(void)",
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_task_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_task_cancel();\n}",
        "func": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(true);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static inline void io_uring_task_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_task_cancel();\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(true);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_task_cancel();"
            ],
            "added_lines": [
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(true);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/do_exit",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "void __noreturn do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\t/*\n\t * We can get here from a kernel oops, sometimes with preemption off.\n\t * Start by checking for critical errors.\n\t * Then fix up important state like USER_DS and preemption.\n\t * Then do everything else.\n\t */\n\n\tWARN_ON(blk_needs_flush_plug(tsk));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\t/*\n\t * If do_exit is called because this processes oopsed, it's possible\n\t * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before\n\t * continuing. Amongst other possible reasons, this is to prevent\n\t * mm_release()->clear_child_tid() from writing to a user-controlled\n\t * kernel address.\n\t */\n\tforce_uaccess_begin();\n\n\tif (unlikely(in_atomic())) {\n\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\tpreempt_count());\n\t\tpreempt_count_set(PREEMPT_ENABLED);\n\t}\n\n\tprofile_task_exit(tsk);\n\tkcov_task_exit(tsk);\n\n\tptrace_event(PTRACE_EVENT_EXIT, code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");\n\t\tfutex_exit_recursive(tsk);\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\tio_uring_files_cancel(tsk->files);\n\texit_signals(tsk);  /* sets PF_EXITING */\n\n\t/* sync mm's RSS info before statistics gathering */\n\tif (tsk->mm)\n\t\tsync_mm_rss(tsk->mm);\n\tacct_update_integrals(tsk);\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n\t\t/*\n\t\t * If the last thread of global init has exited, panic\n\t\t * immediately to get a useable coredump.\n\t\t */\n\t\tif (unlikely(is_global_init(tsk)))\n\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\n\t\t\t\ttsk->signal->group_exit_code ?: (int)code);\n\n#ifdef CONFIG_POSIX_TIMERS\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk);\n#endif\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm();\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_shm(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tif (group_dead)\n\t\tdisassociate_ctty(1);\n\texit_task_namespaces(tsk);\n\texit_task_work(tsk);\n\texit_thread(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t *\n\t * because of cgroup mode, must be called before cgroup_exit()\n\t */\n\tperf_event_exit_task(tsk);\n\n\tsched_autogroup_exit_task(tsk);\n\tcgroup_exit(tsk);\n\n\t/*\n\t * FIXME: do that only when needed, using sched_exit tracepoint\n\t */\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\texit_tasks_rcu_start();\n\texit_notify(tsk, group_dead);\n\tproc_exit_connector(tsk);\n\tmpol_put_task_policy(tsk);\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held();\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\tfree_pipe_info(tsk->splice_pipe);\n\n\tif (tsk->task_frag.page)\n\t\tput_page(tsk->task_frag.page);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tcheck_stack_usage();\n\tpreempt_disable();\n\tif (tsk->nr_dirtied)\n\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\n\texit_rcu();\n\texit_tasks_rcu_finish();\n\n\tlockdep_free_task(tsk);\n\tdo_task_dead();\n}",
        "func": "void __noreturn do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\t/*\n\t * We can get here from a kernel oops, sometimes with preemption off.\n\t * Start by checking for critical errors.\n\t * Then fix up important state like USER_DS and preemption.\n\t * Then do everything else.\n\t */\n\n\tWARN_ON(blk_needs_flush_plug(tsk));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\t/*\n\t * If do_exit is called because this processes oopsed, it's possible\n\t * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before\n\t * continuing. Amongst other possible reasons, this is to prevent\n\t * mm_release()->clear_child_tid() from writing to a user-controlled\n\t * kernel address.\n\t */\n\tforce_uaccess_begin();\n\n\tif (unlikely(in_atomic())) {\n\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\tpreempt_count());\n\t\tpreempt_count_set(PREEMPT_ENABLED);\n\t}\n\n\tprofile_task_exit(tsk);\n\tkcov_task_exit(tsk);\n\n\tptrace_event(PTRACE_EVENT_EXIT, code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");\n\t\tfutex_exit_recursive(tsk);\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\tio_uring_files_cancel();\n\texit_signals(tsk);  /* sets PF_EXITING */\n\n\t/* sync mm's RSS info before statistics gathering */\n\tif (tsk->mm)\n\t\tsync_mm_rss(tsk->mm);\n\tacct_update_integrals(tsk);\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n\t\t/*\n\t\t * If the last thread of global init has exited, panic\n\t\t * immediately to get a useable coredump.\n\t\t */\n\t\tif (unlikely(is_global_init(tsk)))\n\t\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\n\t\t\t\ttsk->signal->group_exit_code ?: (int)code);\n\n#ifdef CONFIG_POSIX_TIMERS\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk);\n#endif\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm();\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_shm(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tif (group_dead)\n\t\tdisassociate_ctty(1);\n\texit_task_namespaces(tsk);\n\texit_task_work(tsk);\n\texit_thread(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t *\n\t * because of cgroup mode, must be called before cgroup_exit()\n\t */\n\tperf_event_exit_task(tsk);\n\n\tsched_autogroup_exit_task(tsk);\n\tcgroup_exit(tsk);\n\n\t/*\n\t * FIXME: do that only when needed, using sched_exit tracepoint\n\t */\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\texit_tasks_rcu_start();\n\texit_notify(tsk, group_dead);\n\tproc_exit_connector(tsk);\n\tmpol_put_task_policy(tsk);\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held();\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\tfree_pipe_info(tsk->splice_pipe);\n\n\tif (tsk->task_frag.page)\n\t\tput_page(tsk->task_frag.page);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tcheck_stack_usage();\n\tpreempt_disable();\n\tif (tsk->nr_dirtied)\n\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\n\texit_rcu();\n\texit_tasks_rcu_finish();\n\n\tlockdep_free_task(tsk);\n\tdo_task_dead();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -51,7 +51,7 @@\n \t\tschedule();\n \t}\n \n-\tio_uring_files_cancel(tsk->files);\n+\tio_uring_files_cancel();\n \texit_signals(tsk);  /* sets PF_EXITING */\n \n \t/* sync mm's RSS info before statistics gathering */",
        "diff_line_info": {
            "deleted_lines": [
                "\tio_uring_files_cancel(tsk->files);"
            ],
            "added_lines": [
                "\tio_uring_files_cancel();"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/dup_task_struct",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static struct task_struct *dup_task_struct(struct task_struct *orig, int node)\n{\n\tstruct task_struct *tsk;\n\tunsigned long *stack;\n\tstruct vm_struct *stack_vm_area __maybe_unused;\n\tint err;\n\n\tif (node == NUMA_NO_NODE)\n\t\tnode = tsk_fork_get_node(orig);\n\ttsk = alloc_task_struct_node(node);\n\tif (!tsk)\n\t\treturn NULL;\n\n\tstack = alloc_thread_stack_node(tsk, node);\n\tif (!stack)\n\t\tgoto free_tsk;\n\n\tif (memcg_charge_kernel_stack(tsk))\n\t\tgoto free_stack;\n\n\tstack_vm_area = task_stack_vm_area(tsk);\n\n\terr = arch_dup_task_struct(tsk, orig);\n\n\t/*\n\t * arch_dup_task_struct() clobbers the stack-related fields.  Make\n\t * sure they're properly initialized before using any stack-related\n\t * functions again.\n\t */\n\ttsk->stack = stack;\n#ifdef CONFIG_VMAP_STACK\n\ttsk->stack_vm_area = stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\trefcount_set(&tsk->stack_refcount, 1);\n#endif\n\n\tif (err)\n\t\tgoto free_stack;\n\n\terr = scs_prepare(tsk, node);\n\tif (err)\n\t\tgoto free_stack;\n\n#ifdef CONFIG_SECCOMP\n\t/*\n\t * We must handle setting up seccomp filters once we're under\n\t * the sighand lock in case orig has changed between now and\n\t * then. Until then, filter must be NULL to avoid messing up\n\t * the usage counts on the error path calling free_task.\n\t */\n\ttsk->seccomp.filter = NULL;\n#endif\n\n\tsetup_thread_stack(tsk, orig);\n\tclear_user_return_notifier(tsk);\n\tclear_tsk_need_resched(tsk);\n\tset_task_stack_end_magic(tsk);\n\n#ifdef CONFIG_STACKPROTECTOR\n\ttsk->stack_canary = get_random_canary();\n#endif\n\tif (orig->cpus_ptr == &orig->cpus_mask)\n\t\ttsk->cpus_ptr = &tsk->cpus_mask;\n\n\t/*\n\t * One for the user space visible state that goes away when reaped.\n\t * One for the scheduler.\n\t */\n\trefcount_set(&tsk->rcu_users, 2);\n\t/* One for the rcu users */\n\trefcount_set(&tsk->usage, 1);\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\ttsk->btrace_seq = 0;\n#endif\n\ttsk->splice_pipe = NULL;\n\ttsk->task_frag.page = NULL;\n\ttsk->wake_q.next = NULL;\n\n\taccount_kernel_stack(tsk, 1);\n\n\tkcov_task_init(tsk);\n\n#ifdef CONFIG_FAULT_INJECTION\n\ttsk->fail_nth = 0;\n#endif\n\n#ifdef CONFIG_BLK_CGROUP\n\ttsk->throttle_queue = NULL;\n\ttsk->use_memdelay = 0;\n#endif\n\n#ifdef CONFIG_MEMCG\n\ttsk->active_memcg = NULL;\n#endif\n\treturn tsk;\n\nfree_stack:\n\tfree_thread_stack(tsk);\nfree_tsk:\n\tfree_task_struct(tsk);\n\treturn NULL;\n}",
        "func": "static struct task_struct *dup_task_struct(struct task_struct *orig, int node)\n{\n\tstruct task_struct *tsk;\n\tunsigned long *stack;\n\tstruct vm_struct *stack_vm_area __maybe_unused;\n\tint err;\n\n\tif (node == NUMA_NO_NODE)\n\t\tnode = tsk_fork_get_node(orig);\n\ttsk = alloc_task_struct_node(node);\n\tif (!tsk)\n\t\treturn NULL;\n\n\tstack = alloc_thread_stack_node(tsk, node);\n\tif (!stack)\n\t\tgoto free_tsk;\n\n\tif (memcg_charge_kernel_stack(tsk))\n\t\tgoto free_stack;\n\n\tstack_vm_area = task_stack_vm_area(tsk);\n\n\terr = arch_dup_task_struct(tsk, orig);\n\n\t/*\n\t * arch_dup_task_struct() clobbers the stack-related fields.  Make\n\t * sure they're properly initialized before using any stack-related\n\t * functions again.\n\t */\n\ttsk->stack = stack;\n#ifdef CONFIG_VMAP_STACK\n\ttsk->stack_vm_area = stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\trefcount_set(&tsk->stack_refcount, 1);\n#endif\n\n\tif (err)\n\t\tgoto free_stack;\n\n\terr = scs_prepare(tsk, node);\n\tif (err)\n\t\tgoto free_stack;\n\n#ifdef CONFIG_SECCOMP\n\t/*\n\t * We must handle setting up seccomp filters once we're under\n\t * the sighand lock in case orig has changed between now and\n\t * then. Until then, filter must be NULL to avoid messing up\n\t * the usage counts on the error path calling free_task.\n\t */\n\ttsk->seccomp.filter = NULL;\n#endif\n\n\tsetup_thread_stack(tsk, orig);\n\tclear_user_return_notifier(tsk);\n\tclear_tsk_need_resched(tsk);\n\tset_task_stack_end_magic(tsk);\n\n#ifdef CONFIG_STACKPROTECTOR\n\ttsk->stack_canary = get_random_canary();\n#endif\n\tif (orig->cpus_ptr == &orig->cpus_mask)\n\t\ttsk->cpus_ptr = &tsk->cpus_mask;\n\n\t/*\n\t * One for the user space visible state that goes away when reaped.\n\t * One for the scheduler.\n\t */\n\trefcount_set(&tsk->rcu_users, 2);\n\t/* One for the rcu users */\n\trefcount_set(&tsk->usage, 1);\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\ttsk->btrace_seq = 0;\n#endif\n\ttsk->splice_pipe = NULL;\n\ttsk->task_frag.page = NULL;\n\ttsk->wake_q.next = NULL;\n\ttsk->pf_io_worker = NULL;\n\n\taccount_kernel_stack(tsk, 1);\n\n\tkcov_task_init(tsk);\n\n#ifdef CONFIG_FAULT_INJECTION\n\ttsk->fail_nth = 0;\n#endif\n\n#ifdef CONFIG_BLK_CGROUP\n\ttsk->throttle_queue = NULL;\n\ttsk->use_memdelay = 0;\n#endif\n\n#ifdef CONFIG_MEMCG\n\ttsk->active_memcg = NULL;\n#endif\n\treturn tsk;\n\nfree_stack:\n\tfree_thread_stack(tsk);\nfree_tsk:\n\tfree_task_struct(tsk);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -76,6 +76,7 @@\n \ttsk->splice_pipe = NULL;\n \ttsk->task_frag.page = NULL;\n \ttsk->wake_q.next = NULL;\n+\ttsk->pf_io_worker = NULL;\n \n \taccount_kernel_stack(tsk, 1);\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\ttsk->pf_io_worker = NULL;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_files_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_files_cancel(struct files_struct *files)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_files_cancel(files);\n}",
        "func": "static inline void io_uring_files_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n-static inline void io_uring_files_cancel(struct files_struct *files)\n+static inline void io_uring_files_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_files_cancel(files);\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(false);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static inline void io_uring_files_cancel(struct files_struct *files)",
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_files_cancel(files);"
            ],
            "added_lines": [
                "static inline void io_uring_files_cancel(void)",
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_task_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_task_cancel();\n}",
        "func": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(true);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static inline void io_uring_task_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_task_cancel();\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(true);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_task_cancel();"
            ],
            "added_lines": [
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(true);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_files_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_files_cancel(struct files_struct *files)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_files_cancel(files);\n}",
        "func": "static inline void io_uring_files_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(false);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n-static inline void io_uring_files_cancel(struct files_struct *files)\n+static inline void io_uring_files_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_files_cancel(files);\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(false);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static inline void io_uring_files_cancel(struct files_struct *files)",
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_files_cancel(files);"
            ],
            "added_lines": [
                "static inline void io_uring_files_cancel(void)",
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0240",
        "func_name": "torvalds/linux/io_uring_task_cancel",
        "description": "There is a logic error in io_uring's implementation which can be used to trigger a use-after-free vulnerability leading to privilege escalation.\n\nIn the io_prep_async_work function the assumption that the last io_grab_identity call cannot return false is not true, and in this case the function will use the init_cred or the previous linked requests identity to do operations instead of using the current identity. This can lead to reference counting issues causing use-after-free. We recommend upgrading past version 5.10.161.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/io_uring?h=linux-5.10.y&h=788d0824269bef539fe31a785b1517882eafed93",
        "commit_title": "No upstream commit exists.",
        "commit_text": " This imports the io_uring codebase from 5.15.85, wholesale. Changes from that code base:  - Drop IOCB_ALLOC_CACHE, we don't have that in 5.10. - Drop MKDIRAT/SYMLINKAT/LINKAT. Would require further VFS backports,   and we don't support these in 5.10 to begin with. - sock_from_file() old style calling convention. - Use compat_get_bitmap() only for CONFIG_COMPAT=y  ",
        "func_before": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n\t\t__io_uring_task_cancel();\n}",
        "func": "static inline void io_uring_task_cancel(void)\n{\n\tif (current->io_uring)\n\t\t__io_uring_cancel(true);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,5 @@\n static inline void io_uring_task_cancel(void)\n {\n-\tif (current->io_uring && !xa_empty(&current->io_uring->xa))\n-\t\t__io_uring_task_cancel();\n+\tif (current->io_uring)\n+\t\t__io_uring_cancel(true);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->io_uring && !xa_empty(&current->io_uring->xa))",
                "\t\t__io_uring_task_cancel();"
            ],
            "added_lines": [
                "\tif (current->io_uring)",
                "\t\t__io_uring_cancel(true);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/map_delete_elem",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int map_delete_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\tvoid *key;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tif (bpf_map_is_dev_bound(map)) {\n\t\terr = bpf_map_offload_delete_elem(map, key);\n\t\tgoto out;\n\t} else if (IS_FD_PROG_ARRAY(map) ||\n\t\t   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\t/* These maps require sleepable context */\n\t\terr = map->ops->map_delete_elem(map, key);\n\t\tgoto out;\n\t}\n\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\n\terr = map->ops->map_delete_elem(map, key);\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tmaybe_wait_bpf_programs(map);\nout:\n\tkvfree(key);\nerr_put:\n\tfdput(f);\n\treturn err;\n}",
        "func": "static int map_delete_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\tvoid *key;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tif (bpf_map_is_dev_bound(map)) {\n\t\terr = bpf_map_offload_delete_elem(map, key);\n\t\tgoto out;\n\t} else if (IS_FD_PROG_ARRAY(map) ||\n\t\t   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\t/* These maps require sleepable context */\n\t\terr = map->ops->map_delete_elem(map, key);\n\t\tgoto out;\n\t}\n\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\n\terr = map->ops->map_delete_elem(map, key);\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tmaybe_wait_bpf_programs(map);\nout:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,6 +14,7 @@\n \tmap = __bpf_map_get(f);\n \tif (IS_ERR(map))\n \t\treturn PTR_ERR(map);\n+\tbpf_map_write_active_inc(map);\n \tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n \t\terr = -EPERM;\n \t\tgoto err_put;\n@@ -44,6 +45,7 @@\n out:\n \tkvfree(key);\n err_put:\n+\tbpf_map_write_active_dec(map);\n \tfdput(f);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tbpf_map_write_active_inc(map);",
                "\tbpf_map_write_active_dec(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/bpf_map_mmap_open",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static void bpf_map_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE) {\n\t\tmutex_lock(&map->freeze_mutex);\n\t\tmap->writecnt++;\n\t\tmutex_unlock(&map->freeze_mutex);\n\t}\n}",
        "func": "static void bpf_map_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_inc(map);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,9 +2,6 @@\n {\n \tstruct bpf_map *map = vma->vm_file->private_data;\n \n-\tif (vma->vm_flags & VM_MAYWRITE) {\n-\t\tmutex_lock(&map->freeze_mutex);\n-\t\tmap->writecnt++;\n-\t\tmutex_unlock(&map->freeze_mutex);\n-\t}\n+\tif (vma->vm_flags & VM_MAYWRITE)\n+\t\tbpf_map_write_active_inc(map);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (vma->vm_flags & VM_MAYWRITE) {",
                "\t\tmutex_lock(&map->freeze_mutex);",
                "\t\tmap->writecnt++;",
                "\t\tmutex_unlock(&map->freeze_mutex);",
                "\t}"
            ],
            "added_lines": [
                "\tif (vma->vm_flags & VM_MAYWRITE)",
                "\t\tbpf_map_write_active_inc(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/bpf_map_mmap_close",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static void bpf_map_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE) {\n\t\tmutex_lock(&map->freeze_mutex);\n\t\tmap->writecnt--;\n\t\tmutex_unlock(&map->freeze_mutex);\n\t}\n}",
        "func": "static void bpf_map_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_dec(map);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,9 +2,6 @@\n {\n \tstruct bpf_map *map = vma->vm_file->private_data;\n \n-\tif (vma->vm_flags & VM_MAYWRITE) {\n-\t\tmutex_lock(&map->freeze_mutex);\n-\t\tmap->writecnt--;\n-\t\tmutex_unlock(&map->freeze_mutex);\n-\t}\n+\tif (vma->vm_flags & VM_MAYWRITE)\n+\t\tbpf_map_write_active_dec(map);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (vma->vm_flags & VM_MAYWRITE) {",
                "\t\tmutex_lock(&map->freeze_mutex);",
                "\t\tmap->writecnt--;",
                "\t\tmutex_unlock(&map->freeze_mutex);",
                "\t}"
            ],
            "added_lines": [
                "\tif (vma->vm_flags & VM_MAYWRITE)",
                "\t\tbpf_map_write_active_dec(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/bpf_map_mmap",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = filp->private_data;\n\tint err;\n\n\tif (!map->ops->map_mmap || map_value_has_spin_lock(map) ||\n\t    map_value_has_timer(map))\n\t\treturn -ENOTSUPP;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&map->freeze_mutex);\n\n\tif (vma->vm_flags & VM_WRITE) {\n\t\tif (map->frozen) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* map is meant to be read-only, so do not allow mapping as\n\t\t * writable, because it's possible to leak a writable page\n\t\t * reference and allows user-space to still modify it after\n\t\t * freezing, while verifier will assume contents do not change\n\t\t */\n\t\tif (map->map_flags & BPF_F_RDONLY_PROG) {\n\t\t\terr = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* set default open/close callbacks */\n\tvma->vm_ops = &bpf_map_default_vmops;\n\tvma->vm_private_data = map;\n\tvma->vm_flags &= ~VM_MAYEXEC;\n\tif (!(vma->vm_flags & VM_WRITE))\n\t\t/* disallow re-mapping with PROT_WRITE */\n\t\tvma->vm_flags &= ~VM_MAYWRITE;\n\n\terr = map->ops->map_mmap(map, vma);\n\tif (err)\n\t\tgoto out;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tmap->writecnt++;\nout:\n\tmutex_unlock(&map->freeze_mutex);\n\treturn err;\n}",
        "func": "static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = filp->private_data;\n\tint err;\n\n\tif (!map->ops->map_mmap || map_value_has_spin_lock(map) ||\n\t    map_value_has_timer(map))\n\t\treturn -ENOTSUPP;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&map->freeze_mutex);\n\n\tif (vma->vm_flags & VM_WRITE) {\n\t\tif (map->frozen) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\t/* map is meant to be read-only, so do not allow mapping as\n\t\t * writable, because it's possible to leak a writable page\n\t\t * reference and allows user-space to still modify it after\n\t\t * freezing, while verifier will assume contents do not change\n\t\t */\n\t\tif (map->map_flags & BPF_F_RDONLY_PROG) {\n\t\t\terr = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* set default open/close callbacks */\n\tvma->vm_ops = &bpf_map_default_vmops;\n\tvma->vm_private_data = map;\n\tvma->vm_flags &= ~VM_MAYEXEC;\n\tif (!(vma->vm_flags & VM_WRITE))\n\t\t/* disallow re-mapping with PROT_WRITE */\n\t\tvma->vm_flags &= ~VM_MAYWRITE;\n\n\terr = map->ops->map_mmap(map, vma);\n\tif (err)\n\t\tgoto out;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_inc(map);\nout:\n\tmutex_unlock(&map->freeze_mutex);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -41,7 +41,7 @@\n \t\tgoto out;\n \n \tif (vma->vm_flags & VM_MAYWRITE)\n-\t\tmap->writecnt++;\n+\t\tbpf_map_write_active_inc(map);\n out:\n \tmutex_unlock(&map->freeze_mutex);\n \treturn err;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tmap->writecnt++;"
            ],
            "added_lines": [
                "\t\tbpf_map_write_active_inc(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/map_update_elem",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int map_update_elem(union bpf_attr *attr, bpfptr_t uattr)\n{\n\tbpfptr_t ukey = make_bpfptr(attr->key, uattr.is_kernel);\n\tbpfptr_t uvalue = make_bpfptr(attr->value, uattr.is_kernel);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !map_value_has_spin_lock(map)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = ___bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\terr = -EFAULT;\n\tif (copy_from_bpfptr(value, uvalue, value_size) != 0)\n\t\tgoto free_value;\n\n\terr = bpf_map_update_value(map, f, key, value, attr->flags);\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tfdput(f);\n\treturn err;\n}",
        "func": "static int map_update_elem(union bpf_attr *attr, bpfptr_t uattr)\n{\n\tbpfptr_t ukey = make_bpfptr(attr->key, uattr.is_kernel);\n\tbpfptr_t uvalue = make_bpfptr(attr->value, uattr.is_kernel);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !map_value_has_spin_lock(map)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = ___bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\terr = -EFAULT;\n\tif (copy_from_bpfptr(value, uvalue, value_size) != 0)\n\t\tgoto free_value;\n\n\terr = bpf_map_update_value(map, f, key, value, attr->flags);\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,6 +16,7 @@\n \tmap = __bpf_map_get(f);\n \tif (IS_ERR(map))\n \t\treturn PTR_ERR(map);\n+\tbpf_map_write_active_inc(map);\n \tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n \t\terr = -EPERM;\n \t\tgoto err_put;\n@@ -51,6 +52,7 @@\n free_key:\n \tkvfree(key);\n err_put:\n+\tbpf_map_write_active_dec(map);\n \tfdput(f);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tbpf_map_write_active_inc(map);",
                "\tbpf_map_write_active_dec(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/bpf_map_do_batch",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int bpf_map_do_batch(const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr,\n\t\t\t    int cmd)\n{\n\tstruct bpf_map *map;\n\tint err, ufd;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_BATCH))\n\t\treturn -EINVAL;\n\n\tufd = attr->batch.map_fd;\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif ((cmd == BPF_MAP_LOOKUP_BATCH ||\n\t     cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH) &&\n\t    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (cmd != BPF_MAP_LOOKUP_BATCH &&\n\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (cmd == BPF_MAP_LOOKUP_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_batch);\n\telse if (cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_and_delete_batch);\n\telse if (cmd == BPF_MAP_UPDATE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_update_batch);\n\telse\n\t\tBPF_DO_BATCH(map->ops->map_delete_batch);\n\nerr_put:\n\tfdput(f);\n\treturn err;\n}",
        "func": "static int bpf_map_do_batch(const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr,\n\t\t\t    int cmd)\n{\n\tbool has_read  = cmd == BPF_MAP_LOOKUP_BATCH ||\n\t\t\t cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH;\n\tbool has_write = cmd != BPF_MAP_LOOKUP_BATCH;\n\tstruct bpf_map *map;\n\tint err, ufd;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_BATCH))\n\t\treturn -EINVAL;\n\n\tufd = attr->batch.map_fd;\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (has_write)\n\t\tbpf_map_write_active_inc(map);\n\tif (has_read && !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\tif (has_write && !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (cmd == BPF_MAP_LOOKUP_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_batch);\n\telse if (cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_and_delete_batch);\n\telse if (cmd == BPF_MAP_UPDATE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_update_batch);\n\telse\n\t\tBPF_DO_BATCH(map->ops->map_delete_batch);\nerr_put:\n\tif (has_write)\n\t\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,9 @@\n \t\t\t    union bpf_attr __user *uattr,\n \t\t\t    int cmd)\n {\n+\tbool has_read  = cmd == BPF_MAP_LOOKUP_BATCH ||\n+\t\t\t cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH;\n+\tbool has_write = cmd != BPF_MAP_LOOKUP_BATCH;\n \tstruct bpf_map *map;\n \tint err, ufd;\n \tstruct fd f;\n@@ -14,16 +17,13 @@\n \tmap = __bpf_map_get(f);\n \tif (IS_ERR(map))\n \t\treturn PTR_ERR(map);\n-\n-\tif ((cmd == BPF_MAP_LOOKUP_BATCH ||\n-\t     cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH) &&\n-\t    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n+\tif (has_write)\n+\t\tbpf_map_write_active_inc(map);\n+\tif (has_read && !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n \t\terr = -EPERM;\n \t\tgoto err_put;\n \t}\n-\n-\tif (cmd != BPF_MAP_LOOKUP_BATCH &&\n-\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n+\tif (has_write && !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n \t\terr = -EPERM;\n \t\tgoto err_put;\n \t}\n@@ -36,8 +36,9 @@\n \t\tBPF_DO_BATCH(map->ops->map_update_batch);\n \telse\n \t\tBPF_DO_BATCH(map->ops->map_delete_batch);\n-\n err_put:\n+\tif (has_write)\n+\t\tbpf_map_write_active_dec(map);\n \tfdput(f);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif ((cmd == BPF_MAP_LOOKUP_BATCH ||",
                "\t     cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH) &&",
                "\t    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {",
                "",
                "\tif (cmd != BPF_MAP_LOOKUP_BATCH &&",
                "\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {",
                ""
            ],
            "added_lines": [
                "\tbool has_read  = cmd == BPF_MAP_LOOKUP_BATCH ||",
                "\t\t\t cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH;",
                "\tbool has_write = cmd != BPF_MAP_LOOKUP_BATCH;",
                "\tif (has_write)",
                "\t\tbpf_map_write_active_inc(map);",
                "\tif (has_read && !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {",
                "\tif (has_write && !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {",
                "\tif (has_write)",
                "\t\tbpf_map_write_active_dec(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/map_lookup_and_delete_elem",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int map_lookup_and_delete_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tvoid __user *uvalue = u64_to_user_ptr(attr->value);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_LOOKUP_AND_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tif (attr->flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ) ||\n\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (attr->flags &&\n\t    (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t     map->map_type == BPF_MAP_TYPE_STACK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !map_value_has_spin_lock(map)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\terr = -ENOTSUPP;\n\tif (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t    map->map_type == BPF_MAP_TYPE_STACK) {\n\t\terr = map->ops->map_pop_elem(map, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\tif (!bpf_map_is_dev_bound(map)) {\n\t\t\tbpf_disable_instrumentation();\n\t\t\trcu_read_lock();\n\t\t\terr = map->ops->map_lookup_and_delete_elem(map, key, value, attr->flags);\n\t\t\trcu_read_unlock();\n\t\t\tbpf_enable_instrumentation();\n\t\t}\n\t}\n\n\tif (err)\n\t\tgoto free_value;\n\n\tif (copy_to_user(uvalue, value, value_size) != 0) {\n\t\terr = -EFAULT;\n\t\tgoto free_value;\n\t}\n\n\terr = 0;\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tfdput(f);\n\treturn err;\n}",
        "func": "static int map_lookup_and_delete_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tvoid __user *uvalue = u64_to_user_ptr(attr->value);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_LOOKUP_AND_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tif (attr->flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ) ||\n\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (attr->flags &&\n\t    (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t     map->map_type == BPF_MAP_TYPE_STACK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !map_value_has_spin_lock(map)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\terr = -ENOTSUPP;\n\tif (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t    map->map_type == BPF_MAP_TYPE_STACK) {\n\t\terr = map->ops->map_pop_elem(map, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\tif (!bpf_map_is_dev_bound(map)) {\n\t\t\tbpf_disable_instrumentation();\n\t\t\trcu_read_lock();\n\t\t\terr = map->ops->map_lookup_and_delete_elem(map, key, value, attr->flags);\n\t\t\trcu_read_unlock();\n\t\t\tbpf_enable_instrumentation();\n\t\t}\n\t}\n\n\tif (err)\n\t\tgoto free_value;\n\n\tif (copy_to_user(uvalue, value, value_size) != 0) {\n\t\terr = -EFAULT;\n\t\tgoto free_value;\n\t}\n\n\terr = 0;\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,6 +19,7 @@\n \tmap = __bpf_map_get(f);\n \tif (IS_ERR(map))\n \t\treturn PTR_ERR(map);\n+\tbpf_map_write_active_inc(map);\n \tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ) ||\n \t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n \t\terr = -EPERM;\n@@ -83,6 +84,7 @@\n free_key:\n \tkvfree(key);\n err_put:\n+\tbpf_map_write_active_dec(map);\n \tfdput(f);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tbpf_map_write_active_inc(map);",
                "\tbpf_map_write_active_dec(map);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/map_freeze",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static int map_freeze(const union bpf_attr *attr)\n{\n\tint err = 0, ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_FREEZE))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS ||\n\t    map_value_has_timer(map)) {\n\t\tfdput(f);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tmutex_lock(&map->freeze_mutex);\n\n\tif (map->writecnt) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\tif (READ_ONCE(map->frozen)) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\tif (!bpf_capable()) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tWRITE_ONCE(map->frozen, true);\nerr_put:\n\tmutex_unlock(&map->freeze_mutex);\n\tfdput(f);\n\treturn err;\n}",
        "func": "static int map_freeze(const union bpf_attr *attr)\n{\n\tint err = 0, ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_FREEZE))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS ||\n\t    map_value_has_timer(map)) {\n\t\tfdput(f);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tmutex_lock(&map->freeze_mutex);\n\tif (bpf_map_write_active(map)) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\tif (READ_ONCE(map->frozen)) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\tif (!bpf_capable()) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tWRITE_ONCE(map->frozen, true);\nerr_put:\n\tmutex_unlock(&map->freeze_mutex);\n\tfdput(f);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,8 +19,7 @@\n \t}\n \n \tmutex_lock(&map->freeze_mutex);\n-\n-\tif (map->writecnt) {\n+\tif (bpf_map_write_active(map)) {\n \t\terr = -EBUSY;\n \t\tgoto err_put;\n \t}",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "\tif (map->writecnt) {"
            ],
            "added_lines": [
                "\tif (bpf_map_write_active(map)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-4001",
        "func_name": "kernel/git/bpf/bpf/bpf_map_is_rdonly",
        "description": "A race condition was found in the Linux kernel's ebpf verifier between bpf_map_update_elem and bpf_map_freeze due to a missing lock in kernel/bpf/syscall.c. In this flaw, a local user with a special privilege (cap_sys_admin or cap_bpf) can modify the frozen mapped address space. This flaw affects kernel versions prior to 5.16 rc2.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf.git/commit/?h=353050be4c19e102178ccc05988101887c25ae53",
        "commit_title": "Commit a23740ec43ba (\"bpf: Track contents of read-only maps as scalars\") is",
        "commit_text": "checking whether maps are read-only both from BPF program side and user space side, and then, given their content is constant, reading out their data via map->ops->map_direct_value_addr() which is then subsequently used as known scalar value for the register, that is, it is marked as __mark_reg_known() with the read value at verification time. Before a23740ec43ba, the register content was marked as an unknown scalar so the verifier could not make any assumptions about the map content.  The current implementation however is prone to a TOCTOU race, meaning, the value read as known scalar for the register is not guaranteed to be exactly the same at a later point when the program is executed, and as such, the prior made assumptions of the verifier with regards to the program will be invalid which can cause issues such as OOB access, etc.  While the BPF_F_RDONLY_PROG map flag is always fixed and required to be specified at map creation time, the map->frozen property is initially set to false for the map given the map value needs to be populated, e.g. for global data sections. Once complete, the loader \"freezes\" the map from user space such that no subsequent updates/deletes are possible anymore. For the rest of the lifetime of the map, this freeze one-time trigger cannot be undone anymore after a successful BPF_MAP_FREEZE cmd return. Meaning, any new BPF_* cmd calls which would update/delete map entries will be rejected with -EPERM since map_get_sys_perms() removes the FMODE_CAN_WRITE permission. This also means that pending update/delete map entries must still complete before this guarantee is given. This corner case is not an issue for loaders since they create and prepare such program private map in successive steps.  However, a malicious user is able to trigger this TOCTOU race in two different ways: i) via userfaultfd, and ii) via batched updates. For i) userfaultfd is used to expand the competition interval, so that map_update_elem() can modify the contents of the map after map_freeze() and bpf_prog_load() were executed. This works, because userfaultfd halts the parallel thread which triggered a map_update_elem() at the time where we copy key/value from the user buffer and this already passed the FMODE_CAN_WRITE capability test given at that time the map was not \"frozen\". Then, the main thread performs the map_freeze() and bpf_prog_load(), and once that had completed successfully, the other thread is woken up to complete the pending map_update_elem() which then changes the map content. For ii) the idea of the batched update is similar, meaning, when there are a large number of updates to be processed, it can increase the competition interval between the two. It is therefore possible in practice to modify the contents of the map after executing map_freeze() and bpf_prog_load().  One way to fix both i) and ii) at the same time is to expand the use of the map's map->writecnt. The latter was introduced in fc9702273e2e (\"bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY\") and further refined in 1f6cb19be2e2 (\"bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping\") with the rationale to make a writable mmap()'ing of a map mutually exclusive with read-only freezing. The counter indicates writable mmap() mappings and then prevents/fails the freeze operation. Its semantics can be expanded beyond just mmap() by generally indicating ongoing write phases. This would essentially span any parallel regular and batched flavor of update/delete operation and then also have map_freeze() fail with -EBUSY. For the check_mem_access() in the verifier we expand upon the bpf_map_is_rdonly() check ensuring that all last pending writes have completed via bpf_map_write_active() test. Once the map->frozen is set and bpf_map_write_active() indicates a map->writecnt of 0 only then we are really guaranteed to use the map's data as known constants. For map->frozen being set and pending writes in process of still being completed we fall back to marking that register as unknown scalar so we don't end up making assumptions about it. With this, both TOCTOU reproducers from i) and ii) are fixed.  Note that the map->writecnt has been converted into a atomic64 in the fix in order to avoid a double freeze_mutex mutex_{un,}lock() pair when updating map->writecnt in the various map update/delete BPF_* cmd flavors. Spanning the freeze_mutex over entire map update/delete operations in syscall side would not be possible due to then causing everything to be serialized. Similarly, something like synchronize_rcu() after setting map->frozen to wait for update/deletes to complete is not possible either since it would also have to span the user copy which can sleep. On the libbpf side, this won't break d66562fba1ce (\"libbpf: Add BPF object skeleton support\") as the anonymous mmap()-ed \"map initialization image\" is remapped as a BPF map-backed mmap()-ed memory where for .rodata it's non-writable.  ",
        "func_before": "static bool bpf_map_is_rdonly(const struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_RDONLY_PROG) && map->frozen;\n}",
        "func": "static bool bpf_map_is_rdonly(const struct bpf_map *map)\n{\n\t/* A map is considered read-only if the following condition are true:\n\t *\n\t * 1) BPF program side cannot change any of the map content. The\n\t *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map\n\t *    and was set at map creation time.\n\t * 2) The map value(s) have been initialized from user space by a\n\t *    loader and then \"frozen\", such that no new map update/delete\n\t *    operations from syscall side are possible for the rest of\n\t *    the map's lifetime from that point onwards.\n\t * 3) Any parallel/pending map update/delete operations from syscall\n\t *    side have been completed. Only after that point, it's safe to\n\t *    assume that map value(s) are immutable.\n\t */\n\treturn (map->map_flags & BPF_F_RDONLY_PROG) &&\n\t       READ_ONCE(map->frozen) &&\n\t       !bpf_map_write_active(map);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,19 @@\n static bool bpf_map_is_rdonly(const struct bpf_map *map)\n {\n-\treturn (map->map_flags & BPF_F_RDONLY_PROG) && map->frozen;\n+\t/* A map is considered read-only if the following condition are true:\n+\t *\n+\t * 1) BPF program side cannot change any of the map content. The\n+\t *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map\n+\t *    and was set at map creation time.\n+\t * 2) The map value(s) have been initialized from user space by a\n+\t *    loader and then \"frozen\", such that no new map update/delete\n+\t *    operations from syscall side are possible for the rest of\n+\t *    the map's lifetime from that point onwards.\n+\t * 3) Any parallel/pending map update/delete operations from syscall\n+\t *    side have been completed. Only after that point, it's safe to\n+\t *    assume that map value(s) are immutable.\n+\t */\n+\treturn (map->map_flags & BPF_F_RDONLY_PROG) &&\n+\t       READ_ONCE(map->frozen) &&\n+\t       !bpf_map_write_active(map);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn (map->map_flags & BPF_F_RDONLY_PROG) && map->frozen;"
            ],
            "added_lines": [
                "\t/* A map is considered read-only if the following condition are true:",
                "\t *",
                "\t * 1) BPF program side cannot change any of the map content. The",
                "\t *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map",
                "\t *    and was set at map creation time.",
                "\t * 2) The map value(s) have been initialized from user space by a",
                "\t *    loader and then \"frozen\", such that no new map update/delete",
                "\t *    operations from syscall side are possible for the rest of",
                "\t *    the map's lifetime from that point onwards.",
                "\t * 3) Any parallel/pending map update/delete operations from syscall",
                "\t *    side have been completed. Only after that point, it's safe to",
                "\t *    assume that map value(s) are immutable.",
                "\t */",
                "\treturn (map->map_flags & BPF_F_RDONLY_PROG) &&",
                "\t       READ_ONCE(map->frozen) &&",
                "\t       !bpf_map_write_active(map);"
            ]
        }
    }
]