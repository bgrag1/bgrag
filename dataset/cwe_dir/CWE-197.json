[
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                    &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,\n                                                           int limit_batch) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      // Skip takes units of 128 bytes.  +3 is so rounding doesn't lead to\n      // us using the same state in different batches.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip(start_batch * 2 * kMaxIterations * (samples_per_batch + 3) /\n                    4);\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n\n      for (int64 b = start_batch; b < limit_batch; ++b) {\n        // We are passed a flat array for each of the parameter tensors.\n        // The input is either a scalar broadcasted to all batches or a vector\n        // with length num_batches, but the scalar becomes an array of length 1.\n        T mean = means((means.dimension(0) == 1) ? 0 : b);\n        T stddev = stddevs((stddevs.dimension(0) == 1) ? 0 : b);\n        T minval = minvals((minvals.dimension(0) == 1) ? 0 : b);\n        T maxval = maxvals((maxvals.dimension(0) == 1) ? 0 : b);\n\n        // The last batch can be short, if we adjusted num_batches and\n        // samples_per_batch.\n        const int64 limit_sample =\n            std::min((b + 1) * samples_per_batch, num_elements);\n        int64 sample = b * samples_per_batch;\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n\n          while (sample < limit_sample) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n\n            for (int i = 0; i < size; i++) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output(sample) = randn_sample[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          while (sample < limit_sample) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n            }\n            for (int i = 0; i < size; i++) {\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output(sample) = z[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          while (sample < limit_sample) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output(sample) = z * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost =\n        batchInitCost + uniformRejectionSamplingCost * 2 * samples_per_batch;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_batches,\n          batchCost, do_work);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                    &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,\n                                                           int64 limit_batch) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      // Skip takes units of 128 bytes.  +3 is so rounding doesn't lead to\n      // us using the same state in different batches.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip(start_batch * 2 * kMaxIterations * (samples_per_batch + 3) /\n                    4);\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n\n      for (int64 b = start_batch; b < limit_batch; ++b) {\n        // We are passed a flat array for each of the parameter tensors.\n        // The input is either a scalar broadcasted to all batches or a vector\n        // with length num_batches, but the scalar becomes an array of length 1.\n        T mean = means((means.dimension(0) == 1) ? 0 : b);\n        T stddev = stddevs((stddevs.dimension(0) == 1) ? 0 : b);\n        T minval = minvals((minvals.dimension(0) == 1) ? 0 : b);\n        T maxval = maxvals((maxvals.dimension(0) == 1) ? 0 : b);\n\n        // The last batch can be short, if we adjusted num_batches and\n        // samples_per_batch.\n        const int64 limit_sample =\n            std::min((b + 1) * samples_per_batch, num_elements);\n        int64 sample = b * samples_per_batch;\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n\n          while (sample < limit_sample) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n\n            for (int i = 0; i < size; i++) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output(sample) = randn_sample[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          while (sample < limit_sample) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n            }\n            for (int i = 0; i < size; i++) {\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output(sample) = z[i] * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          while (sample < limit_sample) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output(sample) = z * stddev + mean;\n                sample++;\n                if (sample >= limit_sample) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost =\n        batchInitCost + uniformRejectionSamplingCost * 2 * samples_per_batch;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_batches,\n          batchCost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,8 +18,8 @@\n \n     auto do_work = [samples_per_batch, num_elements, &ctx, &means, &stddevs,\n                     &minvals, &maxvals, &gen, &output,\n-                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,\n-                                                           int limit_batch) {\n+                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,\n+                                                           int64 limit_batch) {\n       // Capturing \"gen\" by-value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"gen\" by reference and explicitly do a copy assignment here.",
        "diff_line_info": {
            "deleted_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int start_batch,",
                "                                                           int limit_batch) {"
            ],
            "added_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_batch,",
                "                                                           int64 limit_batch) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCastList<4>& bcast,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                    &stddevs, &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,\n                                                           int limit_output) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n      // Skip takes units of 128 bits. The Uniform::kResultElementCount - 1\n      // is so rounding doesn't lead to\n      // us using the same state in different workloads.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip((start_output * 2 * kMaxIterations +\n                     Uniform::kResultElementCount - 1) /\n                    Uniform::kResultElementCount);\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, Uniform::kResultElementCount> z;\n      Eigen::array<T, Uniform::kResultElementCount> g;\n\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& means_batch_indices = bcast.batch_indices(0);\n      const auto& stddevs_batch_indices = bcast.batch_indices(1);\n      const auto& minvals_batch_indices = bcast.batch_indices(2);\n      const auto& maxvals_batch_indices = bcast.batch_indices(3);\n      auto output_flat = output.data();\n\n      // We partition work across batches and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        // The output layout is [samples_per_batch, num_batches]. Thus\n        // the output address is sample_idx * num_batches + batch_idx.\n        // Below, code will index at output_batch_offset[sample_idx *\n        // num_batches] matching this.\n        T* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T mean, stddev, minval, maxval;\n        if (should_bcast) {\n          mean = means(means_batch_indices[batch_idx]);\n          stddev = stddevs(stddevs_batch_indices[batch_idx]);\n          minval = minvals(minvals_batch_indices[batch_idx]);\n          maxval = maxvals(maxvals_batch_indices[batch_idx]);\n        } else {\n          mean = means(batch_idx);\n          stddev = stddevs(batch_idx);\n          minval = minvals(batch_idx);\n          maxval = maxvals(batch_idx);\n        }\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n            for (int i = 0; i < size; ++i) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output_batch_offset[sample_idx * num_batches] =\n                    randn_sample[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                ++num_iterations;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost = batchInitCost + uniformRejectionSamplingCost * 2;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          batchCost, do_work);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCastList<4>& bcast,\n                  typename TTypes<T>::ConstFlat means,\n                  typename TTypes<T>::ConstFlat stddevs,\n                  typename TTypes<T>::ConstFlat minvals,\n                  typename TTypes<T>::ConstFlat maxvals,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<T>::Flat output) {\n    // The randn rejection sampling is used when the mean and at least this many\n    // standard deviations are inside the bounds.\n    // The uniform proposal samplers become less efficient as the bounds are\n    // further from the mean, the reverse is true for the randn sampler.\n    // This number was chosen by empirical benchmarking. If modified, the\n    // benchmarks in parameterized_truncated_normal_op_test should also be\n    // changed.\n    const T kStdDevsInsideBoundsToUseRandnSampler = T(1.3);\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                    &stddevs, &minvals, &maxvals, &gen, &output,\n                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,\n                                                           int64 limit_output) {\n      // Capturing \"gen\" by-value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"gen\" by reference and explicitly do a copy assignment here.\n      random::PhiloxRandom gen_copy = gen;\n      using Uniform = random::UniformDistribution<random::PhiloxRandom, T>;\n      Uniform dist;\n      using Normal = random::NormalDistribution<random::PhiloxRandom, T>;\n      Normal normal_dist;\n      // Skip takes units of 128 bits. The Uniform::kResultElementCount - 1\n      // is so rounding doesn't lead to\n      // us using the same state in different workloads.\n      // The sample from each iteration uses 2 random numbers.\n      gen_copy.Skip((start_output * 2 * kMaxIterations +\n                     Uniform::kResultElementCount - 1) /\n                    Uniform::kResultElementCount);\n\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, Uniform::kResultElementCount> z;\n      Eigen::array<T, Uniform::kResultElementCount> g;\n\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& means_batch_indices = bcast.batch_indices(0);\n      const auto& stddevs_batch_indices = bcast.batch_indices(1);\n      const auto& minvals_batch_indices = bcast.batch_indices(2);\n      const auto& maxvals_batch_indices = bcast.batch_indices(3);\n      auto output_flat = output.data();\n\n      // We partition work across batches and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        // The output layout is [samples_per_batch, num_batches]. Thus\n        // the output address is sample_idx * num_batches + batch_idx.\n        // Below, code will index at output_batch_offset[sample_idx *\n        // num_batches] matching this.\n        T* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T mean, stddev, minval, maxval;\n        if (should_bcast) {\n          mean = means(means_batch_indices[batch_idx]);\n          stddev = stddevs(stddevs_batch_indices[batch_idx]);\n          minval = minvals(minvals_batch_indices[batch_idx]);\n          maxval = maxvals(maxvals_batch_indices[batch_idx]);\n        } else {\n          mean = means(batch_idx);\n          stddev = stddevs(batch_idx);\n          minval = minvals(batch_idx);\n          maxval = maxvals(batch_idx);\n        }\n\n        // On GPU, this check will just fill samples with NAN if it fails.\n        OP_REQUIRES(ctx,\n                    stddev > T(0) && minval < maxval &&\n                        (Eigen::numext::isfinite(minval) ||\n                         Eigen::numext::isfinite(maxval)),\n                    errors::InvalidArgument(\"Invalid parameters\"));\n\n        int num_iterations = 0;\n\n        // If possible, make one-sided bound be the lower bound, or make both\n        // bounds positive. Otherwise, the bounds are on either side of the\n        // mean.\n        if ((Eigen::numext::isinf(minval) && minval < T(0)) || maxval < mean) {\n          // Reverse all calculations. normMin and normMax will be flipped.\n          std::swap(minval, maxval);\n          stddev = -stddev;\n        }\n\n        // Calculate normalized samples, then convert them.\n        const T normMin = (minval - mean) / stddev;\n        const T normMax = (maxval - mean) / stddev;\n\n        // Determine the method to use.\n        const T sqrtFactor = Eigen::numext::sqrt((normMin * normMin) + T(4));\n        const T cutoff =\n            T(2) *\n            Eigen::numext::exp(T(0.5) +\n                               (normMin * (normMin - sqrtFactor)) / T(4)) /\n            (normMin + sqrtFactor);\n        const T diff = normMax - normMin;\n\n        if (((normMin < -kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMax >= T(0.))) ||\n            ((normMax > kStdDevsInsideBoundsToUseRandnSampler) &&\n             (normMin <= T(0.)))) {\n          // If the bounds are a least 3 standard deviations from the mean\n          // on at least one side then we rejection sample by sampling\n          // from the normal distribution and rejecting samples outside\n          // the bounds.\n          // Under this condition the acceptance rate per iteration should\n          // always be ~ 50%. This sampler is more efficient (and more\n          // numerically stable when one or both bounds is far from the mean).\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto randn_sample = normal_dist(&gen_copy);\n            const int size = randn_sample.size();\n            for (int i = 0; i < size; ++i) {\n              if ((randn_sample[i] >= normMin) &&\n                  (randn_sample[i] <= normMax)) {\n                output_batch_offset[sample_idx * num_batches] =\n                    randn_sample[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                ++num_iterations;\n                if (num_iterations > kMaxIterations) {\n                  // This should never occur because this sampler should\n                  // (by the selection criteria above) be used if at least 3\n                  // standard deviations of one side of the distribution\n                  // is within the limits (so acceptance probability per\n                  // iterations >~ 1/2 per iteration).\n                  LOG(ERROR) << \"TruncatedNormal randn rejection sampler \"\n                             << \"exceeded maximum iterations for \"\n                             << \"normMin=\" << normMin << \" normMax=\" << normMax\n                             << \" kMaxIterations=\" << kMaxIterations;\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal randn rejection sampler failed to accept\"\n                      \" a sample.\"));\n                  return;\n                }\n              }\n            }\n          }\n        } else if (diff < cutoff) {\n          // Sample from a uniform distribution on [normMin, normMax].\n\n          const T plusFactor = (normMin < T(0)) ? T(0) : normMin * normMin;\n\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            const auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            // NOTE(ringwalt): These loops seem to only generate packed AVX\n            // instructions for float32.\n            for (int i = 0; i < size; i++) {\n              z[i] = rand[i] * diff + normMin;\n              g[i] = (plusFactor - z[i] * z[i]) / T(2.0);\n            }\n\n            const auto u = dist(&gen_copy);\n            for (int i = 0; i < size; i++) {\n              auto accept = u[i] <= Eigen::numext::exp(g[i]);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                // Accept the sample z.\n                // If we run out of iterations, just use the current uniform\n                // sample, but emit a warning.\n                // TODO(jjhunt) For small entropies (relative to the bounds),\n                // this sampler is poor and may take many iterations since\n                // the proposal distribution is the uniform distribution\n                // U(lower_bound, upper_bound).\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal uniform rejection sampler \"\n                             << \"exceeded max iterations. Sample may contain \"\n                             << \"outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal uniform rejection sampler failed to \"\n                      \" accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z[i] * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        } else {\n          // Sample from an exponential distribution with alpha maximizing\n          // acceptance probability, offset by normMin from the origin.\n          // Accept only if less than normMax.\n          const T alpha =\n              (normMin + Eigen::numext::sqrt((normMin * normMin) + T(4))) /\n              T(2);\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;) {\n            auto rand = dist(&gen_copy);\n            const int size = rand.size();\n            int i = 0;\n            while (i < size) {\n              const T z = -Eigen::numext::log(rand[i]) / alpha + normMin;\n              i++;\n              const T x = normMin < alpha ? alpha - z : normMin - alpha;\n              const T g = Eigen::numext::exp(-x * x / T(2.0));\n              const T u = rand[i];\n              i++;\n              auto accept = (u <= g && z < normMax);\n              if (accept || num_iterations + 1 >= kMaxIterations) {\n                if (!accept) {\n                  LOG(ERROR) << \"TruncatedNormal exponential distribution \"\n                             << \"rejection sampler exceeds max iterations. \"\n                             << \"Sample may contain outliers.\";\n                  ctx->SetStatus(errors::Internal(\n                      \"TruncatedNormal exponential distribution rejection\"\n                      \" sampler failed to accept a sample.\"));\n                  return;\n                }\n                output_batch_offset[sample_idx * num_batches] =\n                    z * stddev + mean;\n                ++sample_idx;\n                ++output_idx;\n                if (sample_idx >= samples_per_batch ||\n                    output_idx >= limit_output) {\n                  break;\n                }\n                num_iterations = 0;\n              } else {\n                num_iterations++;\n              }\n            }\n          }\n        }\n      }\n    };\n    // The cost of the initial calculations for the batch.\n    const int64 batchInitCost =\n        // normMin, normMax\n        (Eigen::TensorOpCost::AddCost<T>() +\n         Eigen::TensorOpCost::MulCost<T>()) *\n            2\n        // sqrtFactor\n        + Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_sqrt_op<T>>::Cost\n        // cutoff\n        + Eigen::TensorOpCost::MulCost<T>() * 4 +\n        Eigen::internal::functor_traits<Eigen::internal::scalar_exp_op<T>>::Cost\n        // diff\n        + Eigen::TensorOpCost::AddCost<T>();\n    const int64 uniformSampleCost =\n        random::PhiloxRandom::kElementCost +\n        random::UniformDistribution<random::PhiloxRandom, T>::kElementCost;\n    // The cost of a single uniform sampling round.\n    const int64 uniformRejectionSamplingCost =\n        uniformSampleCost + Eigen::TensorOpCost::MulCost<T>() +\n        Eigen::TensorOpCost::AddCost<T>() +\n        Eigen::TensorOpCost::MulCost<T>() * 2 +\n        Eigen::TensorOpCost::AddCost<T>() + uniformSampleCost +\n        Eigen::internal::functor_traits<\n            Eigen::internal::scalar_exp_op<T>>::Cost +\n        Eigen::TensorOpCost::MulCost<T>() + Eigen::TensorOpCost::AddCost<T>();\n    // Estimate the cost for an entire batch.\n    // Assume we use uniform sampling, and accept the 2nd sample on average.\n    const int64 batchCost = batchInitCost + uniformRejectionSamplingCost * 2;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          batchCost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,8 +19,8 @@\n \n     auto do_work = [num_batches, samples_per_batch, &ctx, &bcast, &means,\n                     &stddevs, &minvals, &maxvals, &gen, &output,\n-                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,\n-                                                           int limit_output) {\n+                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,\n+                                                           int64 limit_output) {\n       // Capturing \"gen\" by-value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"gen\" by reference and explicitly do a copy assignment here.",
        "diff_line_info": {
            "deleted_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int start_output,",
                "                                                           int limit_output) {"
            ],
            "added_lines": [
                "                    kStdDevsInsideBoundsToUseRandnSampler](int64 start_output,",
                "                                                           int64 limit_output) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCast& bcast, typename TTypes<T>::ConstFlat counts,\n                  typename TTypes<T>::ConstFlat probs,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<U>::Flat output) {\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    // The output layout is [B1, ... Bk, H1, ... Hm]. We have [B1, ... Bk] for\n    // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n    // We have B1 * ... * Bk samples per batch member we need.\n    auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n                   &gen, &output](int start_output, int limit_output) {\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& counts_batch_indices = bcast.x_batch_indices();\n      const auto& probs_batch_indices = bcast.y_batch_indices();\n      auto output_flat = output.data();\n\n      // We partition work across batches (count, prob) and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        U* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T count, prob;\n        if (should_bcast) {\n          count = counts(counts_batch_indices[batch_idx]);\n          prob = probs(probs_batch_indices[batch_idx]);\n        } else {\n          count = counts(batch_idx);\n          prob = probs(batch_idx);\n        }\n\n        // Calculate normalized samples, then convert them.\n        // Determine the method to use.\n        double dcount = static_cast<double>(count);\n        if (dcount <= 0.0 || prob <= T(0.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(0.0);\n          }\n        } else if (prob >= T(1.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] =\n                static_cast<U>(dcount);\n          }\n        } else if (prob <= T(0.5)) {\n          double dp = static_cast<double>(prob);\n          if (count * prob >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(btrs(dcount, dp, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(binomial_inversion(dcount, dp, &gen_copy));\n            }\n          }\n        } else if (prob > T(0.5)) {\n          T q = T(1) - prob;\n          double dcount = static_cast<double>(count);\n          double dq = static_cast<double>(q);\n          if (count * q >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(dcount - btrs(dcount, dq, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] = static_cast<U>(\n                  dcount - binomial_inversion(dcount, dq, &gen_copy));\n            }\n          }\n        } else {  // prob is NaN\n          // TODO(srvasude): What should happen if prob is NaN but the output\n          // type is an integer (which doesn't have a sentinel for NaN)?  Fail\n          // the whole batch sample?  Return a specialized sentinel like -1?\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(NAN);\n          }\n        }\n      }\n    };\n\n    // This will depend on count * p (or count * q).\n    // For n * p < 10, on average, O(n * p) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~200 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the four calls to log\n    // occur for ~72 percent of samples.\n    // 4 x 100 (64-bit cycles per log) * 0.72 = ~288\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .72  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this (rate >= 10) should be ~329 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 329 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          kElementCost, DoWork);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, int64 num_batches,\n                  int64 samples_per_batch, int64 num_elements,\n                  const BCast& bcast, typename TTypes<T>::ConstFlat counts,\n                  typename TTypes<T>::ConstFlat probs,\n                  const random::PhiloxRandom& gen,\n                  typename TTypes<U>::Flat output) {\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n\n    // The output layout is [B1, ... Bk, H1, ... Hm]. We have [B1, ... Bk] for\n    // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n    // We have B1 * ... * Bk samples per batch member we need.\n    auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n                   &gen, &output](int64 start_output, int64 limit_output) {\n      // Vectorized intermediate calculations for uniform rejection sampling.\n      // We always generate at most 4 samples.\n      Eigen::array<T, 4> z;\n      Eigen::array<T, 4> g;\n      const bool should_bcast = bcast.IsBroadcastingRequired();\n      const auto& counts_batch_indices = bcast.x_batch_indices();\n      const auto& probs_batch_indices = bcast.y_batch_indices();\n      auto output_flat = output.data();\n\n      // We partition work across batches (count, prob) and then across samples\n      // per batch member, to avoid extra work.\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           // output_idx is incremented with the inner loops below.\n      ) {\n        int64 batch_idx = output_idx / samples_per_batch;\n        U* const output_batch_offset = output_flat + batch_idx;\n        // Generate batch counts from BCast, as it has the right indices to loop\n        // over.\n        T count, prob;\n        if (should_bcast) {\n          count = counts(counts_batch_indices[batch_idx]);\n          prob = probs(probs_batch_indices[batch_idx]);\n        } else {\n          count = counts(batch_idx);\n          prob = probs(batch_idx);\n        }\n\n        // Calculate normalized samples, then convert them.\n        // Determine the method to use.\n        double dcount = static_cast<double>(count);\n        if (dcount <= 0.0 || prob <= T(0.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(0.0);\n          }\n        } else if (prob >= T(1.0)) {\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] =\n                static_cast<U>(dcount);\n          }\n        } else if (prob <= T(0.5)) {\n          double dp = static_cast<double>(prob);\n          if (count * prob >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(btrs(dcount, dp, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(binomial_inversion(dcount, dp, &gen_copy));\n            }\n          }\n        } else if (prob > T(0.5)) {\n          T q = T(1) - prob;\n          double dcount = static_cast<double>(count);\n          double dq = static_cast<double>(q);\n          if (count * q >= T(10)) {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              gen_copy.Skip(256 * output_idx);\n              output_batch_offset[sample_idx * num_batches] =\n                  static_cast<U>(dcount - btrs(dcount, dq, &gen_copy));\n            }\n          } else {\n            for (int64 sample_idx = output_idx % samples_per_batch;\n                 sample_idx < samples_per_batch && output_idx < limit_output;\n                 ++sample_idx, ++output_idx) {\n              random::PhiloxRandom gen_copy = gen;\n              // For binomial inversion, we have mean <= 10, variance <= 10.\n              // This means on average we need at most 10 number of samples,\n              // and for 10 standard deviations, we need 42 samples. We reserve\n              // that much.\n              gen_copy.Skip(42 * output_idx);\n              output_batch_offset[sample_idx * num_batches] = static_cast<U>(\n                  dcount - binomial_inversion(dcount, dq, &gen_copy));\n            }\n          }\n        } else {  // prob is NaN\n          // TODO(srvasude): What should happen if prob is NaN but the output\n          // type is an integer (which doesn't have a sentinel for NaN)?  Fail\n          // the whole batch sample?  Return a specialized sentinel like -1?\n          for (int64 sample_idx = output_idx % samples_per_batch;\n               sample_idx < samples_per_batch && output_idx < limit_output;\n               ++sample_idx, ++output_idx) {\n            output_batch_offset[sample_idx * num_batches] = static_cast<U>(NAN);\n          }\n        }\n      }\n    };\n\n    // This will depend on count * p (or count * q).\n    // For n * p < 10, on average, O(n * p) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~200 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the four calls to log\n    // occur for ~72 percent of samples.\n    // 4 x 100 (64-bit cycles per log) * 0.72 = ~288\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .72  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this (rate >= 10) should be ~329 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 329 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    Shard(worker_threads.num_threads, worker_threads.workers, num_elements,\n          kElementCost, DoWork);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,7 +10,7 @@\n     // the sample shape and [H1, ... Hm] for the batch shape of the samples.\n     // We have B1 * ... * Bk samples per batch member we need.\n     auto DoWork = [num_batches, samples_per_batch, &bcast, &counts, &probs,\n-                   &gen, &output](int start_output, int limit_output) {\n+                   &gen, &output](int64 start_output, int64 limit_output) {\n       // Vectorized intermediate calculations for uniform rejection sampling.\n       // We always generate at most 4 samples.\n       Eigen::array<T, 4> z;",
        "diff_line_info": {
            "deleted_lines": [
                "                   &gen, &output](int start_output, int limit_output) {"
            ],
            "added_lines": [
                "                   &gen, &output](int64 start_output, int64 limit_output) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<T, 4>::Tensor grads_image,\n                  const string& method_name) {\n    const int batch_size = grads_image.dimension(0);\n    const int image_height = grads_image.dimension(1);\n    const int image_width = grads_image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_image.setZero();\n\n    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            continue;\n          }\n          const int top_y_index = floorf(in_y);\n          const int bottom_y_index = ceilf(in_y);\n          const float y_lerp = in_y - top_y_index;\n\n          for (int x = 0; x < crop_width; ++x) {\n            const float in_x = (crop_width > 1)\n                                   ? x1 * (image_width - 1) + x * width_scale\n                                   : 0.5 * (x1 + x2) * (image_width - 1);\n            if (in_x < 0 || in_x > image_width - 1) {\n              continue;\n            }\n\n            if (method_name == \"bilinear\") {\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float dtop = (1 - y_lerp) * grads(b, y, x, d);\n                grads_image(b_in, top_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dtop);\n                grads_image(b_in, top_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dtop);\n                const float dbottom = y_lerp * grads(b, y, x, d);\n                grads_image(b_in, bottom_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dbottom);\n                grads_image(b_in, bottom_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dbottom);\n              }\n            } else {  // method_name == \"nearest\"\n              for (int d = 0; d < depth; ++d) {\n                int closest_x_index = roundf(in_x);\n                int closest_y_index = roundf(in_y);\n                grads_image(b_in, closest_y_index, closest_x_index, d) +=\n                    static_cast<T>(grads(b, y, x, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    // Including calculation cost in the depth loop and pixel loop.\n    const double cost_per_pixel =\n        (method_name == \"bilinear\"\n             ? depth * (Eigen::TensorOpCost::AddCost<float>() * 7 +\n                        Eigen::TensorOpCost::MulCost<float>() * 6 +\n                        Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n                   Eigen::TensorOpCost::AddCost<float>() * 4\n             : depth * (Eigen::TensorOpCost::AddCost<float>() +\n                        Eigen::TensorOpCost::CastCost<T, float>()) +\n                   Eigen::TensorOpCost::AddCost<float>() * 3);\n\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizeBackImgPerBox);\n\n    return true;\n  }",
        "func": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<float, 4>::ConstTensor grads,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  typename TTypes<T, 4>::Tensor grads_image,\n                  const string& method_name) {\n    const int batch_size = grads_image.dimension(0);\n    const int image_height = grads_image.dimension(1);\n    const int image_width = grads_image.dimension(2);\n\n    const int num_boxes = grads.dimension(0);\n    const int crop_height = grads.dimension(1);\n    const int crop_width = grads.dimension(2);\n    const int depth = grads.dimension(3);\n\n    grads_image.setZero();\n\n    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            continue;\n          }\n          const int top_y_index = floorf(in_y);\n          const int bottom_y_index = ceilf(in_y);\n          const float y_lerp = in_y - top_y_index;\n\n          for (int x = 0; x < crop_width; ++x) {\n            const float in_x = (crop_width > 1)\n                                   ? x1 * (image_width - 1) + x * width_scale\n                                   : 0.5 * (x1 + x2) * (image_width - 1);\n            if (in_x < 0 || in_x > image_width - 1) {\n              continue;\n            }\n\n            if (method_name == \"bilinear\") {\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float dtop = (1 - y_lerp) * grads(b, y, x, d);\n                grads_image(b_in, top_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dtop);\n                grads_image(b_in, top_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dtop);\n                const float dbottom = y_lerp * grads(b, y, x, d);\n                grads_image(b_in, bottom_y_index, left_x_index, d) +=\n                    static_cast<T>((1 - x_lerp) * dbottom);\n                grads_image(b_in, bottom_y_index, right_x_index, d) +=\n                    static_cast<T>(x_lerp * dbottom);\n              }\n            } else {  // method_name == \"nearest\"\n              for (int d = 0; d < depth; ++d) {\n                int closest_x_index = roundf(in_x);\n                int closest_y_index = roundf(in_y);\n                grads_image(b_in, closest_y_index, closest_x_index, d) +=\n                    static_cast<T>(grads(b, y, x, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    // Including calculation cost in the depth loop and pixel loop.\n    const double cost_per_pixel =\n        (method_name == \"bilinear\"\n             ? depth * (Eigen::TensorOpCost::AddCost<float>() * 7 +\n                        Eigen::TensorOpCost::MulCost<float>() * 6 +\n                        Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n                   Eigen::TensorOpCost::AddCost<float>() * 4\n             : depth * (Eigen::TensorOpCost::AddCost<float>() +\n                        Eigen::TensorOpCost::CastCost<T, float>()) +\n                   Eigen::TensorOpCost::AddCost<float>() * 3);\n\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizeBackImgPerBox);\n\n    return true;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,7 +15,7 @@\n \n     grads_image.setZero();\n \n-    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {\n+    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {\n       for (int b = start_box; b < limit_box; ++b) {\n         const float y1 = boxes(b, 0);\n         const float x1 = boxes(b, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "    auto CropAndResizeBackImgPerBox = [&](int start_box, int limit_box) {"
            ],
            "added_lines": [
                "    auto CropAndResizeBackImgPerBox = [&](int64 start_box, int64 limit_box) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  const string& method_name, float extrapolation_value,\n                  typename TTypes<float, 4>::Tensor crops) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = crops.dimension(0);\n    const int crop_height = crops.dimension(1);\n    const int crop_width = crops.dimension(2);\n    const int depth = crops.dimension(3);\n\n    // Sharding across boxes.\n    auto CropAndResizePerBox = [&](int start_box, int limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            for (int x = 0; x < crop_width; ++x) {\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = extrapolation_value;\n              }\n            }\n            continue;\n          }\n          if (method_name == \"bilinear\") {\n            const int top_y_index = floorf(in_y);\n            const int bottom_y_index = ceilf(in_y);\n            const float y_lerp = in_y - top_y_index;\n\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float top_left(static_cast<float>(\n                    image(b_in, top_y_index, left_x_index, d)));\n                const float top_right(static_cast<float>(\n                    image(b_in, top_y_index, right_x_index, d)));\n                const float bottom_left(static_cast<float>(\n                    image(b_in, bottom_y_index, left_x_index, d)));\n                const float bottom_right(static_cast<float>(\n                    image(b_in, bottom_y_index, right_x_index, d)));\n                const float top = top_left + (top_right - top_left) * x_lerp;\n                const float bottom =\n                    bottom_left + (bottom_right - bottom_left) * x_lerp;\n                crops(b, y, x, d) = top + (bottom - top) * y_lerp;\n              }\n            }\n          } else {  // method == \"nearest\"\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int closest_x_index = roundf(in_x);\n              const int closest_y_index = roundf(in_y);\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = static_cast<float>(\n                    image(b_in, closest_y_index, closest_x_index, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    double cost_per_pixel =\n        depth * (Eigen::TensorOpCost::AddCost<float>() * 6 +\n                 Eigen::TensorOpCost::MulCost<float>() * 3 +\n                 Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n        (Eigen::TensorOpCost::AddCost<float>() * 2 +\n         Eigen::TensorOpCost::AddCost<float>() * 3);\n    if (method_name == \"nearest\") {\n      cost_per_pixel = depth * Eigen::TensorOpCost::CastCost<T, float>() +\n                       Eigen::TensorOpCost::AddCost<float>() * 4 +\n                       Eigen::TensorOpCost::MulCost<float>() * 4;\n    }\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizePerBox);\n\n    return true;\n  }",
        "func": "bool operator()(const OpKernelContext* context,\n                  typename TTypes<T, 4>::ConstTensor image,\n                  typename TTypes<float, 2>::ConstTensor boxes,\n                  typename TTypes<int32, 1>::ConstTensor box_index,\n                  const string& method_name, float extrapolation_value,\n                  typename TTypes<float, 4>::Tensor crops) {\n    const int batch_size = image.dimension(0);\n    const int image_height = image.dimension(1);\n    const int image_width = image.dimension(2);\n\n    const int num_boxes = crops.dimension(0);\n    const int crop_height = crops.dimension(1);\n    const int crop_width = crops.dimension(2);\n    const int depth = crops.dimension(3);\n\n    // Sharding across boxes.\n    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {\n      for (int b = start_box; b < limit_box; ++b) {\n        const float y1 = boxes(b, 0);\n        const float x1 = boxes(b, 1);\n        const float y2 = boxes(b, 2);\n        const float x2 = boxes(b, 3);\n\n        const int32 b_in = box_index(b);\n        if (!FastBoundsCheck(b_in, batch_size)) {\n          continue;\n        }\n\n        const float height_scale =\n            (crop_height > 1)\n                ? (y2 - y1) * (image_height - 1) / (crop_height - 1)\n                : 0;\n        const float width_scale =\n            (crop_width > 1) ? (x2 - x1) * (image_width - 1) / (crop_width - 1)\n                             : 0;\n\n        for (int y = 0; y < crop_height; ++y) {\n          const float in_y = (crop_height > 1)\n                                 ? y1 * (image_height - 1) + y * height_scale\n                                 : 0.5 * (y1 + y2) * (image_height - 1);\n          if (in_y < 0 || in_y > image_height - 1) {\n            for (int x = 0; x < crop_width; ++x) {\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = extrapolation_value;\n              }\n            }\n            continue;\n          }\n          if (method_name == \"bilinear\") {\n            const int top_y_index = floorf(in_y);\n            const int bottom_y_index = ceilf(in_y);\n            const float y_lerp = in_y - top_y_index;\n\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int left_x_index = floorf(in_x);\n              const int right_x_index = ceilf(in_x);\n              const float x_lerp = in_x - left_x_index;\n\n              for (int d = 0; d < depth; ++d) {\n                const float top_left(static_cast<float>(\n                    image(b_in, top_y_index, left_x_index, d)));\n                const float top_right(static_cast<float>(\n                    image(b_in, top_y_index, right_x_index, d)));\n                const float bottom_left(static_cast<float>(\n                    image(b_in, bottom_y_index, left_x_index, d)));\n                const float bottom_right(static_cast<float>(\n                    image(b_in, bottom_y_index, right_x_index, d)));\n                const float top = top_left + (top_right - top_left) * x_lerp;\n                const float bottom =\n                    bottom_left + (bottom_right - bottom_left) * x_lerp;\n                crops(b, y, x, d) = top + (bottom - top) * y_lerp;\n              }\n            }\n          } else {  // method == \"nearest\"\n            for (int x = 0; x < crop_width; ++x) {\n              const float in_x = (crop_width > 1)\n                                     ? x1 * (image_width - 1) + x * width_scale\n                                     : 0.5 * (x1 + x2) * (image_width - 1);\n              if (in_x < 0 || in_x > image_width - 1) {\n                for (int d = 0; d < depth; ++d) {\n                  crops(b, y, x, d) = extrapolation_value;\n                }\n                continue;\n              }\n              const int closest_x_index = roundf(in_x);\n              const int closest_y_index = roundf(in_y);\n              for (int d = 0; d < depth; ++d) {\n                crops(b, y, x, d) = static_cast<float>(\n                    image(b_in, closest_y_index, closest_x_index, d));\n              }\n            }\n          }\n        }\n      }\n    };\n\n    // A rough estimation of the cost for each cropped box.\n    double cost_per_pixel =\n        depth * (Eigen::TensorOpCost::AddCost<float>() * 6 +\n                 Eigen::TensorOpCost::MulCost<float>() * 3 +\n                 Eigen::TensorOpCost::CastCost<T, float>() * 4) +\n        (Eigen::TensorOpCost::AddCost<float>() * 2 +\n         Eigen::TensorOpCost::AddCost<float>() * 3);\n    if (method_name == \"nearest\") {\n      cost_per_pixel = depth * Eigen::TensorOpCost::CastCost<T, float>() +\n                       Eigen::TensorOpCost::AddCost<float>() * 4 +\n                       Eigen::TensorOpCost::MulCost<float>() * 4;\n    }\n    const double cost_per_box = crop_height * crop_width * cost_per_pixel;\n\n    const DeviceBase::CpuWorkerThreads& worker_threads =\n        *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_boxes,\n          cost_per_box, CropAndResizePerBox);\n\n    return true;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,7 +14,7 @@\n     const int depth = crops.dimension(3);\n \n     // Sharding across boxes.\n-    auto CropAndResizePerBox = [&](int start_box, int limit_box) {\n+    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {\n       for (int b = start_box; b < limit_box; ++b) {\n         const float y1 = boxes(b, 0);\n         const float x1 = boxes(b, 1);",
        "diff_line_info": {
            "deleted_lines": [
                "    auto CropAndResizePerBox = [&](int start_box, int limit_box) {"
            ],
            "added_lines": [
                "    auto CropAndResizePerBox = [&](int64 start_box, int64 limit_box) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Compute",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "static EIGEN_ALWAYS_INLINE Status\n  Compute(OpKernelContext* context, bool sorted, int k,\n          const typename TTypes<T, 2>::ConstTensor& input, const int64 num_rows,\n          const int64 num_cols, typename TTypes<T, 2>::Tensor values,\n          typename TTypes<int, 2>::Tensor indices) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Special case for k == 1.\n    if (k == 1) {\n#ifdef EIGEN_HAS_INDEX_LIST\n      typename Eigen::IndexList<Eigen::type2index<1>> reduce_on_cols;\n      typename Eigen::IndexList<int, Eigen::type2index<1>> rows_by_one;\n      rows_by_one.set(0, num_rows);\n#else\n      Eigen::array<int, 1> reduce_on_cols = {1};\n      Eigen::array<int, 2> rows_by_one = {static_cast<int>(num_rows), 1};\n#endif\n\n      values.device(d) =\n          input.maximum(/*dims=*/reduce_on_cols).eval().reshape(rows_by_one);\n      // Get the indices of the maximum values.\n      for (int r = 0; r < num_rows; ++r) {\n        indices(r, 0) = 0;\n        for (int c = 0; c < num_cols; ++c) {\n          if (values(r, 0) == input(r, c)) {\n            indices(r, 0) = c;\n            break;\n          }\n        }\n        values(r, 0) = input(r, indices(r, 0));\n      }\n\n      return Status::OK();\n    }\n\n    auto SortIndices = [&](int start_batch, int limit_batch) {\n      for (int32 b = start_batch; b < limit_batch; ++b) {\n        const T* input_data = &input(b, 0);\n        const auto stable_comp = [input_data](const int32 a, const int32 b) {\n          if (input_data[b] < input_data[a]) {\n            return true;\n          } else if (input_data[b] > input_data[a]) {\n            return false;\n          } else {\n            return a < b;\n          }\n        };\n        const auto comp = [input_data](const int32 a, const int32 b) {\n          return input_data[b] < input_data[a];\n        };\n        // TODO(ebrevdo): For large k < num_cols, instead of using\n        // TopN, it may be faster to create a temporary vector of\n        // values 0..num_cols - 1 and then use std::partial_sort_copy\n        // of this into indices. Choosing the appropriate minimum k or\n        // ratio of k/num_cols will require some experimentation.\n        if (k == num_cols) {\n          auto* begin = &indices(b, 0);\n          auto* end = &indices(b, k);\n          // Set the initial array of indices 0 ... k - 1.\n          std::iota(begin, end, 0);\n          // We want an in-place sort, but we can cheat because we're sorting\n          // indices that started out sorted.  First, do a std::sort, which\n          // is notably faster than std::stable_sort.\n          std::sort(begin, end, comp);\n          // Then, for runs of adjacent elements that were equal, sort the\n          // indices in those runs in increasing order.\n          for (auto* run_begin = begin; run_begin != end;) {\n            auto* run_end = run_begin + 1;\n            if (run_end == end) break;\n            if (input_data[*run_begin] == input_data[*run_end]) {\n              while (++run_end != end) {\n                if (input_data[*run_begin] != input_data[*run_end]) break;\n              }\n              std::sort(run_begin, run_end);\n            }\n            run_begin = run_end;\n          }\n        } else {\n          // Use the TopN heap object to sort.\n          gtl::TopN<int32, decltype(stable_comp)> filter(k, stable_comp);\n          filter.reserve(num_cols);\n          for (int32 c = 0; c < num_cols; ++c) {\n            filter.push(c);\n          }\n\n          int32 i = 0;\n          if (sorted) {\n            std::unique_ptr<std::vector<int32>> top_k(filter.Extract());\n            for (auto top_k_it = top_k->begin(); top_k_it != top_k->end();\n                 ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          } else {\n            for (auto top_k_it = filter.unsorted_begin();\n                 top_k_it != filter.unsorted_end(); ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          }\n        }\n        // Now that the indices are sorted, copy the values over in\n        // sorted order.\n        std::transform(&indices(b, 0), &indices(b, k), &values(b, 0),\n                       [b, &input](const int32 loc) { return input(b, loc); });\n      }  // for (int32 b = ...\n    };\n\n    // Guesstimate of cost; 4*N*log(K) where N == num_cols.\n    // If K == N, assume the cost is N*log(K + 1).\n    const double cmp_cost = 3 * Eigen::TensorOpCost::AddCost<int32>() +\n                            Eigen::TensorOpCost::AddCost<T>();\n    const double base_cost =\n        cmp_cost *\n        static_cast<double>(num_cols *\n                            Eigen::numext::log2(static_cast<float>(k + 1)));\n    const double sort_cost = (k == num_cols) ? base_cost : 4 * base_cost;\n    const double copy_cost = 2 * k * Eigen::TensorOpCost::AddCost<T>();\n    const double total_cost = sort_cost + copy_cost;\n    const int64 final_cost = (total_cost >= static_cast<double>(kint64max))\n                                 ? kint64max\n                                 : static_cast<int64>(total_cost);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          final_cost, SortIndices);\n\n    return Status::OK();\n  }",
        "func": "static EIGEN_ALWAYS_INLINE Status\n  Compute(OpKernelContext* context, bool sorted, int k,\n          const typename TTypes<T, 2>::ConstTensor& input, const int64 num_rows,\n          const int64 num_cols, typename TTypes<T, 2>::Tensor values,\n          typename TTypes<int, 2>::Tensor indices) {\n    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    // Special case for k == 1.\n    if (k == 1) {\n#ifdef EIGEN_HAS_INDEX_LIST\n      typename Eigen::IndexList<Eigen::type2index<1>> reduce_on_cols;\n      typename Eigen::IndexList<int, Eigen::type2index<1>> rows_by_one;\n      rows_by_one.set(0, num_rows);\n#else\n      Eigen::array<int, 1> reduce_on_cols = {1};\n      Eigen::array<int, 2> rows_by_one = {static_cast<int>(num_rows), 1};\n#endif\n\n      values.device(d) =\n          input.maximum(/*dims=*/reduce_on_cols).eval().reshape(rows_by_one);\n      // Get the indices of the maximum values.\n      for (int r = 0; r < num_rows; ++r) {\n        indices(r, 0) = 0;\n        for (int c = 0; c < num_cols; ++c) {\n          if (values(r, 0) == input(r, c)) {\n            indices(r, 0) = c;\n            break;\n          }\n        }\n        values(r, 0) = input(r, indices(r, 0));\n      }\n\n      return Status::OK();\n    }\n\n    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {\n      for (int32 b = start_batch; b < limit_batch; ++b) {\n        const T* input_data = &input(b, 0);\n        const auto stable_comp = [input_data](const int32 a, const int32 b) {\n          if (input_data[b] < input_data[a]) {\n            return true;\n          } else if (input_data[b] > input_data[a]) {\n            return false;\n          } else {\n            return a < b;\n          }\n        };\n        const auto comp = [input_data](const int32 a, const int32 b) {\n          return input_data[b] < input_data[a];\n        };\n        // TODO(ebrevdo): For large k < num_cols, instead of using\n        // TopN, it may be faster to create a temporary vector of\n        // values 0..num_cols - 1 and then use std::partial_sort_copy\n        // of this into indices. Choosing the appropriate minimum k or\n        // ratio of k/num_cols will require some experimentation.\n        if (k == num_cols) {\n          auto* begin = &indices(b, 0);\n          auto* end = &indices(b, k);\n          // Set the initial array of indices 0 ... k - 1.\n          std::iota(begin, end, 0);\n          // We want an in-place sort, but we can cheat because we're sorting\n          // indices that started out sorted.  First, do a std::sort, which\n          // is notably faster than std::stable_sort.\n          std::sort(begin, end, comp);\n          // Then, for runs of adjacent elements that were equal, sort the\n          // indices in those runs in increasing order.\n          for (auto* run_begin = begin; run_begin != end;) {\n            auto* run_end = run_begin + 1;\n            if (run_end == end) break;\n            if (input_data[*run_begin] == input_data[*run_end]) {\n              while (++run_end != end) {\n                if (input_data[*run_begin] != input_data[*run_end]) break;\n              }\n              std::sort(run_begin, run_end);\n            }\n            run_begin = run_end;\n          }\n        } else {\n          // Use the TopN heap object to sort.\n          gtl::TopN<int32, decltype(stable_comp)> filter(k, stable_comp);\n          filter.reserve(num_cols);\n          for (int32 c = 0; c < num_cols; ++c) {\n            filter.push(c);\n          }\n\n          int32 i = 0;\n          if (sorted) {\n            std::unique_ptr<std::vector<int32>> top_k(filter.Extract());\n            for (auto top_k_it = top_k->begin(); top_k_it != top_k->end();\n                 ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          } else {\n            for (auto top_k_it = filter.unsorted_begin();\n                 top_k_it != filter.unsorted_end(); ++top_k_it, ++i) {\n              indices(b, i) = *top_k_it;\n            }\n          }\n        }\n        // Now that the indices are sorted, copy the values over in\n        // sorted order.\n        std::transform(&indices(b, 0), &indices(b, k), &values(b, 0),\n                       [b, &input](const int32 loc) { return input(b, loc); });\n      }  // for (int32 b = ...\n    };\n\n    // Guesstimate of cost; 4*N*log(K) where N == num_cols.\n    // If K == N, assume the cost is N*log(K + 1).\n    const double cmp_cost = 3 * Eigen::TensorOpCost::AddCost<int32>() +\n                            Eigen::TensorOpCost::AddCost<T>();\n    const double base_cost =\n        cmp_cost *\n        static_cast<double>(num_cols *\n                            Eigen::numext::log2(static_cast<float>(k + 1)));\n    const double sort_cost = (k == num_cols) ? base_cost : 4 * base_cost;\n    const double copy_cost = 2 * k * Eigen::TensorOpCost::AddCost<T>();\n    const double total_cost = sort_cost + copy_cost;\n    const int64 final_cost = (total_cost >= static_cast<double>(kint64max))\n                                 ? kint64max\n                                 : static_cast<int64>(total_cost);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          final_cost, SortIndices);\n\n    return Status::OK();\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,7 +33,7 @@\n       return Status::OK();\n     }\n \n-    auto SortIndices = [&](int start_batch, int limit_batch) {\n+    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {\n       for (int32 b = start_batch; b < limit_batch; ++b) {\n         const T* input_data = &input(b, 0);\n         const auto stable_comp = [input_data](const int32 a, const int32 b) {",
        "diff_line_info": {
            "deleted_lines": [
                "    auto SortIndices = [&](int start_batch, int limit_batch) {"
            ],
            "added_lines": [
                "    auto SortIndices = [&](int64 start_batch, int64 limit_batch) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Launch",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }",
        "func": "static void Launch(OpKernelContext* context, const Tensor& in_x,\n                     const Tensor& in_y, bool adjoint, bool lower,\n                     const MatMulBCast& bcast, Tensor* out) {\n    // Number of banded matrix triangular solves i.e. size of the batch.\n    const int64 batch_size = bcast.output_batch_size();\n    const int64 cost_per_unit =\n        in_x.dim_size(1) * in_x.dim_size(2) * in_y.dim_size(2);\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n\n    using Matrix =\n        Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;\n    using ConstMatrixMap = Eigen::Map<const Matrix>;\n    using RealScalar = typename Eigen::NumTraits<Scalar>::Real;\n    // Check diagonal before doing any solves. This is the first row in the\n    // lower case and else is the last row.\n    auto matrix = ConstMatrixMap(in_x.flat<Scalar>().data(), in_x.dim_size(1),\n                                 in_x.dim_size(2));\n    RealScalar min_abs_pivot;\n    if (lower) {\n      min_abs_pivot = matrix.row(0).cwiseAbs().minCoeff();\n    } else {\n      min_abs_pivot = matrix.row(in_x.dim_size(1) - 1).cwiseAbs().minCoeff();\n    }\n    OP_REQUIRES(context, min_abs_pivot > RealScalar(0),\n                errors::InvalidArgument(\"Input matrix is not invertible.\"));\n\n    Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n          cost_per_unit,\n          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n                                                      int64 limit) {\n            SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                in_x, in_y, lower, adjoint, bcast, out, start, limit);\n          });\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,7 +26,8 @@\n \n     Shard(worker_threads.num_threads, worker_threads.workers, batch_size,\n           cost_per_unit,\n-          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {\n+          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,\n+                                                      int64 limit) {\n             SequentialBandedTriangularSolveKernel<Scalar>::Run(\n                 in_x, in_y, lower, adjoint, bcast, out, start, limit);\n           });",
        "diff_line_info": {
            "deleted_lines": [
                "          [&in_x, &in_y, adjoint, lower, &bcast, out](int start, int limit) {"
            ],
            "added_lines": [
                "          [&in_x, &in_y, adjoint, lower, &bcast, out](int64 start,",
                "                                                      int64 limit) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* context, const Tensor& input_tensor,\n                  Tensor& output_tensor, int n, bool reverse) {\n    const T* input = input_tensor.flat<T>().data();\n    T* output = output_tensor.flat<T>().data();\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1],\n    // then num_rows = d1*d2...dk-1, last_dim = dk.\n    const int num_rows = output_tensor.NumElements();\n    const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n\n    // Allocate each row to different shard.\n    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {\n      // std::nth_element would rearrange the array, so we need a new buffer.\n      std::vector<T> buf(last_dim);\n\n      for (int b = start; b < limit; ++b) {\n        // Copy from one row of elements to buffer\n        const T* input_start = input + b * last_dim;\n        const T* input_end = input + (b + 1) * last_dim;\n        std::copy(input_start, input_end, buf.begin());\n\n        std::nth_element(buf.begin(), buf.begin() + n, buf.end());\n        // The element placed in the nth position is exactly the element that\n        // would occur in this position if the range was fully sorted.\n        output[b] = buf[n];\n      }\n    };\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    // The average time complexity of partition-based nth_element (BFPRT) is\n    // O(n), although the worst time complexity could be O(n^2). Here, 20 is a\n    // empirical factor of cost_per_unit.\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          20 * last_dim, SubNthElement);\n  }",
        "func": "void operator()(OpKernelContext* context, const Tensor& input_tensor,\n                  Tensor& output_tensor, int n, bool reverse) {\n    const T* input = input_tensor.flat<T>().data();\n    T* output = output_tensor.flat<T>().data();\n\n    // Assume input_shape is [d1,d2,...dk], and output_shape is [d1,d2...dk-1],\n    // then num_rows = d1*d2...dk-1, last_dim = dk.\n    const int num_rows = output_tensor.NumElements();\n    const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n\n    // Allocate each row to different shard.\n    auto SubNthElement = [&, input, output, last_dim, n](int64 start,\n                                                         int64 limit) {\n      // std::nth_element would rearrange the array, so we need a new buffer.\n      std::vector<T> buf(last_dim);\n\n      for (int b = start; b < limit; ++b) {\n        // Copy from one row of elements to buffer\n        const T* input_start = input + b * last_dim;\n        const T* input_end = input + (b + 1) * last_dim;\n        std::copy(input_start, input_end, buf.begin());\n\n        std::nth_element(buf.begin(), buf.begin() + n, buf.end());\n        // The element placed in the nth position is exactly the element that\n        // would occur in this position if the range was fully sorted.\n        output[b] = buf[n];\n      }\n    };\n\n    auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n    // The average time complexity of partition-based nth_element (BFPRT) is\n    // O(n), although the worst time complexity could be O(n^2). Here, 20 is a\n    // empirical factor of cost_per_unit.\n    Shard(worker_threads.num_threads, worker_threads.workers, num_rows,\n          20 * last_dim, SubNthElement);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,8 @@\n     const int last_dim = input_tensor.dim_size(input_tensor.dims() - 1);\n \n     // Allocate each row to different shard.\n-    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {\n+    auto SubNthElement = [&, input, output, last_dim, n](int64 start,\n+                                                         int64 limit) {\n       // std::nth_element would rearrange the array, so we need a new buffer.\n       std::vector<T> buf(last_dim);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    auto SubNthElement = [&, input, output, last_dim, n](int start, int limit) {"
            ],
            "added_lines": [
                "    auto SubNthElement = [&, input, output, last_dim, n](int64 start,",
                "                                                         int64 limit) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/Compute",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void Compute(OpKernelContext* const context) override {\n    core::RefCountPtr<BoostedTreesEnsembleResource> resource;\n    // Get the resource.\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &resource));\n\n    // Get the inputs.\n    OpInputList bucketized_features_list;\n    OP_REQUIRES_OK(context, context->input_list(\"bucketized_features\",\n                                                &bucketized_features_list));\n    std::vector<tensorflow::TTypes<int32>::ConstMatrix> bucketized_features;\n    bucketized_features.reserve(bucketized_features_list.size());\n    ConvertVectorsToMatrices(bucketized_features_list, bucketized_features);\n    const int batch_size = bucketized_features[0].dimension(0);\n\n    // We need to get the feature ids used for splitting and the logits after\n    // each split. We will use these to calculate the changes in the prediction\n    // (contributions) for an arbitrary activation function (done in Python) and\n    // attribute them to the associated feature ids. We will store these in\n    // a proto below.\n    Tensor* output_debug_info_t = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\"examples_debug_outputs_serialized\",\n                                          {batch_size}, &output_debug_info_t));\n    // Will contain serialized protos, per example.\n    auto output_debug_info = output_debug_info_t->flat<tstring>();\n    const int32 last_tree = resource->num_trees() - 1;\n\n    // For each given example, traverse through all trees keeping track of the\n    // features used to split and the associated logits at each point along the\n    // path. Note: feature_ids has one less value than logits_path because the\n    // first value of each logit path will be the bias.\n    auto do_work = [&resource, &bucketized_features, &output_debug_info,\n                    last_tree](int32 start, int32 end) {\n      for (int32 i = start; i < end; ++i) {\n        // Proto to store debug outputs, per example.\n        boosted_trees::DebugOutput example_debug_info;\n        // Initial bias prediction. E.g., prediction based off training mean.\n        const auto& tree_logits = resource->node_value(0, 0);\n        DCHECK_EQ(tree_logits.size(), 1);\n        float tree_logit = resource->GetTreeWeight(0) * tree_logits[0];\n        example_debug_info.add_logits_path(tree_logit);\n        int32 node_id = 0;\n        int32 tree_id = 0;\n        int32 feature_id;\n        float past_trees_logit = 0;  // Sum of leaf logits from prior trees.\n        // Go through each tree and populate proto.\n        while (tree_id <= last_tree) {\n          if (resource->is_leaf(tree_id, node_id)) {  // Move onto other trees.\n            // Accumulate tree_logits only if the leaf is non-root, but do so\n            // for bias tree.\n            if (tree_id == 0 || node_id > 0) {\n              past_trees_logit += tree_logit;\n            }\n            ++tree_id;\n            node_id = 0;\n          } else {  // Add to proto.\n            // Feature id used to split.\n            feature_id = resource->feature_id(tree_id, node_id);\n            example_debug_info.add_feature_ids(feature_id);\n            // Get logit after split.\n            node_id =\n                resource->next_node(tree_id, node_id, i, bucketized_features);\n            const auto& tree_logits = resource->node_value(tree_id, node_id);\n            DCHECK_EQ(tree_logits.size(), 1);\n            tree_logit = resource->GetTreeWeight(tree_id) * tree_logits[0];\n            // Output logit incorporates sum of leaf logits from prior trees.\n            example_debug_info.add_logits_path(tree_logit + past_trees_logit);\n          }\n        }\n        // Set output as serialized proto containing debug info.\n        string serialized = example_debug_info.SerializeAsString();\n        output_debug_info(i) = serialized;\n      }\n    };\n\n    // 10 is the magic number. The actual number might depend on (the number of\n    // layers in the trees) and (cpu cycles spent on each layer), but this\n    // value would work for many cases. May be tuned later.\n    const int64 cost = (last_tree + 1) * 10;\n    thread::ThreadPool* const worker_threads =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    Shard(worker_threads->NumThreads(), worker_threads, batch_size,\n          /*cost_per_unit=*/cost, do_work);\n  }",
        "func": "void Compute(OpKernelContext* const context) override {\n    core::RefCountPtr<BoostedTreesEnsembleResource> resource;\n    // Get the resource.\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &resource));\n\n    // Get the inputs.\n    OpInputList bucketized_features_list;\n    OP_REQUIRES_OK(context, context->input_list(\"bucketized_features\",\n                                                &bucketized_features_list));\n    std::vector<tensorflow::TTypes<int32>::ConstMatrix> bucketized_features;\n    bucketized_features.reserve(bucketized_features_list.size());\n    ConvertVectorsToMatrices(bucketized_features_list, bucketized_features);\n    const int batch_size = bucketized_features[0].dimension(0);\n\n    // We need to get the feature ids used for splitting and the logits after\n    // each split. We will use these to calculate the changes in the prediction\n    // (contributions) for an arbitrary activation function (done in Python) and\n    // attribute them to the associated feature ids. We will store these in\n    // a proto below.\n    Tensor* output_debug_info_t = nullptr;\n    OP_REQUIRES_OK(\n        context, context->allocate_output(\"examples_debug_outputs_serialized\",\n                                          {batch_size}, &output_debug_info_t));\n    // Will contain serialized protos, per example.\n    auto output_debug_info = output_debug_info_t->flat<tstring>();\n    const int32 last_tree = resource->num_trees() - 1;\n\n    // For each given example, traverse through all trees keeping track of the\n    // features used to split and the associated logits at each point along the\n    // path. Note: feature_ids has one less value than logits_path because the\n    // first value of each logit path will be the bias.\n    auto do_work = [&resource, &bucketized_features, &output_debug_info,\n                    last_tree](int64 start, int64 end) {\n      for (int32 i = start; i < end; ++i) {\n        // Proto to store debug outputs, per example.\n        boosted_trees::DebugOutput example_debug_info;\n        // Initial bias prediction. E.g., prediction based off training mean.\n        const auto& tree_logits = resource->node_value(0, 0);\n        DCHECK_EQ(tree_logits.size(), 1);\n        float tree_logit = resource->GetTreeWeight(0) * tree_logits[0];\n        example_debug_info.add_logits_path(tree_logit);\n        int32 node_id = 0;\n        int32 tree_id = 0;\n        int32 feature_id;\n        float past_trees_logit = 0;  // Sum of leaf logits from prior trees.\n        // Go through each tree and populate proto.\n        while (tree_id <= last_tree) {\n          if (resource->is_leaf(tree_id, node_id)) {  // Move onto other trees.\n            // Accumulate tree_logits only if the leaf is non-root, but do so\n            // for bias tree.\n            if (tree_id == 0 || node_id > 0) {\n              past_trees_logit += tree_logit;\n            }\n            ++tree_id;\n            node_id = 0;\n          } else {  // Add to proto.\n            // Feature id used to split.\n            feature_id = resource->feature_id(tree_id, node_id);\n            example_debug_info.add_feature_ids(feature_id);\n            // Get logit after split.\n            node_id =\n                resource->next_node(tree_id, node_id, i, bucketized_features);\n            const auto& tree_logits = resource->node_value(tree_id, node_id);\n            DCHECK_EQ(tree_logits.size(), 1);\n            tree_logit = resource->GetTreeWeight(tree_id) * tree_logits[0];\n            // Output logit incorporates sum of leaf logits from prior trees.\n            example_debug_info.add_logits_path(tree_logit + past_trees_logit);\n          }\n        }\n        // Set output as serialized proto containing debug info.\n        string serialized = example_debug_info.SerializeAsString();\n        output_debug_info(i) = serialized;\n      }\n    };\n\n    // 10 is the magic number. The actual number might depend on (the number of\n    // layers in the trees) and (cpu cycles spent on each layer), but this\n    // value would work for many cases. May be tuned later.\n    const int64 cost = (last_tree + 1) * 10;\n    thread::ThreadPool* const worker_threads =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    Shard(worker_threads->NumThreads(), worker_threads, batch_size,\n          /*cost_per_unit=*/cost, do_work);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,7 @@\n     // path. Note: feature_ids has one less value than logits_path because the\n     // first value of each logit path will be the bias.\n     auto do_work = [&resource, &bucketized_features, &output_debug_info,\n-                    last_tree](int32 start, int32 end) {\n+                    last_tree](int64 start, int64 end) {\n       for (int32 i = start; i < end; ++i) {\n         // Proto to store debug outputs, per example.\n         boosted_trees::DebugOutput example_debug_info;",
        "diff_line_info": {
            "deleted_lines": [
                "                    last_tree](int32 start, int32 end) {"
            ],
            "added_lines": [
                "                    last_tree](int64 start, int64 end) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-15202",
        "func_name": "tensorflow/operator()",
        "description": "In Tensorflow before versions 1.15.4, 2.0.3, 2.1.2, 2.2.1 and 2.3.1, the `Shard` API in TensorFlow expects the last argument to be a function taking two `int64` (i.e., `long long`) arguments. However, there are several places in TensorFlow where a lambda taking `int` or `int32` arguments is being used. In these cases, if the amount of work to be parallelized is large enough, integer truncation occurs. Depending on how the two arguments of the lambda are used, this can result in segfaults, read/write outside of heap allocated arrays, stack overflows, or data corruption. The issue is patched in commits 27b417360cbd671ef55915e4bb6bb06af8b8a832 and ca8c013b5e97b1373b3bb1c97ea655e69f31a575, and is released in TensorFlow versions 1.15.4, 2.0.3, 2.1.2, 2.2.1, or 2.3.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/ca8c013b5e97b1373b3bb1c97ea655e69f31a575",
        "commit_title": "Prevent integer truncation from 64 to 32 bits.",
        "commit_text": " The `tensorflow::Shard` functions last argument must be a 2 argument function where both arguments are `int64` (`long long`, 64 bits). However, there are usages where code passes in a function where arguments are `int` or `int32` (32 bits). In these cases, it is possible that the integer truncation would later cause a segfault or other unexpected behavior.  PiperOrigin-RevId: 332560414",
        "func_before": "void operator()(OpKernelContext* ctx, const CPUDevice& d, const T* rate_flat,\n                  int num_rate, int num_samples,\n                  const random::PhiloxRandom& rng, U* samples_flat) {\n    // Two different algorithms are employed, depending on the size of\n    // rate.\n    // If rate < 10, we use an algorithm attributed to Knuth:\n    // Seminumerical Algorithms. Art of Computer Programming, Volume 2.\n    //\n    // This algorithm runs in O(rate) time, and will require O(rate)\n    // uniform variates.\n    //\n    // If rate >= 10 we use a transformation-rejection algorithm from\n    // pairs of uniform random variables due to Hormann.\n    // http://www.sciencedirect.com/science/article/pii/0167668793909974\n    //\n    // The algorithm has an acceptance rate of ~89% for the smallest rate\n    // (~10),\n    // and higher accept rates for higher rate, so runtime is\n    // O(NumRate * NumSamples * k) with k ~ 1 / 0.89.\n    //\n    // We partition work first across rates then across\n    // samples-per-rate to\n    // avoid a couple flops which can be done on a per-rate basis.\n\n    typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n\n    auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n                      int start_output, int limit_output) {\n      // Capturing \"rng\" by value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"rng\" by reference and explicitly do a copy assignment.\n\n      Uniform uniform;\n      typename Uniform::ResultType uniform_result;\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           /* output_idx incremented within inner loop below */) {\n        const int64 rate_idx = output_idx / num_samples;\n\n        // Several calculations can be done on a per-rate basis.\n        const CT rate = CT(rate_flat[rate_idx]);\n\n        auto samples_rate_output = samples_flat + rate_idx;\n\n        if (rate < CT(10)) {\n          // Knuth's algorithm for generating Poisson random variates.\n          // Given a Poisson process, the time between events is exponentially\n          // distributed. If we have a Poisson process with rate lambda, then,\n          // the time between events is distributed Exp(lambda). If X ~\n          // Uniform(0, 1), then Y ~ Exp(lambda), where Y = -log(X) / lambda.\n          // Thus to simulate a Poisson draw, we can draw X_i ~ Exp(lambda),\n          // and N ~ Poisson(lambda), where N is the least number such that\n          // \\sum_i^N X_i > 1.\n          const CT exp_neg_rate = Eigen::numext::exp(-rate);\n\n          // Compute the rest of the samples for the current rate value.\n          for (int64 sample_idx = output_idx % num_samples;\n               sample_idx < num_samples && output_idx < limit_output;\n               sample_idx++, output_idx++) {\n            random::PhiloxRandom gen = rng;\n            gen.Skip(kReservedSamplesPerOutput * output_idx);\n            int16 uniform_remaining = 0;\n\n            CT prod = 1;\n            CT x = 0;\n\n            // Keep trying until we surpass e^(-rate). This will take\n            // expected time proportional to rate.\n            while (true) {\n              UNIFORM(u);\n              prod = prod * u;\n              if (prod <= exp_neg_rate &&\n                  x <= CT(Eigen::NumTraits<U>::highest())) {\n                samples_rate_output[sample_idx * num_rate] = U(x);\n                break;\n              }\n              x += 1;\n            }\n          }\n          continue;\n        }\n        // Transformed rejection due to Hormann.\n        //\n        // Given a CDF F(x), and G(x), a dominating distribution chosen such\n        // that it is close to the inverse CDF F^-1(x), compute the following\n        // steps:\n        //\n        // 1) Generate U and V, two independent random variates. Set U = U - 0.5\n        // (this step isn't strictly necessary, but is done to make some\n        // calculations symmetric and convenient. Henceforth, G is defined on\n        // [-0.5, 0.5]).\n        //\n        // 2) If V <= alpha * F'(G(U)) * G'(U), return floor(G(U)), else return\n        // to step 1. alpha is the acceptance probability of the rejection\n        // algorithm.\n        //\n        // For more details on transformed rejection, see:\n        // http://citeseer.ist.psu.edu/viewdoc/citations;jsessionid=1BEB35946CC807879F55D42512E5490C?doi=10.1.1.48.3054.\n        //\n        // The dominating distribution in this case:\n        //\n        // G(u) = (2 * a / (2 - |u|) + b) * u + c\n\n        using Eigen::numext::log;\n        const CT log_rate = log(rate);\n\n        // Constants used to define the dominating distribution. Names taken\n        // from Hormann's paper. Constants were chosen to define the tightest\n        // G(u) for the inverse Poisson CDF.\n        const CT b = CT(0.931) + CT(2.53) * Eigen::numext::sqrt(rate);\n        const CT a = CT(-0.059) + CT(0.02483) * b;\n\n        // This is the inverse acceptance rate. At a minimum (when rate = 10),\n        // this corresponds to ~75% acceptance. As the rate becomes larger, this\n        // approaches ~89%.\n        const CT inv_alpha = CT(1.1239) + CT(1.1328) / (b - CT(3.4));\n\n        // Compute the rest of the samples for the current rate value.\n        for (int64 sample_idx = output_idx % num_samples;\n             sample_idx < num_samples && output_idx < limit_output;\n             sample_idx++, output_idx++) {\n          random::PhiloxRandom gen = rng;\n          gen.Skip(kReservedSamplesPerOutput * output_idx);\n          int16 uniform_remaining = 0;\n\n          while (true) {\n            UNIFORM(u);\n            u -= CT(0.5);\n            UNIFORM(v);\n\n            CT u_shifted = CT(0.5) - Eigen::numext::abs(u);\n            CT k = Eigen::numext::floor((CT(2) * a / u_shifted + b) * u + rate +\n                                        CT(0.43));\n\n            if (k > CT(Eigen::NumTraits<U>::highest())) {\n              // retry in case of overflow.\n              continue;\n            }\n\n            // When alpha * f(G(U)) * G'(U) is close to 1, it is possible to\n            // find a rectangle (-u_r, u_r) x (0, v_r) under the curve, such\n            // that if v <= v_r and |u| <= u_r, then we can accept.\n            // Here v_r = 0.9227 - 3.6224 / (b - 2) and u_r = 0.43.\n            if (u_shifted >= CT(0.07) &&\n                v <= CT(0.9277) - CT(3.6224) / (b - CT(2))) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n\n            if (k < 0 || (u_shifted < CT(0.013) && v > u_shifted)) {\n              continue;\n            }\n\n            // The expression below is equivalent to the computation of step 2)\n            // in transformed rejection (v <= alpha * F'(G(u)) * G'(u)).\n            CT s = log(v * inv_alpha / (a / (u_shifted * u_shifted) + b));\n            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);\n            if (s <= t) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n          }\n        }\n      }\n    };\n\n    // This will depend on rate.\n    // For rate < 10, on average, O(rate) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~25 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the single call to log + call to\n    // lgamma\n    // occur for ~60 percent of samples.\n    // 2 x 100 (64-bit cycles per log) * 0.62 = ~124\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .62  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this should be ~165 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 165 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          num_rate * num_samples, kElementCost, DoWork);\n  }",
        "func": "void operator()(OpKernelContext* ctx, const CPUDevice& d, const T* rate_flat,\n                  int num_rate, int num_samples,\n                  const random::PhiloxRandom& rng, U* samples_flat) {\n    // Two different algorithms are employed, depending on the size of\n    // rate.\n    // If rate < 10, we use an algorithm attributed to Knuth:\n    // Seminumerical Algorithms. Art of Computer Programming, Volume 2.\n    //\n    // This algorithm runs in O(rate) time, and will require O(rate)\n    // uniform variates.\n    //\n    // If rate >= 10 we use a transformation-rejection algorithm from\n    // pairs of uniform random variables due to Hormann.\n    // http://www.sciencedirect.com/science/article/pii/0167668793909974\n    //\n    // The algorithm has an acceptance rate of ~89% for the smallest rate\n    // (~10),\n    // and higher accept rates for higher rate, so runtime is\n    // O(NumRate * NumSamples * k) with k ~ 1 / 0.89.\n    //\n    // We partition work first across rates then across\n    // samples-per-rate to\n    // avoid a couple flops which can be done on a per-rate basis.\n\n    typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n\n    auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n                      int64 start_output, int64 limit_output) {\n      // Capturing \"rng\" by value would only make a copy for the _shared_\n      // lambda.  Since we want to let each worker have its own copy, we pass\n      // \"rng\" by reference and explicitly do a copy assignment.\n\n      Uniform uniform;\n      typename Uniform::ResultType uniform_result;\n      for (int64 output_idx = start_output; output_idx < limit_output;\n           /* output_idx incremented within inner loop below */) {\n        const int64 rate_idx = output_idx / num_samples;\n\n        // Several calculations can be done on a per-rate basis.\n        const CT rate = CT(rate_flat[rate_idx]);\n\n        auto samples_rate_output = samples_flat + rate_idx;\n\n        if (rate < CT(10)) {\n          // Knuth's algorithm for generating Poisson random variates.\n          // Given a Poisson process, the time between events is exponentially\n          // distributed. If we have a Poisson process with rate lambda, then,\n          // the time between events is distributed Exp(lambda). If X ~\n          // Uniform(0, 1), then Y ~ Exp(lambda), where Y = -log(X) / lambda.\n          // Thus to simulate a Poisson draw, we can draw X_i ~ Exp(lambda),\n          // and N ~ Poisson(lambda), where N is the least number such that\n          // \\sum_i^N X_i > 1.\n          const CT exp_neg_rate = Eigen::numext::exp(-rate);\n\n          // Compute the rest of the samples for the current rate value.\n          for (int64 sample_idx = output_idx % num_samples;\n               sample_idx < num_samples && output_idx < limit_output;\n               sample_idx++, output_idx++) {\n            random::PhiloxRandom gen = rng;\n            gen.Skip(kReservedSamplesPerOutput * output_idx);\n            int16 uniform_remaining = 0;\n\n            CT prod = 1;\n            CT x = 0;\n\n            // Keep trying until we surpass e^(-rate). This will take\n            // expected time proportional to rate.\n            while (true) {\n              UNIFORM(u);\n              prod = prod * u;\n              if (prod <= exp_neg_rate &&\n                  x <= CT(Eigen::NumTraits<U>::highest())) {\n                samples_rate_output[sample_idx * num_rate] = U(x);\n                break;\n              }\n              x += 1;\n            }\n          }\n          continue;\n        }\n        // Transformed rejection due to Hormann.\n        //\n        // Given a CDF F(x), and G(x), a dominating distribution chosen such\n        // that it is close to the inverse CDF F^-1(x), compute the following\n        // steps:\n        //\n        // 1) Generate U and V, two independent random variates. Set U = U - 0.5\n        // (this step isn't strictly necessary, but is done to make some\n        // calculations symmetric and convenient. Henceforth, G is defined on\n        // [-0.5, 0.5]).\n        //\n        // 2) If V <= alpha * F'(G(U)) * G'(U), return floor(G(U)), else return\n        // to step 1. alpha is the acceptance probability of the rejection\n        // algorithm.\n        //\n        // For more details on transformed rejection, see:\n        // http://citeseer.ist.psu.edu/viewdoc/citations;jsessionid=1BEB35946CC807879F55D42512E5490C?doi=10.1.1.48.3054.\n        //\n        // The dominating distribution in this case:\n        //\n        // G(u) = (2 * a / (2 - |u|) + b) * u + c\n\n        using Eigen::numext::log;\n        const CT log_rate = log(rate);\n\n        // Constants used to define the dominating distribution. Names taken\n        // from Hormann's paper. Constants were chosen to define the tightest\n        // G(u) for the inverse Poisson CDF.\n        const CT b = CT(0.931) + CT(2.53) * Eigen::numext::sqrt(rate);\n        const CT a = CT(-0.059) + CT(0.02483) * b;\n\n        // This is the inverse acceptance rate. At a minimum (when rate = 10),\n        // this corresponds to ~75% acceptance. As the rate becomes larger, this\n        // approaches ~89%.\n        const CT inv_alpha = CT(1.1239) + CT(1.1328) / (b - CT(3.4));\n\n        // Compute the rest of the samples for the current rate value.\n        for (int64 sample_idx = output_idx % num_samples;\n             sample_idx < num_samples && output_idx < limit_output;\n             sample_idx++, output_idx++) {\n          random::PhiloxRandom gen = rng;\n          gen.Skip(kReservedSamplesPerOutput * output_idx);\n          int16 uniform_remaining = 0;\n\n          while (true) {\n            UNIFORM(u);\n            u -= CT(0.5);\n            UNIFORM(v);\n\n            CT u_shifted = CT(0.5) - Eigen::numext::abs(u);\n            CT k = Eigen::numext::floor((CT(2) * a / u_shifted + b) * u + rate +\n                                        CT(0.43));\n\n            if (k > CT(Eigen::NumTraits<U>::highest())) {\n              // retry in case of overflow.\n              continue;\n            }\n\n            // When alpha * f(G(U)) * G'(U) is close to 1, it is possible to\n            // find a rectangle (-u_r, u_r) x (0, v_r) under the curve, such\n            // that if v <= v_r and |u| <= u_r, then we can accept.\n            // Here v_r = 0.9227 - 3.6224 / (b - 2) and u_r = 0.43.\n            if (u_shifted >= CT(0.07) &&\n                v <= CT(0.9277) - CT(3.6224) / (b - CT(2))) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n\n            if (k < 0 || (u_shifted < CT(0.013) && v > u_shifted)) {\n              continue;\n            }\n\n            // The expression below is equivalent to the computation of step 2)\n            // in transformed rejection (v <= alpha * F'(G(u)) * G'(u)).\n            CT s = log(v * inv_alpha / (a / (u_shifted * u_shifted) + b));\n            CT t = -rate + k * log_rate - Eigen::numext::lgamma(k + 1);\n            if (s <= t) {\n              samples_rate_output[sample_idx * num_rate] = U(k);\n              break;\n            }\n          }\n        }\n      }\n    };\n\n    // This will depend on rate.\n    // For rate < 10, on average, O(rate) calls to uniform are\n    // needed, with that\n    // many multiplies. ~10 uniform calls on average with ~25 cost op calls.\n    //\n    // Very roughly, for rate >= 10, the single call to log + call to\n    // lgamma\n    // occur for ~60 percent of samples.\n    // 2 x 100 (64-bit cycles per log) * 0.62 = ~124\n    // Additionally, there are ~10 other ops (+, *, /, ...) at 3-6 cycles each:\n    // 40 * .62  = ~25.\n    //\n    // Finally, there are several other ops that are done every loop along with\n    // 2 uniform generations along with 5 other ops at 3-6 cycles each.\n    // ~15 / .89 = ~16\n    //\n    // In total this should be ~165 + 2 * Uniform::kElementCost.\n    // We assume that half the tensor has rate < 10, so on average 6\n    // uniform's\n    // will be needed. We will upper bound the other op cost by the one for\n    // rate > 10.\n    static const int kElementCost = 165 + 6 * Uniform::kElementCost +\n                                    6 * random::PhiloxRandom::kElementCost;\n    auto worker_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n    Shard(worker_threads.num_threads, worker_threads.workers,\n          num_rate * num_samples, kElementCost, DoWork);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,7 +25,7 @@\n     typedef random::UniformDistribution<random::PhiloxRandom, CT> Uniform;\n \n     auto DoWork = [num_samples, num_rate, &rng, samples_flat, rate_flat](\n-                      int start_output, int limit_output) {\n+                      int64 start_output, int64 limit_output) {\n       // Capturing \"rng\" by value would only make a copy for the _shared_\n       // lambda.  Since we want to let each worker have its own copy, we pass\n       // \"rng\" by reference and explicitly do a copy assignment.",
        "diff_line_info": {
            "deleted_lines": [
                "                      int start_output, int limit_output) {"
            ],
            "added_lines": [
                "                      int64 start_output, int64 limit_output) {"
            ]
        }
    }
]