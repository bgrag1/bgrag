[
    {
        "cve_id": "CVE-2019-1563",
        "func_name": "openssl/cms_RecipientInfo_ktri_decrypt",
        "description": "In situations where an attacker receives automated notification of the success or failure of a decryption attempt an attacker, after sending a very large number of messages to be decrypted, can recover a CMS/PKCS7 transported encryption key or decrypt any RSA encrypted message that was encrypted with the public RSA key, using a Bleichenbacher padding oracle attack. Applications are not affected if they use a certificate together with the private RSA key to the CMS_decrypt or PKCS7_decrypt functions to select the correct recipient info to decrypt. Fixed in OpenSSL 1.1.1d (Affected 1.1.1-1.1.1c). Fixed in OpenSSL 1.1.0l (Affected 1.1.0-1.1.0k). Fixed in OpenSSL 1.0.2t (Affected 1.0.2-1.0.2s).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=631f94db0065c78181ca9ba5546ebc8bb3884b97",
        "commit_title": "",
        "commit_text": "Fix a padding oracle in PKCS7_dataDecode and CMS_decrypt_set1_pkey  An attack is simple, if the first CMS_recipientInfo is valid but the second CMS_recipientInfo is chosen ciphertext. If the second recipientInfo decodes to PKCS #1 v1.5 form plaintext, the correct encryption key will be replaced by garbage, and the message cannot be decoded, but if the RSA decryption fails, the correct encryption key is used and the recipient will not notice the attack.  As a work around for this potential attack the length of the decrypted key must be equal to the cipher default key length, in case the certifiate is not given and all recipientInfo are tried out.  The old behaviour can be re-enabled in the CMS code by setting the CMS_DEBUG_DECRYPT flag.  (Merged from https://github.com/openssl/openssl/pull/9777)  (cherry picked from commit 5840ed0cd1e6487d247efbc1a04136a41d7b3a37) ",
        "func_before": "static int cms_RecipientInfo_ktri_decrypt(CMS_ContentInfo *cms,\n                                          CMS_RecipientInfo *ri)\n{\n    CMS_KeyTransRecipientInfo *ktri = ri->d.ktri;\n    EVP_PKEY *pkey = ktri->pkey;\n    unsigned char *ek = NULL;\n    size_t eklen;\n    int ret = 0;\n    CMS_EncryptedContentInfo *ec;\n    ec = cms->d.envelopedData->encryptedContentInfo;\n\n    if (ktri->pkey == NULL) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_NO_PRIVATE_KEY);\n        return 0;\n    }\n\n    ktri->pctx = EVP_PKEY_CTX_new(pkey, NULL);\n    if (ktri->pctx == NULL)\n        return 0;\n\n    if (EVP_PKEY_decrypt_init(ktri->pctx) <= 0)\n        goto err;\n\n    if (!cms_env_asn1_ctrl(ri, 1))\n        goto err;\n\n    if (EVP_PKEY_CTX_ctrl(ktri->pctx, -1, EVP_PKEY_OP_DECRYPT,\n                          EVP_PKEY_CTRL_CMS_DECRYPT, 0, ri) <= 0) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_CTRL_ERROR);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(ktri->pctx, NULL, &eklen,\n                         ktri->encryptedKey->data,\n                         ktri->encryptedKey->length) <= 0)\n        goto err;\n\n    ek = OPENSSL_malloc(eklen);\n\n    if (ek == NULL) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(ktri->pctx, ek, &eklen,\n                         ktri->encryptedKey->data,\n                         ktri->encryptedKey->length) <= 0) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_CMS_LIB);\n        goto err;\n    }\n\n    ret = 1;\n\n    OPENSSL_clear_free(ec->key, ec->keylen);\n    ec->key = ek;\n    ec->keylen = eklen;\n\n err:\n    EVP_PKEY_CTX_free(ktri->pctx);\n    ktri->pctx = NULL;\n    if (!ret)\n        OPENSSL_free(ek);\n\n    return ret;\n}",
        "func": "static int cms_RecipientInfo_ktri_decrypt(CMS_ContentInfo *cms,\n                                          CMS_RecipientInfo *ri)\n{\n    CMS_KeyTransRecipientInfo *ktri = ri->d.ktri;\n    EVP_PKEY *pkey = ktri->pkey;\n    unsigned char *ek = NULL;\n    size_t eklen;\n    int ret = 0;\n    size_t fixlen = 0;\n    CMS_EncryptedContentInfo *ec;\n    ec = cms->d.envelopedData->encryptedContentInfo;\n\n    if (ktri->pkey == NULL) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_NO_PRIVATE_KEY);\n        return 0;\n    }\n\n    if (cms->d.envelopedData->encryptedContentInfo->havenocert\n            && !cms->d.envelopedData->encryptedContentInfo->debug) {\n        X509_ALGOR *calg = ec->contentEncryptionAlgorithm;\n        const EVP_CIPHER *ciph = EVP_get_cipherbyobj(calg->algorithm);\n\n        if (ciph == NULL) {\n            CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_UNKNOWN_CIPHER);\n            return 0;\n        }\n\n        fixlen = EVP_CIPHER_key_length(ciph);\n    }\n\n    ktri->pctx = EVP_PKEY_CTX_new(pkey, NULL);\n    if (ktri->pctx == NULL)\n        return 0;\n\n    if (EVP_PKEY_decrypt_init(ktri->pctx) <= 0)\n        goto err;\n\n    if (!cms_env_asn1_ctrl(ri, 1))\n        goto err;\n\n    if (EVP_PKEY_CTX_ctrl(ktri->pctx, -1, EVP_PKEY_OP_DECRYPT,\n                          EVP_PKEY_CTRL_CMS_DECRYPT, 0, ri) <= 0) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_CTRL_ERROR);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(ktri->pctx, NULL, &eklen,\n                         ktri->encryptedKey->data,\n                         ktri->encryptedKey->length) <= 0)\n        goto err;\n\n    ek = OPENSSL_malloc(eklen);\n\n    if (ek == NULL) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(ktri->pctx, ek, &eklen,\n                         ktri->encryptedKey->data,\n                         ktri->encryptedKey->length) <= 0\n            || eklen == 0\n            || (fixlen != 0 && eklen != fixlen)) {\n        CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_CMS_LIB);\n        goto err;\n    }\n\n    ret = 1;\n\n    OPENSSL_clear_free(ec->key, ec->keylen);\n    ec->key = ek;\n    ec->keylen = eklen;\n\n err:\n    EVP_PKEY_CTX_free(ktri->pctx);\n    ktri->pctx = NULL;\n    if (!ret)\n        OPENSSL_free(ek);\n\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,12 +6,26 @@\n     unsigned char *ek = NULL;\n     size_t eklen;\n     int ret = 0;\n+    size_t fixlen = 0;\n     CMS_EncryptedContentInfo *ec;\n     ec = cms->d.envelopedData->encryptedContentInfo;\n \n     if (ktri->pkey == NULL) {\n         CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_NO_PRIVATE_KEY);\n         return 0;\n+    }\n+\n+    if (cms->d.envelopedData->encryptedContentInfo->havenocert\n+            && !cms->d.envelopedData->encryptedContentInfo->debug) {\n+        X509_ALGOR *calg = ec->contentEncryptionAlgorithm;\n+        const EVP_CIPHER *ciph = EVP_get_cipherbyobj(calg->algorithm);\n+\n+        if (ciph == NULL) {\n+            CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_UNKNOWN_CIPHER);\n+            return 0;\n+        }\n+\n+        fixlen = EVP_CIPHER_key_length(ciph);\n     }\n \n     ktri->pctx = EVP_PKEY_CTX_new(pkey, NULL);\n@@ -44,7 +58,9 @@\n \n     if (EVP_PKEY_decrypt(ktri->pctx, ek, &eklen,\n                          ktri->encryptedKey->data,\n-                         ktri->encryptedKey->length) <= 0) {\n+                         ktri->encryptedKey->length) <= 0\n+            || eklen == 0\n+            || (fixlen != 0 && eklen != fixlen)) {\n         CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_CMS_LIB);\n         goto err;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "                         ktri->encryptedKey->length) <= 0) {"
            ],
            "added_lines": [
                "    size_t fixlen = 0;",
                "    }",
                "",
                "    if (cms->d.envelopedData->encryptedContentInfo->havenocert",
                "            && !cms->d.envelopedData->encryptedContentInfo->debug) {",
                "        X509_ALGOR *calg = ec->contentEncryptionAlgorithm;",
                "        const EVP_CIPHER *ciph = EVP_get_cipherbyobj(calg->algorithm);",
                "",
                "        if (ciph == NULL) {",
                "            CMSerr(CMS_F_CMS_RECIPIENTINFO_KTRI_DECRYPT, CMS_R_UNKNOWN_CIPHER);",
                "            return 0;",
                "        }",
                "",
                "        fixlen = EVP_CIPHER_key_length(ciph);",
                "                         ktri->encryptedKey->length) <= 0",
                "            || eklen == 0",
                "            || (fixlen != 0 && eklen != fixlen)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1563",
        "func_name": "openssl/CMS_decrypt",
        "description": "In situations where an attacker receives automated notification of the success or failure of a decryption attempt an attacker, after sending a very large number of messages to be decrypted, can recover a CMS/PKCS7 transported encryption key or decrypt any RSA encrypted message that was encrypted with the public RSA key, using a Bleichenbacher padding oracle attack. Applications are not affected if they use a certificate together with the private RSA key to the CMS_decrypt or PKCS7_decrypt functions to select the correct recipient info to decrypt. Fixed in OpenSSL 1.1.1d (Affected 1.1.1-1.1.1c). Fixed in OpenSSL 1.1.0l (Affected 1.1.0-1.1.0k). Fixed in OpenSSL 1.0.2t (Affected 1.0.2-1.0.2s).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=631f94db0065c78181ca9ba5546ebc8bb3884b97",
        "commit_title": "",
        "commit_text": "Fix a padding oracle in PKCS7_dataDecode and CMS_decrypt_set1_pkey  An attack is simple, if the first CMS_recipientInfo is valid but the second CMS_recipientInfo is chosen ciphertext. If the second recipientInfo decodes to PKCS #1 v1.5 form plaintext, the correct encryption key will be replaced by garbage, and the message cannot be decoded, but if the RSA decryption fails, the correct encryption key is used and the recipient will not notice the attack.  As a work around for this potential attack the length of the decrypted key must be equal to the cipher default key length, in case the certifiate is not given and all recipientInfo are tried out.  The old behaviour can be re-enabled in the CMS code by setting the CMS_DEBUG_DECRYPT flag.  (Merged from https://github.com/openssl/openssl/pull/9777)  (cherry picked from commit 5840ed0cd1e6487d247efbc1a04136a41d7b3a37) ",
        "func_before": "int CMS_decrypt(CMS_ContentInfo *cms, EVP_PKEY *pk, X509 *cert,\n                BIO *dcont, BIO *out, unsigned int flags)\n{\n    int r;\n    BIO *cont;\n    if (OBJ_obj2nid(CMS_get0_type(cms)) != NID_pkcs7_enveloped) {\n        CMSerr(CMS_F_CMS_DECRYPT, CMS_R_TYPE_NOT_ENVELOPED_DATA);\n        return 0;\n    }\n    if (!dcont && !check_content(cms))\n        return 0;\n    if (flags & CMS_DEBUG_DECRYPT)\n        cms->d.envelopedData->encryptedContentInfo->debug = 1;\n    else\n        cms->d.envelopedData->encryptedContentInfo->debug = 0;\n    if (!pk && !cert && !dcont && !out)\n        return 1;\n    if (pk && !CMS_decrypt_set1_pkey(cms, pk, cert))\n        return 0;\n    cont = CMS_dataInit(cms, dcont);\n    if (!cont)\n        return 0;\n    r = cms_copy_content(out, cont, flags);\n    do_free_upto(cont, dcont);\n    return r;\n}",
        "func": "int CMS_decrypt(CMS_ContentInfo *cms, EVP_PKEY *pk, X509 *cert,\n                BIO *dcont, BIO *out, unsigned int flags)\n{\n    int r;\n    BIO *cont;\n    if (OBJ_obj2nid(CMS_get0_type(cms)) != NID_pkcs7_enveloped) {\n        CMSerr(CMS_F_CMS_DECRYPT, CMS_R_TYPE_NOT_ENVELOPED_DATA);\n        return 0;\n    }\n    if (!dcont && !check_content(cms))\n        return 0;\n    if (flags & CMS_DEBUG_DECRYPT)\n        cms->d.envelopedData->encryptedContentInfo->debug = 1;\n    else\n        cms->d.envelopedData->encryptedContentInfo->debug = 0;\n    if (!cert)\n        cms->d.envelopedData->encryptedContentInfo->havenocert = 1;\n    else\n        cms->d.envelopedData->encryptedContentInfo->havenocert = 0;\n    if (!pk && !cert && !dcont && !out)\n        return 1;\n    if (pk && !CMS_decrypt_set1_pkey(cms, pk, cert))\n        return 0;\n    cont = CMS_dataInit(cms, dcont);\n    if (!cont)\n        return 0;\n    r = cms_copy_content(out, cont, flags);\n    do_free_upto(cont, dcont);\n    return r;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,6 +13,10 @@\n         cms->d.envelopedData->encryptedContentInfo->debug = 1;\n     else\n         cms->d.envelopedData->encryptedContentInfo->debug = 0;\n+    if (!cert)\n+        cms->d.envelopedData->encryptedContentInfo->havenocert = 1;\n+    else\n+        cms->d.envelopedData->encryptedContentInfo->havenocert = 0;\n     if (!pk && !cert && !dcont && !out)\n         return 1;\n     if (pk && !CMS_decrypt_set1_pkey(cms, pk, cert))",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if (!cert)",
                "        cms->d.envelopedData->encryptedContentInfo->havenocert = 1;",
                "    else",
                "        cms->d.envelopedData->encryptedContentInfo->havenocert = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1563",
        "func_name": "openssl/pkcs7_decrypt_rinfo",
        "description": "In situations where an attacker receives automated notification of the success or failure of a decryption attempt an attacker, after sending a very large number of messages to be decrypted, can recover a CMS/PKCS7 transported encryption key or decrypt any RSA encrypted message that was encrypted with the public RSA key, using a Bleichenbacher padding oracle attack. Applications are not affected if they use a certificate together with the private RSA key to the CMS_decrypt or PKCS7_decrypt functions to select the correct recipient info to decrypt. Fixed in OpenSSL 1.1.1d (Affected 1.1.1-1.1.1c). Fixed in OpenSSL 1.1.0l (Affected 1.1.0-1.1.0k). Fixed in OpenSSL 1.0.2t (Affected 1.0.2-1.0.2s).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=631f94db0065c78181ca9ba5546ebc8bb3884b97",
        "commit_title": "",
        "commit_text": "Fix a padding oracle in PKCS7_dataDecode and CMS_decrypt_set1_pkey  An attack is simple, if the first CMS_recipientInfo is valid but the second CMS_recipientInfo is chosen ciphertext. If the second recipientInfo decodes to PKCS #1 v1.5 form plaintext, the correct encryption key will be replaced by garbage, and the message cannot be decoded, but if the RSA decryption fails, the correct encryption key is used and the recipient will not notice the attack.  As a work around for this potential attack the length of the decrypted key must be equal to the cipher default key length, in case the certifiate is not given and all recipientInfo are tried out.  The old behaviour can be re-enabled in the CMS code by setting the CMS_DEBUG_DECRYPT flag.  (Merged from https://github.com/openssl/openssl/pull/9777)  (cherry picked from commit 5840ed0cd1e6487d247efbc1a04136a41d7b3a37) ",
        "func_before": "static int pkcs7_decrypt_rinfo(unsigned char **pek, int *peklen,\n                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey)\n{\n    EVP_PKEY_CTX *pctx = NULL;\n    unsigned char *ek = NULL;\n    size_t eklen;\n\n    int ret = -1;\n\n    pctx = EVP_PKEY_CTX_new(pkey, NULL);\n    if (!pctx)\n        return -1;\n\n    if (EVP_PKEY_decrypt_init(pctx) <= 0)\n        goto err;\n\n    if (EVP_PKEY_CTX_ctrl(pctx, -1, EVP_PKEY_OP_DECRYPT,\n                          EVP_PKEY_CTRL_PKCS7_DECRYPT, 0, ri) <= 0) {\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, PKCS7_R_CTRL_ERROR);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(pctx, NULL, &eklen,\n                         ri->enc_key->data, ri->enc_key->length) <= 0)\n        goto err;\n\n    ek = OPENSSL_malloc(eklen);\n\n    if (ek == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(pctx, ek, &eklen,\n                         ri->enc_key->data, ri->enc_key->length) <= 0) {\n        ret = 0;\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, ERR_R_EVP_LIB);\n        goto err;\n    }\n\n    ret = 1;\n\n    OPENSSL_clear_free(*pek, *peklen);\n    *pek = ek;\n    *peklen = eklen;\n\n err:\n    EVP_PKEY_CTX_free(pctx);\n    if (!ret)\n        OPENSSL_free(ek);\n\n    return ret;\n}",
        "func": "static int pkcs7_decrypt_rinfo(unsigned char **pek, int *peklen,\n                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey,\n                               size_t fixlen)\n{\n    EVP_PKEY_CTX *pctx = NULL;\n    unsigned char *ek = NULL;\n    size_t eklen;\n\n    int ret = -1;\n\n    pctx = EVP_PKEY_CTX_new(pkey, NULL);\n    if (!pctx)\n        return -1;\n\n    if (EVP_PKEY_decrypt_init(pctx) <= 0)\n        goto err;\n\n    if (EVP_PKEY_CTX_ctrl(pctx, -1, EVP_PKEY_OP_DECRYPT,\n                          EVP_PKEY_CTRL_PKCS7_DECRYPT, 0, ri) <= 0) {\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, PKCS7_R_CTRL_ERROR);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(pctx, NULL, &eklen,\n                         ri->enc_key->data, ri->enc_key->length) <= 0)\n        goto err;\n\n    ek = OPENSSL_malloc(eklen);\n\n    if (ek == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    if (EVP_PKEY_decrypt(pctx, ek, &eklen,\n                         ri->enc_key->data, ri->enc_key->length) <= 0\n            || eklen == 0\n            || (fixlen != 0 && eklen != fixlen)) {\n        ret = 0;\n        PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, ERR_R_EVP_LIB);\n        goto err;\n    }\n\n    ret = 1;\n\n    OPENSSL_clear_free(*pek, *peklen);\n    *pek = ek;\n    *peklen = eklen;\n\n err:\n    EVP_PKEY_CTX_free(pctx);\n    if (!ret)\n        OPENSSL_free(ek);\n\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,6 @@\n static int pkcs7_decrypt_rinfo(unsigned char **pek, int *peklen,\n-                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey)\n+                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey,\n+                               size_t fixlen)\n {\n     EVP_PKEY_CTX *pctx = NULL;\n     unsigned char *ek = NULL;\n@@ -32,7 +33,9 @@\n     }\n \n     if (EVP_PKEY_decrypt(pctx, ek, &eklen,\n-                         ri->enc_key->data, ri->enc_key->length) <= 0) {\n+                         ri->enc_key->data, ri->enc_key->length) <= 0\n+            || eklen == 0\n+            || (fixlen != 0 && eklen != fixlen)) {\n         ret = 0;\n         PKCS7err(PKCS7_F_PKCS7_DECRYPT_RINFO, ERR_R_EVP_LIB);\n         goto err;",
        "diff_line_info": {
            "deleted_lines": [
                "                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey)",
                "                         ri->enc_key->data, ri->enc_key->length) <= 0) {"
            ],
            "added_lines": [
                "                               PKCS7_RECIP_INFO *ri, EVP_PKEY *pkey,",
                "                               size_t fixlen)",
                "                         ri->enc_key->data, ri->enc_key->length) <= 0",
                "            || eklen == 0",
                "            || (fixlen != 0 && eklen != fixlen)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1563",
        "func_name": "openssl/PKCS7_dataDecode",
        "description": "In situations where an attacker receives automated notification of the success or failure of a decryption attempt an attacker, after sending a very large number of messages to be decrypted, can recover a CMS/PKCS7 transported encryption key or decrypt any RSA encrypted message that was encrypted with the public RSA key, using a Bleichenbacher padding oracle attack. Applications are not affected if they use a certificate together with the private RSA key to the CMS_decrypt or PKCS7_decrypt functions to select the correct recipient info to decrypt. Fixed in OpenSSL 1.1.1d (Affected 1.1.1-1.1.1c). Fixed in OpenSSL 1.1.0l (Affected 1.1.0-1.1.0k). Fixed in OpenSSL 1.0.2t (Affected 1.0.2-1.0.2s).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=631f94db0065c78181ca9ba5546ebc8bb3884b97",
        "commit_title": "",
        "commit_text": "Fix a padding oracle in PKCS7_dataDecode and CMS_decrypt_set1_pkey  An attack is simple, if the first CMS_recipientInfo is valid but the second CMS_recipientInfo is chosen ciphertext. If the second recipientInfo decodes to PKCS #1 v1.5 form plaintext, the correct encryption key will be replaced by garbage, and the message cannot be decoded, but if the RSA decryption fails, the correct encryption key is used and the recipient will not notice the attack.  As a work around for this potential attack the length of the decrypted key must be equal to the cipher default key length, in case the certifiate is not given and all recipientInfo are tried out.  The old behaviour can be re-enabled in the CMS code by setting the CMS_DEBUG_DECRYPT flag.  (Merged from https://github.com/openssl/openssl/pull/9777)  (cherry picked from commit 5840ed0cd1e6487d247efbc1a04136a41d7b3a37) ",
        "func_before": "BIO *PKCS7_dataDecode(PKCS7 *p7, EVP_PKEY *pkey, BIO *in_bio, X509 *pcert)\n{\n    int i, j;\n    BIO *out = NULL, *btmp = NULL, *etmp = NULL, *bio = NULL;\n    X509_ALGOR *xa;\n    ASN1_OCTET_STRING *data_body = NULL;\n    const EVP_MD *evp_md;\n    const EVP_CIPHER *evp_cipher = NULL;\n    EVP_CIPHER_CTX *evp_ctx = NULL;\n    X509_ALGOR *enc_alg = NULL;\n    STACK_OF(X509_ALGOR) *md_sk = NULL;\n    STACK_OF(PKCS7_RECIP_INFO) *rsk = NULL;\n    PKCS7_RECIP_INFO *ri = NULL;\n    unsigned char *ek = NULL, *tkey = NULL;\n    int eklen = 0, tkeylen = 0;\n\n    if (p7 == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_INVALID_NULL_POINTER);\n        return NULL;\n    }\n\n    if (p7->d.ptr == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_NO_CONTENT);\n        return NULL;\n    }\n\n    i = OBJ_obj2nid(p7->type);\n    p7->state = PKCS7_S_HEADER;\n\n    switch (i) {\n    case NID_pkcs7_signed:\n        /*\n         * p7->d.sign->contents is a PKCS7 structure consisting of a contentType\n         * field and optional content.\n         * data_body is NULL if that structure has no (=detached) content\n         * or if the contentType is wrong (i.e., not \"data\").\n         */\n        data_body = PKCS7_get_octet_string(p7->d.sign->contents);\n        if (!PKCS7_is_detached(p7) && data_body == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_INVALID_SIGNED_DATA_TYPE);\n            goto err;\n        }\n        md_sk = p7->d.sign->md_algs;\n        break;\n    case NID_pkcs7_signedAndEnveloped:\n        rsk = p7->d.signed_and_enveloped->recipientinfo;\n        md_sk = p7->d.signed_and_enveloped->md_algs;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.signed_and_enveloped->enc_data->enc_data;\n        enc_alg = p7->d.signed_and_enveloped->enc_data->algorithm;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    case NID_pkcs7_enveloped:\n        rsk = p7->d.enveloped->recipientinfo;\n        enc_alg = p7->d.enveloped->enc_data->algorithm;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.enveloped->enc_data->enc_data;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    default:\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_UNSUPPORTED_CONTENT_TYPE);\n        goto err;\n    }\n\n    /* Detached content must be supplied via in_bio instead. */\n    if (data_body == NULL && in_bio == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_NO_CONTENT);\n        goto err;\n    }\n\n    /* We will be checking the signature */\n    if (md_sk != NULL) {\n        for (i = 0; i < sk_X509_ALGOR_num(md_sk); i++) {\n            xa = sk_X509_ALGOR_value(md_sk, i);\n            if ((btmp = BIO_new(BIO_f_md())) == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE, ERR_R_BIO_LIB);\n                goto err;\n            }\n\n            j = OBJ_obj2nid(xa->algorithm);\n            evp_md = EVP_get_digestbynid(j);\n            if (evp_md == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                         PKCS7_R_UNKNOWN_DIGEST_TYPE);\n                goto err;\n            }\n\n            BIO_set_md(btmp, evp_md);\n            if (out == NULL)\n                out = btmp;\n            else\n                BIO_push(out, btmp);\n            btmp = NULL;\n        }\n    }\n\n    if (evp_cipher != NULL) {\n        if ((etmp = BIO_new(BIO_f_cipher())) == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE, ERR_R_BIO_LIB);\n            goto err;\n        }\n\n        /*\n         * It was encrypted, we need to decrypt the secret key with the\n         * private key\n         */\n\n        /*\n         * Find the recipientInfo which matches the passed certificate (if\n         * any)\n         */\n\n        if (pcert) {\n            for (i = 0; i < sk_PKCS7_RECIP_INFO_num(rsk); i++) {\n                ri = sk_PKCS7_RECIP_INFO_value(rsk, i);\n                if (!pkcs7_cmp_ri(ri, pcert))\n                    break;\n                ri = NULL;\n            }\n            if (ri == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                         PKCS7_R_NO_RECIPIENT_MATCHES_CERTIFICATE);\n                goto err;\n            }\n        }\n\n        /* If we haven't got a certificate try each ri in turn */\n        if (pcert == NULL) {\n            /*\n             * Always attempt to decrypt all rinfo even after success as a\n             * defence against MMA timing attacks.\n             */\n            for (i = 0; i < sk_PKCS7_RECIP_INFO_num(rsk); i++) {\n                ri = sk_PKCS7_RECIP_INFO_value(rsk, i);\n\n                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)\n                    goto err;\n                ERR_clear_error();\n            }\n        } else {\n            /* Only exit on fatal errors, not decrypt failure */\n            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)\n                goto err;\n            ERR_clear_error();\n        }\n\n        evp_ctx = NULL;\n        BIO_get_cipher_ctx(etmp, &evp_ctx);\n        if (EVP_CipherInit_ex(evp_ctx, evp_cipher, NULL, NULL, NULL, 0) <= 0)\n            goto err;\n        if (EVP_CIPHER_asn1_to_param(evp_ctx, enc_alg->parameter) < 0)\n            goto err;\n        /* Generate random key as MMA defence */\n        tkeylen = EVP_CIPHER_CTX_key_length(evp_ctx);\n        tkey = OPENSSL_malloc(tkeylen);\n        if (tkey == NULL)\n            goto err;\n        if (EVP_CIPHER_CTX_rand_key(evp_ctx, tkey) <= 0)\n            goto err;\n        if (ek == NULL) {\n            ek = tkey;\n            eklen = tkeylen;\n            tkey = NULL;\n        }\n\n        if (eklen != EVP_CIPHER_CTX_key_length(evp_ctx)) {\n            /*\n             * Some S/MIME clients don't use the same key and effective key\n             * length. The key length is determined by the size of the\n             * decrypted RSA key.\n             */\n            if (!EVP_CIPHER_CTX_set_key_length(evp_ctx, eklen)) {\n                /* Use random key as MMA defence */\n                OPENSSL_clear_free(ek, eklen);\n                ek = tkey;\n                eklen = tkeylen;\n                tkey = NULL;\n            }\n        }\n        /* Clear errors so we don't leak information useful in MMA */\n        ERR_clear_error();\n        if (EVP_CipherInit_ex(evp_ctx, NULL, NULL, ek, NULL, 0) <= 0)\n            goto err;\n\n        OPENSSL_clear_free(ek, eklen);\n        ek = NULL;\n        OPENSSL_clear_free(tkey, tkeylen);\n        tkey = NULL;\n\n        if (out == NULL)\n            out = etmp;\n        else\n            BIO_push(out, etmp);\n        etmp = NULL;\n    }\n    if (in_bio != NULL) {\n        bio = in_bio;\n    } else {\n        if (data_body->length > 0)\n            bio = BIO_new_mem_buf(data_body->data, data_body->length);\n        else {\n            bio = BIO_new(BIO_s_mem());\n            if (bio == NULL)\n                goto err;\n            BIO_set_mem_eof_return(bio, 0);\n        }\n        if (bio == NULL)\n            goto err;\n    }\n    BIO_push(out, bio);\n    bio = NULL;\n    return out;\n\n err:\n    OPENSSL_clear_free(ek, eklen);\n    OPENSSL_clear_free(tkey, tkeylen);\n    BIO_free_all(out);\n    BIO_free_all(btmp);\n    BIO_free_all(etmp);\n    BIO_free_all(bio);\n    return NULL;\n}",
        "func": "BIO *PKCS7_dataDecode(PKCS7 *p7, EVP_PKEY *pkey, BIO *in_bio, X509 *pcert)\n{\n    int i, j;\n    BIO *out = NULL, *btmp = NULL, *etmp = NULL, *bio = NULL;\n    X509_ALGOR *xa;\n    ASN1_OCTET_STRING *data_body = NULL;\n    const EVP_MD *evp_md;\n    const EVP_CIPHER *evp_cipher = NULL;\n    EVP_CIPHER_CTX *evp_ctx = NULL;\n    X509_ALGOR *enc_alg = NULL;\n    STACK_OF(X509_ALGOR) *md_sk = NULL;\n    STACK_OF(PKCS7_RECIP_INFO) *rsk = NULL;\n    PKCS7_RECIP_INFO *ri = NULL;\n    unsigned char *ek = NULL, *tkey = NULL;\n    int eklen = 0, tkeylen = 0;\n\n    if (p7 == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_INVALID_NULL_POINTER);\n        return NULL;\n    }\n\n    if (p7->d.ptr == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_NO_CONTENT);\n        return NULL;\n    }\n\n    i = OBJ_obj2nid(p7->type);\n    p7->state = PKCS7_S_HEADER;\n\n    switch (i) {\n    case NID_pkcs7_signed:\n        /*\n         * p7->d.sign->contents is a PKCS7 structure consisting of a contentType\n         * field and optional content.\n         * data_body is NULL if that structure has no (=detached) content\n         * or if the contentType is wrong (i.e., not \"data\").\n         */\n        data_body = PKCS7_get_octet_string(p7->d.sign->contents);\n        if (!PKCS7_is_detached(p7) && data_body == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_INVALID_SIGNED_DATA_TYPE);\n            goto err;\n        }\n        md_sk = p7->d.sign->md_algs;\n        break;\n    case NID_pkcs7_signedAndEnveloped:\n        rsk = p7->d.signed_and_enveloped->recipientinfo;\n        md_sk = p7->d.signed_and_enveloped->md_algs;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.signed_and_enveloped->enc_data->enc_data;\n        enc_alg = p7->d.signed_and_enveloped->enc_data->algorithm;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    case NID_pkcs7_enveloped:\n        rsk = p7->d.enveloped->recipientinfo;\n        enc_alg = p7->d.enveloped->enc_data->algorithm;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.enveloped->enc_data->enc_data;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    default:\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_UNSUPPORTED_CONTENT_TYPE);\n        goto err;\n    }\n\n    /* Detached content must be supplied via in_bio instead. */\n    if (data_body == NULL && in_bio == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_NO_CONTENT);\n        goto err;\n    }\n\n    /* We will be checking the signature */\n    if (md_sk != NULL) {\n        for (i = 0; i < sk_X509_ALGOR_num(md_sk); i++) {\n            xa = sk_X509_ALGOR_value(md_sk, i);\n            if ((btmp = BIO_new(BIO_f_md())) == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE, ERR_R_BIO_LIB);\n                goto err;\n            }\n\n            j = OBJ_obj2nid(xa->algorithm);\n            evp_md = EVP_get_digestbynid(j);\n            if (evp_md == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                         PKCS7_R_UNKNOWN_DIGEST_TYPE);\n                goto err;\n            }\n\n            BIO_set_md(btmp, evp_md);\n            if (out == NULL)\n                out = btmp;\n            else\n                BIO_push(out, btmp);\n            btmp = NULL;\n        }\n    }\n\n    if (evp_cipher != NULL) {\n        if ((etmp = BIO_new(BIO_f_cipher())) == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE, ERR_R_BIO_LIB);\n            goto err;\n        }\n\n        /*\n         * It was encrypted, we need to decrypt the secret key with the\n         * private key\n         */\n\n        /*\n         * Find the recipientInfo which matches the passed certificate (if\n         * any)\n         */\n\n        if (pcert) {\n            for (i = 0; i < sk_PKCS7_RECIP_INFO_num(rsk); i++) {\n                ri = sk_PKCS7_RECIP_INFO_value(rsk, i);\n                if (!pkcs7_cmp_ri(ri, pcert))\n                    break;\n                ri = NULL;\n            }\n            if (ri == NULL) {\n                PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                         PKCS7_R_NO_RECIPIENT_MATCHES_CERTIFICATE);\n                goto err;\n            }\n        }\n\n        /* If we haven't got a certificate try each ri in turn */\n        if (pcert == NULL) {\n            /*\n             * Always attempt to decrypt all rinfo even after success as a\n             * defence against MMA timing attacks.\n             */\n            for (i = 0; i < sk_PKCS7_RECIP_INFO_num(rsk); i++) {\n                ri = sk_PKCS7_RECIP_INFO_value(rsk, i);\n\n                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey,\n                        EVP_CIPHER_key_length(evp_cipher)) < 0)\n                    goto err;\n                ERR_clear_error();\n            }\n        } else {\n            /* Only exit on fatal errors, not decrypt failure */\n            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey, 0) < 0)\n                goto err;\n            ERR_clear_error();\n        }\n\n        evp_ctx = NULL;\n        BIO_get_cipher_ctx(etmp, &evp_ctx);\n        if (EVP_CipherInit_ex(evp_ctx, evp_cipher, NULL, NULL, NULL, 0) <= 0)\n            goto err;\n        if (EVP_CIPHER_asn1_to_param(evp_ctx, enc_alg->parameter) < 0)\n            goto err;\n        /* Generate random key as MMA defence */\n        tkeylen = EVP_CIPHER_CTX_key_length(evp_ctx);\n        tkey = OPENSSL_malloc(tkeylen);\n        if (tkey == NULL)\n            goto err;\n        if (EVP_CIPHER_CTX_rand_key(evp_ctx, tkey) <= 0)\n            goto err;\n        if (ek == NULL) {\n            ek = tkey;\n            eklen = tkeylen;\n            tkey = NULL;\n        }\n\n        if (eklen != EVP_CIPHER_CTX_key_length(evp_ctx)) {\n            /*\n             * Some S/MIME clients don't use the same key and effective key\n             * length. The key length is determined by the size of the\n             * decrypted RSA key.\n             */\n            if (!EVP_CIPHER_CTX_set_key_length(evp_ctx, eklen)) {\n                /* Use random key as MMA defence */\n                OPENSSL_clear_free(ek, eklen);\n                ek = tkey;\n                eklen = tkeylen;\n                tkey = NULL;\n            }\n        }\n        /* Clear errors so we don't leak information useful in MMA */\n        ERR_clear_error();\n        if (EVP_CipherInit_ex(evp_ctx, NULL, NULL, ek, NULL, 0) <= 0)\n            goto err;\n\n        OPENSSL_clear_free(ek, eklen);\n        ek = NULL;\n        OPENSSL_clear_free(tkey, tkeylen);\n        tkey = NULL;\n\n        if (out == NULL)\n            out = etmp;\n        else\n            BIO_push(out, etmp);\n        etmp = NULL;\n    }\n    if (in_bio != NULL) {\n        bio = in_bio;\n    } else {\n        if (data_body->length > 0)\n            bio = BIO_new_mem_buf(data_body->data, data_body->length);\n        else {\n            bio = BIO_new(BIO_s_mem());\n            if (bio == NULL)\n                goto err;\n            BIO_set_mem_eof_return(bio, 0);\n        }\n        if (bio == NULL)\n            goto err;\n    }\n    BIO_push(out, bio);\n    bio = NULL;\n    return out;\n\n err:\n    OPENSSL_clear_free(ek, eklen);\n    OPENSSL_clear_free(tkey, tkeylen);\n    BIO_free_all(out);\n    BIO_free_all(btmp);\n    BIO_free_all(etmp);\n    BIO_free_all(bio);\n    return NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -144,13 +144,14 @@\n             for (i = 0; i < sk_PKCS7_RECIP_INFO_num(rsk); i++) {\n                 ri = sk_PKCS7_RECIP_INFO_value(rsk, i);\n \n-                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)\n+                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey,\n+                        EVP_CIPHER_key_length(evp_cipher)) < 0)\n                     goto err;\n                 ERR_clear_error();\n             }\n         } else {\n             /* Only exit on fatal errors, not decrypt failure */\n-            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)\n+            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey, 0) < 0)\n                 goto err;\n             ERR_clear_error();\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)",
                "            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey) < 0)"
            ],
            "added_lines": [
                "                if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey,",
                "                        EVP_CIPHER_key_length(evp_cipher)) < 0)",
                "            if (pkcs7_decrypt_rinfo(&ek, &eklen, ri, pkey, 0) < 0)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11683",
        "func_name": "linux4sam/at91bootstrap/secure_decrypt",
        "description": "A timing side channel was discovered in AT91bootstrap before 3.9.2. It can be exploited by attackers with physical access to forge CMAC values and subsequently boot arbitrary code on an affected system.",
        "git_url": "https://github.com/linux4sam/at91bootstrap/commit/7753914c9a622c245f3a3cf2af5e24b6a9904213",
        "commit_title": "driver: secure: use consttime_memequal for memory comparison",
        "commit_text": " Do  not  use memcmp() to compare security critical data, such as cryptographic secrets, because the required CPU time depends on the number of equal bytes. Instead, a function that performs comparisons in constant time is required. Warning: consttime_memequal returns 0 if data are NOT equal, and 1 if they are equal. ",
        "func_before": "int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n{\n\tat91_aes_key_size_t key_size;\n\tunsigned int cmac_key[8], cipher_key[8];\n\tunsigned int iv[AT91_AES_IV_SIZE_WORD];\n\tunsigned int computed_cmac[AT91_AES_BLOCK_SIZE_WORD];\n\tunsigned int fixed_length;\n\tconst unsigned int *cmac;\n\tint rc = -1;\n\n\t/* Init keys */\n\tinit_keys(&key_size, cipher_key, cmac_key, iv);\n\n\t/* Init periph */\n\tat91_aes_init();\n\n\t/* Check signature if required */\n\tif (is_signed) {\n\t\t/* Compute the CMAC */\n\t\tif (at91_aes_cmac(data_length, data, computed_cmac,\n\t\t\t\t  key_size, cmac_key))\n\t\t\tgoto exit;\n\n\t\t/* Check the CMAC */\n\t\tfixed_length = at91_aes_roundup(data_length);\n\t\tcmac = (const unsigned int *)((char *)data + fixed_length);\n\t\tif (memcmp(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n\t\t\tgoto exit;\n\t}\n\n\t/* Decrypt the whole file */\n\tif (at91_aes_cbc(data_length, data, data, 0,\n\t\t\t key_size, cipher_key, iv))\n\t\tgoto exit;\n\n\trc = 0;\nexit:\n\t/* Reset periph */\n\tat91_aes_cleanup();\n\n\t/* Reset keys */\n\tmemset(cmac_key, 0, sizeof(cmac_key));\n\tmemset(cipher_key, 0, sizeof(cipher_key));\n\tmemset(iv, 0, sizeof(iv));\n\n\treturn rc;\n}",
        "func": "int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n{\n\tat91_aes_key_size_t key_size;\n\tunsigned int cmac_key[8], cipher_key[8];\n\tunsigned int iv[AT91_AES_IV_SIZE_WORD];\n\tunsigned int computed_cmac[AT91_AES_BLOCK_SIZE_WORD];\n\tunsigned int fixed_length;\n\tconst unsigned int *cmac;\n\tint rc = -1;\n\n\t/* Init keys */\n\tinit_keys(&key_size, cipher_key, cmac_key, iv);\n\n\t/* Init periph */\n\tat91_aes_init();\n\n\t/* Check signature if required */\n\tif (is_signed) {\n\t\t/* Compute the CMAC */\n\t\tif (at91_aes_cmac(data_length, data, computed_cmac,\n\t\t\t\t  key_size, cmac_key))\n\t\t\tgoto exit;\n\n\t\t/* Check the CMAC */\n\t\tfixed_length = at91_aes_roundup(data_length);\n\t\tcmac = (const unsigned int *)((char *)data + fixed_length);\n\t\tif (!consttime_memequal(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n\t\t\tgoto exit;\n\t}\n\n\t/* Decrypt the whole file */\n\tif (at91_aes_cbc(data_length, data, data, 0,\n\t\t\t key_size, cipher_key, iv))\n\t\tgoto exit;\n\n\trc = 0;\nexit:\n\t/* Reset periph */\n\tat91_aes_cleanup();\n\n\t/* Reset keys */\n\tmemset(cmac_key, 0, sizeof(cmac_key));\n\tmemset(cipher_key, 0, sizeof(cipher_key));\n\tmemset(iv, 0, sizeof(iv));\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,7 +24,7 @@\n \t\t/* Check the CMAC */\n \t\tfixed_length = at91_aes_roundup(data_length);\n \t\tcmac = (const unsigned int *)((char *)data + fixed_length);\n-\t\tif (memcmp(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n+\t\tif (!consttime_memequal(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n \t\t\tgoto exit;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (memcmp(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))"
            ],
            "added_lines": [
                "\t\tif (!consttime_memequal(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1559",
        "func_name": "openssl/dtls1_read_bytes",
        "description": "If an application encounters a fatal protocol error and then calls SSL_shutdown() twice (once to send a close_notify, and once to receive one) then OpenSSL can respond differently to the calling application if a 0 byte record is received with invalid padding compared to if a 0 byte record is received with an invalid MAC. If the application then behaves differently based on that in a way that is detectable to the remote peer, then this amounts to a padding oracle that could be used to decrypt data. In order for this to be exploitable \"non-stitched\" ciphersuites must be in use. Stitched ciphersuites are optimised implementations of certain commonly used ciphersuites. Also the application must call SSL_shutdown() twice even if a protocol error has occurred (applications should not do this but some do anyway). Fixed in OpenSSL 1.0.2r (Affected 1.0.2-1.0.2q).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=e9bbefbf0f24c57645e7ad6a5a71ae649d18ac8e",
        "commit_title": "",
        "commit_text": "Go into the error state if a fatal alert is sent or received  If an application calls SSL_shutdown after a fatal alert has occured and then behaves different based on error codes from that function then the application may be vulnerable to a padding oracle.  CVE-2019-1559  ",
        "func_before": "int dtls1_read_bytes(SSL *s, int type, unsigned char *buf, int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (s->s3->rbuf.buf == NULL) /* Not initialized yet */\n        if (!ssl3_setup_buffers(s))\n            return (-1);\n\n    /* XXX: check what the second '&& type' is about */\n    if ((type && (type != SSL3_RT_APPLICATION_DATA) &&\n         (type != SSL3_RT_HANDSHAKE) && type) ||\n        (peek && (type != SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    /*\n     * check whether there's a handshake message (client hello?) waiting\n     */\n    if ((ret = have_handshake_fragment(s, type, buf, len, peek)))\n        return ret;\n\n    /*\n     * Now s->d1->handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n#ifndef OPENSSL_NO_SCTP\n    /*\n     * Continue handshake if it had to be interrupted to read app data with\n     * SCTP.\n     */\n    if ((!s->in_handshake && SSL_in_init(s)) ||\n        (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n         (s->state == DTLS1_SCTP_ST_SR_READ_SOCK\n          || s->state == DTLS1_SCTP_ST_CR_READ_SOCK)\n         && s->s3->in_read_app_data != 2))\n#else\n    if (!s->in_handshake && SSL_in_init(s))\n#endif\n    {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = &(s->s3->rrec);\n\n    /*\n     * We are not handshaking and have no data yet, so process data buffered\n     * during the last handshake in advance, if any.\n     */\n    if (s->state == SSL_ST_OK && rr->length == 0) {\n        pitem *item;\n        item = pqueue_pop(s->d1->buffered_app_data.q);\n        if (item) {\n#ifndef OPENSSL_NO_SCTP\n            /* Restore bio_dgram_sctp_rcvinfo struct */\n            if (BIO_dgram_is_sctp(SSL_get_rbio(s))) {\n                DTLS1_RECORD_DATA *rdata = (DTLS1_RECORD_DATA *)item->data;\n                BIO_ctrl(SSL_get_rbio(s), BIO_CTRL_DGRAM_SCTP_SET_RCVINFO,\n                         sizeof(rdata->recordinfo), &rdata->recordinfo);\n            }\n#endif\n\n            dtls1_copy_record(s, item);\n\n            OPENSSL_free(item->data);\n            pitem_free(item);\n        }\n    }\n\n    /* Check for timeout */\n    if (dtls1_handle_timeout(s) > 0)\n        goto start;\n\n    /* get new packet if necessary */\n    if ((rr->length == 0) || (s->rstate == SSL_ST_READ_BODY)) {\n        ret = dtls1_get_record(s);\n        if (ret <= 0) {\n            ret = dtls1_read_failed(s, ret);\n            /* anything other than a timeout is an error */\n            if (ret <= 0)\n                return (ret);\n            else\n                goto start;\n        }\n    }\n\n    if (s->d1->listen && rr->type != SSL3_RT_HANDSHAKE) {\n        rr->length = 0;\n        goto start;\n    }\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (rr->type != SSL3_RT_ALERT && rr->length != 0)\n        s->cert->alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (rr->type != SSL3_RT_HANDSHAKE)) {\n        /*\n         * We now have application data between CCS and Finished. Most likely\n         * the packets were reordered on their way, so buffer the application\n         * data for later processing rather than dropping the connection.\n         */\n        if (dtls1_buffer_record(s, &(s->d1->buffered_app_data), rr->seq_num) <\n            0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n            return -1;\n        }\n        rr->length = 0;\n        goto start;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        rr->length = 0;\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == rr->type) {     /* SSL3_RT_APPLICATION_DATA or\n                                 * SSL3_RT_HANDSHAKE */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > rr->length)\n            n = rr->length;\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(rr->data[rr->off]), n);\n        if (!peek) {\n            rr->length -= n;\n            rr->off += n;\n            if (rr->length == 0) {\n                s->rstate = SSL_ST_READ_HEADER;\n                rr->off = 0;\n            }\n        }\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * We were about to renegotiate but had to read belated application\n         * data first, so retry.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            rr->type == SSL3_RT_APPLICATION_DATA &&\n            (s->state == DTLS1_SCTP_ST_SR_READ_SOCK\n             || s->state == DTLS1_SCTP_ST_CR_READ_SOCK)) {\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n        }\n\n        /*\n         * We might had to delay a close_notify alert because of reordered\n         * app data. If there was an alert and there is no message to read\n         * anymore, finally set shutdown.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            s->d1->shutdown_received\n            && !BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            return (0);\n        }\n#endif\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int k, dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (rr->type == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof(s->d1->handshake_fragment);\n            dest = s->d1->handshake_fragment;\n            dest_len = &s->d1->handshake_fragment_len;\n        } else if (rr->type == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->d1->alert_fragment);\n            dest = s->d1->alert_fragment;\n            dest_len = &s->d1->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (rr->type == TLS1_RT_HEARTBEAT) {\n            dtls1_process_heartbeat(s);\n\n            /* Exit and notify application to read again */\n            rr->length = 0;\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n        /* else it's a CCS message, or application data or wrong */\n        else if (rr->type != SSL3_RT_CHANGE_CIPHER_SPEC) {\n            /*\n             * Application data while renegotiating is allowed. Try again\n             * reading.\n             */\n            if (rr->type == SSL3_RT_APPLICATION_DATA) {\n                BIO *bio;\n                s->s3->in_read_app_data = 2;\n                bio = SSL_get_rbio(s);\n                s->rwstate = SSL_READING;\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n\n            /* Not certain if this is the right error handling */\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n\n        if (dest_maxlen > 0) {\n            /*\n             * XDTLS: In a pathalogical case, the Client Hello may be\n             * fragmented--don't always expect dest_maxlen bytes\n             */\n            if (rr->length < dest_maxlen) {\n#ifdef DTLS1_AD_MISSING_HANDSHAKE_MESSAGE\n                /*\n                 * for normal alerts rr->length is 2, while\n                 * dest_maxlen is 7 if we were to handle this\n                 * non-existing alert...\n                 */\n                FIX ME\n#endif\n                 s->rstate = SSL_ST_READ_HEADER;\n                rr->length = 0;\n                goto start;\n            }\n\n            /* now move 'n' bytes: */\n            for (k = 0; k < dest_maxlen; k++) {\n                dest[k] = rr->data[rr->off++];\n                rr->length--;\n            }\n            *dest_len = dest_maxlen;\n        }\n    }\n\n    /*-\n     * s->d1->handshake_fragment_len == 12  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->d1->alert_fragment_len == 7      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        (s->d1->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->d1->handshake_fragment_len = 0;\n\n        if ((s->d1->handshake_fragment[1] != 0) ||\n            (s->d1->handshake_fragment[2] != 0) ||\n            (s->d1->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        /*\n         * no need to check sequence number on HELLO REQUEST messages\n         */\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->d1->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            s->d1->handshake_read_seq++;\n            s->new_session = 1;\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_DTLS1_READ_BYTES,\n                           SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on.\n     */\n    if (s->server\n            && SSL_is_init_finished(s)\n            && !s->s3->send_connection_binding\n            && s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH\n            && s->d1->handshake_fragment[0] == SSL3_MT_CLIENT_HELLO\n            && s->s3->previous_client_finished_len != 0\n            && (s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION) == 0) {\n        s->d1->handshake_fragment_len = 0;\n        rr->length = 0;\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n\n\n    if (s->d1->alert_fragment_len >= DTLS1_AL_HEADER_LENGTH) {\n        int alert_level = s->d1->alert_fragment[0];\n        int alert_descr = s->d1->alert_fragment[1];\n\n        s->d1->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->d1->alert_fragment, 2, s, s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n\n            s->cert->alert_count++;\n            if (s->cert->alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n#ifndef OPENSSL_NO_SCTP\n                /*\n                 * With SCTP and streams the socket may deliver app data\n                 * after a close_notify alert. We have to check this first so\n                 * that nothing gets discarded.\n                 */\n                if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n                    BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n                    s->d1->shutdown_received = 1;\n                    s->rwstate = SSL_READING;\n                    BIO_clear_retry_flags(SSL_get_rbio(s));\n                    BIO_set_retry_read(SSL_get_rbio(s));\n                    return -1;\n                }\n#endif\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n#if 0\n            /* XXX: this is a possible improvement in the future */\n            /* now check if it's a missing record */\n            if (alert_descr == DTLS1_AD_MISSING_HANDSHAKE_MESSAGE) {\n                unsigned short seq;\n                unsigned int frag_off;\n                unsigned char *p = &(s->d1->alert_fragment[2]);\n\n                n2s(p, seq);\n                n2l3(p, frag_off);\n\n                dtls1_retransmit_message(s,\n                                         dtls1_get_queue_priority\n                                         (frag->msg_header.seq, 0), frag_off,\n                                         &found);\n                if (!found && SSL_in_init(s)) {\n                    /*\n                     * fprintf( stderr,\"in init = %d\\n\", SSL_in_init(s));\n                     */\n                    /*\n                     * requested a message not yet sent, send an alert\n                     * ourselves\n                     */\n                    ssl3_send_alert(s, SSL3_AL_WARNING,\n                                    DTLS1_AD_MISSING_HANDSHAKE_MESSAGE);\n                }\n            }\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_DTLS1_READ_BYTES,\n                   SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof(tmp), \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        rr->length = 0;\n        return (0);\n    }\n\n    if (rr->type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        struct ccs_header_st ccs_hdr;\n        unsigned int ccs_hdr_len = DTLS1_CCS_HEADER_LENGTH;\n\n        dtls1_get_ccs_header(rr->data, &ccs_hdr);\n\n        if (s->version == DTLS1_BAD_VER)\n            ccs_hdr_len = 3;\n\n        /*\n         * 'Change Cipher Spec' is just a single byte, so we know exactly\n         * what the record payload has to look like\n         */\n        /* XDTLS: check that epoch is consistent */\n        if ((rr->length != ccs_hdr_len) ||\n            (rr->off != 0) || (rr->data[0] != SSL3_MT_CCS)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_CHANGE_CIPHER_SPEC);\n            goto f_err;\n        }\n\n        rr->length = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_CHANGE_CIPHER_SPEC,\n                            rr->data, 1, s, s->msg_callback_arg);\n\n        /*\n         * We can't process a CCS now, because previous handshake messages\n         * are still missing, so just drop it.\n         */\n        if (!s->d1->change_cipher_spec_ok) {\n            goto start;\n        }\n\n        s->d1->change_cipher_spec_ok = 0;\n\n        s->s3->change_cipher_spec = 1;\n        if (!ssl3_do_change_cipher_spec(s))\n            goto err;\n\n        /* do this whenever CCS is processed */\n        dtls1_reset_seq_numbers(s, SSL3_CC_READ);\n\n        if (s->version == DTLS1_BAD_VER)\n            s->d1->handshake_read_seq++;\n\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * Remember that a CCS has been received, so that an old key of\n         * SCTP-Auth can be deleted when a CCS is sent. Will be ignored if no\n         * SCTP is used\n         */\n        BIO_ctrl(SSL_get_wbio(s), BIO_CTRL_DGRAM_SCTP_AUTH_CCS_RCVD, 1, NULL);\n#endif\n\n        goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        !s->in_handshake) {\n        struct hm_header_st msg_hdr;\n\n        /* this may just be a stale retransmit */\n        dtls1_get_message_header(rr->data, &msg_hdr);\n        if (rr->epoch != s->d1->r_epoch) {\n            rr->length = 0;\n            goto start;\n        }\n\n        /*\n         * If we are server, we may have a repeated FINISHED of the client\n         * here, then retransmit our CCS and FINISHED.\n         */\n        if (msg_hdr.type == SSL3_MT_FINISHED) {\n            if (dtls1_check_timeout_num(s) < 0)\n                return -1;\n\n            dtls1_retransmit_buffered_messages(s);\n            rr->length = 0;\n            goto start;\n        }\n\n        if (((s->state & SSL_ST_MASK) == SSL_ST_OK) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n#if 0                           /* worked only because C operator preferences\n                                 * are not as expected (and because this is\n                                 * not really needed for clients except for\n                                 * detecting protocol violations): */\n            s->state = SSL_ST_BEFORE | (s->server)\n                ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#else\n            s->state = s->server ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#endif\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (rr->type) {\n    default:\n#ifndef OPENSSL_NO_TLS\n        /* TLS just ignores unknown message types */\n        if (s->version == TLS1_VERSION) {\n            rr->length = 0;\n            goto start;\n        }\n#endif\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when s->in_handshake is set, but that should not\n         * happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            (((s->state & SSL_ST_CONNECT) &&\n              (s->state >= SSL3_ST_CW_CLNT_HELLO_A) &&\n              (s->state <= SSL3_ST_CR_SRVR_HELLO_A)\n             ) || ((s->state & SSL_ST_ACCEPT) &&\n                   (s->state <= SSL3_ST_SW_HELLO_REQ_A) &&\n                   (s->state >= SSL3_ST_SR_CLNT_HELLO_A)\n             )\n            )) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    return (-1);\n}",
        "func": "int dtls1_read_bytes(SSL *s, int type, unsigned char *buf, int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (s->s3->rbuf.buf == NULL) /* Not initialized yet */\n        if (!ssl3_setup_buffers(s))\n            return (-1);\n\n    /* XXX: check what the second '&& type' is about */\n    if ((type && (type != SSL3_RT_APPLICATION_DATA) &&\n         (type != SSL3_RT_HANDSHAKE) && type) ||\n        (peek && (type != SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    /*\n     * check whether there's a handshake message (client hello?) waiting\n     */\n    if ((ret = have_handshake_fragment(s, type, buf, len, peek)))\n        return ret;\n\n    /*\n     * Now s->d1->handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n#ifndef OPENSSL_NO_SCTP\n    /*\n     * Continue handshake if it had to be interrupted to read app data with\n     * SCTP.\n     */\n    if ((!s->in_handshake && SSL_in_init(s)) ||\n        (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n         (s->state == DTLS1_SCTP_ST_SR_READ_SOCK\n          || s->state == DTLS1_SCTP_ST_CR_READ_SOCK)\n         && s->s3->in_read_app_data != 2))\n#else\n    if (!s->in_handshake && SSL_in_init(s))\n#endif\n    {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = &(s->s3->rrec);\n\n    /*\n     * We are not handshaking and have no data yet, so process data buffered\n     * during the last handshake in advance, if any.\n     */\n    if (s->state == SSL_ST_OK && rr->length == 0) {\n        pitem *item;\n        item = pqueue_pop(s->d1->buffered_app_data.q);\n        if (item) {\n#ifndef OPENSSL_NO_SCTP\n            /* Restore bio_dgram_sctp_rcvinfo struct */\n            if (BIO_dgram_is_sctp(SSL_get_rbio(s))) {\n                DTLS1_RECORD_DATA *rdata = (DTLS1_RECORD_DATA *)item->data;\n                BIO_ctrl(SSL_get_rbio(s), BIO_CTRL_DGRAM_SCTP_SET_RCVINFO,\n                         sizeof(rdata->recordinfo), &rdata->recordinfo);\n            }\n#endif\n\n            dtls1_copy_record(s, item);\n\n            OPENSSL_free(item->data);\n            pitem_free(item);\n        }\n    }\n\n    /* Check for timeout */\n    if (dtls1_handle_timeout(s) > 0)\n        goto start;\n\n    /* get new packet if necessary */\n    if ((rr->length == 0) || (s->rstate == SSL_ST_READ_BODY)) {\n        ret = dtls1_get_record(s);\n        if (ret <= 0) {\n            ret = dtls1_read_failed(s, ret);\n            /* anything other than a timeout is an error */\n            if (ret <= 0)\n                return (ret);\n            else\n                goto start;\n        }\n    }\n\n    if (s->d1->listen && rr->type != SSL3_RT_HANDSHAKE) {\n        rr->length = 0;\n        goto start;\n    }\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (rr->type != SSL3_RT_ALERT && rr->length != 0)\n        s->cert->alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (rr->type != SSL3_RT_HANDSHAKE)) {\n        /*\n         * We now have application data between CCS and Finished. Most likely\n         * the packets were reordered on their way, so buffer the application\n         * data for later processing rather than dropping the connection.\n         */\n        if (dtls1_buffer_record(s, &(s->d1->buffered_app_data), rr->seq_num) <\n            0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n            return -1;\n        }\n        rr->length = 0;\n        goto start;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        rr->length = 0;\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == rr->type) {     /* SSL3_RT_APPLICATION_DATA or\n                                 * SSL3_RT_HANDSHAKE */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > rr->length)\n            n = rr->length;\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(rr->data[rr->off]), n);\n        if (!peek) {\n            rr->length -= n;\n            rr->off += n;\n            if (rr->length == 0) {\n                s->rstate = SSL_ST_READ_HEADER;\n                rr->off = 0;\n            }\n        }\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * We were about to renegotiate but had to read belated application\n         * data first, so retry.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            rr->type == SSL3_RT_APPLICATION_DATA &&\n            (s->state == DTLS1_SCTP_ST_SR_READ_SOCK\n             || s->state == DTLS1_SCTP_ST_CR_READ_SOCK)) {\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n        }\n\n        /*\n         * We might had to delay a close_notify alert because of reordered\n         * app data. If there was an alert and there is no message to read\n         * anymore, finally set shutdown.\n         */\n        if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n            s->d1->shutdown_received\n            && !BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            return (0);\n        }\n#endif\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int k, dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (rr->type == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof(s->d1->handshake_fragment);\n            dest = s->d1->handshake_fragment;\n            dest_len = &s->d1->handshake_fragment_len;\n        } else if (rr->type == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->d1->alert_fragment);\n            dest = s->d1->alert_fragment;\n            dest_len = &s->d1->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (rr->type == TLS1_RT_HEARTBEAT) {\n            dtls1_process_heartbeat(s);\n\n            /* Exit and notify application to read again */\n            rr->length = 0;\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n        /* else it's a CCS message, or application data or wrong */\n        else if (rr->type != SSL3_RT_CHANGE_CIPHER_SPEC) {\n            /*\n             * Application data while renegotiating is allowed. Try again\n             * reading.\n             */\n            if (rr->type == SSL3_RT_APPLICATION_DATA) {\n                BIO *bio;\n                s->s3->in_read_app_data = 2;\n                bio = SSL_get_rbio(s);\n                s->rwstate = SSL_READING;\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n\n            /* Not certain if this is the right error handling */\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n\n        if (dest_maxlen > 0) {\n            /*\n             * XDTLS: In a pathalogical case, the Client Hello may be\n             * fragmented--don't always expect dest_maxlen bytes\n             */\n            if (rr->length < dest_maxlen) {\n#ifdef DTLS1_AD_MISSING_HANDSHAKE_MESSAGE\n                /*\n                 * for normal alerts rr->length is 2, while\n                 * dest_maxlen is 7 if we were to handle this\n                 * non-existing alert...\n                 */\n                FIX ME\n#endif\n                 s->rstate = SSL_ST_READ_HEADER;\n                rr->length = 0;\n                goto start;\n            }\n\n            /* now move 'n' bytes: */\n            for (k = 0; k < dest_maxlen; k++) {\n                dest[k] = rr->data[rr->off++];\n                rr->length--;\n            }\n            *dest_len = dest_maxlen;\n        }\n    }\n\n    /*-\n     * s->d1->handshake_fragment_len == 12  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->d1->alert_fragment_len == 7      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        (s->d1->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->d1->handshake_fragment_len = 0;\n\n        if ((s->d1->handshake_fragment[1] != 0) ||\n            (s->d1->handshake_fragment[2] != 0) ||\n            (s->d1->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        /*\n         * no need to check sequence number on HELLO REQUEST messages\n         */\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->d1->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            s->d1->handshake_read_seq++;\n            s->new_session = 1;\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_DTLS1_READ_BYTES,\n                           SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on.\n     */\n    if (s->server\n            && SSL_is_init_finished(s)\n            && !s->s3->send_connection_binding\n            && s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH\n            && s->d1->handshake_fragment[0] == SSL3_MT_CLIENT_HELLO\n            && s->s3->previous_client_finished_len != 0\n            && (s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION) == 0) {\n        s->d1->handshake_fragment_len = 0;\n        rr->length = 0;\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n\n\n    if (s->d1->alert_fragment_len >= DTLS1_AL_HEADER_LENGTH) {\n        int alert_level = s->d1->alert_fragment[0];\n        int alert_descr = s->d1->alert_fragment[1];\n\n        s->d1->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->d1->alert_fragment, 2, s, s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n\n            s->cert->alert_count++;\n            if (s->cert->alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n#ifndef OPENSSL_NO_SCTP\n                /*\n                 * With SCTP and streams the socket may deliver app data\n                 * after a close_notify alert. We have to check this first so\n                 * that nothing gets discarded.\n                 */\n                if (BIO_dgram_is_sctp(SSL_get_rbio(s)) &&\n                    BIO_dgram_sctp_msg_waiting(SSL_get_rbio(s))) {\n                    s->d1->shutdown_received = 1;\n                    s->rwstate = SSL_READING;\n                    BIO_clear_retry_flags(SSL_get_rbio(s));\n                    BIO_set_retry_read(SSL_get_rbio(s));\n                    return -1;\n                }\n#endif\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n#if 0\n            /* XXX: this is a possible improvement in the future */\n            /* now check if it's a missing record */\n            if (alert_descr == DTLS1_AD_MISSING_HANDSHAKE_MESSAGE) {\n                unsigned short seq;\n                unsigned int frag_off;\n                unsigned char *p = &(s->d1->alert_fragment[2]);\n\n                n2s(p, seq);\n                n2l3(p, frag_off);\n\n                dtls1_retransmit_message(s,\n                                         dtls1_get_queue_priority\n                                         (frag->msg_header.seq, 0), frag_off,\n                                         &found);\n                if (!found && SSL_in_init(s)) {\n                    /*\n                     * fprintf( stderr,\"in init = %d\\n\", SSL_in_init(s));\n                     */\n                    /*\n                     * requested a message not yet sent, send an alert\n                     * ourselves\n                     */\n                    ssl3_send_alert(s, SSL3_AL_WARNING,\n                                    DTLS1_AD_MISSING_HANDSHAKE_MESSAGE);\n                }\n            }\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_DTLS1_READ_BYTES,\n                   SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof(tmp), \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            s->state = SSL_ST_ERR;\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        rr->length = 0;\n        return (0);\n    }\n\n    if (rr->type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        struct ccs_header_st ccs_hdr;\n        unsigned int ccs_hdr_len = DTLS1_CCS_HEADER_LENGTH;\n\n        dtls1_get_ccs_header(rr->data, &ccs_hdr);\n\n        if (s->version == DTLS1_BAD_VER)\n            ccs_hdr_len = 3;\n\n        /*\n         * 'Change Cipher Spec' is just a single byte, so we know exactly\n         * what the record payload has to look like\n         */\n        /* XDTLS: check that epoch is consistent */\n        if ((rr->length != ccs_hdr_len) ||\n            (rr->off != 0) || (rr->data[0] != SSL3_MT_CCS)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_BAD_CHANGE_CIPHER_SPEC);\n            goto f_err;\n        }\n\n        rr->length = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_CHANGE_CIPHER_SPEC,\n                            rr->data, 1, s, s->msg_callback_arg);\n\n        /*\n         * We can't process a CCS now, because previous handshake messages\n         * are still missing, so just drop it.\n         */\n        if (!s->d1->change_cipher_spec_ok) {\n            goto start;\n        }\n\n        s->d1->change_cipher_spec_ok = 0;\n\n        s->s3->change_cipher_spec = 1;\n        if (!ssl3_do_change_cipher_spec(s))\n            goto err;\n\n        /* do this whenever CCS is processed */\n        dtls1_reset_seq_numbers(s, SSL3_CC_READ);\n\n        if (s->version == DTLS1_BAD_VER)\n            s->d1->handshake_read_seq++;\n\n#ifndef OPENSSL_NO_SCTP\n        /*\n         * Remember that a CCS has been received, so that an old key of\n         * SCTP-Auth can be deleted when a CCS is sent. Will be ignored if no\n         * SCTP is used\n         */\n        BIO_ctrl(SSL_get_wbio(s), BIO_CTRL_DGRAM_SCTP_AUTH_CCS_RCVD, 1, NULL);\n#endif\n\n        goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->d1->handshake_fragment_len >= DTLS1_HM_HEADER_LENGTH) &&\n        !s->in_handshake) {\n        struct hm_header_st msg_hdr;\n\n        /* this may just be a stale retransmit */\n        dtls1_get_message_header(rr->data, &msg_hdr);\n        if (rr->epoch != s->d1->r_epoch) {\n            rr->length = 0;\n            goto start;\n        }\n\n        /*\n         * If we are server, we may have a repeated FINISHED of the client\n         * here, then retransmit our CCS and FINISHED.\n         */\n        if (msg_hdr.type == SSL3_MT_FINISHED) {\n            if (dtls1_check_timeout_num(s) < 0)\n                return -1;\n\n            dtls1_retransmit_buffered_messages(s);\n            rr->length = 0;\n            goto start;\n        }\n\n        if (((s->state & SSL_ST_MASK) == SSL_ST_OK) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n#if 0                           /* worked only because C operator preferences\n                                 * are not as expected (and because this is\n                                 * not really needed for clients except for\n                                 * detecting protocol violations): */\n            s->state = SSL_ST_BEFORE | (s->server)\n                ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#else\n            s->state = s->server ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#endif\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (rr->type) {\n    default:\n#ifndef OPENSSL_NO_TLS\n        /* TLS just ignores unknown message types */\n        if (s->version == TLS1_VERSION) {\n            rr->length = 0;\n            goto start;\n        }\n#endif\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when s->in_handshake is set, but that should not\n         * happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_DTLS1_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            (((s->state & SSL_ST_CONNECT) &&\n              (s->state >= SSL3_ST_CW_CLNT_HELLO_A) &&\n              (s->state <= SSL3_ST_CR_SRVR_HELLO_A)\n             ) || ((s->state & SSL_ST_ACCEPT) &&\n                   (s->state <= SSL3_ST_SW_HELLO_REQ_A) &&\n                   (s->state >= SSL3_ST_SR_CLNT_HELLO_A)\n             )\n            )) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_DTLS1_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -465,6 +465,7 @@\n             ERR_add_error_data(2, \"SSL alert number \", tmp);\n             s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n             SSL_CTX_remove_session(s->session_ctx, s->session);\n+            s->state = SSL_ST_ERR;\n             return (0);\n         } else {\n             al = SSL_AD_ILLEGAL_PARAMETER;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            s->state = SSL_ST_ERR;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1559",
        "func_name": "openssl/ssl3_send_alert",
        "description": "If an application encounters a fatal protocol error and then calls SSL_shutdown() twice (once to send a close_notify, and once to receive one) then OpenSSL can respond differently to the calling application if a 0 byte record is received with invalid padding compared to if a 0 byte record is received with an invalid MAC. If the application then behaves differently based on that in a way that is detectable to the remote peer, then this amounts to a padding oracle that could be used to decrypt data. In order for this to be exploitable \"non-stitched\" ciphersuites must be in use. Stitched ciphersuites are optimised implementations of certain commonly used ciphersuites. Also the application must call SSL_shutdown() twice even if a protocol error has occurred (applications should not do this but some do anyway). Fixed in OpenSSL 1.0.2r (Affected 1.0.2-1.0.2q).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=e9bbefbf0f24c57645e7ad6a5a71ae649d18ac8e",
        "commit_title": "",
        "commit_text": "Go into the error state if a fatal alert is sent or received  If an application calls SSL_shutdown after a fatal alert has occured and then behaves different based on error codes from that function then the application may be vulnerable to a padding oracle.  CVE-2019-1559  ",
        "func_before": "int ssl3_send_alert(SSL *s, int level, int desc)\n{\n    /* Map tls/ssl alert value to correct one */\n    desc = s->method->ssl3_enc->alert_value(desc);\n    if (s->version == SSL3_VERSION && desc == SSL_AD_PROTOCOL_VERSION)\n        desc = SSL_AD_HANDSHAKE_FAILURE; /* SSL 3.0 does not have\n                                          * protocol_version alerts */\n    if (desc < 0)\n        return -1;\n    /* If a fatal one, remove from cache */\n    if ((level == 2) && (s->session != NULL))\n        SSL_CTX_remove_session(s->session_ctx, s->session);\n\n    s->s3->alert_dispatch = 1;\n    s->s3->send_alert[0] = level;\n    s->s3->send_alert[1] = desc;\n    if (s->s3->wbuf.left == 0)  /* data still being written out? */\n        return s->method->ssl_dispatch_alert(s);\n    /*\n     * else data is still being written out, we will get written some time in\n     * the future\n     */\n    return -1;\n}",
        "func": "int ssl3_send_alert(SSL *s, int level, int desc)\n{\n    /* Map tls/ssl alert value to correct one */\n    desc = s->method->ssl3_enc->alert_value(desc);\n    if (s->version == SSL3_VERSION && desc == SSL_AD_PROTOCOL_VERSION)\n        desc = SSL_AD_HANDSHAKE_FAILURE; /* SSL 3.0 does not have\n                                          * protocol_version alerts */\n    if (desc < 0)\n        return -1;\n    /* If a fatal one, remove from cache and go into the error state */\n    if (level == SSL3_AL_FATAL) {\n        if (s->session != NULL)\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n        s->state = SSL_ST_ERR;\n    }\n\n    s->s3->alert_dispatch = 1;\n    s->s3->send_alert[0] = level;\n    s->s3->send_alert[1] = desc;\n    if (s->s3->wbuf.left == 0)  /* data still being written out? */\n        return s->method->ssl_dispatch_alert(s);\n    /*\n     * else data is still being written out, we will get written some time in\n     * the future\n     */\n    return -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,9 +7,12 @@\n                                           * protocol_version alerts */\n     if (desc < 0)\n         return -1;\n-    /* If a fatal one, remove from cache */\n-    if ((level == 2) && (s->session != NULL))\n-        SSL_CTX_remove_session(s->session_ctx, s->session);\n+    /* If a fatal one, remove from cache and go into the error state */\n+    if (level == SSL3_AL_FATAL) {\n+        if (s->session != NULL)\n+            SSL_CTX_remove_session(s->session_ctx, s->session);\n+        s->state = SSL_ST_ERR;\n+    }\n \n     s->s3->alert_dispatch = 1;\n     s->s3->send_alert[0] = level;",
        "diff_line_info": {
            "deleted_lines": [
                "    /* If a fatal one, remove from cache */",
                "    if ((level == 2) && (s->session != NULL))",
                "        SSL_CTX_remove_session(s->session_ctx, s->session);"
            ],
            "added_lines": [
                "    /* If a fatal one, remove from cache and go into the error state */",
                "    if (level == SSL3_AL_FATAL) {",
                "        if (s->session != NULL)",
                "            SSL_CTX_remove_session(s->session_ctx, s->session);",
                "        s->state = SSL_ST_ERR;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-1559",
        "func_name": "openssl/ssl3_read_bytes",
        "description": "If an application encounters a fatal protocol error and then calls SSL_shutdown() twice (once to send a close_notify, and once to receive one) then OpenSSL can respond differently to the calling application if a 0 byte record is received with invalid padding compared to if a 0 byte record is received with an invalid MAC. If the application then behaves differently based on that in a way that is detectable to the remote peer, then this amounts to a padding oracle that could be used to decrypt data. In order for this to be exploitable \"non-stitched\" ciphersuites must be in use. Stitched ciphersuites are optimised implementations of certain commonly used ciphersuites. Also the application must call SSL_shutdown() twice even if a protocol error has occurred (applications should not do this but some do anyway). Fixed in OpenSSL 1.0.2r (Affected 1.0.2-1.0.2q).",
        "git_url": "https://git.openssl.org/gitweb/?p=openssl.git;a=commit;h=e9bbefbf0f24c57645e7ad6a5a71ae649d18ac8e",
        "commit_title": "",
        "commit_text": "Go into the error state if a fatal alert is sent or received  If an application calls SSL_shutdown after a fatal alert has occured and then behaves different based on error codes from that function then the application may be vulnerable to a padding oracle.  CVE-2019-1559  ",
        "func_before": "int ssl3_read_bytes(SSL *s, int type, unsigned char *buf, int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (s->s3->rbuf.buf == NULL) /* Not initialized yet */\n        if (!ssl3_setup_read_buffer(s))\n            return (-1);\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA)\n         && (type != SSL3_RT_HANDSHAKE)) || (peek\n                                             && (type !=\n                                                 SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    if ((type == SSL3_RT_HANDSHAKE) && (s->s3->handshake_fragment_len > 0))\n        /* (partially) satisfy request from storage */\n    {\n        unsigned char *src = s->s3->handshake_fragment;\n        unsigned char *dst = buf;\n        unsigned int k;\n\n        /* peek == 0 */\n        n = 0;\n        while ((len > 0) && (s->s3->handshake_fragment_len > 0)) {\n            *dst++ = *src++;\n            len--;\n            s->s3->handshake_fragment_len--;\n            n++;\n        }\n        /* move any remaining fragment bytes: */\n        for (k = 0; k < s->s3->handshake_fragment_len; k++)\n            s->s3->handshake_fragment[k] = *src++;\n        return n;\n    }\n\n    /*\n     * Now s->s3->handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n    if (!s->in_handshake && SSL_in_init(s)) {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = &(s->s3->rrec);\n\n    /* get new packet if necessary */\n    if ((rr->length == 0) || (s->rstate == SSL_ST_READ_BODY)) {\n        ret = ssl3_get_record(s);\n        if (ret <= 0)\n            return (ret);\n    }\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (rr->type != SSL3_RT_ALERT && rr->length != 0)\n        s->cert->alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (rr->type != SSL3_RT_HANDSHAKE)) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_DATA_BETWEEN_CCS_AND_FINISHED);\n        goto f_err;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        rr->length = 0;\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == rr->type) {     /* SSL3_RT_APPLICATION_DATA or\n                                 * SSL3_RT_HANDSHAKE */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > rr->length)\n            n = rr->length;\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(rr->data[rr->off]), n);\n        if (!peek) {\n            rr->length -= n;\n            rr->off += n;\n            if (rr->length == 0) {\n                s->rstate = SSL_ST_READ_HEADER;\n                rr->off = 0;\n                if (s->mode & SSL_MODE_RELEASE_BUFFERS\n                    && s->s3->rbuf.left == 0)\n                    ssl3_release_read_buffer(s);\n            }\n        }\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (rr->type == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof(s->s3->handshake_fragment);\n            dest = s->s3->handshake_fragment;\n            dest_len = &s->s3->handshake_fragment_len;\n        } else if (rr->type == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->s3->alert_fragment);\n            dest = s->s3->alert_fragment;\n            dest_len = &s->s3->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (rr->type == TLS1_RT_HEARTBEAT) {\n            i = tls1_process_heartbeat(s);\n\n            if (i < 0)\n                return i;\n\n            rr->length = 0;\n            if (s->mode & SSL_MODE_AUTO_RETRY)\n                goto start;\n\n            /* Exit and notify application to read again */\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n\n        if (dest_maxlen > 0) {\n            n = dest_maxlen - *dest_len; /* available space in 'dest' */\n            if (rr->length < n)\n                n = rr->length; /* available bytes */\n\n            /* now move 'n' bytes: */\n            while (n-- > 0) {\n                dest[(*dest_len)++] = rr->data[rr->off++];\n                rr->length--;\n            }\n\n            if (*dest_len < dest_maxlen)\n                goto start;     /* fragment was too small */\n        }\n    }\n\n    /*-\n     * s->s3->handshake_fragment_len == 4  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->s3->alert_fragment_len == 2      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->s3->handshake_fragment_len >= 4) &&\n        (s->s3->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->s3->handshake_fragment_len = 0;\n\n        if ((s->s3->handshake_fragment[1] != 0) ||\n            (s->s3->handshake_fragment[2] != 0) ||\n            (s->s3->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->s3->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_SSL3_READ_BYTES,\n                           SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on.\n     */\n    if (s->server\n            && SSL_is_init_finished(s)\n            && !s->s3->send_connection_binding\n            && s->version > SSL3_VERSION\n            && s->s3->handshake_fragment_len >= SSL3_HM_HEADER_LENGTH\n            && s->s3->handshake_fragment[0] == SSL3_MT_CLIENT_HELLO\n            && s->s3->previous_client_finished_len != 0\n            && (s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION) == 0) {\n        s->s3->handshake_fragment_len = 0;\n        rr->length = 0;\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n\n    if (s->s3->alert_fragment_len >= 2) {\n        int alert_level = s->s3->alert_fragment[0];\n        int alert_descr = s->s3->alert_fragment[1];\n\n        s->s3->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->s3->alert_fragment, 2, s, s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n\n            s->cert->alert_count++;\n            if (s->cert->alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n            /*\n             * This is a warning but we receive it if we requested\n             * renegotiation and the peer denied it. Terminate with a fatal\n             * alert because if application tried to renegotiatie it\n             * presumably had a good reason and expects it to succeed. In\n             * future we might have a renegotiation where we don't care if\n             * the peer refused it where we carry on.\n             */\n            else if (alert_descr == SSL_AD_NO_RENEGOTIATION) {\n                al = SSL_AD_HANDSHAKE_FAILURE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_NO_RENEGOTIATION);\n                goto f_err;\n            }\n#ifdef SSL_AD_MISSING_SRP_USERNAME\n            else if (alert_descr == SSL_AD_MISSING_SRP_USERNAME)\n                return (0);\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof(tmp), \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        rr->length = 0;\n        return (0);\n    }\n\n    if (rr->type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        /*\n         * 'Change Cipher Spec' is just a single byte, so we know exactly\n         * what the record payload has to look like\n         */\n        if ((rr->length != 1) || (rr->off != 0) ||\n            (rr->data[0] != SSL3_MT_CCS)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_CHANGE_CIPHER_SPEC);\n            goto f_err;\n        }\n\n        /* Check we have a cipher to change to */\n        if (s->s3->tmp.new_cipher == NULL) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        if (!(s->s3->flags & SSL3_FLAGS_CCS_OK)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        s->s3->flags &= ~SSL3_FLAGS_CCS_OK;\n\n        rr->length = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_CHANGE_CIPHER_SPEC,\n                            rr->data, 1, s, s->msg_callback_arg);\n\n        s->s3->change_cipher_spec = 1;\n        if (!ssl3_do_change_cipher_spec(s))\n            goto err;\n        else\n            goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->s3->handshake_fragment_len >= 4) && !s->in_handshake) {\n        if (((s->state & SSL_ST_MASK) == SSL_ST_OK) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n#if 0                           /* worked only because C operator preferences\n                                 * are not as expected (and because this is\n                                 * not really needed for clients except for\n                                 * detecting protocol violations): */\n            s->state = SSL_ST_BEFORE | (s->server)\n                ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#else\n            s->state = s->server ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#endif\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (rr->type) {\n    default:\n        /*\n         * TLS 1.0 and 1.1 say you SHOULD ignore unrecognised record types, but\n         * TLS 1.2 says you MUST send an unexpected message alert. We use the\n         * TLS 1.2 behaviour for all protocol versions to prevent issues where\n         * no progress is being made and the peer continually sends unrecognised\n         * record types, using up resources processing them.\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when s->in_handshake is set, but that should not\n         * happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            (((s->state & SSL_ST_CONNECT) &&\n              (s->state >= SSL3_ST_CW_CLNT_HELLO_A) &&\n              (s->state <= SSL3_ST_CR_SRVR_HELLO_A)\n             ) || ((s->state & SSL_ST_ACCEPT) &&\n                   (s->state <= SSL3_ST_SW_HELLO_REQ_A) &&\n                   (s->state >= SSL3_ST_SR_CLNT_HELLO_A)\n             )\n            )) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    return (-1);\n}",
        "func": "int ssl3_read_bytes(SSL *s, int type, unsigned char *buf, int len, int peek)\n{\n    int al, i, j, ret;\n    unsigned int n;\n    SSL3_RECORD *rr;\n    void (*cb) (const SSL *ssl, int type2, int val) = NULL;\n\n    if (s->s3->rbuf.buf == NULL) /* Not initialized yet */\n        if (!ssl3_setup_read_buffer(s))\n            return (-1);\n\n    if ((type && (type != SSL3_RT_APPLICATION_DATA)\n         && (type != SSL3_RT_HANDSHAKE)) || (peek\n                                             && (type !=\n                                                 SSL3_RT_APPLICATION_DATA))) {\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        return -1;\n    }\n\n    if ((type == SSL3_RT_HANDSHAKE) && (s->s3->handshake_fragment_len > 0))\n        /* (partially) satisfy request from storage */\n    {\n        unsigned char *src = s->s3->handshake_fragment;\n        unsigned char *dst = buf;\n        unsigned int k;\n\n        /* peek == 0 */\n        n = 0;\n        while ((len > 0) && (s->s3->handshake_fragment_len > 0)) {\n            *dst++ = *src++;\n            len--;\n            s->s3->handshake_fragment_len--;\n            n++;\n        }\n        /* move any remaining fragment bytes: */\n        for (k = 0; k < s->s3->handshake_fragment_len; k++)\n            s->s3->handshake_fragment[k] = *src++;\n        return n;\n    }\n\n    /*\n     * Now s->s3->handshake_fragment_len == 0 if type == SSL3_RT_HANDSHAKE.\n     */\n\n    if (!s->in_handshake && SSL_in_init(s)) {\n        /* type == SSL3_RT_APPLICATION_DATA */\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n    }\n start:\n    s->rwstate = SSL_NOTHING;\n\n    /*-\n     * s->s3->rrec.type         - is the type of record\n     * s->s3->rrec.data,    - data\n     * s->s3->rrec.off,     - offset into 'data' for next read\n     * s->s3->rrec.length,  - number of bytes.\n     */\n    rr = &(s->s3->rrec);\n\n    /* get new packet if necessary */\n    if ((rr->length == 0) || (s->rstate == SSL_ST_READ_BODY)) {\n        ret = ssl3_get_record(s);\n        if (ret <= 0)\n            return (ret);\n    }\n\n    /*\n     * Reset the count of consecutive warning alerts if we've got a non-empty\n     * record that isn't an alert.\n     */\n    if (rr->type != SSL3_RT_ALERT && rr->length != 0)\n        s->cert->alert_count = 0;\n\n    /* we now have a packet which can be read and processed */\n\n    if (s->s3->change_cipher_spec /* set when we receive ChangeCipherSpec,\n                                   * reset by ssl3_get_finished */\n        && (rr->type != SSL3_RT_HANDSHAKE)) {\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_DATA_BETWEEN_CCS_AND_FINISHED);\n        goto f_err;\n    }\n\n    /*\n     * If the other end has shut down, throw anything we read away (even in\n     * 'peek' mode)\n     */\n    if (s->shutdown & SSL_RECEIVED_SHUTDOWN) {\n        rr->length = 0;\n        s->rwstate = SSL_NOTHING;\n        return (0);\n    }\n\n    if (type == rr->type) {     /* SSL3_RT_APPLICATION_DATA or\n                                 * SSL3_RT_HANDSHAKE */\n        /*\n         * make sure that we are not getting application data when we are\n         * doing a handshake for the first time\n         */\n        if (SSL_in_init(s) && (type == SSL3_RT_APPLICATION_DATA) &&\n            (s->enc_read_ctx == NULL)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_APP_DATA_IN_HANDSHAKE);\n            goto f_err;\n        }\n\n        if (len <= 0)\n            return (len);\n\n        if ((unsigned int)len > rr->length)\n            n = rr->length;\n        else\n            n = (unsigned int)len;\n\n        memcpy(buf, &(rr->data[rr->off]), n);\n        if (!peek) {\n            rr->length -= n;\n            rr->off += n;\n            if (rr->length == 0) {\n                s->rstate = SSL_ST_READ_HEADER;\n                rr->off = 0;\n                if (s->mode & SSL_MODE_RELEASE_BUFFERS\n                    && s->s3->rbuf.left == 0)\n                    ssl3_release_read_buffer(s);\n            }\n        }\n        return (n);\n    }\n\n    /*\n     * If we get here, then type != rr->type; if we have a handshake message,\n     * then it was unexpected (Hello Request or Client Hello).\n     */\n\n    /*\n     * In case of record types for which we have 'fragment' storage, fill\n     * that so that we can process the data at a fixed place.\n     */\n    {\n        unsigned int dest_maxlen = 0;\n        unsigned char *dest = NULL;\n        unsigned int *dest_len = NULL;\n\n        if (rr->type == SSL3_RT_HANDSHAKE) {\n            dest_maxlen = sizeof(s->s3->handshake_fragment);\n            dest = s->s3->handshake_fragment;\n            dest_len = &s->s3->handshake_fragment_len;\n        } else if (rr->type == SSL3_RT_ALERT) {\n            dest_maxlen = sizeof(s->s3->alert_fragment);\n            dest = s->s3->alert_fragment;\n            dest_len = &s->s3->alert_fragment_len;\n        }\n#ifndef OPENSSL_NO_HEARTBEATS\n        else if (rr->type == TLS1_RT_HEARTBEAT) {\n            i = tls1_process_heartbeat(s);\n\n            if (i < 0)\n                return i;\n\n            rr->length = 0;\n            if (s->mode & SSL_MODE_AUTO_RETRY)\n                goto start;\n\n            /* Exit and notify application to read again */\n            s->rwstate = SSL_READING;\n            BIO_clear_retry_flags(SSL_get_rbio(s));\n            BIO_set_retry_read(SSL_get_rbio(s));\n            return (-1);\n        }\n#endif\n\n        if (dest_maxlen > 0) {\n            n = dest_maxlen - *dest_len; /* available space in 'dest' */\n            if (rr->length < n)\n                n = rr->length; /* available bytes */\n\n            /* now move 'n' bytes: */\n            while (n-- > 0) {\n                dest[(*dest_len)++] = rr->data[rr->off++];\n                rr->length--;\n            }\n\n            if (*dest_len < dest_maxlen)\n                goto start;     /* fragment was too small */\n        }\n    }\n\n    /*-\n     * s->s3->handshake_fragment_len == 4  iff  rr->type == SSL3_RT_HANDSHAKE;\n     * s->s3->alert_fragment_len == 2      iff  rr->type == SSL3_RT_ALERT.\n     * (Possibly rr is 'empty' now, i.e. rr->length may be 0.)\n     */\n\n    /* If we are a client, check for an incoming 'Hello Request': */\n    if ((!s->server) &&\n        (s->s3->handshake_fragment_len >= 4) &&\n        (s->s3->handshake_fragment[0] == SSL3_MT_HELLO_REQUEST) &&\n        (s->session != NULL) && (s->session->cipher != NULL)) {\n        s->s3->handshake_fragment_len = 0;\n\n        if ((s->s3->handshake_fragment[1] != 0) ||\n            (s->s3->handshake_fragment[2] != 0) ||\n            (s->s3->handshake_fragment[3] != 0)) {\n            al = SSL_AD_DECODE_ERROR;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_HELLO_REQUEST);\n            goto f_err;\n        }\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_HANDSHAKE,\n                            s->s3->handshake_fragment, 4, s,\n                            s->msg_callback_arg);\n\n        if (SSL_is_init_finished(s) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS) &&\n            !s->s3->renegotiate) {\n            ssl3_renegotiate(s);\n            if (ssl3_renegotiate_check(s)) {\n                i = s->handshake_func(s);\n                if (i < 0)\n                    return (i);\n                if (i == 0) {\n                    SSLerr(SSL_F_SSL3_READ_BYTES,\n                           SSL_R_SSL_HANDSHAKE_FAILURE);\n                    return (-1);\n                }\n\n                if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n                    if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                        BIO *bio;\n                        /*\n                         * In the case where we try to read application data,\n                         * but we trigger an SSL handshake, we return -1 with\n                         * the retry option set.  Otherwise renegotiation may\n                         * cause nasty problems in the blocking world\n                         */\n                        s->rwstate = SSL_READING;\n                        bio = SSL_get_rbio(s);\n                        BIO_clear_retry_flags(bio);\n                        BIO_set_retry_read(bio);\n                        return (-1);\n                    }\n                }\n            }\n        }\n        /*\n         * we either finished a handshake or ignored the request, now try\n         * again to obtain the (application) data we were asked for\n         */\n        goto start;\n    }\n\n    /*\n     * If we are a server and get a client hello when renegotiation isn't\n     * allowed send back a no renegotiation alert and carry on.\n     */\n    if (s->server\n            && SSL_is_init_finished(s)\n            && !s->s3->send_connection_binding\n            && s->version > SSL3_VERSION\n            && s->s3->handshake_fragment_len >= SSL3_HM_HEADER_LENGTH\n            && s->s3->handshake_fragment[0] == SSL3_MT_CLIENT_HELLO\n            && s->s3->previous_client_finished_len != 0\n            && (s->options & SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION) == 0) {\n        s->s3->handshake_fragment_len = 0;\n        rr->length = 0;\n        ssl3_send_alert(s, SSL3_AL_WARNING, SSL_AD_NO_RENEGOTIATION);\n        goto start;\n    }\n\n    if (s->s3->alert_fragment_len >= 2) {\n        int alert_level = s->s3->alert_fragment[0];\n        int alert_descr = s->s3->alert_fragment[1];\n\n        s->s3->alert_fragment_len = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_ALERT,\n                            s->s3->alert_fragment, 2, s, s->msg_callback_arg);\n\n        if (s->info_callback != NULL)\n            cb = s->info_callback;\n        else if (s->ctx->info_callback != NULL)\n            cb = s->ctx->info_callback;\n\n        if (cb != NULL) {\n            j = (alert_level << 8) | alert_descr;\n            cb(s, SSL_CB_READ_ALERT, j);\n        }\n\n        if (alert_level == SSL3_AL_WARNING) {\n            s->s3->warn_alert = alert_descr;\n\n            s->cert->alert_count++;\n            if (s->cert->alert_count == MAX_WARN_ALERT_COUNT) {\n                al = SSL_AD_UNEXPECTED_MESSAGE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_TOO_MANY_WARN_ALERTS);\n                goto f_err;\n            }\n\n            if (alert_descr == SSL_AD_CLOSE_NOTIFY) {\n                s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n                return (0);\n            }\n            /*\n             * This is a warning but we receive it if we requested\n             * renegotiation and the peer denied it. Terminate with a fatal\n             * alert because if application tried to renegotiatie it\n             * presumably had a good reason and expects it to succeed. In\n             * future we might have a renegotiation where we don't care if\n             * the peer refused it where we carry on.\n             */\n            else if (alert_descr == SSL_AD_NO_RENEGOTIATION) {\n                al = SSL_AD_HANDSHAKE_FAILURE;\n                SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_NO_RENEGOTIATION);\n                goto f_err;\n            }\n#ifdef SSL_AD_MISSING_SRP_USERNAME\n            else if (alert_descr == SSL_AD_MISSING_SRP_USERNAME)\n                return (0);\n#endif\n        } else if (alert_level == SSL3_AL_FATAL) {\n            char tmp[16];\n\n            s->rwstate = SSL_NOTHING;\n            s->s3->fatal_alert = alert_descr;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_AD_REASON_OFFSET + alert_descr);\n            BIO_snprintf(tmp, sizeof(tmp), \"%d\", alert_descr);\n            ERR_add_error_data(2, \"SSL alert number \", tmp);\n            s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n            SSL_CTX_remove_session(s->session_ctx, s->session);\n            s->state = SSL_ST_ERR;\n            return (0);\n        } else {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNKNOWN_ALERT_TYPE);\n            goto f_err;\n        }\n\n        goto start;\n    }\n\n    if (s->shutdown & SSL_SENT_SHUTDOWN) { /* but we have not received a\n                                            * shutdown */\n        s->rwstate = SSL_NOTHING;\n        rr->length = 0;\n        return (0);\n    }\n\n    if (rr->type == SSL3_RT_CHANGE_CIPHER_SPEC) {\n        /*\n         * 'Change Cipher Spec' is just a single byte, so we know exactly\n         * what the record payload has to look like\n         */\n        if ((rr->length != 1) || (rr->off != 0) ||\n            (rr->data[0] != SSL3_MT_CCS)) {\n            al = SSL_AD_ILLEGAL_PARAMETER;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_BAD_CHANGE_CIPHER_SPEC);\n            goto f_err;\n        }\n\n        /* Check we have a cipher to change to */\n        if (s->s3->tmp.new_cipher == NULL) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        if (!(s->s3->flags & SSL3_FLAGS_CCS_OK)) {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_CCS_RECEIVED_EARLY);\n            goto f_err;\n        }\n\n        s->s3->flags &= ~SSL3_FLAGS_CCS_OK;\n\n        rr->length = 0;\n\n        if (s->msg_callback)\n            s->msg_callback(0, s->version, SSL3_RT_CHANGE_CIPHER_SPEC,\n                            rr->data, 1, s, s->msg_callback_arg);\n\n        s->s3->change_cipher_spec = 1;\n        if (!ssl3_do_change_cipher_spec(s))\n            goto err;\n        else\n            goto start;\n    }\n\n    /*\n     * Unexpected handshake message (Client Hello, or protocol violation)\n     */\n    if ((s->s3->handshake_fragment_len >= 4) && !s->in_handshake) {\n        if (((s->state & SSL_ST_MASK) == SSL_ST_OK) &&\n            !(s->s3->flags & SSL3_FLAGS_NO_RENEGOTIATE_CIPHERS)) {\n#if 0                           /* worked only because C operator preferences\n                                 * are not as expected (and because this is\n                                 * not really needed for clients except for\n                                 * detecting protocol violations): */\n            s->state = SSL_ST_BEFORE | (s->server)\n                ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#else\n            s->state = s->server ? SSL_ST_ACCEPT : SSL_ST_CONNECT;\n#endif\n            s->renegotiate = 1;\n            s->new_session = 1;\n        }\n        i = s->handshake_func(s);\n        if (i < 0)\n            return (i);\n        if (i == 0) {\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_SSL_HANDSHAKE_FAILURE);\n            return (-1);\n        }\n\n        if (!(s->mode & SSL_MODE_AUTO_RETRY)) {\n            if (s->s3->rbuf.left == 0) { /* no read-ahead left? */\n                BIO *bio;\n                /*\n                 * In the case where we try to read application data, but we\n                 * trigger an SSL handshake, we return -1 with the retry\n                 * option set.  Otherwise renegotiation may cause nasty\n                 * problems in the blocking world\n                 */\n                s->rwstate = SSL_READING;\n                bio = SSL_get_rbio(s);\n                BIO_clear_retry_flags(bio);\n                BIO_set_retry_read(bio);\n                return (-1);\n            }\n        }\n        goto start;\n    }\n\n    switch (rr->type) {\n    default:\n        /*\n         * TLS 1.0 and 1.1 say you SHOULD ignore unrecognised record types, but\n         * TLS 1.2 says you MUST send an unexpected message alert. We use the\n         * TLS 1.2 behaviour for all protocol versions to prevent issues where\n         * no progress is being made and the peer continually sends unrecognised\n         * record types, using up resources processing them.\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n        goto f_err;\n    case SSL3_RT_CHANGE_CIPHER_SPEC:\n    case SSL3_RT_ALERT:\n    case SSL3_RT_HANDSHAKE:\n        /*\n         * we already handled all of these, with the possible exception of\n         * SSL3_RT_HANDSHAKE when s->in_handshake is set, but that should not\n         * happen when type != rr->type\n         */\n        al = SSL_AD_UNEXPECTED_MESSAGE;\n        SSLerr(SSL_F_SSL3_READ_BYTES, ERR_R_INTERNAL_ERROR);\n        goto f_err;\n    case SSL3_RT_APPLICATION_DATA:\n        /*\n         * At this point, we were expecting handshake data, but have\n         * application data.  If the library was running inside ssl3_read()\n         * (i.e. in_read_app_data is set) and it makes sense to read\n         * application data at this point (session renegotiation not yet\n         * started), we will indulge it.\n         */\n        if (s->s3->in_read_app_data &&\n            (s->s3->total_renegotiations != 0) &&\n            (((s->state & SSL_ST_CONNECT) &&\n              (s->state >= SSL3_ST_CW_CLNT_HELLO_A) &&\n              (s->state <= SSL3_ST_CR_SRVR_HELLO_A)\n             ) || ((s->state & SSL_ST_ACCEPT) &&\n                   (s->state <= SSL3_ST_SW_HELLO_REQ_A) &&\n                   (s->state >= SSL3_ST_SR_CLNT_HELLO_A)\n             )\n            )) {\n            s->s3->in_read_app_data = 2;\n            return (-1);\n        } else {\n            al = SSL_AD_UNEXPECTED_MESSAGE;\n            SSLerr(SSL_F_SSL3_READ_BYTES, SSL_R_UNEXPECTED_RECORD);\n            goto f_err;\n        }\n    }\n    /* not reached */\n\n f_err:\n    ssl3_send_alert(s, SSL3_AL_FATAL, al);\n err:\n    return (-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -335,6 +335,7 @@\n             ERR_add_error_data(2, \"SSL alert number \", tmp);\n             s->shutdown |= SSL_RECEIVED_SHUTDOWN;\n             SSL_CTX_remove_session(s->session_ctx, s->session);\n+            s->state = SSL_ST_ERR;\n             return (0);\n         } else {\n             al = SSL_AD_ILLEGAL_PARAMETER;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            s->state = SSL_ST_ERR;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-37848",
        "func_name": "barebox/check_passwd",
        "description": "common/password.c in Pengutronix barebox through 2021.07.0 leaks timing information because strncmp is used during hash comparison.",
        "git_url": "https://github.com/barebox/barebox/commit/a3337563c705bc8e0cf32f910b3e9e3c43d962ff",
        "commit_title": "password: Use crypto_memneq() to compare hashes",
        "commit_text": " Cryptographic verifications should be time-constant so that an attacker cannot get information about the secrets used by observing the system, so use crypto_memneq() rather than memcmp() to compare password hashes. ",
        "func_before": "static int check_passwd(unsigned char *passwd, size_t length)\n{\n\tstruct digest *d = NULL;\n\tunsigned char *passwd1_sum;\n\tunsigned char *passwd2_sum;\n\tint ret = 0;\n\tint hash_len;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\thash_len = PBKDF2_LENGTH;\n\t} else {\n\t\td = digest_alloc(PASSWD_SUM);\n\t\tif (!d) {\n\t\t\tpr_err(\"No such digest: %s\\n\",\n\t\t\t       PASSWD_SUM ? PASSWD_SUM : \"NULL\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\thash_len = digest_length(d);\n\t}\n\n\tpasswd1_sum = calloc(hash_len * 2, sizeof(unsigned char));\n\tif (!passwd1_sum)\n\t\treturn -ENOMEM;\n\n\tpasswd2_sum = passwd1_sum + hash_len;\n\n\tif (is_passwd_env_enable())\n\t\tret = read_env_passwd(passwd2_sum, hash_len);\n\telse if (is_passwd_default_enable())\n\t\tret = read_default_passwd(passwd2_sum, hash_len);\n\telse\n\t\tret = -EINVAL;\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\tchar *key = passwd2_sum + PBKDF2_SALT_LEN;\n\t\tchar *salt = passwd2_sum;\n\t\tint keylen = PBKDF2_LENGTH - PBKDF2_SALT_LEN;\n\n\t\tret = pkcs5_pbkdf2_hmac_sha1(passwd, length, salt,\n\t\t\tPBKDF2_SALT_LEN, PBKDF2_COUNT, keylen, passwd1_sum);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (strncmp(passwd1_sum, key, keylen) == 0)\n\t\t\tret = 1;\n\t} else {\n\t\tret = digest_digest(d, passwd, length, passwd1_sum);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (strncmp(passwd1_sum, passwd2_sum, hash_len) == 0)\n\t\t\tret = 1;\n\t}\n\nerr:\n\tfree(passwd1_sum);\n\tdigest_free(d);\n\n\treturn ret;\n}",
        "func": "static int check_passwd(unsigned char *passwd, size_t length)\n{\n\tstruct digest *d = NULL;\n\tunsigned char *passwd1_sum;\n\tunsigned char *passwd2_sum;\n\tint ret = 0;\n\tint hash_len;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\thash_len = PBKDF2_LENGTH;\n\t} else {\n\t\td = digest_alloc(PASSWD_SUM);\n\t\tif (!d) {\n\t\t\tpr_err(\"No such digest: %s\\n\",\n\t\t\t       PASSWD_SUM ? PASSWD_SUM : \"NULL\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\thash_len = digest_length(d);\n\t}\n\n\tpasswd1_sum = calloc(hash_len * 2, sizeof(unsigned char));\n\tif (!passwd1_sum)\n\t\treturn -ENOMEM;\n\n\tpasswd2_sum = passwd1_sum + hash_len;\n\n\tif (is_passwd_env_enable())\n\t\tret = read_env_passwd(passwd2_sum, hash_len);\n\telse if (is_passwd_default_enable())\n\t\tret = read_default_passwd(passwd2_sum, hash_len);\n\telse\n\t\tret = -EINVAL;\n\n\tif (ret < 0)\n\t\tgoto err;\n\n\tif (IS_ENABLED(CONFIG_PASSWD_CRYPTO_PBKDF2)) {\n\t\tchar *key = passwd2_sum + PBKDF2_SALT_LEN;\n\t\tchar *salt = passwd2_sum;\n\t\tint keylen = PBKDF2_LENGTH - PBKDF2_SALT_LEN;\n\n\t\tret = pkcs5_pbkdf2_hmac_sha1(passwd, length, salt,\n\t\t\tPBKDF2_SALT_LEN, PBKDF2_COUNT, keylen, passwd1_sum);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (!crypto_memneq(passwd1_sum, key, keylen))\n\t\t\tret = 1;\n\t} else {\n\t\tret = digest_digest(d, passwd, length, passwd1_sum);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tif (!crypto_memneq(passwd1_sum, passwd2_sum, hash_len))\n\t\t\tret = 1;\n\t}\n\nerr:\n\tfree(passwd1_sum);\n\tdigest_free(d);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -45,7 +45,7 @@\n \t\tif (ret)\n \t\t\tgoto err;\n \n-\t\tif (strncmp(passwd1_sum, key, keylen) == 0)\n+\t\tif (!crypto_memneq(passwd1_sum, key, keylen))\n \t\t\tret = 1;\n \t} else {\n \t\tret = digest_digest(d, passwd, length, passwd1_sum);\n@@ -53,7 +53,7 @@\n \t\tif (ret)\n \t\t\tgoto err;\n \n-\t\tif (strncmp(passwd1_sum, passwd2_sum, hash_len) == 0)\n+\t\tif (!crypto_memneq(passwd1_sum, passwd2_sum, hash_len))\n \t\t\tret = 1;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (strncmp(passwd1_sum, key, keylen) == 0)",
                "\t\tif (strncmp(passwd1_sum, passwd2_sum, hash_len) == 0)"
            ],
            "added_lines": [
                "\t\tif (!crypto_memneq(passwd1_sum, key, keylen))",
                "\t\tif (!crypto_memneq(passwd1_sum, passwd2_sum, hash_len))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-38209",
        "func_name": "torvalds/linux/nf_conntrack_standalone_init_sysctl",
        "description": "net/netfilter/nf_conntrack_standalone.c in the Linux kernel before 5.12.2 allows observation of changes in any net namespace because these changes are leaked into all other net namespaces. This is related to the NF_SYSCTL_CT_MAX, NF_SYSCTL_CT_EXPECT_MAX, and NF_SYSCTL_CT_BUCKETS sysctls.",
        "git_url": "https://github.com/torvalds/linux/commit/2671fa4dc0109d3fb581bc3078fdf17b5d9080f6",
        "commit_title": "netfilter: conntrack: Make global sysctls readonly in non-init netns",
        "commit_text": " These sysctls point to global variables: - NF_SYSCTL_CT_MAX (&nf_conntrack_max) - NF_SYSCTL_CT_EXPECT_MAX (&nf_ct_expect_max) - NF_SYSCTL_CT_BUCKETS (&nf_conntrack_htable_size_user)  Because their data pointers are not updated to point to per-netns structures, they must be marked read-only in a non-init_net ns. Otherwise, changes in any net namespace are reflected in (leaked into) all other net namespaces. This problem has existed since the introduction of net namespaces.  The current logic marks them read-only only if the net namespace is owned by an unprivileged user (other than init_user_ns).  Commit d0febd81ae77 (\"netfilter: conntrack: re-visit sysctls in unprivileged namespaces\") \"exposes all sysctls even if the namespace is unpriviliged.\" Since we need to mark them readonly in any case, we can forego the unprivileged user check altogether. ",
        "func_before": "static int nf_conntrack_standalone_init_sysctl(struct net *net)\n{\n\tstruct nf_conntrack_net *cnet = net_generic(net, nf_conntrack_net_id);\n\tstruct nf_udp_net *un = nf_udp_pernet(net);\n\tstruct ctl_table *table;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(nf_ct_sysctl_table) != NF_SYSCTL_CT_LAST_SYSCTL);\n\n\ttable = kmemdup(nf_ct_sysctl_table, sizeof(nf_ct_sysctl_table),\n\t\t\tGFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\ttable[NF_SYSCTL_CT_COUNT].data = &net->ct.count;\n\ttable[NF_SYSCTL_CT_CHECKSUM].data = &net->ct.sysctl_checksum;\n\ttable[NF_SYSCTL_CT_LOG_INVALID].data = &net->ct.sysctl_log_invalid;\n\ttable[NF_SYSCTL_CT_ACCT].data = &net->ct.sysctl_acct;\n\ttable[NF_SYSCTL_CT_HELPER].data = &net->ct.sysctl_auto_assign_helper;\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\ttable[NF_SYSCTL_CT_EVENTS].data = &net->ct.sysctl_events;\n#endif\n#ifdef CONFIG_NF_CONNTRACK_TIMESTAMP\n\ttable[NF_SYSCTL_CT_TIMESTAMP].data = &net->ct.sysctl_tstamp;\n#endif\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_GENERIC].data = &nf_generic_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_ICMP].data = &nf_icmp_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_ICMPV6].data = &nf_icmpv6_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_UDP].data = &un->timeouts[UDP_CT_UNREPLIED];\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_UDP_STREAM].data = &un->timeouts[UDP_CT_REPLIED];\n\n\tnf_conntrack_standalone_init_tcp_sysctl(net, table);\n\tnf_conntrack_standalone_init_sctp_sysctl(net, table);\n\tnf_conntrack_standalone_init_dccp_sysctl(net, table);\n\tnf_conntrack_standalone_init_gre_sysctl(net, table);\n\n\t/* Don't allow unprivileged users to alter certain sysctls */\n\tif (net->user_ns != &init_user_ns) {\n\t\ttable[NF_SYSCTL_CT_MAX].mode = 0444;\n\t\ttable[NF_SYSCTL_CT_EXPECT_MAX].mode = 0444;\n\t\ttable[NF_SYSCTL_CT_HELPER].mode = 0444;\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\t\ttable[NF_SYSCTL_CT_EVENTS].mode = 0444;\n#endif\n\t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;\n\t} else if (!net_eq(&init_net, net)) {\n\t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;\n\t}\n\n\tcnet->sysctl_header = register_net_sysctl(net, \"net/netfilter\", table);\n\tif (!cnet->sysctl_header)\n\t\tgoto out_unregister_netfilter;\n\n\treturn 0;\n\nout_unregister_netfilter:\n\tkfree(table);\n\treturn -ENOMEM;\n}",
        "func": "static int nf_conntrack_standalone_init_sysctl(struct net *net)\n{\n\tstruct nf_conntrack_net *cnet = net_generic(net, nf_conntrack_net_id);\n\tstruct nf_udp_net *un = nf_udp_pernet(net);\n\tstruct ctl_table *table;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(nf_ct_sysctl_table) != NF_SYSCTL_CT_LAST_SYSCTL);\n\n\ttable = kmemdup(nf_ct_sysctl_table, sizeof(nf_ct_sysctl_table),\n\t\t\tGFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\ttable[NF_SYSCTL_CT_COUNT].data = &net->ct.count;\n\ttable[NF_SYSCTL_CT_CHECKSUM].data = &net->ct.sysctl_checksum;\n\ttable[NF_SYSCTL_CT_LOG_INVALID].data = &net->ct.sysctl_log_invalid;\n\ttable[NF_SYSCTL_CT_ACCT].data = &net->ct.sysctl_acct;\n\ttable[NF_SYSCTL_CT_HELPER].data = &net->ct.sysctl_auto_assign_helper;\n#ifdef CONFIG_NF_CONNTRACK_EVENTS\n\ttable[NF_SYSCTL_CT_EVENTS].data = &net->ct.sysctl_events;\n#endif\n#ifdef CONFIG_NF_CONNTRACK_TIMESTAMP\n\ttable[NF_SYSCTL_CT_TIMESTAMP].data = &net->ct.sysctl_tstamp;\n#endif\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_GENERIC].data = &nf_generic_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_ICMP].data = &nf_icmp_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_ICMPV6].data = &nf_icmpv6_pernet(net)->timeout;\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_UDP].data = &un->timeouts[UDP_CT_UNREPLIED];\n\ttable[NF_SYSCTL_CT_PROTO_TIMEOUT_UDP_STREAM].data = &un->timeouts[UDP_CT_REPLIED];\n\n\tnf_conntrack_standalone_init_tcp_sysctl(net, table);\n\tnf_conntrack_standalone_init_sctp_sysctl(net, table);\n\tnf_conntrack_standalone_init_dccp_sysctl(net, table);\n\tnf_conntrack_standalone_init_gre_sysctl(net, table);\n\n\t/* Don't allow non-init_net ns to alter global sysctls */\n\tif (!net_eq(&init_net, net)) {\n\t\ttable[NF_SYSCTL_CT_MAX].mode = 0444;\n\t\ttable[NF_SYSCTL_CT_EXPECT_MAX].mode = 0444;\n\t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;\n\t}\n\n\tcnet->sysctl_header = register_net_sysctl(net, \"net/netfilter\", table);\n\tif (!cnet->sysctl_header)\n\t\tgoto out_unregister_netfilter;\n\n\treturn 0;\n\nout_unregister_netfilter:\n\tkfree(table);\n\treturn -ENOMEM;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -33,16 +33,10 @@\n \tnf_conntrack_standalone_init_dccp_sysctl(net, table);\n \tnf_conntrack_standalone_init_gre_sysctl(net, table);\n \n-\t/* Don't allow unprivileged users to alter certain sysctls */\n-\tif (net->user_ns != &init_user_ns) {\n+\t/* Don't allow non-init_net ns to alter global sysctls */\n+\tif (!net_eq(&init_net, net)) {\n \t\ttable[NF_SYSCTL_CT_MAX].mode = 0444;\n \t\ttable[NF_SYSCTL_CT_EXPECT_MAX].mode = 0444;\n-\t\ttable[NF_SYSCTL_CT_HELPER].mode = 0444;\n-#ifdef CONFIG_NF_CONNTRACK_EVENTS\n-\t\ttable[NF_SYSCTL_CT_EVENTS].mode = 0444;\n-#endif\n-\t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;\n-\t} else if (!net_eq(&init_net, net)) {\n \t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t/* Don't allow unprivileged users to alter certain sysctls */",
                "\tif (net->user_ns != &init_user_ns) {",
                "\t\ttable[NF_SYSCTL_CT_HELPER].mode = 0444;",
                "#ifdef CONFIG_NF_CONNTRACK_EVENTS",
                "\t\ttable[NF_SYSCTL_CT_EVENTS].mode = 0444;",
                "#endif",
                "\t\ttable[NF_SYSCTL_CT_BUCKETS].mode = 0444;",
                "\t} else if (!net_eq(&init_net, net)) {"
            ],
            "added_lines": [
                "\t/* Don't allow non-init_net ns to alter global sysctls */",
                "\tif (!net_eq(&init_net, net)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-13099",
        "func_name": "wolfSSL/wolfssl/wolfSSL_ERR_reason_error_string",
        "description": "wolfSSL prior to version 3.12.2 provides a weak Bleichenbacher oracle when any TLS cipher suite using RSA key exchange is negotiated. An attacker can recover the private key from a vulnerable wolfSSL application. This vulnerability is referred to as \"ROBOT.\"",
        "git_url": "https://github.com/wolfSSL/wolfssl/commit/fd455d5a5e9fef24c208e7ac7d3a4bc58834cbf1",
        "commit_title": "Fix for handling of static RSA PKCS formatting failures so they are indistinguishable from from correctly formatted RSA blocks (per RFC5246 section 7.4.7.1). Adjusted the static RSA preMasterSecret RNG creation for consistency in client case. Removed obsolete `PMS_VERSION_ERROR`.",
        "commit_text": "",
        "func_before": "const char* wolfSSL_ERR_reason_error_string(unsigned long e)\n{\n#ifdef NO_ERROR_STRINGS\n\n    (void)e;\n    return \"no support for error strings built in\";\n\n#else\n\n    int error = (int)e;\n\n    /* pass to wolfCrypt */\n    if (error < MAX_CODE_E && error > MIN_CODE_E) {\n        return wc_GetErrorString(error);\n    }\n\n    switch (error) {\n\n    case UNSUPPORTED_SUITE :\n        return \"unsupported cipher suite\";\n\n    case INPUT_CASE_ERROR :\n        return \"input state error\";\n\n    case PREFIX_ERROR :\n        return \"bad index to key rounds\";\n\n    case MEMORY_ERROR :\n        return \"out of memory\";\n\n    case VERIFY_FINISHED_ERROR :\n        return \"verify problem on finished\";\n\n    case VERIFY_MAC_ERROR :\n        return \"verify mac problem\";\n\n    case PARSE_ERROR :\n        return \"parse error on header\";\n\n    case SIDE_ERROR :\n        return \"wrong client/server type\";\n\n    case NO_PEER_CERT :\n        return \"peer didn't send cert\";\n\n    case UNKNOWN_HANDSHAKE_TYPE :\n        return \"weird handshake type\";\n\n    case SOCKET_ERROR_E :\n        return \"error state on socket\";\n\n    case SOCKET_NODATA :\n        return \"expected data, not there\";\n\n    case INCOMPLETE_DATA :\n        return \"don't have enough data to complete task\";\n\n    case UNKNOWN_RECORD_TYPE :\n        return \"unknown type in record hdr\";\n\n    case DECRYPT_ERROR :\n        return \"error during decryption\";\n\n    case FATAL_ERROR :\n        return \"revcd alert fatal error\";\n\n    case ENCRYPT_ERROR :\n        return \"error during encryption\";\n\n    case FREAD_ERROR :\n        return \"fread problem\";\n\n    case NO_PEER_KEY :\n        return \"need peer's key\";\n\n    case NO_PRIVATE_KEY :\n        return \"need the private key\";\n\n    case NO_DH_PARAMS :\n        return \"server missing DH params\";\n\n    case RSA_PRIVATE_ERROR :\n        return \"error during rsa priv op\";\n\n    case MATCH_SUITE_ERROR :\n        return \"can't match cipher suite\";\n\n    case COMPRESSION_ERROR :\n        return \"compression mismatch error\";\n\n    case BUILD_MSG_ERROR :\n        return \"build message failure\";\n\n    case BAD_HELLO :\n        return \"client hello malformed\";\n\n    case DOMAIN_NAME_MISMATCH :\n        return \"peer subject name mismatch\";\n\n    case WANT_READ :\n    case WOLFSSL_ERROR_WANT_READ :\n        return \"non-blocking socket wants data to be read\";\n\n    case NOT_READY_ERROR :\n        return \"handshake layer not ready yet, complete first\";\n\n    case PMS_VERSION_ERROR :\n        return \"premaster secret version mismatch error\";\n\n    case VERSION_ERROR :\n        return \"record layer version error\";\n\n    case WANT_WRITE :\n    case WOLFSSL_ERROR_WANT_WRITE :\n        return \"non-blocking socket write buffer full\";\n\n    case BUFFER_ERROR :\n        return \"malformed buffer input error\";\n\n    case VERIFY_CERT_ERROR :\n        return \"verify problem on certificate\";\n\n    case VERIFY_SIGN_ERROR :\n        return \"verify problem based on signature\";\n\n    case CLIENT_ID_ERROR :\n        return \"psk client identity error\";\n\n    case SERVER_HINT_ERROR:\n        return \"psk server hint error\";\n\n    case PSK_KEY_ERROR:\n        return \"psk key callback error\";\n\n    case NTRU_KEY_ERROR:\n        return \"NTRU key error\";\n\n    case NTRU_DRBG_ERROR:\n        return \"NTRU drbg error\";\n\n    case NTRU_ENCRYPT_ERROR:\n        return \"NTRU encrypt error\";\n\n    case NTRU_DECRYPT_ERROR:\n        return \"NTRU decrypt error\";\n\n    case ZLIB_INIT_ERROR:\n        return \"zlib init error\";\n\n    case ZLIB_COMPRESS_ERROR:\n        return \"zlib compress error\";\n\n    case ZLIB_DECOMPRESS_ERROR:\n        return \"zlib decompress error\";\n\n    case GETTIME_ERROR:\n        return \"gettimeofday() error\";\n\n    case GETITIMER_ERROR:\n        return \"getitimer() error\";\n\n    case SIGACT_ERROR:\n        return \"sigaction() error\";\n\n    case SETITIMER_ERROR:\n        return \"setitimer() error\";\n\n    case LENGTH_ERROR:\n        return \"record layer length error\";\n\n    case PEER_KEY_ERROR:\n        return \"cant decode peer key\";\n\n    case ZERO_RETURN:\n    case WOLFSSL_ERROR_ZERO_RETURN:\n        return \"peer sent close notify alert\";\n\n    case ECC_CURVETYPE_ERROR:\n        return \"Bad ECC Curve Type or unsupported\";\n\n    case ECC_CURVE_ERROR:\n        return \"Bad ECC Curve or unsupported\";\n\n    case ECC_PEERKEY_ERROR:\n        return \"Bad ECC Peer Key\";\n\n    case ECC_MAKEKEY_ERROR:\n        return \"ECC Make Key failure\";\n\n    case ECC_EXPORT_ERROR:\n        return \"ECC Export Key failure\";\n\n    case ECC_SHARED_ERROR:\n        return \"ECC DHE shared failure\";\n\n    case NOT_CA_ERROR:\n        return \"Not a CA by basic constraint error\";\n\n    case HTTP_TIMEOUT:\n        return \"HTTP timeout for OCSP or CRL req\";\n\n    case BAD_CERT_MANAGER_ERROR:\n        return \"Bad Cert Manager error\";\n\n    case OCSP_CERT_REVOKED:\n        return \"OCSP Cert revoked\";\n\n    case CRL_CERT_REVOKED:\n        return \"CRL Cert revoked\";\n\n    case CRL_MISSING:\n        return \"CRL missing, not loaded\";\n\n    case MONITOR_SETUP_E:\n        return \"CRL monitor setup error\";\n\n    case THREAD_CREATE_E:\n        return \"Thread creation problem\";\n\n    case OCSP_NEED_URL:\n        return \"OCSP need URL\";\n\n    case OCSP_CERT_UNKNOWN:\n        return \"OCSP Cert unknown\";\n\n    case OCSP_LOOKUP_FAIL:\n        return \"OCSP Responder lookup fail\";\n\n    case MAX_CHAIN_ERROR:\n        return \"Maximum Chain Depth Exceeded\";\n\n    case COOKIE_ERROR:\n        return \"DTLS Cookie Error\";\n\n    case SEQUENCE_ERROR:\n        return \"DTLS Sequence Error\";\n\n    case SUITES_ERROR:\n        return \"Suites Pointer Error\";\n\n    case SSL_NO_PEM_HEADER:\n        return \"No PEM Header Error\";\n\n    case OUT_OF_ORDER_E:\n        return \"Out of order message, fatal\";\n\n    case BAD_KEA_TYPE_E:\n        return \"Bad KEA type found\";\n\n    case SANITY_CIPHER_E:\n        return \"Sanity check on ciphertext failed\";\n\n    case RECV_OVERFLOW_E:\n        return \"Receive callback returned more than requested\";\n\n    case GEN_COOKIE_E:\n        return \"Generate Cookie Error\";\n\n    case NO_PEER_VERIFY:\n        return \"Need peer certificate verify Error\";\n\n    case FWRITE_ERROR:\n        return \"fwrite Error\";\n\n    case CACHE_MATCH_ERROR:\n        return \"Cache restore header match Error\";\n\n    case UNKNOWN_SNI_HOST_NAME_E:\n        return \"Unrecognized host name Error\";\n\n    case UNKNOWN_MAX_FRAG_LEN_E:\n        return \"Unrecognized max frag len Error\";\n\n    case KEYUSE_SIGNATURE_E:\n        return \"Key Use digitalSignature not set Error\";\n\n    case KEYUSE_ENCIPHER_E:\n        return \"Key Use keyEncipherment not set Error\";\n\n    case EXTKEYUSE_AUTH_E:\n        return \"Ext Key Use server/client auth not set Error\";\n\n    case SEND_OOB_READ_E:\n        return \"Send Callback Out of Bounds Read Error\";\n\n    case SECURE_RENEGOTIATION_E:\n        return \"Invalid Renegotiation Error\";\n\n    case SESSION_TICKET_LEN_E:\n        return \"Session Ticket Too Long Error\";\n\n    case SESSION_TICKET_EXPECT_E:\n        return \"Session Ticket Error\";\n\n    case SCR_DIFFERENT_CERT_E:\n        return \"Peer sent different cert during SCR\";\n\n    case SESSION_SECRET_CB_E:\n        return \"Session Secret Callback Error\";\n\n    case NO_CHANGE_CIPHER_E:\n        return \"Finished received from peer before Change Cipher Error\";\n\n    case SANITY_MSG_E:\n        return \"Sanity Check on message order Error\";\n\n    case DUPLICATE_MSG_E:\n        return \"Duplicate HandShake message Error\";\n\n    case SNI_UNSUPPORTED:\n        return \"Protocol version does not support SNI Error\";\n\n    case SOCKET_PEER_CLOSED_E:\n        return \"Peer closed underlying transport Error\";\n\n    case BAD_TICKET_KEY_CB_SZ:\n        return \"Bad user session ticket key callback Size Error\";\n\n    case BAD_TICKET_MSG_SZ:\n        return \"Bad session ticket message Size Error\";\n\n    case BAD_TICKET_ENCRYPT:\n        return \"Bad user ticket callback encrypt Error\";\n\n    case DH_KEY_SIZE_E:\n        return \"DH key too small Error\";\n\n    case SNI_ABSENT_ERROR:\n        return \"No Server Name Indication extension Error\";\n\n    case RSA_SIGN_FAULT:\n        return \"RSA Signature Fault Error\";\n\n    case HANDSHAKE_SIZE_ERROR:\n        return \"Handshake message too large Error\";\n\n    case UNKNOWN_ALPN_PROTOCOL_NAME_E:\n        return \"Unrecognized protocol name Error\";\n\n    case BAD_CERTIFICATE_STATUS_ERROR:\n        return \"Bad Certificate Status Message Error\";\n\n    case OCSP_INVALID_STATUS:\n        return \"Invalid OCSP Status Error\";\n\n    case RSA_KEY_SIZE_E:\n        return \"RSA key too small\";\n\n    case ECC_KEY_SIZE_E:\n        return \"ECC key too small\";\n\n    case DTLS_EXPORT_VER_E:\n        return \"Version needs updated after code change or version mismatch\";\n\n    case INPUT_SIZE_E:\n        return \"Input size too large Error\";\n\n    case CTX_INIT_MUTEX_E:\n        return \"Initialize ctx mutex error\";\n\n    case EXT_MASTER_SECRET_NEEDED_E:\n        return \"Extended Master Secret must be enabled to resume EMS session\";\n\n    case DTLS_POOL_SZ_E:\n        return \"Maximum DTLS pool size exceeded\";\n\n    case DECODE_E:\n        return \"Decode handshake message error\";\n\n    case WRITE_DUP_READ_E:\n        return \"Write dup write side can't read error\";\n\n    case WRITE_DUP_WRITE_E:\n        return \"Write dup read side can't write error\";\n\n    case INVALID_CERT_CTX_E:\n        return \"Certificate context does not match request or not empty\";\n\n    case BAD_KEY_SHARE_DATA:\n        return \"The Key Share data contains group that was in Client Hello\";\n\n    case MISSING_HANDSHAKE_DATA:\n        return \"The handshake message is missing required data\";\n\n    case BAD_BINDER:\n        return \"Binder value does not match value server calculated\";\n\n    case EXT_NOT_ALLOWED:\n        return \"Extension type not allowed in handshake message type\";\n\n    case INVALID_PARAMETER:\n        return \"The security parameter is invalid\";\n\n    case UNSUPPORTED_EXTENSION:\n        return \"TLS Extension not requested by the client\";\n\n    case KEY_SHARE_ERROR:\n        return \"Key share extension did not contain a valid named group\";\n\n    case POST_HAND_AUTH_ERROR:\n        return \"Client will not do post handshake authentication\";\n\n    case HRR_COOKIE_ERROR:\n        return \"Cookie does not match one sent in HelloRetryRequest\";\n\n    case MCAST_HIGHWATER_CB_E:\n        return \"Multicast highwater callback returned error\";\n\n    case ALERT_COUNT_E:\n        return \"Alert Count exceeded error\";\n\n    case EXT_MISSING:\n        return \"Required TLS extension missing\";\n\n    default :\n        return \"unknown error number\";\n    }\n\n#endif /* NO_ERROR_STRINGS */\n}",
        "func": "const char* wolfSSL_ERR_reason_error_string(unsigned long e)\n{\n#ifdef NO_ERROR_STRINGS\n\n    (void)e;\n    return \"no support for error strings built in\";\n\n#else\n\n    int error = (int)e;\n\n    /* pass to wolfCrypt */\n    if (error < MAX_CODE_E && error > MIN_CODE_E) {\n        return wc_GetErrorString(error);\n    }\n\n    switch (error) {\n\n    case UNSUPPORTED_SUITE :\n        return \"unsupported cipher suite\";\n\n    case INPUT_CASE_ERROR :\n        return \"input state error\";\n\n    case PREFIX_ERROR :\n        return \"bad index to key rounds\";\n\n    case MEMORY_ERROR :\n        return \"out of memory\";\n\n    case VERIFY_FINISHED_ERROR :\n        return \"verify problem on finished\";\n\n    case VERIFY_MAC_ERROR :\n        return \"verify mac problem\";\n\n    case PARSE_ERROR :\n        return \"parse error on header\";\n\n    case SIDE_ERROR :\n        return \"wrong client/server type\";\n\n    case NO_PEER_CERT :\n        return \"peer didn't send cert\";\n\n    case UNKNOWN_HANDSHAKE_TYPE :\n        return \"weird handshake type\";\n\n    case SOCKET_ERROR_E :\n        return \"error state on socket\";\n\n    case SOCKET_NODATA :\n        return \"expected data, not there\";\n\n    case INCOMPLETE_DATA :\n        return \"don't have enough data to complete task\";\n\n    case UNKNOWN_RECORD_TYPE :\n        return \"unknown type in record hdr\";\n\n    case DECRYPT_ERROR :\n        return \"error during decryption\";\n\n    case FATAL_ERROR :\n        return \"revcd alert fatal error\";\n\n    case ENCRYPT_ERROR :\n        return \"error during encryption\";\n\n    case FREAD_ERROR :\n        return \"fread problem\";\n\n    case NO_PEER_KEY :\n        return \"need peer's key\";\n\n    case NO_PRIVATE_KEY :\n        return \"need the private key\";\n\n    case NO_DH_PARAMS :\n        return \"server missing DH params\";\n\n    case RSA_PRIVATE_ERROR :\n        return \"error during rsa priv op\";\n\n    case MATCH_SUITE_ERROR :\n        return \"can't match cipher suite\";\n\n    case COMPRESSION_ERROR :\n        return \"compression mismatch error\";\n\n    case BUILD_MSG_ERROR :\n        return \"build message failure\";\n\n    case BAD_HELLO :\n        return \"client hello malformed\";\n\n    case DOMAIN_NAME_MISMATCH :\n        return \"peer subject name mismatch\";\n\n    case WANT_READ :\n    case WOLFSSL_ERROR_WANT_READ :\n        return \"non-blocking socket wants data to be read\";\n\n    case NOT_READY_ERROR :\n        return \"handshake layer not ready yet, complete first\";\n\n    case VERSION_ERROR :\n        return \"record layer version error\";\n\n    case WANT_WRITE :\n    case WOLFSSL_ERROR_WANT_WRITE :\n        return \"non-blocking socket write buffer full\";\n\n    case BUFFER_ERROR :\n        return \"malformed buffer input error\";\n\n    case VERIFY_CERT_ERROR :\n        return \"verify problem on certificate\";\n\n    case VERIFY_SIGN_ERROR :\n        return \"verify problem based on signature\";\n\n    case CLIENT_ID_ERROR :\n        return \"psk client identity error\";\n\n    case SERVER_HINT_ERROR:\n        return \"psk server hint error\";\n\n    case PSK_KEY_ERROR:\n        return \"psk key callback error\";\n\n    case NTRU_KEY_ERROR:\n        return \"NTRU key error\";\n\n    case NTRU_DRBG_ERROR:\n        return \"NTRU drbg error\";\n\n    case NTRU_ENCRYPT_ERROR:\n        return \"NTRU encrypt error\";\n\n    case NTRU_DECRYPT_ERROR:\n        return \"NTRU decrypt error\";\n\n    case ZLIB_INIT_ERROR:\n        return \"zlib init error\";\n\n    case ZLIB_COMPRESS_ERROR:\n        return \"zlib compress error\";\n\n    case ZLIB_DECOMPRESS_ERROR:\n        return \"zlib decompress error\";\n\n    case GETTIME_ERROR:\n        return \"gettimeofday() error\";\n\n    case GETITIMER_ERROR:\n        return \"getitimer() error\";\n\n    case SIGACT_ERROR:\n        return \"sigaction() error\";\n\n    case SETITIMER_ERROR:\n        return \"setitimer() error\";\n\n    case LENGTH_ERROR:\n        return \"record layer length error\";\n\n    case PEER_KEY_ERROR:\n        return \"cant decode peer key\";\n\n    case ZERO_RETURN:\n    case WOLFSSL_ERROR_ZERO_RETURN:\n        return \"peer sent close notify alert\";\n\n    case ECC_CURVETYPE_ERROR:\n        return \"Bad ECC Curve Type or unsupported\";\n\n    case ECC_CURVE_ERROR:\n        return \"Bad ECC Curve or unsupported\";\n\n    case ECC_PEERKEY_ERROR:\n        return \"Bad ECC Peer Key\";\n\n    case ECC_MAKEKEY_ERROR:\n        return \"ECC Make Key failure\";\n\n    case ECC_EXPORT_ERROR:\n        return \"ECC Export Key failure\";\n\n    case ECC_SHARED_ERROR:\n        return \"ECC DHE shared failure\";\n\n    case NOT_CA_ERROR:\n        return \"Not a CA by basic constraint error\";\n\n    case HTTP_TIMEOUT:\n        return \"HTTP timeout for OCSP or CRL req\";\n\n    case BAD_CERT_MANAGER_ERROR:\n        return \"Bad Cert Manager error\";\n\n    case OCSP_CERT_REVOKED:\n        return \"OCSP Cert revoked\";\n\n    case CRL_CERT_REVOKED:\n        return \"CRL Cert revoked\";\n\n    case CRL_MISSING:\n        return \"CRL missing, not loaded\";\n\n    case MONITOR_SETUP_E:\n        return \"CRL monitor setup error\";\n\n    case THREAD_CREATE_E:\n        return \"Thread creation problem\";\n\n    case OCSP_NEED_URL:\n        return \"OCSP need URL\";\n\n    case OCSP_CERT_UNKNOWN:\n        return \"OCSP Cert unknown\";\n\n    case OCSP_LOOKUP_FAIL:\n        return \"OCSP Responder lookup fail\";\n\n    case MAX_CHAIN_ERROR:\n        return \"Maximum Chain Depth Exceeded\";\n\n    case COOKIE_ERROR:\n        return \"DTLS Cookie Error\";\n\n    case SEQUENCE_ERROR:\n        return \"DTLS Sequence Error\";\n\n    case SUITES_ERROR:\n        return \"Suites Pointer Error\";\n\n    case SSL_NO_PEM_HEADER:\n        return \"No PEM Header Error\";\n\n    case OUT_OF_ORDER_E:\n        return \"Out of order message, fatal\";\n\n    case BAD_KEA_TYPE_E:\n        return \"Bad KEA type found\";\n\n    case SANITY_CIPHER_E:\n        return \"Sanity check on ciphertext failed\";\n\n    case RECV_OVERFLOW_E:\n        return \"Receive callback returned more than requested\";\n\n    case GEN_COOKIE_E:\n        return \"Generate Cookie Error\";\n\n    case NO_PEER_VERIFY:\n        return \"Need peer certificate verify Error\";\n\n    case FWRITE_ERROR:\n        return \"fwrite Error\";\n\n    case CACHE_MATCH_ERROR:\n        return \"Cache restore header match Error\";\n\n    case UNKNOWN_SNI_HOST_NAME_E:\n        return \"Unrecognized host name Error\";\n\n    case UNKNOWN_MAX_FRAG_LEN_E:\n        return \"Unrecognized max frag len Error\";\n\n    case KEYUSE_SIGNATURE_E:\n        return \"Key Use digitalSignature not set Error\";\n\n    case KEYUSE_ENCIPHER_E:\n        return \"Key Use keyEncipherment not set Error\";\n\n    case EXTKEYUSE_AUTH_E:\n        return \"Ext Key Use server/client auth not set Error\";\n\n    case SEND_OOB_READ_E:\n        return \"Send Callback Out of Bounds Read Error\";\n\n    case SECURE_RENEGOTIATION_E:\n        return \"Invalid Renegotiation Error\";\n\n    case SESSION_TICKET_LEN_E:\n        return \"Session Ticket Too Long Error\";\n\n    case SESSION_TICKET_EXPECT_E:\n        return \"Session Ticket Error\";\n\n    case SCR_DIFFERENT_CERT_E:\n        return \"Peer sent different cert during SCR\";\n\n    case SESSION_SECRET_CB_E:\n        return \"Session Secret Callback Error\";\n\n    case NO_CHANGE_CIPHER_E:\n        return \"Finished received from peer before Change Cipher Error\";\n\n    case SANITY_MSG_E:\n        return \"Sanity Check on message order Error\";\n\n    case DUPLICATE_MSG_E:\n        return \"Duplicate HandShake message Error\";\n\n    case SNI_UNSUPPORTED:\n        return \"Protocol version does not support SNI Error\";\n\n    case SOCKET_PEER_CLOSED_E:\n        return \"Peer closed underlying transport Error\";\n\n    case BAD_TICKET_KEY_CB_SZ:\n        return \"Bad user session ticket key callback Size Error\";\n\n    case BAD_TICKET_MSG_SZ:\n        return \"Bad session ticket message Size Error\";\n\n    case BAD_TICKET_ENCRYPT:\n        return \"Bad user ticket callback encrypt Error\";\n\n    case DH_KEY_SIZE_E:\n        return \"DH key too small Error\";\n\n    case SNI_ABSENT_ERROR:\n        return \"No Server Name Indication extension Error\";\n\n    case RSA_SIGN_FAULT:\n        return \"RSA Signature Fault Error\";\n\n    case HANDSHAKE_SIZE_ERROR:\n        return \"Handshake message too large Error\";\n\n    case UNKNOWN_ALPN_PROTOCOL_NAME_E:\n        return \"Unrecognized protocol name Error\";\n\n    case BAD_CERTIFICATE_STATUS_ERROR:\n        return \"Bad Certificate Status Message Error\";\n\n    case OCSP_INVALID_STATUS:\n        return \"Invalid OCSP Status Error\";\n\n    case RSA_KEY_SIZE_E:\n        return \"RSA key too small\";\n\n    case ECC_KEY_SIZE_E:\n        return \"ECC key too small\";\n\n    case DTLS_EXPORT_VER_E:\n        return \"Version needs updated after code change or version mismatch\";\n\n    case INPUT_SIZE_E:\n        return \"Input size too large Error\";\n\n    case CTX_INIT_MUTEX_E:\n        return \"Initialize ctx mutex error\";\n\n    case EXT_MASTER_SECRET_NEEDED_E:\n        return \"Extended Master Secret must be enabled to resume EMS session\";\n\n    case DTLS_POOL_SZ_E:\n        return \"Maximum DTLS pool size exceeded\";\n\n    case DECODE_E:\n        return \"Decode handshake message error\";\n\n    case WRITE_DUP_READ_E:\n        return \"Write dup write side can't read error\";\n\n    case WRITE_DUP_WRITE_E:\n        return \"Write dup read side can't write error\";\n\n    case INVALID_CERT_CTX_E:\n        return \"Certificate context does not match request or not empty\";\n\n    case BAD_KEY_SHARE_DATA:\n        return \"The Key Share data contains group that was in Client Hello\";\n\n    case MISSING_HANDSHAKE_DATA:\n        return \"The handshake message is missing required data\";\n\n    case BAD_BINDER:\n        return \"Binder value does not match value server calculated\";\n\n    case EXT_NOT_ALLOWED:\n        return \"Extension type not allowed in handshake message type\";\n\n    case INVALID_PARAMETER:\n        return \"The security parameter is invalid\";\n\n    case UNSUPPORTED_EXTENSION:\n        return \"TLS Extension not requested by the client\";\n\n    case KEY_SHARE_ERROR:\n        return \"Key share extension did not contain a valid named group\";\n\n    case POST_HAND_AUTH_ERROR:\n        return \"Client will not do post handshake authentication\";\n\n    case HRR_COOKIE_ERROR:\n        return \"Cookie does not match one sent in HelloRetryRequest\";\n\n    case MCAST_HIGHWATER_CB_E:\n        return \"Multicast highwater callback returned error\";\n\n    case ALERT_COUNT_E:\n        return \"Alert Count exceeded error\";\n\n    case EXT_MISSING:\n        return \"Required TLS extension missing\";\n\n    default :\n        return \"unknown error number\";\n    }\n\n#endif /* NO_ERROR_STRINGS */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -104,9 +104,6 @@\n     case NOT_READY_ERROR :\n         return \"handshake layer not ready yet, complete first\";\n \n-    case PMS_VERSION_ERROR :\n-        return \"premaster secret version mismatch error\";\n-\n     case VERSION_ERROR :\n         return \"record layer version error\";\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    case PMS_VERSION_ERROR :",
                "        return \"premaster secret version mismatch error\";",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2016-2178",
        "func_name": "openssl/dsa_sign_setup",
        "description": "The dsa_sign_setup function in crypto/dsa/dsa_ossl.c in OpenSSL through 1.0.2h does not properly ensure the use of constant-time operations, which makes it easier for local users to discover a DSA private key via a timing side-channel attack.",
        "git_url": "https://git.openssl.org/?p=openssl.git;a=commit;h=399944622df7bd81af62e67ea967c470534090e2",
        "commit_title": "",
        "commit_text": "Fix DSA, preserve BN_FLG_CONSTTIME  Operations in the DSA signing algorithm should run in constant time in order to avoid side channel attacks. A flaw in the OpenSSL DSA implementation means that a non-constant time codepath is followed for certain operations. This has been demonstrated through a cache-timing attack to be sufficient for an attacker to recover the private DSA key.  CVE-2016-2178  ",
        "func_before": "static int dsa_sign_setup(DSA *dsa, BN_CTX *ctx_in,\n                          BIGNUM **kinvp, BIGNUM **rp,\n                          const unsigned char *dgst, int dlen)\n{\n    BN_CTX *ctx = NULL;\n    BIGNUM *k, *kq, *K, *kinv = NULL, *r = *rp;\n    int ret = 0;\n\n    if (!dsa->p || !dsa->q || !dsa->g) {\n        DSAerr(DSA_F_DSA_SIGN_SETUP, DSA_R_MISSING_PARAMETERS);\n        return 0;\n    }\n\n    k = BN_new();\n    kq = BN_new();\n    if (k == NULL || kq == NULL)\n        goto err;\n\n    if (ctx_in == NULL) {\n        if ((ctx = BN_CTX_new()) == NULL)\n            goto err;\n    } else\n        ctx = ctx_in;\n\n    /* Get random k */\n    do {\n        if (dgst != NULL) {\n            /*\n             * We calculate k from SHA512(private_key + H(message) + random).\n             * This protects the private key from a weak PRNG.\n             */\n            if (!BN_generate_dsa_nonce(k, dsa->q, dsa->priv_key, dgst,\n                                       dlen, ctx))\n                goto err;\n        } else if (!BN_rand_range(k, dsa->q))\n            goto err;\n    } while (BN_is_zero(k));\n\n    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n        BN_set_flags(k, BN_FLG_CONSTTIME);\n    }\n\n    if (dsa->flags & DSA_FLAG_CACHE_MONT_P) {\n        if (!BN_MONT_CTX_set_locked(&dsa->method_mont_p,\n                                    dsa->lock, dsa->p, ctx))\n            goto err;\n    }\n\n    /* Compute r = (g^k mod p) mod q */\n\n    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n        if (!BN_copy(kq, k))\n            goto err;\n\n        /*\n         * We do not want timing information to leak the length of k, so we\n         * compute g^k using an equivalent exponent of fixed length. (This\n         * is a kludge that we need because the BN_mod_exp_mont() does not\n         * let us specify the desired timing behaviour.)\n         */\n\n        if (!BN_add(kq, kq, dsa->q))\n            goto err;\n        if (BN_num_bits(kq) <= BN_num_bits(dsa->q)) {\n            if (!BN_add(kq, kq, dsa->q))\n                goto err;\n        }\n\n        K = kq;\n    } else {\n        K = k;\n    }\n    DSA_BN_MOD_EXP(goto err, dsa, r, dsa->g, K, dsa->p, ctx,\n                   dsa->method_mont_p);\n    if (!BN_mod(r, r, dsa->q, ctx))\n        goto err;\n\n    /* Compute  part of 's = inv(k) (m + xr) mod q' */\n    if ((kinv = BN_mod_inverse(NULL, k, dsa->q, ctx)) == NULL)\n        goto err;\n\n    BN_clear_free(*kinvp);\n    *kinvp = kinv;\n    kinv = NULL;\n    ret = 1;\n err:\n    if (!ret)\n        DSAerr(DSA_F_DSA_SIGN_SETUP, ERR_R_BN_LIB);\n    if (ctx != ctx_in)\n        BN_CTX_free(ctx);\n    BN_clear_free(k);\n    BN_clear_free(kq);\n    return ret;\n}",
        "func": "static int dsa_sign_setup(DSA *dsa, BN_CTX *ctx_in,\n                          BIGNUM **kinvp, BIGNUM **rp,\n                          const unsigned char *dgst, int dlen)\n{\n    BN_CTX *ctx = NULL;\n    BIGNUM *k, *kq, *K, *kinv = NULL, *r = *rp;\n    int ret = 0;\n\n    if (!dsa->p || !dsa->q || !dsa->g) {\n        DSAerr(DSA_F_DSA_SIGN_SETUP, DSA_R_MISSING_PARAMETERS);\n        return 0;\n    }\n\n    k = BN_new();\n    kq = BN_new();\n    if (k == NULL || kq == NULL)\n        goto err;\n\n    if (ctx_in == NULL) {\n        if ((ctx = BN_CTX_new()) == NULL)\n            goto err;\n    } else\n        ctx = ctx_in;\n\n    /* Get random k */\n    do {\n        if (dgst != NULL) {\n            /*\n             * We calculate k from SHA512(private_key + H(message) + random).\n             * This protects the private key from a weak PRNG.\n             */\n            if (!BN_generate_dsa_nonce(k, dsa->q, dsa->priv_key, dgst,\n                                       dlen, ctx))\n                goto err;\n        } else if (!BN_rand_range(k, dsa->q))\n            goto err;\n    } while (BN_is_zero(k));\n\n    if (dsa->flags & DSA_FLAG_CACHE_MONT_P) {\n        if (!BN_MONT_CTX_set_locked(&dsa->method_mont_p,\n                                    dsa->lock, dsa->p, ctx))\n            goto err;\n    }\n\n    /* Compute r = (g^k mod p) mod q */\n\n    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n        if (!BN_copy(kq, k))\n            goto err;\n\n        /*\n         * We do not want timing information to leak the length of k, so we\n         * compute g^k using an equivalent exponent of fixed length. (This\n         * is a kludge that we need because the BN_mod_exp_mont() does not\n         * let us specify the desired timing behaviour.)\n         */\n\n        if (!BN_add(kq, kq, dsa->q))\n            goto err;\n        if (BN_num_bits(kq) <= BN_num_bits(dsa->q)) {\n            if (!BN_add(kq, kq, dsa->q))\n                goto err;\n        }\n\n        K = kq;\n    } else {\n        K = k;\n    }\n\n    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n        BN_set_flags(K, BN_FLG_CONSTTIME);\n    }\n\n    DSA_BN_MOD_EXP(goto err, dsa, r, dsa->g, K, dsa->p, ctx,\n                   dsa->method_mont_p);\n    if (!BN_mod(r, r, dsa->q, ctx))\n        goto err;\n\n    /* Compute  part of 's = inv(k) (m + xr) mod q' */\n    if ((kinv = BN_mod_inverse(NULL, k, dsa->q, ctx)) == NULL)\n        goto err;\n\n    BN_clear_free(*kinvp);\n    *kinvp = kinv;\n    kinv = NULL;\n    ret = 1;\n err:\n    if (!ret)\n        DSAerr(DSA_F_DSA_SIGN_SETUP, ERR_R_BN_LIB);\n    if (ctx != ctx_in)\n        BN_CTX_free(ctx);\n    BN_clear_free(k);\n    BN_clear_free(kq);\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,10 +36,6 @@\n             goto err;\n     } while (BN_is_zero(k));\n \n-    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n-        BN_set_flags(k, BN_FLG_CONSTTIME);\n-    }\n-\n     if (dsa->flags & DSA_FLAG_CACHE_MONT_P) {\n         if (!BN_MONT_CTX_set_locked(&dsa->method_mont_p,\n                                     dsa->lock, dsa->p, ctx))\n@@ -70,6 +66,11 @@\n     } else {\n         K = k;\n     }\n+\n+    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {\n+        BN_set_flags(K, BN_FLG_CONSTTIME);\n+    }\n+\n     DSA_BN_MOD_EXP(goto err, dsa, r, dsa->g, K, dsa->p, ctx,\n                    dsa->method_mont_p);\n     if (!BN_mod(r, r, dsa->q, ctx))",
        "diff_line_info": {
            "deleted_lines": [
                "    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {",
                "        BN_set_flags(k, BN_FLG_CONSTTIME);",
                "    }",
                ""
            ],
            "added_lines": [
                "",
                "    if ((dsa->flags & DSA_FLAG_NO_EXP_CONSTTIME) == 0) {",
                "        BN_set_flags(K, BN_FLG_CONSTTIME);",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-4087",
        "func_name": "ipxe/tls_new_ciphertext",
        "description": "A vulnerability was found in iPXE. It has been declared as problematic. This vulnerability affects the function tls_new_ciphertext of the file src/net/tls.c of the component TLS. The manipulation of the argument pad_len leads to information exposure through discrepancy. The name of the patch is 186306d6199096b7a7c4b4574d4be8cdb8426729. It is recommended to apply a patch to fix this issue. VDB-214054 is the identifier assigned to this vulnerability.",
        "git_url": "https://github.com/ipxe/ipxe/commit/186306d6199096b7a7c4b4574d4be8cdb8426729",
        "commit_title": "[tls] Treat invalid block padding as zero length padding",
        "commit_text": " Harden against padding oracle attacks by treating invalid block padding as zero length padding, thereby deferring the failure until after computing the (incorrect) MAC. ",
        "func_before": "static int tls_new_ciphertext ( struct tls_connection *tls,\n\t\t\t\tstruct tls_header *tlshdr,\n\t\t\t\tstruct list_head *rx_data ) {\n\tstruct tls_cipherspec *cipherspec = &tls->rx_cipherspec;\n\tstruct tls_cipher_suite *suite = cipherspec->suite;\n\tstruct cipher_algorithm *cipher = suite->cipher;\n\tstruct digest_algorithm *digest = suite->digest;\n\tsize_t len = ntohs ( tlshdr->length );\n\tstruct {\n\t\tuint8_t fixed[suite->fixed_iv_len];\n\t\tuint8_t record[suite->record_iv_len];\n\t} __attribute__ (( packed )) iv;\n\tstruct tls_auth_header authhdr;\n\tuint8_t verify_mac[digest->digestsize];\n\tstruct io_buffer *first;\n\tstruct io_buffer *last;\n\tstruct io_buffer *iobuf;\n\tvoid *mac;\n\tsize_t check_len;\n\tint pad_len;\n\tint rc;\n\n\t/* Locate first and last data buffers */\n\tassert ( ! list_empty ( rx_data ) );\n\tfirst = list_first_entry ( rx_data, struct io_buffer, list );\n\tlast = list_last_entry ( rx_data, struct io_buffer, list );\n\n\t/* Extract initialisation vector */\n\tif ( iob_len ( first ) < sizeof ( iv.record ) ) {\n\t\tDBGC ( tls, \"TLS %p received underlength IV\\n\", tls );\n\t\tDBGC_HD ( tls, first->data, iob_len ( first ) );\n\t\treturn -EINVAL_IV;\n\t}\n\tmemcpy ( iv.fixed, cipherspec->fixed_iv, sizeof ( iv.fixed ) );\n\tmemcpy ( iv.record, first->data, sizeof ( iv.record ) );\n\tiob_pull ( first, sizeof ( iv.record ) );\n\tlen -= sizeof ( iv.record );\n\n\t/* Construct authentication data */\n\tauthhdr.seq = cpu_to_be64 ( tls->rx_seq );\n\tauthhdr.header.type = tlshdr->type;\n\tauthhdr.header.version = tlshdr->version;\n\tauthhdr.header.length = htons ( len );\n\n\t/* Set initialisation vector */\n\tcipher_setiv ( cipher, cipherspec->cipher_ctx, &iv, sizeof ( iv ) );\n\n\t/* Decrypt the received data */\n\tcheck_len = 0;\n\tlist_for_each_entry ( iobuf, &tls->rx_data, list ) {\n\t\tcipher_decrypt ( cipher, cipherspec->cipher_ctx,\n\t\t\t\t iobuf->data, iobuf->data, iob_len ( iobuf ) );\n\t\tcheck_len += iob_len ( iobuf );\n\t}\n\tassert ( check_len == len );\n\n\t/* Strip block padding, if applicable */\n\tif ( is_block_cipher ( cipher ) ) {\n\t\tpad_len = tls_verify_padding ( tls, last );\n\t\tif ( pad_len < 0 ) {\n\t\t\trc = pad_len;\n\t\t\treturn rc;\n\t\t}\n\t\tiob_unput ( last, pad_len );\n\t\tlen -= pad_len;\n\t}\n\n\t/* Extract decrypted MAC */\n\tif ( iob_len ( last ) < suite->mac_len ) {\n\t\tDBGC ( tls, \"TLS %p received underlength MAC\\n\", tls );\n\t\tDBGC_HD ( tls, last->data, iob_len ( last ) );\n\t\treturn -EINVAL_MAC;\n\t}\n\tiob_unput ( last, suite->mac_len );\n\tlen -= suite->mac_len;\n\tmac = last->tail;\n\n\t/* Dump received data */\n\tDBGC2 ( tls, \"Received plaintext data:\\n\" );\n\tcheck_len = 0;\n\tlist_for_each_entry ( iobuf, rx_data, list ) {\n\t\tDBGC2_HD ( tls, iobuf->data, iob_len ( iobuf ) );\n\t\tcheck_len += iob_len ( iobuf );\n\t}\n\tassert ( check_len == len );\n\n\t/* Generate MAC */\n\tauthhdr.header.length = htons ( len );\n\tif ( suite->mac_len )\n\t\ttls_hmac_list ( cipherspec, &authhdr, rx_data, verify_mac );\n\n\t/* Verify MAC */\n\tif ( memcmp ( mac, verify_mac, suite->mac_len ) != 0 ) {\n\t\tDBGC ( tls, \"TLS %p failed MAC verification\\n\", tls );\n\t\treturn -EINVAL_MAC;\n\t}\n\n\t/* Process plaintext record */\n\tif ( ( rc = tls_new_record ( tls, tlshdr->type, rx_data ) ) != 0 )\n\t\treturn rc;\n\n\treturn 0;\n}",
        "func": "static int tls_new_ciphertext ( struct tls_connection *tls,\n\t\t\t\tstruct tls_header *tlshdr,\n\t\t\t\tstruct list_head *rx_data ) {\n\tstruct tls_cipherspec *cipherspec = &tls->rx_cipherspec;\n\tstruct tls_cipher_suite *suite = cipherspec->suite;\n\tstruct cipher_algorithm *cipher = suite->cipher;\n\tstruct digest_algorithm *digest = suite->digest;\n\tsize_t len = ntohs ( tlshdr->length );\n\tstruct {\n\t\tuint8_t fixed[suite->fixed_iv_len];\n\t\tuint8_t record[suite->record_iv_len];\n\t} __attribute__ (( packed )) iv;\n\tstruct tls_auth_header authhdr;\n\tuint8_t verify_mac[digest->digestsize];\n\tstruct io_buffer *first;\n\tstruct io_buffer *last;\n\tstruct io_buffer *iobuf;\n\tvoid *mac;\n\tsize_t check_len;\n\tint pad_len;\n\tint rc;\n\n\t/* Locate first and last data buffers */\n\tassert ( ! list_empty ( rx_data ) );\n\tfirst = list_first_entry ( rx_data, struct io_buffer, list );\n\tlast = list_last_entry ( rx_data, struct io_buffer, list );\n\n\t/* Extract initialisation vector */\n\tif ( iob_len ( first ) < sizeof ( iv.record ) ) {\n\t\tDBGC ( tls, \"TLS %p received underlength IV\\n\", tls );\n\t\tDBGC_HD ( tls, first->data, iob_len ( first ) );\n\t\treturn -EINVAL_IV;\n\t}\n\tmemcpy ( iv.fixed, cipherspec->fixed_iv, sizeof ( iv.fixed ) );\n\tmemcpy ( iv.record, first->data, sizeof ( iv.record ) );\n\tiob_pull ( first, sizeof ( iv.record ) );\n\tlen -= sizeof ( iv.record );\n\n\t/* Construct authentication data */\n\tauthhdr.seq = cpu_to_be64 ( tls->rx_seq );\n\tauthhdr.header.type = tlshdr->type;\n\tauthhdr.header.version = tlshdr->version;\n\tauthhdr.header.length = htons ( len );\n\n\t/* Set initialisation vector */\n\tcipher_setiv ( cipher, cipherspec->cipher_ctx, &iv, sizeof ( iv ) );\n\n\t/* Decrypt the received data */\n\tcheck_len = 0;\n\tlist_for_each_entry ( iobuf, &tls->rx_data, list ) {\n\t\tcipher_decrypt ( cipher, cipherspec->cipher_ctx,\n\t\t\t\t iobuf->data, iobuf->data, iob_len ( iobuf ) );\n\t\tcheck_len += iob_len ( iobuf );\n\t}\n\tassert ( check_len == len );\n\n\t/* Strip block padding, if applicable */\n\tif ( is_block_cipher ( cipher ) ) {\n\t\tpad_len = tls_verify_padding ( tls, last );\n\t\tif ( pad_len < 0 ) {\n\t\t\t/* Assume zero padding length to avoid timing attacks */\n\t\t\tpad_len = 0;\n\t\t}\n\t\tiob_unput ( last, pad_len );\n\t\tlen -= pad_len;\n\t}\n\n\t/* Extract decrypted MAC */\n\tif ( iob_len ( last ) < suite->mac_len ) {\n\t\tDBGC ( tls, \"TLS %p received underlength MAC\\n\", tls );\n\t\tDBGC_HD ( tls, last->data, iob_len ( last ) );\n\t\treturn -EINVAL_MAC;\n\t}\n\tiob_unput ( last, suite->mac_len );\n\tlen -= suite->mac_len;\n\tmac = last->tail;\n\n\t/* Dump received data */\n\tDBGC2 ( tls, \"Received plaintext data:\\n\" );\n\tcheck_len = 0;\n\tlist_for_each_entry ( iobuf, rx_data, list ) {\n\t\tDBGC2_HD ( tls, iobuf->data, iob_len ( iobuf ) );\n\t\tcheck_len += iob_len ( iobuf );\n\t}\n\tassert ( check_len == len );\n\n\t/* Generate MAC */\n\tauthhdr.header.length = htons ( len );\n\tif ( suite->mac_len )\n\t\ttls_hmac_list ( cipherspec, &authhdr, rx_data, verify_mac );\n\n\t/* Verify MAC */\n\tif ( memcmp ( mac, verify_mac, suite->mac_len ) != 0 ) {\n\t\tDBGC ( tls, \"TLS %p failed MAC verification\\n\", tls );\n\t\treturn -EINVAL_MAC;\n\t}\n\n\t/* Process plaintext record */\n\tif ( ( rc = tls_new_record ( tls, tlshdr->type, rx_data ) ) != 0 )\n\t\treturn rc;\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -58,8 +58,8 @@\n \tif ( is_block_cipher ( cipher ) ) {\n \t\tpad_len = tls_verify_padding ( tls, last );\n \t\tif ( pad_len < 0 ) {\n-\t\t\trc = pad_len;\n-\t\t\treturn rc;\n+\t\t\t/* Assume zero padding length to avoid timing attacks */\n+\t\t\tpad_len = 0;\n \t\t}\n \t\tiob_unput ( last, pad_len );\n \t\tlen -= pad_len;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\trc = pad_len;",
                "\t\t\treturn rc;"
            ],
            "added_lines": [
                "\t\t\t/* Assume zero padding length to avoid timing attacks */",
                "\t\t\tpad_len = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1998",
        "func_name": "torvalds/linux/stibp_state",
        "description": "The Linux kernel allows userspace processes to enable mitigations by calling prctl with PR_SET_SPECULATION_CTRL which disables the speculation feature as well as by using seccomp. We had noticed that on VMs of at least one major cloud provider, the kernel still left the victim process exposed to attacks in some cases even after enabling the spectre-BTI mitigation with prctl. The same behavior can be observed on a bare-metal machine when forcing the mitigation to IBRS on boot command line.\n\nThis happened because when plain IBRS was enabled (not enhanced IBRS), the kernel had some logic that determined that STIBP was not needed. The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit was cleared on returning to userspace, due to performance reasons, which disabled the implicit STIBP and left userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.\n\n\n",
        "git_url": "https://github.com/torvalds/linux/commit/6921ed9049bc7457f66c1596c5b78aec0dae4a9d",
        "commit_title": "x86/speculation: Allow enabling STIBP with legacy IBRS",
        "commit_text": " When plain IBRS is enabled (not enhanced IBRS), the logic in spectre_v2_user_select_mitigation() determines that STIBP is not needed.  The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit is cleared on returning to userspace for performance reasons which leaves userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.  Exclude IBRS from the spectre_v2_in_ibrs_mode() check to allow for enabling STIBP (through seccomp/prctl() by default or always-on, if selected by spectre_v2_user kernel cmdline parameter).    [ bp: Massage. ]  Cc: stable@vger.kernel.org Link: https://lore.kernel.org/r/20230220120127.1975241-1-kpsingh@kernel.org Link: https://lore.kernel.org/r/20230221184908.2349578-1-kpsingh@kernel.org",
        "func_before": "static char *stibp_state(void)\n{\n\tif (spectre_v2_in_ibrs_mode(spectre_v2_enabled))\n\t\treturn \"\";\n\n\tswitch (spectre_v2_user_stibp) {\n\tcase SPECTRE_V2_USER_NONE:\n\t\treturn \", STIBP: disabled\";\n\tcase SPECTRE_V2_USER_STRICT:\n\t\treturn \", STIBP: forced\";\n\tcase SPECTRE_V2_USER_STRICT_PREFERRED:\n\t\treturn \", STIBP: always-on\";\n\tcase SPECTRE_V2_USER_PRCTL:\n\tcase SPECTRE_V2_USER_SECCOMP:\n\t\tif (static_key_enabled(&switch_to_cond_stibp))\n\t\t\treturn \", STIBP: conditional\";\n\t}\n\treturn \"\";\n}",
        "func": "static char *stibp_state(void)\n{\n\tif (spectre_v2_in_eibrs_mode(spectre_v2_enabled))\n\t\treturn \"\";\n\n\tswitch (spectre_v2_user_stibp) {\n\tcase SPECTRE_V2_USER_NONE:\n\t\treturn \", STIBP: disabled\";\n\tcase SPECTRE_V2_USER_STRICT:\n\t\treturn \", STIBP: forced\";\n\tcase SPECTRE_V2_USER_STRICT_PREFERRED:\n\t\treturn \", STIBP: always-on\";\n\tcase SPECTRE_V2_USER_PRCTL:\n\tcase SPECTRE_V2_USER_SECCOMP:\n\t\tif (static_key_enabled(&switch_to_cond_stibp))\n\t\t\treturn \", STIBP: conditional\";\n\t}\n\treturn \"\";\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static char *stibp_state(void)\n {\n-\tif (spectre_v2_in_ibrs_mode(spectre_v2_enabled))\n+\tif (spectre_v2_in_eibrs_mode(spectre_v2_enabled))\n \t\treturn \"\";\n \n \tswitch (spectre_v2_user_stibp) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (spectre_v2_in_ibrs_mode(spectre_v2_enabled))"
            ],
            "added_lines": [
                "\tif (spectre_v2_in_eibrs_mode(spectre_v2_enabled))"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1998",
        "func_name": "torvalds/linux/spectre_v2_in_ibrs_mode",
        "description": "The Linux kernel allows userspace processes to enable mitigations by calling prctl with PR_SET_SPECULATION_CTRL which disables the speculation feature as well as by using seccomp. We had noticed that on VMs of at least one major cloud provider, the kernel still left the victim process exposed to attacks in some cases even after enabling the spectre-BTI mitigation with prctl. The same behavior can be observed on a bare-metal machine when forcing the mitigation to IBRS on boot command line.\n\nThis happened because when plain IBRS was enabled (not enhanced IBRS), the kernel had some logic that determined that STIBP was not needed. The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit was cleared on returning to userspace, due to performance reasons, which disabled the implicit STIBP and left userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.\n\n\n",
        "git_url": "https://github.com/torvalds/linux/commit/6921ed9049bc7457f66c1596c5b78aec0dae4a9d",
        "commit_title": "x86/speculation: Allow enabling STIBP with legacy IBRS",
        "commit_text": " When plain IBRS is enabled (not enhanced IBRS), the logic in spectre_v2_user_select_mitigation() determines that STIBP is not needed.  The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit is cleared on returning to userspace for performance reasons which leaves userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.  Exclude IBRS from the spectre_v2_in_ibrs_mode() check to allow for enabling STIBP (through seccomp/prctl() by default or always-on, if selected by spectre_v2_user kernel cmdline parameter).    [ bp: Massage. ]  Cc: stable@vger.kernel.org Link: https://lore.kernel.org/r/20230220120127.1975241-1-kpsingh@kernel.org Link: https://lore.kernel.org/r/20230221184908.2349578-1-kpsingh@kernel.org",
        "func_before": "static inline bool spectre_v2_in_ibrs_mode(enum spectre_v2_mitigation mode)\n{\n\treturn mode == SPECTRE_V2_IBRS ||\n\t       mode == SPECTRE_V2_EIBRS ||\n\t       mode == SPECTRE_V2_EIBRS_RETPOLINE ||\n\t       mode == SPECTRE_V2_EIBRS_LFENCE;\n}",
        "func": "static inline bool spectre_v2_in_ibrs_mode(enum spectre_v2_mitigation mode)\n{\n\treturn spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,4 @@\n static inline bool spectre_v2_in_ibrs_mode(enum spectre_v2_mitigation mode)\n {\n-\treturn mode == SPECTRE_V2_IBRS ||\n-\t       mode == SPECTRE_V2_EIBRS ||\n-\t       mode == SPECTRE_V2_EIBRS_RETPOLINE ||\n-\t       mode == SPECTRE_V2_EIBRS_LFENCE;\n+\treturn spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treturn mode == SPECTRE_V2_IBRS ||",
                "\t       mode == SPECTRE_V2_EIBRS ||",
                "\t       mode == SPECTRE_V2_EIBRS_RETPOLINE ||",
                "\t       mode == SPECTRE_V2_EIBRS_LFENCE;"
            ],
            "added_lines": [
                "\treturn spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1998",
        "func_name": "torvalds/linux/spectre_v2_user_select_mitigation",
        "description": "The Linux kernel allows userspace processes to enable mitigations by calling prctl with PR_SET_SPECULATION_CTRL which disables the speculation feature as well as by using seccomp. We had noticed that on VMs of at least one major cloud provider, the kernel still left the victim process exposed to attacks in some cases even after enabling the spectre-BTI mitigation with prctl. The same behavior can be observed on a bare-metal machine when forcing the mitigation to IBRS on boot command line.\n\nThis happened because when plain IBRS was enabled (not enhanced IBRS), the kernel had some logic that determined that STIBP was not needed. The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit was cleared on returning to userspace, due to performance reasons, which disabled the implicit STIBP and left userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.\n\n\n",
        "git_url": "https://github.com/torvalds/linux/commit/6921ed9049bc7457f66c1596c5b78aec0dae4a9d",
        "commit_title": "x86/speculation: Allow enabling STIBP with legacy IBRS",
        "commit_text": " When plain IBRS is enabled (not enhanced IBRS), the logic in spectre_v2_user_select_mitigation() determines that STIBP is not needed.  The IBRS bit implicitly protects against cross-thread branch target injection. However, with legacy IBRS, the IBRS bit is cleared on returning to userspace for performance reasons which leaves userspace threads vulnerable to cross-thread branch target injection against which STIBP protects.  Exclude IBRS from the spectre_v2_in_ibrs_mode() check to allow for enabling STIBP (through seccomp/prctl() by default or always-on, if selected by spectre_v2_user kernel cmdline parameter).    [ bp: Massage. ]  Cc: stable@vger.kernel.org Link: https://lore.kernel.org/r/20230220120127.1975241-1-kpsingh@kernel.org Link: https://lore.kernel.org/r/20230221184908.2349578-1-kpsingh@kernel.org",
        "func_before": "static void __init\nspectre_v2_user_select_mitigation(void)\n{\n\tenum spectre_v2_user_mitigation mode = SPECTRE_V2_USER_NONE;\n\tbool smt_possible = IS_ENABLED(CONFIG_SMP);\n\tenum spectre_v2_user_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_STIBP))\n\t\treturn;\n\n\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||\n\t    cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\n\t\tsmt_possible = false;\n\n\tcmd = spectre_v2_parse_user_cmdline();\n\tswitch (cmd) {\n\tcase SPECTRE_V2_USER_CMD_NONE:\n\t\tgoto set_mode;\n\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tmode = SPECTRE_V2_USER_STRICT;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_AUTO:\n\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPECTRE_V2_USER_SECCOMP;\n\t\telse\n\t\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\t}\n\n\t/* Initialize Indirect Branch Prediction Barrier */\n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB);\n\n\t\tspectre_v2_user_ibpb = mode;\n\t\tswitch (cmd) {\n\t\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\t\tstatic_branch_enable(&switch_mm_always_ibpb);\n\t\t\tspectre_v2_user_ibpb = SPECTRE_V2_USER_STRICT;\n\t\t\tbreak;\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\t\tcase SPECTRE_V2_USER_CMD_AUTO:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\t\t\tstatic_branch_enable(&switch_mm_cond_ibpb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tpr_info(\"mitigation: Enabling %s Indirect Branch Prediction Barrier\\n\",\n\t\t\tstatic_key_enabled(&switch_mm_always_ibpb) ?\n\t\t\t\"always-on\" : \"conditional\");\n\t}\n\n\t/*\n\t * If no STIBP, IBRS or enhanced IBRS is enabled, or SMT impossible,\n\t * STIBP is not required.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_STIBP) ||\n\t    !smt_possible ||\n\t    spectre_v2_in_ibrs_mode(spectre_v2_enabled))\n\t\treturn;\n\n\t/*\n\t * At this point, an STIBP mode other than \"off\" has been set.\n\t * If STIBP support is not being forced, check if STIBP always-on\n\t * is preferred.\n\t */\n\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t    boot_cpu_has(X86_FEATURE_AMD_STIBP_ALWAYS_ON))\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\n\tif (retbleed_mitigation == RETBLEED_MITIGATION_UNRET ||\n\t    retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {\n\t\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t\t    mode != SPECTRE_V2_USER_STRICT_PREFERRED)\n\t\t\tpr_info(\"Selecting STIBP always-on mode to complement retbleed mitigation\\n\");\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\t}\n\n\tspectre_v2_user_stibp = mode;\n\nset_mode:\n\tpr_info(\"%s\\n\", spectre_v2_user_strings[mode]);\n}",
        "func": "static void __init\nspectre_v2_user_select_mitigation(void)\n{\n\tenum spectre_v2_user_mitigation mode = SPECTRE_V2_USER_NONE;\n\tbool smt_possible = IS_ENABLED(CONFIG_SMP);\n\tenum spectre_v2_user_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_STIBP))\n\t\treturn;\n\n\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||\n\t    cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\n\t\tsmt_possible = false;\n\n\tcmd = spectre_v2_parse_user_cmdline();\n\tswitch (cmd) {\n\tcase SPECTRE_V2_USER_CMD_NONE:\n\t\tgoto set_mode;\n\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tmode = SPECTRE_V2_USER_STRICT;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_AUTO:\n\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPECTRE_V2_USER_SECCOMP;\n\t\telse\n\t\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\t}\n\n\t/* Initialize Indirect Branch Prediction Barrier */\n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB);\n\n\t\tspectre_v2_user_ibpb = mode;\n\t\tswitch (cmd) {\n\t\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\t\tstatic_branch_enable(&switch_mm_always_ibpb);\n\t\t\tspectre_v2_user_ibpb = SPECTRE_V2_USER_STRICT;\n\t\t\tbreak;\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\t\tcase SPECTRE_V2_USER_CMD_AUTO:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\t\t\tstatic_branch_enable(&switch_mm_cond_ibpb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tpr_info(\"mitigation: Enabling %s Indirect Branch Prediction Barrier\\n\",\n\t\t\tstatic_key_enabled(&switch_mm_always_ibpb) ?\n\t\t\t\"always-on\" : \"conditional\");\n\t}\n\n\t/*\n\t * If no STIBP, enhanced IBRS is enabled, or SMT impossible, STIBP\n\t * is not required.\n\t *\n\t * Enhanced IBRS also protects against cross-thread branch target\n\t * injection in user-mode as the IBRS bit remains always set which\n\t * implicitly enables cross-thread protections.  However, in legacy IBRS\n\t * mode, the IBRS bit is set only on kernel entry and cleared on return\n\t * to userspace. This disables the implicit cross-thread protection,\n\t * so allow for STIBP to be selected in that case.\n\t */\n\tif (!boot_cpu_has(X86_FEATURE_STIBP) ||\n\t    !smt_possible ||\n\t    spectre_v2_in_eibrs_mode(spectre_v2_enabled))\n\t\treturn;\n\n\t/*\n\t * At this point, an STIBP mode other than \"off\" has been set.\n\t * If STIBP support is not being forced, check if STIBP always-on\n\t * is preferred.\n\t */\n\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t    boot_cpu_has(X86_FEATURE_AMD_STIBP_ALWAYS_ON))\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\n\tif (retbleed_mitigation == RETBLEED_MITIGATION_UNRET ||\n\t    retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {\n\t\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t\t    mode != SPECTRE_V2_USER_STRICT_PREFERRED)\n\t\t\tpr_info(\"Selecting STIBP always-on mode to complement retbleed mitigation\\n\");\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\t}\n\n\tspectre_v2_user_stibp = mode;\n\nset_mode:\n\tpr_info(\"%s\\n\", spectre_v2_user_strings[mode]);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -60,12 +60,19 @@\n \t}\n \n \t/*\n-\t * If no STIBP, IBRS or enhanced IBRS is enabled, or SMT impossible,\n-\t * STIBP is not required.\n+\t * If no STIBP, enhanced IBRS is enabled, or SMT impossible, STIBP\n+\t * is not required.\n+\t *\n+\t * Enhanced IBRS also protects against cross-thread branch target\n+\t * injection in user-mode as the IBRS bit remains always set which\n+\t * implicitly enables cross-thread protections.  However, in legacy IBRS\n+\t * mode, the IBRS bit is set only on kernel entry and cleared on return\n+\t * to userspace. This disables the implicit cross-thread protection,\n+\t * so allow for STIBP to be selected in that case.\n \t */\n \tif (!boot_cpu_has(X86_FEATURE_STIBP) ||\n \t    !smt_possible ||\n-\t    spectre_v2_in_ibrs_mode(spectre_v2_enabled))\n+\t    spectre_v2_in_eibrs_mode(spectre_v2_enabled))\n \t\treturn;\n \n \t/*",
        "diff_line_info": {
            "deleted_lines": [
                "\t * If no STIBP, IBRS or enhanced IBRS is enabled, or SMT impossible,",
                "\t * STIBP is not required.",
                "\t    spectre_v2_in_ibrs_mode(spectre_v2_enabled))"
            ],
            "added_lines": [
                "\t * If no STIBP, enhanced IBRS is enabled, or SMT impossible, STIBP",
                "\t * is not required.",
                "\t *",
                "\t * Enhanced IBRS also protects against cross-thread branch target",
                "\t * injection in user-mode as the IBRS bit remains always set which",
                "\t * implicitly enables cross-thread protections.  However, in legacy IBRS",
                "\t * mode, the IBRS bit is set only on kernel entry and cleared on return",
                "\t * to userspace. This disables the implicit cross-thread protection,",
                "\t * so allow for STIBP to be selected in that case.",
                "\t    spectre_v2_in_eibrs_mode(spectre_v2_enabled))"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-13456",
        "func_name": "FreeRADIUS/freeradius-server/compute_password_element",
        "description": "In FreeRADIUS 3.0 through 3.0.19, on average 1 in every 2048 EAP-pwd handshakes fails because the password element cannot be found within 10 iterations of the hunting and pecking loop. This leaks information that an attacker can use to recover the password of any user. This information leakage is similar to the \"Dragonblood\" attack and CVE-2019-9494.",
        "git_url": "https://github.com/FreeRADIUS/freeradius-server/commit/3ea2a5a026e73d81cd9a3e9bbd4300c433004bfa",
        "commit_title": "EAP-pwd: fix side-channel leak where 1 in 2018 handshakes fail",
        "commit_text": " Previously the Hunting and Pecking algorithm of EAP-pwd aborted when more than 10 iterations are needed. Every iteration has a 50% chance of finding the password element. This means one in every 2048 handshakes will fail, in which case an error frame is sent to the client. This event leaks information that can be abused in an offline password brute-force attack. More precisely, the adversary learns that all 10 iterations failed for the given random EAP-pwd token. Using the same techniques as in the Dragonblood attack, this can be used to brute-force the password.  This patch fixes the above issue by executing enough iterations such that the password element is always found eventually.  Note that timing and cache leaks remain a risk against the current implementation of EAP-pwd.",
        "func_before": "int compute_password_element (pwd_session_t *session, uint16_t grp_num,\n\t\t\t      char const *password, int password_len,\n\t\t\t      char const *id_server, int id_server_len,\n\t\t\t      char const *id_peer, int id_peer_len,\n\t\t\t      uint32_t *token)\n{\n\tBIGNUM *x_candidate = NULL, *rnd = NULL, *cofactor = NULL;\n\tHMAC_CTX *ctx = NULL;\n\tuint8_t pwe_digest[SHA256_DIGEST_LENGTH], *prfbuf = NULL, ctr;\n\tint nid, is_odd, primebitlen, primebytelen, ret = 0;\n\n\tctx = HMAC_CTX_new();\n\tif (ctx == NULL) {\n\t\tDEBUG(\"failed allocating HMAC context\");\n\t\tgoto fail;\n\t}\n\n\tswitch (grp_num) { /* from IANA registry for IKE D-H groups */\n\tcase 19:\n\t\tnid = NID_X9_62_prime256v1;\n\t\tbreak;\n\n\tcase 20:\n\t\tnid = NID_secp384r1;\n\t\tbreak;\n\n\tcase 21:\n\t\tnid = NID_secp521r1;\n\t\tbreak;\n\n\tcase 25:\n\t\tnid = NID_X9_62_prime192v1;\n\t\tbreak;\n\n\tcase 26:\n\t\tnid = NID_secp224r1;\n\t\tbreak;\n\n\tdefault:\n\t\tDEBUG(\"unknown group %d\", grp_num);\n\t\tgoto fail;\n\t}\n\n\tsession->pwe = NULL;\n\tsession->order = NULL;\n\tsession->prime = NULL;\n\n\tif ((session->group = EC_GROUP_new_by_curve_name(nid)) == NULL) {\n\t\tDEBUG(\"unable to create EC_GROUP\");\n\t\tgoto fail;\n\t}\n\n\tif (((rnd = BN_new()) == NULL) ||\n\t    ((cofactor = BN_new()) == NULL) ||\n\t    ((session->pwe = EC_POINT_new(session->group)) == NULL) ||\n\t    ((session->order = BN_new()) == NULL) ||\n\t    ((session->prime = BN_new()) == NULL) ||\n\t    ((x_candidate = BN_new()) == NULL)) {\n\t\tDEBUG(\"unable to create bignums\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_curve_GFp(session->group, session->prime, NULL, NULL, NULL)) {\n\t\tDEBUG(\"unable to get prime for GFp curve\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_order(session->group, session->order, NULL)) {\n\t\tDEBUG(\"unable to get order for curve\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_cofactor(session->group, cofactor, NULL)) {\n\t\tDEBUG(\"unable to get cofactor for curve\");\n\t\tgoto fail;\n\t}\n\n\tprimebitlen = BN_num_bits(session->prime);\n\tprimebytelen = BN_num_bytes(session->prime);\n\tif ((prfbuf = talloc_zero_array(session, uint8_t, primebytelen)) == NULL) {\n\t\tDEBUG(\"unable to alloc space for prf buffer\");\n\t\tgoto fail;\n\t}\n\tctr = 0;\n\twhile (1) {\n\t\tif (ctr > 10) {\n\t\t\tDEBUG(\"unable to find random point on curve for group %d, something's fishy\", grp_num);\n\t\t\tgoto fail;\n\t\t}\n\t\tctr++;\n\n\t\t/*\n\t\t * compute counter-mode password value and stretch to prime\n\t\t *    pwd-seed = H(token | peer-id | server-id | password |\n\t\t *\t\t   counter)\n\t\t */\n\t\tH_Init(ctx);\n\t\tH_Update(ctx, (uint8_t *)token, sizeof(*token));\n\t\tH_Update(ctx, (uint8_t const *)id_peer, id_peer_len);\n\t\tH_Update(ctx, (uint8_t const *)id_server, id_server_len);\n\t\tH_Update(ctx, (uint8_t const *)password, password_len);\n\t\tH_Update(ctx, (uint8_t *)&ctr, sizeof(ctr));\n\t\tH_Final(ctx, pwe_digest);\n\n\t\tBN_bin2bn(pwe_digest, SHA256_DIGEST_LENGTH, rnd);\n\t\tif (eap_pwd_kdf(pwe_digest, SHA256_DIGEST_LENGTH, \"EAP-pwd Hunting And Pecking\",\n\t\t\t        strlen(\"EAP-pwd Hunting And Pecking\"), prfbuf, primebitlen) != 0) {\n\t\t\tDEBUG(\"key derivation function failed\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tBN_bin2bn(prfbuf, primebytelen, x_candidate);\n\t\t/*\n\t\t * eap_pwd_kdf() returns a string of bits 0..primebitlen but\n\t\t * BN_bin2bn will treat that string of bits as a big endian\n\t\t * number. If the primebitlen is not an even multiple of 8\n\t\t * then excessive bits-- those _after_ primebitlen-- so now\n\t\t * we have to shift right the amount we masked off.\n\t\t */\n\t\tif (primebitlen % 8) BN_rshift(x_candidate, x_candidate, (8 - (primebitlen % 8)));\n\t\tif (BN_ucmp(x_candidate, session->prime) >= 0) continue;\n\n\t\t/*\n\t\t * need to unambiguously identify the solution, if there is\n\t\t * one...\n\t\t */\n\t\tis_odd = BN_is_odd(rnd) ? 1 : 0;\n\n\t\t/*\n\t\t * solve the quadratic equation, if it's not solvable then we\n\t\t * don't have a point\n\t\t */\n\t\tif (!EC_POINT_set_compressed_coordinates_GFp(session->group, session->pwe, x_candidate, is_odd, NULL)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * If there's a solution to the equation then the point must be\n\t\t * on the curve so why check again explicitly? OpenSSL code\n\t\t * says this is required by X9.62. We're not X9.62 but it can't\n\t\t * hurt just to be sure.\n\t\t */\n\t\tif (!EC_POINT_is_on_curve(session->group, session->pwe, NULL)) {\n\t\t\tDEBUG(\"EAP-pwd: point is not on curve\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BN_cmp(cofactor, BN_value_one())) {\n\t\t\t/* make sure the point is not in a small sub-group */\n\t\t\tif (!EC_POINT_mul(session->group, session->pwe, NULL, session->pwe,\n\t\t\t\tcofactor, NULL)) {\n\t\t\t\tDEBUG(\"EAP-pwd: cannot multiply generator by order\");\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (EC_POINT_is_at_infinity(session->group, session->pwe)) {\n\t\t\t\tDEBUG(\"EAP-pwd: point is at infinity\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\t/* if we got here then we have a new generator. */\n\t\tbreak;\n\t}\n\n\tsession->group_num = grp_num;\n\tif (0) {\n\t\tfail:\t\t/* DON'T free session, it's in handler->opaque */\n\t\tret = -1;\n\t}\n\n\t/* cleanliness and order.... */\n\tBN_clear_free(cofactor);\n\tBN_clear_free(x_candidate);\n\tBN_clear_free(rnd);\n\ttalloc_free(prfbuf);\n\tHMAC_CTX_free(ctx);\n\n\treturn ret;\n}",
        "func": "int compute_password_element (pwd_session_t *session, uint16_t grp_num,\n\t\t\t      char const *password, int password_len,\n\t\t\t      char const *id_server, int id_server_len,\n\t\t\t      char const *id_peer, int id_peer_len,\n\t\t\t      uint32_t *token)\n{\n\tBIGNUM *x_candidate = NULL, *rnd = NULL, *cofactor = NULL;\n\tHMAC_CTX *ctx = NULL;\n\tuint8_t pwe_digest[SHA256_DIGEST_LENGTH], *prfbuf = NULL, ctr;\n\tint nid, is_odd, primebitlen, primebytelen, ret = 0;\n\n\tctx = HMAC_CTX_new();\n\tif (ctx == NULL) {\n\t\tDEBUG(\"failed allocating HMAC context\");\n\t\tgoto fail;\n\t}\n\n\tswitch (grp_num) { /* from IANA registry for IKE D-H groups */\n\tcase 19:\n\t\tnid = NID_X9_62_prime256v1;\n\t\tbreak;\n\n\tcase 20:\n\t\tnid = NID_secp384r1;\n\t\tbreak;\n\n\tcase 21:\n\t\tnid = NID_secp521r1;\n\t\tbreak;\n\n\tcase 25:\n\t\tnid = NID_X9_62_prime192v1;\n\t\tbreak;\n\n\tcase 26:\n\t\tnid = NID_secp224r1;\n\t\tbreak;\n\n\tdefault:\n\t\tDEBUG(\"unknown group %d\", grp_num);\n\t\tgoto fail;\n\t}\n\n\tsession->pwe = NULL;\n\tsession->order = NULL;\n\tsession->prime = NULL;\n\n\tif ((session->group = EC_GROUP_new_by_curve_name(nid)) == NULL) {\n\t\tDEBUG(\"unable to create EC_GROUP\");\n\t\tgoto fail;\n\t}\n\n\tif (((rnd = BN_new()) == NULL) ||\n\t    ((cofactor = BN_new()) == NULL) ||\n\t    ((session->pwe = EC_POINT_new(session->group)) == NULL) ||\n\t    ((session->order = BN_new()) == NULL) ||\n\t    ((session->prime = BN_new()) == NULL) ||\n\t    ((x_candidate = BN_new()) == NULL)) {\n\t\tDEBUG(\"unable to create bignums\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_curve_GFp(session->group, session->prime, NULL, NULL, NULL)) {\n\t\tDEBUG(\"unable to get prime for GFp curve\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_order(session->group, session->order, NULL)) {\n\t\tDEBUG(\"unable to get order for curve\");\n\t\tgoto fail;\n\t}\n\n\tif (!EC_GROUP_get_cofactor(session->group, cofactor, NULL)) {\n\t\tDEBUG(\"unable to get cofactor for curve\");\n\t\tgoto fail;\n\t}\n\n\tprimebitlen = BN_num_bits(session->prime);\n\tprimebytelen = BN_num_bytes(session->prime);\n\tif ((prfbuf = talloc_zero_array(session, uint8_t, primebytelen)) == NULL) {\n\t\tDEBUG(\"unable to alloc space for prf buffer\");\n\t\tgoto fail;\n\t}\n\tctr = 0;\n\twhile (1) {\n\t\tif (ctr > 100) {\n\t\t\tDEBUG(\"unable to find random point on curve for group %d, something's fishy\", grp_num);\n\t\t\tgoto fail;\n\t\t}\n\t\tctr++;\n\n\t\t/*\n\t\t * compute counter-mode password value and stretch to prime\n\t\t *    pwd-seed = H(token | peer-id | server-id | password |\n\t\t *\t\t   counter)\n\t\t */\n\t\tH_Init(ctx);\n\t\tH_Update(ctx, (uint8_t *)token, sizeof(*token));\n\t\tH_Update(ctx, (uint8_t const *)id_peer, id_peer_len);\n\t\tH_Update(ctx, (uint8_t const *)id_server, id_server_len);\n\t\tH_Update(ctx, (uint8_t const *)password, password_len);\n\t\tH_Update(ctx, (uint8_t *)&ctr, sizeof(ctr));\n\t\tH_Final(ctx, pwe_digest);\n\n\t\tBN_bin2bn(pwe_digest, SHA256_DIGEST_LENGTH, rnd);\n\t\tif (eap_pwd_kdf(pwe_digest, SHA256_DIGEST_LENGTH, \"EAP-pwd Hunting And Pecking\",\n\t\t\t        strlen(\"EAP-pwd Hunting And Pecking\"), prfbuf, primebitlen) != 0) {\n\t\t\tDEBUG(\"key derivation function failed\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tBN_bin2bn(prfbuf, primebytelen, x_candidate);\n\t\t/*\n\t\t * eap_pwd_kdf() returns a string of bits 0..primebitlen but\n\t\t * BN_bin2bn will treat that string of bits as a big endian\n\t\t * number. If the primebitlen is not an even multiple of 8\n\t\t * then excessive bits-- those _after_ primebitlen-- so now\n\t\t * we have to shift right the amount we masked off.\n\t\t */\n\t\tif (primebitlen % 8) BN_rshift(x_candidate, x_candidate, (8 - (primebitlen % 8)));\n\t\tif (BN_ucmp(x_candidate, session->prime) >= 0) continue;\n\n\t\t/*\n\t\t * need to unambiguously identify the solution, if there is\n\t\t * one...\n\t\t */\n\t\tis_odd = BN_is_odd(rnd) ? 1 : 0;\n\n\t\t/*\n\t\t * solve the quadratic equation, if it's not solvable then we\n\t\t * don't have a point\n\t\t */\n\t\tif (!EC_POINT_set_compressed_coordinates_GFp(session->group, session->pwe, x_candidate, is_odd, NULL)) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t/*\n\t\t * If there's a solution to the equation then the point must be\n\t\t * on the curve so why check again explicitly? OpenSSL code\n\t\t * says this is required by X9.62. We're not X9.62 but it can't\n\t\t * hurt just to be sure.\n\t\t */\n\t\tif (!EC_POINT_is_on_curve(session->group, session->pwe, NULL)) {\n\t\t\tDEBUG(\"EAP-pwd: point is not on curve\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BN_cmp(cofactor, BN_value_one())) {\n\t\t\t/* make sure the point is not in a small sub-group */\n\t\t\tif (!EC_POINT_mul(session->group, session->pwe, NULL, session->pwe,\n\t\t\t\tcofactor, NULL)) {\n\t\t\t\tDEBUG(\"EAP-pwd: cannot multiply generator by order\");\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (EC_POINT_is_at_infinity(session->group, session->pwe)) {\n\t\t\t\tDEBUG(\"EAP-pwd: point is at infinity\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\t/* if we got here then we have a new generator. */\n\t\tbreak;\n\t}\n\n\tsession->group_num = grp_num;\n\tif (0) {\n\t\tfail:\t\t/* DON'T free session, it's in handler->opaque */\n\t\tret = -1;\n\t}\n\n\t/* cleanliness and order.... */\n\tBN_clear_free(cofactor);\n\tBN_clear_free(x_candidate);\n\tBN_clear_free(rnd);\n\ttalloc_free(prfbuf);\n\tHMAC_CTX_free(ctx);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -83,7 +83,7 @@\n \t}\n \tctr = 0;\n \twhile (1) {\n-\t\tif (ctr > 10) {\n+\t\tif (ctr > 100) {\n \t\t\tDEBUG(\"unable to find random point on curve for group %d, something's fishy\", grp_num);\n \t\t\tgoto fail;\n \t\t}",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (ctr > 10) {"
            ],
            "added_lines": [
                "\t\tif (ctr > 100) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-40090",
        "func_name": "android/BTM_BleVerifySignature",
        "description": "In BTM_BleVerifySignature of btm_ble.cc, there is a possible way to bypass signature validation due to side channel information disclosure. This could lead to remote escalation of privilege with no additional execution privileges needed. User interaction is not needed for exploitation.",
        "git_url": "https://android.googlesource.com/platform/packages/modules/Bluetooth/+/495417bd068c35de0729d9a332639bd0699153ff",
        "commit_title": "Fix timing attack in BTM_BleVerifySignature",
        "commit_text": " BTM_BleVerifySignature uses a stock memcmp, allowing signature contents to be deduced through a side-channel attack.  Change to CRYPTO_memcmp, which is hardened against this attack, to eliminate this attack.  Bug: 274478807 Test: atest bluetooth_test_gd_unit Tag: #security Ignore-AOSP-First: Security (cherry picked from https://googleplex-android-review.googlesource.com/q/commit:7a960ac1c0cbc6d3949b6eaa7a86302a0b20c04f) Merged-In: Iddeff055d9064f51a1e0cfb851d8b74135a714c2 ",
        "func_before": "bool BTM_BleVerifySignature(const RawAddress& bd_addr, uint8_t* p_orig,\n                            uint16_t len, uint32_t counter, uint8_t* p_comp) {\n  if (bluetooth::shim::is_gd_shim_enabled()) {\n    return bluetooth::shim::BTM_BleVerifySignature(bd_addr, p_orig, len,\n                                                   counter, p_comp);\n  }\n  bool verified = false;\n  tBTM_SEC_DEV_REC* p_rec = btm_find_dev(bd_addr);\n  uint8_t p_mac[BTM_CMAC_TLEN_SIZE];\n\n  if (p_rec == NULL || (p_rec && !(p_rec->ble.key_type & BTM_LE_KEY_PCSRK))) {\n    BTM_TRACE_ERROR(\"can not verify signature for unknown device\");\n  } else if (counter < p_rec->ble.keys.counter) {\n    BTM_TRACE_ERROR(\"signature received with out dated sign counter\");\n  } else if (p_orig == NULL) {\n    BTM_TRACE_ERROR(\"No signature to verify\");\n  } else {\n    BTM_TRACE_DEBUG(\"%s rcv_cnt=%d >= expected_cnt=%d\", __func__, counter,\n                    p_rec->ble.keys.counter);\n\n    crypto_toolbox::aes_cmac(p_rec->ble.keys.pcsrk, p_orig, len,\n                             BTM_CMAC_TLEN_SIZE, p_mac);\n    if (memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {\n      btm_ble_increment_sign_ctr(bd_addr, false);\n      verified = true;\n    }\n  }\n  return verified;\n}",
        "func": "bool BTM_BleVerifySignature(const RawAddress& bd_addr, uint8_t* p_orig,\n                            uint16_t len, uint32_t counter, uint8_t* p_comp) {\n  if (bluetooth::shim::is_gd_shim_enabled()) {\n    return bluetooth::shim::BTM_BleVerifySignature(bd_addr, p_orig, len,\n                                                   counter, p_comp);\n  }\n  bool verified = false;\n  tBTM_SEC_DEV_REC* p_rec = btm_find_dev(bd_addr);\n  uint8_t p_mac[BTM_CMAC_TLEN_SIZE];\n\n  if (p_rec == NULL || (p_rec && !(p_rec->ble.key_type & BTM_LE_KEY_PCSRK))) {\n    BTM_TRACE_ERROR(\"can not verify signature for unknown device\");\n  } else if (counter < p_rec->ble.keys.counter) {\n    BTM_TRACE_ERROR(\"signature received with out dated sign counter\");\n  } else if (p_orig == NULL) {\n    BTM_TRACE_ERROR(\"No signature to verify\");\n  } else {\n    BTM_TRACE_DEBUG(\"%s rcv_cnt=%d >= expected_cnt=%d\", __func__, counter,\n                    p_rec->ble.keys.counter);\n\n    crypto_toolbox::aes_cmac(p_rec->ble.keys.pcsrk, p_orig, len,\n                             BTM_CMAC_TLEN_SIZE, p_mac);\n    if (CRYPTO_memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {\n      btm_ble_increment_sign_ctr(bd_addr, false);\n      verified = true;\n    }\n  }\n  return verified;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -20,7 +20,7 @@\n \n     crypto_toolbox::aes_cmac(p_rec->ble.keys.pcsrk, p_orig, len,\n                              BTM_CMAC_TLEN_SIZE, p_mac);\n-    if (memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {\n+    if (CRYPTO_memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {\n       btm_ble_increment_sign_ctr(bd_addr, false);\n       verified = true;\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {"
            ],
            "added_lines": [
                "    if (CRYPTO_memcmp(p_mac, p_comp, BTM_CMAC_TLEN_SIZE) == 0) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-23771",
        "func_name": "emikulic/darkhttpd/process_request",
        "description": "darkhttpd before 1.15 uses strcmp (which is not constant time) to verify authentication, which makes it easier for remote attackers to bypass authentication via a timing side channel.",
        "git_url": "https://github.com/emikulic/darkhttpd/commit/f477619d49f3c4de9ad59bd194265a48ddc03f04",
        "commit_title": "Don't use strcmp to compare passwords.",
        "commit_text": " Add password_equal() which tries to avoid leaking the length of the secret through timing side-channels.  Suggested-by: SUSE Security Team <security@suse.de>",
        "func_before": "static void process_request(struct connection *conn) {\n    num_requests++;\n\n    if (!parse_request(conn)) {\n        default_reply(conn, 400, \"Bad Request\",\n            \"You sent a request that the server couldn't understand.\");\n    }\n    else if (is_https_redirect(conn)) {\n        redirect_https(conn);\n    }\n    /* fail if: (auth_enabled) AND (client supplied invalid credentials) */\n    else if (auth_key != NULL &&\n            (conn->authorization == NULL ||\n             strcmp(conn->authorization, auth_key)))\n    {\n        default_reply(conn, 401, \"Unauthorized\",\n            \"Access denied due to invalid credentials.\");\n    }\n    else if (strcmp(conn->method, \"GET\") == 0) {\n        process_get(conn);\n    }\n    else if (strcmp(conn->method, \"HEAD\") == 0) {\n        process_get(conn);\n        conn->header_only = 1;\n    }\n    else {\n        default_reply(conn, 501, \"Not Implemented\",\n                      \"The method you specified is not implemented.\");\n    }\n\n    /* advance state */\n    conn->state = SEND_HEADER;\n\n    /* request not needed anymore */\n    free(conn->request);\n    conn->request = NULL; /* important: don't free it again later */\n}",
        "func": "static void process_request(struct connection *conn) {\n    num_requests++;\n\n    if (!parse_request(conn)) {\n        default_reply(conn, 400, \"Bad Request\",\n            \"You sent a request that the server couldn't understand.\");\n    }\n    else if (is_https_redirect(conn)) {\n        redirect_https(conn);\n    }\n    /* fail if: (auth_enabled) AND (client supplied invalid credentials) */\n    else if (auth_key != NULL &&\n            (conn->authorization == NULL ||\n             !password_equal(conn->authorization, auth_key))) {\n        default_reply(conn, 401, \"Unauthorized\",\n            \"Access denied due to invalid credentials.\");\n    }\n    else if (strcmp(conn->method, \"GET\") == 0) {\n        process_get(conn);\n    }\n    else if (strcmp(conn->method, \"HEAD\") == 0) {\n        process_get(conn);\n        conn->header_only = 1;\n    }\n    else {\n        default_reply(conn, 501, \"Not Implemented\",\n                      \"The method you specified is not implemented.\");\n    }\n\n    /* advance state */\n    conn->state = SEND_HEADER;\n\n    /* request not needed anymore */\n    free(conn->request);\n    conn->request = NULL; /* important: don't free it again later */\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,8 +11,7 @@\n     /* fail if: (auth_enabled) AND (client supplied invalid credentials) */\n     else if (auth_key != NULL &&\n             (conn->authorization == NULL ||\n-             strcmp(conn->authorization, auth_key)))\n-    {\n+             !password_equal(conn->authorization, auth_key))) {\n         default_reply(conn, 401, \"Unauthorized\",\n             \"Access denied due to invalid credentials.\");\n     }",
        "diff_line_info": {
            "deleted_lines": [
                "             strcmp(conn->authorization, auth_key)))",
                "    {"
            ],
            "added_lines": [
                "             !password_equal(conn->authorization, auth_key))) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-6258",
        "func_name": "latchset/pkcs11-provider/p11prov_rsaenc_decrypt",
        "description": "A security vulnerability has been identified in the pkcs11-provider, which is associated with Public-Key Cryptography Standards (PKCS#11). If exploited successfully, this vulnerability could result in a Bleichenbacher-like security flaw, potentially enabling a side-channel attack on PKCS#1 1.5 decryption.",
        "git_url": "https://github.com/latchset/pkcs11-provider/commit/86e7395165ad785380f9064bf96206eb8433d234",
        "commit_title": "Side-channel proofing PKCS#1 1.5 paths",
        "commit_text": " Fixes CVE-2023-6258 (Marvin) ",
        "func_before": "static int p11prov_rsaenc_decrypt(void *ctx, unsigned char *out, size_t *outlen,\n                                  size_t outsize, const unsigned char *in,\n                                  size_t inlen)\n{\n    struct p11prov_rsaenc_ctx *encctx = (struct p11prov_rsaenc_ctx *)ctx;\n    CK_MECHANISM mechanism;\n    P11PROV_SESSION *session;\n    CK_SESSION_HANDLE sess;\n    CK_SLOT_ID slotid;\n    CK_OBJECT_HANDLE handle;\n    CK_ULONG out_size = *outlen;\n    int result = RET_OSSL_ERR;\n    CK_RV ret;\n\n    P11PROV_debug(\"decrypt (ctx=%p)\", ctx);\n\n    if (out == NULL) {\n        CK_ULONG size = p11prov_obj_get_key_size(encctx->key);\n        if (size == CK_UNAVAILABLE_INFORMATION) {\n            ERR_raise(ERR_LIB_PROV, PROV_R_INVALID_KEY);\n            return RET_OSSL_ERR;\n        }\n        *outlen = size;\n        return RET_OSSL_OK;\n    }\n\n    slotid = p11prov_obj_get_slotid(encctx->key);\n    if (slotid == CK_UNAVAILABLE_INFORMATION) {\n        P11PROV_raise(encctx->provctx, CKR_SLOT_ID_INVALID,\n                      \"Provided key has invalid slot\");\n        return RET_OSSL_ERR;\n    }\n    handle = p11prov_obj_get_handle(encctx->key);\n    if (handle == CK_INVALID_HANDLE) {\n        P11PROV_raise(encctx->provctx, CKR_KEY_HANDLE_INVALID,\n                      \"Provided key has invalid handle\");\n        return RET_OSSL_ERR;\n    }\n\n    ret = p11prov_rsaenc_set_mechanism(encctx, &mechanism);\n    if (ret != CKR_OK) {\n        return RET_OSSL_ERR;\n    }\n\n    ret = p11prov_get_session(encctx->provctx, &slotid, NULL, NULL,\n                              mechanism.mechanism, NULL, NULL, true, false,\n                              &session);\n    if (ret != CKR_OK) {\n        P11PROV_raise(encctx->provctx, ret,\n                      \"Failed to open session on slot %lu\", slotid);\n        return RET_OSSL_ERR;\n    }\n    sess = p11prov_session_handle(session);\n\n    ret = p11prov_DecryptInit(encctx->provctx, sess, &mechanism, handle);\n    if (ret != CKR_OK) {\n        if (ret == CKR_MECHANISM_INVALID\n            || ret == CKR_MECHANISM_PARAM_INVALID) {\n            ERR_raise(ERR_LIB_PROV, PROV_R_ILLEGAL_OR_UNSUPPORTED_PADDING_MODE);\n        }\n        goto endsess;\n    }\n\n    ret = p11prov_Decrypt(encctx->provctx, sess, (void *)in, inlen, out,\n                          &out_size);\n    if (ret != CKR_OK) {\n        goto endsess;\n    }\n\n    *outlen = out_size;\n    result = RET_OSSL_OK;\n\nendsess:\n    p11prov_return_session(session);\n    return result;\n}",
        "func": "static int p11prov_rsaenc_decrypt(void *ctx, unsigned char *out, size_t *outlen,\n                                  size_t outsize, const unsigned char *in,\n                                  size_t inlen)\n{\n    struct p11prov_rsaenc_ctx *encctx = (struct p11prov_rsaenc_ctx *)ctx;\n    CK_MECHANISM mechanism;\n    P11PROV_SESSION *session;\n    CK_SESSION_HANDLE sess;\n    CK_SLOT_ID slotid;\n    CK_OBJECT_HANDLE handle;\n    CK_ULONG out_size = *outlen;\n    int result = RET_OSSL_ERR;\n    CK_RV ret;\n\n    P11PROV_debug(\"decrypt (ctx=%p)\", ctx);\n\n    if (out == NULL) {\n        CK_ULONG size = p11prov_obj_get_key_size(encctx->key);\n        if (size == CK_UNAVAILABLE_INFORMATION) {\n            ERR_raise(ERR_LIB_PROV, PROV_R_INVALID_KEY);\n            return RET_OSSL_ERR;\n        }\n        *outlen = size;\n        return RET_OSSL_OK;\n    }\n\n    slotid = p11prov_obj_get_slotid(encctx->key);\n    if (slotid == CK_UNAVAILABLE_INFORMATION) {\n        P11PROV_raise(encctx->provctx, CKR_SLOT_ID_INVALID,\n                      \"Provided key has invalid slot\");\n        return RET_OSSL_ERR;\n    }\n    handle = p11prov_obj_get_handle(encctx->key);\n    if (handle == CK_INVALID_HANDLE) {\n        P11PROV_raise(encctx->provctx, CKR_KEY_HANDLE_INVALID,\n                      \"Provided key has invalid handle\");\n        return RET_OSSL_ERR;\n    }\n\n    ret = p11prov_rsaenc_set_mechanism(encctx, &mechanism);\n    if (ret != CKR_OK) {\n        return RET_OSSL_ERR;\n    }\n\n    ret = p11prov_get_session(encctx->provctx, &slotid, NULL, NULL,\n                              mechanism.mechanism, NULL, NULL, true, false,\n                              &session);\n    if (ret != CKR_OK) {\n        P11PROV_raise(encctx->provctx, ret,\n                      \"Failed to open session on slot %lu\", slotid);\n        return RET_OSSL_ERR;\n    }\n    sess = p11prov_session_handle(session);\n\n    ret = p11prov_DecryptInit(encctx->provctx, sess, &mechanism, handle);\n    if (ret != CKR_OK) {\n        if (ret == CKR_MECHANISM_INVALID\n            || ret == CKR_MECHANISM_PARAM_INVALID) {\n            ERR_raise(ERR_LIB_PROV, PROV_R_ILLEGAL_OR_UNSUPPORTED_PADDING_MODE);\n        }\n        goto endsess;\n    }\n\n    /* Special handling against PKCS#1 1.5 side channel leaking */\n    if (mechanism.mechanism == CKM_RSA_PKCS) {\n        CK_ULONG cond;\n        ret = side_channel_free_Decrypt(encctx->provctx, sess, (void *)in,\n                                        inlen, out, &out_size);\n        /* the error case need to be handled in a side-channel free way, so\n         * conditionals need to be constant time. Always setting outlen is\n         * fine because out_size is initialized to the value of outlen\n         * and the value should not matter in an error condition anyway */\n        *outlen = out_size;\n        cond = constant_equal(ret, CKR_OK);\n        result = constant_select_int(cond, RET_OSSL_OK, RET_OSSL_ERR);\n        goto endsess;\n    }\n\n    ret = p11prov_Decrypt(encctx->provctx, sess, (void *)in, inlen, out,\n                          &out_size);\n    if (ret != CKR_OK) {\n        goto endsess;\n    }\n    *outlen = out_size;\n    result = RET_OSSL_OK;\n\nendsess:\n    p11prov_return_session(session);\n    return result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -61,12 +61,26 @@\n         goto endsess;\n     }\n \n+    /* Special handling against PKCS#1 1.5 side channel leaking */\n+    if (mechanism.mechanism == CKM_RSA_PKCS) {\n+        CK_ULONG cond;\n+        ret = side_channel_free_Decrypt(encctx->provctx, sess, (void *)in,\n+                                        inlen, out, &out_size);\n+        /* the error case need to be handled in a side-channel free way, so\n+         * conditionals need to be constant time. Always setting outlen is\n+         * fine because out_size is initialized to the value of outlen\n+         * and the value should not matter in an error condition anyway */\n+        *outlen = out_size;\n+        cond = constant_equal(ret, CKR_OK);\n+        result = constant_select_int(cond, RET_OSSL_OK, RET_OSSL_ERR);\n+        goto endsess;\n+    }\n+\n     ret = p11prov_Decrypt(encctx->provctx, sess, (void *)in, inlen, out,\n                           &out_size);\n     if (ret != CKR_OK) {\n         goto endsess;\n     }\n-\n     *outlen = out_size;\n     result = RET_OSSL_OK;\n ",
        "diff_line_info": {
            "deleted_lines": [
                ""
            ],
            "added_lines": [
                "    /* Special handling against PKCS#1 1.5 side channel leaking */",
                "    if (mechanism.mechanism == CKM_RSA_PKCS) {",
                "        CK_ULONG cond;",
                "        ret = side_channel_free_Decrypt(encctx->provctx, sess, (void *)in,",
                "                                        inlen, out, &out_size);",
                "        /* the error case need to be handled in a side-channel free way, so",
                "         * conditionals need to be constant time. Always setting outlen is",
                "         * fine because out_size is initialized to the value of outlen",
                "         * and the value should not matter in an error condition anyway */",
                "        *outlen = out_size;",
                "        cond = constant_equal(ret, CKR_OK);",
                "        result = constant_select_int(cond, RET_OSSL_OK, RET_OSSL_ERR);",
                "        goto endsess;",
                "    }",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27170",
        "func_name": "torvalds/linux/adjust_ptr_min_max_vals",
        "description": "An issue was discovered in the Linux kernel before 5.11.8. kernel/bpf/verifier.c performs undesirable out-of-bounds speculation on pointer arithmetic, leading to side-channel attacks that defeat Spectre mitigations and obtain sensitive information from kernel memory, aka CID-f232326f6966. This affects pointer types that do not define a ptr_limit.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f232326f6966cf2a1d1db7bc917a4ce5f9f55f76",
        "commit_title": "The purpose of this patch is to streamline error propagation and in particular",
        "commit_text": "to propagate retrieve_ptr_limit() errors for pointer types that are not defining a ptr_limit such that register-based alu ops against these types can be rejected.  The main rationale is that a gap has been identified by Piotr in the existing protection against speculatively out-of-bounds loads, for example, in case of ctx pointers, unprivileged programs can still perform pointer arithmetic. This can be abused to execute speculatively out-of-bounds loads without restrictions and thus extract contents of kernel memory.  Fix this by rejecting unprivileged programs that attempt any pointer arithmetic on unprotected pointer types. The two affected ones are pointer to ctx as well as pointer to map. Field access to a modified ctx' pointer is rejected at a later point in time in the verifier, and 7c6967326267 (\"bpf: Permit map_ptr arithmetic with opcode add and offset 0\") only relevant for root-only use cases. Risk of unprivileged program breakage is considered very low.  Co-developed-by: Daniel Borkmann <daniel@iogearbox.net> ",
        "func_before": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->bypass_spec_v1) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access_for_ptr_arithmetic(\n\t\t\t\t   env, dst, dst_reg, dst_reg->off +\n\t\t\t\t   dst_reg->var_off.value)) {\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func": "static int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\t\t/* smin_val represents the known value */\n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps, paths, or prohibited types\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps, paths, or prohibited types\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->bypass_spec_v1) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access_for_ptr_arithmetic(\n\t\t\t\t   env, dst, dst_reg, dst_reg->off +\n\t\t\t\t   dst_reg->var_off.value)) {\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -88,7 +88,7 @@\n \tcase BPF_ADD:\n \t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n \t\tif (ret < 0) {\n-\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n+\t\t\tverbose(env, \"R%d tried to add from different maps, paths, or prohibited types\\n\", dst);\n \t\t\treturn ret;\n \t\t}\n \t\t/* We can take a fixed offset as long as it doesn't overflow\n@@ -143,7 +143,7 @@\n \tcase BPF_SUB:\n \t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n \t\tif (ret < 0) {\n-\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n+\t\t\tverbose(env, \"R%d tried to sub from different maps, paths, or prohibited types\\n\", dst);\n \t\t\treturn ret;\n \t\t}\n \t\tif (dst_reg == off_reg) {",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);",
                "\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);"
            ],
            "added_lines": [
                "\t\t\tverbose(env, \"R%d tried to add from different maps, paths, or prohibited types\\n\", dst);",
                "\t\t\tverbose(env, \"R%d tried to sub from different maps, paths, or prohibited types\\n\", dst);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-27170",
        "func_name": "torvalds/linux/sanitize_ptr_alu",
        "description": "An issue was discovered in the Linux kernel before 5.11.8. kernel/bpf/verifier.c performs undesirable out-of-bounds speculation on pointer arithmetic, leading to side-channel attacks that defeat Spectre mitigations and obtain sensitive information from kernel memory, aka CID-f232326f6966. This affects pointer types that do not define a ptr_limit.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=f232326f6966cf2a1d1db7bc917a4ce5f9f55f76",
        "commit_title": "The purpose of this patch is to streamline error propagation and in particular",
        "commit_text": "to propagate retrieve_ptr_limit() errors for pointer types that are not defining a ptr_limit such that register-based alu ops against these types can be rejected.  The main rationale is that a gap has been identified by Piotr in the existing protection against speculatively out-of-bounds loads, for example, in case of ctx pointers, unprivileged programs can still perform pointer arithmetic. This can be abused to execute speculatively out-of-bounds loads without restrictions and thus extract contents of kernel memory.  Fix this by rejecting unprivileged programs that attempt any pointer arithmetic on unprotected pointer types. The two affected ones are pointer to ctx as well as pointer to map. Field access to a modified ctx' pointer is rejected at a later point in time in the verifier, and 7c6967326267 (\"bpf: Permit map_ptr arithmetic with opcode add and offset 0\") only relevant for root-only use cases. Risk of unprivileged program breakage is considered very low.  Co-developed-by: Daniel Borkmann <daniel@iogearbox.net> ",
        "func_before": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}",
        "func": "static int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\tint err;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\terr = retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);\n\tif (err < 0)\n\t\treturn err;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,7 @@\n \tu32 alu_state, alu_limit;\n \tstruct bpf_reg_state tmp;\n \tbool ret;\n+\tint err;\n \n \tif (can_skip_alu_sanitation(env, insn))\n \t\treturn 0;\n@@ -26,10 +27,13 @@\n \talu_state |= ptr_is_dst_reg ?\n \t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n \n-\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n-\t\treturn 0;\n-\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n-\t\treturn -EACCES;\n+\terr = retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg);\n+\tif (err < 0)\n+\t\treturn err;\n+\n+\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);\n+\tif (err < 0)\n+\t\treturn err;\n do_sim:\n \t/* Simulate and find potential out-of-bounds access under\n \t * speculative execution from truncation as a result of",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))",
                "\t\treturn 0;",
                "\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))",
                "\t\treturn -EACCES;"
            ],
            "added_lines": [
                "\tint err;",
                "\terr = retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg);",
                "\tif (err < 0)",
                "\t\treturn err;",
                "",
                "\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);",
                "\tif (err < 0)",
                "\t\treturn err;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-35518",
        "func_name": "389ds/389-ds-base/ldbm_config_search_entry_callback",
        "description": "When binding against a DN during authentication, the reply from 389-ds-base will be different whether the DN exists or not. This can be used by an unauthenticated attacker to check the existence of an entry in the LDAP database.",
        "git_url": "https://github.com/389ds/389-ds-base/commit/cc0f69283abc082488824702dae485b8eae938bc",
        "commit_title": "Issue 4480 - Unexpected info returned to ldap request (#4491)",
        "commit_text": " Bug description:\r \tIf the bind entry does not exist, the bind result info\r         reports that 'No such entry'. It should not give any\r         information if the target entry exists or not\r \r Fix description:\r \tDoes not return any additional information during a bind\r \r relates: https://github.com/389ds/389-ds-base/issues/4480\r \r Reviewed by: William Brown, Viktor Ashirov, Mark Reynolds (thank you all)\r \r Platforms tested:  F31",
        "func_before": "int\nldbm_config_search_entry_callback(Slapi_PBlock *pb __attribute__((unused)),\n                                  Slapi_Entry *e,\n                                  Slapi_Entry *entryAfter __attribute__((unused)),\n                                  int *returncode,\n                                  char *returntext,\n                                  void *arg)\n{\n    char buf[BUFSIZ];\n    struct berval *vals[2];\n    struct berval val;\n    struct ldbminfo *li = (struct ldbminfo *)arg;\n    config_info *config;\n    int scope;\n\n    vals[0] = &val;\n    vals[1] = NULL;\n\n    returntext[0] = '\\0';\n\n    PR_Lock(li->li_config_mutex);\n\n    if (pb) {\n        slapi_pblock_get(pb, SLAPI_SEARCH_SCOPE, &scope);\n        if (scope == LDAP_SCOPE_BASE) {\n            char **attrs = NULL;\n            slapi_pblock_get(pb, SLAPI_SEARCH_ATTRS, &attrs);\n            if (attrs) {\n                for (size_t i = 0; attrs[i]; i++) {\n                    if (ldbm_config_moved_attr(attrs[i])) {\n                        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");\n                        break;\n                    }\n                }\n            }\n        \n        }\n    }\n\n    for (config = ldbm_config; config->config_name != NULL; config++) {\n        /* Go through the ldbm_config table and fill in the entry. */\n\n        if (!(config->config_flags & (CONFIG_FLAG_ALWAYS_SHOW | CONFIG_FLAG_PREVIOUSLY_SET))) {\n            /* This config option shouldn't be shown */\n            continue;\n        }\n\n        ldbm_config_get((void *)li, config, buf);\n\n        val.bv_val = buf;\n        val.bv_len = strlen(buf);\n        slapi_entry_attr_replace(e, config->config_name, vals);\n    }\n\n    PR_Unlock(li->li_config_mutex);\n\n    *returncode = LDAP_SUCCESS;\n    return SLAPI_DSE_CALLBACK_OK;\n}",
        "func": "int\nldbm_config_search_entry_callback(Slapi_PBlock *pb __attribute__((unused)),\n                                  Slapi_Entry *e,\n                                  Slapi_Entry *entryAfter __attribute__((unused)),\n                                  int *returncode,\n                                  char *returntext,\n                                  void *arg)\n{\n    char buf[BUFSIZ];\n    struct berval *vals[2];\n    struct berval val;\n    struct ldbminfo *li = (struct ldbminfo *)arg;\n    config_info *config;\n    int scope;\n\n    vals[0] = &val;\n    vals[1] = NULL;\n\n    returntext[0] = '\\0';\n\n    PR_Lock(li->li_config_mutex);\n\n    if (pb) {\n        slapi_pblock_get(pb, SLAPI_SEARCH_SCOPE, &scope);\n        if (scope == LDAP_SCOPE_BASE) {\n            char **attrs = NULL;\n            slapi_pblock_get(pb, SLAPI_SEARCH_ATTRS, &attrs);\n            if (attrs) {\n                for (size_t i = 0; attrs[i]; i++) {\n                    if (ldbm_config_moved_attr(attrs[i])) {\n                        slapi_pblock_set(pb, SLAPI_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");\n                        break;\n                    }\n                }\n            }\n        \n        }\n    }\n\n    for (config = ldbm_config; config->config_name != NULL; config++) {\n        /* Go through the ldbm_config table and fill in the entry. */\n\n        if (!(config->config_flags & (CONFIG_FLAG_ALWAYS_SHOW | CONFIG_FLAG_PREVIOUSLY_SET))) {\n            /* This config option shouldn't be shown */\n            continue;\n        }\n\n        ldbm_config_get((void *)li, config, buf);\n\n        val.bv_val = buf;\n        val.bv_len = strlen(buf);\n        slapi_entry_attr_replace(e, config->config_name, vals);\n    }\n\n    PR_Unlock(li->li_config_mutex);\n\n    *returncode = LDAP_SUCCESS;\n    return SLAPI_DSE_CALLBACK_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,7 +28,7 @@\n             if (attrs) {\n                 for (size_t i = 0; attrs[i]; i++) {\n                     if (ldbm_config_moved_attr(attrs[i])) {\n-                        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");\n+                        slapi_pblock_set(pb, SLAPI_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");\n                         break;\n                     }\n                 }",
        "diff_line_info": {
            "deleted_lines": [
                "                        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");"
            ],
            "added_lines": [
                "                        slapi_pblock_set(pb, SLAPI_RESULT_TEXT, \"at least one required attribute has been moved to the BDB scecific configuration entry\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-35518",
        "func_name": "389ds/389-ds-base/send_ldap_result_ext",
        "description": "When binding against a DN during authentication, the reply from 389-ds-base will be different whether the DN exists or not. This can be used by an unauthenticated attacker to check the existence of an entry in the LDAP database.",
        "git_url": "https://github.com/389ds/389-ds-base/commit/cc0f69283abc082488824702dae485b8eae938bc",
        "commit_title": "Issue 4480 - Unexpected info returned to ldap request (#4491)",
        "commit_text": " Bug description:\r \tIf the bind entry does not exist, the bind result info\r         reports that 'No such entry'. It should not give any\r         information if the target entry exists or not\r \r Fix description:\r \tDoes not return any additional information during a bind\r \r relates: https://github.com/389ds/389-ds-base/issues/4480\r \r Reviewed by: William Brown, Viktor Ashirov, Mark Reynolds (thank you all)\r \r Platforms tested:  F31",
        "func_before": "void\nsend_ldap_result_ext(\n    Slapi_PBlock *pb,\n    int err,\n    char *matched,\n    char *text,\n    int nentries,\n    struct berval **urls,\n    BerElement *ber)\n{\n    Slapi_Operation *operation;\n    passwdPolicy *pwpolicy = NULL;\n    Connection *conn = NULL;\n    Slapi_DN *sdn = NULL;\n    const char *dn = NULL;\n    ber_tag_t tag;\n    int flush_ber_element = 1;\n    ber_tag_t bind_method = 0;\n    int internal_op;\n    int i, rc, logit = 0;\n    char *pbtext;\n\n    slapi_pblock_get(pb, SLAPI_BIND_METHOD, &bind_method);\n    slapi_pblock_get(pb, SLAPI_OPERATION, &operation);\n    slapi_pblock_get(pb, SLAPI_CONNECTION, &conn);\n\n    if (text) {\n        pbtext = text;\n    } else {\n        slapi_pblock_get(pb, SLAPI_PB_RESULT_TEXT, &pbtext);\n    }\n\n    if (operation == NULL) {\n        slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"No operation found: slapi_search_internal_set_pb was incomplete (invalid 'base' ?)\\n\");\n        return;\n    }\n\n    if (operation->o_status == SLAPI_OP_STATUS_RESULT_SENT) {\n        return; /* result already sent */\n    }\n\n    if (ber != NULL) {\n        flush_ber_element = 0;\n    }\n\n    if (err != LDAP_SUCCESS) {\n        /* count the error for snmp */\n        /* first check for security errors */\n        if (err == LDAP_INVALID_CREDENTIALS || err == LDAP_INAPPROPRIATE_AUTH || err == LDAP_AUTH_METHOD_NOT_SUPPORTED || err == LDAP_STRONG_AUTH_NOT_SUPPORTED || err == LDAP_STRONG_AUTH_REQUIRED || err == LDAP_CONFIDENTIALITY_REQUIRED || err == LDAP_INSUFFICIENT_ACCESS || err == LDAP_AUTH_UNKNOWN) {\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsSecurityErrors);\n        } else if (err != LDAP_REFERRAL && err != LDAP_OPT_REFERRALS && err != LDAP_PARTIAL_RESULTS) {\n            /*madman man spec says not to count as normal errors\n                --security errors\n                --referrals\n                -- partially seviced operations will not be conted as an error\n                      */\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsErrors);\n        }\n    }\n\n    slapi_log_err(SLAPI_LOG_TRACE, \"send_ldap_result_ext\", \"=> %d:%s:%s\\n\", err,\n                  matched ? matched : \"\", text ? text : \"\");\n\n    switch (operation->o_tag) {\n    case LBER_DEFAULT:\n        tag = LBER_SEQUENCE;\n        break;\n\n    case LDAP_REQ_SEARCH:\n        tag = LDAP_RES_SEARCH_RESULT;\n        break;\n\n    case LDAP_REQ_DELETE:\n        tag = LDAP_RES_DELETE;\n        break;\n\n    case LDAP_REFERRAL:\n        if (conn && conn->c_ldapversion > LDAP_VERSION2) {\n            tag = LDAP_TAG_REFERRAL;\n            break;\n        }\n    /* FALLTHROUGH */\n\n    default:\n        tag = operation->o_tag + 1;\n        break;\n    }\n\n    internal_op = operation_is_flag_set(operation, OP_FLAG_INTERNAL);\n    if ((conn == NULL) || (internal_op)) {\n        if (operation->o_result_handler != NULL) {\n            operation->o_result_handler(conn, operation, err,\n                                        matched, text, nentries, urls);\n            logit = 1;\n        }\n        goto log_and_return;\n    }\n\n    /* invalid password.  Update the password retry here */\n    /* put this here for now.  It could be a send_result pre-op plugin. */\n    if ((err == LDAP_INVALID_CREDENTIALS) && (bind_method != LDAP_AUTH_SASL)) {\n        slapi_pblock_get(pb, SLAPI_TARGET_SDN, &sdn);\n        dn = slapi_sdn_get_dn(sdn);\n        pwpolicy = new_passwdPolicy(pb, dn);\n        if (pwpolicy && (pwpolicy->pw_lockout == 1)) {\n            if (update_pw_retry(pb) == LDAP_CONSTRAINT_VIOLATION && !pwpolicy->pw_is_legacy) {\n                /*\n                 * If we are not using the legacy pw policy behavior,\n                 * convert the error 49 to 19 (constraint violation)\n                 * and log a message\n                 */\n                err = LDAP_CONSTRAINT_VIOLATION;\n                text = \"Invalid credentials, you now have exceeded the password retry limit.\";\n            }\n        }\n    }\n\n    if (ber == NULL) {\n        if ((ber = der_alloc()) == NULL) {\n            slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"ber_alloc failed\\n\");\n            goto log_and_return;\n        }\n    }\n\n    /* there is no admin limit exceeded in v2 - change to size limit XXX */\n    if (err == LDAP_ADMINLIMIT_EXCEEDED &&\n        conn->c_ldapversion < LDAP_VERSION3) {\n        err = LDAP_SIZELIMIT_EXCEEDED;\n    }\n\n    if (conn->c_ldapversion < LDAP_VERSION3 || urls == NULL) {\n        char *save, *buf = NULL;\n\n        /*\n         * if there are v2 referrals to send, construct\n         * the v2 referral string.\n         */\n        if (urls != NULL) {\n            int len;\n\n            /* count the referral */\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsReferrals);\n\n            /*\n             * figure out how much space we need\n             */\n            len = 10; /* strlen(\"Referral:\") + NULL */\n            for (i = 0; urls[i] != NULL; i++) {\n                len += urls[i]->bv_len + 1; /* newline + ref */\n            }\n            if (text != NULL) {\n                len += strlen(text) + 1; /* text + newline */\n            }\n            /*\n             * allocate buffer and fill it in with the error\n             * message plus v2-style referrals.\n             */\n            buf = slapi_ch_malloc(len);\n            *buf = '\\0';\n            if (text != NULL) {\n                strcpy(buf, text);\n                strcat(buf, \"\\n\");\n            }\n            strcat(buf, \"Referral:\");\n            for (i = 0; urls[i] != NULL; i++) {\n                strcat(buf, \"\\n\");\n                strcat(buf, urls[i]->bv_val);\n            }\n            save = text;\n            text = buf;\n        }\n\n        if ((conn->c_ldapversion < LDAP_VERSION3 &&\n             err == LDAP_REFERRAL) ||\n            urls != NULL) {\n            err = LDAP_PARTIAL_RESULTS;\n        }\n        rc = ber_printf(ber, \"{it{ess\", operation->o_msgid, tag, err,\n                        matched ? matched : \"\", pbtext ? pbtext : \"\");\n\n        /*\n         * if this is an LDAPv3 ExtendedResponse to an ExtendedRequest,\n         * check to see if the optional responseName and response OCTET\n         * STRING need to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_extended_result(pb, tag, ber);\n        }\n\n        /*\n         * if this is an LDAPv3 BindResponse, check to see if the\n         * optional serverSaslCreds OCTET STRING is present and needs\n         * to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_SASL_response(pb, tag, ber, conn);\n            /* XXXmcs: should we also check for a missing auth response control? */\n        }\n\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* one more } to come */\n        }\n\n        if (buf != NULL) {\n            text = save;\n            slapi_ch_free((void **)&buf);\n        }\n    } else {\n        /*\n         * there are v3 referrals to add to the result\n         */\n        /* count the referral */\n        if (!config_check_referral_mode())\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsReferrals);\n        rc = ber_printf(ber, \"{it{esst{s\", operation->o_msgid, tag, err,\n                        matched ? matched : \"\", text ? text : \"\", LDAP_TAG_REFERRAL,\n                        urls[0]->bv_val);\n        for (i = 1; urls[i] != NULL && rc != LBER_ERROR; i++) {\n            rc = ber_printf(ber, \"s\", urls[i]->bv_val);\n        }\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* two more } to come */\n        }\n\n        /*\n         * if this is an LDAPv3 ExtendedResponse to an ExtendedRequest,\n         * check to see if the optional responseName and response OCTET\n         * STRING need to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_extended_result(pb, tag, ber);\n        }\n\n        /*\n         * if this is an LDAPv3 BindResponse, check to see if the\n         * optional serverSaslCreds OCTET STRING is present and needs\n         * to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_SASL_response(pb, tag, ber, conn);\n        }\n\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* one more } to come */\n        }\n    }\n    if (err == LDAP_SUCCESS) {\n        /*\n         * Process the Read Entry Controls (if any)\n         */\n        if (process_read_entry_controls(pb, LDAP_CONTROL_PRE_READ_ENTRY)) {\n            err = LDAP_UNAVAILABLE_CRITICAL_EXTENSION;\n            goto log_and_return;\n        }\n        if (process_read_entry_controls(pb, LDAP_CONTROL_POST_READ_ENTRY)) {\n            err = LDAP_UNAVAILABLE_CRITICAL_EXTENSION;\n            goto log_and_return;\n        }\n    }\n    if (operation->o_results.result_controls != NULL && conn->c_ldapversion >= LDAP_VERSION3 && write_controls(ber, operation->o_results.result_controls) != 0) {\n        rc = (int)LBER_ERROR;\n    }\n\n    if (rc != LBER_ERROR) { /* end the LDAPMessage sequence */\n        rc = ber_put_seq(ber);\n    }\n\n    if (rc == LBER_ERROR) {\n        slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"ber_printf failed 1\\n\");\n        if (flush_ber_element == 1) {\n            /* we alloced the ber */\n            ber_free(ber, 1 /* freebuf */);\n        }\n        goto log_and_return;\n    }\n\n    if (flush_ber_element) {\n        /* write only one pdu at a time - wait til it's our turn */\n        if (flush_ber(pb, conn, operation, ber, _LDAP_SEND_RESULT) == 0) {\n            logit = 1;\n        }\n    }\n\nlog_and_return:\n    operation->o_status = SLAPI_OP_STATUS_RESULT_SENT; /* in case this has not yet been set */\n\n    if (logit && (operation_is_flag_set(operation, OP_FLAG_ACTION_LOG_ACCESS) ||\n                  (internal_op && config_get_plugin_logging()))) {\n        log_result(pb, operation, err, tag, nentries);\n    }\n\n    slapi_log_err(SLAPI_LOG_TRACE, \"send_ldap_result_ext\", \"<= %d\\n\", err);\n}",
        "func": "void\nsend_ldap_result_ext(\n    Slapi_PBlock *pb,\n    int err,\n    char *matched,\n    char *text,\n    int nentries,\n    struct berval **urls,\n    BerElement *ber)\n{\n    Slapi_Operation *operation;\n    passwdPolicy *pwpolicy = NULL;\n    Connection *conn = NULL;\n    Slapi_DN *sdn = NULL;\n    const char *dn = NULL;\n    ber_tag_t tag;\n    int flush_ber_element = 1;\n    ber_tag_t bind_method = 0;\n    int internal_op;\n    int i, rc, logit = 0;\n    char *pbtext;\n\n    slapi_pblock_get(pb, SLAPI_BIND_METHOD, &bind_method);\n    slapi_pblock_get(pb, SLAPI_OPERATION, &operation);\n    slapi_pblock_get(pb, SLAPI_CONNECTION, &conn);\n\n    if (text) {\n        pbtext = text;\n    } else {\n        slapi_pblock_get(pb, SLAPI_RESULT_TEXT, &pbtext);\n    }\n\n    if (operation == NULL) {\n        slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"No operation found: slapi_search_internal_set_pb was incomplete (invalid 'base' ?)\\n\");\n        return;\n    }\n\n    if (operation->o_status == SLAPI_OP_STATUS_RESULT_SENT) {\n        return; /* result already sent */\n    }\n\n    if (ber != NULL) {\n        flush_ber_element = 0;\n    }\n\n    if (err != LDAP_SUCCESS) {\n        /* count the error for snmp */\n        /* first check for security errors */\n        if (err == LDAP_INVALID_CREDENTIALS || err == LDAP_INAPPROPRIATE_AUTH || err == LDAP_AUTH_METHOD_NOT_SUPPORTED || err == LDAP_STRONG_AUTH_NOT_SUPPORTED || err == LDAP_STRONG_AUTH_REQUIRED || err == LDAP_CONFIDENTIALITY_REQUIRED || err == LDAP_INSUFFICIENT_ACCESS || err == LDAP_AUTH_UNKNOWN) {\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsSecurityErrors);\n        } else if (err != LDAP_REFERRAL && err != LDAP_OPT_REFERRALS && err != LDAP_PARTIAL_RESULTS) {\n            /*madman man spec says not to count as normal errors\n                --security errors\n                --referrals\n                -- partially seviced operations will not be conted as an error\n                      */\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsErrors);\n        }\n    }\n\n    slapi_log_err(SLAPI_LOG_TRACE, \"send_ldap_result_ext\", \"=> %d:%s:%s\\n\", err,\n                  matched ? matched : \"\", text ? text : \"\");\n\n    switch (operation->o_tag) {\n    case LBER_DEFAULT:\n        tag = LBER_SEQUENCE;\n        break;\n\n    case LDAP_REQ_SEARCH:\n        tag = LDAP_RES_SEARCH_RESULT;\n        break;\n\n    case LDAP_REQ_DELETE:\n        tag = LDAP_RES_DELETE;\n        break;\n\n    case LDAP_REFERRAL:\n        if (conn && conn->c_ldapversion > LDAP_VERSION2) {\n            tag = LDAP_TAG_REFERRAL;\n            break;\n        }\n    /* FALLTHROUGH */\n\n    default:\n        tag = operation->o_tag + 1;\n        break;\n    }\n\n    internal_op = operation_is_flag_set(operation, OP_FLAG_INTERNAL);\n    if ((conn == NULL) || (internal_op)) {\n        if (operation->o_result_handler != NULL) {\n            operation->o_result_handler(conn, operation, err,\n                                        matched, text, nentries, urls);\n            logit = 1;\n        }\n        goto log_and_return;\n    }\n\n    /* invalid password.  Update the password retry here */\n    /* put this here for now.  It could be a send_result pre-op plugin. */\n    if ((err == LDAP_INVALID_CREDENTIALS) && (bind_method != LDAP_AUTH_SASL)) {\n        slapi_pblock_get(pb, SLAPI_TARGET_SDN, &sdn);\n        dn = slapi_sdn_get_dn(sdn);\n        pwpolicy = new_passwdPolicy(pb, dn);\n        if (pwpolicy && (pwpolicy->pw_lockout == 1)) {\n            if (update_pw_retry(pb) == LDAP_CONSTRAINT_VIOLATION && !pwpolicy->pw_is_legacy) {\n                /*\n                 * If we are not using the legacy pw policy behavior,\n                 * convert the error 49 to 19 (constraint violation)\n                 * and log a message\n                 */\n                err = LDAP_CONSTRAINT_VIOLATION;\n                text = \"Invalid credentials, you now have exceeded the password retry limit.\";\n            }\n        }\n    }\n\n    if (ber == NULL) {\n        if ((ber = der_alloc()) == NULL) {\n            slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"ber_alloc failed\\n\");\n            goto log_and_return;\n        }\n    }\n\n    /* there is no admin limit exceeded in v2 - change to size limit XXX */\n    if (err == LDAP_ADMINLIMIT_EXCEEDED &&\n        conn->c_ldapversion < LDAP_VERSION3) {\n        err = LDAP_SIZELIMIT_EXCEEDED;\n    }\n\n    if (conn->c_ldapversion < LDAP_VERSION3 || urls == NULL) {\n        char *save, *buf = NULL;\n\n        /*\n         * if there are v2 referrals to send, construct\n         * the v2 referral string.\n         */\n        if (urls != NULL) {\n            int len;\n\n            /* count the referral */\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsReferrals);\n\n            /*\n             * figure out how much space we need\n             */\n            len = 10; /* strlen(\"Referral:\") + NULL */\n            for (i = 0; urls[i] != NULL; i++) {\n                len += urls[i]->bv_len + 1; /* newline + ref */\n            }\n            if (text != NULL) {\n                len += strlen(text) + 1; /* text + newline */\n            }\n            /*\n             * allocate buffer and fill it in with the error\n             * message plus v2-style referrals.\n             */\n            buf = slapi_ch_malloc(len);\n            *buf = '\\0';\n            if (text != NULL) {\n                strcpy(buf, text);\n                strcat(buf, \"\\n\");\n            }\n            strcat(buf, \"Referral:\");\n            for (i = 0; urls[i] != NULL; i++) {\n                strcat(buf, \"\\n\");\n                strcat(buf, urls[i]->bv_val);\n            }\n            save = text;\n            text = buf;\n        }\n\n        if ((conn->c_ldapversion < LDAP_VERSION3 &&\n             err == LDAP_REFERRAL) ||\n            urls != NULL) {\n            err = LDAP_PARTIAL_RESULTS;\n        }\n        rc = ber_printf(ber, \"{it{ess\", operation->o_msgid, tag, err,\n                        matched ? matched : \"\", pbtext ? pbtext : \"\");\n\n        /*\n         * if this is an LDAPv3 ExtendedResponse to an ExtendedRequest,\n         * check to see if the optional responseName and response OCTET\n         * STRING need to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_extended_result(pb, tag, ber);\n        }\n\n        /*\n         * if this is an LDAPv3 BindResponse, check to see if the\n         * optional serverSaslCreds OCTET STRING is present and needs\n         * to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_SASL_response(pb, tag, ber, conn);\n            /* XXXmcs: should we also check for a missing auth response control? */\n        }\n\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* one more } to come */\n        }\n\n        if (buf != NULL) {\n            text = save;\n            slapi_ch_free((void **)&buf);\n        }\n    } else {\n        /*\n         * there are v3 referrals to add to the result\n         */\n        /* count the referral */\n        if (!config_check_referral_mode())\n            slapi_counter_increment(g_get_global_snmp_vars()->ops_tbl.dsReferrals);\n        rc = ber_printf(ber, \"{it{esst{s\", operation->o_msgid, tag, err,\n                        matched ? matched : \"\", text ? text : \"\", LDAP_TAG_REFERRAL,\n                        urls[0]->bv_val);\n        for (i = 1; urls[i] != NULL && rc != LBER_ERROR; i++) {\n            rc = ber_printf(ber, \"s\", urls[i]->bv_val);\n        }\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* two more } to come */\n        }\n\n        /*\n         * if this is an LDAPv3 ExtendedResponse to an ExtendedRequest,\n         * check to see if the optional responseName and response OCTET\n         * STRING need to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_extended_result(pb, tag, ber);\n        }\n\n        /*\n         * if this is an LDAPv3 BindResponse, check to see if the\n         * optional serverSaslCreds OCTET STRING is present and needs\n         * to be appended.\n         */\n        if (rc != LBER_ERROR) {\n            rc = check_and_send_SASL_response(pb, tag, ber, conn);\n        }\n\n        if (rc != LBER_ERROR) {\n            rc = ber_printf(ber, \"}\"); /* one more } to come */\n        }\n    }\n    if (err == LDAP_SUCCESS) {\n        /*\n         * Process the Read Entry Controls (if any)\n         */\n        if (process_read_entry_controls(pb, LDAP_CONTROL_PRE_READ_ENTRY)) {\n            err = LDAP_UNAVAILABLE_CRITICAL_EXTENSION;\n            goto log_and_return;\n        }\n        if (process_read_entry_controls(pb, LDAP_CONTROL_POST_READ_ENTRY)) {\n            err = LDAP_UNAVAILABLE_CRITICAL_EXTENSION;\n            goto log_and_return;\n        }\n    }\n    if (operation->o_results.result_controls != NULL && conn->c_ldapversion >= LDAP_VERSION3 && write_controls(ber, operation->o_results.result_controls) != 0) {\n        rc = (int)LBER_ERROR;\n    }\n\n    if (rc != LBER_ERROR) { /* end the LDAPMessage sequence */\n        rc = ber_put_seq(ber);\n    }\n\n    if (rc == LBER_ERROR) {\n        slapi_log_err(SLAPI_LOG_ERR, \"send_ldap_result_ext\", \"ber_printf failed 1\\n\");\n        if (flush_ber_element == 1) {\n            /* we alloced the ber */\n            ber_free(ber, 1 /* freebuf */);\n        }\n        goto log_and_return;\n    }\n\n    if (flush_ber_element) {\n        /* write only one pdu at a time - wait til it's our turn */\n        if (flush_ber(pb, conn, operation, ber, _LDAP_SEND_RESULT) == 0) {\n            logit = 1;\n        }\n    }\n\nlog_and_return:\n    operation->o_status = SLAPI_OP_STATUS_RESULT_SENT; /* in case this has not yet been set */\n\n    if (logit && (operation_is_flag_set(operation, OP_FLAG_ACTION_LOG_ACCESS) ||\n                  (internal_op && config_get_plugin_logging()))) {\n        log_result(pb, operation, err, tag, nentries);\n    }\n\n    slapi_log_err(SLAPI_LOG_TRACE, \"send_ldap_result_ext\", \"<= %d\\n\", err);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,7 +27,7 @@\n     if (text) {\n         pbtext = text;\n     } else {\n-        slapi_pblock_get(pb, SLAPI_PB_RESULT_TEXT, &pbtext);\n+        slapi_pblock_get(pb, SLAPI_RESULT_TEXT, &pbtext);\n     }\n \n     if (operation == NULL) {",
        "diff_line_info": {
            "deleted_lines": [
                "        slapi_pblock_get(pb, SLAPI_PB_RESULT_TEXT, &pbtext);"
            ],
            "added_lines": [
                "        slapi_pblock_get(pb, SLAPI_RESULT_TEXT, &pbtext);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-35518",
        "func_name": "389ds/389-ds-base/ldbm_back_bind",
        "description": "When binding against a DN during authentication, the reply from 389-ds-base will be different whether the DN exists or not. This can be used by an unauthenticated attacker to check the existence of an entry in the LDAP database.",
        "git_url": "https://github.com/389ds/389-ds-base/commit/b6aae4d8e7c8a6ddd21646f94fef1bf7f22c3f32",
        "commit_title": "Issue 4609 - CVE - info disclosure when authenticating",
        "commit_text": " Description:  If you bind as a user that does not exist.  Error 49 is returned               instead of error 32.  As error 32 discloses that the entry does               not exist.  When you bind as an entry that does not have userpassword               set then error 48 (inappropriate auth) is returned, but this               discloses that the entry does indeed exist.  Instead we should               always return error 49, even if the password is not set in the               entry.  This way we do not disclose to an attacker if the Bind               DN exists or not.  Relates: https://github.com/389ds/389-ds-base/issues/4609  Reviewed by: tbordaz(Thanks!)",
        "func_before": "int\nldbm_back_bind(Slapi_PBlock *pb)\n{\n    backend *be;\n    ldbm_instance *inst;\n    ber_tag_t method;\n    struct berval *cred;\n    struct ldbminfo *li;\n    struct backentry *e;\n    Slapi_Attr *attr;\n    Slapi_Value **bvals;\n    entry_address *addr;\n    back_txn txn = {NULL};\n    int rc = SLAPI_BIND_SUCCESS;\n    int result_sent = 0;\n\n    /* get parameters */\n    slapi_pblock_get(pb, SLAPI_BACKEND, &be);\n    slapi_pblock_get(pb, SLAPI_PLUGIN_PRIVATE, &li);\n    slapi_pblock_get(pb, SLAPI_TARGET_ADDRESS, &addr);\n    slapi_pblock_get(pb, SLAPI_BIND_METHOD, &method);\n    slapi_pblock_get(pb, SLAPI_BIND_CREDENTIALS, &cred);\n    slapi_pblock_get(pb, SLAPI_TXN, &txn.back_txn_txn);\n\n    if (!txn.back_txn_txn) {\n        dblayer_txn_init(li, &txn);\n        slapi_pblock_set(pb, SLAPI_TXN, txn.back_txn_txn);\n    }\n\n    inst = (ldbm_instance *)be->be_instance_info;\n    if (inst->inst_ref_count) {\n        slapi_counter_increment(inst->inst_ref_count);\n    } else {\n        slapi_log_err(SLAPI_LOG_ERR, \"ldbm_back_bind\",\n                      \"instance %s does not exist.\\n\", inst->inst_name);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    /* always allow noauth simple binds (front end will send the result) */\n    if (method == LDAP_AUTH_SIMPLE && cred->bv_len == 0) {\n        rc = SLAPI_BIND_ANONYMOUS;\n        goto bail;\n    }\n\n    /*\n     * find the target entry.  find_entry() takes care of referrals\n     *   and sending errors if the entry does not exist.\n     */\n    if ((e = find_entry(pb, be, addr, &txn, &result_sent)) == NULL) {\n        rc = SLAPI_BIND_FAIL;\n        /* In the failure case, the result is supposed to be sent in the backend. */\n        if (!result_sent) {\n            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL, NULL, 0, NULL);\n        }\n        goto bail;\n    }\n\n    switch (method) {\n    case LDAP_AUTH_SIMPLE: {\n        Slapi_Value cv;\n        if (slapi_entry_attr_find(e->ep_entry, \"userpassword\", &attr) != 0) {\n            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL,\n                                   NULL, 0, NULL);\n            CACHE_RETURN(&inst->inst_cache, &e);\n            rc = SLAPI_BIND_FAIL;\n            goto bail;\n        }\n        bvals = attr_get_present_values(attr);\n        slapi_value_init_berval(&cv, cred);\n        if (slapi_pw_find_sv(bvals, &cv) != 0) {\n            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Invalid credentials\");\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            CACHE_RETURN(&inst->inst_cache, &e);\n            value_done(&cv);\n            rc = SLAPI_BIND_FAIL;\n            goto bail;\n        }\n        value_done(&cv);\n    } break;\n\n    default:\n        slapi_send_ldap_result(pb, LDAP_STRONG_AUTH_NOT_SUPPORTED, NULL,\n                               \"auth method not supported\", 0, NULL);\n        CACHE_RETURN(&inst->inst_cache, &e);\n        rc = SLAPI_BIND_FAIL;\n        goto bail;\n    }\n\n    CACHE_RETURN(&inst->inst_cache, &e);\nbail:\n    if (inst->inst_ref_count) {\n        slapi_counter_decrement(inst->inst_ref_count);\n    }\n    /* success:  front end will send result */\n    return rc;\n}",
        "func": "int\nldbm_back_bind(Slapi_PBlock *pb)\n{\n    backend *be;\n    ldbm_instance *inst;\n    ber_tag_t method;\n    struct berval *cred;\n    struct ldbminfo *li;\n    struct backentry *e;\n    Slapi_Attr *attr;\n    Slapi_Value **bvals;\n    entry_address *addr;\n    back_txn txn = {NULL};\n    int rc = SLAPI_BIND_SUCCESS;\n    int result_sent = 0;\n\n    /* get parameters */\n    slapi_pblock_get(pb, SLAPI_BACKEND, &be);\n    slapi_pblock_get(pb, SLAPI_PLUGIN_PRIVATE, &li);\n    slapi_pblock_get(pb, SLAPI_TARGET_ADDRESS, &addr);\n    slapi_pblock_get(pb, SLAPI_BIND_METHOD, &method);\n    slapi_pblock_get(pb, SLAPI_BIND_CREDENTIALS, &cred);\n    slapi_pblock_get(pb, SLAPI_TXN, &txn.back_txn_txn);\n\n    if (!txn.back_txn_txn) {\n        dblayer_txn_init(li, &txn);\n        slapi_pblock_set(pb, SLAPI_TXN, txn.back_txn_txn);\n    }\n\n    inst = (ldbm_instance *)be->be_instance_info;\n    if (inst->inst_ref_count) {\n        slapi_counter_increment(inst->inst_ref_count);\n    } else {\n        slapi_log_err(SLAPI_LOG_ERR, \"ldbm_back_bind\",\n                      \"instance %s does not exist.\\n\", inst->inst_name);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    /* always allow noauth simple binds (front end will send the result) */\n    if (method == LDAP_AUTH_SIMPLE && cred->bv_len == 0) {\n        rc = SLAPI_BIND_ANONYMOUS;\n        goto bail;\n    }\n\n    /*\n     * find the target entry.  find_entry() takes care of referrals\n     *   and sending errors if the entry does not exist.\n     */\n    if ((e = find_entry(pb, be, addr, &txn, &result_sent)) == NULL) {\n        rc = SLAPI_BIND_FAIL;\n        /* In the failure case, the result is supposed to be sent in the backend. */\n        if (!result_sent) {\n            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL, NULL, 0, NULL);\n        }\n        goto bail;\n    }\n\n    switch (method) {\n    case LDAP_AUTH_SIMPLE: {\n        Slapi_Value cv;\n        if (slapi_entry_attr_find(e->ep_entry, \"userpassword\", &attr) != 0) {\n            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            CACHE_RETURN(&inst->inst_cache, &e);\n            rc = SLAPI_BIND_FAIL;\n            goto bail;\n        }\n        bvals = attr_get_present_values(attr);\n        slapi_value_init_berval(&cv, cred);\n        if (slapi_pw_find_sv(bvals, &cv) != 0) {\n            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Invalid credentials\");\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            CACHE_RETURN(&inst->inst_cache, &e);\n            value_done(&cv);\n            rc = SLAPI_BIND_FAIL;\n            goto bail;\n        }\n        value_done(&cv);\n    } break;\n\n    default:\n        slapi_send_ldap_result(pb, LDAP_STRONG_AUTH_NOT_SUPPORTED, NULL,\n                               \"auth method not supported\", 0, NULL);\n        CACHE_RETURN(&inst->inst_cache, &e);\n        rc = SLAPI_BIND_FAIL;\n        goto bail;\n    }\n\n    CACHE_RETURN(&inst->inst_cache, &e);\nbail:\n    if (inst->inst_ref_count) {\n        slapi_counter_decrement(inst->inst_ref_count);\n    }\n    /* success:  front end will send result */\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -59,8 +59,8 @@\n     case LDAP_AUTH_SIMPLE: {\n         Slapi_Value cv;\n         if (slapi_entry_attr_find(e->ep_entry, \"userpassword\", &attr) != 0) {\n-            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL,\n-                                   NULL, 0, NULL);\n+            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");\n+            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n             CACHE_RETURN(&inst->inst_cache, &e);\n             rc = SLAPI_BIND_FAIL;\n             goto bail;",
        "diff_line_info": {
            "deleted_lines": [
                "            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL,",
                "                                   NULL, 0, NULL);"
            ],
            "added_lines": [
                "            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");",
                "            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-35518",
        "func_name": "389ds/389-ds-base/dse_bind",
        "description": "When binding against a DN during authentication, the reply from 389-ds-base will be different whether the DN exists or not. This can be used by an unauthenticated attacker to check the existence of an entry in the LDAP database.",
        "git_url": "https://github.com/389ds/389-ds-base/commit/b6aae4d8e7c8a6ddd21646f94fef1bf7f22c3f32",
        "commit_title": "Issue 4609 - CVE - info disclosure when authenticating",
        "commit_text": " Description:  If you bind as a user that does not exist.  Error 49 is returned               instead of error 32.  As error 32 discloses that the entry does               not exist.  When you bind as an entry that does not have userpassword               set then error 48 (inappropriate auth) is returned, but this               discloses that the entry does indeed exist.  Instead we should               always return error 49, even if the password is not set in the               entry.  This way we do not disclose to an attacker if the Bind               DN exists or not.  Relates: https://github.com/389ds/389-ds-base/issues/4609  Reviewed by: tbordaz(Thanks!)",
        "func_before": "int\ndse_bind(Slapi_PBlock *pb) /* JCM There should only be one exit point from this function! */\n{\n    ber_tag_t method;    /* The bind method */\n    struct berval *cred; /* The bind credentials */\n    Slapi_Value **bvals;\n    struct dse *pdse;\n    Slapi_Attr *attr;\n    Slapi_DN *sdn = NULL;\n    Slapi_Entry *ec = NULL;\n\n    /*Get the parameters*/\n    if (slapi_pblock_get(pb, SLAPI_PLUGIN_PRIVATE, &pdse) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_TARGET_SDN, &sdn) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_METHOD, &method) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_CREDENTIALS, &cred) < 0) {\n        slapi_send_ldap_result(pb, LDAP_OPERATIONS_ERROR, NULL, NULL, 0, NULL);\n        return SLAPI_BIND_FAIL;\n    }\n\n    /* always allow noauth simple binds */\n    if (method == LDAP_AUTH_SIMPLE && cred->bv_len == 0) {\n        /*\n         * report success to client, but return\n         * SLAPI_BIND_FAIL so we don't\n         * authorize based on noauth credentials\n         */\n        slapi_send_ldap_result(pb, LDAP_SUCCESS, NULL, NULL, 0, NULL);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    ec = dse_get_entry_copy(pdse, sdn, DSE_USE_LOCK);\n    if (ec == NULL) {\n        slapi_send_ldap_result(pb, LDAP_NO_SUCH_OBJECT, NULL, NULL, 0, NULL);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    switch (method) {\n    case LDAP_AUTH_SIMPLE: {\n        Slapi_Value cv;\n        if (slapi_entry_attr_find(ec, \"userpassword\", &attr) != 0) {\n            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL, NULL, 0, NULL);\n            slapi_entry_free(ec);\n            return SLAPI_BIND_FAIL;\n        }\n        bvals = attr_get_present_values(attr);\n\n        slapi_value_init_berval(&cv, cred);\n        if (slapi_pw_find_sv(bvals, &cv) != 0) {\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            slapi_entry_free(ec);\n            value_done(&cv);\n            return SLAPI_BIND_FAIL;\n        }\n        value_done(&cv);\n    } break;\n\n    default:\n        slapi_send_ldap_result(pb, LDAP_STRONG_AUTH_NOT_SUPPORTED, NULL, \"auth method not supported\", 0, NULL);\n        slapi_entry_free(ec);\n        return SLAPI_BIND_FAIL;\n    }\n    slapi_entry_free(ec);\n    /* success:  front end will send result */\n    return SLAPI_BIND_SUCCESS;\n}",
        "func": "int\ndse_bind(Slapi_PBlock *pb) /* JCM There should only be one exit point from this function! */\n{\n    ber_tag_t method;    /* The bind method */\n    struct berval *cred; /* The bind credentials */\n    Slapi_Value **bvals;\n    struct dse *pdse;\n    Slapi_Attr *attr;\n    Slapi_DN *sdn = NULL;\n    Slapi_Entry *ec = NULL;\n\n    /*Get the parameters*/\n    if (slapi_pblock_get(pb, SLAPI_PLUGIN_PRIVATE, &pdse) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_TARGET_SDN, &sdn) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_METHOD, &method) < 0 ||\n        slapi_pblock_get(pb, SLAPI_BIND_CREDENTIALS, &cred) < 0) {\n        slapi_send_ldap_result(pb, LDAP_OPERATIONS_ERROR, NULL, NULL, 0, NULL);\n        return SLAPI_BIND_FAIL;\n    }\n\n    /* always allow noauth simple binds */\n    if (method == LDAP_AUTH_SIMPLE && cred->bv_len == 0) {\n        /*\n         * report success to client, but return\n         * SLAPI_BIND_FAIL so we don't\n         * authorize based on noauth credentials\n         */\n        slapi_send_ldap_result(pb, LDAP_SUCCESS, NULL, NULL, 0, NULL);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    ec = dse_get_entry_copy(pdse, sdn, DSE_USE_LOCK);\n    if (ec == NULL) {\n        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not exist\");\n        slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n        return (SLAPI_BIND_FAIL);\n    }\n\n    switch (method) {\n    case LDAP_AUTH_SIMPLE: {\n        Slapi_Value cv;\n        if (slapi_entry_attr_find(ec, \"userpassword\", &attr) != 0) {\n            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            slapi_entry_free(ec);\n            return SLAPI_BIND_FAIL;\n        }\n        bvals = attr_get_present_values(attr);\n\n        slapi_value_init_berval(&cv, cred);\n        if (slapi_pw_find_sv(bvals, &cv) != 0) {\n            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Invalid credentials\");\n            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n            slapi_entry_free(ec);\n            value_done(&cv);\n            return SLAPI_BIND_FAIL;\n        }\n        value_done(&cv);\n    } break;\n\n    default:\n        slapi_send_ldap_result(pb, LDAP_STRONG_AUTH_NOT_SUPPORTED, NULL, \"auth method not supported\", 0, NULL);\n        slapi_entry_free(ec);\n        return SLAPI_BIND_FAIL;\n    }\n    slapi_entry_free(ec);\n    /* success:  front end will send result */\n    return SLAPI_BIND_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -31,7 +31,8 @@\n \n     ec = dse_get_entry_copy(pdse, sdn, DSE_USE_LOCK);\n     if (ec == NULL) {\n-        slapi_send_ldap_result(pb, LDAP_NO_SUCH_OBJECT, NULL, NULL, 0, NULL);\n+        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not exist\");\n+        slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n         return (SLAPI_BIND_FAIL);\n     }\n \n@@ -39,7 +40,8 @@\n     case LDAP_AUTH_SIMPLE: {\n         Slapi_Value cv;\n         if (slapi_entry_attr_find(ec, \"userpassword\", &attr) != 0) {\n-            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL, NULL, 0, NULL);\n+            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");\n+            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n             slapi_entry_free(ec);\n             return SLAPI_BIND_FAIL;\n         }\n@@ -47,6 +49,7 @@\n \n         slapi_value_init_berval(&cv, cred);\n         if (slapi_pw_find_sv(bvals, &cv) != 0) {\n+            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Invalid credentials\");\n             slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);\n             slapi_entry_free(ec);\n             value_done(&cv);",
        "diff_line_info": {
            "deleted_lines": [
                "        slapi_send_ldap_result(pb, LDAP_NO_SUCH_OBJECT, NULL, NULL, 0, NULL);",
                "            slapi_send_ldap_result(pb, LDAP_INAPPROPRIATE_AUTH, NULL, NULL, 0, NULL);"
            ],
            "added_lines": [
                "        slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not exist\");",
                "        slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);",
                "            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Entry does not have userpassword set\");",
                "            slapi_send_ldap_result(pb, LDAP_INVALID_CREDENTIALS, NULL, NULL, 0, NULL);",
                "            slapi_pblock_set(pb, SLAPI_PB_RESULT_TEXT, \"Invalid credentials\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-20569",
        "func_name": "xen-project/xen/ibpb_calculations",
        "description": "\n\n\nA side channel vulnerability on some of the AMD CPUs may allow an attacker to influence the return address prediction. This may result in speculative execution at an attacker-controlledaddress, potentially leading to information disclosure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "git_url": "https://github.com/xen-project/xen/commit/220c06e6fefe2378f40e2a7391f5e265a2aa50f7",
        "commit_title": "x86/spec-ctrl: Mitigate Speculative Return Stack Overflow",
        "commit_text": " On native, synthesise the SRSO bits by probing various hardware properties as given by AMD.  Extend the IBPB-on-entry mitigations to Zen3/4 CPUs.  There is a microcode prerequisite to make this an effective mitigation.  This is part of XSA-434 / CVE-2023-20569 ",
        "func_before": "static void __init ibpb_calculations(void)\n{\n    bool def_ibpb_entry = false;\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n    {\n        opt_ibpb_entry_hvm = opt_ibpb_entry_pv = opt_ibpb_ctxt_switch = 0;\n        opt_ibpb_entry_dom0 = false;\n        return;\n    }\n\n    if ( boot_cpu_data.x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON) )\n    {\n        /*\n         * AMD/Hygon CPUs to date (June 2022) don't flush the RAS.  Future\n         * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n         * Until then, cover the difference with the software sequence.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n            setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n\n        /*\n         * AMD/Hygon CPUs up to and including Zen2 suffer from Branch Type\n         * Confusion.  Mitigate with IBPB-on-entry.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )\n            def_ibpb_entry = true;\n    }\n\n    if ( opt_ibpb_entry_pv == -1 )\n        opt_ibpb_entry_pv = IS_ENABLED(CONFIG_PV) && def_ibpb_entry;\n    if ( opt_ibpb_entry_hvm == -1 )\n        opt_ibpb_entry_hvm = IS_ENABLED(CONFIG_HVM) && def_ibpb_entry;\n\n    if ( opt_ibpb_entry_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_PV);\n\n        /*\n         * We only need to flush in IST context if we're protecting against PV\n         * guests.  HVM IBPB-on-entry protections are both atomic with\n         * NMI/#MC, so can't interrupt Xen ahead of having already flushed the\n         * BTB.\n         */\n        default_spec_ctrl_flags |= SCF_ist_ibpb;\n    }\n    if ( opt_ibpb_entry_hvm )\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_HVM);\n\n    /*\n     * If we're using IBPB-on-entry to protect against PV and HVM guests\n     * (ignoring dom0 if trusted), then there's no need to also issue IBPB on\n     * context switch too.\n     */\n    if ( opt_ibpb_ctxt_switch == -1 )\n        opt_ibpb_ctxt_switch = !(opt_ibpb_entry_hvm && opt_ibpb_entry_pv);\n}",
        "func": "static void __init ibpb_calculations(void)\n{\n    bool def_ibpb_entry = false;\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n    {\n        opt_ibpb_entry_hvm = opt_ibpb_entry_pv = opt_ibpb_ctxt_switch = 0;\n        opt_ibpb_entry_dom0 = false;\n        return;\n    }\n\n    if ( boot_cpu_data.x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON) )\n    {\n        /*\n         * AMD/Hygon CPUs to date (June 2022) don't flush the RAS.  Future\n         * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n         * Until then, cover the difference with the software sequence.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n            setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n\n        /*\n         * AMD/Hygon CPUs up to and including Zen2 suffer from Branch Type\n         * Confusion.  Mitigate with IBPB-on-entry.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )\n            def_ibpb_entry = true;\n\n        /*\n         * Further to BTC, Zen3/4 CPUs suffer from Speculative Return Stack\n         * Overflow in most configurations.  Mitigate with IBPB-on-entry if we\n         * have the microcode that makes this an effective option.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_SRSO_NO) &&\n             boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) )\n            def_ibpb_entry = true;\n    }\n\n    if ( opt_ibpb_entry_pv == -1 )\n        opt_ibpb_entry_pv = IS_ENABLED(CONFIG_PV) && def_ibpb_entry;\n    if ( opt_ibpb_entry_hvm == -1 )\n        opt_ibpb_entry_hvm = IS_ENABLED(CONFIG_HVM) && def_ibpb_entry;\n\n    if ( opt_ibpb_entry_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_PV);\n\n        /*\n         * We only need to flush in IST context if we're protecting against PV\n         * guests.  HVM IBPB-on-entry protections are both atomic with\n         * NMI/#MC, so can't interrupt Xen ahead of having already flushed the\n         * BTB.\n         */\n        default_spec_ctrl_flags |= SCF_ist_ibpb;\n    }\n    if ( opt_ibpb_entry_hvm )\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_HVM);\n\n    /*\n     * If we're using IBPB-on-entry to protect against PV and HVM guests\n     * (ignoring dom0 if trusted), then there's no need to also issue IBPB on\n     * context switch too.\n     */\n    if ( opt_ibpb_ctxt_switch == -1 )\n        opt_ibpb_ctxt_switch = !(opt_ibpb_entry_hvm && opt_ibpb_entry_pv);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,15 @@\n          * Confusion.  Mitigate with IBPB-on-entry.\n          */\n         if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )\n+            def_ibpb_entry = true;\n+\n+        /*\n+         * Further to BTC, Zen3/4 CPUs suffer from Speculative Return Stack\n+         * Overflow in most configurations.  Mitigate with IBPB-on-entry if we\n+         * have the microcode that makes this an effective option.\n+         */\n+        if ( !boot_cpu_has(X86_FEATURE_SRSO_NO) &&\n+             boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) )\n             def_ibpb_entry = true;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            def_ibpb_entry = true;",
                "",
                "        /*",
                "         * Further to BTC, Zen3/4 CPUs suffer from Speculative Return Stack",
                "         * Overflow in most configurations.  Mitigate with IBPB-on-entry if we",
                "         * have the microcode that makes this an effective option.",
                "         */",
                "        if ( !boot_cpu_has(X86_FEATURE_SRSO_NO) &&",
                "             boot_cpu_has(X86_FEATURE_IBPB_BRTYPE) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-20569",
        "func_name": "xen-project/xen/init_speculation_mitigations",
        "description": "\n\n\nA side channel vulnerability on some of the AMD CPUs may allow an attacker to influence the return address prediction. This may result in speculative execution at an attacker-controlledaddress, potentially leading to information disclosure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "git_url": "https://github.com/xen-project/xen/commit/220c06e6fefe2378f40e2a7391f5e265a2aa50f7",
        "commit_title": "x86/spec-ctrl: Mitigate Speculative Return Stack Overflow",
        "commit_text": " On native, synthesise the SRSO bits by probing various hardware properties as given by AMD.  Extend the IBPB-on-entry mitigations to Zen3/4 CPUs.  There is a microcode prerequisite to make this an effective mitigation.  This is part of XSA-434 / CVE-2023-20569 ",
        "func_before": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa, retpoline_safe;\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n        {\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n            add_taint(TAINT_CPU_OUT_OF_SPEC);\n        }\n        else if ( opt_ibrs == -1 )\n            opt_ibrs = ibrs = true;\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /* Determine if retpoline is safe on this CPU.  Fix up RSBA/RRSBA enumerations. */\n    retpoline_safe = retpoline_calculations();\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_sc_msr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* Support VIRT_SPEC_CTRL.SSBD if AMD_SSBD is not available. */\n    if ( opt_msr_sc_hvm && !cpu_has_amd_ssbd &&\n         (cpu_has_virt_ssbd || (amd_legacy_ssbd && amd_setup_legacy_ssbd())) )\n        amd_virt_spec_ctrl = true;\n\n    /* Figure out default_xen_spec_ctrl. */\n    if ( has_spec_ctrl && ibrs )\n    {\n        /* IBRS implies STIBP.  */\n        if ( opt_stibp == -1 )\n            opt_stibp = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n    }\n\n    /*\n     * Use STIBP by default on all AMD systems.  Zen3 and later enumerate\n     * STIBP_ALWAYS, but STIBP is needed on Zen2 as part of the mitigations\n     * for Branch Type Confusion.\n     *\n     * Leave STIBP off by default on Intel.  Pre-eIBRS systems suffer a\n     * substantial perf hit when it was implemented in microcode.\n     */\n    if ( opt_stibp == -1 )\n        opt_stibp = !!boot_cpu_has(X86_FEATURE_AMD_STIBP);\n\n    if ( opt_stibp && (boot_cpu_has(X86_FEATURE_STIBP) ||\n                       boot_cpu_has(X86_FEATURE_AMD_STIBP)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_STIBP;\n\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n    {\n        /* SSBD implies PSFD */\n        if ( opt_psfd == -1 )\n            opt_psfd = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n    }\n\n    /*\n     * Don't use PSFD by default.  AMD designed the predictor to\n     * auto-clear on privilege change.  PSFD is implied by SSBD, which is\n     * off by default.\n     */\n    if ( opt_psfd == -1 )\n        opt_psfd = 0;\n\n    if ( opt_psfd && (boot_cpu_has(X86_FEATURE_PSFD) ||\n                      boot_cpu_has(X86_FEATURE_INTEL_PSFD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_PSFD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * 4) Some CPUs have RSBs which are re-partitioned based on thread\n     *    idleness, which allows an attacker to inject entries into the other\n     *    thread.  We still active the optimisation in this case, and mitigate\n     *    in the idle path which has lower overhead.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 )\n    {\n        opt_rsb_pv = (opt_pv32 || !boot_cpu_has(X86_FEATURE_XEN_SMEP) ||\n                      !rsb_is_full_width());\n\n        /*\n         * Cross-Thread Return Address Predictions.\n         *\n         * Vulnerable systems are Zen1/Zen2 uarch, which is AMD Fam17 / Hygon\n         * Fam18, when SMT is active.\n         *\n         * To mitigate, we must flush the RSB/RAS/RAP once between entering\n         * Xen and going idle.\n         *\n         * Most cases flush on entry to Xen anyway.  The one case where we\n         * don't is when using the SMEP optimisation for PV guests.  Flushing\n         * before going idle is less overhead than flushing on PV entry.\n         */\n        if ( !opt_rsb_pv && hw_smt_enabled &&\n             (boot_cpu_data.x86_vendor & (X86_VENDOR_AMD|X86_VENDOR_HYGON)) &&\n             (boot_cpu_data.x86 == 0x17 || boot_cpu_data.x86 == 0x18) )\n            setup_force_cpu_cap(X86_FEATURE_SC_RSB_IDLE);\n    }\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n        /*\n         * For SVM, Xen's RSB safety actions are performed before STGI, so\n         * behave atomically with respect to IST sources.\n         *\n         * For VT-x, NMIs are atomic with VMExit (the NMI gets queued but not\n         * delivered) whereas other IST sources are not atomic.  Specifically,\n         * #MC can hit ahead the RSB safety action in the vmexit path.\n         *\n         * Therefore, it is necessary for the IST logic to protect Xen against\n         * possible rogue RSB speculation.\n         */\n        if ( !cpu_has_svm )\n            default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    ibpb_calculations();\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /*\n     * For microcoded IBRS only (i.e. Intel, pre eIBRS), it is recommended to\n     * clear MSR_SPEC_CTRL before going idle, to avoid impacting sibling\n     * threads.  Activate this if SMT is enabled, and Xen is using a non-zero\n     * MSR_SPEC_CTRL setting.\n     */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) && !cpu_has_eibrs &&\n         hw_smt_enabled && default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default();\n\n    l1tf_calculations();\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !cpu_has_skip_l1dfl;\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations();\n\n    /*\n     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n     * reintroduced the VERW fill buffer flushing side effect because of a\n     * susceptibility to FBSDP.\n     *\n     * If unprivileged guests have (or will have) MMIO mappings, we can\n     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n     * the return-to-guest path.\n     */\n    if ( opt_unpriv_mmio )\n        opt_fb_clear_mmio = cpu_has_fb_clear;\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n     * either the PV or HVM MDS defences are used, or if we may give MMIO\n     * access to untrusted guests.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !cpu_has_skip_l1dfl && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || cpu_has_tsx_ctrl) && cpu_has_mds_no && !cpu_has_taa_no;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && cpu_has_tsx_ctrl &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n     * data becomes available to other contexts.  To recover the data, an\n     * attacker needs to use:\n     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n     *  - DRPW (Architecturally latch stale fill buffer data)\n     *\n     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n             cpu_has_mds_no && !cpu_has_taa_no &&\n             (!cpu_has_hle || (cpu_has_tsx_ctrl && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "func": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa, retpoline_safe;\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n        {\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n            add_taint(TAINT_CPU_OUT_OF_SPEC);\n        }\n        else if ( opt_ibrs == -1 )\n            opt_ibrs = ibrs = true;\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /* Determine if retpoline is safe on this CPU.  Fix up RSBA/RRSBA enumerations. */\n    retpoline_safe = retpoline_calculations();\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_sc_msr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* Support VIRT_SPEC_CTRL.SSBD if AMD_SSBD is not available. */\n    if ( opt_msr_sc_hvm && !cpu_has_amd_ssbd &&\n         (cpu_has_virt_ssbd || (amd_legacy_ssbd && amd_setup_legacy_ssbd())) )\n        amd_virt_spec_ctrl = true;\n\n    /* Figure out default_xen_spec_ctrl. */\n    if ( has_spec_ctrl && ibrs )\n    {\n        /* IBRS implies STIBP.  */\n        if ( opt_stibp == -1 )\n            opt_stibp = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n    }\n\n    /*\n     * Use STIBP by default on all AMD systems.  Zen3 and later enumerate\n     * STIBP_ALWAYS, but STIBP is needed on Zen2 as part of the mitigations\n     * for Branch Type Confusion.\n     *\n     * Leave STIBP off by default on Intel.  Pre-eIBRS systems suffer a\n     * substantial perf hit when it was implemented in microcode.\n     */\n    if ( opt_stibp == -1 )\n        opt_stibp = !!boot_cpu_has(X86_FEATURE_AMD_STIBP);\n\n    if ( opt_stibp && (boot_cpu_has(X86_FEATURE_STIBP) ||\n                       boot_cpu_has(X86_FEATURE_AMD_STIBP)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_STIBP;\n\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n    {\n        /* SSBD implies PSFD */\n        if ( opt_psfd == -1 )\n            opt_psfd = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n    }\n\n    /*\n     * Don't use PSFD by default.  AMD designed the predictor to\n     * auto-clear on privilege change.  PSFD is implied by SSBD, which is\n     * off by default.\n     */\n    if ( opt_psfd == -1 )\n        opt_psfd = 0;\n\n    if ( opt_psfd && (boot_cpu_has(X86_FEATURE_PSFD) ||\n                      boot_cpu_has(X86_FEATURE_INTEL_PSFD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_PSFD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * 4) Some CPUs have RSBs which are re-partitioned based on thread\n     *    idleness, which allows an attacker to inject entries into the other\n     *    thread.  We still active the optimisation in this case, and mitigate\n     *    in the idle path which has lower overhead.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 )\n    {\n        opt_rsb_pv = (opt_pv32 || !boot_cpu_has(X86_FEATURE_XEN_SMEP) ||\n                      !rsb_is_full_width());\n\n        /*\n         * Cross-Thread Return Address Predictions.\n         *\n         * Vulnerable systems are Zen1/Zen2 uarch, which is AMD Fam17 / Hygon\n         * Fam18, when SMT is active.\n         *\n         * To mitigate, we must flush the RSB/RAS/RAP once between entering\n         * Xen and going idle.\n         *\n         * Most cases flush on entry to Xen anyway.  The one case where we\n         * don't is when using the SMEP optimisation for PV guests.  Flushing\n         * before going idle is less overhead than flushing on PV entry.\n         */\n        if ( !opt_rsb_pv && hw_smt_enabled &&\n             (boot_cpu_data.x86_vendor & (X86_VENDOR_AMD|X86_VENDOR_HYGON)) &&\n             (boot_cpu_data.x86 == 0x17 || boot_cpu_data.x86 == 0x18) )\n            setup_force_cpu_cap(X86_FEATURE_SC_RSB_IDLE);\n    }\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n        /*\n         * For SVM, Xen's RSB safety actions are performed before STGI, so\n         * behave atomically with respect to IST sources.\n         *\n         * For VT-x, NMIs are atomic with VMExit (the NMI gets queued but not\n         * delivered) whereas other IST sources are not atomic.  Specifically,\n         * #MC can hit ahead the RSB safety action in the vmexit path.\n         *\n         * Therefore, it is necessary for the IST logic to protect Xen against\n         * possible rogue RSB speculation.\n         */\n        if ( !cpu_has_svm )\n            default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    srso_calculations(hw_smt_enabled);\n\n    ibpb_calculations();\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /*\n     * For microcoded IBRS only (i.e. Intel, pre eIBRS), it is recommended to\n     * clear MSR_SPEC_CTRL before going idle, to avoid impacting sibling\n     * threads.  Activate this if SMT is enabled, and Xen is using a non-zero\n     * MSR_SPEC_CTRL setting.\n     */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) && !cpu_has_eibrs &&\n         hw_smt_enabled && default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default();\n\n    l1tf_calculations();\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !cpu_has_skip_l1dfl;\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations();\n\n    /*\n     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n     * reintroduced the VERW fill buffer flushing side effect because of a\n     * susceptibility to FBSDP.\n     *\n     * If unprivileged guests have (or will have) MMIO mappings, we can\n     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n     * the return-to-guest path.\n     */\n    if ( opt_unpriv_mmio )\n        opt_fb_clear_mmio = cpu_has_fb_clear;\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n     * either the PV or HVM MDS defences are used, or if we may give MMIO\n     * access to untrusted guests.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !cpu_has_skip_l1dfl && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || cpu_has_tsx_ctrl) && cpu_has_mds_no && !cpu_has_taa_no;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && cpu_has_tsx_ctrl &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n     * data becomes available to other contexts.  To recover the data, an\n     * attacker needs to use:\n     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n     *  - DRPW (Architecturally latch stale fill buffer data)\n     *\n     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n             cpu_has_mds_no && !cpu_has_taa_no &&\n             (!cpu_has_hle || (cpu_has_tsx_ctrl && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -269,6 +269,8 @@\n             default_spec_ctrl_flags |= SCF_ist_rsb;\n     }\n \n+    srso_calculations(hw_smt_enabled);\n+\n     ibpb_calculations();\n \n     /* Check whether Eager FPU should be enabled by default. */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    srso_calculations(hw_smt_enabled);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-20569",
        "func_name": "xen-project/xen/ibpb_calculations",
        "description": "\n\n\nA side channel vulnerability on some of the AMD CPUs may allow an attacker to influence the return address prediction. This may result in speculative execution at an attacker-controlledaddress, potentially leading to information disclosure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "git_url": "https://github.com/xen-project/xen/commit/292f68fb77196a35ac92b296792770d0f3190d75",
        "commit_title": "x86/spec-ctrl: Rework ibpb_calculations()",
        "commit_text": " ... in order to make the SRSO mitigations easier to integrate.   * Check for AMD/Hygon CPUs directly, rather than assuming based on IBPB.    In particular, Xen supports synthesising the IBPB bit to guests on Intel to    allow IBPB while dissuading the use of (legacy) IBRS.  * Collect def_ibpb_entry rather than opencoding the BTC_NO calculation for    both opt_ibpb_entry_{pv,hvm}.  No functional change.  This is part of XSA-434 / CVE-2023-20569 ",
        "func_before": "static void __init ibpb_calculations(void)\n{\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n    {\n        opt_ibpb_entry_hvm = opt_ibpb_entry_pv = opt_ibpb_ctxt_switch = 0;\n        opt_ibpb_entry_dom0 = false;\n        return;\n    }\n\n    /*\n     * AMD/Hygon CPUs to date (June 2022) don't flush the the RAS.  Future\n     * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n     * Until then, cover the difference with the software sequence.\n     */\n    if ( boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n        setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n\n    /*\n     * IBPB-on-entry mitigations for Branch Type Confusion.\n     *\n     * IBPB && !BTC_NO selects all AMD/Hygon hardware, not known to be safe,\n     * that we can provide some form of mitigation on.\n     */\n    if ( opt_ibpb_entry_pv == -1 )\n        opt_ibpb_entry_pv = (IS_ENABLED(CONFIG_PV) &&\n                             boot_cpu_has(X86_FEATURE_IBPB) &&\n                             !boot_cpu_has(X86_FEATURE_BTC_NO));\n    if ( opt_ibpb_entry_hvm == -1 )\n        opt_ibpb_entry_hvm = (IS_ENABLED(CONFIG_HVM) &&\n                              boot_cpu_has(X86_FEATURE_IBPB) &&\n                              !boot_cpu_has(X86_FEATURE_BTC_NO));\n\n    if ( opt_ibpb_entry_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_PV);\n\n        /*\n         * We only need to flush in IST context if we're protecting against PV\n         * guests.  HVM IBPB-on-entry protections are both atomic with\n         * NMI/#MC, so can't interrupt Xen ahead of having already flushed the\n         * BTB.\n         */\n        default_spec_ctrl_flags |= SCF_ist_ibpb;\n    }\n    if ( opt_ibpb_entry_hvm )\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_HVM);\n\n    /*\n     * If we're using IBPB-on-entry to protect against PV and HVM guests\n     * (ignoring dom0 if trusted), then there's no need to also issue IBPB on\n     * context switch too.\n     */\n    if ( opt_ibpb_ctxt_switch == -1 )\n        opt_ibpb_ctxt_switch = !(opt_ibpb_entry_hvm && opt_ibpb_entry_pv);\n}",
        "func": "static void __init ibpb_calculations(void)\n{\n    bool def_ibpb_entry = false;\n\n    /* Check we have hardware IBPB support before using it... */\n    if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n    {\n        opt_ibpb_entry_hvm = opt_ibpb_entry_pv = opt_ibpb_ctxt_switch = 0;\n        opt_ibpb_entry_dom0 = false;\n        return;\n    }\n\n    if ( boot_cpu_data.x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON) )\n    {\n        /*\n         * AMD/Hygon CPUs to date (June 2022) don't flush the RAS.  Future\n         * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n         * Until then, cover the difference with the software sequence.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n            setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n\n        /*\n         * AMD/Hygon CPUs up to and including Zen2 suffer from Branch Type\n         * Confusion.  Mitigate with IBPB-on-entry.\n         */\n        if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )\n            def_ibpb_entry = true;\n    }\n\n    if ( opt_ibpb_entry_pv == -1 )\n        opt_ibpb_entry_pv = IS_ENABLED(CONFIG_PV) && def_ibpb_entry;\n    if ( opt_ibpb_entry_hvm == -1 )\n        opt_ibpb_entry_hvm = IS_ENABLED(CONFIG_HVM) && def_ibpb_entry;\n\n    if ( opt_ibpb_entry_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_PV);\n\n        /*\n         * We only need to flush in IST context if we're protecting against PV\n         * guests.  HVM IBPB-on-entry protections are both atomic with\n         * NMI/#MC, so can't interrupt Xen ahead of having already flushed the\n         * BTB.\n         */\n        default_spec_ctrl_flags |= SCF_ist_ibpb;\n    }\n    if ( opt_ibpb_entry_hvm )\n        setup_force_cpu_cap(X86_FEATURE_IBPB_ENTRY_HVM);\n\n    /*\n     * If we're using IBPB-on-entry to protect against PV and HVM guests\n     * (ignoring dom0 if trusted), then there's no need to also issue IBPB on\n     * context switch too.\n     */\n    if ( opt_ibpb_ctxt_switch == -1 )\n        opt_ibpb_ctxt_switch = !(opt_ibpb_entry_hvm && opt_ibpb_entry_pv);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,5 +1,7 @@\n static void __init ibpb_calculations(void)\n {\n+    bool def_ibpb_entry = false;\n+\n     /* Check we have hardware IBPB support before using it... */\n     if ( !boot_cpu_has(X86_FEATURE_IBRSB) && !boot_cpu_has(X86_FEATURE_IBPB) )\n     {\n@@ -8,28 +10,28 @@\n         return;\n     }\n \n-    /*\n-     * AMD/Hygon CPUs to date (June 2022) don't flush the the RAS.  Future\n-     * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n-     * Until then, cover the difference with the software sequence.\n-     */\n-    if ( boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n-        setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n+    if ( boot_cpu_data.x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON) )\n+    {\n+        /*\n+         * AMD/Hygon CPUs to date (June 2022) don't flush the RAS.  Future\n+         * CPUs are expected to enumerate IBPB_RET when this has been fixed.\n+         * Until then, cover the difference with the software sequence.\n+         */\n+        if ( !boot_cpu_has(X86_FEATURE_IBPB_RET) )\n+            setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);\n \n-    /*\n-     * IBPB-on-entry mitigations for Branch Type Confusion.\n-     *\n-     * IBPB && !BTC_NO selects all AMD/Hygon hardware, not known to be safe,\n-     * that we can provide some form of mitigation on.\n-     */\n+        /*\n+         * AMD/Hygon CPUs up to and including Zen2 suffer from Branch Type\n+         * Confusion.  Mitigate with IBPB-on-entry.\n+         */\n+        if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )\n+            def_ibpb_entry = true;\n+    }\n+\n     if ( opt_ibpb_entry_pv == -1 )\n-        opt_ibpb_entry_pv = (IS_ENABLED(CONFIG_PV) &&\n-                             boot_cpu_has(X86_FEATURE_IBPB) &&\n-                             !boot_cpu_has(X86_FEATURE_BTC_NO));\n+        opt_ibpb_entry_pv = IS_ENABLED(CONFIG_PV) && def_ibpb_entry;\n     if ( opt_ibpb_entry_hvm == -1 )\n-        opt_ibpb_entry_hvm = (IS_ENABLED(CONFIG_HVM) &&\n-                              boot_cpu_has(X86_FEATURE_IBPB) &&\n-                              !boot_cpu_has(X86_FEATURE_BTC_NO));\n+        opt_ibpb_entry_hvm = IS_ENABLED(CONFIG_HVM) && def_ibpb_entry;\n \n     if ( opt_ibpb_entry_pv )\n     {",
        "diff_line_info": {
            "deleted_lines": [
                "    /*",
                "     * AMD/Hygon CPUs to date (June 2022) don't flush the the RAS.  Future",
                "     * CPUs are expected to enumerate IBPB_RET when this has been fixed.",
                "     * Until then, cover the difference with the software sequence.",
                "     */",
                "    if ( boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_IBPB_RET) )",
                "        setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);",
                "    /*",
                "     * IBPB-on-entry mitigations for Branch Type Confusion.",
                "     *",
                "     * IBPB && !BTC_NO selects all AMD/Hygon hardware, not known to be safe,",
                "     * that we can provide some form of mitigation on.",
                "     */",
                "        opt_ibpb_entry_pv = (IS_ENABLED(CONFIG_PV) &&",
                "                             boot_cpu_has(X86_FEATURE_IBPB) &&",
                "                             !boot_cpu_has(X86_FEATURE_BTC_NO));",
                "        opt_ibpb_entry_hvm = (IS_ENABLED(CONFIG_HVM) &&",
                "                              boot_cpu_has(X86_FEATURE_IBPB) &&",
                "                              !boot_cpu_has(X86_FEATURE_BTC_NO));"
            ],
            "added_lines": [
                "    bool def_ibpb_entry = false;",
                "",
                "    if ( boot_cpu_data.x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON) )",
                "    {",
                "        /*",
                "         * AMD/Hygon CPUs to date (June 2022) don't flush the RAS.  Future",
                "         * CPUs are expected to enumerate IBPB_RET when this has been fixed.",
                "         * Until then, cover the difference with the software sequence.",
                "         */",
                "        if ( !boot_cpu_has(X86_FEATURE_IBPB_RET) )",
                "            setup_force_cpu_cap(X86_BUG_IBPB_NO_RET);",
                "        /*",
                "         * AMD/Hygon CPUs up to and including Zen2 suffer from Branch Type",
                "         * Confusion.  Mitigate with IBPB-on-entry.",
                "         */",
                "        if ( !boot_cpu_has(X86_FEATURE_BTC_NO) )",
                "            def_ibpb_entry = true;",
                "    }",
                "",
                "        opt_ibpb_entry_pv = IS_ENABLED(CONFIG_PV) && def_ibpb_entry;",
                "        opt_ibpb_entry_hvm = IS_ENABLED(CONFIG_HVM) && def_ibpb_entry;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-20569",
        "func_name": "xen-project/xen/guest_wrmsr",
        "description": "\n\n\nA side channel vulnerability on some of the AMD CPUs may allow an attacker to influence the return address prediction. This may result in speculative execution at an attacker-controlledaddress, potentially leading to information disclosure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "git_url": "https://github.com/xen-project/xen/commit/2280b0ee2aed6e0fd4af3fa31bf99bc04d038bfe",
        "commit_title": "x86/spec-ctrl: Enumerations for Speculative Return Stack Overflow",
        "commit_text": " AMD have specified new CPUID bits relating to SRSO.   * SRSO_NO indicates that hardware is no longer vulnerable to SRSO.  * IBPB_BRTYPE indicates that IBPB flushes branch type information too.  * SBPB indicates support for a relaxed form of IBPB that does not flush    branch type information.  Current CPUs (Zen4 and older) are not expected to enumerate these bits. Native software is expected to synthesise them for guests using model and microcode revision checks.  Two are just status bits, and SBPB is trivial to support for guests by tweaking the reserved bit calculation in guest_wrmsr() and feature dependencies.  Expose all by default to guests, so they start showing up when Xen synthesises them.  While adding feature dependenies for IBPB, fix up an overlooked issue from XSA-422.  It's inappropriate to advertise that IBPB flushes RET predictions if IBPB is unavailable itself.  This is part of XSA-434 / CVE-2023-20569 ",
        "func_before": "int guest_wrmsr(struct vcpu *v, uint32_t msr, uint64_t val)\n{\n    const struct vcpu *curr = current;\n    struct domain *d = v->domain;\n    const struct cpu_policy *cp = d->arch.cpu_policy;\n    struct vcpu_msrs *msrs = v->arch.msrs;\n    int ret = X86EMUL_OKAY;\n\n    switch ( msr )\n    {\n        uint64_t rsvd;\n\n        /* Read-only */\n    case MSR_IA32_PLATFORM_ID:\n    case MSR_CORE_CAPABILITIES:\n    case MSR_INTEL_CORE_THREAD_COUNT:\n    case MSR_INTEL_PLATFORM_INFO:\n    case MSR_ARCH_CAPABILITIES:\n\n        /* Not offered to guests. */\n    case MSR_TEST_CTRL:\n    case MSR_TSX_FORCE_ABORT:\n    case MSR_TSX_CTRL:\n    case MSR_MCU_OPT_CTRL:\n    case MSR_RTIT_OUTPUT_BASE ... MSR_RTIT_ADDR_B(7):\n    case MSR_U_CET:\n    case MSR_S_CET:\n    case MSR_PL0_SSP ... MSR_INTERRUPT_SSP_TABLE:\n    case MSR_AMD64_LWP_CFG:\n    case MSR_AMD64_LWP_CBADDR:\n    case MSR_PPIN_CTL:\n    case MSR_PPIN:\n    case MSR_AMD_PPIN_CTL:\n    case MSR_AMD_PPIN:\n        goto gp_fault;\n\n    case MSR_AMD_PATCHLEVEL:\n        BUILD_BUG_ON(MSR_IA32_UCODE_REV != MSR_AMD_PATCHLEVEL);\n        /*\n         * AMD and Intel use the same MSR for the current microcode version.\n         *\n         * Both document it as read-only.  However Intel also document that,\n         * for backwards compatiblity, the OS should write 0 to it before\n         * trying to access the current microcode version.\n         */\n        if ( cp->x86_vendor != X86_VENDOR_INTEL || val != 0 )\n            goto gp_fault;\n        break;\n\n    case MSR_AMD_PATCHLOADER:\n        /*\n         * See note on MSR_IA32_UCODE_WRITE below, which may or may not apply\n         * to AMD CPUs as well (at least the architectural/CPUID part does).\n         */\n        if ( is_pv_domain(d) ||\n             cp->x86_vendor != X86_VENDOR_AMD )\n            goto gp_fault;\n        break;\n\n    case MSR_IA32_UCODE_WRITE:\n        /*\n         * Some versions of Windows at least on certain hardware try to load\n         * microcode before setting up an IDT. Therefore we must not inject #GP\n         * for such attempts. Also the MSR is architectural and not qualified\n         * by any CPUID bit.\n         */\n        if ( is_pv_domain(d) ||\n             cp->x86_vendor != X86_VENDOR_INTEL )\n            goto gp_fault;\n        break;\n\n    case MSR_SPEC_CTRL:\n        if ( (!cp->feat.ibrsb && !cp->extd.ibrs) ||\n             (val & ~msr_spec_ctrl_valid_bits(cp)) )\n            goto gp_fault;\n        goto set_reg;\n\n    case MSR_PRED_CMD:\n        if ( !cp->feat.ibrsb && !cp->extd.ibpb )\n            goto gp_fault; /* MSR available? */\n\n        if ( val & ~PRED_CMD_IBPB )\n            goto gp_fault; /* Rsvd bit set? */\n\n        if ( v == curr )\n            wrmsrl(MSR_PRED_CMD, val);\n        break;\n\n    case MSR_FLUSH_CMD:\n        if ( !cp->feat.l1d_flush )\n            goto gp_fault; /* MSR available? */\n\n        if ( val & ~FLUSH_CMD_L1D )\n            goto gp_fault; /* Rsvd bit set? */\n\n        if ( v == curr )\n            wrmsrl(MSR_FLUSH_CMD, val);\n        break;\n\n    case MSR_INTEL_MISC_FEATURES_ENABLES:\n    {\n        bool old_cpuid_faulting = msrs->misc_features_enables.cpuid_faulting;\n\n        rsvd = ~0ULL;\n        if ( cp->platform_info.cpuid_faulting )\n            rsvd &= ~MSR_MISC_FEATURES_CPUID_FAULTING;\n\n        if ( val & rsvd )\n            goto gp_fault;\n\n        msrs->misc_features_enables.raw = val;\n\n        if ( v == curr && is_hvm_domain(d) && cpu_has_cpuid_faulting &&\n             (old_cpuid_faulting ^ msrs->misc_features_enables.cpuid_faulting) )\n            ctxt_switch_levelling(v);\n        break;\n    }\n\n    case MSR_IA32_MCG_CAP     ... MSR_IA32_MCG_CTL:      /* 0x179 -> 0x17b */\n    case MSR_IA32_MCx_CTL2(0) ... MSR_IA32_MCx_CTL2(31): /* 0x280 -> 0x29f */\n    case MSR_IA32_MCx_CTL(0)  ... MSR_IA32_MCx_MISC(31): /* 0x400 -> 0x47f */\n    case MSR_IA32_MCG_EXT_CTL:                           /* 0x4d0 */\n        if ( vmce_wrmsr(msr, val) < 0 )\n            goto gp_fault;\n        break;\n\n        /*\n         * This MSR is not enumerated in CPUID.  It has been around since the\n         * Pentium 4, and implemented by other vendors.\n         *\n         * To match the RAZ semantics, implement as write-discard, except for\n         * a cpufreq controller dom0 which has full access.\n         */\n    case MSR_IA32_PERF_CTL:\n        if ( !(cp->x86_vendor & (X86_VENDOR_INTEL | X86_VENDOR_CENTAUR)) )\n            goto gp_fault;\n\n        if ( likely(!is_cpufreq_controller(d)) || wrmsr_safe(msr, val) == 0 )\n            break;\n        goto gp_fault;\n\n    case MSR_PKRS:\n        if ( !cp->feat.pks || val != (uint32_t)val )\n            goto gp_fault;\n        goto set_reg;\n\n    case MSR_X2APIC_FIRST ... MSR_X2APIC_LAST:\n        if ( !is_hvm_domain(d) || v != curr )\n            goto gp_fault;\n\n        ret = guest_wrmsr_x2apic(v, msr, val);\n        break;\n\n#ifdef CONFIG_HVM\n    case MSR_IA32_BNDCFGS:\n        if ( !cp->feat.mpx || /* Implies Intel HVM only */\n             !is_canonical_address(val) || (val & IA32_BNDCFGS_RESERVED) )\n            goto gp_fault;\n\n        /*\n         * While MPX instructions are supposed to be gated on XCR0.BND*, let's\n         * nevertheless force the relevant XCR0 bits on when the feature is\n         * being enabled in BNDCFGS.\n         */\n        if ( (val & IA32_BNDCFGS_ENABLE) &&\n             !(v->arch.xcr0_accum & (X86_XCR0_BNDREGS | X86_XCR0_BNDCSR)) )\n        {\n            uint64_t xcr0 = get_xcr0();\n\n            if ( v != curr ||\n                 handle_xsetbv(XCR_XFEATURE_ENABLED_MASK,\n                               xcr0 | X86_XCR0_BNDREGS | X86_XCR0_BNDCSR) )\n                goto gp_fault;\n\n            if ( handle_xsetbv(XCR_XFEATURE_ENABLED_MASK, xcr0) )\n                /* nothing, best effort only */;\n        }\n\n        goto set_reg;\n#endif /* CONFIG_HVM */\n\n    case MSR_IA32_XSS:\n        if ( !cp->xstate.xsaves )\n            goto gp_fault;\n\n        /* No XSS features currently supported for guests */\n        if ( val != 0 )\n            goto gp_fault;\n\n        msrs->xss.raw = val;\n        break;\n\n    case 0x40000000 ... 0x400001ff:\n        if ( is_viridian_domain(d) )\n        {\n            ret = guest_wrmsr_viridian(v, msr, val);\n            break;\n        }\n\n        /* Fallthrough. */\n    case 0x40000200 ... 0x400002ff:\n        ret = guest_wrmsr_xen(v, msr, val);\n        break;\n\n    case MSR_TSC_AUX:\n        if ( !cp->extd.rdtscp && !cp->feat.rdpid )\n            goto gp_fault;\n        if ( val != (uint32_t)val )\n            goto gp_fault;\n\n        msrs->tsc_aux = val;\n        if ( v == curr )\n            wrmsr_tsc_aux(val);\n        break;\n\n    case MSR_VIRT_SPEC_CTRL:\n        if ( !cp->extd.virt_ssbd )\n            goto gp_fault;\n\n        /* Only supports SSBD bit, the rest are ignored. */\n        if ( cpu_has_amd_ssbd )\n        {\n            if ( val & SPEC_CTRL_SSBD )\n                msrs->spec_ctrl.raw |= SPEC_CTRL_SSBD;\n            else\n                msrs->spec_ctrl.raw &= ~SPEC_CTRL_SSBD;\n        }\n        else\n        {\n            msrs->virt_spec_ctrl.raw = val & SPEC_CTRL_SSBD;\n            if ( v == curr )\n                /*\n                 * Propagate the value to hardware, as it won't be set on guest\n                 * resume path.\n                 */\n                amd_set_legacy_ssbd(val & SPEC_CTRL_SSBD);\n        }\n        break;\n\n    case MSR_AMD64_DE_CFG:\n        /*\n         * OpenBSD 6.7 will panic if writing to DE_CFG triggers a #GP:\n         * https://www.illumos.org/issues/12998 - drop writes.\n         */\n        if ( !(cp->x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON)) )\n            goto gp_fault;\n        break;\n\n    case MSR_AMD64_DR0_ADDRESS_MASK:\n    case MSR_AMD64_DR1_ADDRESS_MASK ... MSR_AMD64_DR3_ADDRESS_MASK:\n        if ( !cp->extd.dbext || val != (uint32_t)val )\n            goto gp_fault;\n\n        msrs->dr_mask[\n            array_index_nospec((msr == MSR_AMD64_DR0_ADDRESS_MASK)\n                               ? 0 : (msr - MSR_AMD64_DR1_ADDRESS_MASK + 1),\n                               ARRAY_SIZE(msrs->dr_mask))] = val;\n\n        if ( v == curr && (curr->arch.dr7 & DR7_ACTIVE_MASK) )\n            wrmsrl(msr, val);\n        break;\n\n    default:\n        return X86EMUL_UNHANDLEABLE;\n    }\n\n    /*\n     * Interim safety check that functions we dispatch to don't alias \"Not yet\n     * handled by the new MSR infrastructure\".\n     */\n    ASSERT(ret != X86EMUL_UNHANDLEABLE);\n\n    return ret;\n\n set_reg: /* Delegate register access to per-vm-type logic. */\n    if ( is_pv_domain(d) )\n        pv_set_reg(v, msr, val);\n    else\n        hvm_set_reg(v, msr, val);\n    return X86EMUL_OKAY;\n\n gp_fault:\n    return X86EMUL_EXCEPTION;\n}",
        "func": "int guest_wrmsr(struct vcpu *v, uint32_t msr, uint64_t val)\n{\n    const struct vcpu *curr = current;\n    struct domain *d = v->domain;\n    const struct cpu_policy *cp = d->arch.cpu_policy;\n    struct vcpu_msrs *msrs = v->arch.msrs;\n    int ret = X86EMUL_OKAY;\n\n    switch ( msr )\n    {\n        uint64_t rsvd;\n\n        /* Read-only */\n    case MSR_IA32_PLATFORM_ID:\n    case MSR_CORE_CAPABILITIES:\n    case MSR_INTEL_CORE_THREAD_COUNT:\n    case MSR_INTEL_PLATFORM_INFO:\n    case MSR_ARCH_CAPABILITIES:\n\n        /* Not offered to guests. */\n    case MSR_TEST_CTRL:\n    case MSR_TSX_FORCE_ABORT:\n    case MSR_TSX_CTRL:\n    case MSR_MCU_OPT_CTRL:\n    case MSR_RTIT_OUTPUT_BASE ... MSR_RTIT_ADDR_B(7):\n    case MSR_U_CET:\n    case MSR_S_CET:\n    case MSR_PL0_SSP ... MSR_INTERRUPT_SSP_TABLE:\n    case MSR_AMD64_LWP_CFG:\n    case MSR_AMD64_LWP_CBADDR:\n    case MSR_PPIN_CTL:\n    case MSR_PPIN:\n    case MSR_AMD_PPIN_CTL:\n    case MSR_AMD_PPIN:\n        goto gp_fault;\n\n    case MSR_AMD_PATCHLEVEL:\n        BUILD_BUG_ON(MSR_IA32_UCODE_REV != MSR_AMD_PATCHLEVEL);\n        /*\n         * AMD and Intel use the same MSR for the current microcode version.\n         *\n         * Both document it as read-only.  However Intel also document that,\n         * for backwards compatiblity, the OS should write 0 to it before\n         * trying to access the current microcode version.\n         */\n        if ( cp->x86_vendor != X86_VENDOR_INTEL || val != 0 )\n            goto gp_fault;\n        break;\n\n    case MSR_AMD_PATCHLOADER:\n        /*\n         * See note on MSR_IA32_UCODE_WRITE below, which may or may not apply\n         * to AMD CPUs as well (at least the architectural/CPUID part does).\n         */\n        if ( is_pv_domain(d) ||\n             cp->x86_vendor != X86_VENDOR_AMD )\n            goto gp_fault;\n        break;\n\n    case MSR_IA32_UCODE_WRITE:\n        /*\n         * Some versions of Windows at least on certain hardware try to load\n         * microcode before setting up an IDT. Therefore we must not inject #GP\n         * for such attempts. Also the MSR is architectural and not qualified\n         * by any CPUID bit.\n         */\n        if ( is_pv_domain(d) ||\n             cp->x86_vendor != X86_VENDOR_INTEL )\n            goto gp_fault;\n        break;\n\n    case MSR_SPEC_CTRL:\n        if ( (!cp->feat.ibrsb && !cp->extd.ibrs) ||\n             (val & ~msr_spec_ctrl_valid_bits(cp)) )\n            goto gp_fault;\n        goto set_reg;\n\n    case MSR_PRED_CMD:\n        if ( !cp->feat.ibrsb && !cp->extd.ibpb )\n            goto gp_fault; /* MSR available? */\n\n        rsvd = ~(PRED_CMD_IBPB |\n                 (cp->extd.sbpb ? PRED_CMD_SBPB : 0));\n\n        if ( val & rsvd )\n            goto gp_fault; /* Rsvd bit set? */\n\n        if ( v == curr )\n            wrmsrl(MSR_PRED_CMD, val);\n        break;\n\n    case MSR_FLUSH_CMD:\n        if ( !cp->feat.l1d_flush )\n            goto gp_fault; /* MSR available? */\n\n        if ( val & ~FLUSH_CMD_L1D )\n            goto gp_fault; /* Rsvd bit set? */\n\n        if ( v == curr )\n            wrmsrl(MSR_FLUSH_CMD, val);\n        break;\n\n    case MSR_INTEL_MISC_FEATURES_ENABLES:\n    {\n        bool old_cpuid_faulting = msrs->misc_features_enables.cpuid_faulting;\n\n        rsvd = ~0ULL;\n        if ( cp->platform_info.cpuid_faulting )\n            rsvd &= ~MSR_MISC_FEATURES_CPUID_FAULTING;\n\n        if ( val & rsvd )\n            goto gp_fault;\n\n        msrs->misc_features_enables.raw = val;\n\n        if ( v == curr && is_hvm_domain(d) && cpu_has_cpuid_faulting &&\n             (old_cpuid_faulting ^ msrs->misc_features_enables.cpuid_faulting) )\n            ctxt_switch_levelling(v);\n        break;\n    }\n\n    case MSR_IA32_MCG_CAP     ... MSR_IA32_MCG_CTL:      /* 0x179 -> 0x17b */\n    case MSR_IA32_MCx_CTL2(0) ... MSR_IA32_MCx_CTL2(31): /* 0x280 -> 0x29f */\n    case MSR_IA32_MCx_CTL(0)  ... MSR_IA32_MCx_MISC(31): /* 0x400 -> 0x47f */\n    case MSR_IA32_MCG_EXT_CTL:                           /* 0x4d0 */\n        if ( vmce_wrmsr(msr, val) < 0 )\n            goto gp_fault;\n        break;\n\n        /*\n         * This MSR is not enumerated in CPUID.  It has been around since the\n         * Pentium 4, and implemented by other vendors.\n         *\n         * To match the RAZ semantics, implement as write-discard, except for\n         * a cpufreq controller dom0 which has full access.\n         */\n    case MSR_IA32_PERF_CTL:\n        if ( !(cp->x86_vendor & (X86_VENDOR_INTEL | X86_VENDOR_CENTAUR)) )\n            goto gp_fault;\n\n        if ( likely(!is_cpufreq_controller(d)) || wrmsr_safe(msr, val) == 0 )\n            break;\n        goto gp_fault;\n\n    case MSR_PKRS:\n        if ( !cp->feat.pks || val != (uint32_t)val )\n            goto gp_fault;\n        goto set_reg;\n\n    case MSR_X2APIC_FIRST ... MSR_X2APIC_LAST:\n        if ( !is_hvm_domain(d) || v != curr )\n            goto gp_fault;\n\n        ret = guest_wrmsr_x2apic(v, msr, val);\n        break;\n\n#ifdef CONFIG_HVM\n    case MSR_IA32_BNDCFGS:\n        if ( !cp->feat.mpx || /* Implies Intel HVM only */\n             !is_canonical_address(val) || (val & IA32_BNDCFGS_RESERVED) )\n            goto gp_fault;\n\n        /*\n         * While MPX instructions are supposed to be gated on XCR0.BND*, let's\n         * nevertheless force the relevant XCR0 bits on when the feature is\n         * being enabled in BNDCFGS.\n         */\n        if ( (val & IA32_BNDCFGS_ENABLE) &&\n             !(v->arch.xcr0_accum & (X86_XCR0_BNDREGS | X86_XCR0_BNDCSR)) )\n        {\n            uint64_t xcr0 = get_xcr0();\n\n            if ( v != curr ||\n                 handle_xsetbv(XCR_XFEATURE_ENABLED_MASK,\n                               xcr0 | X86_XCR0_BNDREGS | X86_XCR0_BNDCSR) )\n                goto gp_fault;\n\n            if ( handle_xsetbv(XCR_XFEATURE_ENABLED_MASK, xcr0) )\n                /* nothing, best effort only */;\n        }\n\n        goto set_reg;\n#endif /* CONFIG_HVM */\n\n    case MSR_IA32_XSS:\n        if ( !cp->xstate.xsaves )\n            goto gp_fault;\n\n        /* No XSS features currently supported for guests */\n        if ( val != 0 )\n            goto gp_fault;\n\n        msrs->xss.raw = val;\n        break;\n\n    case 0x40000000 ... 0x400001ff:\n        if ( is_viridian_domain(d) )\n        {\n            ret = guest_wrmsr_viridian(v, msr, val);\n            break;\n        }\n\n        /* Fallthrough. */\n    case 0x40000200 ... 0x400002ff:\n        ret = guest_wrmsr_xen(v, msr, val);\n        break;\n\n    case MSR_TSC_AUX:\n        if ( !cp->extd.rdtscp && !cp->feat.rdpid )\n            goto gp_fault;\n        if ( val != (uint32_t)val )\n            goto gp_fault;\n\n        msrs->tsc_aux = val;\n        if ( v == curr )\n            wrmsr_tsc_aux(val);\n        break;\n\n    case MSR_VIRT_SPEC_CTRL:\n        if ( !cp->extd.virt_ssbd )\n            goto gp_fault;\n\n        /* Only supports SSBD bit, the rest are ignored. */\n        if ( cpu_has_amd_ssbd )\n        {\n            if ( val & SPEC_CTRL_SSBD )\n                msrs->spec_ctrl.raw |= SPEC_CTRL_SSBD;\n            else\n                msrs->spec_ctrl.raw &= ~SPEC_CTRL_SSBD;\n        }\n        else\n        {\n            msrs->virt_spec_ctrl.raw = val & SPEC_CTRL_SSBD;\n            if ( v == curr )\n                /*\n                 * Propagate the value to hardware, as it won't be set on guest\n                 * resume path.\n                 */\n                amd_set_legacy_ssbd(val & SPEC_CTRL_SSBD);\n        }\n        break;\n\n    case MSR_AMD64_DE_CFG:\n        /*\n         * OpenBSD 6.7 will panic if writing to DE_CFG triggers a #GP:\n         * https://www.illumos.org/issues/12998 - drop writes.\n         */\n        if ( !(cp->x86_vendor & (X86_VENDOR_AMD | X86_VENDOR_HYGON)) )\n            goto gp_fault;\n        break;\n\n    case MSR_AMD64_DR0_ADDRESS_MASK:\n    case MSR_AMD64_DR1_ADDRESS_MASK ... MSR_AMD64_DR3_ADDRESS_MASK:\n        if ( !cp->extd.dbext || val != (uint32_t)val )\n            goto gp_fault;\n\n        msrs->dr_mask[\n            array_index_nospec((msr == MSR_AMD64_DR0_ADDRESS_MASK)\n                               ? 0 : (msr - MSR_AMD64_DR1_ADDRESS_MASK + 1),\n                               ARRAY_SIZE(msrs->dr_mask))] = val;\n\n        if ( v == curr && (curr->arch.dr7 & DR7_ACTIVE_MASK) )\n            wrmsrl(msr, val);\n        break;\n\n    default:\n        return X86EMUL_UNHANDLEABLE;\n    }\n\n    /*\n     * Interim safety check that functions we dispatch to don't alias \"Not yet\n     * handled by the new MSR infrastructure\".\n     */\n    ASSERT(ret != X86EMUL_UNHANDLEABLE);\n\n    return ret;\n\n set_reg: /* Delegate register access to per-vm-type logic. */\n    if ( is_pv_domain(d) )\n        pv_set_reg(v, msr, val);\n    else\n        hvm_set_reg(v, msr, val);\n    return X86EMUL_OKAY;\n\n gp_fault:\n    return X86EMUL_EXCEPTION;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -79,7 +79,10 @@\n         if ( !cp->feat.ibrsb && !cp->extd.ibpb )\n             goto gp_fault; /* MSR available? */\n \n-        if ( val & ~PRED_CMD_IBPB )\n+        rsvd = ~(PRED_CMD_IBPB |\n+                 (cp->extd.sbpb ? PRED_CMD_SBPB : 0));\n+\n+        if ( val & rsvd )\n             goto gp_fault; /* Rsvd bit set? */\n \n         if ( v == curr )",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( val & ~PRED_CMD_IBPB )"
            ],
            "added_lines": [
                "        rsvd = ~(PRED_CMD_IBPB |",
                "                 (cp->extd.sbpb ? PRED_CMD_SBPB : 0));",
                "",
                "        if ( val & rsvd )"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-20569",
        "func_name": "xen-project/xen/print_details",
        "description": "\n\n\nA side channel vulnerability on some of the AMD CPUs may allow an attacker to influence the return address prediction. This may result in speculative execution at an attacker-controlledaddress, potentially leading to information disclosure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
        "git_url": "https://github.com/xen-project/xen/commit/2280b0ee2aed6e0fd4af3fa31bf99bc04d038bfe",
        "commit_title": "x86/spec-ctrl: Enumerations for Speculative Return Stack Overflow",
        "commit_text": " AMD have specified new CPUID bits relating to SRSO.   * SRSO_NO indicates that hardware is no longer vulnerable to SRSO.  * IBPB_BRTYPE indicates that IBPB flushes branch type information too.  * SBPB indicates support for a relaxed form of IBPB that does not flush    branch type information.  Current CPUs (Zen4 and older) are not expected to enumerate these bits. Native software is expected to synthesise them for guests using model and microcode revision checks.  Two are just status bits, and SBPB is trivial to support for guests by tweaking the reserved bit calculation in guest_wrmsr() and feature dependencies.  Expose all by default to guests, so they start showing up when Xen synthesises them.  While adding feature dependenies for IBPB, fix up an overlooked issue from XSA-422.  It's inappropriate to advertise that IBPB flushes RET predictions if IBPB is unavailable itself.  This is part of XSA-434 / CVE-2023-20569 ",
        "func_before": "static void __init print_details(enum ind_thunk thunk)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n    uint64_t caps = 0;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_RRSBA)                          ? \" RRSBA\"          : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_PBRSB_NO)                       ? \" PBRSB_NO\"       : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_BTC_NO))         ? \" BTC_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           (!boot_cpu_has(X86_FEATURE_PSFD) &&\n            !boot_cpu_has(X86_FEATURE_INTEL_PSFD))   ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_PSFD)  ? \" PSFD+\" : \" PSFD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb_ctxt_switch                      ? \" IBPB-ctxt\" : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm ||\n           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM) ||\n            amd_virt_spec_ctrl ||\n            opt_eager_fpu || opt_md_clear_hvm)       ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            amd_virt_spec_ctrl)                      ? \" MSR_VIRT_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_hvm                          ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM)  ? \" IBPB-entry\"    : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV) ||\n            opt_eager_fpu || opt_md_clear_pv)        ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_pv                           ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV)   ? \" IBPB-entry\"    : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "func": "static void __init print_details(enum ind_thunk thunk)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, e21a = 0, max = 0, tmp;\n    uint64_t caps = 0;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000021 )\n        cpuid(0x80000021, &e21a, &tmp, &tmp, &tmp);\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_RRSBA)                          ? \" RRSBA\"          : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_PBRSB_NO)                       ? \" PBRSB_NO\"       : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_BTC_NO))         ? \" BTC_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_IBPB_BRTYPE))    ? \" IBPB_BRTYPE\"    : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           (!boot_cpu_has(X86_FEATURE_PSFD) &&\n            !boot_cpu_has(X86_FEATURE_INTEL_PSFD))   ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_PSFD)  ? \" PSFD+\" : \" PSFD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb_ctxt_switch                      ? \" IBPB-ctxt\" : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm ||\n           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM) ||\n            amd_virt_spec_ctrl ||\n            opt_eager_fpu || opt_md_clear_hvm)       ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            amd_virt_spec_ctrl)                      ? \" MSR_VIRT_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_hvm                          ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM)  ? \" IBPB-entry\"    : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV) ||\n            opt_eager_fpu || opt_md_clear_pv)        ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_pv                           ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV)   ? \" IBPB-entry\"    : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,6 @@\n static void __init print_details(enum ind_thunk thunk)\n {\n-    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;\n+    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, e21a = 0, max = 0, tmp;\n     uint64_t caps = 0;\n \n     /* Collect diagnostics about available mitigations. */\n@@ -10,6 +10,8 @@\n         cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n     if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n         cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n+    if ( boot_cpu_data.extended_cpuid_level >= 0x80000021 )\n+        cpuid(0x80000021, &e21a, &tmp, &tmp, &tmp);\n     if ( cpu_has_arch_caps )\n         rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n \n@@ -19,7 +21,7 @@\n      * Hardware read-only information, stating immunity to certain issues, or\n      * suggestions of which mitigation to use.\n      */\n-    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n            (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n            (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n@@ -39,10 +41,12 @@\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_BTC_NO))         ? \" BTC_NO\"         : \"\",\n-           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\");\n+           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\",\n+           (e21a & cpufeat_mask(X86_FEATURE_IBPB_BRTYPE))    ? \" IBPB_BRTYPE\"    : \"\",\n+           (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");\n \n     /* Hardware features which need driving to mitigate issues. */\n-    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n            (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n@@ -58,7 +62,8 @@\n            (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n            (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n-           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");\n+           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",\n+           (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");\n \n     /* Compiled-in support which pertains to mitigations. */\n     if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )",
        "diff_line_info": {
            "deleted_lines": [
                "    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, max = 0, tmp;",
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\");",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\");"
            ],
            "added_lines": [
                "    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, e21a = 0, max = 0, tmp;",
                "    if ( boot_cpu_data.extended_cpuid_level >= 0x80000021 )",
                "        cpuid(0x80000021, &e21a, &tmp, &tmp, &tmp);",
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\",",
                "           (e21a & cpufeat_mask(X86_FEATURE_IBPB_BRTYPE))    ? \" IBPB_BRTYPE\"    : \"\",",
                "           (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",",
                "           (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/guest_common_max_feature_adjustments",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/2dd06b4ea10891750af38e4a0e1efaeb0a9b3518",
        "commit_title": "x86/cpu-policy: Hide CLWB by default on SKX/CLX/CPX",
        "commit_text": " The August 2023 microcode for GDS has an impact on the CLWB instruction.  See code comments for full details.  This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "static void __init guest_common_max_feature_adjustments(uint32_t *fs)\n{\n    if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )\n    {\n        /*\n         * MSR_ARCH_CAPS is just feature data, and we can offer it to guests\n         * unconditionally, although limit it to Intel systems as it is highly\n         * uarch-specific.\n         *\n         * In particular, the RSBA and RRSBA bits mean \"you might migrate to a\n         * system where RSB underflow uses alternative predictors (a.k.a\n         * Retpoline not safe)\", so these need to be visible to a guest in all\n         * cases, even when it's only some other server in the pool which\n         * suffers the identified behaviour.\n         *\n         * We can always run any VM which has previously (or will\n         * subsequently) run on hardware where Retpoline is not safe.\n         * Note:\n         *  - The dependency logic may hide RRSBA for other reasons.\n         *  - The max policy does not constitute a sensible configuration to\n         *    run a guest in.\n         */\n        __set_bit(X86_FEATURE_ARCH_CAPS, fs);\n        __set_bit(X86_FEATURE_RSBA, fs);\n        __set_bit(X86_FEATURE_RRSBA, fs);\n    }\n}",
        "func": "static void __init guest_common_max_feature_adjustments(uint32_t *fs)\n{\n    if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )\n    {\n        /*\n         * MSR_ARCH_CAPS is just feature data, and we can offer it to guests\n         * unconditionally, although limit it to Intel systems as it is highly\n         * uarch-specific.\n         *\n         * In particular, the RSBA and RRSBA bits mean \"you might migrate to a\n         * system where RSB underflow uses alternative predictors (a.k.a\n         * Retpoline not safe)\", so these need to be visible to a guest in all\n         * cases, even when it's only some other server in the pool which\n         * suffers the identified behaviour.\n         *\n         * We can always run any VM which has previously (or will\n         * subsequently) run on hardware where Retpoline is not safe.\n         * Note:\n         *  - The dependency logic may hide RRSBA for other reasons.\n         *  - The max policy does not constitute a sensible configuration to\n         *    run a guest in.\n         */\n        __set_bit(X86_FEATURE_ARCH_CAPS, fs);\n        __set_bit(X86_FEATURE_RSBA, fs);\n        __set_bit(X86_FEATURE_RRSBA, fs);\n\n        /*\n         * The Gather Data Sampling microcode mitigation (August 2023) has an\n         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n         *\n         * We hid CLWB in the host policy to stop Xen using it, but VMs which\n         * have previously seen the CLWB feature can safely run on this CPU.\n         */\n        if ( boot_cpu_data.x86 == 6 &&\n             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&\n             raw_cpu_policy.feat.clwb )\n            __set_bit(X86_FEATURE_CLWB, fs);\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -23,5 +23,17 @@\n         __set_bit(X86_FEATURE_ARCH_CAPS, fs);\n         __set_bit(X86_FEATURE_RSBA, fs);\n         __set_bit(X86_FEATURE_RRSBA, fs);\n+\n+        /*\n+         * The Gather Data Sampling microcode mitigation (August 2023) has an\n+         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n+         *\n+         * We hid CLWB in the host policy to stop Xen using it, but VMs which\n+         * have previously seen the CLWB feature can safely run on this CPU.\n+         */\n+        if ( boot_cpu_data.x86 == 6 &&\n+             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&\n+             raw_cpu_policy.feat.clwb )\n+            __set_bit(X86_FEATURE_CLWB, fs);\n     }\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "        /*",
                "         * The Gather Data Sampling microcode mitigation (August 2023) has an",
                "         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.",
                "         *",
                "         * We hid CLWB in the host policy to stop Xen using it, but VMs which",
                "         * have previously seen the CLWB feature can safely run on this CPU.",
                "         */",
                "        if ( boot_cpu_data.x86 == 6 &&",
                "             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&",
                "             raw_cpu_policy.feat.clwb )",
                "            __set_bit(X86_FEATURE_CLWB, fs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/guest_common_default_feature_adjustments",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/2dd06b4ea10891750af38e4a0e1efaeb0a9b3518",
        "commit_title": "x86/cpu-policy: Hide CLWB by default on SKX/CLX/CPX",
        "commit_text": " The August 2023 microcode for GDS has an impact on the CLWB instruction.  See code comments for full details.  This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "static void __init guest_common_default_feature_adjustments(uint32_t *fs)\n{\n    if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )\n    {\n        /*\n         * IvyBridge client parts suffer from leakage of RDRAND data due to SRBDS\n         * (XSA-320 / CVE-2020-0543), and won't be receiving microcode to\n         * compensate.\n         *\n         * Mitigate by hiding RDRAND from guests by default, unless explicitly\n         * overridden on the Xen command line (cpuid=rdrand).  Irrespective of the\n         * default setting, guests can use RDRAND if explicitly enabled\n         * (cpuid=\"host,rdrand=1\") in the VM's config file, and VMs which were\n         * previously using RDRAND can migrate in.\n         */\n        if ( boot_cpu_data.x86 == 6 &&\n             boot_cpu_data.x86_model == INTEL_FAM6_IVYBRIDGE &&\n             cpu_has_rdrand && !is_forced_cpu_cap(X86_FEATURE_RDRAND) )\n            __clear_bit(X86_FEATURE_RDRAND, fs);\n    }\n\n    /*\n     * On certain hardware, speculative or errata workarounds can result in\n     * TSX being placed in \"force-abort\" mode, where it doesn't actually\n     * function as expected, but is technically compatible with the ISA.\n     *\n     * Do not advertise RTM to guests by default if it won't actually work.\n     */\n    if ( rtm_disabled )\n        __clear_bit(X86_FEATURE_RTM, fs);\n}",
        "func": "static void __init guest_common_default_feature_adjustments(uint32_t *fs)\n{\n    if ( boot_cpu_data.x86_vendor == X86_VENDOR_INTEL )\n    {\n        /*\n         * IvyBridge client parts suffer from leakage of RDRAND data due to SRBDS\n         * (XSA-320 / CVE-2020-0543), and won't be receiving microcode to\n         * compensate.\n         *\n         * Mitigate by hiding RDRAND from guests by default, unless explicitly\n         * overridden on the Xen command line (cpuid=rdrand).  Irrespective of the\n         * default setting, guests can use RDRAND if explicitly enabled\n         * (cpuid=\"host,rdrand=1\") in the VM's config file, and VMs which were\n         * previously using RDRAND can migrate in.\n         */\n        if ( boot_cpu_data.x86 == 6 &&\n             boot_cpu_data.x86_model == INTEL_FAM6_IVYBRIDGE &&\n             cpu_has_rdrand && !is_forced_cpu_cap(X86_FEATURE_RDRAND) )\n            __clear_bit(X86_FEATURE_RDRAND, fs);\n\n        /*\n         * The Gather Data Sampling microcode mitigation (August 2023) has an\n         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n         *\n         * We hid CLWB in the host policy to stop Xen using it, but re-added\n         * it to the max policy to let VMs migrate in.  Re-hide it in the\n         * default policy to disuade VMs from using it in the common case.\n         */\n        if ( boot_cpu_data.x86 == 6 &&\n             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&\n             raw_cpu_policy.feat.clwb )\n            __clear_bit(X86_FEATURE_CLWB, fs);\n    }\n\n    /*\n     * On certain hardware, speculative or errata workarounds can result in\n     * TSX being placed in \"force-abort\" mode, where it doesn't actually\n     * function as expected, but is technically compatible with the ISA.\n     *\n     * Do not advertise RTM to guests by default if it won't actually work.\n     */\n    if ( rtm_disabled )\n        __clear_bit(X86_FEATURE_RTM, fs);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,6 +17,19 @@\n              boot_cpu_data.x86_model == INTEL_FAM6_IVYBRIDGE &&\n              cpu_has_rdrand && !is_forced_cpu_cap(X86_FEATURE_RDRAND) )\n             __clear_bit(X86_FEATURE_RDRAND, fs);\n+\n+        /*\n+         * The Gather Data Sampling microcode mitigation (August 2023) has an\n+         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n+         *\n+         * We hid CLWB in the host policy to stop Xen using it, but re-added\n+         * it to the max policy to let VMs migrate in.  Re-hide it in the\n+         * default policy to disuade VMs from using it in the common case.\n+         */\n+        if ( boot_cpu_data.x86 == 6 &&\n+             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&\n+             raw_cpu_policy.feat.clwb )\n+            __clear_bit(X86_FEATURE_CLWB, fs);\n     }\n \n     /*",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "        /*",
                "         * The Gather Data Sampling microcode mitigation (August 2023) has an",
                "         * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.",
                "         *",
                "         * We hid CLWB in the host policy to stop Xen using it, but re-added",
                "         * it to the max policy to let VMs migrate in.  Re-hide it in the",
                "         * default policy to disuade VMs from using it in the common case.",
                "         */",
                "        if ( boot_cpu_data.x86 == 6 &&",
                "             boot_cpu_data.x86_model == INTEL_FAM6_SKYLAKE_X &&",
                "             raw_cpu_policy.feat.clwb )",
                "            __clear_bit(X86_FEATURE_CLWB, fs);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/init_intel",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/2dd06b4ea10891750af38e4a0e1efaeb0a9b3518",
        "commit_title": "x86/cpu-policy: Hide CLWB by default on SKX/CLX/CPX",
        "commit_text": " The August 2023 microcode for GDS has an impact on the CLWB instruction.  See code comments for full details.  This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "static void cf_check init_intel(struct cpuinfo_x86 *c)\n{\n\t/* Detect the extended topology information if available */\n\tdetect_extended_topology(c);\n\n\tinit_intel_cacheinfo(c);\n\tif (c->cpuid_level > 9) {\n\t\tunsigned eax = cpuid_eax(10);\n\t\t/* Check for version and the number of counters */\n\t\tif ((eax & 0xff) && (((eax>>8) & 0xff) > 1))\n\t\t\t__set_bit(X86_FEATURE_ARCH_PERFMON, c->x86_capability);\n\t}\n\n\tif ( !cpu_has(c, X86_FEATURE_XTOPOLOGY) )\n\t{\n\t\tc->x86_max_cores = num_cpu_cores(c);\n\t\tdetect_ht(c);\n\t}\n\n\t/* Work around errata */\n\tIntel_errata_workarounds(c);\n\n\tif ((c->x86 == 0xf && c->x86_model >= 0x03) ||\n\t\t(c->x86 == 0x6 && c->x86_model >= 0x0e))\n\t\t__set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);\n\tif (cpu_has(c, X86_FEATURE_ITSC)) {\n\t\t__set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);\n\t\t__set_bit(X86_FEATURE_NONSTOP_TSC, c->x86_capability);\n\t\t__set_bit(X86_FEATURE_TSC_RELIABLE, c->x86_capability);\n\t}\n\tif ( opt_arat &&\n\t     ( c->cpuid_level >= 0x00000006 ) &&\n\t     ( cpuid_eax(0x00000006) & (1u<<2) ) )\n\t\t__set_bit(X86_FEATURE_ARAT, c->x86_capability);\n\n\tif ((opt_cpu_info && !(c->apicid & (c->x86_num_siblings - 1))) ||\n\t    c == &boot_cpu_data )\n\t\tintel_log_freq(c);\n}",
        "func": "static void cf_check init_intel(struct cpuinfo_x86 *c)\n{\n\t/* Detect the extended topology information if available */\n\tdetect_extended_topology(c);\n\n\tinit_intel_cacheinfo(c);\n\tif (c->cpuid_level > 9) {\n\t\tunsigned eax = cpuid_eax(10);\n\t\t/* Check for version and the number of counters */\n\t\tif ((eax & 0xff) && (((eax>>8) & 0xff) > 1))\n\t\t\t__set_bit(X86_FEATURE_ARCH_PERFMON, c->x86_capability);\n\t}\n\n\tif ( !cpu_has(c, X86_FEATURE_XTOPOLOGY) )\n\t{\n\t\tc->x86_max_cores = num_cpu_cores(c);\n\t\tdetect_ht(c);\n\t}\n\n\t/* Work around errata */\n\tIntel_errata_workarounds(c);\n\n\tif ((c->x86 == 0xf && c->x86_model >= 0x03) ||\n\t\t(c->x86 == 0x6 && c->x86_model >= 0x0e))\n\t\t__set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);\n\tif (cpu_has(c, X86_FEATURE_ITSC)) {\n\t\t__set_bit(X86_FEATURE_CONSTANT_TSC, c->x86_capability);\n\t\t__set_bit(X86_FEATURE_NONSTOP_TSC, c->x86_capability);\n\t\t__set_bit(X86_FEATURE_TSC_RELIABLE, c->x86_capability);\n\t}\n\tif ( opt_arat &&\n\t     ( c->cpuid_level >= 0x00000006 ) &&\n\t     ( cpuid_eax(0x00000006) & (1u<<2) ) )\n\t\t__set_bit(X86_FEATURE_ARAT, c->x86_capability);\n\n\tif ((opt_cpu_info && !(c->apicid & (c->x86_num_siblings - 1))) ||\n\t    c == &boot_cpu_data )\n\t\tintel_log_freq(c);\n\n\t/*\n\t * The Gather Data Sampling microcode mitigation (August 2023) has an\n\t * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n\t *\n\t * On this model, CLWB has equivalent behaviour to CLFLUSHOPT but the\n\t * latter is not impacted.  Hide CLWB to cause Xen to fall back to\n\t * using CLFLUSHOPT instead.\n\t */\n\tif (c == &boot_cpu_data &&\n\t    c->x86 == 6 && c->x86_model == INTEL_FAM6_SKYLAKE_X)\n\t\tsetup_clear_cpu_cap(X86_FEATURE_CLWB);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,4 +36,16 @@\n \tif ((opt_cpu_info && !(c->apicid & (c->x86_num_siblings - 1))) ||\n \t    c == &boot_cpu_data )\n \t\tintel_log_freq(c);\n+\n+\t/*\n+\t * The Gather Data Sampling microcode mitigation (August 2023) has an\n+\t * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.\n+\t *\n+\t * On this model, CLWB has equivalent behaviour to CLFLUSHOPT but the\n+\t * latter is not impacted.  Hide CLWB to cause Xen to fall back to\n+\t * using CLFLUSHOPT instead.\n+\t */\n+\tif (c == &boot_cpu_data &&\n+\t    c->x86 == 6 && c->x86_model == INTEL_FAM6_SKYLAKE_X)\n+\t\tsetup_clear_cpu_cap(X86_FEATURE_CLWB);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "",
                "\t/*",
                "\t * The Gather Data Sampling microcode mitigation (August 2023) has an",
                "\t * adverse performance impact on the CLWB instruction on SKX/CLX/CPX.",
                "\t *",
                "\t * On this model, CLWB has equivalent behaviour to CLFLUSHOPT but the",
                "\t * latter is not impacted.  Hide CLWB to cause Xen to fall back to",
                "\t * using CLFLUSHOPT instead.",
                "\t */",
                "\tif (c == &boot_cpu_data &&",
                "\t    c->x86 == 6 && c->x86_model == INTEL_FAM6_SKYLAKE_X)",
                "\t\tsetup_clear_cpu_cap(X86_FEATURE_CLWB);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/print_details",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/9f585f59d90c8d3a1b21369a852b7d7eee8a29b9",
        "commit_title": "x86/spec-ctrl: Enumerations for Gather Data Sampling",
        "commit_text": " GDS_CTRL is introduced by the August 2023 microcode.  GDS_NO is for current and future processors not susceptible to GDS.  This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "static void __init print_details(enum ind_thunk thunk)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, e21a = 0, max = 0, tmp;\n    uint64_t caps = 0;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000021 )\n        cpuid(0x80000021, &e21a, &tmp, &tmp, &tmp);\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_RRSBA)                          ? \" RRSBA\"          : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_PBRSB_NO)                       ? \" PBRSB_NO\"       : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_BTC_NO))         ? \" BTC_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_IBPB_BRTYPE))    ? \" IBPB_BRTYPE\"    : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           (!boot_cpu_has(X86_FEATURE_PSFD) &&\n            !boot_cpu_has(X86_FEATURE_INTEL_PSFD))   ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_PSFD)  ? \" PSFD+\" : \" PSFD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb_ctxt_switch                      ? \" IBPB-ctxt\" : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm ||\n           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM) ||\n            amd_virt_spec_ctrl ||\n            opt_eager_fpu || opt_md_clear_hvm)       ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            amd_virt_spec_ctrl)                      ? \" MSR_VIRT_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_hvm                          ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM)  ? \" IBPB-entry\"    : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV) ||\n            opt_eager_fpu || opt_md_clear_pv)        ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_pv                           ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV)   ? \" IBPB-entry\"    : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "func": "static void __init print_details(enum ind_thunk thunk)\n{\n    unsigned int _7d0 = 0, _7d2 = 0, e8b = 0, e21a = 0, max = 0, tmp;\n    uint64_t caps = 0;\n\n    /* Collect diagnostics about available mitigations. */\n    if ( boot_cpu_data.cpuid_level >= 7 )\n        cpuid_count(7, 0, &max, &tmp, &tmp, &_7d0);\n    if ( max >= 2 )\n        cpuid_count(7, 2, &tmp, &tmp, &tmp, &_7d2);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000008 )\n        cpuid(0x80000008, &tmp, &e8b, &tmp, &tmp);\n    if ( boot_cpu_data.extended_cpuid_level >= 0x80000021 )\n        cpuid(0x80000021, &e21a, &tmp, &tmp, &tmp);\n    if ( cpu_has_arch_caps )\n        rdmsrl(MSR_ARCH_CAPABILITIES, caps);\n\n    printk(\"Speculative mitigation facilities:\\n\");\n\n    /*\n     * Hardware read-only information, stating immunity to certain issues, or\n     * suggestions of which mitigation to use.\n     */\n    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n           (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n           (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n           (caps & ARCH_CAPS_RRSBA)                          ? \" RRSBA\"          : \"\",\n           (caps & ARCH_CAPS_SKIP_L1DFL)                     ? \" SKIP_L1DFL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_SSB_NO)) ||\n           (caps & ARCH_CAPS_SSB_NO)                         ? \" SSB_NO\"         : \"\",\n           (caps & ARCH_CAPS_MDS_NO)                         ? \" MDS_NO\"         : \"\",\n           (caps & ARCH_CAPS_TAA_NO)                         ? \" TAA_NO\"         : \"\",\n           (caps & ARCH_CAPS_SBDR_SSDP_NO)                   ? \" SBDR_SSDP_NO\"   : \"\",\n           (caps & ARCH_CAPS_FBSDP_NO)                       ? \" FBSDP_NO\"       : \"\",\n           (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n           (caps & ARCH_CAPS_PBRSB_NO)                       ? \" PBRSB_NO\"       : \"\",\n           (caps & ARCH_CAPS_GDS_NO)                         ? \" GDS_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS_SAME_MODE)) ? \" IBRS_SAME_MODE\" : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_BTC_NO))         ? \" BTC_NO\"         : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB_RET))       ? \" IBPB_RET\"       : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_IBPB_BRTYPE))    ? \" IBPB_BRTYPE\"    : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");\n\n    /* Hardware features which need driving to mitigate issues. */\n    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBRS\"           : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_STIBP)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_STIBP))          ? \" STIBP\"          : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_AMD_SSBD)) ||\n           (_7d0 & cpufeat_mask(X86_FEATURE_SSBD))           ? \" SSBD\"           : \"\",\n           (_7d2 & cpufeat_mask(X86_FEATURE_INTEL_PSFD)) ||\n           (e8b  & cpufeat_mask(X86_FEATURE_PSFD))           ? \" PSFD\"           : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_L1D_FLUSH))      ? \" L1D_FLUSH\"      : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_MD_CLEAR))       ? \" MD_CLEAR\"       : \"\",\n           (_7d0 & cpufeat_mask(X86_FEATURE_SRBDS_CTRL))     ? \" SRBDS_CTRL\"     : \"\",\n           (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n           (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n           (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",\n           (caps & ARCH_CAPS_GDS_CTRL)                       ? \" GDS_CTRL\"       : \"\",\n           (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");\n\n    /* Compiled-in support which pertains to mitigations. */\n    if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) || IS_ENABLED(CONFIG_SHADOW_PAGING) )\n        printk(\"  Compiled-in support:\"\n#ifdef CONFIG_INDIRECT_THUNK\n               \" INDIRECT_THUNK\"\n#endif\n#ifdef CONFIG_SHADOW_PAGING\n               \" SHADOW_PAGING\"\n#endif\n               \"\\n\");\n\n    /* Settings for Xen's protection, irrespective of guests. */\n    printk(\"  Xen settings: BTI-Thunk %s, SPEC_CTRL: %s%s%s%s%s, Other:%s%s%s%s%s\\n\",\n           thunk == THUNK_NONE      ? \"N/A\" :\n           thunk == THUNK_RETPOLINE ? \"RETPOLINE\" :\n           thunk == THUNK_LFENCE    ? \"LFENCE\" :\n           thunk == THUNK_JMP       ? \"JMP\" : \"?\",\n           (!boot_cpu_has(X86_FEATURE_IBRSB) &&\n            !boot_cpu_has(X86_FEATURE_IBRS))         ? \"No\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_IBRS)  ? \"IBRS+\" :  \"IBRS-\",\n           (!boot_cpu_has(X86_FEATURE_STIBP) &&\n            !boot_cpu_has(X86_FEATURE_AMD_STIBP))    ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_STIBP) ? \" STIBP+\" : \" STIBP-\",\n           (!boot_cpu_has(X86_FEATURE_SSBD) &&\n            !boot_cpu_has(X86_FEATURE_AMD_SSBD))     ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_SSBD)  ? \" SSBD+\" : \" SSBD-\",\n           (!boot_cpu_has(X86_FEATURE_PSFD) &&\n            !boot_cpu_has(X86_FEATURE_INTEL_PSFD))   ? \"\" :\n           (default_xen_spec_ctrl & SPEC_CTRL_PSFD)  ? \" PSFD+\" : \" PSFD-\",\n           !(caps & ARCH_CAPS_TSX_CTRL)              ? \"\" :\n           (opt_tsx & 1)                             ? \" TSX+\" : \" TSX-\",\n           !cpu_has_srbds_ctrl                       ? \"\" :\n           opt_srb_lock                              ? \" SRB_LOCK+\" : \" SRB_LOCK-\",\n           opt_ibpb_ctxt_switch                      ? \" IBPB-ctxt\" : \"\",\n           opt_l1d_flush                             ? \" L1D_FLUSH\" : \"\",\n           opt_md_clear_pv || opt_md_clear_hvm ||\n           opt_fb_clear_mmio                         ? \" VERW\"  : \"\",\n           opt_branch_harden                         ? \" BRANCH_HARDEN\" : \"\");\n\n    /* L1TF diagnostics, printed if vulnerable or PV shadowing is in use. */\n    if ( cpu_has_bug_l1tf || opt_pv_l1tf_hwdom || opt_pv_l1tf_domu )\n        printk(\"  L1TF: believed%s vulnerable, maxphysaddr L1D %u, CPUID %u\"\n               \", Safe address %\"PRIx64\"\\n\",\n               cpu_has_bug_l1tf ? \"\" : \" not\",\n               l1d_maxphysaddr, paddr_bits, l1tf_safe_maddr);\n\n    /*\n     * Alternatives blocks for protecting against and/or virtualising\n     * mitigation support for guests.\n     */\n#ifdef CONFIG_HVM\n    printk(\"  Support for HVM VMs:%s%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_HVM) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM) ||\n            amd_virt_spec_ctrl ||\n            opt_eager_fpu || opt_md_clear_hvm)       ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_HVM)      ? \" MSR_SPEC_CTRL\" : \"\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_HVM) ||\n            amd_virt_spec_ctrl)                      ? \" MSR_VIRT_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_HVM)      ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_hvm                          ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_HVM)  ? \" IBPB-entry\"    : \"\");\n\n#endif\n#ifdef CONFIG_PV\n    printk(\"  Support for PV VMs:%s%s%s%s%s%s\\n\",\n           (boot_cpu_has(X86_FEATURE_SC_MSR_PV) ||\n            boot_cpu_has(X86_FEATURE_SC_RSB_PV) ||\n            boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV) ||\n            opt_eager_fpu || opt_md_clear_pv)        ? \"\"               : \" None\",\n           boot_cpu_has(X86_FEATURE_SC_MSR_PV)       ? \" MSR_SPEC_CTRL\" : \"\",\n           boot_cpu_has(X86_FEATURE_SC_RSB_PV)       ? \" RSB\"           : \"\",\n           opt_eager_fpu                             ? \" EAGER_FPU\"     : \"\",\n           opt_md_clear_pv                           ? \" MD_CLEAR\"      : \"\",\n           boot_cpu_has(X86_FEATURE_IBPB_ENTRY_PV)   ? \" IBPB-entry\"    : \"\");\n\n    printk(\"  XPTI (64-bit PV only): Dom0 %s, DomU %s (with%s PCID)\\n\",\n           opt_xpti_hwdom ? \"enabled\" : \"disabled\",\n           opt_xpti_domu  ? \"enabled\" : \"disabled\",\n           xpti_pcid_enabled() ? \"\" : \"out\");\n\n    printk(\"  PV L1TF shadowing: Dom0 %s, DomU %s\\n\",\n           opt_pv_l1tf_hwdom ? \"enabled\"  : \"disabled\",\n           opt_pv_l1tf_domu  ? \"enabled\"  : \"disabled\");\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,7 @@\n      * Hardware read-only information, stating immunity to certain issues, or\n      * suggestions of which mitigation to use.\n      */\n-    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (caps & ARCH_CAPS_RDCL_NO)                        ? \" RDCL_NO\"        : \"\",\n            (caps & ARCH_CAPS_EIBRS)                          ? \" EIBRS\"          : \"\",\n            (caps & ARCH_CAPS_RSBA)                           ? \" RSBA\"           : \"\",\n@@ -36,6 +36,7 @@\n            (caps & ARCH_CAPS_PSDP_NO)                        ? \" PSDP_NO\"        : \"\",\n            (caps & ARCH_CAPS_FB_CLEAR)                       ? \" FB_CLEAR\"       : \"\",\n            (caps & ARCH_CAPS_PBRSB_NO)                       ? \" PBRSB_NO\"       : \"\",\n+           (caps & ARCH_CAPS_GDS_NO)                         ? \" GDS_NO\"         : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_ALWAYS))    ? \" IBRS_ALWAYS\"    : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_STIBP_ALWAYS))   ? \" STIBP_ALWAYS\"   : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS_FAST))      ? \" IBRS_FAST\"      : \"\",\n@@ -46,7 +47,7 @@\n            (e21a & cpufeat_mask(X86_FEATURE_SRSO_NO))        ? \" SRSO_NO\"        : \"\");\n \n     /* Hardware features which need driving to mitigate issues. */\n-    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n+    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBPB)) ||\n            (_7d0 & cpufeat_mask(X86_FEATURE_IBRSB))          ? \" IBPB\"           : \"\",\n            (e8b  & cpufeat_mask(X86_FEATURE_IBRS)) ||\n@@ -63,6 +64,7 @@\n            (e8b  & cpufeat_mask(X86_FEATURE_VIRT_SSBD))      ? \" VIRT_SSBD\"      : \"\",\n            (caps & ARCH_CAPS_TSX_CTRL)                       ? \" TSX_CTRL\"       : \"\",\n            (caps & ARCH_CAPS_FB_CLEAR_CTRL)                  ? \" FB_CLEAR_CTRL\"  : \"\",\n+           (caps & ARCH_CAPS_GDS_CTRL)                       ? \" GDS_CTRL\"       : \"\",\n            (e21a & cpufeat_mask(X86_FEATURE_SBPB))           ? \" SBPB\"           : \"\");\n \n     /* Compiled-in support which pertains to mitigations. */",
        "diff_line_info": {
            "deleted_lines": [
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s\\n\","
            ],
            "added_lines": [
                "    printk(\"  Hardware hints:%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_GDS_NO)                         ? \" GDS_NO\"         : \"\",",
                "    printk(\"  Hardware features:%s%s%s%s%s%s%s%s%s%s%s%s%s\\n\",",
                "           (caps & ARCH_CAPS_GDS_CTRL)                       ? \" GDS_CTRL\"       : \"\","
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/parse_spec_ctrl",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/56d690efd3ca3c68e1d222f259fb3d216206e8e5",
        "commit_title": "x86/spec-ctrl: Mitigate Gather Data Sampling",
        "commit_text": " This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "static int __init cf_check parse_spec_ctrl(const char *s)\n{\n    const char *ss;\n    int val, rc = 0;\n\n    do {\n        ss = strchr(s, ',');\n        if ( !ss )\n            ss = strchr(s, '\\0');\n\n        /* Global and Xen-wide disable. */\n        val = parse_bool(s, ss);\n        if ( !val )\n        {\n            opt_msr_sc_pv = false;\n            opt_msr_sc_hvm = false;\n\n            opt_eager_fpu = 0;\n\n            if ( opt_xpti_hwdom < 0 )\n                opt_xpti_hwdom = 0;\n            if ( opt_xpti_domu < 0 )\n                opt_xpti_domu = 0;\n\n            if ( opt_smt < 0 )\n                opt_smt = 1;\n\n            if ( opt_pv_l1tf_hwdom < 0 )\n                opt_pv_l1tf_hwdom = 0;\n            if ( opt_pv_l1tf_domu < 0 )\n                opt_pv_l1tf_domu = 0;\n\n            if ( opt_tsx == -1 )\n                opt_tsx = -3;\n\n        disable_common:\n            opt_rsb_pv = false;\n            opt_rsb_hvm = false;\n            opt_md_clear_pv = 0;\n            opt_md_clear_hvm = 0;\n            opt_ibpb_entry_pv = 0;\n            opt_ibpb_entry_hvm = 0;\n            opt_ibpb_entry_dom0 = false;\n\n            opt_thunk = THUNK_JMP;\n            opt_ibrs = 0;\n            opt_ibpb_ctxt_switch = false;\n            opt_ssbd = false;\n            opt_l1d_flush = 0;\n            opt_branch_harden = false;\n            opt_srb_lock = 0;\n            opt_unpriv_mmio = false;\n        }\n        else if ( val > 0 )\n            rc = -EINVAL;\n        else if ( (val = parse_boolean(\"xen\", s, ss)) >= 0 )\n        {\n            if ( !val )\n                goto disable_common;\n\n            rc = -EINVAL;\n        }\n\n        /* Xen's alternative blocks. */\n        else if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_rsb_pv = val;\n            opt_md_clear_pv = val;\n            opt_ibpb_entry_pv = val;\n        }\n        else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_hvm = val;\n            opt_rsb_hvm = val;\n            opt_md_clear_hvm = val;\n            opt_ibpb_entry_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"msr-sc\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_msr_sc_pv = opt_msr_sc_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"msr-sc=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_msr_sc_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_msr_sc_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"rsb\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_rsb_pv = opt_rsb_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"rsb=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_rsb_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_rsb_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"md-clear\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_md_clear_pv = opt_md_clear_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"md-clear=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_md_clear_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_md_clear_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"ibpb-entry\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_ibpb_entry_pv = opt_ibpb_entry_hvm =\n                    opt_ibpb_entry_dom0 = val;\n                break;\n\n            case -2:\n                s += strlen(\"ibpb-entry=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_ibpb_entry_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_ibpb_entry_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n\n        /* Xen's speculative sidechannel mitigation settings. */\n        else if ( !strncmp(s, \"bti-thunk=\", 10) )\n        {\n            s += 10;\n\n            if ( !cmdline_strcmp(s, \"retpoline\") )\n                opt_thunk = THUNK_RETPOLINE;\n            else if ( !cmdline_strcmp(s, \"lfence\") )\n                opt_thunk = THUNK_LFENCE;\n            else if ( !cmdline_strcmp(s, \"jmp\") )\n                opt_thunk = THUNK_JMP;\n            else\n                rc = -EINVAL;\n        }\n\n        /* Bits in MSR_SPEC_CTRL. */\n        else if ( (val = parse_boolean(\"ibrs\", s, ss)) >= 0 )\n            opt_ibrs = val;\n        else if ( (val = parse_boolean(\"stibp\", s, ss)) >= 0 )\n            opt_stibp = val;\n        else if ( (val = parse_boolean(\"ssbd\", s, ss)) >= 0 )\n            opt_ssbd = val;\n        else if ( (val = parse_boolean(\"psfd\", s, ss)) >= 0 )\n            opt_psfd = val;\n\n        /* Misc settings. */\n        else if ( (val = parse_boolean(\"ibpb\", s, ss)) >= 0 )\n            opt_ibpb_ctxt_switch = val;\n        else if ( (val = parse_boolean(\"eager-fpu\", s, ss)) >= 0 )\n            opt_eager_fpu = val;\n        else if ( (val = parse_boolean(\"l1d-flush\", s, ss)) >= 0 )\n            opt_l1d_flush = val;\n        else if ( (val = parse_boolean(\"branch-harden\", s, ss)) >= 0 )\n            opt_branch_harden = val;\n        else if ( (val = parse_boolean(\"srb-lock\", s, ss)) >= 0 )\n            opt_srb_lock = val;\n        else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )\n            opt_unpriv_mmio = val;\n        else\n            rc = -EINVAL;\n\n        s = ss + 1;\n    } while ( *ss );\n\n    return rc;\n}",
        "func": "static int __init cf_check parse_spec_ctrl(const char *s)\n{\n    const char *ss;\n    int val, rc = 0;\n\n    do {\n        ss = strchr(s, ',');\n        if ( !ss )\n            ss = strchr(s, '\\0');\n\n        /* Global and Xen-wide disable. */\n        val = parse_bool(s, ss);\n        if ( !val )\n        {\n            opt_msr_sc_pv = false;\n            opt_msr_sc_hvm = false;\n\n            opt_eager_fpu = 0;\n\n            if ( opt_xpti_hwdom < 0 )\n                opt_xpti_hwdom = 0;\n            if ( opt_xpti_domu < 0 )\n                opt_xpti_domu = 0;\n\n            if ( opt_smt < 0 )\n                opt_smt = 1;\n\n            if ( opt_pv_l1tf_hwdom < 0 )\n                opt_pv_l1tf_hwdom = 0;\n            if ( opt_pv_l1tf_domu < 0 )\n                opt_pv_l1tf_domu = 0;\n\n            if ( opt_tsx == -1 )\n                opt_tsx = -3;\n\n        disable_common:\n            opt_rsb_pv = false;\n            opt_rsb_hvm = false;\n            opt_md_clear_pv = 0;\n            opt_md_clear_hvm = 0;\n            opt_ibpb_entry_pv = 0;\n            opt_ibpb_entry_hvm = 0;\n            opt_ibpb_entry_dom0 = false;\n\n            opt_thunk = THUNK_JMP;\n            opt_ibrs = 0;\n            opt_ibpb_ctxt_switch = false;\n            opt_ssbd = false;\n            opt_l1d_flush = 0;\n            opt_branch_harden = false;\n            opt_srb_lock = 0;\n            opt_unpriv_mmio = false;\n            opt_gds_mit = 0;\n        }\n        else if ( val > 0 )\n            rc = -EINVAL;\n        else if ( (val = parse_boolean(\"xen\", s, ss)) >= 0 )\n        {\n            if ( !val )\n                goto disable_common;\n\n            rc = -EINVAL;\n        }\n\n        /* Xen's alternative blocks. */\n        else if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_pv = val;\n            opt_rsb_pv = val;\n            opt_md_clear_pv = val;\n            opt_ibpb_entry_pv = val;\n        }\n        else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n        {\n            opt_msr_sc_hvm = val;\n            opt_rsb_hvm = val;\n            opt_md_clear_hvm = val;\n            opt_ibpb_entry_hvm = val;\n        }\n        else if ( (val = parse_boolean(\"msr-sc\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_msr_sc_pv = opt_msr_sc_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"msr-sc=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_msr_sc_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_msr_sc_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"rsb\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_rsb_pv = opt_rsb_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"rsb=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_rsb_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_rsb_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"md-clear\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_md_clear_pv = opt_md_clear_hvm = val;\n                break;\n\n            case -2:\n                s += strlen(\"md-clear=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_md_clear_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_md_clear_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n        else if ( (val = parse_boolean(\"ibpb-entry\", s, ss)) != -1 )\n        {\n            switch ( val )\n            {\n            case 0:\n            case 1:\n                opt_ibpb_entry_pv = opt_ibpb_entry_hvm =\n                    opt_ibpb_entry_dom0 = val;\n                break;\n\n            case -2:\n                s += strlen(\"ibpb-entry=\");\n                if ( (val = parse_boolean(\"pv\", s, ss)) >= 0 )\n                    opt_ibpb_entry_pv = val;\n                else if ( (val = parse_boolean(\"hvm\", s, ss)) >= 0 )\n                    opt_ibpb_entry_hvm = val;\n                else\n            default:\n                    rc = -EINVAL;\n                break;\n            }\n        }\n\n        /* Xen's speculative sidechannel mitigation settings. */\n        else if ( !strncmp(s, \"bti-thunk=\", 10) )\n        {\n            s += 10;\n\n            if ( !cmdline_strcmp(s, \"retpoline\") )\n                opt_thunk = THUNK_RETPOLINE;\n            else if ( !cmdline_strcmp(s, \"lfence\") )\n                opt_thunk = THUNK_LFENCE;\n            else if ( !cmdline_strcmp(s, \"jmp\") )\n                opt_thunk = THUNK_JMP;\n            else\n                rc = -EINVAL;\n        }\n\n        /* Bits in MSR_SPEC_CTRL. */\n        else if ( (val = parse_boolean(\"ibrs\", s, ss)) >= 0 )\n            opt_ibrs = val;\n        else if ( (val = parse_boolean(\"stibp\", s, ss)) >= 0 )\n            opt_stibp = val;\n        else if ( (val = parse_boolean(\"ssbd\", s, ss)) >= 0 )\n            opt_ssbd = val;\n        else if ( (val = parse_boolean(\"psfd\", s, ss)) >= 0 )\n            opt_psfd = val;\n\n        /* Misc settings. */\n        else if ( (val = parse_boolean(\"ibpb\", s, ss)) >= 0 )\n            opt_ibpb_ctxt_switch = val;\n        else if ( (val = parse_boolean(\"eager-fpu\", s, ss)) >= 0 )\n            opt_eager_fpu = val;\n        else if ( (val = parse_boolean(\"l1d-flush\", s, ss)) >= 0 )\n            opt_l1d_flush = val;\n        else if ( (val = parse_boolean(\"branch-harden\", s, ss)) >= 0 )\n            opt_branch_harden = val;\n        else if ( (val = parse_boolean(\"srb-lock\", s, ss)) >= 0 )\n            opt_srb_lock = val;\n        else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )\n            opt_unpriv_mmio = val;\n        else if ( (val = parse_boolean(\"gds-mit\", s, ss)) >= 0 )\n            opt_gds_mit = val;\n        else\n            rc = -EINVAL;\n\n        s = ss + 1;\n    } while ( *ss );\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,6 +50,7 @@\n             opt_branch_harden = false;\n             opt_srb_lock = 0;\n             opt_unpriv_mmio = false;\n+            opt_gds_mit = 0;\n         }\n         else if ( val > 0 )\n             rc = -EINVAL;\n@@ -200,6 +201,8 @@\n             opt_srb_lock = val;\n         else if ( (val = parse_boolean(\"unpriv-mmio\", s, ss)) >= 0 )\n             opt_unpriv_mmio = val;\n+        else if ( (val = parse_boolean(\"gds-mit\", s, ss)) >= 0 )\n+            opt_gds_mit = val;\n         else\n             rc = -EINVAL;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "            opt_gds_mit = 0;",
                "        else if ( (val = parse_boolean(\"gds-mit\", s, ss)) >= 0 )",
                "            opt_gds_mit = val;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-40982",
        "func_name": "xen-project/xen/init_speculation_mitigations",
        "description": "Information exposure through microarchitectural state after transient execution in certain vector execution units for some Intel(R) Processors may allow an authenticated user to potentially enable information disclosure via local access.",
        "git_url": "https://github.com/xen-project/xen/commit/56d690efd3ca3c68e1d222f259fb3d216206e8e5",
        "commit_title": "x86/spec-ctrl: Mitigate Gather Data Sampling",
        "commit_text": " This is part of XSA-435 / CVE-2022-40982 ",
        "func_before": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa, retpoline_safe;\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n        {\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n            add_taint(TAINT_CPU_OUT_OF_SPEC);\n        }\n        else if ( opt_ibrs == -1 )\n            opt_ibrs = ibrs = true;\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /* Determine if retpoline is safe on this CPU.  Fix up RSBA/RRSBA enumerations. */\n    retpoline_safe = retpoline_calculations();\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_sc_msr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* Support VIRT_SPEC_CTRL.SSBD if AMD_SSBD is not available. */\n    if ( opt_msr_sc_hvm && !cpu_has_amd_ssbd &&\n         (cpu_has_virt_ssbd || (amd_legacy_ssbd && amd_setup_legacy_ssbd())) )\n        amd_virt_spec_ctrl = true;\n\n    /* Figure out default_xen_spec_ctrl. */\n    if ( has_spec_ctrl && ibrs )\n    {\n        /* IBRS implies STIBP.  */\n        if ( opt_stibp == -1 )\n            opt_stibp = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n    }\n\n    /*\n     * Use STIBP by default on all AMD systems.  Zen3 and later enumerate\n     * STIBP_ALWAYS, but STIBP is needed on Zen2 as part of the mitigations\n     * for Branch Type Confusion.\n     *\n     * Leave STIBP off by default on Intel.  Pre-eIBRS systems suffer a\n     * substantial perf hit when it was implemented in microcode.\n     */\n    if ( opt_stibp == -1 )\n        opt_stibp = !!boot_cpu_has(X86_FEATURE_AMD_STIBP);\n\n    if ( opt_stibp && (boot_cpu_has(X86_FEATURE_STIBP) ||\n                       boot_cpu_has(X86_FEATURE_AMD_STIBP)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_STIBP;\n\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n    {\n        /* SSBD implies PSFD */\n        if ( opt_psfd == -1 )\n            opt_psfd = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n    }\n\n    /*\n     * Don't use PSFD by default.  AMD designed the predictor to\n     * auto-clear on privilege change.  PSFD is implied by SSBD, which is\n     * off by default.\n     */\n    if ( opt_psfd == -1 )\n        opt_psfd = 0;\n\n    if ( opt_psfd && (boot_cpu_has(X86_FEATURE_PSFD) ||\n                      boot_cpu_has(X86_FEATURE_INTEL_PSFD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_PSFD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * 4) Some CPUs have RSBs which are re-partitioned based on thread\n     *    idleness, which allows an attacker to inject entries into the other\n     *    thread.  We still active the optimisation in this case, and mitigate\n     *    in the idle path which has lower overhead.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 )\n    {\n        opt_rsb_pv = (opt_pv32 || !boot_cpu_has(X86_FEATURE_XEN_SMEP) ||\n                      !rsb_is_full_width());\n\n        /*\n         * Cross-Thread Return Address Predictions.\n         *\n         * Vulnerable systems are Zen1/Zen2 uarch, which is AMD Fam17 / Hygon\n         * Fam18, when SMT is active.\n         *\n         * To mitigate, we must flush the RSB/RAS/RAP once between entering\n         * Xen and going idle.\n         *\n         * Most cases flush on entry to Xen anyway.  The one case where we\n         * don't is when using the SMEP optimisation for PV guests.  Flushing\n         * before going idle is less overhead than flushing on PV entry.\n         */\n        if ( !opt_rsb_pv && hw_smt_enabled &&\n             (boot_cpu_data.x86_vendor & (X86_VENDOR_AMD|X86_VENDOR_HYGON)) &&\n             (boot_cpu_data.x86 == 0x17 || boot_cpu_data.x86 == 0x18) )\n            setup_force_cpu_cap(X86_FEATURE_SC_RSB_IDLE);\n    }\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n        /*\n         * For SVM, Xen's RSB safety actions are performed before STGI, so\n         * behave atomically with respect to IST sources.\n         *\n         * For VT-x, NMIs are atomic with VMExit (the NMI gets queued but not\n         * delivered) whereas other IST sources are not atomic.  Specifically,\n         * #MC can hit ahead the RSB safety action in the vmexit path.\n         *\n         * Therefore, it is necessary for the IST logic to protect Xen against\n         * possible rogue RSB speculation.\n         */\n        if ( !cpu_has_svm )\n            default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    srso_calculations(hw_smt_enabled);\n\n    ibpb_calculations();\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /*\n     * For microcoded IBRS only (i.e. Intel, pre eIBRS), it is recommended to\n     * clear MSR_SPEC_CTRL before going idle, to avoid impacting sibling\n     * threads.  Activate this if SMT is enabled, and Xen is using a non-zero\n     * MSR_SPEC_CTRL setting.\n     */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) && !cpu_has_eibrs &&\n         hw_smt_enabled && default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default();\n\n    l1tf_calculations();\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !cpu_has_skip_l1dfl;\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations();\n\n    /*\n     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n     * reintroduced the VERW fill buffer flushing side effect because of a\n     * susceptibility to FBSDP.\n     *\n     * If unprivileged guests have (or will have) MMIO mappings, we can\n     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n     * the return-to-guest path.\n     */\n    if ( opt_unpriv_mmio )\n        opt_fb_clear_mmio = cpu_has_fb_clear;\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n     * either the PV or HVM MDS defences are used, or if we may give MMIO\n     * access to untrusted guests.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !cpu_has_skip_l1dfl && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || cpu_has_tsx_ctrl) && cpu_has_mds_no && !cpu_has_taa_no;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && cpu_has_tsx_ctrl &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n     * data becomes available to other contexts.  To recover the data, an\n     * attacker needs to use:\n     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n     *  - DRPW (Architecturally latch stale fill buffer data)\n     *\n     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n             cpu_has_mds_no && !cpu_has_taa_no &&\n             (!cpu_has_hle || (cpu_has_tsx_ctrl && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    print_details(thunk);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "func": "void __init init_speculation_mitigations(void)\n{\n    enum ind_thunk thunk = THUNK_DEFAULT;\n    bool has_spec_ctrl, ibrs = false, hw_smt_enabled;\n    bool cpu_has_bug_taa, retpoline_safe;\n\n    hw_smt_enabled = check_smt_enabled();\n\n    has_spec_ctrl = (boot_cpu_has(X86_FEATURE_IBRSB) ||\n                     boot_cpu_has(X86_FEATURE_IBRS));\n\n    /*\n     * First, disable the use of retpolines if Xen is using CET.  Retpolines\n     * are a ROP gadget so incompatbile with Shadow Stacks, while IBT depends\n     * on executing indirect branches for the safety properties to apply.\n     *\n     * In the absence of retpolines, IBRS needs to be used for speculative\n     * safety.  All CET-capable hardware has efficient IBRS.\n     */\n    if ( read_cr4() & X86_CR4_CET )\n    {\n        if ( !has_spec_ctrl )\n        {\n            printk(XENLOG_WARNING \"?!? CET active, but no MSR_SPEC_CTRL?\\n\");\n            add_taint(TAINT_CPU_OUT_OF_SPEC);\n        }\n        else if ( opt_ibrs == -1 )\n            opt_ibrs = ibrs = true;\n\n        if ( opt_thunk == THUNK_DEFAULT || opt_thunk == THUNK_RETPOLINE )\n            thunk = THUNK_JMP;\n    }\n\n    /* Determine if retpoline is safe on this CPU.  Fix up RSBA/RRSBA enumerations. */\n    retpoline_safe = retpoline_calculations();\n\n    /*\n     * Has the user specified any custom BTI mitigations?  If so, follow their\n     * instructions exactly and disable all heuristics.\n     */\n    if ( opt_thunk != THUNK_DEFAULT || opt_ibrs != -1 )\n    {\n        thunk = opt_thunk;\n        ibrs  = !!opt_ibrs;\n    }\n    else\n    {\n        /*\n         * Evaluate the safest Branch Target Injection mitigations to use.\n         * First, begin with compiler-aided mitigations.\n         */\n        if ( IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        {\n            /*\n             * On all hardware, we'd like to use retpoline in preference to\n             * IBRS, but only if it is safe on this hardware.\n             */\n            if ( retpoline_safe )\n                thunk = THUNK_RETPOLINE;\n            else if ( has_spec_ctrl )\n                ibrs = true;\n        }\n        /* Without compiler thunk support, use IBRS if available. */\n        else if ( has_spec_ctrl )\n            ibrs = true;\n    }\n\n    /*\n     * Supplimentary minor adjustments.  Without compiler support, there are\n     * no thunks.\n     */\n    if ( !IS_ENABLED(CONFIG_INDIRECT_THUNK) )\n        thunk = THUNK_NONE;\n\n    /*\n     * If IBRS is in use and thunks are compiled in, there is no point\n     * suffering extra overhead.  Switch to the least-overhead thunk.\n     */\n    if ( ibrs && thunk == THUNK_DEFAULT )\n        thunk = THUNK_JMP;\n\n    /*\n     * If there are still no thunk preferences, the compiled default is\n     * actually retpoline, and it is better than nothing.\n     */\n    if ( thunk == THUNK_DEFAULT )\n        thunk = THUNK_RETPOLINE;\n\n    /* Apply the chosen settings. */\n    if ( thunk == THUNK_LFENCE )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_LFENCE);\n    else if ( thunk == THUNK_JMP )\n        setup_force_cpu_cap(X86_FEATURE_IND_THUNK_JMP);\n\n    /* Intel hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) )\n    {\n        if ( opt_msr_sc_pv )\n        {\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_PV);\n        }\n\n        if ( opt_msr_sc_hvm )\n        {\n            /*\n             * While the guest MSR_SPEC_CTRL value is loaded/saved atomically,\n             * Xen's value is not restored atomically.  An early NMI hitting\n             * the VMExit path needs to restore Xen's value for safety.\n             */\n            default_spec_ctrl_flags |= SCF_ist_sc_msr;\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n        }\n    }\n\n    /* AMD hardware: MSR_SPEC_CTRL alternatives setup. */\n    if ( boot_cpu_has(X86_FEATURE_IBRS) )\n    {\n        /*\n         * Virtualising MSR_SPEC_CTRL for guests depends on SVM support, which\n         * on real hardware matches the availability of MSR_SPEC_CTRL in the\n         * first place.\n         *\n         * No need for SCF_ist_sc_msr because Xen's value is restored\n         * atomically WRT NMIs in the VMExit path.\n         *\n         * TODO: Adjust cpu_has_svm_spec_ctrl to be usable earlier on boot.\n         */\n        if ( opt_msr_sc_hvm &&\n             (boot_cpu_data.extended_cpuid_level >= 0x8000000a) &&\n             (cpuid_edx(0x8000000a) & (1u << SVM_FEATURE_SPEC_CTRL)) )\n            setup_force_cpu_cap(X86_FEATURE_SC_MSR_HVM);\n    }\n\n    /* Support VIRT_SPEC_CTRL.SSBD if AMD_SSBD is not available. */\n    if ( opt_msr_sc_hvm && !cpu_has_amd_ssbd &&\n         (cpu_has_virt_ssbd || (amd_legacy_ssbd && amd_setup_legacy_ssbd())) )\n        amd_virt_spec_ctrl = true;\n\n    /* Figure out default_xen_spec_ctrl. */\n    if ( has_spec_ctrl && ibrs )\n    {\n        /* IBRS implies STIBP.  */\n        if ( opt_stibp == -1 )\n            opt_stibp = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_IBRS;\n    }\n\n    /*\n     * Use STIBP by default on all AMD systems.  Zen3 and later enumerate\n     * STIBP_ALWAYS, but STIBP is needed on Zen2 as part of the mitigations\n     * for Branch Type Confusion.\n     *\n     * Leave STIBP off by default on Intel.  Pre-eIBRS systems suffer a\n     * substantial perf hit when it was implemented in microcode.\n     */\n    if ( opt_stibp == -1 )\n        opt_stibp = !!boot_cpu_has(X86_FEATURE_AMD_STIBP);\n\n    if ( opt_stibp && (boot_cpu_has(X86_FEATURE_STIBP) ||\n                       boot_cpu_has(X86_FEATURE_AMD_STIBP)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_STIBP;\n\n    if ( opt_ssbd && (boot_cpu_has(X86_FEATURE_SSBD) ||\n                      boot_cpu_has(X86_FEATURE_AMD_SSBD)) )\n    {\n        /* SSBD implies PSFD */\n        if ( opt_psfd == -1 )\n            opt_psfd = 1;\n\n        default_xen_spec_ctrl |= SPEC_CTRL_SSBD;\n    }\n\n    /*\n     * Don't use PSFD by default.  AMD designed the predictor to\n     * auto-clear on privilege change.  PSFD is implied by SSBD, which is\n     * off by default.\n     */\n    if ( opt_psfd == -1 )\n        opt_psfd = 0;\n\n    if ( opt_psfd && (boot_cpu_has(X86_FEATURE_PSFD) ||\n                      boot_cpu_has(X86_FEATURE_INTEL_PSFD)) )\n        default_xen_spec_ctrl |= SPEC_CTRL_PSFD;\n\n    /*\n     * PV guests can create RSB entries for any linear address they control,\n     * which are outside of Xen's mappings.\n     *\n     * SMEP inhibits speculation to any user mappings, so in principle it is\n     * safe to not overwrite the RSB when SMEP is active.\n     *\n     * However, some caveats apply:\n     *\n     * 1) CALL instructions push the next sequential linear address into the\n     *    RSB, meaning that there is a boundary case at the user=>supervisor\n     *    split.  This can be compensated for by having an unmapped or NX\n     *    page, or an instruction which halts speculation.\n     *\n     *    For Xen, the next sequential linear address is the start of M2P\n     *    (mapped NX), or a zapped hole (unmapped).\n     *\n     * 2) 32bit PV kernels execute in Ring 1 and use supervisor mappings.\n     *    SMEP offers no protection in this case.\n     *\n     * 3) Some CPUs have RSBs which are not full width, which allow the\n     *    attacker's entries to alias Xen addresses.\n     *\n     * 4) Some CPUs have RSBs which are re-partitioned based on thread\n     *    idleness, which allows an attacker to inject entries into the other\n     *    thread.  We still active the optimisation in this case, and mitigate\n     *    in the idle path which has lower overhead.\n     *\n     * It is safe to turn off RSB stuffing when Xen is using SMEP itself, and\n     * 32bit PV guests are disabled, and when the RSB is full width.\n     */\n    BUILD_BUG_ON(RO_MPT_VIRT_START != PML4_ADDR(256));\n    if ( opt_rsb_pv == -1 )\n    {\n        opt_rsb_pv = (opt_pv32 || !boot_cpu_has(X86_FEATURE_XEN_SMEP) ||\n                      !rsb_is_full_width());\n\n        /*\n         * Cross-Thread Return Address Predictions.\n         *\n         * Vulnerable systems are Zen1/Zen2 uarch, which is AMD Fam17 / Hygon\n         * Fam18, when SMT is active.\n         *\n         * To mitigate, we must flush the RSB/RAS/RAP once between entering\n         * Xen and going idle.\n         *\n         * Most cases flush on entry to Xen anyway.  The one case where we\n         * don't is when using the SMEP optimisation for PV guests.  Flushing\n         * before going idle is less overhead than flushing on PV entry.\n         */\n        if ( !opt_rsb_pv && hw_smt_enabled &&\n             (boot_cpu_data.x86_vendor & (X86_VENDOR_AMD|X86_VENDOR_HYGON)) &&\n             (boot_cpu_data.x86 == 0x17 || boot_cpu_data.x86 == 0x18) )\n            setup_force_cpu_cap(X86_FEATURE_SC_RSB_IDLE);\n    }\n\n    if ( opt_rsb_pv )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_PV);\n        default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    /*\n     * HVM guests can always poison the RSB to point at Xen supervisor\n     * mappings.\n     */\n    if ( opt_rsb_hvm )\n    {\n        setup_force_cpu_cap(X86_FEATURE_SC_RSB_HVM);\n\n        /*\n         * For SVM, Xen's RSB safety actions are performed before STGI, so\n         * behave atomically with respect to IST sources.\n         *\n         * For VT-x, NMIs are atomic with VMExit (the NMI gets queued but not\n         * delivered) whereas other IST sources are not atomic.  Specifically,\n         * #MC can hit ahead the RSB safety action in the vmexit path.\n         *\n         * Therefore, it is necessary for the IST logic to protect Xen against\n         * possible rogue RSB speculation.\n         */\n        if ( !cpu_has_svm )\n            default_spec_ctrl_flags |= SCF_ist_rsb;\n    }\n\n    srso_calculations(hw_smt_enabled);\n\n    ibpb_calculations();\n\n    /* Check whether Eager FPU should be enabled by default. */\n    if ( opt_eager_fpu == -1 )\n        opt_eager_fpu = should_use_eager_fpu();\n\n    /* (Re)init BSP state now that default_spec_ctrl_flags has been calculated. */\n    init_shadow_spec_ctrl_state();\n\n    /*\n     * For microcoded IBRS only (i.e. Intel, pre eIBRS), it is recommended to\n     * clear MSR_SPEC_CTRL before going idle, to avoid impacting sibling\n     * threads.  Activate this if SMT is enabled, and Xen is using a non-zero\n     * MSR_SPEC_CTRL setting.\n     */\n    if ( boot_cpu_has(X86_FEATURE_IBRSB) && !cpu_has_eibrs &&\n         hw_smt_enabled && default_xen_spec_ctrl )\n        setup_force_cpu_cap(X86_FEATURE_SC_MSR_IDLE);\n\n    xpti_init_default();\n\n    l1tf_calculations();\n\n    /*\n     * By default, enable PV domU L1TF mitigations on all L1TF-vulnerable\n     * hardware, except when running in shim mode.\n     *\n     * In shim mode, SHADOW is expected to be compiled out, and a malicious\n     * guest kernel can only attack the shim Xen, not the host Xen.\n     */\n    if ( opt_pv_l1tf_hwdom == -1 )\n        opt_pv_l1tf_hwdom = 0;\n    if ( opt_pv_l1tf_domu == -1 )\n        opt_pv_l1tf_domu = !pv_shim && cpu_has_bug_l1tf;\n\n    /*\n     * By default, enable L1D_FLUSH on L1TF-vulnerable hardware, unless\n     * instructed to skip the flush on vmentry by our outer hypervisor.\n     */\n    if ( !boot_cpu_has(X86_FEATURE_L1D_FLUSH) )\n        opt_l1d_flush = 0;\n    else if ( opt_l1d_flush == -1 )\n        opt_l1d_flush = cpu_has_bug_l1tf && !cpu_has_skip_l1dfl;\n\n    /* We compile lfence's in by default, and nop them out if requested. */\n    if ( !opt_branch_harden )\n        setup_force_cpu_cap(X86_FEATURE_SC_NO_BRANCH_HARDEN);\n\n    /*\n     * We do not disable HT by default on affected hardware.\n     *\n     * Firstly, if the user intends to use exclusively PV, or HVM shadow\n     * guests, HT isn't a concern and should remain fully enabled.  Secondly,\n     * safety for HVM HAP guests can be arranged by the toolstack with core\n     * parking, pinning or cpupool configurations, including mixed setups.\n     *\n     * However, if we are on affected hardware, with HT enabled, and the user\n     * hasn't explicitly chosen whether to use HT or not, nag them to do so.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_l1tf && !pv_shim && hw_smt_enabled )\n        warning_add(\n            \"Booted on L1TF-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Please assess your configuration and choose an\\n\"\n            \"explicit 'smt=<bool>' setting.  See XSA-273.\\n\");\n\n    mds_calculations();\n\n    /*\n     * Parts which enumerate FB_CLEAR are those which are post-MDS_NO and have\n     * reintroduced the VERW fill buffer flushing side effect because of a\n     * susceptibility to FBSDP.\n     *\n     * If unprivileged guests have (or will have) MMIO mappings, we can\n     * mitigate cross-domain leakage of fill buffer data by issuing VERW on\n     * the return-to-guest path.\n     */\n    if ( opt_unpriv_mmio )\n        opt_fb_clear_mmio = cpu_has_fb_clear;\n\n    /*\n     * By default, enable PV and HVM mitigations on MDS-vulnerable hardware.\n     * This will only be a token effort for MLPDS/MFBDS when HT is enabled,\n     * but it is somewhat better than nothing.\n     */\n    if ( opt_md_clear_pv == -1 )\n        opt_md_clear_pv = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                           boot_cpu_has(X86_FEATURE_MD_CLEAR));\n    if ( opt_md_clear_hvm == -1 )\n        opt_md_clear_hvm = ((cpu_has_bug_mds || cpu_has_bug_msbds_only) &&\n                            boot_cpu_has(X86_FEATURE_MD_CLEAR));\n\n    /*\n     * Enable MDS/MMIO defences as applicable.  The Idle blocks need using if\n     * either the PV or HVM MDS defences are used, or if we may give MMIO\n     * access to untrusted guests.\n     *\n     * HVM is more complicated.  The MD_CLEAR microcode extends L1D_FLUSH with\n     * equivalent semantics to avoid needing to perform both flushes on the\n     * HVM path.  Therefore, we don't need VERW in addition to L1D_FLUSH (for\n     * MDS mitigations.  L1D_FLUSH is not safe for MMIO mitigations.)\n     *\n     * After calculating the appropriate idle setting, simplify\n     * opt_md_clear_hvm to mean just \"should we VERW on the way into HVM\n     * guests\", so spec_ctrl_init_domain() can calculate suitable settings.\n     */\n    if ( opt_md_clear_pv || opt_md_clear_hvm || opt_fb_clear_mmio )\n        setup_force_cpu_cap(X86_FEATURE_SC_VERW_IDLE);\n    opt_md_clear_hvm &= !cpu_has_skip_l1dfl && !opt_l1d_flush;\n\n    /*\n     * Warn the user if they are on MLPDS/MFBDS-vulnerable hardware with HT\n     * active and no explicit SMT choice.\n     */\n    if ( opt_smt == -1 && cpu_has_bug_mds && hw_smt_enabled )\n        warning_add(\n            \"Booted on MLPDS/MFBDS-vulnerable hardware with SMT/Hyperthreading\\n\"\n            \"enabled.  Mitigations will not be fully effective.  Please\\n\"\n            \"choose an explicit smt=<bool> setting.  See XSA-297.\\n\");\n\n    /*\n     * Vulnerability to TAA is a little complicated to quantify.\n     *\n     * In the pipeline, it is just another way to get speculative access to\n     * stale load port, store buffer or fill buffer data, and therefore can be\n     * considered a superset of MDS (on TSX-capable parts).  On parts which\n     * predate MDS_NO, the existing VERW flushing will mitigate this\n     * sidechannel as well.\n     *\n     * On parts which contain MDS_NO, the lack of VERW flushing means that an\n     * attacker can still use TSX to target microarchitectural buffers to leak\n     * secrets.  Therefore, we consider TAA to be the set of TSX-capable parts\n     * which have MDS_NO but lack TAA_NO.\n     *\n     * Note: cpu_has_rtm (== hle) could already be hidden by `tsx=0` on the\n     *       cmdline.  MSR_TSX_CTRL will only appear on TSX-capable parts, so\n     *       we check both to spot TSX in a microcode/cmdline independent way.\n     */\n    cpu_has_bug_taa =\n        (cpu_has_rtm || cpu_has_tsx_ctrl) && cpu_has_mds_no && !cpu_has_taa_no;\n\n    /*\n     * On TAA-affected hardware, disabling TSX is the preferred mitigation, vs\n     * the MDS mitigation of disabling HT and using VERW flushing.\n     *\n     * On CPUs which advertise MDS_NO, VERW has no flushing side effect until\n     * the TSX_CTRL microcode (Nov 2019), despite the MD_CLEAR CPUID bit being\n     * advertised, and there isn't a MD_CLEAR_2 flag to use...\n     *\n     * Furthermore, the VERW flushing side effect is removed again on client\n     * parts with the Feb 2022 microcode.\n     *\n     * If we're on affected hardware, able to do something about it (which\n     * implies that VERW might work), no explicit TSX choice and traditional\n     * MDS mitigations (no-SMT, VERW) not obviosuly in use (someone might\n     * plausibly value TSX higher than Hyperthreading...), disable TSX to\n     * mitigate TAA.\n     */\n    if ( opt_tsx == -1 && cpu_has_bug_taa && cpu_has_tsx_ctrl &&\n         ((hw_smt_enabled && opt_smt) ||\n          !boot_cpu_has(X86_FEATURE_SC_VERW_IDLE)) )\n    {\n        opt_tsx = 0;\n        tsx_init();\n    }\n\n    /*\n     * On some SRBDS-affected hardware, it may be safe to relax srb-lock by\n     * default.\n     *\n     * All parts with SRBDS_CTRL suffer SSDP, the mechanism by which stale RNG\n     * data becomes available to other contexts.  To recover the data, an\n     * attacker needs to use:\n     *  - SBDS (MDS or TAA to sample the cores fill buffer)\n     *  - SBDR (Architecturally retrieve stale transaction buffer contents)\n     *  - DRPW (Architecturally latch stale fill buffer data)\n     *\n     * On MDS_NO parts, and with TAA_NO or TSX unavailable/disabled, and there\n     * is no unprivileged MMIO access, the RNG data doesn't need protecting.\n     */\n    if ( cpu_has_srbds_ctrl )\n    {\n        if ( opt_srb_lock == -1 && !opt_unpriv_mmio &&\n             cpu_has_mds_no && !cpu_has_taa_no &&\n             (!cpu_has_hle || (cpu_has_tsx_ctrl && rtm_disabled)) )\n            opt_srb_lock = 0;\n\n        set_in_mcu_opt_ctrl(MCU_OPT_CTRL_RNGDS_MITG_DIS,\n                            opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n    }\n\n    gds_calculations();\n\n    print_details(thunk);\n\n    /*\n     * If MSR_SPEC_CTRL is available, apply Xen's default setting and discard\n     * any firmware settings.  For performance reasons, when safe to do so, we\n     * delay applying non-zero settings until after dom0 has been constructed.\n     *\n     * \"when safe to do so\" is based on whether we are virtualised.  A native\n     * boot won't have any other code running in a position to mount an\n     * attack.\n     */\n    if ( has_spec_ctrl )\n    {\n        struct cpu_info *info = get_cpu_info();\n        unsigned int val;\n\n        bsp_delay_spec_ctrl = !cpu_has_hypervisor && default_xen_spec_ctrl;\n\n        /*\n         * If delaying MSR_SPEC_CTRL setup, use the same mechanism as\n         * spec_ctrl_enter_idle(), by using a shadow value of zero.\n         */\n        if ( bsp_delay_spec_ctrl )\n        {\n            info->shadow_spec_ctrl = 0;\n            barrier();\n            info->spec_ctrl_flags |= SCF_use_shadow;\n            barrier();\n        }\n\n        val = bsp_delay_spec_ctrl ? 0 : default_xen_spec_ctrl;\n\n        wrmsrl(MSR_SPEC_CTRL, val);\n        info->last_spec_ctrl = val;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -461,6 +461,8 @@\n                             opt_srb_lock ? 0 : MCU_OPT_CTRL_RNGDS_MITG_DIS);\n     }\n \n+    gds_calculations();\n+\n     print_details(thunk);\n \n     /*",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    gds_calculations();",
                ""
            ]
        }
    }
]