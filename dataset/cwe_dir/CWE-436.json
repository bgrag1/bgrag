[
    {
        "cve_id": "CVE-2019-5892",
        "func_name": "FRRouting/frr/bgp_attr_parse",
        "description": "bgpd in FRRouting FRR (aka Free Range Routing) 2.x and 3.x before 3.0.4, 4.x before 4.0.1, 5.x before 5.0.2, and 6.x before 6.0.2 (not affecting Cumulus Linux or VyOS), when ENABLE_BGP_VNC is used for Virtual Network Control, allows remote attackers to cause a denial of service (peering session flap) via attribute 255 in a BGP UPDATE packet. This occurred during Disco in January 2019 because FRR does not implement RFC 7606, and therefore the packets with 255 were considered invalid VNC data and the BGP session was closed.",
        "git_url": "https://github.com/FRRouting/frr/commit/943d595a018e69b550db08cccba1d0778a86705a",
        "commit_title": "bgpd: don't use BGP_ATTR_VNC(255) unless ENABLE_BGP_VNC_ATTR is defined",
        "commit_text": "",
        "func_before": "bgp_attr_parse_ret_t bgp_attr_parse(struct peer *peer, struct attr *attr,\n\t\t\t\t    bgp_size_t size, struct bgp_nlri *mp_update,\n\t\t\t\t    struct bgp_nlri *mp_withdraw)\n{\n\tbgp_attr_parse_ret_t ret;\n\tuint8_t flag = 0;\n\tuint8_t type = 0;\n\tbgp_size_t length;\n\tuint8_t *startp, *endp;\n\tuint8_t *attr_endp;\n\tuint8_t seen[BGP_ATTR_BITMAP_SIZE];\n\t/* we need the as4_path only until we have synthesized the as_path with\n\t * it */\n\t/* same goes for as4_aggregator */\n\tstruct aspath *as4_path = NULL;\n\tas_t as4_aggregator = 0;\n\tstruct in_addr as4_aggregator_addr = {.s_addr = 0};\n\n\t/* Initialize bitmap. */\n\tmemset(seen, 0, BGP_ATTR_BITMAP_SIZE);\n\n\t/* End pointer of BGP attribute. */\n\tendp = BGP_INPUT_PNT(peer) + size;\n\n\t/* Get attributes to the end of attribute length. */\n\twhile (BGP_INPUT_PNT(peer) < endp) {\n\t\t/* Check remaining length check.*/\n\t\tif (endp - BGP_INPUT_PNT(peer) < BGP_ATTR_MIN_LEN) {\n\t\t\t/* XXX warning: long int format, int arg (arg 5) */\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_TOO_SMALL,\n\t\t\t\t\"%s: error BGP attribute length %lu is smaller than min len\",\n\t\t\t\tpeer->host,\n\t\t\t\t(unsigned long)(endp\n\t\t\t\t\t\t- stream_pnt(BGP_INPUT(peer))));\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Fetch attribute flag and type. */\n\t\tstartp = BGP_INPUT_PNT(peer);\n\t\t/* \"The lower-order four bits of the Attribute Flags octet are\n\t\t   unused.  They MUST be zero when sent and MUST be ignored when\n\t\t   received.\" */\n\t\tflag = 0xF0 & stream_getc(BGP_INPUT(peer));\n\t\ttype = stream_getc(BGP_INPUT(peer));\n\n\t\t/* Check whether Extended-Length applies and is in bounds */\n\t\tif (CHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN)\n\t\t    && ((endp - startp) < (BGP_ATTR_MIN_LEN + 1))) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_EXT_ATTRIBUTE_TOO_SMALL,\n\t\t\t\t\"%s: Extended length set, but just %lu bytes of attr header\",\n\t\t\t\tpeer->host,\n\t\t\t\t(unsigned long)(endp\n\t\t\t\t\t\t- stream_pnt(BGP_INPUT(peer))));\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Check extended attribue length bit. */\n\t\tif (CHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN))\n\t\t\tlength = stream_getw(BGP_INPUT(peer));\n\t\telse\n\t\t\tlength = stream_getc(BGP_INPUT(peer));\n\n\t\t/* If any attribute appears more than once in the UPDATE\n\t\t   message, then the Error Subcode is set to Malformed Attribute\n\t\t   List. */\n\n\t\tif (CHECK_BITMAP(seen, type)) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_REPEATED,\n\t\t\t\t\"%s: error BGP attribute type %d appears twice in a message\",\n\t\t\t\tpeer->host, type);\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Set type to bitmap to check duplicate attribute.  `type' is\n\t\t   unsigned char so it never overflow bitmap range. */\n\n\t\tSET_BITMAP(seen, type);\n\n\t\t/* Overflow check. */\n\t\tattr_endp = BGP_INPUT_PNT(peer) + length;\n\n\t\tif (attr_endp > endp) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_TOO_LARGE,\n\t\t\t\t\"%s: BGP type %d length %d is too large, attribute total length is %d.  attr_endp is %p.  endp is %p\",\n\t\t\t\tpeer->host, type, length, size, attr_endp,\n\t\t\t\tendp);\n\t\t\t/*\n\t\t\t * RFC 4271 6.3\n\t\t\t * If any recognized attribute has an Attribute\n\t\t\t * Length that conflicts with the expected length\n\t\t\t * (based on the attribute type code), then the\n\t\t\t * Error Subcode MUST be set to Attribute Length\n\t\t\t * Error.  The Data field MUST contain the erroneous\n\t\t\t * attribute (type, length, and value).\n\t\t\t * ----------\n\t\t\t * We do not currently have a good way to determine the\n\t\t\t * length of the attribute independent of the length\n\t\t\t * received in the message. Instead we send the\n\t\t\t * minimum between the amount of data we have and the\n\t\t\t * amount specified by the attribute length field.\n\t\t\t *\n\t\t\t * Instead of directly passing in the packet buffer and\n\t\t\t * offset we use the stream_get* functions to read into\n\t\t\t * a stack buffer, since they perform bounds checking\n\t\t\t * and we are working with untrusted data.\n\t\t\t */\n\t\t\tunsigned char ndata[BGP_MAX_PACKET_SIZE];\n\t\t\tmemset(ndata, 0x00, sizeof(ndata));\n\t\t\tsize_t lfl =\n\t\t\t\tCHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN) ? 2 : 1;\n\t\t\t/* Rewind to end of flag field */\n\t\t\tstream_forward_getp(BGP_INPUT(peer), -(1 + lfl));\n\t\t\t/* Type */\n\t\t\tstream_get(&ndata[0], BGP_INPUT(peer), 1);\n\t\t\t/* Length */\n\t\t\tstream_get(&ndata[1], BGP_INPUT(peer), lfl);\n\t\t\t/* Value */\n\t\t\tsize_t atl = attr_endp - startp;\n\t\t\tsize_t ndl = MIN(atl, STREAM_READABLE(BGP_INPUT(peer)));\n\t\t\tstream_get(&ndata[lfl + 1], BGP_INPUT(peer), ndl);\n\n\t\t\tbgp_notify_send_with_data(\n\t\t\t\tpeer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR, ndata,\n\t\t\t\tndl + lfl + 1);\n\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\tstruct bgp_attr_parser_args attr_args = {\n\t\t\t.peer = peer,\n\t\t\t.length = length,\n\t\t\t.attr = attr,\n\t\t\t.type = type,\n\t\t\t.flags = flag,\n\t\t\t.startp = startp,\n\t\t\t.total = attr_endp - startp,\n\t\t};\n\n\n\t\t/* If any recognized attribute has Attribute Flags that conflict\n\t\t   with the Attribute Type Code, then the Error Subcode is set\n\t\t   to\n\t\t   Attribute Flags Error.  The Data field contains the erroneous\n\t\t   attribute (type, length and value). */\n\t\tif (bgp_attr_flag_invalid(&attr_args)) {\n\t\t\tret = bgp_attr_malformed(\n\t\t\t\t&attr_args, BGP_NOTIFY_UPDATE_ATTR_FLAG_ERR,\n\t\t\t\tattr_args.total);\n\t\t\tif (ret == BGP_ATTR_PARSE_PROCEED)\n\t\t\t\tcontinue;\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* OK check attribute and store it's value. */\n\t\tswitch (type) {\n\t\tcase BGP_ATTR_ORIGIN:\n\t\t\tret = bgp_attr_origin(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS_PATH:\n\t\t\tret = bgp_attr_aspath(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS4_PATH:\n\t\t\tret = bgp_attr_as4_path(&attr_args, &as4_path);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_NEXT_HOP:\n\t\t\tret = bgp_attr_nexthop(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MULTI_EXIT_DISC:\n\t\t\tret = bgp_attr_med(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_LOCAL_PREF:\n\t\t\tret = bgp_attr_local_pref(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_ATOMIC_AGGREGATE:\n\t\t\tret = bgp_attr_atomic(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AGGREGATOR:\n\t\t\tret = bgp_attr_aggregator(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS4_AGGREGATOR:\n\t\t\tret = bgp_attr_as4_aggregator(&attr_args,\n\t\t\t\t\t\t      &as4_aggregator,\n\t\t\t\t\t\t      &as4_aggregator_addr);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_COMMUNITIES:\n\t\t\tret = bgp_attr_community(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_LARGE_COMMUNITIES:\n\t\t\tret = bgp_attr_large_community(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_ORIGINATOR_ID:\n\t\t\tret = bgp_attr_originator_id(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_CLUSTER_LIST:\n\t\t\tret = bgp_attr_cluster_list(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MP_REACH_NLRI:\n\t\t\tret = bgp_mp_reach_parse(&attr_args, mp_update);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MP_UNREACH_NLRI:\n\t\t\tret = bgp_mp_unreach_parse(&attr_args, mp_withdraw);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_EXT_COMMUNITIES:\n\t\t\tret = bgp_attr_ext_communities(&attr_args);\n\t\t\tbreak;\n#if ENABLE_BGP_VNC\n\t\tcase BGP_ATTR_VNC:\n#endif\n\t\tcase BGP_ATTR_ENCAP:\n\t\t\tret = bgp_attr_encap(type, peer, length, attr, flag,\n\t\t\t\t\t     startp);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_PREFIX_SID:\n\t\t\tret = bgp_attr_prefix_sid(length,\n\t\t\t\t\t\t  &attr_args, mp_update);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_PMSI_TUNNEL:\n\t\t\tret = bgp_attr_pmsi_tunnel(&attr_args);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = bgp_attr_unknown(&attr_args);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret == BGP_ATTR_PARSE_ERROR_NOTIFYPLS) {\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\t\tret = BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\tif (ret == BGP_ATTR_PARSE_EOR) {\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* If hard error occurred immediately return to the caller. */\n\t\tif (ret == BGP_ATTR_PARSE_ERROR) {\n\t\t\tflog_warn(EC_BGP_ATTRIBUTE_PARSE_ERROR,\n\t\t\t\t  \"%s: Attribute %s, parse error\", peer->host,\n\t\t\t\t  lookup_msg(attr_str, type, NULL));\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\t\tif (ret == BGP_ATTR_PARSE_WITHDRAW) {\n\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_PARSE_WITHDRAW,\n\t\t\t\t\"%s: Attribute %s, parse error - treating as withdrawal\",\n\t\t\t\tpeer->host, lookup_msg(attr_str, type, NULL));\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* Check the fetched length. */\n\t\tif (BGP_INPUT_PNT(peer) != attr_endp) {\n\t\t\tflog_warn(EC_BGP_ATTRIBUTE_FETCH_ERROR,\n\t\t\t\t  \"%s: BGP attribute %s, fetch error\",\n\t\t\t\t  peer->host, lookup_msg(attr_str, type, NULL));\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\t}\n\n\t/* Check final read pointer is same as end pointer. */\n\tif (BGP_INPUT_PNT(peer) != endp) {\n\t\tflog_warn(EC_BGP_ATTRIBUTES_MISMATCH,\n\t\t\t  \"%s: BGP attribute %s, length mismatch\", peer->host,\n\t\t\t  lookup_msg(attr_str, type, NULL));\n\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn BGP_ATTR_PARSE_ERROR;\n\t}\n\n\t/* Check all mandatory well-known attributes are present */\n\tif ((ret = bgp_attr_check(peer, attr)) < 0) {\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * At this place we can see whether we got AS4_PATH and/or\n\t * AS4_AGGREGATOR from a 16Bit peer and act accordingly.\n\t * We can not do this before we've read all attributes because\n\t * the as4 handling does not say whether AS4_PATH has to be sent\n\t * after AS_PATH or not - and when AS4_AGGREGATOR will be send\n\t * in relationship to AGGREGATOR.\n\t * So, to be defensive, we are not relying on any order and read\n\t * all attributes first, including these 32bit ones, and now,\n\t * afterwards, we look what and if something is to be done for as4.\n\t *\n\t * It is possible to not have AS_PATH, e.g. GR EoR and sole\n\t * MP_UNREACH_NLRI.\n\t */\n\t/* actually... this doesn't ever return failure currently, but\n\t * better safe than sorry */\n\tif (CHECK_FLAG(attr->flag, ATTR_FLAG_BIT(BGP_ATTR_AS_PATH))\n\t    && bgp_attr_munge_as4_attrs(peer, attr, as4_path, as4_aggregator,\n\t\t\t\t\t&as4_aggregator_addr)) {\n\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn BGP_ATTR_PARSE_ERROR;\n\t}\n\n\t/* At this stage, we have done all fiddling with as4, and the\n\t * resulting info is in attr->aggregator resp. attr->aspath\n\t * so we can chuck as4_aggregator and as4_path alltogether in\n\t * order to save memory\n\t */\n\tif (as4_path) {\n\t\taspath_unintern(&as4_path); /* unintern - it is in the hash */\n\t\t/* The flag that we got this is still there, but that does not\n\t\t * do any trouble\n\t\t */\n\t}\n\t/*\n\t * The \"rest\" of the code does nothing with as4_aggregator.\n\t * there is no memory attached specifically which is not part\n\t * of the attr.\n\t * so ignoring just means do nothing.\n\t */\n\t/*\n\t * Finally do the checks on the aspath we did not do yet\n\t * because we waited for a potentially synthesized aspath.\n\t */\n\tif (attr->flag & (ATTR_FLAG_BIT(BGP_ATTR_AS_PATH))) {\n\t\tret = bgp_attr_aspath_check(peer, attr);\n\t\tif (ret != BGP_ATTR_PARSE_PROCEED)\n\t\t\treturn ret;\n\t}\n\t/* Finally intern unknown attribute. */\n\tif (attr->transit)\n\t\tattr->transit = transit_intern(attr->transit);\n\tif (attr->encap_subtlvs)\n\t\tattr->encap_subtlvs =\n\t\t\tencap_intern(attr->encap_subtlvs, ENCAP_SUBTLV_TYPE);\n#if ENABLE_BGP_VNC\n\tif (attr->vnc_subtlvs)\n\t\tattr->vnc_subtlvs =\n\t\t\tencap_intern(attr->vnc_subtlvs, VNC_SUBTLV_TYPE);\n#endif\n\n\treturn BGP_ATTR_PARSE_PROCEED;\n}",
        "func": "bgp_attr_parse_ret_t bgp_attr_parse(struct peer *peer, struct attr *attr,\n\t\t\t\t    bgp_size_t size, struct bgp_nlri *mp_update,\n\t\t\t\t    struct bgp_nlri *mp_withdraw)\n{\n\tbgp_attr_parse_ret_t ret;\n\tuint8_t flag = 0;\n\tuint8_t type = 0;\n\tbgp_size_t length;\n\tuint8_t *startp, *endp;\n\tuint8_t *attr_endp;\n\tuint8_t seen[BGP_ATTR_BITMAP_SIZE];\n\t/* we need the as4_path only until we have synthesized the as_path with\n\t * it */\n\t/* same goes for as4_aggregator */\n\tstruct aspath *as4_path = NULL;\n\tas_t as4_aggregator = 0;\n\tstruct in_addr as4_aggregator_addr = {.s_addr = 0};\n\n\t/* Initialize bitmap. */\n\tmemset(seen, 0, BGP_ATTR_BITMAP_SIZE);\n\n\t/* End pointer of BGP attribute. */\n\tendp = BGP_INPUT_PNT(peer) + size;\n\n\t/* Get attributes to the end of attribute length. */\n\twhile (BGP_INPUT_PNT(peer) < endp) {\n\t\t/* Check remaining length check.*/\n\t\tif (endp - BGP_INPUT_PNT(peer) < BGP_ATTR_MIN_LEN) {\n\t\t\t/* XXX warning: long int format, int arg (arg 5) */\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_TOO_SMALL,\n\t\t\t\t\"%s: error BGP attribute length %lu is smaller than min len\",\n\t\t\t\tpeer->host,\n\t\t\t\t(unsigned long)(endp\n\t\t\t\t\t\t- stream_pnt(BGP_INPUT(peer))));\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Fetch attribute flag and type. */\n\t\tstartp = BGP_INPUT_PNT(peer);\n\t\t/* \"The lower-order four bits of the Attribute Flags octet are\n\t\t   unused.  They MUST be zero when sent and MUST be ignored when\n\t\t   received.\" */\n\t\tflag = 0xF0 & stream_getc(BGP_INPUT(peer));\n\t\ttype = stream_getc(BGP_INPUT(peer));\n\n\t\t/* Check whether Extended-Length applies and is in bounds */\n\t\tif (CHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN)\n\t\t    && ((endp - startp) < (BGP_ATTR_MIN_LEN + 1))) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_EXT_ATTRIBUTE_TOO_SMALL,\n\t\t\t\t\"%s: Extended length set, but just %lu bytes of attr header\",\n\t\t\t\tpeer->host,\n\t\t\t\t(unsigned long)(endp\n\t\t\t\t\t\t- stream_pnt(BGP_INPUT(peer))));\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Check extended attribue length bit. */\n\t\tif (CHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN))\n\t\t\tlength = stream_getw(BGP_INPUT(peer));\n\t\telse\n\t\t\tlength = stream_getc(BGP_INPUT(peer));\n\n\t\t/* If any attribute appears more than once in the UPDATE\n\t\t   message, then the Error Subcode is set to Malformed Attribute\n\t\t   List. */\n\n\t\tif (CHECK_BITMAP(seen, type)) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_REPEATED,\n\t\t\t\t\"%s: error BGP attribute type %d appears twice in a message\",\n\t\t\t\tpeer->host, type);\n\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\t/* Set type to bitmap to check duplicate attribute.  `type' is\n\t\t   unsigned char so it never overflow bitmap range. */\n\n\t\tSET_BITMAP(seen, type);\n\n\t\t/* Overflow check. */\n\t\tattr_endp = BGP_INPUT_PNT(peer) + length;\n\n\t\tif (attr_endp > endp) {\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_TOO_LARGE,\n\t\t\t\t\"%s: BGP type %d length %d is too large, attribute total length is %d.  attr_endp is %p.  endp is %p\",\n\t\t\t\tpeer->host, type, length, size, attr_endp,\n\t\t\t\tendp);\n\t\t\t/*\n\t\t\t * RFC 4271 6.3\n\t\t\t * If any recognized attribute has an Attribute\n\t\t\t * Length that conflicts with the expected length\n\t\t\t * (based on the attribute type code), then the\n\t\t\t * Error Subcode MUST be set to Attribute Length\n\t\t\t * Error.  The Data field MUST contain the erroneous\n\t\t\t * attribute (type, length, and value).\n\t\t\t * ----------\n\t\t\t * We do not currently have a good way to determine the\n\t\t\t * length of the attribute independent of the length\n\t\t\t * received in the message. Instead we send the\n\t\t\t * minimum between the amount of data we have and the\n\t\t\t * amount specified by the attribute length field.\n\t\t\t *\n\t\t\t * Instead of directly passing in the packet buffer and\n\t\t\t * offset we use the stream_get* functions to read into\n\t\t\t * a stack buffer, since they perform bounds checking\n\t\t\t * and we are working with untrusted data.\n\t\t\t */\n\t\t\tunsigned char ndata[BGP_MAX_PACKET_SIZE];\n\t\t\tmemset(ndata, 0x00, sizeof(ndata));\n\t\t\tsize_t lfl =\n\t\t\t\tCHECK_FLAG(flag, BGP_ATTR_FLAG_EXTLEN) ? 2 : 1;\n\t\t\t/* Rewind to end of flag field */\n\t\t\tstream_forward_getp(BGP_INPUT(peer), -(1 + lfl));\n\t\t\t/* Type */\n\t\t\tstream_get(&ndata[0], BGP_INPUT(peer), 1);\n\t\t\t/* Length */\n\t\t\tstream_get(&ndata[1], BGP_INPUT(peer), lfl);\n\t\t\t/* Value */\n\t\t\tsize_t atl = attr_endp - startp;\n\t\t\tsize_t ndl = MIN(atl, STREAM_READABLE(BGP_INPUT(peer)));\n\t\t\tstream_get(&ndata[lfl + 1], BGP_INPUT(peer), ndl);\n\n\t\t\tbgp_notify_send_with_data(\n\t\t\t\tpeer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR, ndata,\n\t\t\t\tndl + lfl + 1);\n\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\tstruct bgp_attr_parser_args attr_args = {\n\t\t\t.peer = peer,\n\t\t\t.length = length,\n\t\t\t.attr = attr,\n\t\t\t.type = type,\n\t\t\t.flags = flag,\n\t\t\t.startp = startp,\n\t\t\t.total = attr_endp - startp,\n\t\t};\n\n\n\t\t/* If any recognized attribute has Attribute Flags that conflict\n\t\t   with the Attribute Type Code, then the Error Subcode is set\n\t\t   to\n\t\t   Attribute Flags Error.  The Data field contains the erroneous\n\t\t   attribute (type, length and value). */\n\t\tif (bgp_attr_flag_invalid(&attr_args)) {\n\t\t\tret = bgp_attr_malformed(\n\t\t\t\t&attr_args, BGP_NOTIFY_UPDATE_ATTR_FLAG_ERR,\n\t\t\t\tattr_args.total);\n\t\t\tif (ret == BGP_ATTR_PARSE_PROCEED)\n\t\t\t\tcontinue;\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* OK check attribute and store it's value. */\n\t\tswitch (type) {\n\t\tcase BGP_ATTR_ORIGIN:\n\t\t\tret = bgp_attr_origin(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS_PATH:\n\t\t\tret = bgp_attr_aspath(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS4_PATH:\n\t\t\tret = bgp_attr_as4_path(&attr_args, &as4_path);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_NEXT_HOP:\n\t\t\tret = bgp_attr_nexthop(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MULTI_EXIT_DISC:\n\t\t\tret = bgp_attr_med(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_LOCAL_PREF:\n\t\t\tret = bgp_attr_local_pref(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_ATOMIC_AGGREGATE:\n\t\t\tret = bgp_attr_atomic(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AGGREGATOR:\n\t\t\tret = bgp_attr_aggregator(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_AS4_AGGREGATOR:\n\t\t\tret = bgp_attr_as4_aggregator(&attr_args,\n\t\t\t\t\t\t      &as4_aggregator,\n\t\t\t\t\t\t      &as4_aggregator_addr);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_COMMUNITIES:\n\t\t\tret = bgp_attr_community(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_LARGE_COMMUNITIES:\n\t\t\tret = bgp_attr_large_community(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_ORIGINATOR_ID:\n\t\t\tret = bgp_attr_originator_id(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_CLUSTER_LIST:\n\t\t\tret = bgp_attr_cluster_list(&attr_args);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MP_REACH_NLRI:\n\t\t\tret = bgp_mp_reach_parse(&attr_args, mp_update);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_MP_UNREACH_NLRI:\n\t\t\tret = bgp_mp_unreach_parse(&attr_args, mp_withdraw);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_EXT_COMMUNITIES:\n\t\t\tret = bgp_attr_ext_communities(&attr_args);\n\t\t\tbreak;\n#if ENABLE_BGP_VNC_ATTR\n\t\tcase BGP_ATTR_VNC:\n#endif\n\t\tcase BGP_ATTR_ENCAP:\n\t\t\tret = bgp_attr_encap(type, peer, length, attr, flag,\n\t\t\t\t\t     startp);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_PREFIX_SID:\n\t\t\tret = bgp_attr_prefix_sid(length,\n\t\t\t\t\t\t  &attr_args, mp_update);\n\t\t\tbreak;\n\t\tcase BGP_ATTR_PMSI_TUNNEL:\n\t\t\tret = bgp_attr_pmsi_tunnel(&attr_args);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = bgp_attr_unknown(&attr_args);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret == BGP_ATTR_PARSE_ERROR_NOTIFYPLS) {\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\t\tret = BGP_ATTR_PARSE_ERROR;\n\t\t}\n\n\t\tif (ret == BGP_ATTR_PARSE_EOR) {\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* If hard error occurred immediately return to the caller. */\n\t\tif (ret == BGP_ATTR_PARSE_ERROR) {\n\t\t\tflog_warn(EC_BGP_ATTRIBUTE_PARSE_ERROR,\n\t\t\t\t  \"%s: Attribute %s, parse error\", peer->host,\n\t\t\t\t  lookup_msg(attr_str, type, NULL));\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\t\tif (ret == BGP_ATTR_PARSE_WITHDRAW) {\n\n\t\t\tflog_warn(\n\t\t\t\tEC_BGP_ATTRIBUTE_PARSE_WITHDRAW,\n\t\t\t\t\"%s: Attribute %s, parse error - treating as withdrawal\",\n\t\t\t\tpeer->host, lookup_msg(attr_str, type, NULL));\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn ret;\n\t\t}\n\n\t\t/* Check the fetched length. */\n\t\tif (BGP_INPUT_PNT(peer) != attr_endp) {\n\t\t\tflog_warn(EC_BGP_ATTRIBUTE_FETCH_ERROR,\n\t\t\t\t  \"%s: BGP attribute %s, fetch error\",\n\t\t\t\t  peer->host, lookup_msg(attr_str, type, NULL));\n\t\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\t\tif (as4_path)\n\t\t\t\taspath_unintern(&as4_path);\n\t\t\treturn BGP_ATTR_PARSE_ERROR;\n\t\t}\n\t}\n\n\t/* Check final read pointer is same as end pointer. */\n\tif (BGP_INPUT_PNT(peer) != endp) {\n\t\tflog_warn(EC_BGP_ATTRIBUTES_MISMATCH,\n\t\t\t  \"%s: BGP attribute %s, length mismatch\", peer->host,\n\t\t\t  lookup_msg(attr_str, type, NULL));\n\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_ATTR_LENG_ERR);\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn BGP_ATTR_PARSE_ERROR;\n\t}\n\n\t/* Check all mandatory well-known attributes are present */\n\tif ((ret = bgp_attr_check(peer, attr)) < 0) {\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * At this place we can see whether we got AS4_PATH and/or\n\t * AS4_AGGREGATOR from a 16Bit peer and act accordingly.\n\t * We can not do this before we've read all attributes because\n\t * the as4 handling does not say whether AS4_PATH has to be sent\n\t * after AS_PATH or not - and when AS4_AGGREGATOR will be send\n\t * in relationship to AGGREGATOR.\n\t * So, to be defensive, we are not relying on any order and read\n\t * all attributes first, including these 32bit ones, and now,\n\t * afterwards, we look what and if something is to be done for as4.\n\t *\n\t * It is possible to not have AS_PATH, e.g. GR EoR and sole\n\t * MP_UNREACH_NLRI.\n\t */\n\t/* actually... this doesn't ever return failure currently, but\n\t * better safe than sorry */\n\tif (CHECK_FLAG(attr->flag, ATTR_FLAG_BIT(BGP_ATTR_AS_PATH))\n\t    && bgp_attr_munge_as4_attrs(peer, attr, as4_path, as4_aggregator,\n\t\t\t\t\t&as4_aggregator_addr)) {\n\t\tbgp_notify_send(peer, BGP_NOTIFY_UPDATE_ERR,\n\t\t\t\tBGP_NOTIFY_UPDATE_MAL_ATTR);\n\t\tif (as4_path)\n\t\t\taspath_unintern(&as4_path);\n\t\treturn BGP_ATTR_PARSE_ERROR;\n\t}\n\n\t/* At this stage, we have done all fiddling with as4, and the\n\t * resulting info is in attr->aggregator resp. attr->aspath\n\t * so we can chuck as4_aggregator and as4_path alltogether in\n\t * order to save memory\n\t */\n\tif (as4_path) {\n\t\taspath_unintern(&as4_path); /* unintern - it is in the hash */\n\t\t/* The flag that we got this is still there, but that does not\n\t\t * do any trouble\n\t\t */\n\t}\n\t/*\n\t * The \"rest\" of the code does nothing with as4_aggregator.\n\t * there is no memory attached specifically which is not part\n\t * of the attr.\n\t * so ignoring just means do nothing.\n\t */\n\t/*\n\t * Finally do the checks on the aspath we did not do yet\n\t * because we waited for a potentially synthesized aspath.\n\t */\n\tif (attr->flag & (ATTR_FLAG_BIT(BGP_ATTR_AS_PATH))) {\n\t\tret = bgp_attr_aspath_check(peer, attr);\n\t\tif (ret != BGP_ATTR_PARSE_PROCEED)\n\t\t\treturn ret;\n\t}\n\t/* Finally intern unknown attribute. */\n\tif (attr->transit)\n\t\tattr->transit = transit_intern(attr->transit);\n\tif (attr->encap_subtlvs)\n\t\tattr->encap_subtlvs =\n\t\t\tencap_intern(attr->encap_subtlvs, ENCAP_SUBTLV_TYPE);\n#if ENABLE_BGP_VNC\n\tif (attr->vnc_subtlvs)\n\t\tattr->vnc_subtlvs =\n\t\t\tencap_intern(attr->vnc_subtlvs, VNC_SUBTLV_TYPE);\n#endif\n\n\treturn BGP_ATTR_PARSE_PROCEED;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -217,7 +217,7 @@\n \t\tcase BGP_ATTR_EXT_COMMUNITIES:\n \t\t\tret = bgp_attr_ext_communities(&attr_args);\n \t\t\tbreak;\n-#if ENABLE_BGP_VNC\n+#if ENABLE_BGP_VNC_ATTR\n \t\tcase BGP_ATTR_VNC:\n #endif\n \t\tcase BGP_ATTR_ENCAP:",
        "diff_line_info": {
            "deleted_lines": [
                "#if ENABLE_BGP_VNC"
            ],
            "added_lines": [
                "#if ENABLE_BGP_VNC_ATTR"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-5892",
        "func_name": "FRRouting/frr/bgp_packet_attribute",
        "description": "bgpd in FRRouting FRR (aka Free Range Routing) 2.x and 3.x before 3.0.4, 4.x before 4.0.1, 5.x before 5.0.2, and 6.x before 6.0.2 (not affecting Cumulus Linux or VyOS), when ENABLE_BGP_VNC is used for Virtual Network Control, allows remote attackers to cause a denial of service (peering session flap) via attribute 255 in a BGP UPDATE packet. This occurred during Disco in January 2019 because FRR does not implement RFC 7606, and therefore the packets with 255 were considered invalid VNC data and the BGP session was closed.",
        "git_url": "https://github.com/FRRouting/frr/commit/943d595a018e69b550db08cccba1d0778a86705a",
        "commit_title": "bgpd: don't use BGP_ATTR_VNC(255) unless ENABLE_BGP_VNC_ATTR is defined",
        "commit_text": "",
        "func_before": "bgp_size_t bgp_packet_attribute(struct bgp *bgp, struct peer *peer,\n\t\t\t\tstruct stream *s, struct attr *attr,\n\t\t\t\tstruct bpacket_attr_vec_arr *vecarr,\n\t\t\t\tstruct prefix *p, afi_t afi, safi_t safi,\n\t\t\t\tstruct peer *from, struct prefix_rd *prd,\n\t\t\t\tmpls_label_t *label, uint32_t num_labels,\n\t\t\t\tint addpath_encode, uint32_t addpath_tx_id)\n{\n\tsize_t cp;\n\tsize_t aspath_sizep;\n\tstruct aspath *aspath;\n\tint send_as4_path = 0;\n\tint send_as4_aggregator = 0;\n\tint use32bit = (CHECK_FLAG(peer->cap, PEER_CAP_AS4_RCV)) ? 1 : 0;\n\n\tif (!bgp)\n\t\tbgp = peer->bgp;\n\n\t/* Remember current pointer. */\n\tcp = stream_get_endp(s);\n\n\tif (p\n\t    && !((afi == AFI_IP && safi == SAFI_UNICAST)\n\t\t && !peer_cap_enhe(peer, afi, safi))) {\n\t\tsize_t mpattrlen_pos = 0;\n\n\t\tmpattrlen_pos = bgp_packet_mpattr_start(s, peer, afi, safi,\n\t\t\t\t\t\t\tvecarr, attr);\n\t\tbgp_packet_mpattr_prefix(s, afi, safi, p, prd, label,\n\t\t\t\t\t num_labels, addpath_encode,\n\t\t\t\t\t addpath_tx_id, attr);\n\t\tbgp_packet_mpattr_end(s, mpattrlen_pos);\n\t}\n\n\t/* Origin attribute. */\n\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\tstream_putc(s, BGP_ATTR_ORIGIN);\n\tstream_putc(s, 1);\n\tstream_putc(s, attr->origin);\n\n\t/* AS path attribute. */\n\n\t/* If remote-peer is EBGP */\n\tif (peer->sort == BGP_PEER_EBGP\n\t    && (!CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t\t    PEER_FLAG_AS_PATH_UNCHANGED)\n\t\t|| attr->aspath->segments == NULL)\n\t    && (!CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t\t    PEER_FLAG_RSERVER_CLIENT))) {\n\t\taspath = aspath_dup(attr->aspath);\n\n\t\t/* Even though we may not be configured for confederations we\n\t\t * may have\n\t\t * RXed an AS_PATH with AS_CONFED_SEQUENCE or AS_CONFED_SET */\n\t\taspath = aspath_delete_confed_seq(aspath);\n\n\t\tif (CHECK_FLAG(bgp->config, BGP_CONFIG_CONFEDERATION)) {\n\t\t\t/* Stuff our path CONFED_ID on the front */\n\t\t\taspath = aspath_add_seq(aspath, bgp->confed_id);\n\t\t} else {\n\t\t\tif (peer->change_local_as) {\n\t\t\t\t/* If replace-as is specified, we only use the\n\t\t\t\t   change_local_as when\n\t\t\t\t   advertising routes. */\n\t\t\t\tif (!CHECK_FLAG(\n\t\t\t\t\t    peer->flags,\n\t\t\t\t\t    PEER_FLAG_LOCAL_AS_REPLACE_AS)) {\n\t\t\t\t\taspath = aspath_add_seq(aspath,\n\t\t\t\t\t\t\t\tpeer->local_as);\n\t\t\t\t}\n\t\t\t\taspath = aspath_add_seq(aspath,\n\t\t\t\t\t\t\tpeer->change_local_as);\n\t\t\t} else {\n\t\t\t\taspath = aspath_add_seq(aspath, peer->local_as);\n\t\t\t}\n\t\t}\n\t} else if (peer->sort == BGP_PEER_CONFED) {\n\t\t/* A confed member, so we need to do the AS_CONFED_SEQUENCE\n\t\t * thing */\n\t\taspath = aspath_dup(attr->aspath);\n\t\taspath = aspath_add_confed_seq(aspath, peer->local_as);\n\t} else\n\t\taspath = attr->aspath;\n\n\t/* If peer is not AS4 capable, then:\n\t * - send the created AS_PATH out as AS4_PATH (optional, transitive),\n\t *   but ensure that no AS_CONFED_SEQUENCE and AS_CONFED_SET path\n\t * segment\n\t *   types are in it (i.e. exclude them if they are there)\n\t *   AND do this only if there is at least one asnum > 65535 in the\n\t * path!\n\t * - send an AS_PATH out, but put 16Bit ASnums in it, not 32bit, and\n\t * change\n\t *   all ASnums > 65535 to BGP_AS_TRANS\n\t */\n\n\tstream_putc(s, BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_EXTLEN);\n\tstream_putc(s, BGP_ATTR_AS_PATH);\n\taspath_sizep = stream_get_endp(s);\n\tstream_putw(s, 0);\n\tstream_putw_at(s, aspath_sizep, aspath_put(s, aspath, use32bit));\n\n\t/* OLD session may need NEW_AS_PATH sent, if there are 4-byte ASNs\n\t * in the path\n\t */\n\tif (!use32bit && aspath_has_as4(aspath))\n\t\tsend_as4_path =\n\t\t\t1; /* we'll do this later, at the correct place */\n\n\t/* Nexthop attribute. */\n\tif (afi == AFI_IP && safi == SAFI_UNICAST\n\t    && !peer_cap_enhe(peer, afi, safi)) {\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_NEXT_HOP)) {\n\t\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_NEXT_HOP);\n\t\t\tbpacket_attr_vec_arr_set_vec(vecarr, BGP_ATTR_VEC_NH, s,\n\t\t\t\t\t\t     attr);\n\t\t\tstream_putc(s, 4);\n\t\t\tstream_put_ipv4(s, attr->nexthop.s_addr);\n\t\t} else if (peer_cap_enhe(from, afi, safi)) {\n\t\t\t/*\n\t\t\t * Likely this is the case when an IPv4 prefix was\n\t\t\t * received with\n\t\t\t * Extended Next-hop capability and now being advertised\n\t\t\t * to\n\t\t\t * non-ENHE peers.\n\t\t\t * Setting the mandatory (ipv4) next-hop attribute here\n\t\t\t * to enable\n\t\t\t * implicit next-hop self with correct (ipv4 address\n\t\t\t * family).\n\t\t\t */\n\t\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_NEXT_HOP);\n\t\t\tbpacket_attr_vec_arr_set_vec(vecarr, BGP_ATTR_VEC_NH, s,\n\t\t\t\t\t\t     NULL);\n\t\t\tstream_putc(s, 4);\n\t\t\tstream_put_ipv4(s, 0);\n\t\t}\n\t}\n\n\t/* MED attribute. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_MULTI_EXIT_DISC)\n\t    || bgp->maxmed_active) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_MULTI_EXIT_DISC);\n\t\tstream_putc(s, 4);\n\t\tstream_putl(s, (bgp->maxmed_active ? bgp->maxmed_value\n\t\t\t\t\t\t   : attr->med));\n\t}\n\n\t/* Local preference. */\n\tif (peer->sort == BGP_PEER_IBGP || peer->sort == BGP_PEER_CONFED) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_LOCAL_PREF);\n\t\tstream_putc(s, 4);\n\t\tstream_putl(s, attr->local_pref);\n\t}\n\n\t/* Atomic aggregate. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_ATOMIC_AGGREGATE)) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_ATOMIC_AGGREGATE);\n\t\tstream_putc(s, 0);\n\t}\n\n\t/* Aggregator. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_AGGREGATOR)) {\n\t\t/* Common to BGP_ATTR_AGGREGATOR, regardless of ASN size */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_AGGREGATOR);\n\n\t\tif (use32bit) {\n\t\t\t/* AS4 capable peer */\n\t\t\tstream_putc(s, 8);\n\t\t\tstream_putl(s, attr->aggregator_as);\n\t\t} else {\n\t\t\t/* 2-byte AS peer */\n\t\t\tstream_putc(s, 6);\n\n\t\t\t/* Is ASN representable in 2-bytes? Or must AS_TRANS be\n\t\t\t * used? */\n\t\t\tif (attr->aggregator_as > 65535) {\n\t\t\t\tstream_putw(s, BGP_AS_TRANS);\n\n\t\t\t\t/* we have to send AS4_AGGREGATOR, too.\n\t\t\t\t * we'll do that later in order to send\n\t\t\t\t * attributes in ascending\n\t\t\t\t * order.\n\t\t\t\t */\n\t\t\t\tsend_as4_aggregator = 1;\n\t\t\t} else\n\t\t\t\tstream_putw(s, (uint16_t)attr->aggregator_as);\n\t\t}\n\t\tstream_put_ipv4(s, attr->aggregator_addr.s_addr);\n\t}\n\n\t/* Community attribute. */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi], PEER_FLAG_SEND_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_COMMUNITIES))) {\n\t\tif (attr->community->size * 4 > 255) {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\tstream_putc(s, BGP_ATTR_COMMUNITIES);\n\t\t\tstream_putw(s, attr->community->size * 4);\n\t\t} else {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_COMMUNITIES);\n\t\t\tstream_putc(s, attr->community->size * 4);\n\t\t}\n\t\tstream_put(s, attr->community->val, attr->community->size * 4);\n\t}\n\n\t/*\n\t * Large Community attribute.\n\t */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t       PEER_FLAG_SEND_LARGE_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_LARGE_COMMUNITIES))) {\n\t\tif (lcom_length(attr->lcommunity) > 255) {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\tstream_putc(s, BGP_ATTR_LARGE_COMMUNITIES);\n\t\t\tstream_putw(s, lcom_length(attr->lcommunity));\n\t\t} else {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_LARGE_COMMUNITIES);\n\t\t\tstream_putc(s, lcom_length(attr->lcommunity));\n\t\t}\n\t\tstream_put(s, attr->lcommunity->val,\n\t\t\t   lcom_length(attr->lcommunity));\n\t}\n\n\t/* Route Reflector. */\n\tif (peer->sort == BGP_PEER_IBGP && from\n\t    && from->sort == BGP_PEER_IBGP) {\n\t\t/* Originator ID. */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_ORIGINATOR_ID);\n\t\tstream_putc(s, 4);\n\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_ORIGINATOR_ID))\n\t\t\tstream_put_in_addr(s, &attr->originator_id);\n\t\telse\n\t\t\tstream_put_in_addr(s, &from->remote_id);\n\n\t\t/* Cluster list. */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_CLUSTER_LIST);\n\n\t\tif (attr->cluster) {\n\t\t\tstream_putc(s, attr->cluster->length + 4);\n\t\t\t/* If this peer configuration's parent BGP has\n\t\t\t * cluster_id. */\n\t\t\tif (bgp->config & BGP_CONFIG_CLUSTER_ID)\n\t\t\t\tstream_put_in_addr(s, &bgp->cluster_id);\n\t\t\telse\n\t\t\t\tstream_put_in_addr(s, &bgp->router_id);\n\t\t\tstream_put(s, attr->cluster->list,\n\t\t\t\t   attr->cluster->length);\n\t\t} else {\n\t\t\tstream_putc(s, 4);\n\t\t\t/* If this peer configuration's parent BGP has\n\t\t\t * cluster_id. */\n\t\t\tif (bgp->config & BGP_CONFIG_CLUSTER_ID)\n\t\t\t\tstream_put_in_addr(s, &bgp->cluster_id);\n\t\t\telse\n\t\t\t\tstream_put_in_addr(s, &bgp->router_id);\n\t\t}\n\t}\n\n\t/* Extended Communities attribute. */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi], PEER_FLAG_SEND_EXT_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_EXT_COMMUNITIES))) {\n\t\tif (peer->sort == BGP_PEER_IBGP\n\t\t    || peer->sort == BGP_PEER_CONFED) {\n\t\t\tif (attr->ecommunity->size * 8 > 255) {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\t\tstream_putc(s, BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\tstream_putw(s, attr->ecommunity->size * 8);\n\t\t\t} else {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\t\tstream_putc(s, BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\tstream_putc(s, attr->ecommunity->size * 8);\n\t\t\t}\n\t\t\tstream_put(s, attr->ecommunity->val,\n\t\t\t\t   attr->ecommunity->size * 8);\n\t\t} else {\n\t\t\tuint8_t *pnt;\n\t\t\tint tbit;\n\t\t\tint ecom_tr_size = 0;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < attr->ecommunity->size; i++) {\n\t\t\t\tpnt = attr->ecommunity->val + (i * 8);\n\t\t\t\ttbit = *pnt;\n\n\t\t\t\tif (CHECK_FLAG(tbit,\n\t\t\t\t\t       ECOMMUNITY_FLAG_NON_TRANSITIVE))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tecom_tr_size++;\n\t\t\t}\n\n\t\t\tif (ecom_tr_size) {\n\t\t\t\tif (ecom_tr_size * 8 > 255) {\n\t\t\t\t\tstream_putc(\n\t\t\t\t\t\ts,\n\t\t\t\t\t\tBGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_EXTLEN);\n\t\t\t\t\tstream_putc(s,\n\t\t\t\t\t\t    BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\t\tstream_putw(s, ecom_tr_size * 8);\n\t\t\t\t} else {\n\t\t\t\t\tstream_putc(\n\t\t\t\t\t\ts,\n\t\t\t\t\t\tBGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_TRANS);\n\t\t\t\t\tstream_putc(s,\n\t\t\t\t\t\t    BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\t\tstream_putc(s, ecom_tr_size * 8);\n\t\t\t\t}\n\n\t\t\t\tfor (i = 0; i < attr->ecommunity->size; i++) {\n\t\t\t\t\tpnt = attr->ecommunity->val + (i * 8);\n\t\t\t\t\ttbit = *pnt;\n\n\t\t\t\t\tif (CHECK_FLAG(\n\t\t\t\t\t\t    tbit,\n\t\t\t\t\t\t    ECOMMUNITY_FLAG_NON_TRANSITIVE))\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\tstream_put(s, pnt, 8);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Label index attribute. */\n\tif (safi == SAFI_LABELED_UNICAST) {\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_PREFIX_SID)) {\n\t\t\tuint32_t label_index;\n\n\t\t\tlabel_index = attr->label_index;\n\n\t\t\tif (label_index != BGP_INVALID_LABEL_INDEX) {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\t\tstream_putc(s, BGP_ATTR_PREFIX_SID);\n\t\t\t\tstream_putc(s, 10);\n\t\t\t\tstream_putc(s, BGP_PREFIX_SID_LABEL_INDEX);\n\t\t\t\tstream_putw(s,\n\t\t\t\t\t    BGP_PREFIX_SID_LABEL_INDEX_LENGTH);\n\t\t\t\tstream_putc(s, 0); // reserved\n\t\t\t\tstream_putw(s, 0); // flags\n\t\t\t\tstream_putl(s, label_index);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (send_as4_path) {\n\t\t/* If the peer is NOT As4 capable, AND */\n\t\t/* there are ASnums > 65535 in path  THEN\n\t\t * give out AS4_PATH */\n\n\t\t/* Get rid of all AS_CONFED_SEQUENCE and AS_CONFED_SET\n\t\t * path segments!\n\t\t * Hm, I wonder...  confederation things *should* only be at\n\t\t * the beginning of an aspath, right?  Then we should use\n\t\t * aspath_delete_confed_seq for this, because it is already\n\t\t * there! (JK)\n\t\t * Folks, talk to me: what is reasonable here!?\n\t\t */\n\t\taspath = aspath_delete_confed_seq(aspath);\n\n\t\tstream_putc(s,\n\t\t\t    BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\tstream_putc(s, BGP_ATTR_AS4_PATH);\n\t\taspath_sizep = stream_get_endp(s);\n\t\tstream_putw(s, 0);\n\t\tstream_putw_at(s, aspath_sizep, aspath_put(s, aspath, 1));\n\t}\n\n\tif (aspath != attr->aspath)\n\t\taspath_free(aspath);\n\n\tif (send_as4_aggregator) {\n\t\t/* send AS4_AGGREGATOR, at this place */\n\t\t/* this section of code moved here in order to ensure the\n\t\t * correct\n\t\t * *ascending* order of attributes\n\t\t */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_AS4_AGGREGATOR);\n\t\tstream_putc(s, 8);\n\t\tstream_putl(s, attr->aggregator_as);\n\t\tstream_put_ipv4(s, attr->aggregator_addr.s_addr);\n\t}\n\n\tif (((afi == AFI_IP || afi == AFI_IP6)\n\t     && (safi == SAFI_ENCAP || safi == SAFI_MPLS_VPN))\n\t    || (afi == AFI_L2VPN && safi == SAFI_EVPN)) {\n\t\t/* Tunnel Encap attribute */\n\t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_ENCAP);\n\n#if ENABLE_BGP_VNC\n\t\t/* VNC attribute */\n\t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_VNC);\n#endif\n\t}\n\n\t/* PMSI Tunnel */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_PMSI_TUNNEL)) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_PMSI_TUNNEL);\n\t\tstream_putc(s, 9); // Length\n\t\tstream_putc(s, 0); // Flags\n\t\tstream_putc(s, PMSI_TNLTYPE_INGR_REPL); // IR (6)\n\t\tstream_put(s, &(attr->label),\n\t\t\t   BGP_LABEL_BYTES); // MPLS Label / VXLAN VNI\n\t\tstream_put_ipv4(s, attr->nexthop.s_addr);\n\t\t// Unicast tunnel endpoint IP address\n\t}\n\n\t/* Unknown transit attribute. */\n\tif (attr->transit)\n\t\tstream_put(s, attr->transit->val, attr->transit->length);\n\n\t/* Return total size of attribute. */\n\treturn stream_get_endp(s) - cp;\n}",
        "func": "bgp_size_t bgp_packet_attribute(struct bgp *bgp, struct peer *peer,\n\t\t\t\tstruct stream *s, struct attr *attr,\n\t\t\t\tstruct bpacket_attr_vec_arr *vecarr,\n\t\t\t\tstruct prefix *p, afi_t afi, safi_t safi,\n\t\t\t\tstruct peer *from, struct prefix_rd *prd,\n\t\t\t\tmpls_label_t *label, uint32_t num_labels,\n\t\t\t\tint addpath_encode, uint32_t addpath_tx_id)\n{\n\tsize_t cp;\n\tsize_t aspath_sizep;\n\tstruct aspath *aspath;\n\tint send_as4_path = 0;\n\tint send_as4_aggregator = 0;\n\tint use32bit = (CHECK_FLAG(peer->cap, PEER_CAP_AS4_RCV)) ? 1 : 0;\n\n\tif (!bgp)\n\t\tbgp = peer->bgp;\n\n\t/* Remember current pointer. */\n\tcp = stream_get_endp(s);\n\n\tif (p\n\t    && !((afi == AFI_IP && safi == SAFI_UNICAST)\n\t\t && !peer_cap_enhe(peer, afi, safi))) {\n\t\tsize_t mpattrlen_pos = 0;\n\n\t\tmpattrlen_pos = bgp_packet_mpattr_start(s, peer, afi, safi,\n\t\t\t\t\t\t\tvecarr, attr);\n\t\tbgp_packet_mpattr_prefix(s, afi, safi, p, prd, label,\n\t\t\t\t\t num_labels, addpath_encode,\n\t\t\t\t\t addpath_tx_id, attr);\n\t\tbgp_packet_mpattr_end(s, mpattrlen_pos);\n\t}\n\n\t/* Origin attribute. */\n\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\tstream_putc(s, BGP_ATTR_ORIGIN);\n\tstream_putc(s, 1);\n\tstream_putc(s, attr->origin);\n\n\t/* AS path attribute. */\n\n\t/* If remote-peer is EBGP */\n\tif (peer->sort == BGP_PEER_EBGP\n\t    && (!CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t\t    PEER_FLAG_AS_PATH_UNCHANGED)\n\t\t|| attr->aspath->segments == NULL)\n\t    && (!CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t\t    PEER_FLAG_RSERVER_CLIENT))) {\n\t\taspath = aspath_dup(attr->aspath);\n\n\t\t/* Even though we may not be configured for confederations we\n\t\t * may have\n\t\t * RXed an AS_PATH with AS_CONFED_SEQUENCE or AS_CONFED_SET */\n\t\taspath = aspath_delete_confed_seq(aspath);\n\n\t\tif (CHECK_FLAG(bgp->config, BGP_CONFIG_CONFEDERATION)) {\n\t\t\t/* Stuff our path CONFED_ID on the front */\n\t\t\taspath = aspath_add_seq(aspath, bgp->confed_id);\n\t\t} else {\n\t\t\tif (peer->change_local_as) {\n\t\t\t\t/* If replace-as is specified, we only use the\n\t\t\t\t   change_local_as when\n\t\t\t\t   advertising routes. */\n\t\t\t\tif (!CHECK_FLAG(\n\t\t\t\t\t    peer->flags,\n\t\t\t\t\t    PEER_FLAG_LOCAL_AS_REPLACE_AS)) {\n\t\t\t\t\taspath = aspath_add_seq(aspath,\n\t\t\t\t\t\t\t\tpeer->local_as);\n\t\t\t\t}\n\t\t\t\taspath = aspath_add_seq(aspath,\n\t\t\t\t\t\t\tpeer->change_local_as);\n\t\t\t} else {\n\t\t\t\taspath = aspath_add_seq(aspath, peer->local_as);\n\t\t\t}\n\t\t}\n\t} else if (peer->sort == BGP_PEER_CONFED) {\n\t\t/* A confed member, so we need to do the AS_CONFED_SEQUENCE\n\t\t * thing */\n\t\taspath = aspath_dup(attr->aspath);\n\t\taspath = aspath_add_confed_seq(aspath, peer->local_as);\n\t} else\n\t\taspath = attr->aspath;\n\n\t/* If peer is not AS4 capable, then:\n\t * - send the created AS_PATH out as AS4_PATH (optional, transitive),\n\t *   but ensure that no AS_CONFED_SEQUENCE and AS_CONFED_SET path\n\t * segment\n\t *   types are in it (i.e. exclude them if they are there)\n\t *   AND do this only if there is at least one asnum > 65535 in the\n\t * path!\n\t * - send an AS_PATH out, but put 16Bit ASnums in it, not 32bit, and\n\t * change\n\t *   all ASnums > 65535 to BGP_AS_TRANS\n\t */\n\n\tstream_putc(s, BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_EXTLEN);\n\tstream_putc(s, BGP_ATTR_AS_PATH);\n\taspath_sizep = stream_get_endp(s);\n\tstream_putw(s, 0);\n\tstream_putw_at(s, aspath_sizep, aspath_put(s, aspath, use32bit));\n\n\t/* OLD session may need NEW_AS_PATH sent, if there are 4-byte ASNs\n\t * in the path\n\t */\n\tif (!use32bit && aspath_has_as4(aspath))\n\t\tsend_as4_path =\n\t\t\t1; /* we'll do this later, at the correct place */\n\n\t/* Nexthop attribute. */\n\tif (afi == AFI_IP && safi == SAFI_UNICAST\n\t    && !peer_cap_enhe(peer, afi, safi)) {\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_NEXT_HOP)) {\n\t\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_NEXT_HOP);\n\t\t\tbpacket_attr_vec_arr_set_vec(vecarr, BGP_ATTR_VEC_NH, s,\n\t\t\t\t\t\t     attr);\n\t\t\tstream_putc(s, 4);\n\t\t\tstream_put_ipv4(s, attr->nexthop.s_addr);\n\t\t} else if (peer_cap_enhe(from, afi, safi)) {\n\t\t\t/*\n\t\t\t * Likely this is the case when an IPv4 prefix was\n\t\t\t * received with\n\t\t\t * Extended Next-hop capability and now being advertised\n\t\t\t * to\n\t\t\t * non-ENHE peers.\n\t\t\t * Setting the mandatory (ipv4) next-hop attribute here\n\t\t\t * to enable\n\t\t\t * implicit next-hop self with correct (ipv4 address\n\t\t\t * family).\n\t\t\t */\n\t\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_NEXT_HOP);\n\t\t\tbpacket_attr_vec_arr_set_vec(vecarr, BGP_ATTR_VEC_NH, s,\n\t\t\t\t\t\t     NULL);\n\t\t\tstream_putc(s, 4);\n\t\t\tstream_put_ipv4(s, 0);\n\t\t}\n\t}\n\n\t/* MED attribute. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_MULTI_EXIT_DISC)\n\t    || bgp->maxmed_active) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_MULTI_EXIT_DISC);\n\t\tstream_putc(s, 4);\n\t\tstream_putl(s, (bgp->maxmed_active ? bgp->maxmed_value\n\t\t\t\t\t\t   : attr->med));\n\t}\n\n\t/* Local preference. */\n\tif (peer->sort == BGP_PEER_IBGP || peer->sort == BGP_PEER_CONFED) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_LOCAL_PREF);\n\t\tstream_putc(s, 4);\n\t\tstream_putl(s, attr->local_pref);\n\t}\n\n\t/* Atomic aggregate. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_ATOMIC_AGGREGATE)) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_ATOMIC_AGGREGATE);\n\t\tstream_putc(s, 0);\n\t}\n\n\t/* Aggregator. */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_AGGREGATOR)) {\n\t\t/* Common to BGP_ATTR_AGGREGATOR, regardless of ASN size */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_AGGREGATOR);\n\n\t\tif (use32bit) {\n\t\t\t/* AS4 capable peer */\n\t\t\tstream_putc(s, 8);\n\t\t\tstream_putl(s, attr->aggregator_as);\n\t\t} else {\n\t\t\t/* 2-byte AS peer */\n\t\t\tstream_putc(s, 6);\n\n\t\t\t/* Is ASN representable in 2-bytes? Or must AS_TRANS be\n\t\t\t * used? */\n\t\t\tif (attr->aggregator_as > 65535) {\n\t\t\t\tstream_putw(s, BGP_AS_TRANS);\n\n\t\t\t\t/* we have to send AS4_AGGREGATOR, too.\n\t\t\t\t * we'll do that later in order to send\n\t\t\t\t * attributes in ascending\n\t\t\t\t * order.\n\t\t\t\t */\n\t\t\t\tsend_as4_aggregator = 1;\n\t\t\t} else\n\t\t\t\tstream_putw(s, (uint16_t)attr->aggregator_as);\n\t\t}\n\t\tstream_put_ipv4(s, attr->aggregator_addr.s_addr);\n\t}\n\n\t/* Community attribute. */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi], PEER_FLAG_SEND_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_COMMUNITIES))) {\n\t\tif (attr->community->size * 4 > 255) {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\tstream_putc(s, BGP_ATTR_COMMUNITIES);\n\t\t\tstream_putw(s, attr->community->size * 4);\n\t\t} else {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_COMMUNITIES);\n\t\t\tstream_putc(s, attr->community->size * 4);\n\t\t}\n\t\tstream_put(s, attr->community->val, attr->community->size * 4);\n\t}\n\n\t/*\n\t * Large Community attribute.\n\t */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi],\n\t\t       PEER_FLAG_SEND_LARGE_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_LARGE_COMMUNITIES))) {\n\t\tif (lcom_length(attr->lcommunity) > 255) {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\tstream_putc(s, BGP_ATTR_LARGE_COMMUNITIES);\n\t\t\tstream_putw(s, lcom_length(attr->lcommunity));\n\t\t} else {\n\t\t\tstream_putc(s,\n\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\tstream_putc(s, BGP_ATTR_LARGE_COMMUNITIES);\n\t\t\tstream_putc(s, lcom_length(attr->lcommunity));\n\t\t}\n\t\tstream_put(s, attr->lcommunity->val,\n\t\t\t   lcom_length(attr->lcommunity));\n\t}\n\n\t/* Route Reflector. */\n\tif (peer->sort == BGP_PEER_IBGP && from\n\t    && from->sort == BGP_PEER_IBGP) {\n\t\t/* Originator ID. */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_ORIGINATOR_ID);\n\t\tstream_putc(s, 4);\n\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_ORIGINATOR_ID))\n\t\t\tstream_put_in_addr(s, &attr->originator_id);\n\t\telse\n\t\t\tstream_put_in_addr(s, &from->remote_id);\n\n\t\t/* Cluster list. */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, BGP_ATTR_CLUSTER_LIST);\n\n\t\tif (attr->cluster) {\n\t\t\tstream_putc(s, attr->cluster->length + 4);\n\t\t\t/* If this peer configuration's parent BGP has\n\t\t\t * cluster_id. */\n\t\t\tif (bgp->config & BGP_CONFIG_CLUSTER_ID)\n\t\t\t\tstream_put_in_addr(s, &bgp->cluster_id);\n\t\t\telse\n\t\t\t\tstream_put_in_addr(s, &bgp->router_id);\n\t\t\tstream_put(s, attr->cluster->list,\n\t\t\t\t   attr->cluster->length);\n\t\t} else {\n\t\t\tstream_putc(s, 4);\n\t\t\t/* If this peer configuration's parent BGP has\n\t\t\t * cluster_id. */\n\t\t\tif (bgp->config & BGP_CONFIG_CLUSTER_ID)\n\t\t\t\tstream_put_in_addr(s, &bgp->cluster_id);\n\t\t\telse\n\t\t\t\tstream_put_in_addr(s, &bgp->router_id);\n\t\t}\n\t}\n\n\t/* Extended Communities attribute. */\n\tif (CHECK_FLAG(peer->af_flags[afi][safi], PEER_FLAG_SEND_EXT_COMMUNITY)\n\t    && (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_EXT_COMMUNITIES))) {\n\t\tif (peer->sort == BGP_PEER_IBGP\n\t\t    || peer->sort == BGP_PEER_CONFED) {\n\t\t\tif (attr->ecommunity->size * 8 > 255) {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\t\t\tstream_putc(s, BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\tstream_putw(s, attr->ecommunity->size * 8);\n\t\t\t} else {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\t\tstream_putc(s, BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\tstream_putc(s, attr->ecommunity->size * 8);\n\t\t\t}\n\t\t\tstream_put(s, attr->ecommunity->val,\n\t\t\t\t   attr->ecommunity->size * 8);\n\t\t} else {\n\t\t\tuint8_t *pnt;\n\t\t\tint tbit;\n\t\t\tint ecom_tr_size = 0;\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < attr->ecommunity->size; i++) {\n\t\t\t\tpnt = attr->ecommunity->val + (i * 8);\n\t\t\t\ttbit = *pnt;\n\n\t\t\t\tif (CHECK_FLAG(tbit,\n\t\t\t\t\t       ECOMMUNITY_FLAG_NON_TRANSITIVE))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tecom_tr_size++;\n\t\t\t}\n\n\t\t\tif (ecom_tr_size) {\n\t\t\t\tif (ecom_tr_size * 8 > 255) {\n\t\t\t\t\tstream_putc(\n\t\t\t\t\t\ts,\n\t\t\t\t\t\tBGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_TRANS\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_EXTLEN);\n\t\t\t\t\tstream_putc(s,\n\t\t\t\t\t\t    BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\t\tstream_putw(s, ecom_tr_size * 8);\n\t\t\t\t} else {\n\t\t\t\t\tstream_putc(\n\t\t\t\t\t\ts,\n\t\t\t\t\t\tBGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t\t| BGP_ATTR_FLAG_TRANS);\n\t\t\t\t\tstream_putc(s,\n\t\t\t\t\t\t    BGP_ATTR_EXT_COMMUNITIES);\n\t\t\t\t\tstream_putc(s, ecom_tr_size * 8);\n\t\t\t\t}\n\n\t\t\t\tfor (i = 0; i < attr->ecommunity->size; i++) {\n\t\t\t\t\tpnt = attr->ecommunity->val + (i * 8);\n\t\t\t\t\ttbit = *pnt;\n\n\t\t\t\t\tif (CHECK_FLAG(\n\t\t\t\t\t\t    tbit,\n\t\t\t\t\t\t    ECOMMUNITY_FLAG_NON_TRANSITIVE))\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\tstream_put(s, pnt, 8);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Label index attribute. */\n\tif (safi == SAFI_LABELED_UNICAST) {\n\t\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_PREFIX_SID)) {\n\t\t\tuint32_t label_index;\n\n\t\t\tlabel_index = attr->label_index;\n\n\t\t\tif (label_index != BGP_INVALID_LABEL_INDEX) {\n\t\t\t\tstream_putc(s,\n\t\t\t\t\t    BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t\t\t    | BGP_ATTR_FLAG_TRANS);\n\t\t\t\tstream_putc(s, BGP_ATTR_PREFIX_SID);\n\t\t\t\tstream_putc(s, 10);\n\t\t\t\tstream_putc(s, BGP_PREFIX_SID_LABEL_INDEX);\n\t\t\t\tstream_putw(s,\n\t\t\t\t\t    BGP_PREFIX_SID_LABEL_INDEX_LENGTH);\n\t\t\t\tstream_putc(s, 0); // reserved\n\t\t\t\tstream_putw(s, 0); // flags\n\t\t\t\tstream_putl(s, label_index);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (send_as4_path) {\n\t\t/* If the peer is NOT As4 capable, AND */\n\t\t/* there are ASnums > 65535 in path  THEN\n\t\t * give out AS4_PATH */\n\n\t\t/* Get rid of all AS_CONFED_SEQUENCE and AS_CONFED_SET\n\t\t * path segments!\n\t\t * Hm, I wonder...  confederation things *should* only be at\n\t\t * the beginning of an aspath, right?  Then we should use\n\t\t * aspath_delete_confed_seq for this, because it is already\n\t\t * there! (JK)\n\t\t * Folks, talk to me: what is reasonable here!?\n\t\t */\n\t\taspath = aspath_delete_confed_seq(aspath);\n\n\t\tstream_putc(s,\n\t\t\t    BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\tstream_putc(s, BGP_ATTR_AS4_PATH);\n\t\taspath_sizep = stream_get_endp(s);\n\t\tstream_putw(s, 0);\n\t\tstream_putw_at(s, aspath_sizep, aspath_put(s, aspath, 1));\n\t}\n\n\tif (aspath != attr->aspath)\n\t\taspath_free(aspath);\n\n\tif (send_as4_aggregator) {\n\t\t/* send AS4_AGGREGATOR, at this place */\n\t\t/* this section of code moved here in order to ensure the\n\t\t * correct\n\t\t * *ascending* order of attributes\n\t\t */\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_AS4_AGGREGATOR);\n\t\tstream_putc(s, 8);\n\t\tstream_putl(s, attr->aggregator_as);\n\t\tstream_put_ipv4(s, attr->aggregator_addr.s_addr);\n\t}\n\n\tif (((afi == AFI_IP || afi == AFI_IP6)\n\t     && (safi == SAFI_ENCAP || safi == SAFI_MPLS_VPN))\n\t    || (afi == AFI_L2VPN && safi == SAFI_EVPN)) {\n\t\t/* Tunnel Encap attribute */\n\t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_ENCAP);\n\n#if ENABLE_BGP_VNC_ATTR\n\t\t/* VNC attribute */\n\t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_VNC);\n#endif\n\t}\n\n\t/* PMSI Tunnel */\n\tif (attr->flag & ATTR_FLAG_BIT(BGP_ATTR_PMSI_TUNNEL)) {\n\t\tstream_putc(s, BGP_ATTR_FLAG_OPTIONAL | BGP_ATTR_FLAG_TRANS);\n\t\tstream_putc(s, BGP_ATTR_PMSI_TUNNEL);\n\t\tstream_putc(s, 9); // Length\n\t\tstream_putc(s, 0); // Flags\n\t\tstream_putc(s, PMSI_TNLTYPE_INGR_REPL); // IR (6)\n\t\tstream_put(s, &(attr->label),\n\t\t\t   BGP_LABEL_BYTES); // MPLS Label / VXLAN VNI\n\t\tstream_put_ipv4(s, attr->nexthop.s_addr);\n\t\t// Unicast tunnel endpoint IP address\n\t}\n\n\t/* Unknown transit attribute. */\n\tif (attr->transit)\n\t\tstream_put(s, attr->transit->val, attr->transit->length);\n\n\t/* Return total size of attribute. */\n\treturn stream_get_endp(s) - cp;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -416,7 +416,7 @@\n \t\t/* Tunnel Encap attribute */\n \t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_ENCAP);\n \n-#if ENABLE_BGP_VNC\n+#if ENABLE_BGP_VNC_ATTR\n \t\t/* VNC attribute */\n \t\tbgp_packet_mpattr_tea(bgp, peer, s, attr, BGP_ATTR_VNC);\n #endif",
        "diff_line_info": {
            "deleted_lines": [
                "#if ENABLE_BGP_VNC"
            ],
            "added_lines": [
                "#if ENABLE_BGP_VNC_ATTR"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-5892",
        "func_name": "FRRouting/frr/bgp_packet_mpattr_tea",
        "description": "bgpd in FRRouting FRR (aka Free Range Routing) 2.x and 3.x before 3.0.4, 4.x before 4.0.1, 5.x before 5.0.2, and 6.x before 6.0.2 (not affecting Cumulus Linux or VyOS), when ENABLE_BGP_VNC is used for Virtual Network Control, allows remote attackers to cause a denial of service (peering session flap) via attribute 255 in a BGP UPDATE packet. This occurred during Disco in January 2019 because FRR does not implement RFC 7606, and therefore the packets with 255 were considered invalid VNC data and the BGP session was closed.",
        "git_url": "https://github.com/FRRouting/frr/commit/943d595a018e69b550db08cccba1d0778a86705a",
        "commit_title": "bgpd: don't use BGP_ATTR_VNC(255) unless ENABLE_BGP_VNC_ATTR is defined",
        "commit_text": "",
        "func_before": "static void bgp_packet_mpattr_tea(struct bgp *bgp, struct peer *peer,\n\t\t\t\t  struct stream *s, struct attr *attr,\n\t\t\t\t  uint8_t attrtype)\n{\n\tunsigned int attrlenfield = 0;\n\tunsigned int attrhdrlen = 0;\n\tstruct bgp_attr_encap_subtlv *subtlvs;\n\tstruct bgp_attr_encap_subtlv *st;\n\tconst char *attrname;\n\n\tif (!attr || (attrtype == BGP_ATTR_ENCAP\n\t\t      && (!attr->encap_tunneltype\n\t\t\t  || attr->encap_tunneltype == BGP_ENCAP_TYPE_MPLS)))\n\t\treturn;\n\n\tswitch (attrtype) {\n\tcase BGP_ATTR_ENCAP:\n\t\tattrname = \"Tunnel Encap\";\n\t\tsubtlvs = attr->encap_subtlvs;\n\t\tif (subtlvs == NULL) /* nothing to do */\n\t\t\treturn;\n\t\t/*\n\t\t * The tunnel encap attr has an \"outer\" tlv.\n\t\t * T = tunneltype,\n\t\t * L = total length of subtlvs,\n\t\t * V = concatenated subtlvs.\n\t\t */\n\t\tattrlenfield = 2 + 2; /* T + L */\n\t\tattrhdrlen = 1 + 1;   /* subTLV T + L */\n\t\tbreak;\n\n#if ENABLE_BGP_VNC\n\tcase BGP_ATTR_VNC:\n\t\tattrname = \"VNC\";\n\t\tsubtlvs = attr->vnc_subtlvs;\n\t\tif (subtlvs == NULL) /* nothing to do */\n\t\t\treturn;\n\t\tattrlenfield = 0;   /* no outer T + L */\n\t\tattrhdrlen = 2 + 2; /* subTLV T + L */\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tassert(0);\n\t}\n\n\t/* compute attr length */\n\tfor (st = subtlvs; st; st = st->next) {\n\t\tattrlenfield += (attrhdrlen + st->length);\n\t}\n\n\tif (attrlenfield > 0xffff) {\n\t\tzlog_info(\"%s attribute is too long (length=%d), can't send it\",\n\t\t\t  attrname, attrlenfield);\n\t\treturn;\n\t}\n\n\tif (attrlenfield > 0xff) {\n\t\t/* 2-octet length field */\n\t\tstream_putc(s,\n\t\t\t    BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\tstream_putc(s, attrtype);\n\t\tstream_putw(s, attrlenfield & 0xffff);\n\t} else {\n\t\t/* 1-octet length field */\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, attrtype);\n\t\tstream_putc(s, attrlenfield & 0xff);\n\t}\n\n\tif (attrtype == BGP_ATTR_ENCAP) {\n\t\t/* write outer T+L */\n\t\tstream_putw(s, attr->encap_tunneltype);\n\t\tstream_putw(s, attrlenfield - 4);\n\t}\n\n\t/* write each sub-tlv */\n\tfor (st = subtlvs; st; st = st->next) {\n\t\tif (attrtype == BGP_ATTR_ENCAP) {\n\t\t\tstream_putc(s, st->type);\n\t\t\tstream_putc(s, st->length);\n#if ENABLE_BGP_VNC\n\t\t} else {\n\t\t\tstream_putw(s, st->type);\n\t\t\tstream_putw(s, st->length);\n#endif\n\t\t}\n\t\tstream_put(s, st->value, st->length);\n\t}\n}",
        "func": "static void bgp_packet_mpattr_tea(struct bgp *bgp, struct peer *peer,\n\t\t\t\t  struct stream *s, struct attr *attr,\n\t\t\t\t  uint8_t attrtype)\n{\n\tunsigned int attrlenfield = 0;\n\tunsigned int attrhdrlen = 0;\n\tstruct bgp_attr_encap_subtlv *subtlvs;\n\tstruct bgp_attr_encap_subtlv *st;\n\tconst char *attrname;\n\n\tif (!attr || (attrtype == BGP_ATTR_ENCAP\n\t\t      && (!attr->encap_tunneltype\n\t\t\t  || attr->encap_tunneltype == BGP_ENCAP_TYPE_MPLS)))\n\t\treturn;\n\n\tswitch (attrtype) {\n\tcase BGP_ATTR_ENCAP:\n\t\tattrname = \"Tunnel Encap\";\n\t\tsubtlvs = attr->encap_subtlvs;\n\t\tif (subtlvs == NULL) /* nothing to do */\n\t\t\treturn;\n\t\t/*\n\t\t * The tunnel encap attr has an \"outer\" tlv.\n\t\t * T = tunneltype,\n\t\t * L = total length of subtlvs,\n\t\t * V = concatenated subtlvs.\n\t\t */\n\t\tattrlenfield = 2 + 2; /* T + L */\n\t\tattrhdrlen = 1 + 1;   /* subTLV T + L */\n\t\tbreak;\n\n#if ENABLE_BGP_VNC_ATTR\n\tcase BGP_ATTR_VNC:\n\t\tattrname = \"VNC\";\n\t\tsubtlvs = attr->vnc_subtlvs;\n\t\tif (subtlvs == NULL) /* nothing to do */\n\t\t\treturn;\n\t\tattrlenfield = 0;   /* no outer T + L */\n\t\tattrhdrlen = 2 + 2; /* subTLV T + L */\n\t\tbreak;\n#endif\n\n\tdefault:\n\t\tassert(0);\n\t}\n\n\t/* compute attr length */\n\tfor (st = subtlvs; st; st = st->next) {\n\t\tattrlenfield += (attrhdrlen + st->length);\n\t}\n\n\tif (attrlenfield > 0xffff) {\n\t\tzlog_info(\"%s attribute is too long (length=%d), can't send it\",\n\t\t\t  attrname, attrlenfield);\n\t\treturn;\n\t}\n\n\tif (attrlenfield > 0xff) {\n\t\t/* 2-octet length field */\n\t\tstream_putc(s,\n\t\t\t    BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL\n\t\t\t\t    | BGP_ATTR_FLAG_EXTLEN);\n\t\tstream_putc(s, attrtype);\n\t\tstream_putw(s, attrlenfield & 0xffff);\n\t} else {\n\t\t/* 1-octet length field */\n\t\tstream_putc(s, BGP_ATTR_FLAG_TRANS | BGP_ATTR_FLAG_OPTIONAL);\n\t\tstream_putc(s, attrtype);\n\t\tstream_putc(s, attrlenfield & 0xff);\n\t}\n\n\tif (attrtype == BGP_ATTR_ENCAP) {\n\t\t/* write outer T+L */\n\t\tstream_putw(s, attr->encap_tunneltype);\n\t\tstream_putw(s, attrlenfield - 4);\n\t}\n\n\t/* write each sub-tlv */\n\tfor (st = subtlvs; st; st = st->next) {\n\t\tif (attrtype == BGP_ATTR_ENCAP) {\n\t\t\tstream_putc(s, st->type);\n\t\t\tstream_putc(s, st->length);\n#if ENABLE_BGP_VNC\n\t\t} else {\n\t\t\tstream_putw(s, st->type);\n\t\t\tstream_putw(s, st->length);\n#endif\n\t\t}\n\t\tstream_put(s, st->value, st->length);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -29,7 +29,7 @@\n \t\tattrhdrlen = 1 + 1;   /* subTLV T + L */\n \t\tbreak;\n \n-#if ENABLE_BGP_VNC\n+#if ENABLE_BGP_VNC_ATTR\n \tcase BGP_ATTR_VNC:\n \t\tattrname = \"VNC\";\n \t\tsubtlvs = attr->vnc_subtlvs;",
        "diff_line_info": {
            "deleted_lines": [
                "#if ENABLE_BGP_VNC"
            ],
            "added_lines": [
                "#if ENABLE_BGP_VNC_ATTR"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-6560",
        "func_name": "flatpak/side_in_cb",
        "description": "In dbus-proxy/flatpak-proxy.c in Flatpak before 0.8.9, and 0.9.x and 0.10.x before 0.10.3, crafted D-Bus messages to the host can be used to break out of the sandbox, because whitespace handling in the proxy is not identical to whitespace handling in the daemon.",
        "git_url": "https://github.com/flatpak/flatpak/commit/52346bf187b5a7f1c0fe9075b328b7ad6abe78f6",
        "commit_title": "Fix vulnerability in dbus proxy",
        "commit_text": " During the authentication all client data is directly forwarded to the dbus daemon as is, until we detect the BEGIN command after which we start filtering the binary dbus protocol.  Unfortunately the detection of the BEGIN command in the proxy did not exactly match the detection in the dbus daemon. A BEGIN followed by a space or tab was considered ok in the daemon but not by the proxy. This could be exploited to send arbitrary dbus messages to the host, which can be used to break out of the sandbox.  This was noticed by Gabriel Campana of The Google Security Team.  This fix makes the detection of the authentication phase end match the dbus code. In addition we duplicate the authentication line validation from dbus, which includes ensuring all data is ASCII, and limiting the size of a line to 16k. In fact, we add some extra stringent checks, disallowing ASCII control chars and requiring that auth lines start with a capital letter.",
        "func_before": "static gboolean\nside_in_cb (GSocket *socket, GIOCondition condition, gpointer user_data)\n{\n  ProxySide *side = user_data;\n  FlatpakProxyClient *client = side->client;\n  GError *error = NULL;\n  Buffer *buffer;\n  gboolean retval = G_SOURCE_CONTINUE;\n\n  g_object_ref (client);\n\n  while (!side->closed)\n    {\n      if (!side->got_first_byte)\n        buffer = buffer_new (1, NULL);\n      else if (!client->authenticated)\n        buffer = buffer_new (64, NULL);\n      else\n        buffer = side->current_read_buffer;\n\n      if (!buffer_read (side, buffer, socket))\n        {\n          if (buffer != side->current_read_buffer)\n              buffer_unref (buffer);\n          break;\n        }\n\n      if (!client->authenticated)\n        {\n          if (buffer->pos > 0)\n            {\n              gboolean found_auth_end = FALSE;\n              gsize extra_data;\n\n              buffer->size = buffer->pos;\n              if (!side->got_first_byte)\n                {\n                  buffer->send_credentials = TRUE;\n                  side->got_first_byte = TRUE;\n                }\n              /* Look for end of authentication mechanism */\n              else if (side == &client->client_side)\n                {\n                  gssize auth_end = find_auth_end (client, buffer);\n\n                  if (auth_end >= 0)\n                    {\n                      found_auth_end = TRUE;\n                      buffer->size = auth_end;\n                      extra_data = buffer->pos - buffer->size;\n\n                      /* We may have gotten some extra data which is not part of\n                         the auth handshake, keep it for the next iteration. */\n                      if (extra_data > 0)\n                        side->extra_input_data = g_bytes_new (buffer->data + buffer->size, extra_data);\n                    }\n                }\n\n              got_buffer_from_side (side, buffer);\n\n              if (found_auth_end)\n                client->authenticated = TRUE;\n            }\n          else\n            {\n              buffer_unref (buffer);\n            }\n        }\n      else if (buffer->pos == buffer->size)\n        {\n          if (buffer == &side->header_buffer)\n            {\n              gssize required;\n              required = g_dbus_message_bytes_needed (buffer->data, buffer->size, &error);\n              if (required < 0)\n                {\n                  g_warning (\"Invalid message header read\");\n                  side_closed (side);\n                }\n              else\n                {\n                  side->current_read_buffer = buffer_new (required, buffer);\n                }\n            }\n          else\n            {\n              got_buffer_from_side (side, buffer);\n              side->header_buffer.pos = 0;\n              side->current_read_buffer = &side->header_buffer;\n            }\n        }\n    }\n\n  if (side->closed)\n    {\n      side->in_source = NULL;\n      retval = G_SOURCE_REMOVE;\n    }\n\n  g_object_unref (client);\n\n  return retval;\n}",
        "func": "static gboolean\nside_in_cb (GSocket *socket, GIOCondition condition, gpointer user_data)\n{\n  ProxySide *side = user_data;\n  FlatpakProxyClient *client = side->client;\n  GError *error = NULL;\n  Buffer *buffer;\n  gboolean retval = G_SOURCE_CONTINUE;\n\n  g_object_ref (client);\n\n  while (!side->closed)\n    {\n      if (!side->got_first_byte)\n        buffer = buffer_new (1, NULL);\n      else if (!client->authenticated)\n        buffer = buffer_new (64, NULL);\n      else\n        buffer = side->current_read_buffer;\n\n      if (!buffer_read (side, buffer, socket))\n        {\n          if (buffer != side->current_read_buffer)\n              buffer_unref (buffer);\n          break;\n        }\n\n      if (!client->authenticated)\n        {\n          if (buffer->pos > 0)\n            {\n              gboolean found_auth_end = FALSE;\n              gsize extra_data;\n\n              buffer->size = buffer->pos;\n              if (!side->got_first_byte)\n                {\n                  buffer->send_credentials = TRUE;\n                  side->got_first_byte = TRUE;\n                }\n              /* Look for end of authentication mechanism */\n              else if (side == &client->client_side)\n                {\n                  gssize auth_end = find_auth_end (client, buffer);\n\n                  if (auth_end >= 0)\n                    {\n                      found_auth_end = TRUE;\n                      buffer->size = auth_end;\n                      extra_data = buffer->pos - buffer->size;\n\n                      /* We may have gotten some extra data which is not part of\n                         the auth handshake, keep it for the next iteration. */\n                      if (extra_data > 0)\n                        side->extra_input_data = g_bytes_new (buffer->data + buffer->size, extra_data);\n                    }\n                  else if (auth_end == FIND_AUTH_END_ABORT)\n                    {\n                      buffer_unref (buffer);\n                      if (client->proxy->log_messages)\n                        g_print (\"Invalid AUTH line, aborting\\n\");\n                      side_closed (side);\n                      break;\n                    }\n                }\n\n              got_buffer_from_side (side, buffer);\n\n              if (found_auth_end)\n                client->authenticated = TRUE;\n            }\n          else\n            {\n              buffer_unref (buffer);\n            }\n        }\n      else if (buffer->pos == buffer->size)\n        {\n          if (buffer == &side->header_buffer)\n            {\n              gssize required;\n              required = g_dbus_message_bytes_needed (buffer->data, buffer->size, &error);\n              if (required < 0)\n                {\n                  g_warning (\"Invalid message header read\");\n                  side_closed (side);\n                }\n              else\n                {\n                  side->current_read_buffer = buffer_new (required, buffer);\n                }\n            }\n          else\n            {\n              got_buffer_from_side (side, buffer);\n              side->header_buffer.pos = 0;\n              side->current_read_buffer = &side->header_buffer;\n            }\n        }\n    }\n\n  if (side->closed)\n    {\n      side->in_source = NULL;\n      retval = G_SOURCE_REMOVE;\n    }\n\n  g_object_unref (client);\n\n  return retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -54,6 +54,14 @@\n                       if (extra_data > 0)\n                         side->extra_input_data = g_bytes_new (buffer->data + buffer->size, extra_data);\n                     }\n+                  else if (auth_end == FIND_AUTH_END_ABORT)\n+                    {\n+                      buffer_unref (buffer);\n+                      if (client->proxy->log_messages)\n+                        g_print (\"Invalid AUTH line, aborting\\n\");\n+                      side_closed (side);\n+                      break;\n+                    }\n                 }\n \n               got_buffer_from_side (side, buffer);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                  else if (auth_end == FIND_AUTH_END_ABORT)",
                "                    {",
                "                      buffer_unref (buffer);",
                "                      if (client->proxy->log_messages)",
                "                        g_print (\"Invalid AUTH line, aborting\\n\");",
                "                      side_closed (side);",
                "                      break;",
                "                    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-6560",
        "func_name": "flatpak/flatpak_proxy_client_finalize",
        "description": "In dbus-proxy/flatpak-proxy.c in Flatpak before 0.8.9, and 0.9.x and 0.10.x before 0.10.3, crafted D-Bus messages to the host can be used to break out of the sandbox, because whitespace handling in the proxy is not identical to whitespace handling in the daemon.",
        "git_url": "https://github.com/flatpak/flatpak/commit/52346bf187b5a7f1c0fe9075b328b7ad6abe78f6",
        "commit_title": "Fix vulnerability in dbus proxy",
        "commit_text": " During the authentication all client data is directly forwarded to the dbus daemon as is, until we detect the BEGIN command after which we start filtering the binary dbus protocol.  Unfortunately the detection of the BEGIN command in the proxy did not exactly match the detection in the dbus daemon. A BEGIN followed by a space or tab was considered ok in the daemon but not by the proxy. This could be exploited to send arbitrary dbus messages to the host, which can be used to break out of the sandbox.  This was noticed by Gabriel Campana of The Google Security Team.  This fix makes the detection of the authentication phase end match the dbus code. In addition we duplicate the authentication line validation from dbus, which includes ensuring all data is ASCII, and limiting the size of a line to 16k. In fact, we add some extra stringent checks, disallowing ASCII control chars and requiring that auth lines start with a capital letter.",
        "func_before": "static void\nflatpak_proxy_client_finalize (GObject *object)\n{\n  FlatpakProxyClient *client = FLATPAK_PROXY_CLIENT (object);\n\n  client->proxy->clients = g_list_remove (client->proxy->clients, client);\n  g_clear_object (&client->proxy);\n\n  g_hash_table_destroy (client->rewrite_reply);\n  g_hash_table_destroy (client->get_owner_reply);\n  g_hash_table_destroy (client->unique_id_policy);\n\n  free_side (&client->client_side);\n  free_side (&client->bus_side);\n\n  G_OBJECT_CLASS (flatpak_proxy_client_parent_class)->finalize (object);\n}",
        "func": "static void\nflatpak_proxy_client_finalize (GObject *object)\n{\n  FlatpakProxyClient *client = FLATPAK_PROXY_CLIENT (object);\n\n  client->proxy->clients = g_list_remove (client->proxy->clients, client);\n  g_clear_object (&client->proxy);\n\n  g_byte_array_free (client->auth_buffer, TRUE);\n  g_hash_table_destroy (client->rewrite_reply);\n  g_hash_table_destroy (client->get_owner_reply);\n  g_hash_table_destroy (client->unique_id_policy);\n\n  free_side (&client->client_side);\n  free_side (&client->bus_side);\n\n  G_OBJECT_CLASS (flatpak_proxy_client_parent_class)->finalize (object);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n   client->proxy->clients = g_list_remove (client->proxy->clients, client);\n   g_clear_object (&client->proxy);\n \n+  g_byte_array_free (client->auth_buffer, TRUE);\n   g_hash_table_destroy (client->rewrite_reply);\n   g_hash_table_destroy (client->get_owner_reply);\n   g_hash_table_destroy (client->unique_id_policy);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  g_byte_array_free (client->auth_buffer, TRUE);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-6560",
        "func_name": "flatpak/flatpak_proxy_client_init",
        "description": "In dbus-proxy/flatpak-proxy.c in Flatpak before 0.8.9, and 0.9.x and 0.10.x before 0.10.3, crafted D-Bus messages to the host can be used to break out of the sandbox, because whitespace handling in the proxy is not identical to whitespace handling in the daemon.",
        "git_url": "https://github.com/flatpak/flatpak/commit/52346bf187b5a7f1c0fe9075b328b7ad6abe78f6",
        "commit_title": "Fix vulnerability in dbus proxy",
        "commit_text": " During the authentication all client data is directly forwarded to the dbus daemon as is, until we detect the BEGIN command after which we start filtering the binary dbus protocol.  Unfortunately the detection of the BEGIN command in the proxy did not exactly match the detection in the dbus daemon. A BEGIN followed by a space or tab was considered ok in the daemon but not by the proxy. This could be exploited to send arbitrary dbus messages to the host, which can be used to break out of the sandbox.  This was noticed by Gabriel Campana of The Google Security Team.  This fix makes the detection of the authentication phase end match the dbus code. In addition we duplicate the authentication line validation from dbus, which includes ensuring all data is ASCII, and limiting the size of a line to 16k. In fact, we add some extra stringent checks, disallowing ASCII control chars and requiring that auth lines start with a capital letter.",
        "func_before": "static void\nflatpak_proxy_client_init (FlatpakProxyClient *client)\n{\n  init_side (client, &client->client_side);\n  init_side (client, &client->bus_side);\n\n  client->auth_end_offset = AUTH_END_INIT_OFFSET;\n  client->rewrite_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_object_unref);\n  client->get_owner_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_free);\n  client->unique_id_policy = g_hash_table_new_full (g_str_hash, g_str_equal, g_free, NULL);\n}",
        "func": "static void\nflatpak_proxy_client_init (FlatpakProxyClient *client)\n{\n  init_side (client, &client->client_side);\n  init_side (client, &client->bus_side);\n\n  client->auth_buffer = g_byte_array_new ();\n  client->rewrite_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_object_unref);\n  client->get_owner_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_free);\n  client->unique_id_policy = g_hash_table_new_full (g_str_hash, g_str_equal, g_free, NULL);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n   init_side (client, &client->client_side);\n   init_side (client, &client->bus_side);\n \n-  client->auth_end_offset = AUTH_END_INIT_OFFSET;\n+  client->auth_buffer = g_byte_array_new ();\n   client->rewrite_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_object_unref);\n   client->get_owner_reply = g_hash_table_new_full (g_direct_hash, g_direct_equal, NULL, g_free);\n   client->unique_id_policy = g_hash_table_new_full (g_str_hash, g_str_equal, g_free, NULL);",
        "diff_line_info": {
            "deleted_lines": [
                "  client->auth_end_offset = AUTH_END_INIT_OFFSET;"
            ],
            "added_lines": [
                "  client->auth_buffer = g_byte_array_new ();"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-6560",
        "func_name": "flatpak/find_auth_end",
        "description": "In dbus-proxy/flatpak-proxy.c in Flatpak before 0.8.9, and 0.9.x and 0.10.x before 0.10.3, crafted D-Bus messages to the host can be used to break out of the sandbox, because whitespace handling in the proxy is not identical to whitespace handling in the daemon.",
        "git_url": "https://github.com/flatpak/flatpak/commit/52346bf187b5a7f1c0fe9075b328b7ad6abe78f6",
        "commit_title": "Fix vulnerability in dbus proxy",
        "commit_text": " During the authentication all client data is directly forwarded to the dbus daemon as is, until we detect the BEGIN command after which we start filtering the binary dbus protocol.  Unfortunately the detection of the BEGIN command in the proxy did not exactly match the detection in the dbus daemon. A BEGIN followed by a space or tab was considered ok in the daemon but not by the proxy. This could be exploited to send arbitrary dbus messages to the host, which can be used to break out of the sandbox.  This was noticed by Gabriel Campana of The Google Security Team.  This fix makes the detection of the authentication phase end match the dbus code. In addition we duplicate the authentication line validation from dbus, which includes ensuring all data is ASCII, and limiting the size of a line to 16k. In fact, we add some extra stringent checks, disallowing ASCII control chars and requiring that auth lines start with a capital letter.",
        "func_before": "static gssize\nfind_auth_end (FlatpakProxyClient *client, Buffer *buffer)\n{\n  guchar *match;\n  int i;\n\n  /* First try to match any leftover at the start */\n  if (client->auth_end_offset > 0)\n    {\n      gsize left = strlen (AUTH_END_STRING) - client->auth_end_offset;\n      gsize to_match = MIN (left, buffer->pos);\n      /* Matched at least up to to_match */\n      if (memcmp (buffer->data, &AUTH_END_STRING[client->auth_end_offset], to_match) == 0)\n        {\n          client->auth_end_offset += to_match;\n\n          /* Matched all */\n          if (client->auth_end_offset == strlen (AUTH_END_STRING))\n            return to_match;\n\n          /* Matched to end of buffer */\n          return -1;\n        }\n\n      /* Did not actually match at start */\n      client->auth_end_offset = -1;\n    }\n\n  /* Look for whole match inside buffer */\n  match = memmem (buffer, buffer->pos,\n                  AUTH_END_STRING, strlen (AUTH_END_STRING));\n  if (match != NULL)\n    return match - buffer->data + strlen (AUTH_END_STRING);\n\n  /* Record longest prefix match at the end */\n  for (i = MIN (strlen (AUTH_END_STRING) - 1, buffer->pos); i > 0; i--)\n    {\n      if (memcmp (buffer->data + buffer->pos - i, AUTH_END_STRING, i) == 0)\n        {\n          client->auth_end_offset = i;\n          break;\n        }\n    }\n\n  return -1;\n}",
        "func": "static gssize\nfind_auth_end (FlatpakProxyClient *client, Buffer *buffer)\n{\n  goffset offset = 0;\n  gsize original_size = client->auth_buffer->len;\n\n  /* Add the new data to the remaining data from last iteration */\n  g_byte_array_append (client->auth_buffer, buffer->data, buffer->pos);\n\n  while (TRUE)\n    {\n      guint8 *line_start = client->auth_buffer->data + offset;\n      gsize remaining_data = client->auth_buffer->len - offset;\n      guint8 *line_end;\n\n      line_end = memmem (line_start, remaining_data,\n                         AUTH_LINE_SENTINEL, strlen (AUTH_LINE_SENTINEL));\n      if (line_end) /* Found end of line */\n        {\n          offset = (line_end + strlen (AUTH_LINE_SENTINEL) - line_start);\n\n          if (!auth_line_is_valid (line_start, line_end))\n            return FIND_AUTH_END_ABORT;\n\n          *line_end = 0;\n          if (auth_line_is_begin (line_start))\n            return offset - original_size;\n\n          /* continue with next line */\n        }\n      else\n        {\n          /* No end-of-line in this buffer */\n          g_byte_array_remove_range (client->auth_buffer, 0, offset);\n\n          /* Abort if more than 16k before newline, similar to what dbus-daemon does */\n          if (client->auth_buffer->len >= 16*1024)\n            return FIND_AUTH_END_ABORT;\n\n          return FIND_AUTH_END_CONTINUE;\n        }\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,46 +1,43 @@\n static gssize\n find_auth_end (FlatpakProxyClient *client, Buffer *buffer)\n {\n-  guchar *match;\n-  int i;\n+  goffset offset = 0;\n+  gsize original_size = client->auth_buffer->len;\n \n-  /* First try to match any leftover at the start */\n-  if (client->auth_end_offset > 0)\n+  /* Add the new data to the remaining data from last iteration */\n+  g_byte_array_append (client->auth_buffer, buffer->data, buffer->pos);\n+\n+  while (TRUE)\n     {\n-      gsize left = strlen (AUTH_END_STRING) - client->auth_end_offset;\n-      gsize to_match = MIN (left, buffer->pos);\n-      /* Matched at least up to to_match */\n-      if (memcmp (buffer->data, &AUTH_END_STRING[client->auth_end_offset], to_match) == 0)\n+      guint8 *line_start = client->auth_buffer->data + offset;\n+      gsize remaining_data = client->auth_buffer->len - offset;\n+      guint8 *line_end;\n+\n+      line_end = memmem (line_start, remaining_data,\n+                         AUTH_LINE_SENTINEL, strlen (AUTH_LINE_SENTINEL));\n+      if (line_end) /* Found end of line */\n         {\n-          client->auth_end_offset += to_match;\n+          offset = (line_end + strlen (AUTH_LINE_SENTINEL) - line_start);\n \n-          /* Matched all */\n-          if (client->auth_end_offset == strlen (AUTH_END_STRING))\n-            return to_match;\n+          if (!auth_line_is_valid (line_start, line_end))\n+            return FIND_AUTH_END_ABORT;\n \n-          /* Matched to end of buffer */\n-          return -1;\n+          *line_end = 0;\n+          if (auth_line_is_begin (line_start))\n+            return offset - original_size;\n+\n+          /* continue with next line */\n         }\n+      else\n+        {\n+          /* No end-of-line in this buffer */\n+          g_byte_array_remove_range (client->auth_buffer, 0, offset);\n \n-      /* Did not actually match at start */\n-      client->auth_end_offset = -1;\n-    }\n+          /* Abort if more than 16k before newline, similar to what dbus-daemon does */\n+          if (client->auth_buffer->len >= 16*1024)\n+            return FIND_AUTH_END_ABORT;\n \n-  /* Look for whole match inside buffer */\n-  match = memmem (buffer, buffer->pos,\n-                  AUTH_END_STRING, strlen (AUTH_END_STRING));\n-  if (match != NULL)\n-    return match - buffer->data + strlen (AUTH_END_STRING);\n-\n-  /* Record longest prefix match at the end */\n-  for (i = MIN (strlen (AUTH_END_STRING) - 1, buffer->pos); i > 0; i--)\n-    {\n-      if (memcmp (buffer->data + buffer->pos - i, AUTH_END_STRING, i) == 0)\n-        {\n-          client->auth_end_offset = i;\n-          break;\n+          return FIND_AUTH_END_CONTINUE;\n         }\n     }\n-\n-  return -1;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  guchar *match;",
                "  int i;",
                "  /* First try to match any leftover at the start */",
                "  if (client->auth_end_offset > 0)",
                "      gsize left = strlen (AUTH_END_STRING) - client->auth_end_offset;",
                "      gsize to_match = MIN (left, buffer->pos);",
                "      /* Matched at least up to to_match */",
                "      if (memcmp (buffer->data, &AUTH_END_STRING[client->auth_end_offset], to_match) == 0)",
                "          client->auth_end_offset += to_match;",
                "          /* Matched all */",
                "          if (client->auth_end_offset == strlen (AUTH_END_STRING))",
                "            return to_match;",
                "          /* Matched to end of buffer */",
                "          return -1;",
                "      /* Did not actually match at start */",
                "      client->auth_end_offset = -1;",
                "    }",
                "  /* Look for whole match inside buffer */",
                "  match = memmem (buffer, buffer->pos,",
                "                  AUTH_END_STRING, strlen (AUTH_END_STRING));",
                "  if (match != NULL)",
                "    return match - buffer->data + strlen (AUTH_END_STRING);",
                "",
                "  /* Record longest prefix match at the end */",
                "  for (i = MIN (strlen (AUTH_END_STRING) - 1, buffer->pos); i > 0; i--)",
                "    {",
                "      if (memcmp (buffer->data + buffer->pos - i, AUTH_END_STRING, i) == 0)",
                "        {",
                "          client->auth_end_offset = i;",
                "          break;",
                "",
                "  return -1;"
            ],
            "added_lines": [
                "  goffset offset = 0;",
                "  gsize original_size = client->auth_buffer->len;",
                "  /* Add the new data to the remaining data from last iteration */",
                "  g_byte_array_append (client->auth_buffer, buffer->data, buffer->pos);",
                "",
                "  while (TRUE)",
                "      guint8 *line_start = client->auth_buffer->data + offset;",
                "      gsize remaining_data = client->auth_buffer->len - offset;",
                "      guint8 *line_end;",
                "",
                "      line_end = memmem (line_start, remaining_data,",
                "                         AUTH_LINE_SENTINEL, strlen (AUTH_LINE_SENTINEL));",
                "      if (line_end) /* Found end of line */",
                "          offset = (line_end + strlen (AUTH_LINE_SENTINEL) - line_start);",
                "          if (!auth_line_is_valid (line_start, line_end))",
                "            return FIND_AUTH_END_ABORT;",
                "          *line_end = 0;",
                "          if (auth_line_is_begin (line_start))",
                "            return offset - original_size;",
                "",
                "          /* continue with next line */",
                "      else",
                "        {",
                "          /* No end-of-line in this buffer */",
                "          g_byte_array_remove_range (client->auth_buffer, 0, offset);",
                "          /* Abort if more than 16k before newline, similar to what dbus-daemon does */",
                "          if (client->auth_buffer->len >= 16*1024)",
                "            return FIND_AUTH_END_ABORT;",
                "          return FIND_AUTH_END_CONTINUE;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-18792",
        "func_name": "OISF/suricata/StreamTcpPacket",
        "description": "An issue was discovered in Suricata 5.0.0. It is possible to bypass/evade any tcp based signature by overlapping a TCP segment with a fake FIN packet. The fake FIN packet is injected just before the PUSH ACK packet we want to bypass. The PUSH ACK packet (containing the data) will be ignored by Suricata because it overlaps the FIN packet (the sequence and ack number are identical in the two packets). The client will ignore the fake FIN packet because the ACK flag is not set. Both linux and windows clients are ignoring the injected packet.",
        "git_url": "https://github.com/OISF/suricata/commit/fa692df37a796c3330c81988d15ef1a219afc006",
        "commit_title": "stream: reject broken ACK packets",
        "commit_text": " Fix evasion posibility by rejecting packets with a broken ACK field. These packets have a non-0 ACK field, but do not have a ACK flag set.  Bug #3324. ",
        "func_before": "int StreamTcpPacket (ThreadVars *tv, Packet *p, StreamTcpThread *stt,\n                     PacketQueue *pq)\n{\n    SCEnter();\n\n    DEBUG_ASSERT_FLOW_LOCKED(p->flow);\n\n    SCLogDebug(\"p->pcap_cnt %\"PRIu64, p->pcap_cnt);\n\n    HandleThreadId(tv, p, stt);\n\n    TcpSession *ssn = (TcpSession *)p->flow->protoctx;\n\n    /* track TCP flags */\n    if (ssn != NULL) {\n        ssn->tcp_packet_flags |= p->tcph->th_flags;\n        if (PKT_IS_TOSERVER(p))\n            ssn->client.tcp_flags |= p->tcph->th_flags;\n        else if (PKT_IS_TOCLIENT(p))\n            ssn->server.tcp_flags |= p->tcph->th_flags;\n\n        /* check if we need to unset the ASYNC flag */\n        if (ssn->flags & STREAMTCP_FLAG_ASYNC &&\n            ssn->client.tcp_flags != 0 &&\n            ssn->server.tcp_flags != 0)\n        {\n            SCLogDebug(\"ssn %p: removing ASYNC flag as we have packets on both sides\", ssn);\n            ssn->flags &= ~STREAMTCP_FLAG_ASYNC;\n        }\n    }\n\n    /* update counters */\n    if ((p->tcph->th_flags & (TH_SYN|TH_ACK)) == (TH_SYN|TH_ACK)) {\n        StatsIncr(tv, stt->counter_tcp_synack);\n    } else if (p->tcph->th_flags & (TH_SYN)) {\n        StatsIncr(tv, stt->counter_tcp_syn);\n    }\n    if (p->tcph->th_flags & (TH_RST)) {\n        StatsIncr(tv, stt->counter_tcp_rst);\n    }\n\n    /* broken TCP http://ask.wireshark.org/questions/3183/acknowledgment-number-broken-tcp-the-acknowledge-field-is-nonzero-while-the-ack-flag-is-not-set */\n    if (!(p->tcph->th_flags & TH_ACK) && TCP_GET_ACK(p) != 0) {\n        StreamTcpSetEvent(p, STREAM_PKT_BROKEN_ACK);\n    }\n\n    /* If we are on IPS mode, and got a drop action triggered from\n     * the IP only module, or from a reassembled msg and/or from an\n     * applayer detection, then drop the rest of the packets of the\n     * same stream and avoid inspecting it any further */\n    if (StreamTcpCheckFlowDrops(p) == 1) {\n        SCLogDebug(\"This flow/stream triggered a drop rule\");\n        FlowSetNoPacketInspectionFlag(p->flow);\n        DecodeSetNoPacketInspectionFlag(p);\n        StreamTcpDisableAppLayer(p->flow);\n        PACKET_DROP(p);\n        /* return the segments to the pool */\n        StreamTcpSessionPktFree(p);\n        SCReturnInt(0);\n    }\n\n    if (ssn == NULL || ssn->state == TCP_NONE) {\n        if (StreamTcpPacketStateNone(tv, p, stt, ssn, &stt->pseudo_queue) == -1) {\n            goto error;\n        }\n\n        if (ssn != NULL)\n            SCLogDebug(\"ssn->alproto %\"PRIu16\"\", p->flow->alproto);\n    } else {\n        /* special case for PKT_PSEUDO_STREAM_END packets:\n         * bypass the state handling and various packet checks,\n         * we care about reassembly here. */\n        if (p->flags & PKT_PSEUDO_STREAM_END) {\n            if (PKT_IS_TOCLIENT(p)) {\n                ssn->client.last_ack = TCP_GET_ACK(p);\n                StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                        &ssn->server, p, pq);\n            } else {\n                ssn->server.last_ack = TCP_GET_ACK(p);\n                StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                        &ssn->client, p, pq);\n            }\n            /* straight to 'skip' as we already handled reassembly */\n            goto skip;\n        }\n\n        if (p->flow->flags & FLOW_WRONG_THREAD ||\n            ssn->client.flags & STREAMTCP_STREAM_FLAG_GAP ||\n            ssn->server.flags & STREAMTCP_STREAM_FLAG_GAP)\n        {\n            /* Stream and/or session in known bad condition. Block events\n             * from being set. */\n            p->flags |= PKT_STREAM_NO_EVENTS;\n        }\n\n        if (StreamTcpPacketIsKeepAlive(ssn, p) == 1) {\n            goto skip;\n        }\n        if (StreamTcpPacketIsKeepAliveACK(ssn, p) == 1) {\n            StreamTcpClearKeepAliveFlag(ssn, p);\n            goto skip;\n        }\n        StreamTcpClearKeepAliveFlag(ssn, p);\n\n        /* if packet is not a valid window update, check if it is perhaps\n         * a bad window update that we should ignore (and alert on) */\n        if (StreamTcpPacketIsFinShutdownAck(ssn, p) == 0)\n            if (StreamTcpPacketIsWindowUpdate(ssn, p) == 0)\n                if (StreamTcpPacketIsBadWindowUpdate(ssn,p))\n                    goto skip;\n\n        /* handle the per 'state' logic */\n        if (StreamTcpStateDispatch(tv, p, stt, ssn, &stt->pseudo_queue, ssn->state) < 0)\n            goto error;\n\n    skip:\n        StreamTcpPacketCheckPostRst(ssn, p);\n\n        if (ssn->state >= TCP_ESTABLISHED) {\n            p->flags |= PKT_STREAM_EST;\n        }\n    }\n\n    /* deal with a pseudo packet that is created upon receiving a RST\n     * segment. To be sure we process both sides of the connection, we\n     * inject a fake packet into the system, forcing reassembly of the\n     * opposing direction.\n     * There should be only one, but to be sure we do a while loop. */\n    if (ssn != NULL) {\n        while (stt->pseudo_queue.len > 0) {\n            SCLogDebug(\"processing pseudo packet / stream end\");\n            Packet *np = PacketDequeue(&stt->pseudo_queue);\n            if (np != NULL) {\n                /* process the opposing direction of the original packet */\n                if (PKT_IS_TOSERVER(np)) {\n                    SCLogDebug(\"pseudo packet is to server\");\n                    StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                            &ssn->client, np, NULL);\n                } else {\n                    SCLogDebug(\"pseudo packet is to client\");\n                    StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                            &ssn->server, np, NULL);\n                }\n\n                /* enqueue this packet so we inspect it in detect etc */\n                PacketEnqueue(pq, np);\n            }\n            SCLogDebug(\"processing pseudo packet / stream end done\");\n        }\n\n        /* recalc the csum on the packet if it was modified */\n        if (p->flags & PKT_STREAM_MODIFIED) {\n            ReCalculateChecksum(p);\n        }\n        /* check for conditions that may make us not want to log this packet */\n\n        /* streams that hit depth */\n        if ((ssn->client.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED) ||\n             (ssn->server.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED))\n        {\n            /* we can call bypass callback, if enabled */\n            if (StreamTcpBypassEnabled()) {\n                PacketBypassCallback(p);\n            }\n        }\n\n        if ((ssn->client.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED) ||\n             (ssn->server.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED))\n        {\n            p->flags |= PKT_STREAM_NOPCAPLOG;\n        }\n\n        /* encrypted packets */\n        if ((PKT_IS_TOSERVER(p) && (ssn->client.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY)) ||\n            (PKT_IS_TOCLIENT(p) && (ssn->server.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY)))\n        {\n            p->flags |= PKT_STREAM_NOPCAPLOG;\n        }\n\n        if (ssn->flags & STREAMTCP_FLAG_BYPASS) {\n            /* we can call bypass callback, if enabled */\n            if (StreamTcpBypassEnabled()) {\n                PacketBypassCallback(p);\n            }\n\n        /* if stream is dead and we have no detect engine at all, bypass. */\n        } else if (g_detect_disabled &&\n                (ssn->client.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY) &&\n                (ssn->server.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY) &&\n                StreamTcpBypassEnabled())\n        {\n            SCLogDebug(\"bypass as stream is dead and we have no rules\");\n            PacketBypassCallback(p);\n        }\n    }\n\n    SCReturnInt(0);\n\nerror:\n    /* make sure we don't leave packets in our pseudo queue */\n    while (stt->pseudo_queue.len > 0) {\n        Packet *np = PacketDequeue(&stt->pseudo_queue);\n        if (np != NULL) {\n            PacketEnqueue(pq, np);\n        }\n    }\n\n    /* recalc the csum on the packet if it was modified */\n    if (p->flags & PKT_STREAM_MODIFIED) {\n        ReCalculateChecksum(p);\n    }\n\n    if (StreamTcpInlineDropInvalid()) {\n        /* disable payload inspection as we're dropping this packet\n         * anyway. Doesn't disable all detection, so we can still\n         * match on the stream event that was set. */\n        DecodeSetNoPayloadInspectionFlag(p);\n        PACKET_DROP(p);\n    }\n    SCReturnInt(-1);\n}",
        "func": "int StreamTcpPacket (ThreadVars *tv, Packet *p, StreamTcpThread *stt,\n                     PacketQueue *pq)\n{\n    SCEnter();\n\n    DEBUG_ASSERT_FLOW_LOCKED(p->flow);\n\n    SCLogDebug(\"p->pcap_cnt %\"PRIu64, p->pcap_cnt);\n\n    HandleThreadId(tv, p, stt);\n\n    TcpSession *ssn = (TcpSession *)p->flow->protoctx;\n\n    /* track TCP flags */\n    if (ssn != NULL) {\n        ssn->tcp_packet_flags |= p->tcph->th_flags;\n        if (PKT_IS_TOSERVER(p))\n            ssn->client.tcp_flags |= p->tcph->th_flags;\n        else if (PKT_IS_TOCLIENT(p))\n            ssn->server.tcp_flags |= p->tcph->th_flags;\n\n        /* check if we need to unset the ASYNC flag */\n        if (ssn->flags & STREAMTCP_FLAG_ASYNC &&\n            ssn->client.tcp_flags != 0 &&\n            ssn->server.tcp_flags != 0)\n        {\n            SCLogDebug(\"ssn %p: removing ASYNC flag as we have packets on both sides\", ssn);\n            ssn->flags &= ~STREAMTCP_FLAG_ASYNC;\n        }\n    }\n\n    /* update counters */\n    if ((p->tcph->th_flags & (TH_SYN|TH_ACK)) == (TH_SYN|TH_ACK)) {\n        StatsIncr(tv, stt->counter_tcp_synack);\n    } else if (p->tcph->th_flags & (TH_SYN)) {\n        StatsIncr(tv, stt->counter_tcp_syn);\n    }\n    if (p->tcph->th_flags & (TH_RST)) {\n        StatsIncr(tv, stt->counter_tcp_rst);\n    }\n\n    /* broken TCP http://ask.wireshark.org/questions/3183/acknowledgment-number-broken-tcp-the-acknowledge-field-is-nonzero-while-the-ack-flag-is-not-set */\n    if (!(p->tcph->th_flags & TH_ACK) && TCP_GET_ACK(p) != 0) {\n        StreamTcpSetEvent(p, STREAM_PKT_BROKEN_ACK);\n        goto error;\n    }\n\n    /* If we are on IPS mode, and got a drop action triggered from\n     * the IP only module, or from a reassembled msg and/or from an\n     * applayer detection, then drop the rest of the packets of the\n     * same stream and avoid inspecting it any further */\n    if (StreamTcpCheckFlowDrops(p) == 1) {\n        SCLogDebug(\"This flow/stream triggered a drop rule\");\n        FlowSetNoPacketInspectionFlag(p->flow);\n        DecodeSetNoPacketInspectionFlag(p);\n        StreamTcpDisableAppLayer(p->flow);\n        PACKET_DROP(p);\n        /* return the segments to the pool */\n        StreamTcpSessionPktFree(p);\n        SCReturnInt(0);\n    }\n\n    if (ssn == NULL || ssn->state == TCP_NONE) {\n        if (StreamTcpPacketStateNone(tv, p, stt, ssn, &stt->pseudo_queue) == -1) {\n            goto error;\n        }\n\n        if (ssn != NULL)\n            SCLogDebug(\"ssn->alproto %\"PRIu16\"\", p->flow->alproto);\n    } else {\n        /* special case for PKT_PSEUDO_STREAM_END packets:\n         * bypass the state handling and various packet checks,\n         * we care about reassembly here. */\n        if (p->flags & PKT_PSEUDO_STREAM_END) {\n            if (PKT_IS_TOCLIENT(p)) {\n                ssn->client.last_ack = TCP_GET_ACK(p);\n                StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                        &ssn->server, p, pq);\n            } else {\n                ssn->server.last_ack = TCP_GET_ACK(p);\n                StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                        &ssn->client, p, pq);\n            }\n            /* straight to 'skip' as we already handled reassembly */\n            goto skip;\n        }\n\n        if (p->flow->flags & FLOW_WRONG_THREAD ||\n            ssn->client.flags & STREAMTCP_STREAM_FLAG_GAP ||\n            ssn->server.flags & STREAMTCP_STREAM_FLAG_GAP)\n        {\n            /* Stream and/or session in known bad condition. Block events\n             * from being set. */\n            p->flags |= PKT_STREAM_NO_EVENTS;\n        }\n\n        if (StreamTcpPacketIsKeepAlive(ssn, p) == 1) {\n            goto skip;\n        }\n        if (StreamTcpPacketIsKeepAliveACK(ssn, p) == 1) {\n            StreamTcpClearKeepAliveFlag(ssn, p);\n            goto skip;\n        }\n        StreamTcpClearKeepAliveFlag(ssn, p);\n\n        /* if packet is not a valid window update, check if it is perhaps\n         * a bad window update that we should ignore (and alert on) */\n        if (StreamTcpPacketIsFinShutdownAck(ssn, p) == 0)\n            if (StreamTcpPacketIsWindowUpdate(ssn, p) == 0)\n                if (StreamTcpPacketIsBadWindowUpdate(ssn,p))\n                    goto skip;\n\n        /* handle the per 'state' logic */\n        if (StreamTcpStateDispatch(tv, p, stt, ssn, &stt->pseudo_queue, ssn->state) < 0)\n            goto error;\n\n    skip:\n        StreamTcpPacketCheckPostRst(ssn, p);\n\n        if (ssn->state >= TCP_ESTABLISHED) {\n            p->flags |= PKT_STREAM_EST;\n        }\n    }\n\n    /* deal with a pseudo packet that is created upon receiving a RST\n     * segment. To be sure we process both sides of the connection, we\n     * inject a fake packet into the system, forcing reassembly of the\n     * opposing direction.\n     * There should be only one, but to be sure we do a while loop. */\n    if (ssn != NULL) {\n        while (stt->pseudo_queue.len > 0) {\n            SCLogDebug(\"processing pseudo packet / stream end\");\n            Packet *np = PacketDequeue(&stt->pseudo_queue);\n            if (np != NULL) {\n                /* process the opposing direction of the original packet */\n                if (PKT_IS_TOSERVER(np)) {\n                    SCLogDebug(\"pseudo packet is to server\");\n                    StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                            &ssn->client, np, NULL);\n                } else {\n                    SCLogDebug(\"pseudo packet is to client\");\n                    StreamTcpReassembleHandleSegment(tv, stt->ra_ctx, ssn,\n                            &ssn->server, np, NULL);\n                }\n\n                /* enqueue this packet so we inspect it in detect etc */\n                PacketEnqueue(pq, np);\n            }\n            SCLogDebug(\"processing pseudo packet / stream end done\");\n        }\n\n        /* recalc the csum on the packet if it was modified */\n        if (p->flags & PKT_STREAM_MODIFIED) {\n            ReCalculateChecksum(p);\n        }\n        /* check for conditions that may make us not want to log this packet */\n\n        /* streams that hit depth */\n        if ((ssn->client.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED) ||\n             (ssn->server.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED))\n        {\n            /* we can call bypass callback, if enabled */\n            if (StreamTcpBypassEnabled()) {\n                PacketBypassCallback(p);\n            }\n        }\n\n        if ((ssn->client.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED) ||\n             (ssn->server.flags & STREAMTCP_STREAM_FLAG_DEPTH_REACHED))\n        {\n            p->flags |= PKT_STREAM_NOPCAPLOG;\n        }\n\n        /* encrypted packets */\n        if ((PKT_IS_TOSERVER(p) && (ssn->client.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY)) ||\n            (PKT_IS_TOCLIENT(p) && (ssn->server.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY)))\n        {\n            p->flags |= PKT_STREAM_NOPCAPLOG;\n        }\n\n        if (ssn->flags & STREAMTCP_FLAG_BYPASS) {\n            /* we can call bypass callback, if enabled */\n            if (StreamTcpBypassEnabled()) {\n                PacketBypassCallback(p);\n            }\n\n        /* if stream is dead and we have no detect engine at all, bypass. */\n        } else if (g_detect_disabled &&\n                (ssn->client.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY) &&\n                (ssn->server.flags & STREAMTCP_STREAM_FLAG_NOREASSEMBLY) &&\n                StreamTcpBypassEnabled())\n        {\n            SCLogDebug(\"bypass as stream is dead and we have no rules\");\n            PacketBypassCallback(p);\n        }\n    }\n\n    SCReturnInt(0);\n\nerror:\n    /* make sure we don't leave packets in our pseudo queue */\n    while (stt->pseudo_queue.len > 0) {\n        Packet *np = PacketDequeue(&stt->pseudo_queue);\n        if (np != NULL) {\n            PacketEnqueue(pq, np);\n        }\n    }\n\n    /* recalc the csum on the packet if it was modified */\n    if (p->flags & PKT_STREAM_MODIFIED) {\n        ReCalculateChecksum(p);\n    }\n\n    if (StreamTcpInlineDropInvalid()) {\n        /* disable payload inspection as we're dropping this packet\n         * anyway. Doesn't disable all detection, so we can still\n         * match on the stream event that was set. */\n        DecodeSetNoPayloadInspectionFlag(p);\n        PACKET_DROP(p);\n    }\n    SCReturnInt(-1);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -42,6 +42,7 @@\n     /* broken TCP http://ask.wireshark.org/questions/3183/acknowledgment-number-broken-tcp-the-acknowledge-field-is-nonzero-while-the-ack-flag-is-not-set */\n     if (!(p->tcph->th_flags & TH_ACK) && TCP_GET_ACK(p) != 0) {\n         StreamTcpSetEvent(p, STREAM_PKT_BROKEN_ACK);\n+        goto error;\n     }\n \n     /* If we are on IPS mode, and got a drop action triggered from",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        goto error;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-18792",
        "func_name": "OISF/suricata/StreamTcpTest10",
        "description": "An issue was discovered in Suricata 5.0.0. It is possible to bypass/evade any tcp based signature by overlapping a TCP segment with a fake FIN packet. The fake FIN packet is injected just before the PUSH ACK packet we want to bypass. The PUSH ACK packet (containing the data) will be ignored by Suricata because it overlaps the FIN packet (the sequence and ack number are identical in the two packets). The client will ignore the fake FIN packet because the ACK flag is not set. Both linux and windows clients are ignoring the injected packet.",
        "git_url": "https://github.com/OISF/suricata/commit/fa692df37a796c3330c81988d15ef1a219afc006",
        "commit_title": "stream: reject broken ACK packets",
        "commit_text": " Fix evasion posibility by rejecting packets with a broken ACK field. These packets have a non-0 ACK field, but do not have a ACK flag set.  Bug #3324. ",
        "func_before": "static int StreamTcpTest10 (void)\n{\n    Packet *p = SCMalloc(SIZE_OF_PACKET);\n    FAIL_IF(unlikely(p == NULL));\n    Flow f;\n    ThreadVars tv;\n    StreamTcpThread stt;\n    TCPHdr tcph;\n    uint8_t payload[4];\n    memset(p, 0, SIZE_OF_PACKET);\n    PacketQueue pq;\n    memset(&pq,0,sizeof(PacketQueue));\n    memset (&f, 0, sizeof(Flow));\n    memset(&tv, 0, sizeof (ThreadVars));\n    memset(&stt, 0, sizeof (StreamTcpThread));\n    memset(&tcph, 0, sizeof (TCPHdr));\n    FLOW_INITIALIZE(&f);\n    p->flow = &f;\n\n    StreamTcpUTInit(&stt.ra_ctx);\n    stream_config.async_oneside = TRUE;\n\n    tcph.th_win = htons(5480);\n    tcph.th_seq = htonl(10);\n    tcph.th_ack = htonl(11);\n    tcph.th_flags = TH_SYN;\n    p->tcph = &tcph;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(11);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(11);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK|TH_PUSH;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    StreamTcpCreateTestPacket(payload, 0x42, 3, 4); /*BBB*/\n    p->payload = payload;\n    p->payload_len = 3;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(6);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK|TH_PUSH;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    StreamTcpCreateTestPacket(payload, 0x42, 3, 4); /*BBB*/\n    p->payload = payload;\n    p->payload_len = 3;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    FAIL_IF(((TcpSession *)(p->flow->protoctx))->state != TCP_ESTABLISHED);\n\n    FAIL_IF(! (((TcpSession *)(p->flow->protoctx))->flags & STREAMTCP_FLAG_ASYNC));\n\n    FAIL_IF(((TcpSession *)(p->flow->protoctx))->client.last_ack != 6 &&\n            ((TcpSession *)(p->flow->protoctx))->server.next_seq != 11);\n\n    StreamTcpSessionClear(p->flow->protoctx);\n\n    SCFree(p);\n    FLOW_DESTROY(&f);\n    StreamTcpUTDeinit(stt.ra_ctx);\n    PASS;\n}",
        "func": "static int StreamTcpTest10 (void)\n{\n    Packet *p = SCMalloc(SIZE_OF_PACKET);\n    FAIL_IF(unlikely(p == NULL));\n    Flow f;\n    ThreadVars tv;\n    StreamTcpThread stt;\n    TCPHdr tcph;\n    uint8_t payload[4];\n    memset(p, 0, SIZE_OF_PACKET);\n    PacketQueue pq;\n    memset(&pq,0,sizeof(PacketQueue));\n    memset (&f, 0, sizeof(Flow));\n    memset(&tv, 0, sizeof (ThreadVars));\n    memset(&stt, 0, sizeof (StreamTcpThread));\n    memset(&tcph, 0, sizeof (TCPHdr));\n    FLOW_INITIALIZE(&f);\n    p->flow = &f;\n\n    StreamTcpUTInit(&stt.ra_ctx);\n    stream_config.async_oneside = TRUE;\n\n    tcph.th_win = htons(5480);\n    tcph.th_seq = htonl(10);\n    tcph.th_ack = 0;\n    tcph.th_flags = TH_SYN;\n    p->tcph = &tcph;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(11);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(11);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK|TH_PUSH;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    StreamTcpCreateTestPacket(payload, 0x42, 3, 4); /*BBB*/\n    p->payload = payload;\n    p->payload_len = 3;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    p->tcph->th_seq = htonl(6);\n    p->tcph->th_ack = htonl(11);\n    p->tcph->th_flags = TH_ACK|TH_PUSH;\n    p->flowflags = FLOW_PKT_TOSERVER;\n\n    StreamTcpCreateTestPacket(payload, 0x42, 3, 4); /*BBB*/\n    p->payload = payload;\n    p->payload_len = 3;\n\n    FAIL_IF(StreamTcpPacket(&tv, p, &stt, &pq) == -1);\n\n    FAIL_IF(((TcpSession *)(p->flow->protoctx))->state != TCP_ESTABLISHED);\n\n    FAIL_IF(! (((TcpSession *)(p->flow->protoctx))->flags & STREAMTCP_FLAG_ASYNC));\n\n    FAIL_IF(((TcpSession *)(p->flow->protoctx))->client.last_ack != 6 &&\n            ((TcpSession *)(p->flow->protoctx))->server.next_seq != 11);\n\n    StreamTcpSessionClear(p->flow->protoctx);\n\n    SCFree(p);\n    FLOW_DESTROY(&f);\n    StreamTcpUTDeinit(stt.ra_ctx);\n    PASS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,7 +22,7 @@\n \n     tcph.th_win = htons(5480);\n     tcph.th_seq = htonl(10);\n-    tcph.th_ack = htonl(11);\n+    tcph.th_ack = 0;\n     tcph.th_flags = TH_SYN;\n     p->tcph = &tcph;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    tcph.th_ack = htonl(11);"
            ],
            "added_lines": [
                "    tcph.th_ack = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/sh_unsync",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/d68e1070c3e8f4af7a31040f08bdd98e6d6eac1d",
        "commit_title": "x86/shadow: move OOS flag bit positions",
        "commit_text": " In preparation of reducing struct page_info's shadow_flags field to 16 bits, lower the bit positions used for SHF_out_of_sync and SHF_oos_may_write.  Instead of also adjusting the open coded use in _get_page_type(), introduce shadow_prepare_page_type_change() to contain knowledge of the bit positions to shadow code.  This is part of XSA-280. ",
        "func_before": "int sh_unsync(struct vcpu *v, mfn_t gmfn)\n{\n    struct page_info *pg;\n\n    ASSERT(paging_locked_by_me(v->domain));\n\n    SHADOW_PRINTK(\"%pv gmfn=%\"PRI_mfn\"\\n\", v, mfn_x(gmfn));\n\n    pg = mfn_to_page(gmfn);\n\n    /* Guest page must be shadowed *only* as L1 and *only* once when out\n     * of sync.  Also, get out now if it's already out of sync.\n     * Also, can't safely unsync if some vcpus have paging disabled.*/\n    if ( pg->shadow_flags &\n         ((SHF_page_type_mask & ~SHF_L1_ANY) | SHF_out_of_sync)\n         || sh_page_has_multiple_shadows(pg)\n         || is_pv_vcpu(v)\n         || !v->domain->arch.paging.shadow.oos_active )\n        return 0;\n\n    pg->shadow_flags |= SHF_out_of_sync|SHF_oos_may_write;\n    oos_hash_add(v, gmfn);\n    perfc_incr(shadow_unsync);\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_UNSYNC);\n    return 1;\n}",
        "func": "int sh_unsync(struct vcpu *v, mfn_t gmfn)\n{\n    struct page_info *pg;\n\n    ASSERT(paging_locked_by_me(v->domain));\n\n    SHADOW_PRINTK(\"%pv gmfn=%\"PRI_mfn\"\\n\", v, mfn_x(gmfn));\n\n    pg = mfn_to_page(gmfn);\n\n    /* Guest page must be shadowed *only* as L1 and *only* once when out\n     * of sync.  Also, get out now if it's already out of sync.\n     * Also, can't safely unsync if some vcpus have paging disabled.*/\n    if ( pg->shadow_flags &\n         ((SHF_page_type_mask & ~SHF_L1_ANY) | SHF_out_of_sync)\n         || sh_page_has_multiple_shadows(pg)\n         || is_pv_vcpu(v)\n         || !v->domain->arch.paging.shadow.oos_active )\n        return 0;\n\n    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_out_of_sync);\n    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_oos_may_write);\n\n    pg->shadow_flags |= SHF_out_of_sync|SHF_oos_may_write;\n    oos_hash_add(v, gmfn);\n    perfc_incr(shadow_unsync);\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_UNSYNC);\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,6 +18,9 @@\n          || !v->domain->arch.paging.shadow.oos_active )\n         return 0;\n \n+    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_out_of_sync);\n+    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_oos_may_write);\n+\n     pg->shadow_flags |= SHF_out_of_sync|SHF_oos_may_write;\n     oos_hash_add(v, gmfn);\n     perfc_incr(shadow_unsync);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_out_of_sync);",
                "    BUILD_BUG_ON(!(typeof(pg->shadow_flags))SHF_oos_may_write);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/_get_page_type",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/d68e1070c3e8f4af7a31040f08bdd98e6d6eac1d",
        "commit_title": "x86/shadow: move OOS flag bit positions",
        "commit_text": " In preparation of reducing struct page_info's shadow_flags field to 16 bits, lower the bit positions used for SHF_out_of_sync and SHF_oos_may_write.  Instead of also adjusting the open coded use in _get_page_type(), introduce shadow_prepare_page_type_change() to contain knowledge of the bit positions to shadow code.  This is part of XSA-280. ",
        "func_before": "static int _get_page_type(struct page_info *page, unsigned long type,\n                          bool preemptible)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0, iommu_ret = 0;\n\n    ASSERT(!(type & ~(PGT_type_mask | PGT_pae_xen_l2)));\n    ASSERT(!in_irq());\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x + 1;\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Type count overflow on mfn %\"PRI_mfn\"\\n\",\n                     mfn_x(page_to_mfn(page)));\n            return -EINVAL;\n        }\n        else if ( unlikely((x & PGT_count_mask) == 0) )\n        {\n            struct domain *d = page_get_owner(page);\n\n            /*\n             * Normally we should never let a page go from type count 0\n             * to type count 1 when it is shadowed. One exception:\n             * out-of-sync shadowed pages are allowed to become\n             * writeable.\n             */\n            if ( d && shadow_mode_enabled(d)\n                 && (page->count_info & PGC_page_table)\n                 && !((page->shadow_flags & (1u<<29))\n                      && type == PGT_writable_page) )\n               shadow_remove_all_shadows(d, page_to_mfn(page));\n\n            ASSERT(!(x & PGT_pae_xen_l2));\n            if ( (x & PGT_type_mask) != type )\n            {\n                /*\n                 * On type change we check to flush stale TLB entries. It is\n                 * vital that no other CPUs are left with mappings of a frame\n                 * which is about to become writeable to the guest.\n                 */\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                BUG_ON(in_irq());\n                cpumask_copy(mask, d->dirty_cpumask);\n\n                /* Don't flush if the timestamp is old enough */\n                tlbflush_filter(mask, page->tlbflush_timestamp);\n\n                if ( unlikely(!cpumask_empty(mask)) &&\n                     /* Shadow mode: track only writable pages. */\n                     (!shadow_mode_enabled(page_get_owner(page)) ||\n                      ((nx & PGT_type_mask) == PGT_writable_page)) )\n                {\n                    perfc_incr(need_flush_tlb_flush);\n                    flush_tlb_mask(mask);\n                }\n\n                /* We lose existing type and validity. */\n                nx &= ~(PGT_type_mask | PGT_validated);\n                nx |= type;\n\n                /*\n                 * No special validation needed for writable pages.\n                 * Page tables and GDT/LDT need to be scanned for validity.\n                 */\n                if ( type == PGT_writable_page || type == PGT_shared_page )\n                    nx |= PGT_validated;\n            }\n        }\n        else if ( unlikely((x & (PGT_type_mask|PGT_pae_xen_l2)) != type) )\n        {\n            /* Don't log failure if it could be a recursive-mapping attempt. */\n            if ( ((x & PGT_type_mask) == PGT_l2_page_table) &&\n                 (type == PGT_l1_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l3_page_table) &&\n                 (type == PGT_l2_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l4_page_table) &&\n                 (type == PGT_l3_page_table) )\n                return -EINVAL;\n            gdprintk(XENLOG_WARNING,\n                     \"Bad type (saw %\" PRtype_info \" != exp %\" PRtype_info \") \"\n                     \"for mfn %\" PRI_mfn \" (pfn %\" PRI_pfn \")\\n\",\n                     x, type, mfn_x(page_to_mfn(page)),\n                     get_gpfn_from_mfn(mfn_x(page_to_mfn(page))));\n            return -EINVAL;\n        }\n        else if ( unlikely(!(x & PGT_validated)) )\n        {\n            if ( !(x & PGT_partial) )\n            {\n                /* Someone else is updating validation of this page. Wait... */\n                do {\n                    if ( preemptible && hypercall_preempt_check() )\n                        return -EINTR;\n                    cpu_relax();\n                } while ( (y = page->u.inuse.type_info) == x );\n                continue;\n            }\n            /* Type ref count was left at 1 when PGT_partial got set. */\n            ASSERT((x & PGT_count_mask) == 1);\n            nx = x & ~PGT_partial;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( unlikely((x & PGT_type_mask) != type) )\n    {\n        /* Special pages should not be accessible from devices. */\n        struct domain *d = page_get_owner(page);\n        if ( d && is_pv_domain(d) && unlikely(need_iommu_pt_sync(d)) )\n        {\n            mfn_t mfn = page_to_mfn(page);\n\n            if ( (x & PGT_type_mask) == PGT_writable_page )\n                iommu_ret = iommu_unmap_page(d, _dfn(mfn_x(mfn)));\n            else if ( type == PGT_writable_page )\n                iommu_ret = iommu_map_page(d, _dfn(mfn_x(mfn)), mfn,\n                                           IOMMUF_readable |\n                                           IOMMUF_writable);\n        }\n    }\n\n    if ( unlikely(!(nx & PGT_validated)) )\n    {\n        if ( !(x & PGT_partial) )\n        {\n            page->nr_validated_ptes = 0;\n            page->partial_pte = 0;\n        }\n        page->linear_pt_count = 0;\n        rc = alloc_page_type(page, type, preemptible);\n    }\n\n    if ( (x & PGT_partial) && !(nx & PGT_partial) )\n        put_page(page);\n\n    if ( !rc )\n        rc = iommu_ret;\n\n    return rc;\n}",
        "func": "static int _get_page_type(struct page_info *page, unsigned long type,\n                          bool preemptible)\n{\n    unsigned long nx, x, y = page->u.inuse.type_info;\n    int rc = 0, iommu_ret = 0;\n\n    ASSERT(!(type & ~(PGT_type_mask | PGT_pae_xen_l2)));\n    ASSERT(!in_irq());\n\n    for ( ; ; )\n    {\n        x  = y;\n        nx = x + 1;\n        if ( unlikely((nx & PGT_count_mask) == 0) )\n        {\n            gdprintk(XENLOG_WARNING,\n                     \"Type count overflow on mfn %\"PRI_mfn\"\\n\",\n                     mfn_x(page_to_mfn(page)));\n            return -EINVAL;\n        }\n        else if ( unlikely((x & PGT_count_mask) == 0) )\n        {\n            struct domain *d = page_get_owner(page);\n\n            if ( d && shadow_mode_enabled(d) )\n               shadow_prepare_page_type_change(d, page, type);\n\n            ASSERT(!(x & PGT_pae_xen_l2));\n            if ( (x & PGT_type_mask) != type )\n            {\n                /*\n                 * On type change we check to flush stale TLB entries. It is\n                 * vital that no other CPUs are left with mappings of a frame\n                 * which is about to become writeable to the guest.\n                 */\n                cpumask_t *mask = this_cpu(scratch_cpumask);\n\n                BUG_ON(in_irq());\n                cpumask_copy(mask, d->dirty_cpumask);\n\n                /* Don't flush if the timestamp is old enough */\n                tlbflush_filter(mask, page->tlbflush_timestamp);\n\n                if ( unlikely(!cpumask_empty(mask)) &&\n                     /* Shadow mode: track only writable pages. */\n                     (!shadow_mode_enabled(page_get_owner(page)) ||\n                      ((nx & PGT_type_mask) == PGT_writable_page)) )\n                {\n                    perfc_incr(need_flush_tlb_flush);\n                    flush_tlb_mask(mask);\n                }\n\n                /* We lose existing type and validity. */\n                nx &= ~(PGT_type_mask | PGT_validated);\n                nx |= type;\n\n                /*\n                 * No special validation needed for writable pages.\n                 * Page tables and GDT/LDT need to be scanned for validity.\n                 */\n                if ( type == PGT_writable_page || type == PGT_shared_page )\n                    nx |= PGT_validated;\n            }\n        }\n        else if ( unlikely((x & (PGT_type_mask|PGT_pae_xen_l2)) != type) )\n        {\n            /* Don't log failure if it could be a recursive-mapping attempt. */\n            if ( ((x & PGT_type_mask) == PGT_l2_page_table) &&\n                 (type == PGT_l1_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l3_page_table) &&\n                 (type == PGT_l2_page_table) )\n                return -EINVAL;\n            if ( ((x & PGT_type_mask) == PGT_l4_page_table) &&\n                 (type == PGT_l3_page_table) )\n                return -EINVAL;\n            gdprintk(XENLOG_WARNING,\n                     \"Bad type (saw %\" PRtype_info \" != exp %\" PRtype_info \") \"\n                     \"for mfn %\" PRI_mfn \" (pfn %\" PRI_pfn \")\\n\",\n                     x, type, mfn_x(page_to_mfn(page)),\n                     get_gpfn_from_mfn(mfn_x(page_to_mfn(page))));\n            return -EINVAL;\n        }\n        else if ( unlikely(!(x & PGT_validated)) )\n        {\n            if ( !(x & PGT_partial) )\n            {\n                /* Someone else is updating validation of this page. Wait... */\n                do {\n                    if ( preemptible && hypercall_preempt_check() )\n                        return -EINTR;\n                    cpu_relax();\n                } while ( (y = page->u.inuse.type_info) == x );\n                continue;\n            }\n            /* Type ref count was left at 1 when PGT_partial got set. */\n            ASSERT((x & PGT_count_mask) == 1);\n            nx = x & ~PGT_partial;\n        }\n\n        if ( likely((y = cmpxchg(&page->u.inuse.type_info, x, nx)) == x) )\n            break;\n\n        if ( preemptible && hypercall_preempt_check() )\n            return -EINTR;\n    }\n\n    if ( unlikely((x & PGT_type_mask) != type) )\n    {\n        /* Special pages should not be accessible from devices. */\n        struct domain *d = page_get_owner(page);\n        if ( d && is_pv_domain(d) && unlikely(need_iommu_pt_sync(d)) )\n        {\n            mfn_t mfn = page_to_mfn(page);\n\n            if ( (x & PGT_type_mask) == PGT_writable_page )\n                iommu_ret = iommu_unmap_page(d, _dfn(mfn_x(mfn)));\n            else if ( type == PGT_writable_page )\n                iommu_ret = iommu_map_page(d, _dfn(mfn_x(mfn)), mfn,\n                                           IOMMUF_readable |\n                                           IOMMUF_writable);\n        }\n    }\n\n    if ( unlikely(!(nx & PGT_validated)) )\n    {\n        if ( !(x & PGT_partial) )\n        {\n            page->nr_validated_ptes = 0;\n            page->partial_pte = 0;\n        }\n        page->linear_pt_count = 0;\n        rc = alloc_page_type(page, type, preemptible);\n    }\n\n    if ( (x & PGT_partial) && !(nx & PGT_partial) )\n        put_page(page);\n\n    if ( !rc )\n        rc = iommu_ret;\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,17 +22,8 @@\n         {\n             struct domain *d = page_get_owner(page);\n \n-            /*\n-             * Normally we should never let a page go from type count 0\n-             * to type count 1 when it is shadowed. One exception:\n-             * out-of-sync shadowed pages are allowed to become\n-             * writeable.\n-             */\n-            if ( d && shadow_mode_enabled(d)\n-                 && (page->count_info & PGC_page_table)\n-                 && !((page->shadow_flags & (1u<<29))\n-                      && type == PGT_writable_page) )\n-               shadow_remove_all_shadows(d, page_to_mfn(page));\n+            if ( d && shadow_mode_enabled(d) )\n+               shadow_prepare_page_type_change(d, page, type);\n \n             ASSERT(!(x & PGT_pae_xen_l2));\n             if ( (x & PGT_type_mask) != type )",
        "diff_line_info": {
            "deleted_lines": [
                "            /*",
                "             * Normally we should never let a page go from type count 0",
                "             * to type count 1 when it is shadowed. One exception:",
                "             * out-of-sync shadowed pages are allowed to become",
                "             * writeable.",
                "             */",
                "            if ( d && shadow_mode_enabled(d)",
                "                 && (page->count_info & PGC_page_table)",
                "                 && !((page->shadow_flags & (1u<<29))",
                "                      && type == PGT_writable_page) )",
                "               shadow_remove_all_shadows(d, page_to_mfn(page));"
            ],
            "added_lines": [
                "            if ( d && shadow_mode_enabled(d) )",
                "               shadow_prepare_page_type_change(d, page, type);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/shadow_demote",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "void shadow_demote(struct domain *d, mfn_t gmfn, u32 type)\n{\n    struct page_info *page = mfn_to_page(gmfn);\n\n    ASSERT(test_bit(_PGC_page_table, &page->count_info));\n    ASSERT(test_bit(type, &page->shadow_flags));\n\n    clear_bit(type, &page->shadow_flags);\n\n    if ( (page->shadow_flags & SHF_page_type_mask) == 0 )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* Was the page out of sync? */\n        if ( page_is_out_of_sync(page) )\n        {\n            oos_hash_remove(d, gmfn);\n        }\n#endif\n        clear_bit(_PGC_page_table, &page->count_info);\n    }\n\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_DEMOTE);\n}",
        "func": "void shadow_demote(struct domain *d, mfn_t gmfn, u32 type)\n{\n    struct page_info *page = mfn_to_page(gmfn);\n\n    ASSERT(test_bit(_PGC_page_table, &page->count_info));\n    ASSERT(page->shadow_flags & (1u << type));\n\n    page->shadow_flags &= ~(1u << type);\n\n    if ( (page->shadow_flags & SHF_page_type_mask) == 0 )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* Was the page out of sync? */\n        if ( page_is_out_of_sync(page) )\n        {\n            oos_hash_remove(d, gmfn);\n        }\n#endif\n        clear_bit(_PGC_page_table, &page->count_info);\n    }\n\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_DEMOTE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,9 +3,9 @@\n     struct page_info *page = mfn_to_page(gmfn);\n \n     ASSERT(test_bit(_PGC_page_table, &page->count_info));\n-    ASSERT(test_bit(type, &page->shadow_flags));\n+    ASSERT(page->shadow_flags & (1u << type));\n \n-    clear_bit(type, &page->shadow_flags);\n+    page->shadow_flags &= ~(1u << type);\n \n     if ( (page->shadow_flags & SHF_page_type_mask) == 0 )\n     {",
        "diff_line_info": {
            "deleted_lines": [
                "    ASSERT(test_bit(type, &page->shadow_flags));",
                "    clear_bit(type, &page->shadow_flags);"
            ],
            "added_lines": [
                "    ASSERT(page->shadow_flags & (1u << type));",
                "    page->shadow_flags &= ~(1u << type);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/sh_remove_shadows",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "void sh_remove_shadows(struct domain *d, mfn_t gmfn, int fast, int all)\n/* Remove the shadows of this guest page.\n * If fast != 0, just try the quick heuristic, which will remove\n * at most one reference to each shadow of the page.  Otherwise, walk\n * all the shadow tables looking for refs to shadows of this gmfn.\n * If all != 0, kill the domain if we can't find all the shadows.\n * (all != 0 implies fast == 0)\n */\n{\n    struct page_info *pg = mfn_to_page(gmfn);\n    mfn_t smfn;\n    unsigned char t;\n\n    /* Dispatch table for getting per-type functions: each level must\n     * be called with the function to remove a lower-level shadow. */\n    static const hash_domain_callback_t callbacks[SH_type_unused] = {\n        NULL, /* none    */\n        NULL, /* l1_32   */\n        NULL, /* fl1_32  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 2), /* l2_32   */\n        NULL, /* l1_pae  */\n        NULL, /* fl1_pae */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 3), /* l2_pae  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 3), /* l2h_pae */\n        NULL, /* l1_64   */\n        NULL, /* fl1_64  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 4), /* l2_64   */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 4), /* l2h_64  */\n        SHADOW_INTERNAL_NAME(sh_remove_l2_shadow, 4), /* l3_64   */\n        SHADOW_INTERNAL_NAME(sh_remove_l3_shadow, 4), /* l4_64   */\n        NULL, /* p2m     */\n        NULL  /* unused  */\n    };\n\n    /* Another lookup table, for choosing which mask to use */\n    static const unsigned int masks[SH_type_unused] = {\n        0, /* none    */\n        SHF_L2_32, /* l1_32   */\n        0, /* fl1_32  */\n        0, /* l2_32   */\n        SHF_L2H_PAE | SHF_L2_PAE, /* l1_pae  */\n        0, /* fl1_pae */\n        0, /* l2_pae  */\n        0, /* l2h_pae  */\n        SHF_L2H_64 | SHF_L2_64, /* l1_64   */\n        0, /* fl1_64  */\n        SHF_L3_64, /* l2_64   */\n        SHF_L3_64, /* l2h_64  */\n        SHF_L4_64, /* l3_64   */\n        0, /* l4_64   */\n        0, /* p2m     */\n        0  /* unused  */\n    };\n\n    ASSERT(!(all && fast));\n    ASSERT(mfn_valid(gmfn));\n\n    /* Although this is an externally visible function, we do not know\n     * whether the paging lock will be held when it is called (since it\n     * can be called via put_page_type when we clear a shadow l1e).*/\n    paging_lock_recursive(d);\n\n    SHADOW_PRINTK(\"d%d gmfn=%\"PRI_mfn\"\\n\", d->domain_id, mfn_x(gmfn));\n\n    /* Bail out now if the page is not shadowed */\n    if ( (pg->count_info & PGC_page_table) == 0 )\n    {\n        paging_unlock(d);\n        return;\n    }\n\n    /* Search for this shadow in all appropriate shadows */\n    perfc_incr(shadow_unshadow);\n\n    /* Lower-level shadows need to be excised from upper-level shadows.\n     * This call to hash_vcpu_foreach() looks dangerous but is in fact OK: each\n     * call will remove at most one shadow, and terminate immediately when\n     * it does remove it, so we never walk the hash after doing a deletion.  */\n#define DO_UNSHADOW(_type) do {                                         \\\n    t = (_type);                                                        \\\n    if( !(pg->count_info & PGC_page_table)                              \\\n        || !(pg->shadow_flags & (1 << t)) )                             \\\n        break;                                                          \\\n    smfn = shadow_hash_lookup(d, mfn_x(gmfn), t);                       \\\n    if ( unlikely(!mfn_valid(smfn)) )                                   \\\n    {                                                                   \\\n        printk(XENLOG_G_ERR \"gmfn %\"PRI_mfn\" has flags %#x\"             \\\n               \" but no type-%#x shadow\\n\",                             \\\n               mfn_x(gmfn), pg->shadow_flags, t);                       \\\n        break;                                                          \\\n    }                                                                   \\\n    if ( sh_type_is_pinnable(d, t) )                                    \\\n        sh_unpin(d, smfn);                                              \\\n    else if ( sh_type_has_up_pointer(d, t) )                            \\\n        sh_remove_shadow_via_pointer(d, smfn);                          \\\n    if( !fast                                                           \\\n        && (pg->count_info & PGC_page_table)                            \\\n        && (pg->shadow_flags & (1 << t)) )                              \\\n        hash_domain_foreach(d, masks[t], callbacks, smfn);              \\\n} while (0)\n\n    DO_UNSHADOW(SH_type_l2_32_shadow);\n    DO_UNSHADOW(SH_type_l1_32_shadow);\n    DO_UNSHADOW(SH_type_l2h_pae_shadow);\n    DO_UNSHADOW(SH_type_l2_pae_shadow);\n    DO_UNSHADOW(SH_type_l1_pae_shadow);\n    DO_UNSHADOW(SH_type_l4_64_shadow);\n    DO_UNSHADOW(SH_type_l3_64_shadow);\n    DO_UNSHADOW(SH_type_l2h_64_shadow);\n    DO_UNSHADOW(SH_type_l2_64_shadow);\n    DO_UNSHADOW(SH_type_l1_64_shadow);\n\n#undef DO_UNSHADOW\n\n    /* If that didn't catch the shadows, something is wrong */\n    if ( !fast && all && (pg->count_info & PGC_page_table) )\n    {\n        printk(XENLOG_G_ERR \"can't find all shadows of mfn %\"PRI_mfn\n               \" (shadow_flags=%08x)\\n\", mfn_x(gmfn), pg->shadow_flags);\n        domain_crash(d);\n    }\n\n    /* Need to flush TLBs now, so that linear maps are safe next time we\n     * take a fault. */\n    flush_tlb_mask(d->dirty_cpumask);\n\n    paging_unlock(d);\n}",
        "func": "void sh_remove_shadows(struct domain *d, mfn_t gmfn, int fast, int all)\n/* Remove the shadows of this guest page.\n * If fast != 0, just try the quick heuristic, which will remove\n * at most one reference to each shadow of the page.  Otherwise, walk\n * all the shadow tables looking for refs to shadows of this gmfn.\n * If all != 0, kill the domain if we can't find all the shadows.\n * (all != 0 implies fast == 0)\n */\n{\n    struct page_info *pg = mfn_to_page(gmfn);\n    mfn_t smfn;\n    unsigned char t;\n\n    /* Dispatch table for getting per-type functions: each level must\n     * be called with the function to remove a lower-level shadow. */\n    static const hash_domain_callback_t callbacks[SH_type_unused] = {\n        NULL, /* none    */\n        NULL, /* l1_32   */\n        NULL, /* fl1_32  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 2), /* l2_32   */\n        NULL, /* l1_pae  */\n        NULL, /* fl1_pae */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 3), /* l2_pae  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 3), /* l2h_pae */\n        NULL, /* l1_64   */\n        NULL, /* fl1_64  */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 4), /* l2_64   */\n        SHADOW_INTERNAL_NAME(sh_remove_l1_shadow, 4), /* l2h_64  */\n        SHADOW_INTERNAL_NAME(sh_remove_l2_shadow, 4), /* l3_64   */\n        SHADOW_INTERNAL_NAME(sh_remove_l3_shadow, 4), /* l4_64   */\n        NULL, /* p2m     */\n        NULL  /* unused  */\n    };\n\n    /* Another lookup table, for choosing which mask to use */\n    static const unsigned int masks[SH_type_unused] = {\n        0, /* none    */\n        SHF_L2_32, /* l1_32   */\n        0, /* fl1_32  */\n        0, /* l2_32   */\n        SHF_L2H_PAE | SHF_L2_PAE, /* l1_pae  */\n        0, /* fl1_pae */\n        0, /* l2_pae  */\n        0, /* l2h_pae  */\n        SHF_L2H_64 | SHF_L2_64, /* l1_64   */\n        0, /* fl1_64  */\n        SHF_L3_64, /* l2_64   */\n        SHF_L3_64, /* l2h_64  */\n        SHF_L4_64, /* l3_64   */\n        0, /* l4_64   */\n        0, /* p2m     */\n        0  /* unused  */\n    };\n\n    ASSERT(!(all && fast));\n    ASSERT(mfn_valid(gmfn));\n\n    /* Although this is an externally visible function, we do not know\n     * whether the paging lock will be held when it is called (since it\n     * can be called via put_page_type when we clear a shadow l1e).*/\n    paging_lock_recursive(d);\n\n    SHADOW_PRINTK(\"d%d gmfn=%\"PRI_mfn\"\\n\", d->domain_id, mfn_x(gmfn));\n\n    /* Bail out now if the page is not shadowed */\n    if ( (pg->count_info & PGC_page_table) == 0 )\n    {\n        paging_unlock(d);\n        return;\n    }\n\n    /* Search for this shadow in all appropriate shadows */\n    perfc_incr(shadow_unshadow);\n\n    /* Lower-level shadows need to be excised from upper-level shadows.\n     * This call to hash_vcpu_foreach() looks dangerous but is in fact OK: each\n     * call will remove at most one shadow, and terminate immediately when\n     * it does remove it, so we never walk the hash after doing a deletion.  */\n#define DO_UNSHADOW(_type) do {                                         \\\n    t = (_type);                                                        \\\n    if( !(pg->count_info & PGC_page_table)                              \\\n        || !(pg->shadow_flags & (1 << t)) )                             \\\n        break;                                                          \\\n    smfn = shadow_hash_lookup(d, mfn_x(gmfn), t);                       \\\n    if ( unlikely(!mfn_valid(smfn)) )                                   \\\n    {                                                                   \\\n        printk(XENLOG_G_ERR \"gmfn %\"PRI_mfn\" has flags %#x\"             \\\n               \" but no type-%#x shadow\\n\",                             \\\n               mfn_x(gmfn), pg->shadow_flags, t);                       \\\n        break;                                                          \\\n    }                                                                   \\\n    if ( sh_type_is_pinnable(d, t) )                                    \\\n        sh_unpin(d, smfn);                                              \\\n    else if ( sh_type_has_up_pointer(d, t) )                            \\\n        sh_remove_shadow_via_pointer(d, smfn);                          \\\n    if( !fast                                                           \\\n        && (pg->count_info & PGC_page_table)                            \\\n        && (pg->shadow_flags & (1 << t)) )                              \\\n        hash_domain_foreach(d, masks[t], callbacks, smfn);              \\\n} while (0)\n\n    DO_UNSHADOW(SH_type_l2_32_shadow);\n    DO_UNSHADOW(SH_type_l1_32_shadow);\n    DO_UNSHADOW(SH_type_l2h_pae_shadow);\n    DO_UNSHADOW(SH_type_l2_pae_shadow);\n    DO_UNSHADOW(SH_type_l1_pae_shadow);\n    DO_UNSHADOW(SH_type_l4_64_shadow);\n    DO_UNSHADOW(SH_type_l3_64_shadow);\n    DO_UNSHADOW(SH_type_l2h_64_shadow);\n    DO_UNSHADOW(SH_type_l2_64_shadow);\n    DO_UNSHADOW(SH_type_l1_64_shadow);\n\n#undef DO_UNSHADOW\n\n    /* If that didn't catch the shadows, something is wrong */\n    if ( !fast && all && (pg->count_info & PGC_page_table) )\n    {\n        printk(XENLOG_G_ERR \"can't find all shadows of mfn %\"PRI_mfn\n               \" (shadow_flags=%04x)\\n\", mfn_x(gmfn), pg->shadow_flags);\n        domain_crash(d);\n    }\n\n    /* Need to flush TLBs now, so that linear maps are safe next time we\n     * take a fault. */\n    flush_tlb_mask(d->dirty_cpumask);\n\n    paging_unlock(d);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -116,7 +116,7 @@\n     if ( !fast && all && (pg->count_info & PGC_page_table) )\n     {\n         printk(XENLOG_G_ERR \"can't find all shadows of mfn %\"PRI_mfn\n-               \" (shadow_flags=%08x)\\n\", mfn_x(gmfn), pg->shadow_flags);\n+               \" (shadow_flags=%04x)\\n\", mfn_x(gmfn), pg->shadow_flags);\n         domain_crash(d);\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [
                "               \" (shadow_flags=%08x)\\n\", mfn_x(gmfn), pg->shadow_flags);"
            ],
            "added_lines": [
                "               \" (shadow_flags=%04x)\\n\", mfn_x(gmfn), pg->shadow_flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/shadow_promote",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "void shadow_promote(struct domain *d, mfn_t gmfn, unsigned int type)\n{\n    struct page_info *page = mfn_to_page(gmfn);\n\n    ASSERT(mfn_valid(gmfn));\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Is the page already shadowed and out of sync? */\n    if ( page_is_out_of_sync(page) )\n        sh_resync(d, gmfn);\n#endif\n\n    /* We should never try to promote a gmfn that has writeable mappings */\n    ASSERT((page->u.inuse.type_info & PGT_type_mask) != PGT_writable_page\n           || (page->u.inuse.type_info & PGT_count_mask) == 0\n           || d->is_shutting_down);\n\n    /* Is the page already shadowed? */\n    if ( !test_and_set_bit(_PGC_page_table, &page->count_info) )\n        page->shadow_flags = 0;\n\n    ASSERT(!test_bit(type, &page->shadow_flags));\n    set_bit(type, &page->shadow_flags);\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_PROMOTE);\n}",
        "func": "void shadow_promote(struct domain *d, mfn_t gmfn, unsigned int type)\n{\n    struct page_info *page = mfn_to_page(gmfn);\n\n    ASSERT(mfn_valid(gmfn));\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Is the page already shadowed and out of sync? */\n    if ( page_is_out_of_sync(page) )\n        sh_resync(d, gmfn);\n#endif\n\n    /* We should never try to promote a gmfn that has writeable mappings */\n    ASSERT((page->u.inuse.type_info & PGT_type_mask) != PGT_writable_page\n           || (page->u.inuse.type_info & PGT_count_mask) == 0\n           || d->is_shutting_down);\n\n    /* Is the page already shadowed? */\n    if ( !test_and_set_bit(_PGC_page_table, &page->count_info) )\n    {\n        page->shadow_flags = 0;\n        if ( is_hvm_domain(d) )\n            page->pagetable_dying = false;\n    }\n\n    ASSERT(!(page->shadow_flags & (1u << type)));\n    page->shadow_flags |= 1u << type;\n    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_PROMOTE);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -17,9 +17,13 @@\n \n     /* Is the page already shadowed? */\n     if ( !test_and_set_bit(_PGC_page_table, &page->count_info) )\n+    {\n         page->shadow_flags = 0;\n+        if ( is_hvm_domain(d) )\n+            page->pagetable_dying = false;\n+    }\n \n-    ASSERT(!test_bit(type, &page->shadow_flags));\n-    set_bit(type, &page->shadow_flags);\n+    ASSERT(!(page->shadow_flags & (1u << type)));\n+    page->shadow_flags |= 1u << type;\n     TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_PROMOTE);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    ASSERT(!test_bit(type, &page->shadow_flags));",
                "    set_bit(type, &page->shadow_flags);"
            ],
            "added_lines": [
                "    {",
                "        if ( is_hvm_domain(d) )",
                "            page->pagetable_dying = false;",
                "    }",
                "    ASSERT(!(page->shadow_flags & (1u << type)));",
                "    page->shadow_flags |= 1u << type;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/sh_pagetable_dying",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "static void sh_pagetable_dying(paddr_t gpa)\n{\n    struct vcpu *v = current;\n    struct domain *d = v->domain;\n    int i = 0;\n    int flush = 0;\n    int fast_path = 0;\n    paddr_t gcr3 = 0;\n    p2m_type_t p2mt;\n    char *gl3pa = NULL;\n    guest_l3e_t *gl3e = NULL;\n    unsigned long l3gfn;\n    mfn_t l3mfn;\n\n    gcr3 = v->arch.hvm.guest_cr[3];\n    /* fast path: the pagetable belongs to the current context */\n    if ( gcr3 == gpa )\n        fast_path = 1;\n\n    l3gfn = gpa >> PAGE_SHIFT;\n    l3mfn = get_gfn_query(d, _gfn(l3gfn), &p2mt);\n    if ( !mfn_valid(l3mfn) || !p2m_is_ram(p2mt) )\n    {\n        printk(XENLOG_DEBUG \"sh_pagetable_dying: gpa not valid %\"PRIpaddr\"\\n\",\n               gpa);\n        goto out_put_gfn;\n    }\n\n    paging_lock(d);\n\n    if ( !fast_path )\n    {\n        gl3pa = map_domain_page(l3mfn);\n        gl3e = (guest_l3e_t *)(gl3pa + ((unsigned long)gpa & ~PAGE_MASK));\n    }\n    for ( i = 0; i < 4; i++ )\n    {\n        mfn_t smfn, gmfn;\n\n        if ( fast_path ) {\n            if ( pagetable_is_null(v->arch.shadow_table[i]) )\n                smfn = INVALID_MFN;\n            else\n                smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n        }\n        else\n        {\n            /* retrieving the l2s */\n            gmfn = get_gfn_query_unlocked(d, gfn_x(guest_l3e_get_gfn(gl3e[i])),\n                                          &p2mt);\n            smfn = unlikely(mfn_eq(gmfn, INVALID_MFN))\n                   ? INVALID_MFN\n                   : shadow_hash_lookup(d, mfn_x(gmfn), SH_type_l2_pae_shadow);\n        }\n\n        if ( mfn_valid(smfn) )\n        {\n            gmfn = _mfn(mfn_to_page(smfn)->v.sh.back);\n            mfn_to_page(gmfn)->shadow_flags |= SHF_pagetable_dying;\n            shadow_unhook_mappings(d, smfn, 1/* user pages only */);\n            flush = 1;\n        }\n    }\n    if ( flush )\n        flush_tlb_mask(d->dirty_cpumask);\n\n    /* Remember that we've seen the guest use this interface, so we\n     * can rely on it using it in future, instead of guessing at\n     * when processes are being torn down. */\n    d->arch.paging.shadow.pagetable_dying_op = 1;\n\n    v->arch.paging.shadow.pagetable_dying = 1;\n\n    if ( !fast_path )\n        unmap_domain_page(gl3pa);\n    paging_unlock(d);\nout_put_gfn:\n    put_gfn(d, l3gfn);\n}",
        "func": "static void sh_pagetable_dying(paddr_t gpa)\n{\n    struct vcpu *v = current;\n    struct domain *d = v->domain;\n    int i = 0;\n    int flush = 0;\n    int fast_path = 0;\n    paddr_t gcr3 = 0;\n    p2m_type_t p2mt;\n    char *gl3pa = NULL;\n    guest_l3e_t *gl3e = NULL;\n    unsigned long l3gfn;\n    mfn_t l3mfn;\n\n    gcr3 = v->arch.hvm.guest_cr[3];\n    /* fast path: the pagetable belongs to the current context */\n    if ( gcr3 == gpa )\n        fast_path = 1;\n\n    l3gfn = gpa >> PAGE_SHIFT;\n    l3mfn = get_gfn_query(d, _gfn(l3gfn), &p2mt);\n    if ( !mfn_valid(l3mfn) || !p2m_is_ram(p2mt) )\n    {\n        printk(XENLOG_DEBUG \"sh_pagetable_dying: gpa not valid %\"PRIpaddr\"\\n\",\n               gpa);\n        goto out_put_gfn;\n    }\n\n    paging_lock(d);\n\n    if ( !fast_path )\n    {\n        gl3pa = map_domain_page(l3mfn);\n        gl3e = (guest_l3e_t *)(gl3pa + ((unsigned long)gpa & ~PAGE_MASK));\n    }\n    for ( i = 0; i < 4; i++ )\n    {\n        mfn_t smfn, gmfn;\n\n        if ( fast_path ) {\n            if ( pagetable_is_null(v->arch.shadow_table[i]) )\n                smfn = INVALID_MFN;\n            else\n                smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n        }\n        else\n        {\n            /* retrieving the l2s */\n            gmfn = get_gfn_query_unlocked(d, gfn_x(guest_l3e_get_gfn(gl3e[i])),\n                                          &p2mt);\n            smfn = unlikely(mfn_eq(gmfn, INVALID_MFN))\n                   ? INVALID_MFN\n                   : shadow_hash_lookup(d, mfn_x(gmfn), SH_type_l2_pae_shadow);\n        }\n\n        if ( mfn_valid(smfn) && is_hvm_domain(d) )\n        {\n            gmfn = _mfn(mfn_to_page(smfn)->v.sh.back);\n            mfn_to_page(gmfn)->pagetable_dying = true;\n            shadow_unhook_mappings(d, smfn, 1/* user pages only */);\n            flush = 1;\n        }\n    }\n    if ( flush )\n        flush_tlb_mask(d->dirty_cpumask);\n\n    /* Remember that we've seen the guest use this interface, so we\n     * can rely on it using it in future, instead of guessing at\n     * when processes are being torn down. */\n    d->arch.paging.shadow.pagetable_dying_op = 1;\n\n    v->arch.paging.shadow.pagetable_dying = 1;\n\n    if ( !fast_path )\n        unmap_domain_page(gl3pa);\n    paging_unlock(d);\nout_put_gfn:\n    put_gfn(d, l3gfn);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -53,10 +53,10 @@\n                    : shadow_hash_lookup(d, mfn_x(gmfn), SH_type_l2_pae_shadow);\n         }\n \n-        if ( mfn_valid(smfn) )\n+        if ( mfn_valid(smfn) && is_hvm_domain(d) )\n         {\n             gmfn = _mfn(mfn_to_page(smfn)->v.sh.back);\n-            mfn_to_page(gmfn)->shadow_flags |= SHF_pagetable_dying;\n+            mfn_to_page(gmfn)->pagetable_dying = true;\n             shadow_unhook_mappings(d, smfn, 1/* user pages only */);\n             flush = 1;\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( mfn_valid(smfn) )",
                "            mfn_to_page(gmfn)->shadow_flags |= SHF_pagetable_dying;"
            ],
            "added_lines": [
                "        if ( mfn_valid(smfn) && is_hvm_domain(d) )",
                "            mfn_to_page(gmfn)->pagetable_dying = true;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/sh_rm_write_access_from_sl1p",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "int sh_rm_write_access_from_sl1p(struct domain *d, mfn_t gmfn,\n                                 mfn_t smfn, unsigned long off)\n{\n    struct vcpu *curr = current;\n    int r;\n    shadow_l1e_t *sl1p, sl1e;\n    struct page_info *sp;\n\n    ASSERT(mfn_valid(gmfn));\n    ASSERT(mfn_valid(smfn));\n\n    /* Remember if we've been told that this process is being torn down */\n    if ( curr->domain == d )\n        curr->arch.paging.shadow.pagetable_dying\n            = !!(mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying);\n\n    sp = mfn_to_page(smfn);\n\n    if ( ((sp->count_info & PGC_count_mask) != 0)\n         || (sp->u.sh.type != SH_type_l1_shadow\n             && sp->u.sh.type != SH_type_fl1_shadow) )\n        goto fail;\n\n    sl1p = map_domain_page(smfn);\n    sl1p += off;\n    sl1e = *sl1p;\n    if ( ((shadow_l1e_get_flags(sl1e) & (_PAGE_PRESENT|_PAGE_RW))\n          != (_PAGE_PRESENT|_PAGE_RW))\n         || !mfn_eq(shadow_l1e_get_mfn(sl1e), gmfn) )\n    {\n        unmap_domain_page(sl1p);\n        goto fail;\n    }\n\n    /* Found it!  Need to remove its write permissions. */\n    sl1e = shadow_l1e_remove_flags(sl1e, _PAGE_RW);\n    r = shadow_set_l1e(d, sl1p, sl1e, p2m_ram_rw, smfn);\n    ASSERT( !(r & SHADOW_SET_ERROR) );\n\n    unmap_domain_page(sl1p);\n    perfc_incr(shadow_writeable_h_7);\n    return 1;\n\n fail:\n    perfc_incr(shadow_writeable_h_8);\n    return 0;\n}",
        "func": "int sh_rm_write_access_from_sl1p(struct domain *d, mfn_t gmfn,\n                                 mfn_t smfn, unsigned long off)\n{\n    struct vcpu *curr = current;\n    int r;\n    shadow_l1e_t *sl1p, sl1e;\n    struct page_info *sp;\n\n    ASSERT(mfn_valid(gmfn));\n    ASSERT(mfn_valid(smfn));\n\n    /* Remember if we've been told that this process is being torn down */\n    if ( curr->domain == d && is_hvm_domain(d) )\n        curr->arch.paging.shadow.pagetable_dying\n            = mfn_to_page(gmfn)->pagetable_dying;\n\n    sp = mfn_to_page(smfn);\n\n    if ( ((sp->count_info & PGC_count_mask) != 0)\n         || (sp->u.sh.type != SH_type_l1_shadow\n             && sp->u.sh.type != SH_type_fl1_shadow) )\n        goto fail;\n\n    sl1p = map_domain_page(smfn);\n    sl1p += off;\n    sl1e = *sl1p;\n    if ( ((shadow_l1e_get_flags(sl1e) & (_PAGE_PRESENT|_PAGE_RW))\n          != (_PAGE_PRESENT|_PAGE_RW))\n         || !mfn_eq(shadow_l1e_get_mfn(sl1e), gmfn) )\n    {\n        unmap_domain_page(sl1p);\n        goto fail;\n    }\n\n    /* Found it!  Need to remove its write permissions. */\n    sl1e = shadow_l1e_remove_flags(sl1e, _PAGE_RW);\n    r = shadow_set_l1e(d, sl1p, sl1e, p2m_ram_rw, smfn);\n    ASSERT( !(r & SHADOW_SET_ERROR) );\n\n    unmap_domain_page(sl1p);\n    perfc_incr(shadow_writeable_h_7);\n    return 1;\n\n fail:\n    perfc_incr(shadow_writeable_h_8);\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,9 +10,9 @@\n     ASSERT(mfn_valid(smfn));\n \n     /* Remember if we've been told that this process is being torn down */\n-    if ( curr->domain == d )\n+    if ( curr->domain == d && is_hvm_domain(d) )\n         curr->arch.paging.shadow.pagetable_dying\n-            = !!(mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying);\n+            = mfn_to_page(gmfn)->pagetable_dying;\n \n     sp = mfn_to_page(smfn);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( curr->domain == d )",
                "            = !!(mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying);"
            ],
            "added_lines": [
                "    if ( curr->domain == d && is_hvm_domain(d) )",
                "            = mfn_to_page(gmfn)->pagetable_dying;"
            ]
        }
    },
    {
        "cve_id": "CVE-2018-19966",
        "func_name": "xen-project/xen/sh_page_fault",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service (host OS crash) or possibly gain host OS privileges because of an interpretation conflict for a union data structure associated with shadow paging. NOTE: this issue exists because of an incorrect fix for CVE-2017-15595.",
        "git_url": "https://github.com/xen-project/xen/commit/789589968ed90e82a832dbc60e958c76b787be7e",
        "commit_title": "x86/shadow: shrink struct page_info's shadow_flags to 16 bits",
        "commit_text": " This is to avoid it overlapping the linear_pt_count field needed for PV domains. Introduce a separate, HVM-only pagetable_dying field to replace the sole one left in the upper 16 bits.  Note that the accesses to ->shadow_flags in shadow_{pro,de}mote() get switched to non-atomic, non-bitops operations, as {test,set,clear}_bit() are not allowed on uint16_t fields and hence their use would have required ugly casts. This is fine because all updates of the field ought to occur with the paging lock held, and other updates of it use |= and &= as well (i.e. using atomic operations here didn't really guard against potentially racing updates elsewhere).  This is part of XSA-280. ",
        "func_before": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && (mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying) )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "func": "static int sh_page_fault(struct vcpu *v,\n                          unsigned long va,\n                          struct cpu_user_regs *regs)\n{\n    struct domain *d = v->domain;\n    walk_t gw;\n    gfn_t gfn = _gfn(0);\n    mfn_t gmfn, sl1mfn = _mfn(0);\n    shadow_l1e_t sl1e, *ptr_sl1e;\n    paddr_t gpa;\n    struct sh_emulate_ctxt emul_ctxt;\n    const struct x86_emulate_ops *emul_ops;\n    int r;\n    p2m_type_t p2mt;\n    uint32_t rc, error_code;\n    bool walk_ok;\n    int version;\n    unsigned int cpl;\n    const struct npfec access = {\n         .read_access = 1,\n         .write_access = !!(regs->error_code & PFEC_write_access),\n         .gla_valid = 1,\n         .kind = npfec_kind_with_gla\n    };\n    const fetch_type_t ft =\n        access.write_access ? ft_demand_write : ft_demand_read;\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    int fast_emul = 0;\n#endif\n\n    SHADOW_PRINTK(\"%pv va=%#lx err=%#x, rip=%lx\\n\",\n                  v, va, regs->error_code, regs->rip);\n\n    perfc_incr(shadow_fault);\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* If faulting frame is successfully emulated in last shadow fault\n     * it's highly likely to reach same emulation action for this frame.\n     * Then try to emulate early to avoid lock aquisition.\n     */\n    if ( v->arch.paging.last_write_emul_ok\n         && v->arch.paging.shadow.last_emulated_frame == (va >> PAGE_SHIFT) )\n    {\n        /* check whether error code is 3, or else fall back to normal path\n         * in case of some validation is required\n         */\n        if ( regs->error_code == (PFEC_write_access | PFEC_page_present) )\n        {\n            fast_emul = 1;\n            gmfn = _mfn(v->arch.paging.shadow.last_emulated_mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n            /* Fall back to the slow path if we're trying to emulate\n               writes to an out of sync page. */\n            if ( mfn_valid(gmfn) && mfn_is_out_of_sync(gmfn) )\n            {\n                fast_emul = 0;\n                v->arch.paging.last_write_emul_ok = 0;\n                goto page_fault_slow_path;\n            }\n#endif /* OOS */\n\n            perfc_incr(shadow_fault_fast_emulate);\n            goto early_emulation;\n        }\n        else\n            v->arch.paging.last_write_emul_ok = 0;\n    }\n#endif\n\n    //\n    // XXX: Need to think about eventually mapping superpages directly in the\n    //      shadow (when possible), as opposed to splintering them into a\n    //      bunch of 4K maps.\n    //\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_FAST_FAULT_PATH)\n    if ( (regs->error_code & PFEC_reserved_bit) )\n    {\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n        /* First, need to check that this isn't an out-of-sync\n         * shadow l1e.  If it is, we fall back to the slow path, which\n         * will sync it up again. */\n        {\n            shadow_l2e_t sl2e;\n            mfn_t gl1mfn;\n            if ( (__copy_from_user(&sl2e,\n                                   (sh_linear_l2_table(v)\n                                    + shadow_l2_linear_offset(va)),\n                                   sizeof(sl2e)) != 0)\n                 || !(shadow_l2e_get_flags(sl2e) & _PAGE_PRESENT)\n                 || !mfn_valid(gl1mfn = backpointer(mfn_to_page(\n                                  shadow_l2e_get_mfn(sl2e))))\n                 || unlikely(mfn_is_out_of_sync(gl1mfn)) )\n            {\n                /* Hit the slow path as if there had been no\n                 * shadow entry at all, and let it tidy up */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                goto page_fault_slow_path;\n            }\n        }\n#endif /* SHOPT_OUT_OF_SYNC */\n        /* The only reasons for reserved bits to be set in shadow entries\n         * are the two \"magic\" shadow_l1e entries. */\n        if ( likely((__copy_from_user(&sl1e,\n                                      (sh_linear_l1_table(v)\n                                       + shadow_l1_linear_offset(va)),\n                                      sizeof(sl1e)) == 0)\n                    && sh_l1e_is_magic(sl1e)) )\n        {\n\n            if ( sh_l1e_is_gnp(sl1e) )\n            {\n                /* Not-present in a guest PT: pass to the guest as\n                 * a not-present fault (by flipping two bits). */\n                ASSERT(regs->error_code & PFEC_page_present);\n                regs->error_code ^= (PFEC_reserved_bit|PFEC_page_present);\n                sh_reset_early_unshadow(v);\n                perfc_incr(shadow_fault_fast_gnp);\n                SHADOW_PRINTK(\"fast path not-present\\n\");\n                trace_shadow_gen(TRC_SHADOW_FAST_PROPAGATE, va);\n                return 0;\n            }\n#ifdef CONFIG_HVM\n            /* Magic MMIO marker: extract gfn for MMIO address */\n            ASSERT(sh_l1e_is_mmio(sl1e));\n            ASSERT(is_hvm_vcpu(v));\n            gpa = gfn_to_gaddr(sh_l1e_mmio_get_gfn(sl1e)) | (va & ~PAGE_MASK);\n            perfc_incr(shadow_fault_fast_mmio);\n            SHADOW_PRINTK(\"fast path mmio %#\"PRIpaddr\"\\n\", gpa);\n            sh_reset_early_unshadow(v);\n            trace_shadow_gen(TRC_SHADOW_FAST_MMIO, va);\n            return handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n                   ? EXCRET_fault_fixed : 0;\n#else\n            /* When HVM is not enabled, there shouldn't be MMIO marker */\n            BUG();\n#endif\n        }\n        else\n        {\n            /* This should be exceptionally rare: another vcpu has fixed\n             * the tables between the fault and our reading the l1e.\n             * Retry and let the hardware give us the right fault next time. */\n            perfc_incr(shadow_fault_fast_fail);\n            SHADOW_PRINTK(\"fast path false alarm!\\n\");\n            trace_shadow_gen(TRC_SHADOW_FALSE_FAST_PATH, va);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n page_fault_slow_path:\n#endif\n#endif /* SHOPT_FAST_FAULT_PATH */\n\n    /* Detect if this page fault happened while we were already in Xen\n     * doing a shadow operation.  If that happens, the only thing we can\n     * do is let Xen's normal fault handlers try to fix it.  In any case,\n     * a diagnostic trace of the fault will be more useful than\n     * a BUG() when we try to take the lock again. */\n    if ( unlikely(paging_locked_by_me(d)) )\n    {\n        printk(XENLOG_G_ERR \"Recursive shadow fault: lock taken by %s\\n\",\n               d->arch.paging.lock.locker_function);\n        return 0;\n    }\n\n    cpl = is_pv_vcpu(v) ? (regs->ss & 3) : hvm_get_cpl(v);\n\n rewalk:\n\n    error_code = regs->error_code;\n\n    /*\n     * When CR4.SMAP is enabled, instructions which have a side effect of\n     * accessing the system data structures (e.g. mov to %ds accessing the\n     * LDT/GDT, or int $n accessing the IDT) are known as implicit supervisor\n     * accesses.\n     *\n     * The distinction between implicit and explicit accesses form part of the\n     * determination of access rights, controlling whether the access is\n     * successful, or raises a #PF.\n     *\n     * Unfortunately, the processor throws away the implicit/explicit\n     * distinction and does not provide it to the pagefault handler\n     * (i.e. here.) in the #PF error code.  Therefore, we must try to\n     * reconstruct the lost state so it can be fed back into our pagewalk\n     * through the guest tables.\n     *\n     * User mode accesses are easy to reconstruct:\n     *\n     *   If we observe a cpl3 data fetch which was a supervisor walk, this\n     *   must have been an implicit access to a system table.\n     *\n     * Supervisor mode accesses are not easy:\n     *\n     *   In principle, we could decode the instruction under %rip and have the\n     *   instruction emulator tell us if there is an implicit access.\n     *   However, this is racy with other vcpus updating the pagetable or\n     *   rewriting the instruction stream under our feet.\n     *\n     *   Therefore, we do nothing.  (If anyone has a sensible suggestion for\n     *   how to distinguish these cases, xen-devel@ is all ears...)\n     *\n     * As a result, one specific corner case will fail.  If a guest OS with\n     * SMAP enabled ends up mapping a system table with user mappings, sets\n     * EFLAGS.AC to allow explicit accesses to user mappings, and implicitly\n     * accesses the user mapping, hardware and the shadow code will disagree\n     * on whether a #PF should be raised.\n     *\n     * Hardware raises #PF because implicit supervisor accesses to user\n     * mappings are strictly disallowed.  As we can't reconstruct the correct\n     * input, the pagewalk is performed as if it were an explicit access,\n     * which concludes that the access should have succeeded and the shadow\n     * pagetables need modifying.  The shadow pagetables are modified (to the\n     * same value), and we re-enter the guest to re-execute the instruction,\n     * which causes another #PF, and the vcpu livelocks, unable to make\n     * forward progress.\n     *\n     * In practice, this is tolerable.  No production OS will deliberately\n     * construct this corner case (as doing so would mean that a system table\n     * is directly accessable to userspace, and the OS is trivially rootable.)\n     * If this corner case comes about accidentally, then a security-relevant\n     * bug has been tickled.\n     */\n    if ( !(error_code & (PFEC_insn_fetch|PFEC_user_mode)) && cpl == 3 )\n        error_code |= PFEC_implicit;\n\n    /* The walk is done in a lock-free style, with some sanity check\n     * postponed after grabbing paging lock later. Those delayed checks\n     * will make sure no inconsistent mapping being translated into\n     * shadow page table. */\n    version = atomic_read(&d->arch.paging.shadow.gtable_dirty_version);\n    smp_rmb();\n    walk_ok = sh_walk_guest_tables(v, va, &gw, error_code);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    regs->error_code &= ~PFEC_page_present;\n    if ( gw.pfec & PFEC_page_present )\n        regs->error_code |= PFEC_page_present;\n#endif\n\n    if ( !walk_ok )\n    {\n        perfc_incr(shadow_fault_bail_real_fault);\n        SHADOW_PRINTK(\"not a shadow fault\\n\");\n        sh_reset_early_unshadow(v);\n        regs->error_code = gw.pfec & PFEC_arch_mask;\n        goto propagate;\n    }\n\n    /* It's possible that the guest has put pagetables in memory that it has\n     * already used for some special purpose (ioreq pages, or granted pages).\n     * If that happens we'll have killed the guest already but it's still not\n     * safe to propagate entries out of the guest PT so get out now. */\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        SHADOW_PRINTK(\"guest is shutting down\\n\");\n        goto propagate;\n    }\n\n    /* What mfn is the guest trying to access? */\n    gfn = guest_walk_to_gfn(&gw);\n    gmfn = get_gfn(d, gfn, &p2mt);\n\n    if ( shadow_mode_refcounts(d) &&\n         ((!p2m_is_valid(p2mt) && !p2m_is_grant(p2mt)) ||\n          (!p2m_is_mmio(p2mt) && !mfn_valid(gmfn))) )\n    {\n        perfc_incr(shadow_fault_bail_bad_gfn);\n        SHADOW_PRINTK(\"BAD gfn=%\"SH_PRI_gfn\" gmfn=%\"PRI_mfn\"\\n\",\n                      gfn_x(gfn), mfn_x(gmfn));\n        sh_reset_early_unshadow(v);\n        put_gfn(d, gfn_x(gfn));\n        goto propagate;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB)\n    /* Remember this successful VA->GFN translation for later. */\n    vtlb_insert(v, va >> PAGE_SHIFT, gfn_x(gfn),\n                regs->error_code | PFEC_page_present);\n#endif /* (SHADOW_OPTIMIZATIONS & SHOPT_VIRTUAL_TLB) */\n\n    paging_lock(d);\n\n    TRACE_CLEAR_PATH_FLAGS;\n\n    /* Make sure there is enough free shadow memory to build a chain of\n     * shadow tables. (We never allocate a top-level shadow on this path,\n     * only a 32b l1, pae l1, or 64b l3+2+1. Note that while\n     * SH_type_l1_shadow isn't correct in the latter case, all page\n     * tables are the same size there.)\n     *\n     * Preallocate shadow pages *before* removing writable accesses\n     * otherwhise an OOS L1 might be demoted and promoted again with\n     * writable mappings. */\n    shadow_prealloc(d,\n                    SH_type_l1_shadow,\n                    GUEST_PAGING_LEVELS < 4 ? 1 : GUEST_PAGING_LEVELS - 1);\n\n    rc = gw_remove_write_accesses(v, va, &gw);\n\n    /* First bit set: Removed write access to a page. */\n    if ( rc & GW_RMWR_FLUSHTLB )\n    {\n        /* Write permission removal is also a hint that other gwalks\n         * overlapping with this one may be inconsistent\n         */\n        perfc_incr(shadow_rm_write_flush_tlb);\n        smp_wmb();\n        atomic_inc(&d->arch.paging.shadow.gtable_dirty_version);\n        flush_tlb_mask(d->dirty_cpumask);\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Second bit set: Resynced a page. Re-walk needed. */\n    if ( rc & GW_RMWR_REWALK )\n    {\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    if ( !shadow_check_gwalk(v, va, &gw, version) )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n\n    shadow_audit_tables(v);\n    sh_audit_gw(v, &gw);\n\n    /* Acquire the shadow.  This must happen before we figure out the rights\n     * for the shadow entry, since we might promote a page here. */\n    ptr_sl1e = shadow_get_and_create_l1e(v, &gw, &sl1mfn, ft);\n    if ( unlikely(ptr_sl1e == NULL) )\n    {\n        /* Couldn't get the sl1e!  Since we know the guest entries\n         * are OK, this can only have been caused by a failed\n         * shadow_set_l*e(), which will have crashed the guest.\n         * Get out of the fault handler immediately. */\n        /* Windows 7 apparently relies on the hardware to do something\n         * it explicitly hasn't promised to do: load l3 values after\n         * the cr3 is loaded.\n         * In any case, in the PAE case, the ASSERT is not true; it can\n         * happen because of actions the guest is taking. */\n#if GUEST_PAGING_LEVELS == 3\n        v->arch.paging.mode->update_cr3(v, 0, false);\n#else\n        ASSERT(d->is_shutting_down);\n#endif\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        trace_shadow_gen(TRC_SHADOW_DOMF_DYING, va);\n        return 0;\n    }\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    /* Always unsync when writing to L1 page tables. */\n    if ( sh_mfn_is_a_page_table(gmfn)\n         && ft == ft_demand_write )\n        sh_unsync(v, gmfn);\n\n    if ( unlikely(d->is_shutting_down && d->shutdown_code == SHUTDOWN_crash) )\n    {\n        /* We might end up with a crashed domain here if\n         * sh_remove_shadows() in a previous sh_resync() call has\n         * failed. We cannot safely continue since some page is still\n         * OOS but not in the hash table anymore. */\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        return 0;\n    }\n\n    /* Final check: if someone has synced a page, it's possible that\n     * our l1e is stale.  Compare the entries, and rewalk if necessary. */\n    if ( shadow_check_gl1e(v, &gw)  )\n    {\n        perfc_incr(shadow_inconsistent_gwalk);\n        paging_unlock(d);\n        put_gfn(d, gfn_x(gfn));\n        goto rewalk;\n    }\n#endif /* OOS */\n\n    /* Calculate the shadow entry and write it */\n    l1e_propagate_from_guest(v, gw.l1e, gmfn, &sl1e, ft, p2mt);\n    r = shadow_set_l1e(d, ptr_sl1e, sl1e, p2mt, sl1mfn);\n\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n    if ( mfn_valid(gw.l1mfn)\n         && mfn_is_out_of_sync(gw.l1mfn) )\n    {\n        /* Update the OOS snapshot. */\n        mfn_t snpmfn = oos_snapshot_lookup(d, gw.l1mfn);\n        guest_l1e_t *snp;\n\n        ASSERT(mfn_valid(snpmfn));\n\n        snp = map_domain_page(snpmfn);\n        snp[guest_l1_table_offset(va)] = gw.l1e;\n        unmap_domain_page(snp);\n    }\n#endif /* OOS */\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_PREFETCH\n    /* Prefetch some more shadow entries */\n    sh_prefetch(v, &gw, ptr_sl1e, sl1mfn);\n#endif\n\n    /* Need to emulate accesses to page tables */\n    if ( sh_mfn_is_a_page_table(gmfn)\n#if (SHADOW_OPTIMIZATIONS & SHOPT_OUT_OF_SYNC)\n         /* Unless they've been allowed to go out of sync with their\n            shadows and we don't need to unshadow it. */\n         && !(mfn_is_out_of_sync(gmfn)\n              && !(regs->error_code & PFEC_user_mode))\n#endif\n         && (ft == ft_demand_write) )\n    {\n        perfc_incr(shadow_fault_emulate_write);\n        goto emulate;\n    }\n\n    /* Need to hand off device-model MMIO to the device model */\n    if ( p2mt == p2m_mmio_dm )\n    {\n        gpa = guest_walk_to_gpa(&gw);\n        goto mmio;\n    }\n\n    /* Ignore attempts to write to read-only memory. */\n    if ( p2m_is_readonly(p2mt) && (ft == ft_demand_write) )\n    {\n        static unsigned long lastpage;\n        if ( xchg(&lastpage, va & PAGE_MASK) != (va & PAGE_MASK) )\n            gdprintk(XENLOG_DEBUG, \"guest attempted write to read-only memory\"\n                     \" page. va page=%#lx, mfn=%#lx\\n\",\n                     va & PAGE_MASK, mfn_x(gmfn));\n        goto emulate_readonly; /* skip over the instruction */\n    }\n\n    /* In HVM guests, we force CR0.WP always to be set, so that the\n     * pagetables are always write-protected.  If the guest thinks\n     * CR0.WP is clear, we must emulate faulting supervisor writes to\n     * allow the guest to write through read-only PTEs.  Emulate if the\n     * fault was a non-user write to a present page.  */\n    if ( is_hvm_domain(d)\n         && unlikely(!hvm_wp_enabled(v))\n         && regs->error_code == (PFEC_write_access|PFEC_page_present)\n         && mfn_valid(gmfn) )\n    {\n        perfc_incr(shadow_fault_emulate_wp);\n        goto emulate;\n    }\n\n    perfc_incr(shadow_fault_fixed);\n    d->arch.paging.log_dirty.fault_count++;\n    sh_reset_early_unshadow(v);\n\n    trace_shadow_fixup(gw.l1e, va);\n done:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"fixed\\n\");\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    return EXCRET_fault_fixed;\n\n emulate:\n    if ( !shadow_mode_refcounts(d) || !guest_mode(regs) )\n        goto not_a_shadow_fault;\n\n    /*\n     * We do not emulate user writes. Instead we use them as a hint that the\n     * page is no longer a page table. This behaviour differs from native, but\n     * it seems very unlikely that any OS grants user access to page tables.\n     */\n    if ( (regs->error_code & PFEC_user_mode) )\n    {\n        SHADOW_PRINTK(\"user-mode fault to PT, unshadowing mfn %#lx\\n\",\n                      mfn_x(gmfn));\n        perfc_incr(shadow_fault_emulate_failed);\n        sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_USER,\n                                      va, gfn);\n        goto done;\n    }\n\n    /*\n     * Write from userspace to ro-mem needs to jump here to avoid getting\n     * caught by user-mode page-table check above.\n     */\n emulate_readonly:\n\n    /* Unshadow if we are writing to a toplevel pagetable that is\n     * flagged as a dying process, and that is not currently used. */\n    if ( sh_mfn_is_a_page_table(gmfn) && is_hvm_domain(d) &&\n         mfn_to_page(gmfn)->pagetable_dying )\n    {\n        int used = 0;\n        struct vcpu *tmp;\n        for_each_vcpu(d, tmp)\n        {\n#if GUEST_PAGING_LEVELS == 3\n            int i;\n            for ( i = 0; i < 4; i++ )\n            {\n                mfn_t smfn = pagetable_get_mfn(v->arch.shadow_table[i]);\n\n                if ( mfn_valid(smfn) && (mfn_x(smfn) != 0) )\n                {\n                    used |= (mfn_to_page(smfn)->v.sh.back == mfn_x(gmfn));\n\n                    if ( used )\n                        break;\n                }\n            }\n#else /* 32 or 64 */\n            used = mfn_eq(pagetable_get_mfn(tmp->arch.guest_table), gmfn);\n#endif\n            if ( used )\n                break;\n        }\n\n        if ( !used )\n            sh_remove_shadows(d, gmfn, 1 /* fast */, 0 /* can fail */);\n    }\n\n    /*\n     * We don't need to hold the lock for the whole emulation; we will\n     * take it again when we write to the pagetables.\n     */\n    sh_audit_gw(v, &gw);\n    shadow_audit_tables(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\n    this_cpu(trace_emulate_write_val) = 0;\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n early_emulation:\n#endif\n    if ( is_hvm_domain(d) )\n    {\n        /*\n         * If we are in the middle of injecting an exception or interrupt then\n         * we should not emulate: it is not the instruction at %eip that caused\n         * the fault. Furthermore it is almost certainly the case the handler\n         * stack is currently considered to be a page table, so we should\n         * unshadow the faulting page before exiting.\n         */\n        if ( unlikely(hvm_event_pending(v)) )\n        {\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n            if ( fast_emul )\n            {\n                perfc_incr(shadow_fault_fast_emulate_fail);\n                v->arch.paging.last_write_emul_ok = 0;\n            }\n#endif\n            gdprintk(XENLOG_DEBUG, \"write to pagetable during event \"\n                     \"injection: cr2=%#lx, mfn=%#lx\\n\",\n                     va, mfn_x(gmfn));\n            sh_remove_shadows(d, gmfn, 0 /* thorough */, 1 /* must succeed */);\n            trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_EVTINJ,\n                                       va, gfn);\n            return EXCRET_fault_fixed;\n        }\n    }\n\n    SHADOW_PRINTK(\"emulate: eip=%#lx esp=%#lx\\n\", regs->rip, regs->rsp);\n\n    emul_ops = shadow_init_emulation(&emul_ctxt, regs, GUEST_PTE_SIZE);\n\n    r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n#ifdef CONFIG_HVM\n    if ( r == X86EMUL_EXCEPTION )\n    {\n        ASSERT(is_hvm_domain(d));\n        /*\n         * This emulation covers writes to shadow pagetables.  We tolerate #PF\n         * (from accesses spanning pages, concurrent paging updated from\n         * vcpus, etc) and #GP[0]/#SS[0] (from segmentation errors).  Anything\n         * else is an emulation bug, or a guest playing with the instruction\n         * stream under Xen's feet.\n         */\n        if ( emul_ctxt.ctxt.event.type == X86_EVENTTYPE_HW_EXCEPTION &&\n             ((emul_ctxt.ctxt.event.vector == TRAP_page_fault) ||\n              (((emul_ctxt.ctxt.event.vector == TRAP_gp_fault) ||\n                (emul_ctxt.ctxt.event.vector == TRAP_stack_error)) &&\n               emul_ctxt.ctxt.event.error_code == 0)) )\n            hvm_inject_event(&emul_ctxt.ctxt.event);\n        else\n        {\n            SHADOW_PRINTK(\n                \"Unexpected event (type %u, vector %#x) from emulation\\n\",\n                emul_ctxt.ctxt.event.type, emul_ctxt.ctxt.event.vector);\n            r = X86EMUL_UNHANDLEABLE;\n        }\n    }\n#endif\n\n    /*\n     * NB. We do not unshadow on X86EMUL_EXCEPTION. It's not clear that it\n     * would be a good unshadow hint. If we *do* decide to unshadow-on-fault\n     * then it must be 'failable': we cannot require the unshadow to succeed.\n     */\n    if ( r == X86EMUL_UNHANDLEABLE || r == X86EMUL_UNIMPLEMENTED )\n    {\n        perfc_incr(shadow_fault_emulate_failed);\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n        if ( fast_emul )\n        {\n            perfc_incr(shadow_fault_fast_emulate_fail);\n            v->arch.paging.last_write_emul_ok = 0;\n        }\n#endif\n        SHADOW_PRINTK(\"emulator failure (rc=%d), unshadowing mfn %#lx\\n\",\n                       r, mfn_x(gmfn));\n        /* If this is actually a page table, then we have a bug, and need\n         * to support more operations in the emulator.  More likely,\n         * though, this is a hint that this page should not be shadowed. */\n        shadow_remove_all_shadows(d, gmfn);\n\n        trace_shadow_emulate_other(TRC_SHADOW_EMULATE_UNSHADOW_UNHANDLED,\n                                   va, gfn);\n        goto emulate_done;\n    }\n\n#if SHADOW_OPTIMIZATIONS & SHOPT_FAST_EMULATION\n    /* Record successfully emulated information as heuristics to next\n     * fault on same frame for acceleration. But be careful to verify\n     * its attribute still as page table, or else unshadow triggered\n     * in write emulation normally requires a re-sync with guest page\n     * table to recover r/w permission. Incorrect record for such case\n     * will cause unexpected more shadow faults due to propagation is\n     * skipped.\n     */\n    if ( (r == X86EMUL_OKAY) && sh_mfn_is_a_page_table(gmfn) )\n    {\n        if ( !fast_emul )\n        {\n            v->arch.paging.shadow.last_emulated_frame = va >> PAGE_SHIFT;\n            v->arch.paging.shadow.last_emulated_mfn = mfn_x(gmfn);\n            v->arch.paging.last_write_emul_ok = 1;\n        }\n    }\n    else if ( fast_emul )\n        v->arch.paging.last_write_emul_ok = 0;\n#endif\n\n    if ( emul_ctxt.ctxt.retire.singlestep )\n        hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n#if GUEST_PAGING_LEVELS == 3 /* PAE guest */\n    /*\n     * If there are no pending actions, emulate up to four extra instructions\n     * in the hope of catching the \"second half\" of a 64-bit pagetable write.\n     */\n    if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n    {\n        int i, emulation_count=0;\n        this_cpu(trace_emulate_initial_va) = va;\n\n        for ( i = 0 ; i < 4 ; i++ )\n        {\n            shadow_continue_emulation(&emul_ctxt, regs);\n            v->arch.paging.last_write_was_pt = 0;\n            r = x86_emulate(&emul_ctxt.ctxt, emul_ops);\n\n            /*\n             * Only continue the search for the second half if there are no\n             * exceptions or pending actions.  Otherwise, give up and re-enter\n             * the guest.\n             */\n            if ( r == X86EMUL_OKAY && !emul_ctxt.ctxt.retire.raw )\n            {\n                emulation_count++;\n                if ( v->arch.paging.last_write_was_pt )\n                {\n                    perfc_incr(shadow_em_ex_pt);\n                    TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_2ND_PT_WRITTEN);\n                    break; /* Don't emulate past the other half of the write */\n                }\n                else\n                    perfc_incr(shadow_em_ex_non_pt);\n            }\n            else\n            {\n                perfc_incr(shadow_em_ex_fail);\n                TRACE_SHADOW_PATH_FLAG(TRCE_SFLAG_EMULATION_LAST_FAILED);\n\n                if ( emul_ctxt.ctxt.retire.singlestep )\n                    hvm_inject_hw_exception(TRAP_debug, X86_EVENT_NO_EC);\n\n                break; /* Don't emulate again if we failed! */\n            }\n        }\n        this_cpu(trace_extra_emulation_count)=emulation_count;\n    }\n#endif /* PAE guest */\n\n    trace_shadow_emulate(gw.l1e, va);\n emulate_done:\n    SHADOW_PRINTK(\"emulated\\n\");\n    return EXCRET_fault_fixed;\n\n mmio:\n    if ( !guest_mode(regs) )\n        goto not_a_shadow_fault;\n#ifdef CONFIG_HVM\n    ASSERT(is_hvm_vcpu(v));\n    perfc_incr(shadow_fault_mmio);\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"mmio %#\"PRIpaddr\"\\n\", gpa);\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n    trace_shadow_gen(TRC_SHADOW_MMIO, va);\n    return (handle_mmio_with_translation(va, gpa >> PAGE_SHIFT, access)\n            ? EXCRET_fault_fixed : 0);\n#else\n    BUG();\n#endif\n\n not_a_shadow_fault:\n    sh_audit_gw(v, &gw);\n    SHADOW_PRINTK(\"not a shadow fault\\n\");\n    shadow_audit_tables(v);\n    sh_reset_early_unshadow(v);\n    paging_unlock(d);\n    put_gfn(d, gfn_x(gfn));\n\npropagate:\n    trace_not_shadow_fault(gw.l1e, va);\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -500,8 +500,8 @@\n \n     /* Unshadow if we are writing to a toplevel pagetable that is\n      * flagged as a dying process, and that is not currently used. */\n-    if ( sh_mfn_is_a_page_table(gmfn)\n-         && (mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying) )\n+    if ( sh_mfn_is_a_page_table(gmfn) && is_hvm_domain(d) &&\n+         mfn_to_page(gmfn)->pagetable_dying )\n     {\n         int used = 0;\n         struct vcpu *tmp;",
        "diff_line_info": {
            "deleted_lines": [
                "    if ( sh_mfn_is_a_page_table(gmfn)",
                "         && (mfn_to_page(gmfn)->shadow_flags & SHF_pagetable_dying) )"
            ],
            "added_lines": [
                "    if ( sh_mfn_is_a_page_table(gmfn) && is_hvm_domain(d) &&",
                "         mfn_to_page(gmfn)->pagetable_dying )"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-22998",
        "func_name": "torvalds/linux/virtio_gpu_object_shmem_init",
        "description": "In the Linux kernel before 6.0.3, drivers/gpu/drm/virtio/virtgpu_object.c misinterprets the drm_gem_shmem_get_sg_table return value (expects it to be NULL in the error case, whereas it is actually an error pointer).",
        "git_url": "https://github.com/torvalds/linux/commit/c24968734abfed81c8f93dc5f44a7b7a9aecadfa",
        "commit_title": "drm/virtio: Fix NULL vs IS_ERR checking in virtio_gpu_object_shmem_init",
        "commit_text": " Since drm_prime_pages_to_sg() function return error pointers. The drm_gem_shmem_get_sg_table() function returns error pointers too. Using IS_ERR() to check the return value to fix this.  Link: http://patchwork.freedesktop.org/patch/msgid/20220602104223.54527-1-linmq006@gmail.com",
        "func_before": "static int virtio_gpu_object_shmem_init(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tstruct virtio_gpu_object *bo,\n\t\t\t\t\tstruct virtio_gpu_mem_entry **ents,\n\t\t\t\t\tunsigned int *nents)\n{\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\tstruct scatterlist *sg;\n\tint si, ret;\n\n\tret = drm_gem_shmem_pin(&bo->base);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * virtio_gpu uses drm_gem_shmem_get_sg_table instead of\n\t * drm_gem_shmem_get_pages_sgt because virtio has it's own set of\n\t * dma-ops. This is discouraged for other drivers, but should be fine\n\t * since virtio_gpu doesn't support dma-buf import from other devices.\n\t */\n\tshmem->pages = drm_gem_shmem_get_sg_table(&bo->base);\n\tif (!shmem->pages) {\n\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\treturn -EINVAL;\n\t}\n\n\tif (use_dma_api) {\n\t\tret = dma_map_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t      shmem->pages, DMA_TO_DEVICE, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t*nents = shmem->mapped = shmem->pages->nents;\n\t} else {\n\t\t*nents = shmem->pages->orig_nents;\n\t}\n\n\t*ents = kvmalloc_array(*nents,\n\t\t\t       sizeof(struct virtio_gpu_mem_entry),\n\t\t\t       GFP_KERNEL);\n\tif (!(*ents)) {\n\t\tDRM_ERROR(\"failed to allocate ent list\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (use_dma_api) {\n\t\tfor_each_sgtable_dma_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_dma_address(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg_dma_len(sg));\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t} else {\n\t\tfor_each_sgtable_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_phys(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg->length);\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "func": "static int virtio_gpu_object_shmem_init(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tstruct virtio_gpu_object *bo,\n\t\t\t\t\tstruct virtio_gpu_mem_entry **ents,\n\t\t\t\t\tunsigned int *nents)\n{\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\tstruct virtio_gpu_object_shmem *shmem = to_virtio_gpu_shmem(bo);\n\tstruct scatterlist *sg;\n\tint si, ret;\n\n\tret = drm_gem_shmem_pin(&bo->base);\n\tif (ret < 0)\n\t\treturn -EINVAL;\n\n\t/*\n\t * virtio_gpu uses drm_gem_shmem_get_sg_table instead of\n\t * drm_gem_shmem_get_pages_sgt because virtio has it's own set of\n\t * dma-ops. This is discouraged for other drivers, but should be fine\n\t * since virtio_gpu doesn't support dma-buf import from other devices.\n\t */\n\tshmem->pages = drm_gem_shmem_get_sg_table(&bo->base);\n\tif (IS_ERR(shmem->pages)) {\n\t\tdrm_gem_shmem_unpin(&bo->base);\n\t\treturn PTR_ERR(shmem->pages);\n\t}\n\n\tif (use_dma_api) {\n\t\tret = dma_map_sgtable(vgdev->vdev->dev.parent,\n\t\t\t\t      shmem->pages, DMA_TO_DEVICE, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\t*nents = shmem->mapped = shmem->pages->nents;\n\t} else {\n\t\t*nents = shmem->pages->orig_nents;\n\t}\n\n\t*ents = kvmalloc_array(*nents,\n\t\t\t       sizeof(struct virtio_gpu_mem_entry),\n\t\t\t       GFP_KERNEL);\n\tif (!(*ents)) {\n\t\tDRM_ERROR(\"failed to allocate ent list\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (use_dma_api) {\n\t\tfor_each_sgtable_dma_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_dma_address(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg_dma_len(sg));\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t} else {\n\t\tfor_each_sgtable_sg(shmem->pages, sg, si) {\n\t\t\t(*ents)[si].addr = cpu_to_le64(sg_phys(sg));\n\t\t\t(*ents)[si].length = cpu_to_le32(sg->length);\n\t\t\t(*ents)[si].padding = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -19,9 +19,9 @@\n \t * since virtio_gpu doesn't support dma-buf import from other devices.\n \t */\n \tshmem->pages = drm_gem_shmem_get_sg_table(&bo->base);\n-\tif (!shmem->pages) {\n+\tif (IS_ERR(shmem->pages)) {\n \t\tdrm_gem_shmem_unpin(&bo->base);\n-\t\treturn -EINVAL;\n+\t\treturn PTR_ERR(shmem->pages);\n \t}\n \n \tif (use_dma_api) {",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!shmem->pages) {",
                "\t\treturn -EINVAL;"
            ],
            "added_lines": [
                "\tif (IS_ERR(shmem->pages)) {",
                "\t\treturn PTR_ERR(shmem->pages);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-49284",
        "func_name": "fish-shell/str2wcs_internal",
        "description": "fish is a smart and user-friendly command line shell for macOS, Linux, and the rest of the family. fish shell uses certain Unicode non-characters internally for marking wildcards and expansions. It will incorrectly allow these markers to be read on command substitution output, rather than transforming them into a safe internal representation. While this may cause unexpected behavior with direct input (for example, echo \\UFDD2HOME has the same output as echo $HOME), this may become a minor security problem if the output is being fed from an external program into a command substitution where this output may not be expected. This design flaw was introduced in very early versions of fish, predating the version control system, and is thought to be present in every version of fish released in the last 15 years or more, although with different characters. Code execution does not appear to be possible, but denial of service (through large brace expansion) or information disclosure (such as variable expansion) is potentially possible under certain circumstances. fish shell 3.6.2 has been released to correct this issue. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
        "git_url": "https://github.com/fish-shell/fish-shell/commit/09986f5563e31e2c900a606438f1d60d008f3a14",
        "commit_title": "Encode all ENCODE_DIRECT codepoints with encode_direct",
        "commit_text": "",
        "func_before": "static wcstring str2wcs_internal(const char *in, const size_t in_len) {\n    if (in_len == 0) return wcstring();\n    assert(in != nullptr);\n\n    wcstring result;\n    result.reserve(in_len);\n\n    size_t in_pos = 0;\n    mbstate_t state = {};\n    while (in_pos < in_len) {\n        // Append any initial sequence of ascii characters.\n        // Note we do not support character sets which are not supersets of ASCII.\n        size_t ascii_prefix_length = count_ascii_prefix(&in[in_pos], in_len - in_pos);\n        result.insert(result.end(), &in[in_pos], &in[in_pos + ascii_prefix_length]);\n        in_pos += ascii_prefix_length;\n        assert(in_pos <= in_len && \"Position overflowed length\");\n        if (in_pos == in_len) break;\n\n        // We have found a non-ASCII character.\n        bool use_encode_direct = false;\n        size_t ret = 0;\n        wchar_t wc = 0;\n\n        if (false) {\n#if defined(HAVE_BROKEN_MBRTOWC_UTF8)\n        } else if ((in[in_pos] & 0xF8) == 0xF8) {\n            // Protect against broken std::mbrtowc() implementations which attempt to encode UTF-8\n            // sequences longer than four bytes (e.g., OS X Snow Leopard).\n            use_encode_direct = true;\n#endif\n        } else if (sizeof(wchar_t) == 2 &&  //!OCLINT(constant if expression)\n                   (in[in_pos] & 0xF8) == 0xF0) {\n            // Assume we are in a UTF-16 environment (e.g., Cygwin) using a UTF-8 encoding.\n            // The bits set check will be true for a four byte UTF-8 sequence that requires\n            // two UTF-16 chars. Something that doesn't work with our simple use of std::mbrtowc().\n            use_encode_direct = true;\n        } else {\n            ret = std::mbrtowc(&wc, &in[in_pos], in_len - in_pos, &state);\n            // Determine whether to encode this character with our crazy scheme.\n            if (wc >= ENCODE_DIRECT_BASE && wc < ENCODE_DIRECT_BASE + 256) {\n                use_encode_direct = true;\n            } else if (wc == INTERNAL_SEPARATOR) {\n                use_encode_direct = true;\n            } else if (ret == static_cast<size_t>(-2)) {\n                // Incomplete sequence.\n                use_encode_direct = true;\n            } else if (ret == static_cast<size_t>(-1)) {\n                // Invalid data.\n                use_encode_direct = true;\n            } else if (ret > in_len - in_pos) {\n                // Other error codes? Terrifying, should never happen.\n                use_encode_direct = true;\n            } else if (sizeof(wchar_t) == 2 && wc >= 0xD800 &&  //!OCLINT(constant if expression)\n                       wc <= 0xDFFF) {\n                // If we get a surrogate pair char on a UTF-16 system (e.g., Cygwin) then\n                // it's guaranteed the UTF-8 decoding is wrong so use direct encoding.\n                use_encode_direct = true;\n            }\n        }\n\n        if (use_encode_direct) {\n            wc = ENCODE_DIRECT_BASE + static_cast<unsigned char>(in[in_pos]);\n            result.push_back(wc);\n            in_pos++;\n            std::memset(&state, 0, sizeof state);\n        } else if (ret == 0) {  // embedded null byte!\n            result.push_back(L'\\0');\n            in_pos++;\n            std::memset(&state, 0, sizeof state);\n        } else {  // normal case\n            result.push_back(wc);\n            in_pos += ret;\n        }\n    }\n\n    return result;\n}",
        "func": "static wcstring str2wcs_internal(const char *in, const size_t in_len) {\n    if (in_len == 0) return wcstring();\n    assert(in != nullptr);\n\n    wcstring result;\n    result.reserve(in_len);\n\n    size_t in_pos = 0;\n    mbstate_t state = {};\n    while (in_pos < in_len) {\n        // Append any initial sequence of ascii characters.\n        // Note we do not support character sets which are not supersets of ASCII.\n        size_t ascii_prefix_length = count_ascii_prefix(&in[in_pos], in_len - in_pos);\n        result.insert(result.end(), &in[in_pos], &in[in_pos + ascii_prefix_length]);\n        in_pos += ascii_prefix_length;\n        assert(in_pos <= in_len && \"Position overflowed length\");\n        if (in_pos == in_len) break;\n\n        // We have found a non-ASCII character.\n        bool use_encode_direct = false;\n        size_t ret = 0;\n        wchar_t wc = 0;\n\n        if (false) {\n#if defined(HAVE_BROKEN_MBRTOWC_UTF8)\n        } else if ((in[in_pos] & 0xF8) == 0xF8) {\n            // Protect against broken std::mbrtowc() implementations which attempt to encode UTF-8\n            // sequences longer than four bytes (e.g., OS X Snow Leopard).\n            use_encode_direct = true;\n#endif\n        } else if (sizeof(wchar_t) == 2 &&  //!OCLINT(constant if expression)\n                   (in[in_pos] & 0xF8) == 0xF0) {\n            // Assume we are in a UTF-16 environment (e.g., Cygwin) using a UTF-8 encoding.\n            // The bits set check will be true for a four byte UTF-8 sequence that requires\n            // two UTF-16 chars. Something that doesn't work with our simple use of std::mbrtowc().\n            use_encode_direct = true;\n        } else {\n            ret = std::mbrtowc(&wc, &in[in_pos], in_len - in_pos, &state);\n            // Determine whether to encode this character with our crazy scheme.\n            if (fish_reserved_codepoint(wc)) {\n                use_encode_direct = true;\n            } else if (ret == static_cast<size_t>(-2)) {\n                // Incomplete sequence.\n                use_encode_direct = true;\n            } else if (ret == static_cast<size_t>(-1)) {\n                // Invalid data.\n                use_encode_direct = true;\n            } else if (ret > in_len - in_pos) {\n                // Other error codes? Terrifying, should never happen.\n                use_encode_direct = true;\n            } else if (sizeof(wchar_t) == 2 && wc >= 0xD800 &&  //!OCLINT(constant if expression)\n                       wc <= 0xDFFF) {\n                // If we get a surrogate pair char on a UTF-16 system (e.g., Cygwin) then\n                // it's guaranteed the UTF-8 decoding is wrong so use direct encoding.\n                use_encode_direct = true;\n            }\n        }\n\n        if (use_encode_direct) {\n            wc = ENCODE_DIRECT_BASE + static_cast<unsigned char>(in[in_pos]);\n            result.push_back(wc);\n            in_pos++;\n            std::memset(&state, 0, sizeof state);\n        } else if (ret == 0) {  // embedded null byte!\n            result.push_back(L'\\0');\n            in_pos++;\n            std::memset(&state, 0, sizeof state);\n        } else {  // normal case\n            result.push_back(wc);\n            in_pos += ret;\n        }\n    }\n\n    return result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -37,9 +37,7 @@\n         } else {\n             ret = std::mbrtowc(&wc, &in[in_pos], in_len - in_pos, &state);\n             // Determine whether to encode this character with our crazy scheme.\n-            if (wc >= ENCODE_DIRECT_BASE && wc < ENCODE_DIRECT_BASE + 256) {\n-                use_encode_direct = true;\n-            } else if (wc == INTERNAL_SEPARATOR) {\n+            if (fish_reserved_codepoint(wc)) {\n                 use_encode_direct = true;\n             } else if (ret == static_cast<size_t>(-2)) {\n                 // Incomplete sequence.",
        "diff_line_info": {
            "deleted_lines": [
                "            if (wc >= ENCODE_DIRECT_BASE && wc < ENCODE_DIRECT_BASE + 256) {",
                "                use_encode_direct = true;",
                "            } else if (wc == INTERNAL_SEPARATOR) {"
            ],
            "added_lines": [
                "            if (fish_reserved_codepoint(wc)) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-49284",
        "func_name": "fish-shell/read_unquoted_escape",
        "description": "fish is a smart and user-friendly command line shell for macOS, Linux, and the rest of the family. fish shell uses certain Unicode non-characters internally for marking wildcards and expansions. It will incorrectly allow these markers to be read on command substitution output, rather than transforming them into a safe internal representation. While this may cause unexpected behavior with direct input (for example, echo \\UFDD2HOME has the same output as echo $HOME), this may become a minor security problem if the output is being fed from an external program into a command substitution where this output may not be expected. This design flaw was introduced in very early versions of fish, predating the version control system, and is thought to be present in every version of fish released in the last 15 years or more, although with different characters. Code execution does not appear to be possible, but denial of service (through large brace expansion) or information disclosure (such as variable expansion) is potentially possible under certain circumstances. fish shell 3.6.2 has been released to correct this issue. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
        "git_url": "https://github.com/fish-shell/fish-shell/commit/09986f5563e31e2c900a606438f1d60d008f3a14",
        "commit_title": "Encode all ENCODE_DIRECT codepoints with encode_direct",
        "commit_text": "",
        "func_before": "maybe_t<size_t> read_unquoted_escape(const wchar_t *input, wcstring *result, bool allow_incomplete,\n                                     bool unescape_special) {\n    assert(input[0] == L'\\\\' && \"Not an escape\");\n\n    // Here's the character we'll ultimately append, or none. Note that L'\\0' is a\n    // valid thing to append.\n    maybe_t<wchar_t> result_char_or_none = none();\n\n    bool errored = false;\n    size_t in_pos = 1;  // in_pos always tracks the next character to read (and therefore the number\n                        // of characters read so far)\n\n    // For multibyte \\X sequences.\n    std::string byte_buff;\n    while (true) {\n        const wchar_t c = input[in_pos++];\n        switch (c) {\n                // A null character after a backslash is an error.\n            case L'\\0': {\n                // Adjust in_pos to only include the backslash.\n                assert(in_pos > 0);\n                in_pos--;\n\n                // It's an error, unless we're allowing incomplete escapes.\n                if (!allow_incomplete) errored = true;\n                break;\n            }\n                // Numeric escape sequences. No prefix means octal escape, otherwise hexadecimal.\n            case L'0':\n            case L'1':\n            case L'2':\n            case L'3':\n            case L'4':\n            case L'5':\n            case L'6':\n            case L'7':\n            case L'u':\n            case L'U':\n            case L'x':\n            case L'X': {\n                long long res = 0;\n                size_t chars = 2;\n                int base = 16;\n                bool byte_literal = false;\n                wchar_t max_val = ASCII_MAX;\n\n                switch (c) {\n                    case L'u': {\n                        chars = 4;\n                        max_val = UCS2_MAX;\n                        break;\n                    }\n                    case L'U': {\n                        chars = 8;\n                        max_val = WCHAR_MAX;\n\n                        // Don't exceed the largest Unicode code point - see #1107.\n                        if (0x10FFFF < max_val) max_val = static_cast<wchar_t>(0x10FFFF);\n                        break;\n                    }\n                    case L'x':\n                    case L'X': {\n                        byte_literal = true;\n                        max_val = BYTE_MAX;\n                        break;\n                    }\n                    default: {\n                        base = 8;\n                        chars = 3;\n                        // Note that in_pos currently is just after the first post-backslash\n                        // character; we want to start our escape from there.\n                        assert(in_pos > 0);\n                        in_pos--;\n                        break;\n                    }\n                }\n\n                for (size_t i = 0; i < chars; i++) {\n                    long d = convert_digit(input[in_pos], base);\n                    if (d < 0) {\n                        // If we have no digit, this is a tokenizer error.\n                        if (i == 0) errored = true;\n                        break;\n                    }\n\n                    res = (res * base) + d;\n                    in_pos++;\n                }\n\n                if (!errored && res <= max_val) {\n                    if (byte_literal) {\n                        // Multibyte encodings necessitate that we keep adjacent byte escapes.\n                        // - `\\Xc3\\Xb6` is \"\", but only together.\n                        // (this assumes a valid codepoint can't consist of multiple bytes\n                        // that are valid on their own, which is true for UTF-8)\n                        byte_buff.push_back(static_cast<char>(res));\n                        result_char_or_none = none();\n                        if (input[in_pos] == L'\\\\'\n                            && (input[in_pos + 1] == L'X' || input[in_pos + 1] == L'x')) {\n                            in_pos++;\n                            continue;\n                        }\n                    } else {\n                        result_char_or_none = static_cast<wchar_t>(res);\n                    }\n                } else {\n                    errored = true;\n                }\n\n                break;\n            }\n                // \\a means bell (alert).\n            case L'a': {\n                result_char_or_none = L'\\a';\n                break;\n            }\n                // \\b means backspace.\n            case L'b': {\n                result_char_or_none = L'\\b';\n                break;\n            }\n                // \\cX means control sequence X.\n            case L'c': {\n                const wchar_t sequence_char = input[in_pos++];\n                if (sequence_char >= L'a' && sequence_char <= (L'a' + 32)) {\n                    result_char_or_none = sequence_char - L'a' + 1;\n                } else if (sequence_char >= L'A' && sequence_char <= (L'A' + 32)) {\n                    result_char_or_none = sequence_char - L'A' + 1;\n                } else {\n                    errored = true;\n                }\n                break;\n            }\n                // \\x1B means escape.\n            case L'e': {\n                result_char_or_none = L'\\x1B';\n                break;\n            }\n                // \\f means form feed.\n            case L'f': {\n                result_char_or_none = L'\\f';\n                break;\n            }\n                // \\n means newline.\n            case L'n': {\n                result_char_or_none = L'\\n';\n                break;\n            }\n                // \\r means carriage return.\n            case L'r': {\n                result_char_or_none = L'\\r';\n                break;\n            }\n                // \\t means tab.\n            case L't': {\n                result_char_or_none = L'\\t';\n                break;\n            }\n                // \\v means vertical tab.\n            case L'v': {\n                result_char_or_none = L'\\v';\n                break;\n            }\n                // If a backslash is followed by an actual newline, swallow them both.\n            case L'\\n': {\n                result_char_or_none = none();\n                break;\n            }\n            default: {\n                if (unescape_special) result->push_back(INTERNAL_SEPARATOR);\n                result_char_or_none = c;\n                break;\n            }\n        }\n\n        if (errored) return none();\n\n        if (!byte_buff.empty()) {\n            result->append(str2wcstring(byte_buff));\n        }\n\n        break;\n    }\n\n    if (result_char_or_none.has_value()) {\n        result->push_back(*result_char_or_none);\n    }\n\n    return in_pos;\n}",
        "func": "maybe_t<size_t> read_unquoted_escape(const wchar_t *input, wcstring *result, bool allow_incomplete,\n                                     bool unescape_special) {\n    assert(input[0] == L'\\\\' && \"Not an escape\");\n\n    // Here's the character we'll ultimately append, or none. Note that L'\\0' is a\n    // valid thing to append.\n    maybe_t<wchar_t> result_char_or_none = none();\n\n    bool errored = false;\n    size_t in_pos = 1;  // in_pos always tracks the next character to read (and therefore the number\n                        // of characters read so far)\n\n    // For multibyte \\X sequences.\n    std::string byte_buff;\n    while (true) {\n        const wchar_t c = input[in_pos++];\n        switch (c) {\n                // A null character after a backslash is an error.\n            case L'\\0': {\n                // Adjust in_pos to only include the backslash.\n                assert(in_pos > 0);\n                in_pos--;\n\n                // It's an error, unless we're allowing incomplete escapes.\n                if (!allow_incomplete) errored = true;\n                break;\n            }\n                // Numeric escape sequences. No prefix means octal escape, otherwise hexadecimal.\n            case L'0':\n            case L'1':\n            case L'2':\n            case L'3':\n            case L'4':\n            case L'5':\n            case L'6':\n            case L'7':\n            case L'u':\n            case L'U':\n            case L'x':\n            case L'X': {\n                long long res = 0;\n                size_t chars = 2;\n                int base = 16;\n                bool byte_literal = false;\n                wchar_t max_val = ASCII_MAX;\n\n                switch (c) {\n                    case L'u': {\n                        chars = 4;\n                        max_val = UCS2_MAX;\n                        break;\n                    }\n                    case L'U': {\n                        chars = 8;\n                        max_val = WCHAR_MAX;\n\n                        // Don't exceed the largest Unicode code point - see #1107.\n                        if (0x10FFFF < max_val) max_val = static_cast<wchar_t>(0x10FFFF);\n                        break;\n                    }\n                    case L'x':\n                    case L'X': {\n                        byte_literal = true;\n                        max_val = BYTE_MAX;\n                        break;\n                    }\n                    default: {\n                        base = 8;\n                        chars = 3;\n                        // Note that in_pos currently is just after the first post-backslash\n                        // character; we want to start our escape from there.\n                        assert(in_pos > 0);\n                        in_pos--;\n                        break;\n                    }\n                }\n\n                for (size_t i = 0; i < chars; i++) {\n                    long d = convert_digit(input[in_pos], base);\n                    if (d < 0) {\n                        // If we have no digit, this is a tokenizer error.\n                        if (i == 0) errored = true;\n                        break;\n                    }\n\n                    res = (res * base) + d;\n                    in_pos++;\n                }\n\n                if (!errored && res <= max_val) {\n                    if (byte_literal) {\n                        // Multibyte encodings necessitate that we keep adjacent byte escapes.\n                        // - `\\Xc3\\Xb6` is \"\", but only together.\n                        // (this assumes a valid codepoint can't consist of multiple bytes\n                        // that are valid on their own, which is true for UTF-8)\n                        byte_buff.push_back(static_cast<char>(res));\n                        result_char_or_none = none();\n                        if (input[in_pos] == L'\\\\'\n                            && (input[in_pos + 1] == L'X' || input[in_pos + 1] == L'x')) {\n                            in_pos++;\n                            continue;\n                        }\n                    } else {\n                        result_char_or_none = static_cast<wchar_t>(res);\n                    }\n                } else {\n                    errored = true;\n                }\n\n                break;\n            }\n                // \\a means bell (alert).\n            case L'a': {\n                result_char_or_none = L'\\a';\n                break;\n            }\n                // \\b means backspace.\n            case L'b': {\n                result_char_or_none = L'\\b';\n                break;\n            }\n                // \\cX means control sequence X.\n            case L'c': {\n                const wchar_t sequence_char = input[in_pos++];\n                if (sequence_char >= L'a' && sequence_char <= (L'a' + 32)) {\n                    result_char_or_none = sequence_char - L'a' + 1;\n                } else if (sequence_char >= L'A' && sequence_char <= (L'A' + 32)) {\n                    result_char_or_none = sequence_char - L'A' + 1;\n                } else {\n                    errored = true;\n                }\n                break;\n            }\n                // \\x1B means escape.\n            case L'e': {\n                result_char_or_none = L'\\x1B';\n                break;\n            }\n                // \\f means form feed.\n            case L'f': {\n                result_char_or_none = L'\\f';\n                break;\n            }\n                // \\n means newline.\n            case L'n': {\n                result_char_or_none = L'\\n';\n                break;\n            }\n                // \\r means carriage return.\n            case L'r': {\n                result_char_or_none = L'\\r';\n                break;\n            }\n                // \\t means tab.\n            case L't': {\n                result_char_or_none = L'\\t';\n                break;\n            }\n                // \\v means vertical tab.\n            case L'v': {\n                result_char_or_none = L'\\v';\n                break;\n            }\n                // If a backslash is followed by an actual newline, swallow them both.\n            case L'\\n': {\n                result_char_or_none = none();\n                break;\n            }\n            default: {\n                if (unescape_special) result->push_back(INTERNAL_SEPARATOR);\n                result_char_or_none = c;\n                break;\n            }\n        }\n\n        if (errored) return none();\n\n        if (!byte_buff.empty()) {\n            result->append(str2wcstring(byte_buff));\n        }\n\n        break;\n    }\n\n    if (result_char_or_none.has_value()) {\n        if (fish_reserved_codepoint(*result_char_or_none)) {\n            return none();\n        }\n        result->push_back(*result_char_or_none);\n    }\n\n    return in_pos;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -183,6 +183,9 @@\n     }\n \n     if (result_char_or_none.has_value()) {\n+        if (fish_reserved_codepoint(*result_char_or_none)) {\n+            return none();\n+        }\n         result->push_back(*result_char_or_none);\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        if (fish_reserved_codepoint(*result_char_or_none)) {",
                "            return none();",
                "        }"
            ]
        }
    }
]