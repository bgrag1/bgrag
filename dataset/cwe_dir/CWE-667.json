[
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/find_extend_vma",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "struct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && (vma->vm_start <= addr))\n\t\treturn vma;\n\tif (!prev || expand_stack(prev, addr))\n\t\treturn NULL;\n\tif (prev->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n\treturn prev;\n}",
        "func": "struct vm_area_struct *\nfind_extend_vma(struct mm_struct *mm, unsigned long addr)\n{\n\tstruct vm_area_struct *vma, *prev;\n\n\taddr &= PAGE_MASK;\n\tvma = find_vma_prev(mm, addr, &prev);\n\tif (vma && (vma->vm_start <= addr))\n\t\treturn vma;\n\t/* don't alter vm_end if the coredump is running */\n\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n\t\treturn NULL;\n\tif (prev->vm_flags & VM_LOCKED)\n\t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);\n\treturn prev;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -7,7 +7,8 @@\n \tvma = find_vma_prev(mm, addr, &prev);\n \tif (vma && (vma->vm_start <= addr))\n \t\treturn vma;\n-\tif (!prev || expand_stack(prev, addr))\n+\t/* don't alter vm_end if the coredump is running */\n+\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))\n \t\treturn NULL;\n \tif (prev->vm_flags & VM_LOCKED)\n \t\tpopulate_vma_page_range(prev, addr, prev->vm_end, NULL);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!prev || expand_stack(prev, addr))"
            ],
            "added_lines": [
                "\t/* don't alter vm_end if the coredump is running */",
                "\tif (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/uverbs_user_mmap_disassociate",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n{\n\tstruct rdma_umap_priv *priv, *next_priv;\n\n\tlockdep_assert_held(&ufile->hw_destroy_rwsem);\n\n\twhile (1) {\n\t\tstruct mm_struct *mm = NULL;\n\n\t\t/* Get an arbitrary mm pointer that hasn't been cleaned yet */\n\t\tmutex_lock(&ufile->umap_lock);\n\t\twhile (!list_empty(&ufile->umaps)) {\n\t\t\tint ret;\n\n\t\t\tpriv = list_first_entry(&ufile->umaps,\n\t\t\t\t\t\tstruct rdma_umap_priv, list);\n\t\t\tmm = priv->vma->vm_mm;\n\t\t\tret = mmget_not_zero(mm);\n\t\t\tif (!ret) {\n\t\t\t\tlist_del_init(&priv->list);\n\t\t\t\tmm = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tif (!mm)\n\t\t\treturn;\n\n\t\t/*\n\t\t * The umap_lock is nested under mmap_sem since it used within\n\t\t * the vma_ops callbacks, so we have to clean the list one mm\n\t\t * at a time to get the lock ordering right. Typically there\n\t\t * will only be one mm, so no big deal.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tmutex_lock(&ufile->umap_lock);\n\t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n\t\t\t\t\t  list) {\n\t\t\tstruct vm_area_struct *vma = priv->vma;\n\n\t\t\tif (vma->vm_mm != mm)\n\t\t\t\tcontinue;\n\t\t\tlist_del_init(&priv->list);\n\n\t\t\tzap_vma_ptes(vma, vma->vm_start,\n\t\t\t\t     vma->vm_end - vma->vm_start);\n\t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tup_write(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}",
        "func": "void uverbs_user_mmap_disassociate(struct ib_uverbs_file *ufile)\n{\n\tstruct rdma_umap_priv *priv, *next_priv;\n\n\tlockdep_assert_held(&ufile->hw_destroy_rwsem);\n\n\twhile (1) {\n\t\tstruct mm_struct *mm = NULL;\n\n\t\t/* Get an arbitrary mm pointer that hasn't been cleaned yet */\n\t\tmutex_lock(&ufile->umap_lock);\n\t\twhile (!list_empty(&ufile->umaps)) {\n\t\t\tint ret;\n\n\t\t\tpriv = list_first_entry(&ufile->umaps,\n\t\t\t\t\t\tstruct rdma_umap_priv, list);\n\t\t\tmm = priv->vma->vm_mm;\n\t\t\tret = mmget_not_zero(mm);\n\t\t\tif (!ret) {\n\t\t\t\tlist_del_init(&priv->list);\n\t\t\t\tmm = NULL;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\t\tif (!mm)\n\t\t\treturn;\n\n\t\t/*\n\t\t * The umap_lock is nested under mmap_sem since it used within\n\t\t * the vma_ops callbacks, so we have to clean the list one mm\n\t\t * at a time to get the lock ordering right. Typically there\n\t\t * will only be one mm, so no big deal.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tif (!mmget_still_valid(mm))\n\t\t\tgoto skip_mm;\n\t\tmutex_lock(&ufile->umap_lock);\n\t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n\t\t\t\t\t  list) {\n\t\t\tstruct vm_area_struct *vma = priv->vma;\n\n\t\t\tif (vma->vm_mm != mm)\n\t\t\t\tcontinue;\n\t\t\tlist_del_init(&priv->list);\n\n\t\t\tzap_vma_ptes(vma, vma->vm_start,\n\t\t\t\t     vma->vm_end - vma->vm_start);\n\t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n\t\t}\n\t\tmutex_unlock(&ufile->umap_lock);\n\tskip_mm:\n\t\tup_write(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -34,6 +34,8 @@\n \t\t * will only be one mm, so no big deal.\n \t\t */\n \t\tdown_write(&mm->mmap_sem);\n+\t\tif (!mmget_still_valid(mm))\n+\t\t\tgoto skip_mm;\n \t\tmutex_lock(&ufile->umap_lock);\n \t\tlist_for_each_entry_safe (priv, next_priv, &ufile->umaps,\n \t\t\t\t\t  list) {\n@@ -48,6 +50,7 @@\n \t\t\tvma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);\n \t\t}\n \t\tmutex_unlock(&ufile->umap_lock);\n+\tskip_mm:\n \t\tup_write(&mm->mmap_sem);\n \t\tmmput(mm);\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tif (!mmget_still_valid(mm))",
                "\t\t\tgoto skip_mm;",
                "\tskip_mm:"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/userfaultfd_release",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}",
        "func": "static int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t/* len == 0 means wake all */\n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t/*\n\t * Flush page faults out of all CPUs. NOTE: all page faults\n\t * must be retried without returning VM_FAULT_SIGBUS if\n\t * userfaultfd_ctx_get() succeeds but vma->vma_userfault_ctx\n\t * changes while handle_userfault released the mmap_sem. So\n\t * it's critical that released is set to true (above), before\n\t * taking the mmap_sem for writing.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto skip_mm;\n\tprev = NULL;\n\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev)\n\t\t\tvma = prev;\n\t\telse\n\t\t\tprev = vma;\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\nskip_mm:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nwakeup:\n\t/*\n\t * After no new page faults can wait on this fault_*wqh, flush\n\t * the last page faults that may have been already waiting on\n\t * the fault_*wqh.\n\t */\n\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t/* Flush pending events that may still wait on event_wqh */\n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,8 @@\n \t * taking the mmap_sem for writing.\n \t */\n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto skip_mm;\n \tprev = NULL;\n \tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\tcond_resched();\n@@ -43,6 +45,7 @@\n \t\tvma->vm_flags = new_flags;\n \t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n \t}\n+skip_mm:\n \tup_write(&mm->mmap_sem);\n \tmmput(mm);\n wakeup:",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!mmget_still_valid(mm))",
                "\t\tgoto skip_mm;",
                "skip_mm:"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/userfaultfd_register",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~(UFFDIO_REGISTER_MODE_MISSING|\n\t\t\t\t     UFFDIO_REGISTER_MODE_WP))\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n\t\tvm_flags |= VM_UFFD_WP;\n\t\t/*\n\t\t * FIXME: remove the below error constraint by\n\t\t * implementing the wprotect tracking mode.\n\t\t */\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto out_unlock;\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tbasic_ioctls = false;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/* check not compatible vmas */\n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * UFFDIO_COPY will fill file holes even without\n\t\t * PROT_WRITE. This check enforces that if this is a\n\t\t * MAP_SHARED, the process has write permission to the backing\n\t\t * file. If VM_MAYWRITE is set it also enforces that on a\n\t\t * MAP_SHARED vma: there is no F_WRITE_SEAL and no further\n\t\t * F_WRITE_SEAL can be taken until the vma is destroyed.\n\t\t */\n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If this vma contains ending address, and huge pages\n\t\t * check alignment.\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t/*\n\t\t * Check that this vma isn't already owned by a\n\t\t * different userfaultfd. We can't allow more than one\n\t\t * userfaultfd to own a single vma simultaneously or we\n\t\t * wouldn't know which one to deliver the userfaults to.\n\t\t */\n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * Note vmas containing huge pages\n\t\t */\n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~vm_flags) | vm_flags;\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\n\tif (!ret) {\n\t\t/*\n\t\t * Now that we scanned all vmas we can already tell\n\t\t * userland which ioctls methods are guaranteed to\n\t\t * succeed on this range.\n\t\t */\n\t\tif (put_user(basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t\t     UFFD_API_RANGE_IOCTLS,\n\t\t\t     &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,6 +50,8 @@\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!mmget_still_valid(mm))",
                "\t\tgoto out_unlock;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/userfaultfd_unregister",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "func": "static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tdown_write(&mm->mmap_sem);\n\tif (!mmget_still_valid(mm))\n\t\tgoto out_unlock;\n\tvma = find_vma_prev(mm, start, &prev);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t/* check that there's at least one vma in the range */\n\tret = -EINVAL;\n\tif (vma->vm_start >= end)\n\t\tgoto out_unlock;\n\n\t/*\n\t * If the first vma contains huge pages, make sure start address\n\t * is aligned to huge page size.\n\t */\n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t/*\n\t * Search for not compatible vmas.\n\t */\n\tfound = false;\n\tret = -EINVAL;\n\tfor (cur = vma; cur && cur->vm_start < end; cur = cur->vm_next) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & (VM_UFFD_MISSING | VM_UFFD_WP)));\n\n\t\t/*\n\t\t * Check not compatible vmas, not strictly required\n\t\t * here as not compatible vmas cannot have an\n\t\t * userfaultfd_ctx registered on them, but this\n\t\t * provides for more strict behavior to notice\n\t\t * unregistration errors.\n\t\t */\n\t\tif (!vma_can_userfault(cur))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t}\n\tBUG_ON(!found);\n\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma));\n\n\t\t/*\n\t\t * Nothing to do: this vma is already registered into this\n\t\t * userfaultfd and with the right tracking mode too.\n\t\t */\n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t/*\n\t\t\t * Wake any concurrent pending userfault while\n\t\t\t * we unregister, so they will not hang\n\t\t\t * permanently and it avoids userland to call\n\t\t\t * UFFDIO_WAKE explicitly.\n\t\t\t */\n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\tnew_flags = vma->vm_flags & ~(VM_UFFD_MISSING | VM_UFFD_WP);\n\t\tprev = vma_merge(mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX);\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(mm, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(mm, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t/*\n\t\t * In the vma_merge() successful mprotect-like case 8:\n\t\t * the next vma was merged into the current one and\n\t\t * the current one has not been updated yet.\n\t\t */\n\t\tvma->vm_flags = new_flags;\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t\tvma = vma->vm_next;\n\t} while (vma && vma->vm_start < end);\nout_unlock:\n\tup_write(&mm->mmap_sem);\n\tmmput(mm);\nout:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,6 +27,8 @@\n \t\tgoto out;\n \n \tdown_write(&mm->mmap_sem);\n+\tif (!mmget_still_valid(mm))\n+\t\tgoto out_unlock;\n \tvma = find_vma_prev(mm, start, &prev);\n \tif (!vma)\n \t\tgoto out_unlock;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (!mmget_still_valid(mm))",
                "\t\tgoto out_unlock;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/userfaultfd_event_wait_completion",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}",
        "func": "static void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock(&ctx->event_wqh.lock);\n\t/*\n\t * After the __add_wait_queue the uwq is visible to userland\n\t * through poll/read().\n\t */\n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t/*\n\t\t\t * &ewq->wq may be queued in fork_event, but\n\t\t\t * __remove_wait_queue ignores the head\n\t\t\t * parameter. It would be a problem if it\n\t\t\t * didn't.\n\t\t\t */\n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\n\t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n\t\tdown_write(&mm->mmap_sem);\n\t\t/* no task can run (and in turn coredump) yet */\n\t\tVM_WARN_ON(!mmget_still_valid(mm));\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tvma->vm_flags &= ~(VM_UFFD_WP | VM_UFFD_MISSING);\n\t\t\t}\n\t\tup_write(&mm->mmap_sem);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t/*\n\t * ctx may go away after this if the userfault pseudo fd is\n\t * already released.\n\t */\nout:\n\tWRITE_ONCE(ctx->mmap_changing, false);\n\tuserfaultfd_ctx_put(ctx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -56,6 +56,8 @@\n \n \t\t/* the various vma->vm_userfaultfd_ctx still points to it */\n \t\tdown_write(&mm->mmap_sem);\n+\t\t/* no task can run (and in turn coredump) yet */\n+\t\tVM_WARN_ON(!mmget_still_valid(mm));\n \t\tfor (vma = mm->mmap; vma; vma = vma->vm_next)\n \t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n \t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t/* no task can run (and in turn coredump) yet */",
                "\t\tVM_WARN_ON(!mmget_still_valid(mm));"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-11599",
        "func_name": "torvalds/linux/clear_refs_write",
        "description": "The coredump implementation in the Linux kernel before 5.0.10 does not use locking or other mechanisms to prevent vma layout or vma flags changes while it runs, which allows local users to obtain sensitive information, cause a denial of service, or possibly have unspecified other impact by triggering a race condition with mmget_not_zero or get_task_mm calls. This is related to fs/userfaultfd.c, mm/mmap.c, fs/proc/task_mmu.c, and drivers/infiniband/core/uverbs_main.c.",
        "git_url": "https://github.com/torvalds/linux/commit/04f5866e41fb70690e28397487d8bd8eea7d712a",
        "commit_title": "coredump: fix race condition between mmget_not_zero()/get_task_mm() and core dumping",
        "commit_text": " The core dumping code has always run without holding the mmap_sem for writing, despite that is the only way to ensure that the entire vma layout will not change from under it.  Only using some signal serialization on the processes belonging to the mm is not nearly enough. This was pointed out earlier.  For example in Hugh's post from Jul 2017:    https://lkml.kernel.org/r/alpine.LSU.2.11.1707191716030.2055@eggly.anvils    \"Not strictly relevant here, but a related note: I was very surprised    to discover, only quite recently, how handle_mm_fault() may be called    without down_read(mmap_sem) - when core dumping. That seems a    misguided optimization to me, which would also be nice to correct\"  In particular because the growsdown and growsup can move the vm_start/vm_end the various loops the core dump does around the vma will not be consistent if page faults can happen concurrently.  Pretty much all users calling mmget_not_zero()/get_task_mm() and then taking the mmap_sem had the potential to introduce unexpected side effects in the core dumping code.  Adding mmap_sem for writing around the ->core_dump invocation is a viable long term fix, but it requires removing all copy user and page faults and to replace them with get_dump_page() for all binary formats which is not suitable as a short term fix.  For the time being this solution manually covers the places that can confuse the core dump either by altering the vma layout or the vma flags while it runs.  Once ->core_dump runs under mmap_sem for writing the function mmget_still_valid() can be dropped.  Allowing mmap_sem protected sections to run in parallel with the coredump provides some minor parallelism advantage to the swapoff code (which seems to be safe enough by never mangling any vma field and can keep doing swapins in parallel to the core dumping) and to some other corner case.  In order to facilitate the backporting I added \"Fixes: 86039bd3b4e6\" however the side effect of this same race condition in /proc/pid/mem should be reproducible since before 2.6.12-rc2 so I couldn't add any other \"Fixes:\" because there's no hash beyond the git genesis commit.  Because find_extend_vma() is the only location outside of the process context that could modify the \"mm\" structures under mmap_sem for reading, by adding the mmget_still_valid() check to it, all other cases that take the mmap_sem for reading don't need the new check after mmget_not_zero()/get_task_mm().  The expand_stack() in page fault context also doesn't need the new check, because all tasks under core dumping are frozen.  Link: http://lkml.kernel.org/r/20190325224949.11068-1-aarcange@redhat.com Suggested-by: Oleg Nesterov <oleg@redhat.com> Cc: <stable@vger.kernel.org>",
        "func_before": "static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tenum clear_refs_types type;\n\tstruct mmu_gather tlb;\n\tint itype;\n\tint rv;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\trv = kstrtoint(strstrip(buffer), 10, &itype);\n\tif (rv < 0)\n\t\treturn rv;\n\ttype = (enum clear_refs_types)itype;\n\tif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\n\t\treturn -EINVAL;\n\n\ttask = get_proc_task(file_inode(file));\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mmu_notifier_range range;\n\t\tstruct clear_refs_private cp = {\n\t\t\t.type = type,\n\t\t};\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.test_walk = clear_refs_test_walk,\n\t\t\t.mm = mm,\n\t\t\t.private = &cp,\n\t\t};\n\n\t\tif (type == CLEAR_REFS_MM_HIWATER_RSS) {\n\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\tcount = -EINTR;\n\t\t\t\tgoto out_mm;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Writing 5 to /proc/pid/clear_refs resets the peak\n\t\t\t * resident set size to this mm's current rss value.\n\t\t\t */\n\t\t\treset_mm_hiwater_rss(mm);\n\t\t\tup_write(&mm->mmap_sem);\n\t\t\tgoto out_mm;\n\t\t}\n\n\t\tdown_read(&mm->mmap_sem);\n\t\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\tif (!(vma->vm_flags & VM_SOFTDIRTY))\n\t\t\t\t\tcontinue;\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\t\tcount = -EINTR;\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n\t\t\t\t\tvma_set_page_prot(vma);\n\t\t\t\t}\n\t\t\t\tdowngrade_write(&mm->mmap_sem);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmmu_notifier_range_init(&range, mm, 0, -1UL);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\t\twalk_page_range(0, mm->highest_vm_end, &clear_refs_walk);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY)\n\t\t\tmmu_notifier_invalidate_range_end(&range);\n\t\ttlb_finish_mmu(&tlb, 0, -1);\n\t\tup_read(&mm->mmap_sem);\nout_mm:\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}",
        "func": "static ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tenum clear_refs_types type;\n\tstruct mmu_gather tlb;\n\tint itype;\n\tint rv;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\trv = kstrtoint(strstrip(buffer), 10, &itype);\n\tif (rv < 0)\n\t\treturn rv;\n\ttype = (enum clear_refs_types)itype;\n\tif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\n\t\treturn -EINVAL;\n\n\ttask = get_proc_task(file_inode(file));\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mmu_notifier_range range;\n\t\tstruct clear_refs_private cp = {\n\t\t\t.type = type,\n\t\t};\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.test_walk = clear_refs_test_walk,\n\t\t\t.mm = mm,\n\t\t\t.private = &cp,\n\t\t};\n\n\t\tif (type == CLEAR_REFS_MM_HIWATER_RSS) {\n\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\tcount = -EINTR;\n\t\t\t\tgoto out_mm;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Writing 5 to /proc/pid/clear_refs resets the peak\n\t\t\t * resident set size to this mm's current rss value.\n\t\t\t */\n\t\t\treset_mm_hiwater_rss(mm);\n\t\t\tup_write(&mm->mmap_sem);\n\t\t\tgoto out_mm;\n\t\t}\n\n\t\tdown_read(&mm->mmap_sem);\n\t\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\tif (!(vma->vm_flags & VM_SOFTDIRTY))\n\t\t\t\t\tcontinue;\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\tif (down_write_killable(&mm->mmap_sem)) {\n\t\t\t\t\tcount = -EINTR;\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * Avoid to modify vma->vm_flags\n\t\t\t\t * without locked ops while the\n\t\t\t\t * coredump reads the vm_flags.\n\t\t\t\t */\n\t\t\t\tif (!mmget_still_valid(mm)) {\n\t\t\t\t\t/*\n\t\t\t\t\t * Silently return \"count\"\n\t\t\t\t\t * like if get_task_mm()\n\t\t\t\t\t * failed. FIXME: should this\n\t\t\t\t\t * function have returned\n\t\t\t\t\t * -ESRCH if get_task_mm()\n\t\t\t\t\t * failed like if\n\t\t\t\t\t * get_proc_task() fails?\n\t\t\t\t\t */\n\t\t\t\t\tup_write(&mm->mmap_sem);\n\t\t\t\t\tgoto out_mm;\n\t\t\t\t}\n\t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n\t\t\t\t\tvma_set_page_prot(vma);\n\t\t\t\t}\n\t\t\t\tdowngrade_write(&mm->mmap_sem);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmmu_notifier_range_init(&range, mm, 0, -1UL);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\t\twalk_page_range(0, mm->highest_vm_end, &clear_refs_walk);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY)\n\t\t\tmmu_notifier_invalidate_range_end(&range);\n\t\ttlb_finish_mmu(&tlb, 0, -1);\n\t\tup_read(&mm->mmap_sem);\nout_mm:\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -64,6 +64,24 @@\n \t\t\t\t\tcount = -EINTR;\n \t\t\t\t\tgoto out_mm;\n \t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * Avoid to modify vma->vm_flags\n+\t\t\t\t * without locked ops while the\n+\t\t\t\t * coredump reads the vm_flags.\n+\t\t\t\t */\n+\t\t\t\tif (!mmget_still_valid(mm)) {\n+\t\t\t\t\t/*\n+\t\t\t\t\t * Silently return \"count\"\n+\t\t\t\t\t * like if get_task_mm()\n+\t\t\t\t\t * failed. FIXME: should this\n+\t\t\t\t\t * function have returned\n+\t\t\t\t\t * -ESRCH if get_task_mm()\n+\t\t\t\t\t * failed like if\n+\t\t\t\t\t * get_proc_task() fails?\n+\t\t\t\t\t */\n+\t\t\t\t\tup_write(&mm->mmap_sem);\n+\t\t\t\t\tgoto out_mm;\n+\t\t\t\t}\n \t\t\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n \t\t\t\t\tvma->vm_flags &= ~VM_SOFTDIRTY;\n \t\t\t\t\tvma_set_page_prot(vma);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\t\t/*",
                "\t\t\t\t * Avoid to modify vma->vm_flags",
                "\t\t\t\t * without locked ops while the",
                "\t\t\t\t * coredump reads the vm_flags.",
                "\t\t\t\t */",
                "\t\t\t\tif (!mmget_still_valid(mm)) {",
                "\t\t\t\t\t/*",
                "\t\t\t\t\t * Silently return \"count\"",
                "\t\t\t\t\t * like if get_task_mm()",
                "\t\t\t\t\t * failed. FIXME: should this",
                "\t\t\t\t\t * function have returned",
                "\t\t\t\t\t * -ESRCH if get_task_mm()",
                "\t\t\t\t\t * failed like if",
                "\t\t\t\t\t * get_proc_task() fails?",
                "\t\t\t\t\t */",
                "\t\t\t\t\tup_write(&mm->mmap_sem);",
                "\t\t\t\t\tgoto out_mm;",
                "\t\t\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-14763",
        "func_name": "torvalds/linux/__dwc3_gadget_kick_transfer",
        "description": "In the Linux kernel before 4.16.4, a double-locking error in drivers/usb/dwc3/gadget.c may potentially cause a deadlock with f_hid.",
        "git_url": "https://github.com/torvalds/linux/commit/c91815b596245fd7da349ecc43c8def670d2269e",
        "commit_title": "usb: dwc3: gadget: never call ->complete() from ->ep_queue()",
        "commit_text": " This is a requirement which has always existed but, somehow, wasn't reflected in the documentation and problems weren't found until now when Tuba Yavuz found a possible deadlock happening between dwc3 and f_hid. She described the situation as follows:  spin_lock_irqsave(&hidg->write_spinlock, flags); // first acquire /* we our function has been disabled by host */ if (!hidg->req) { \tfree_ep_req(hidg->in_ep, hidg->req); \tgoto try_again; }  [...]  status = usb_ep_queue(hidg->in_ep, hidg->req, GFP_ATOMIC); => \t[...] \t=> usb_gadget_giveback_request \t\t=> \t\tf_hidg_req_complete \t\t\t=> \t\t\tspin_lock_irqsave(&hidg->write_spinlock, flags); // second acquire  Note that this happens because dwc3 would call ->complete() on a failed usb_ep_queue() due to failed Start Transfer command. This is, anyway, a theoretical situation because dwc3 currently uses \"No Response Update Transfer\" command for Bulk and Interrupt endpoints.  It's still good to make this case impossible to happen even if the \"No Reponse Update Transfer\" command is changed.  Cc: stable <stable@vger.kernel.org>",
        "func_before": "static int __dwc3_gadget_kick_transfer(struct dwc3_ep *dep)\n{\n\tstruct dwc3_gadget_ep_cmd_params params;\n\tstruct dwc3_request\t\t*req;\n\tint\t\t\t\tstarting;\n\tint\t\t\t\tret;\n\tu32\t\t\t\tcmd;\n\n\tif (!dwc3_calc_trbs_left(dep))\n\t\treturn 0;\n\n\tstarting = !(dep->flags & DWC3_EP_BUSY);\n\n\tdwc3_prepare_trbs(dep);\n\treq = next_request(&dep->started_list);\n\tif (!req) {\n\t\tdep->flags |= DWC3_EP_PENDING_REQUEST;\n\t\treturn 0;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (starting) {\n\t\tparams.param0 = upper_32_bits(req->trb_dma);\n\t\tparams.param1 = lower_32_bits(req->trb_dma);\n\t\tcmd = DWC3_DEPCMD_STARTTRANSFER;\n\n\t\tif (usb_endpoint_xfer_isoc(dep->endpoint.desc))\n\t\t\tcmd |= DWC3_DEPCMD_PARAM(dep->frame_number);\n\t} else {\n\t\tcmd = DWC3_DEPCMD_UPDATETRANSFER |\n\t\t\tDWC3_DEPCMD_PARAM(dep->resource_index);\n\t}\n\n\tret = dwc3_send_gadget_ep_cmd(dep, cmd, &params);\n\tif (ret < 0) {\n\t\t/*\n\t\t * FIXME we need to iterate over the list of requests\n\t\t * here and stop, unmap, free and del each of the linked\n\t\t * requests instead of what we do now.\n\t\t */\n\t\tif (req->trb)\n\t\t\tmemset(req->trb, 0, sizeof(struct dwc3_trb));\n\t\tdep->queued_requests--;\n\t\tdwc3_gadget_giveback(dep, req, ret);\n\t\treturn ret;\n\t}\n\n\tdep->flags |= DWC3_EP_BUSY;\n\n\tif (starting) {\n\t\tdep->resource_index = dwc3_gadget_ep_get_transfer_index(dep);\n\t\tWARN_ON_ONCE(!dep->resource_index);\n\t}\n\n\treturn 0;\n}",
        "func": "static int __dwc3_gadget_kick_transfer(struct dwc3_ep *dep)\n{\n\tstruct dwc3_gadget_ep_cmd_params params;\n\tstruct dwc3_request\t\t*req;\n\tint\t\t\t\tstarting;\n\tint\t\t\t\tret;\n\tu32\t\t\t\tcmd;\n\n\tif (!dwc3_calc_trbs_left(dep))\n\t\treturn 0;\n\n\tstarting = !(dep->flags & DWC3_EP_BUSY);\n\n\tdwc3_prepare_trbs(dep);\n\treq = next_request(&dep->started_list);\n\tif (!req) {\n\t\tdep->flags |= DWC3_EP_PENDING_REQUEST;\n\t\treturn 0;\n\t}\n\n\tmemset(&params, 0, sizeof(params));\n\n\tif (starting) {\n\t\tparams.param0 = upper_32_bits(req->trb_dma);\n\t\tparams.param1 = lower_32_bits(req->trb_dma);\n\t\tcmd = DWC3_DEPCMD_STARTTRANSFER;\n\n\t\tif (usb_endpoint_xfer_isoc(dep->endpoint.desc))\n\t\t\tcmd |= DWC3_DEPCMD_PARAM(dep->frame_number);\n\t} else {\n\t\tcmd = DWC3_DEPCMD_UPDATETRANSFER |\n\t\t\tDWC3_DEPCMD_PARAM(dep->resource_index);\n\t}\n\n\tret = dwc3_send_gadget_ep_cmd(dep, cmd, &params);\n\tif (ret < 0) {\n\t\t/*\n\t\t * FIXME we need to iterate over the list of requests\n\t\t * here and stop, unmap, free and del each of the linked\n\t\t * requests instead of what we do now.\n\t\t */\n\t\tif (req->trb)\n\t\t\tmemset(req->trb, 0, sizeof(struct dwc3_trb));\n\t\tdep->queued_requests--;\n\t\tdwc3_gadget_del_and_unmap_request(dep, req, ret);\n\t\treturn ret;\n\t}\n\n\tdep->flags |= DWC3_EP_BUSY;\n\n\tif (starting) {\n\t\tdep->resource_index = dwc3_gadget_ep_get_transfer_index(dep);\n\t\tWARN_ON_ONCE(!dep->resource_index);\n\t}\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -42,7 +42,7 @@\n \t\tif (req->trb)\n \t\t\tmemset(req->trb, 0, sizeof(struct dwc3_trb));\n \t\tdep->queued_requests--;\n-\t\tdwc3_gadget_giveback(dep, req, ret);\n+\t\tdwc3_gadget_del_and_unmap_request(dep, req, ret);\n \t\treturn ret;\n \t}\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tdwc3_gadget_giveback(dep, req, ret);"
            ],
            "added_lines": [
                "\t\tdwc3_gadget_del_and_unmap_request(dep, req, ret);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-14763",
        "func_name": "torvalds/linux/dwc3_gadget_giveback",
        "description": "In the Linux kernel before 4.16.4, a double-locking error in drivers/usb/dwc3/gadget.c may potentially cause a deadlock with f_hid.",
        "git_url": "https://github.com/torvalds/linux/commit/c91815b596245fd7da349ecc43c8def670d2269e",
        "commit_title": "usb: dwc3: gadget: never call ->complete() from ->ep_queue()",
        "commit_text": " This is a requirement which has always existed but, somehow, wasn't reflected in the documentation and problems weren't found until now when Tuba Yavuz found a possible deadlock happening between dwc3 and f_hid. She described the situation as follows:  spin_lock_irqsave(&hidg->write_spinlock, flags); // first acquire /* we our function has been disabled by host */ if (!hidg->req) { \tfree_ep_req(hidg->in_ep, hidg->req); \tgoto try_again; }  [...]  status = usb_ep_queue(hidg->in_ep, hidg->req, GFP_ATOMIC); => \t[...] \t=> usb_gadget_giveback_request \t\t=> \t\tf_hidg_req_complete \t\t\t=> \t\t\tspin_lock_irqsave(&hidg->write_spinlock, flags); // second acquire  Note that this happens because dwc3 would call ->complete() on a failed usb_ep_queue() due to failed Start Transfer command. This is, anyway, a theoretical situation because dwc3 currently uses \"No Response Update Transfer\" command for Bulk and Interrupt endpoints.  It's still good to make this case impossible to happen even if the \"No Reponse Update Transfer\" command is changed.  Cc: stable <stable@vger.kernel.org>",
        "func_before": "void dwc3_gadget_giveback(struct dwc3_ep *dep, struct dwc3_request *req,\n\t\tint status)\n{\n\tstruct dwc3\t\t\t*dwc = dep->dwc;\n\n\treq->started = false;\n\tlist_del(&req->list);\n\treq->remaining = 0;\n\n\tif (req->request.status == -EINPROGRESS)\n\t\treq->request.status = status;\n\n\tif (req->trb)\n\t\tusb_gadget_unmap_request_by_dev(dwc->sysdev,\n\t\t\t\t\t\t&req->request, req->direction);\n\n\treq->trb = NULL;\n\n\ttrace_dwc3_gadget_giveback(req);\n\n\tspin_unlock(&dwc->lock);\n\tusb_gadget_giveback_request(&dep->endpoint, &req->request);\n\tspin_lock(&dwc->lock);\n\n\tif (dep->number > 1)\n\t\tpm_runtime_put(dwc->dev);\n}",
        "func": "void dwc3_gadget_giveback(struct dwc3_ep *dep, struct dwc3_request *req,\n\t\tint status)\n{\n\tstruct dwc3\t\t\t*dwc = dep->dwc;\n\n\tdwc3_gadget_del_and_unmap_request(dep, req, status);\n\n\tspin_unlock(&dwc->lock);\n\tusb_gadget_giveback_request(&dep->endpoint, &req->request);\n\tspin_lock(&dwc->lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,25 +3,9 @@\n {\n \tstruct dwc3\t\t\t*dwc = dep->dwc;\n \n-\treq->started = false;\n-\tlist_del(&req->list);\n-\treq->remaining = 0;\n-\n-\tif (req->request.status == -EINPROGRESS)\n-\t\treq->request.status = status;\n-\n-\tif (req->trb)\n-\t\tusb_gadget_unmap_request_by_dev(dwc->sysdev,\n-\t\t\t\t\t\t&req->request, req->direction);\n-\n-\treq->trb = NULL;\n-\n-\ttrace_dwc3_gadget_giveback(req);\n+\tdwc3_gadget_del_and_unmap_request(dep, req, status);\n \n \tspin_unlock(&dwc->lock);\n \tusb_gadget_giveback_request(&dep->endpoint, &req->request);\n \tspin_lock(&dwc->lock);\n-\n-\tif (dep->number > 1)\n-\t\tpm_runtime_put(dwc->dev);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\treq->started = false;",
                "\tlist_del(&req->list);",
                "\treq->remaining = 0;",
                "",
                "\tif (req->request.status == -EINPROGRESS)",
                "\t\treq->request.status = status;",
                "",
                "\tif (req->trb)",
                "\t\tusb_gadget_unmap_request_by_dev(dwc->sysdev,",
                "\t\t\t\t\t\t&req->request, req->direction);",
                "",
                "\treq->trb = NULL;",
                "",
                "\ttrace_dwc3_gadget_giveback(req);",
                "",
                "\tif (dep->number > 1)",
                "\t\tpm_runtime_put(dwc->dev);"
            ],
            "added_lines": [
                "\tdwc3_gadget_del_and_unmap_request(dep, req, status);"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-14763",
        "func_name": "torvalds/linux/f_hidg_write",
        "description": "In the Linux kernel before 4.16.4, a double-locking error in drivers/usb/dwc3/gadget.c may potentially cause a deadlock with f_hid.",
        "git_url": "https://github.com/torvalds/linux/commit/072684e8c58d17e853f8e8b9f6d9ce2e58d2b036",
        "commit_title": "USB: gadget: f_hid: fix deadlock in f_hidg_write()",
        "commit_text": " In f_hidg_write() the write_spinlock is acquired before calling usb_ep_queue() which causes a deadlock when dummy_hcd is being used. This is because dummy_queue() callbacks into f_hidg_req_complete() which tries to acquire the same spinlock. This is (part of) the backtrace when the deadlock occurs:    0xffffffffc06b1410 in f_hidg_req_complete   0xffffffffc06a590a in usb_gadget_giveback_request   0xffffffffc06cfff2 in dummy_queue   0xffffffffc06a4b96 in usb_ep_queue   0xffffffffc06b1eb6 in f_hidg_write   0xffffffff8127730b in __vfs_write   0xffffffff812774d1 in vfs_write   0xffffffff81277725 in SYSC_write  Fix this by releasing the write_spinlock before calling usb_ep_queue()  Cc: stable@vger.kernel.org # 4.11+",
        "func_before": "static ssize_t f_hidg_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *offp)\n{\n\tstruct f_hidg *hidg  = file->private_data;\n\tstruct usb_request *req;\n\tunsigned long flags;\n\tssize_t status = -ENOMEM;\n\n\tif (!access_ok(buffer, count))\n\t\treturn -EFAULT;\n\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\n#define WRITE_COND (!hidg->write_pending)\ntry_again:\n\t/* write queue */\n\twhile (!WRITE_COND) {\n\t\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\t\tif (file->f_flags & O_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (wait_event_interruptible_exclusive(\n\t\t\t\thidg->write_queue, WRITE_COND))\n\t\t\treturn -ERESTARTSYS;\n\n\t\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\t}\n\n\thidg->write_pending = 1;\n\treq = hidg->req;\n\tcount  = min_t(unsigned, count, hidg->report_length);\n\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\tstatus = copy_from_user(req->buf, buffer, count);\n\n\tif (status != 0) {\n\t\tERROR(hidg->func.config->cdev,\n\t\t\t\"copy_from_user error\\n\");\n\t\tstatus = -EINVAL;\n\t\tgoto release_write_pending;\n\t}\n\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\n\t/* when our function has been disabled by host */\n\tif (!hidg->req) {\n\t\tfree_ep_req(hidg->in_ep, req);\n\t\t/*\n\t\t * TODO\n\t\t * Should we fail with error here?\n\t\t */\n\t\tgoto try_again;\n\t}\n\n\treq->status   = 0;\n\treq->zero     = 0;\n\treq->length   = count;\n\treq->complete = f_hidg_req_complete;\n\treq->context  = hidg;\n\n\tstatus = usb_ep_queue(hidg->in_ep, req, GFP_ATOMIC);\n\tif (status < 0) {\n\t\tERROR(hidg->func.config->cdev,\n\t\t\t\"usb_ep_queue error on int endpoint %zd\\n\", status);\n\t\tgoto release_write_pending_unlocked;\n\t} else {\n\t\tstatus = count;\n\t}\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\n\treturn status;\nrelease_write_pending:\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\nrelease_write_pending_unlocked:\n\thidg->write_pending = 0;\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\n\twake_up(&hidg->write_queue);\n\n\treturn status;\n}",
        "func": "static ssize_t f_hidg_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *offp)\n{\n\tstruct f_hidg *hidg  = file->private_data;\n\tstruct usb_request *req;\n\tunsigned long flags;\n\tssize_t status = -ENOMEM;\n\n\tif (!access_ok(buffer, count))\n\t\treturn -EFAULT;\n\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\n#define WRITE_COND (!hidg->write_pending)\ntry_again:\n\t/* write queue */\n\twhile (!WRITE_COND) {\n\t\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\t\tif (file->f_flags & O_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (wait_event_interruptible_exclusive(\n\t\t\t\thidg->write_queue, WRITE_COND))\n\t\t\treturn -ERESTARTSYS;\n\n\t\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\t}\n\n\thidg->write_pending = 1;\n\treq = hidg->req;\n\tcount  = min_t(unsigned, count, hidg->report_length);\n\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\tstatus = copy_from_user(req->buf, buffer, count);\n\n\tif (status != 0) {\n\t\tERROR(hidg->func.config->cdev,\n\t\t\t\"copy_from_user error\\n\");\n\t\tstatus = -EINVAL;\n\t\tgoto release_write_pending;\n\t}\n\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\n\t/* when our function has been disabled by host */\n\tif (!hidg->req) {\n\t\tfree_ep_req(hidg->in_ep, req);\n\t\t/*\n\t\t * TODO\n\t\t * Should we fail with error here?\n\t\t */\n\t\tgoto try_again;\n\t}\n\n\treq->status   = 0;\n\treq->zero     = 0;\n\treq->length   = count;\n\treq->complete = f_hidg_req_complete;\n\treq->context  = hidg;\n\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\n\tstatus = usb_ep_queue(hidg->in_ep, req, GFP_ATOMIC);\n\tif (status < 0) {\n\t\tERROR(hidg->func.config->cdev,\n\t\t\t\"usb_ep_queue error on int endpoint %zd\\n\", status);\n\t\tgoto release_write_pending;\n\t} else {\n\t\tstatus = count;\n\t}\n\n\treturn status;\nrelease_write_pending:\n\tspin_lock_irqsave(&hidg->write_spinlock, flags);\n\thidg->write_pending = 0;\n\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n\n\twake_up(&hidg->write_queue);\n\n\treturn status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -58,20 +58,20 @@\n \treq->complete = f_hidg_req_complete;\n \treq->context  = hidg;\n \n+\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n+\n \tstatus = usb_ep_queue(hidg->in_ep, req, GFP_ATOMIC);\n \tif (status < 0) {\n \t\tERROR(hidg->func.config->cdev,\n \t\t\t\"usb_ep_queue error on int endpoint %zd\\n\", status);\n-\t\tgoto release_write_pending_unlocked;\n+\t\tgoto release_write_pending;\n \t} else {\n \t\tstatus = count;\n \t}\n-\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n \n \treturn status;\n release_write_pending:\n \tspin_lock_irqsave(&hidg->write_spinlock, flags);\n-release_write_pending_unlocked:\n \thidg->write_pending = 0;\n \tspin_unlock_irqrestore(&hidg->write_spinlock, flags);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgoto release_write_pending_unlocked;",
                "\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);",
                "release_write_pending_unlocked:"
            ],
            "added_lines": [
                "\tspin_unlock_irqrestore(&hidg->write_spinlock, flags);",
                "",
                "\t\tgoto release_write_pending;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17343",
        "func_name": "xen-project/xen/steal_page",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service or gain privileges by leveraging incorrect use of the HVM physmap concept for PV domains.",
        "git_url": "https://github.com/xen-project/xen/commit/fe21b78ef99a1b505cfb6d3789ede9591609dd70",
        "commit_title": "xen: Make coherent PV IOMMU discipline",
        "commit_text": " In order for a PV domain to set up DMA from a passed-through device to one of its pages, the page must be mapped in the IOMMU.  On the other hand, before a PV page may be used as a \"special\" page type (such as a pagetable or descriptor table), it _must not_ be writable in the IOMMU (otherwise a malicious guest could DMA arbitrary page tables into the memory, bypassing Xen's safety checks); and Xen's current rule is to have such pages not in the IOMMU at all.  At the moment, in order to accomplish this, the code borrows HVM domain's \"physmap\" concept: When a page is assigned to a guest, guess_physmap_add_entry() is called, which for PV guests, will create a writable IOMMU mapping; and when a page is removed, guest_physmap_remove_entry() is called, which will remove the mapping.  Additionally, when a page gains the PGT_writable page type, the page will be added into the IOMMU; and when the page changes away from a PGT_writable type, the page will be removed from the IOMMU.  Unfortunately, borrowing the \"physmap\" concept from HVM domains is problematic.  HVM domains have a lock on their p2m tables, ensuring synchronization between modifications to the p2m; and all hypercall parameters must first be translated through the p2m before being used.  Trying to mix this locked-and-gated approach with PV's lock-free approach leads to several races and inconsistencies:  * A race between a page being assigned and it being put into the   physmap; for example:   - P1: call populate_physmap() { A = allocate_domheap_pages() }   - P2: Guess page A's mfn, and call decrease_reservation(A).  A is owned by the domain,         and so Xen will clear the PGC_allocated bit and free the page   - P1: finishes populate_physmap() { guest_physmap_add_entry() }    Now the domain has a writable IOMMU mapping to a page it no longer owns.  * Pages start out as type PGT_none, but with a writable IOMMU mapping.   If a guest uses a page as a page table without ever having created a   writable mapping, the IOMMU mapping will not be removed; the guest   will have a writable IOMMU mapping to a page it is currently using   as a page table.  * A newly-allocated page can be DMA'd into with no special actions on   the part of the guest; However, if a page is promoted to a   non-writable type, the page must be mapped with a writable type before   DMA'ing to it again, or the transaction will fail.  To fix this, do away with the \"PV physmap\" concept entirely, and replace it with the following IOMMU discipline for PV guests:  - (type == PGT_writable) <=> in iommu (even if type_count == 0)  - Upon a final put_page(), check to see if type is PGT_writable; if so,    iommu_unmap.  In order to achieve that:  - Remove PV IOMMU related code from guest_physmap_*  - Repurpose cleanup_page_cacheattr() into a general   cleanup_page_mappings() function, which will both fix up Xen   mappings for pages with special cache attributes, and also check for   a PGT_writable type and remove pages if appropriate.  - For compatibility with current guests, grab-and-release a   PGT_writable_page type for PV guests in guest_physmap_add_entry().   This will cause most \"normal\" guest pages to start out life with   PGT_writable_page type (and thus an IOMMU mapping), but no type   count (so that they can be used as special cases at will).  Also, note that there is one exception to to the \"PGT_writable => in iommu\" rule: xenheap pages shared with guests may be given a PGT_writable type with one type reference.  This reference prevents the type from changing, which in turn prevents page from gaining an IOMMU mapping in get_page_type().  It's not clear whether this was intentional or not, but it's not something to change in a security update.  This is XSA-288. ",
        "func_before": "int steal_page(\n    struct domain *d, struct page_info *page, unsigned int memflags)\n{\n    unsigned long x, y;\n    bool drop_dom_ref = false;\n    const struct domain *owner;\n    int rc;\n\n    if ( paging_mode_external(d) )\n        return -EOPNOTSUPP;\n\n    /* Grab a reference to make sure the page doesn't change under our feet */\n    rc = -EINVAL;\n    if ( !(owner = page_get_owner_and_reference(page)) )\n        goto fail;\n\n    if ( owner != d || is_xen_heap_page(page) )\n        goto fail_put;\n\n    /*\n     * We require there are exactly two references -- the one we just\n     * took, and PGC_allocated. We temporarily drop both these\n     * references so that the page becomes effectively non-\"live\" for\n     * the domain.\n     */\n    y = page->count_info;\n    do {\n        x = y;\n        if ( (x & (PGC_count_mask|PGC_allocated)) != (2 | PGC_allocated) )\n            goto fail_put;\n        y = cmpxchg(&page->count_info, x, x & ~(PGC_count_mask|PGC_allocated));\n    } while ( y != x );\n\n    /*\n     * NB this is safe even if the page ends up being given back to\n     * the domain, because the count is zero: subsequent mappings will\n     * cause the cache attributes to be re-instated inside\n     * get_page_from_l1e().\n     */\n    if ( (rc = cleanup_page_cacheattr(page)) )\n    {\n        /*\n         * Couldn't fixup Xen's mappings; put things the way we found\n         * it and return an error\n         */\n        page->count_info |= PGC_allocated | 1;\n        goto fail;\n    }\n\n    /*\n     * With the reference count now zero, nobody can grab references\n     * to do anything else with the page.  Return the page to a state\n     * that it might be upon return from alloc_domheap_pages with\n     * MEMF_no_owner set.\n     */\n    spin_lock(&d->page_alloc_lock);\n\n    BUG_ON(page->u.inuse.type_info & (PGT_count_mask | PGT_locked |\n                                      PGT_pinned));\n    page->u.inuse.type_info = 0;\n    page_set_owner(page, NULL);\n    page_list_del(page, &d->page_list);\n\n    /* Unlink from original owner. */\n    if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )\n        drop_dom_ref = true;\n\n    spin_unlock(&d->page_alloc_lock);\n\n    if ( unlikely(drop_dom_ref) )\n        put_domain(d);\n\n    return 0;\n\n fail_put:\n    put_page(page);\n fail:\n    gdprintk(XENLOG_WARNING, \"Bad steal mfn %\" PRI_mfn\n             \" from d%d (owner d%d) caf=%08lx taf=%\" PRtype_info \"\\n\",\n             mfn_x(page_to_mfn(page)), d->domain_id,\n             owner ? owner->domain_id : DOMID_INVALID,\n             page->count_info, page->u.inuse.type_info);\n    return rc;\n}",
        "func": "int steal_page(\n    struct domain *d, struct page_info *page, unsigned int memflags)\n{\n    unsigned long x, y;\n    bool drop_dom_ref = false;\n    const struct domain *owner;\n    int rc;\n\n    if ( paging_mode_external(d) )\n        return -EOPNOTSUPP;\n\n    /* Grab a reference to make sure the page doesn't change under our feet */\n    rc = -EINVAL;\n    if ( !(owner = page_get_owner_and_reference(page)) )\n        goto fail;\n\n    if ( owner != d || is_xen_heap_page(page) )\n        goto fail_put;\n\n    /*\n     * We require there are exactly two references -- the one we just\n     * took, and PGC_allocated. We temporarily drop both these\n     * references so that the page becomes effectively non-\"live\" for\n     * the domain.\n     */\n    y = page->count_info;\n    do {\n        x = y;\n        if ( (x & (PGC_count_mask|PGC_allocated)) != (2 | PGC_allocated) )\n            goto fail_put;\n        y = cmpxchg(&page->count_info, x, x & ~(PGC_count_mask|PGC_allocated));\n    } while ( y != x );\n\n    /*\n     * NB this is safe even if the page ends up being given back to\n     * the domain, because the count is zero: subsequent mappings will\n     * cause the cache attributes to be re-instated inside\n     * get_page_from_l1e(), or the page to be added back to the IOMMU\n     * upon the type changing to PGT_writeable, as appropriate.\n     */\n    if ( (rc = cleanup_page_mappings(page)) )\n    {\n        /*\n         * Couldn't fixup Xen's mappings; put things the way we found\n         * it and return an error\n         */\n        page->count_info |= PGC_allocated | 1;\n        goto fail;\n    }\n\n    /*\n     * With the reference count now zero, nobody can grab references\n     * to do anything else with the page.  Return the page to a state\n     * that it might be upon return from alloc_domheap_pages with\n     * MEMF_no_owner set.\n     */\n    spin_lock(&d->page_alloc_lock);\n\n    BUG_ON(page->u.inuse.type_info & (PGT_count_mask | PGT_locked |\n                                      PGT_pinned));\n    page->u.inuse.type_info = 0;\n    page_set_owner(page, NULL);\n    page_list_del(page, &d->page_list);\n\n    /* Unlink from original owner. */\n    if ( !(memflags & MEMF_no_refcount) && !domain_adjust_tot_pages(d, -1) )\n        drop_dom_ref = true;\n\n    spin_unlock(&d->page_alloc_lock);\n\n    if ( unlikely(drop_dom_ref) )\n        put_domain(d);\n\n    return 0;\n\n fail_put:\n    put_page(page);\n fail:\n    gdprintk(XENLOG_WARNING, \"Bad steal mfn %\" PRI_mfn\n             \" from d%d (owner d%d) caf=%08lx taf=%\" PRtype_info \"\\n\",\n             mfn_x(page_to_mfn(page)), d->domain_id,\n             owner ? owner->domain_id : DOMID_INVALID,\n             page->count_info, page->u.inuse.type_info);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -35,9 +35,10 @@\n      * NB this is safe even if the page ends up being given back to\n      * the domain, because the count is zero: subsequent mappings will\n      * cause the cache attributes to be re-instated inside\n-     * get_page_from_l1e().\n+     * get_page_from_l1e(), or the page to be added back to the IOMMU\n+     * upon the type changing to PGT_writeable, as appropriate.\n      */\n-    if ( (rc = cleanup_page_cacheattr(page)) )\n+    if ( (rc = cleanup_page_mappings(page)) )\n     {\n         /*\n          * Couldn't fixup Xen's mappings; put things the way we found",
        "diff_line_info": {
            "deleted_lines": [
                "     * get_page_from_l1e().",
                "    if ( (rc = cleanup_page_cacheattr(page)) )"
            ],
            "added_lines": [
                "     * get_page_from_l1e(), or the page to be added back to the IOMMU",
                "     * upon the type changing to PGT_writeable, as appropriate.",
                "    if ( (rc = cleanup_page_mappings(page)) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17343",
        "func_name": "xen-project/xen/put_page",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service or gain privileges by leveraging incorrect use of the HVM physmap concept for PV domains.",
        "git_url": "https://github.com/xen-project/xen/commit/fe21b78ef99a1b505cfb6d3789ede9591609dd70",
        "commit_title": "xen: Make coherent PV IOMMU discipline",
        "commit_text": " In order for a PV domain to set up DMA from a passed-through device to one of its pages, the page must be mapped in the IOMMU.  On the other hand, before a PV page may be used as a \"special\" page type (such as a pagetable or descriptor table), it _must not_ be writable in the IOMMU (otherwise a malicious guest could DMA arbitrary page tables into the memory, bypassing Xen's safety checks); and Xen's current rule is to have such pages not in the IOMMU at all.  At the moment, in order to accomplish this, the code borrows HVM domain's \"physmap\" concept: When a page is assigned to a guest, guess_physmap_add_entry() is called, which for PV guests, will create a writable IOMMU mapping; and when a page is removed, guest_physmap_remove_entry() is called, which will remove the mapping.  Additionally, when a page gains the PGT_writable page type, the page will be added into the IOMMU; and when the page changes away from a PGT_writable type, the page will be removed from the IOMMU.  Unfortunately, borrowing the \"physmap\" concept from HVM domains is problematic.  HVM domains have a lock on their p2m tables, ensuring synchronization between modifications to the p2m; and all hypercall parameters must first be translated through the p2m before being used.  Trying to mix this locked-and-gated approach with PV's lock-free approach leads to several races and inconsistencies:  * A race between a page being assigned and it being put into the   physmap; for example:   - P1: call populate_physmap() { A = allocate_domheap_pages() }   - P2: Guess page A's mfn, and call decrease_reservation(A).  A is owned by the domain,         and so Xen will clear the PGC_allocated bit and free the page   - P1: finishes populate_physmap() { guest_physmap_add_entry() }    Now the domain has a writable IOMMU mapping to a page it no longer owns.  * Pages start out as type PGT_none, but with a writable IOMMU mapping.   If a guest uses a page as a page table without ever having created a   writable mapping, the IOMMU mapping will not be removed; the guest   will have a writable IOMMU mapping to a page it is currently using   as a page table.  * A newly-allocated page can be DMA'd into with no special actions on   the part of the guest; However, if a page is promoted to a   non-writable type, the page must be mapped with a writable type before   DMA'ing to it again, or the transaction will fail.  To fix this, do away with the \"PV physmap\" concept entirely, and replace it with the following IOMMU discipline for PV guests:  - (type == PGT_writable) <=> in iommu (even if type_count == 0)  - Upon a final put_page(), check to see if type is PGT_writable; if so,    iommu_unmap.  In order to achieve that:  - Remove PV IOMMU related code from guest_physmap_*  - Repurpose cleanup_page_cacheattr() into a general   cleanup_page_mappings() function, which will both fix up Xen   mappings for pages with special cache attributes, and also check for   a PGT_writable type and remove pages if appropriate.  - For compatibility with current guests, grab-and-release a   PGT_writable_page type for PV guests in guest_physmap_add_entry().   This will cause most \"normal\" guest pages to start out life with   PGT_writable_page type (and thus an IOMMU mapping), but no type   count (so that they can be used as special cases at will).  Also, note that there is one exception to to the \"PGT_writable => in iommu\" rule: xenheap pages shared with guests may be given a PGT_writable type with one type reference.  This reference prevents the type from changing, which in turn prevents page from gaining an IOMMU mapping in get_page_type().  It's not clear whether this was intentional or not, but it's not something to change in a security update.  This is XSA-288. ",
        "func_before": "void put_page(struct page_info *page)\n{\n    unsigned long nx, x, y = page->count_info;\n\n    do {\n        ASSERT((y & PGC_count_mask) != 0);\n        x  = y;\n        nx = x - 1;\n    }\n    while ( unlikely((y = cmpxchg(&page->count_info, x, nx)) != x) );\n\n    if ( unlikely((nx & PGC_count_mask) == 0) )\n    {\n        if ( cleanup_page_cacheattr(page) == 0 )\n            free_domheap_page(page);\n        else\n            gdprintk(XENLOG_WARNING,\n                     \"Leaking mfn %\" PRI_mfn \"\\n\", mfn_x(page_to_mfn(page)));\n    }\n}",
        "func": "void put_page(struct page_info *page)\n{\n    unsigned long nx, x, y = page->count_info;\n\n    do {\n        ASSERT((y & PGC_count_mask) != 0);\n        x  = y;\n        nx = x - 1;\n    }\n    while ( unlikely((y = cmpxchg(&page->count_info, x, nx)) != x) );\n\n    if ( unlikely((nx & PGC_count_mask) == 0) )\n    {\n        if ( !cleanup_page_mappings(page) )\n            free_domheap_page(page);\n        else\n            gdprintk(XENLOG_WARNING,\n                     \"Leaking mfn %\" PRI_mfn \"\\n\", mfn_x(page_to_mfn(page)));\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,7 +11,7 @@\n \n     if ( unlikely((nx & PGC_count_mask) == 0) )\n     {\n-        if ( cleanup_page_cacheattr(page) == 0 )\n+        if ( !cleanup_page_mappings(page) )\n             free_domheap_page(page);\n         else\n             gdprintk(XENLOG_WARNING,",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( cleanup_page_cacheattr(page) == 0 )"
            ],
            "added_lines": [
                "        if ( !cleanup_page_mappings(page) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17343",
        "func_name": "xen-project/xen/p2m_remove_page",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service or gain privileges by leveraging incorrect use of the HVM physmap concept for PV domains.",
        "git_url": "https://github.com/xen-project/xen/commit/fe21b78ef99a1b505cfb6d3789ede9591609dd70",
        "commit_title": "xen: Make coherent PV IOMMU discipline",
        "commit_text": " In order for a PV domain to set up DMA from a passed-through device to one of its pages, the page must be mapped in the IOMMU.  On the other hand, before a PV page may be used as a \"special\" page type (such as a pagetable or descriptor table), it _must not_ be writable in the IOMMU (otherwise a malicious guest could DMA arbitrary page tables into the memory, bypassing Xen's safety checks); and Xen's current rule is to have such pages not in the IOMMU at all.  At the moment, in order to accomplish this, the code borrows HVM domain's \"physmap\" concept: When a page is assigned to a guest, guess_physmap_add_entry() is called, which for PV guests, will create a writable IOMMU mapping; and when a page is removed, guest_physmap_remove_entry() is called, which will remove the mapping.  Additionally, when a page gains the PGT_writable page type, the page will be added into the IOMMU; and when the page changes away from a PGT_writable type, the page will be removed from the IOMMU.  Unfortunately, borrowing the \"physmap\" concept from HVM domains is problematic.  HVM domains have a lock on their p2m tables, ensuring synchronization between modifications to the p2m; and all hypercall parameters must first be translated through the p2m before being used.  Trying to mix this locked-and-gated approach with PV's lock-free approach leads to several races and inconsistencies:  * A race between a page being assigned and it being put into the   physmap; for example:   - P1: call populate_physmap() { A = allocate_domheap_pages() }   - P2: Guess page A's mfn, and call decrease_reservation(A).  A is owned by the domain,         and so Xen will clear the PGC_allocated bit and free the page   - P1: finishes populate_physmap() { guest_physmap_add_entry() }    Now the domain has a writable IOMMU mapping to a page it no longer owns.  * Pages start out as type PGT_none, but with a writable IOMMU mapping.   If a guest uses a page as a page table without ever having created a   writable mapping, the IOMMU mapping will not be removed; the guest   will have a writable IOMMU mapping to a page it is currently using   as a page table.  * A newly-allocated page can be DMA'd into with no special actions on   the part of the guest; However, if a page is promoted to a   non-writable type, the page must be mapped with a writable type before   DMA'ing to it again, or the transaction will fail.  To fix this, do away with the \"PV physmap\" concept entirely, and replace it with the following IOMMU discipline for PV guests:  - (type == PGT_writable) <=> in iommu (even if type_count == 0)  - Upon a final put_page(), check to see if type is PGT_writable; if so,    iommu_unmap.  In order to achieve that:  - Remove PV IOMMU related code from guest_physmap_*  - Repurpose cleanup_page_cacheattr() into a general   cleanup_page_mappings() function, which will both fix up Xen   mappings for pages with special cache attributes, and also check for   a PGT_writable type and remove pages if appropriate.  - For compatibility with current guests, grab-and-release a   PGT_writable_page type for PV guests in guest_physmap_add_entry().   This will cause most \"normal\" guest pages to start out life with   PGT_writable_page type (and thus an IOMMU mapping), but no type   count (so that they can be used as special cases at will).  Also, note that there is one exception to to the \"PGT_writable => in iommu\" rule: xenheap pages shared with guests may be given a PGT_writable type with one type reference.  This reference prevents the type from changing, which in turn prevents page from gaining an IOMMU mapping in get_page_type().  It's not clear whether this was intentional or not, but it's not something to change in a security update.  This is XSA-288. ",
        "func_before": "static int\np2m_remove_page(struct p2m_domain *p2m, unsigned long gfn_l, unsigned long mfn,\n                unsigned int page_order)\n{\n    unsigned long i;\n    gfn_t gfn = _gfn(gfn_l);\n    mfn_t mfn_return;\n    p2m_type_t t;\n    p2m_access_t a;\n\n    if ( !paging_mode_translate(p2m->domain) )\n        return need_iommu_pt_sync(p2m->domain) ?\n            iommu_legacy_unmap(p2m->domain, _dfn(mfn), page_order) : 0;\n\n    ASSERT(gfn_locked_by_me(p2m, gfn));\n    P2M_DEBUG(\"removing gfn=%#lx mfn=%#lx\\n\", gfn_l, mfn);\n\n    if ( mfn_valid(_mfn(mfn)) )\n    {\n        for ( i = 0; i < (1UL << page_order); i++ )\n        {\n            mfn_return = p2m->get_entry(p2m, gfn_add(gfn, i), &t, &a, 0,\n                                        NULL, NULL);\n            if ( !p2m_is_grant(t) && !p2m_is_shared(t) && !p2m_is_foreign(t) )\n                set_gpfn_from_mfn(mfn+i, INVALID_M2P_ENTRY);\n            ASSERT( !p2m_is_valid(t) || mfn + i == mfn_x(mfn_return) );\n        }\n    }\n    return p2m_set_entry(p2m, gfn, INVALID_MFN, page_order, p2m_invalid,\n                         p2m->default_access);\n}",
        "func": "static int\np2m_remove_page(struct p2m_domain *p2m, unsigned long gfn_l, unsigned long mfn,\n                unsigned int page_order)\n{\n    unsigned long i;\n    gfn_t gfn = _gfn(gfn_l);\n    mfn_t mfn_return;\n    p2m_type_t t;\n    p2m_access_t a;\n\n    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */\n    if ( !paging_mode_translate(p2m->domain) )\n        return 0;\n\n    ASSERT(gfn_locked_by_me(p2m, gfn));\n    P2M_DEBUG(\"removing gfn=%#lx mfn=%#lx\\n\", gfn_l, mfn);\n\n    if ( mfn_valid(_mfn(mfn)) )\n    {\n        for ( i = 0; i < (1UL << page_order); i++ )\n        {\n            mfn_return = p2m->get_entry(p2m, gfn_add(gfn, i), &t, &a, 0,\n                                        NULL, NULL);\n            if ( !p2m_is_grant(t) && !p2m_is_shared(t) && !p2m_is_foreign(t) )\n                set_gpfn_from_mfn(mfn+i, INVALID_M2P_ENTRY);\n            ASSERT( !p2m_is_valid(t) || mfn + i == mfn_x(mfn_return) );\n        }\n    }\n    return p2m_set_entry(p2m, gfn, INVALID_MFN, page_order, p2m_invalid,\n                         p2m->default_access);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,9 +8,9 @@\n     p2m_type_t t;\n     p2m_access_t a;\n \n+    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */\n     if ( !paging_mode_translate(p2m->domain) )\n-        return need_iommu_pt_sync(p2m->domain) ?\n-            iommu_legacy_unmap(p2m->domain, _dfn(mfn), page_order) : 0;\n+        return 0;\n \n     ASSERT(gfn_locked_by_me(p2m, gfn));\n     P2M_DEBUG(\"removing gfn=%#lx mfn=%#lx\\n\", gfn_l, mfn);",
        "diff_line_info": {
            "deleted_lines": [
                "        return need_iommu_pt_sync(p2m->domain) ?",
                "            iommu_legacy_unmap(p2m->domain, _dfn(mfn), page_order) : 0;"
            ],
            "added_lines": [
                "    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */",
                "        return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2019-17343",
        "func_name": "xen-project/xen/guest_physmap_add_entry",
        "description": "An issue was discovered in Xen through 4.11.x allowing x86 PV guest OS users to cause a denial of service or gain privileges by leveraging incorrect use of the HVM physmap concept for PV domains.",
        "git_url": "https://github.com/xen-project/xen/commit/fe21b78ef99a1b505cfb6d3789ede9591609dd70",
        "commit_title": "xen: Make coherent PV IOMMU discipline",
        "commit_text": " In order for a PV domain to set up DMA from a passed-through device to one of its pages, the page must be mapped in the IOMMU.  On the other hand, before a PV page may be used as a \"special\" page type (such as a pagetable or descriptor table), it _must not_ be writable in the IOMMU (otherwise a malicious guest could DMA arbitrary page tables into the memory, bypassing Xen's safety checks); and Xen's current rule is to have such pages not in the IOMMU at all.  At the moment, in order to accomplish this, the code borrows HVM domain's \"physmap\" concept: When a page is assigned to a guest, guess_physmap_add_entry() is called, which for PV guests, will create a writable IOMMU mapping; and when a page is removed, guest_physmap_remove_entry() is called, which will remove the mapping.  Additionally, when a page gains the PGT_writable page type, the page will be added into the IOMMU; and when the page changes away from a PGT_writable type, the page will be removed from the IOMMU.  Unfortunately, borrowing the \"physmap\" concept from HVM domains is problematic.  HVM domains have a lock on their p2m tables, ensuring synchronization between modifications to the p2m; and all hypercall parameters must first be translated through the p2m before being used.  Trying to mix this locked-and-gated approach with PV's lock-free approach leads to several races and inconsistencies:  * A race between a page being assigned and it being put into the   physmap; for example:   - P1: call populate_physmap() { A = allocate_domheap_pages() }   - P2: Guess page A's mfn, and call decrease_reservation(A).  A is owned by the domain,         and so Xen will clear the PGC_allocated bit and free the page   - P1: finishes populate_physmap() { guest_physmap_add_entry() }    Now the domain has a writable IOMMU mapping to a page it no longer owns.  * Pages start out as type PGT_none, but with a writable IOMMU mapping.   If a guest uses a page as a page table without ever having created a   writable mapping, the IOMMU mapping will not be removed; the guest   will have a writable IOMMU mapping to a page it is currently using   as a page table.  * A newly-allocated page can be DMA'd into with no special actions on   the part of the guest; However, if a page is promoted to a   non-writable type, the page must be mapped with a writable type before   DMA'ing to it again, or the transaction will fail.  To fix this, do away with the \"PV physmap\" concept entirely, and replace it with the following IOMMU discipline for PV guests:  - (type == PGT_writable) <=> in iommu (even if type_count == 0)  - Upon a final put_page(), check to see if type is PGT_writable; if so,    iommu_unmap.  In order to achieve that:  - Remove PV IOMMU related code from guest_physmap_*  - Repurpose cleanup_page_cacheattr() into a general   cleanup_page_mappings() function, which will both fix up Xen   mappings for pages with special cache attributes, and also check for   a PGT_writable type and remove pages if appropriate.  - For compatibility with current guests, grab-and-release a   PGT_writable_page type for PV guests in guest_physmap_add_entry().   This will cause most \"normal\" guest pages to start out life with   PGT_writable_page type (and thus an IOMMU mapping), but no type   count (so that they can be used as special cases at will).  Also, note that there is one exception to to the \"PGT_writable => in iommu\" rule: xenheap pages shared with guests may be given a PGT_writable type with one type reference.  This reference prevents the type from changing, which in turn prevents page from gaining an IOMMU mapping in get_page_type().  It's not clear whether this was intentional or not, but it's not something to change in a security update.  This is XSA-288. ",
        "func_before": "int\nguest_physmap_add_entry(struct domain *d, gfn_t gfn, mfn_t mfn,\n                        unsigned int page_order, p2m_type_t t)\n{\n    struct p2m_domain *p2m = p2m_get_hostp2m(d);\n    unsigned long i;\n    gfn_t ogfn;\n    p2m_type_t ot;\n    p2m_access_t a;\n    mfn_t omfn;\n    int pod_count = 0;\n    int rc = 0;\n\n    if ( !paging_mode_translate(d) )\n        return (need_iommu_pt_sync(d) && t == p2m_ram_rw) ?\n            iommu_legacy_map(d, _dfn(mfn_x(mfn)), mfn, page_order,\n                             IOMMUF_readable | IOMMUF_writable) : 0;\n\n    /* foreign pages are added thru p2m_add_foreign */\n    if ( p2m_is_foreign(t) )\n        return -EINVAL;\n\n    p2m_lock(p2m);\n\n    P2M_DEBUG(\"adding gfn=%#lx mfn=%#lx\\n\", gfn_x(gfn), mfn_x(mfn));\n\n    /* First, remove m->p mappings for existing p->m mappings */\n    for ( i = 0; i < (1UL << page_order); i++ )\n    {\n        omfn = p2m->get_entry(p2m, gfn_add(gfn, i), &ot,\n                              &a, 0, NULL, NULL);\n        if ( p2m_is_shared(ot) )\n        {\n            /* Do an unshare to cleanly take care of all corner \n             * cases. */\n            int rc;\n            rc = mem_sharing_unshare_page(p2m->domain,\n                                          gfn_x(gfn_add(gfn, i)), 0);\n            if ( rc )\n            {\n                p2m_unlock(p2m);\n                /* NOTE: Should a guest domain bring this upon itself,\n                 * there is not a whole lot we can do. We are buried\n                 * deep in locks from most code paths by now. So, fail\n                 * the call and don't try to sleep on a wait queue\n                 * while placing the mem event.\n                 *\n                 * However, all current (changeset 3432abcf9380) code\n                 * paths avoid this unsavoury situation. For now.\n                 *\n                 * Foreign domains are okay to place an event as they \n                 * won't go to sleep. */\n                (void)mem_sharing_notify_enomem(p2m->domain,\n                                                gfn_x(gfn_add(gfn, i)), false);\n                return rc;\n            }\n            omfn = p2m->get_entry(p2m, gfn_add(gfn, i),\n                                  &ot, &a, 0, NULL, NULL);\n            ASSERT(!p2m_is_shared(ot));\n        }\n        if ( p2m_is_grant(ot) || p2m_is_foreign(ot) )\n        {\n            /* Really shouldn't be unmapping grant/foreign maps this way */\n            domain_crash(d);\n            p2m_unlock(p2m);\n            \n            return -EINVAL;\n        }\n        else if ( p2m_is_ram(ot) && !p2m_is_paged(ot) )\n        {\n            ASSERT(mfn_valid(omfn));\n            set_gpfn_from_mfn(mfn_x(omfn), INVALID_M2P_ENTRY);\n        }\n        else if ( ot == p2m_populate_on_demand )\n        {\n            /* Count how man PoD entries we'll be replacing if successful */\n            pod_count++;\n        }\n        else if ( p2m_is_paging(ot) && (ot != p2m_ram_paging_out) )\n        {\n            /* We're plugging a hole in the physmap where a paged out page was */\n            atomic_dec(&d->paged_pages);\n        }\n    }\n\n    /* Then, look for m->p mappings for this range and deal with them */\n    for ( i = 0; i < (1UL << page_order); i++ )\n    {\n        if ( page_get_owner(mfn_to_page(mfn_add(mfn, i))) == dom_cow )\n        {\n            /* This is no way to add a shared page to your physmap! */\n            gdprintk(XENLOG_ERR, \"Adding shared mfn %lx directly to dom%d physmap not allowed.\\n\",\n                     mfn_x(mfn_add(mfn, i)), d->domain_id);\n            p2m_unlock(p2m);\n            return -EINVAL;\n        }\n        if ( page_get_owner(mfn_to_page(mfn_add(mfn, i))) != d )\n            continue;\n        ogfn = _gfn(mfn_to_gfn(d, mfn_add(mfn, i)));\n        if ( !gfn_eq(ogfn, _gfn(INVALID_M2P_ENTRY)) &&\n             !gfn_eq(ogfn, gfn_add(gfn, i)) )\n        {\n            /* This machine frame is already mapped at another physical\n             * address */\n            P2M_DEBUG(\"aliased! mfn=%#lx, old gfn=%#lx, new gfn=%#lx\\n\",\n                      mfn_x(mfn_add(mfn, i)), gfn_x(ogfn),\n                      gfn_x(gfn_add(gfn, i)));\n            omfn = p2m->get_entry(p2m, ogfn, &ot, &a, 0, NULL, NULL);\n            if ( p2m_is_ram(ot) && !p2m_is_paged(ot) )\n            {\n                ASSERT(mfn_valid(omfn));\n                P2M_DEBUG(\"old gfn=%#lx -> mfn %#lx\\n\",\n                          gfn_x(ogfn) , mfn_x(omfn));\n                if ( mfn_eq(omfn, mfn_add(mfn, i)) )\n                    p2m_remove_page(p2m, gfn_x(ogfn), mfn_x(mfn_add(mfn, i)),\n                                    0);\n            }\n        }\n    }\n\n    /* Now, actually do the two-way mapping */\n    if ( mfn_valid(mfn) )\n    {\n        rc = p2m_set_entry(p2m, gfn, mfn, page_order, t,\n                           p2m->default_access);\n        if ( rc )\n            goto out; /* Failed to update p2m, bail without updating m2p. */\n\n        if ( !p2m_is_grant(t) )\n        {\n            for ( i = 0; i < (1UL << page_order); i++ )\n                set_gpfn_from_mfn(mfn_x(mfn_add(mfn, i)),\n                                  gfn_x(gfn_add(gfn, i)));\n        }\n    }\n    else\n    {\n        gdprintk(XENLOG_WARNING, \"Adding bad mfn to p2m map (%#lx -> %#lx)\\n\",\n                 gfn_x(gfn), mfn_x(mfn));\n        rc = p2m_set_entry(p2m, gfn, INVALID_MFN, page_order,\n                           p2m_invalid, p2m->default_access);\n#ifdef CONFIG_HVM\n        if ( rc == 0 )\n        {\n            pod_lock(p2m);\n            p2m->pod.entry_count -= pod_count;\n            BUG_ON(p2m->pod.entry_count < 0);\n            pod_unlock(p2m);\n        }\n#endif\n    }\n\nout:\n    p2m_unlock(p2m);\n\n    return rc;\n}",
        "func": "int\nguest_physmap_add_entry(struct domain *d, gfn_t gfn, mfn_t mfn,\n                        unsigned int page_order, p2m_type_t t)\n{\n    struct p2m_domain *p2m = p2m_get_hostp2m(d);\n    unsigned long i;\n    gfn_t ogfn;\n    p2m_type_t ot;\n    p2m_access_t a;\n    mfn_t omfn;\n    int pod_count = 0;\n    int rc = 0;\n\n    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */\n    if ( !paging_mode_translate(d) )\n    {\n        struct page_info *page = mfn_to_page(mfn);\n\n        /*\n         * Our interface for PV guests wrt IOMMU entries hasn't been very\n         * clear; but historically, pages have started out with IOMMU mappings,\n         * and only lose them when changed to a different page type.\n         *\n         * Retain this property by grabbing a writable type ref and then\n         * dropping it immediately.  The result will be pages that have a\n         * writable type (and an IOMMU entry), but a count of 0 (such that\n         * any guest-requested type changes succeed and remove the IOMMU\n         * entry).\n         */\n        if ( !need_iommu_pt_sync(d) || t != p2m_ram_rw )\n            return 0;\n\n        for ( i = 0; i < (1UL << page_order); ++i, ++page )\n        {\n            if ( get_page_and_type(page, d, PGT_writable_page) )\n                put_page_and_type(page);\n            else\n                return -EINVAL;\n        }\n\n        return 0;\n    }\n\n    /* foreign pages are added thru p2m_add_foreign */\n    if ( p2m_is_foreign(t) )\n        return -EINVAL;\n\n    p2m_lock(p2m);\n\n    P2M_DEBUG(\"adding gfn=%#lx mfn=%#lx\\n\", gfn_x(gfn), mfn_x(mfn));\n\n    /* First, remove m->p mappings for existing p->m mappings */\n    for ( i = 0; i < (1UL << page_order); i++ )\n    {\n        omfn = p2m->get_entry(p2m, gfn_add(gfn, i), &ot,\n                              &a, 0, NULL, NULL);\n        if ( p2m_is_shared(ot) )\n        {\n            /* Do an unshare to cleanly take care of all corner \n             * cases. */\n            int rc;\n            rc = mem_sharing_unshare_page(p2m->domain,\n                                          gfn_x(gfn_add(gfn, i)), 0);\n            if ( rc )\n            {\n                p2m_unlock(p2m);\n                /* NOTE: Should a guest domain bring this upon itself,\n                 * there is not a whole lot we can do. We are buried\n                 * deep in locks from most code paths by now. So, fail\n                 * the call and don't try to sleep on a wait queue\n                 * while placing the mem event.\n                 *\n                 * However, all current (changeset 3432abcf9380) code\n                 * paths avoid this unsavoury situation. For now.\n                 *\n                 * Foreign domains are okay to place an event as they \n                 * won't go to sleep. */\n                (void)mem_sharing_notify_enomem(p2m->domain,\n                                                gfn_x(gfn_add(gfn, i)), false);\n                return rc;\n            }\n            omfn = p2m->get_entry(p2m, gfn_add(gfn, i),\n                                  &ot, &a, 0, NULL, NULL);\n            ASSERT(!p2m_is_shared(ot));\n        }\n        if ( p2m_is_grant(ot) || p2m_is_foreign(ot) )\n        {\n            /* Really shouldn't be unmapping grant/foreign maps this way */\n            domain_crash(d);\n            p2m_unlock(p2m);\n            \n            return -EINVAL;\n        }\n        else if ( p2m_is_ram(ot) && !p2m_is_paged(ot) )\n        {\n            ASSERT(mfn_valid(omfn));\n            set_gpfn_from_mfn(mfn_x(omfn), INVALID_M2P_ENTRY);\n        }\n        else if ( ot == p2m_populate_on_demand )\n        {\n            /* Count how man PoD entries we'll be replacing if successful */\n            pod_count++;\n        }\n        else if ( p2m_is_paging(ot) && (ot != p2m_ram_paging_out) )\n        {\n            /* We're plugging a hole in the physmap where a paged out page was */\n            atomic_dec(&d->paged_pages);\n        }\n    }\n\n    /* Then, look for m->p mappings for this range and deal with them */\n    for ( i = 0; i < (1UL << page_order); i++ )\n    {\n        if ( page_get_owner(mfn_to_page(mfn_add(mfn, i))) == dom_cow )\n        {\n            /* This is no way to add a shared page to your physmap! */\n            gdprintk(XENLOG_ERR, \"Adding shared mfn %lx directly to dom%d physmap not allowed.\\n\",\n                     mfn_x(mfn_add(mfn, i)), d->domain_id);\n            p2m_unlock(p2m);\n            return -EINVAL;\n        }\n        if ( page_get_owner(mfn_to_page(mfn_add(mfn, i))) != d )\n            continue;\n        ogfn = _gfn(mfn_to_gfn(d, mfn_add(mfn, i)));\n        if ( !gfn_eq(ogfn, _gfn(INVALID_M2P_ENTRY)) &&\n             !gfn_eq(ogfn, gfn_add(gfn, i)) )\n        {\n            /* This machine frame is already mapped at another physical\n             * address */\n            P2M_DEBUG(\"aliased! mfn=%#lx, old gfn=%#lx, new gfn=%#lx\\n\",\n                      mfn_x(mfn_add(mfn, i)), gfn_x(ogfn),\n                      gfn_x(gfn_add(gfn, i)));\n            omfn = p2m->get_entry(p2m, ogfn, &ot, &a, 0, NULL, NULL);\n            if ( p2m_is_ram(ot) && !p2m_is_paged(ot) )\n            {\n                ASSERT(mfn_valid(omfn));\n                P2M_DEBUG(\"old gfn=%#lx -> mfn %#lx\\n\",\n                          gfn_x(ogfn) , mfn_x(omfn));\n                if ( mfn_eq(omfn, mfn_add(mfn, i)) )\n                    p2m_remove_page(p2m, gfn_x(ogfn), mfn_x(mfn_add(mfn, i)),\n                                    0);\n            }\n        }\n    }\n\n    /* Now, actually do the two-way mapping */\n    if ( mfn_valid(mfn) )\n    {\n        rc = p2m_set_entry(p2m, gfn, mfn, page_order, t,\n                           p2m->default_access);\n        if ( rc )\n            goto out; /* Failed to update p2m, bail without updating m2p. */\n\n        if ( !p2m_is_grant(t) )\n        {\n            for ( i = 0; i < (1UL << page_order); i++ )\n                set_gpfn_from_mfn(mfn_x(mfn_add(mfn, i)),\n                                  gfn_x(gfn_add(gfn, i)));\n        }\n    }\n    else\n    {\n        gdprintk(XENLOG_WARNING, \"Adding bad mfn to p2m map (%#lx -> %#lx)\\n\",\n                 gfn_x(gfn), mfn_x(mfn));\n        rc = p2m_set_entry(p2m, gfn, INVALID_MFN, page_order,\n                           p2m_invalid, p2m->default_access);\n#ifdef CONFIG_HVM\n        if ( rc == 0 )\n        {\n            pod_lock(p2m);\n            p2m->pod.entry_count -= pod_count;\n            BUG_ON(p2m->pod.entry_count < 0);\n            pod_unlock(p2m);\n        }\n#endif\n    }\n\nout:\n    p2m_unlock(p2m);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,10 +11,35 @@\n     int pod_count = 0;\n     int rc = 0;\n \n+    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */\n     if ( !paging_mode_translate(d) )\n-        return (need_iommu_pt_sync(d) && t == p2m_ram_rw) ?\n-            iommu_legacy_map(d, _dfn(mfn_x(mfn)), mfn, page_order,\n-                             IOMMUF_readable | IOMMUF_writable) : 0;\n+    {\n+        struct page_info *page = mfn_to_page(mfn);\n+\n+        /*\n+         * Our interface for PV guests wrt IOMMU entries hasn't been very\n+         * clear; but historically, pages have started out with IOMMU mappings,\n+         * and only lose them when changed to a different page type.\n+         *\n+         * Retain this property by grabbing a writable type ref and then\n+         * dropping it immediately.  The result will be pages that have a\n+         * writable type (and an IOMMU entry), but a count of 0 (such that\n+         * any guest-requested type changes succeed and remove the IOMMU\n+         * entry).\n+         */\n+        if ( !need_iommu_pt_sync(d) || t != p2m_ram_rw )\n+            return 0;\n+\n+        for ( i = 0; i < (1UL << page_order); ++i, ++page )\n+        {\n+            if ( get_page_and_type(page, d, PGT_writable_page) )\n+                put_page_and_type(page);\n+            else\n+                return -EINVAL;\n+        }\n+\n+        return 0;\n+    }\n \n     /* foreign pages are added thru p2m_add_foreign */\n     if ( p2m_is_foreign(t) )",
        "diff_line_info": {
            "deleted_lines": [
                "        return (need_iommu_pt_sync(d) && t == p2m_ram_rw) ?",
                "            iommu_legacy_map(d, _dfn(mfn_x(mfn)), mfn, page_order,",
                "                             IOMMUF_readable | IOMMUF_writable) : 0;"
            ],
            "added_lines": [
                "    /* IOMMU for PV guests is handled in get_page_type() and put_page(). */",
                "    {",
                "        struct page_info *page = mfn_to_page(mfn);",
                "",
                "        /*",
                "         * Our interface for PV guests wrt IOMMU entries hasn't been very",
                "         * clear; but historically, pages have started out with IOMMU mappings,",
                "         * and only lose them when changed to a different page type.",
                "         *",
                "         * Retain this property by grabbing a writable type ref and then",
                "         * dropping it immediately.  The result will be pages that have a",
                "         * writable type (and an IOMMU entry), but a count of 0 (such that",
                "         * any guest-requested type changes succeed and remove the IOMMU",
                "         * entry).",
                "         */",
                "        if ( !need_iommu_pt_sync(d) || t != p2m_ram_rw )",
                "            return 0;",
                "",
                "        for ( i = 0; i < (1UL << page_order); ++i, ++page )",
                "        {",
                "            if ( get_page_and_type(page, d, PGT_writable_page) )",
                "                put_page_and_type(page);",
                "            else",
                "                return -EINVAL;",
                "        }",
                "",
                "        return 0;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-2959",
        "func_name": "torvalds/linux/pipe_resize_ring",
        "description": "A race condition was found in the Linux kernel's watch queue due to a missing lock in pipe_resize_ring(). The specific flaw exists within the handling of pipe buffers. The issue results from the lack of proper locking when performing operations on an object. This flaw allows a local user to crash the system or escalate their privileges on the system.",
        "git_url": "https://github.com/torvalds/linux/commit/189b0ddc245139af81198d1a3637cac74f96e13a",
        "commit_title": "pipe: Fix missing lock in pipe_resize_ring()",
        "commit_text": " pipe_resize_ring() needs to take the pipe->rd_wait.lock spinlock to prevent post_one_notification() from trying to insert into the ring whilst the ring is being replaced.  The occupancy check must be done after the lock is taken, and the lock must be taken after the new ring is allocated.  The bug can lead to an oops looking something like:   BUG: KASAN: use-after-free in post_one_notification.isra.0+0x62e/0x840  Read of size 4 at addr ffff88801cc72a70 by task poc/27196  ...  Call Trace:   post_one_notification.isra.0+0x62e/0x840   __post_watch_notification+0x3b7/0x650   key_create_or_update+0xb8b/0xd20   __do_sys_add_key+0x175/0x340   __x64_sys_add_key+0xbe/0x140   do_syscall_64+0x5c/0xc0   entry_SYSCALL_64_after_hwframe+0x44/0xae  Reported by Selim Enes Karaduman @Enesdex working with Trend Micro Zero Day Initiative. ",
        "func_before": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\t/*\n\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n\t * Since we don't expect a lot of shrink+grow operations, just free and\n\t * allocate again like we would do for growing.  If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\tn = pipe_occupancy(pipe->head, pipe->tail);\n\tif (nr_slots < n)\n\t\treturn -EBUSY;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "func": "int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int head, tail, mask, n;\n\n\tbufs = kcalloc(nr_slots, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs))\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&pipe->rd_wait.lock);\n\tmask = pipe->ring_size - 1;\n\thead = pipe->head;\n\ttail = pipe->tail;\n\n\tn = pipe_occupancy(head, tail);\n\tif (nr_slots < n) {\n\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n\t\tkfree(bufs);\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indices.\n\t */\n\tif (n > 0) {\n\t\tunsigned int h = head & mask;\n\t\tunsigned int t = tail & mask;\n\t\tif (h > t) {\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       n * sizeof(struct pipe_buffer));\n\t\t} else {\n\t\t\tunsigned int tsize = pipe->ring_size - t;\n\t\t\tif (h > 0)\n\t\t\t\tmemcpy(bufs + tsize, pipe->bufs,\n\t\t\t\t       h * sizeof(struct pipe_buffer));\n\t\t\tmemcpy(bufs, pipe->bufs + t,\n\t\t\t       tsize * sizeof(struct pipe_buffer));\n\t\t}\n\t}\n\n\thead = n;\n\ttail = 0;\n\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->ring_size = nr_slots;\n\tif (pipe->max_usage > nr_slots)\n\t\tpipe->max_usage = nr_slots;\n\tpipe->tail = tail;\n\tpipe->head = head;\n\n\tspin_unlock_irq(&pipe->rd_wait.lock);\n\n\t/* This might have made more room for writers */\n\twake_up_interruptible(&pipe->wr_wait);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,23 +3,22 @@\n \tstruct pipe_buffer *bufs;\n \tunsigned int head, tail, mask, n;\n \n-\t/*\n-\t * We can shrink the pipe, if arg is greater than the ring occupancy.\n-\t * Since we don't expect a lot of shrink+grow operations, just free and\n-\t * allocate again like we would do for growing.  If the pipe currently\n-\t * contains more buffers than arg, then return busy.\n-\t */\n-\tmask = pipe->ring_size - 1;\n-\thead = pipe->head;\n-\ttail = pipe->tail;\n-\tn = pipe_occupancy(pipe->head, pipe->tail);\n-\tif (nr_slots < n)\n-\t\treturn -EBUSY;\n-\n \tbufs = kcalloc(nr_slots, sizeof(*bufs),\n \t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n \tif (unlikely(!bufs))\n \t\treturn -ENOMEM;\n+\n+\tspin_lock_irq(&pipe->rd_wait.lock);\n+\tmask = pipe->ring_size - 1;\n+\thead = pipe->head;\n+\ttail = pipe->tail;\n+\n+\tn = pipe_occupancy(head, tail);\n+\tif (nr_slots < n) {\n+\t\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\t\tkfree(bufs);\n+\t\treturn -EBUSY;\n+\t}\n \n \t/*\n \t * The pipe array wraps around, so just start the new one at zero\n@@ -52,6 +51,8 @@\n \tpipe->tail = tail;\n \tpipe->head = head;\n \n+\tspin_unlock_irq(&pipe->rd_wait.lock);\n+\n \t/* This might have made more room for writers */\n \twake_up_interruptible(&pipe->wr_wait);\n \treturn 0;",
        "diff_line_info": {
            "deleted_lines": [
                "\t/*",
                "\t * We can shrink the pipe, if arg is greater than the ring occupancy.",
                "\t * Since we don't expect a lot of shrink+grow operations, just free and",
                "\t * allocate again like we would do for growing.  If the pipe currently",
                "\t * contains more buffers than arg, then return busy.",
                "\t */",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "\tn = pipe_occupancy(pipe->head, pipe->tail);",
                "\tif (nr_slots < n)",
                "\t\treturn -EBUSY;",
                ""
            ],
            "added_lines": [
                "",
                "\tspin_lock_irq(&pipe->rd_wait.lock);",
                "\tmask = pipe->ring_size - 1;",
                "\thead = pipe->head;",
                "\ttail = pipe->tail;",
                "",
                "\tn = pipe_occupancy(head, tail);",
                "\tif (nr_slots < n) {",
                "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
                "\t\tkfree(bufs);",
                "\t\treturn -EBUSY;",
                "\t}",
                "\tspin_unlock_irq(&pipe->rd_wait.lock);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-3303",
        "func_name": "torvalds/linux/snd_pcm_oss_sync",
        "description": "A race condition flaw was found in the Linux kernel sound subsystem due to improper locking. It could lead to a NULL pointer dereference while handling the SNDCTL_DSP_SYNC ioctl. A privileged local user (root or member of the audio group) could use this flaw to crash the system, resulting in a denial of service condition",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=8423f0b6d513b259fdab9c9bf4aaa6188d054c2d",
        "commit_title": "There is a small race window at snd_pcm_oss_sync() that is called from",
        "commit_text": "OSS PCM SNDCTL_DSP_SYNC ioctl; namely the function calls snd_pcm_oss_make_ready() at first, then takes the params_lock mutex for the rest.  When the stream is set up again by another thread between them, it leads to inconsistency, and may result in unexpected results such as NULL dereference of OSS buffer as a fuzzer spotted recently.  The fix is simply to cover snd_pcm_oss_make_ready() call into the same params_lock mutex with snd_pcm_oss_make_ready_locked() variant.  Reported-and-tested-by: butt3rflyh4ck <butterflyhuangxx@gmail.com> Cc: <stable@vger.kernel.org> Link: https://lore.kernel.org/r/CAFcO6XN7JDM4xSXGhtusQfS2mSBcx50VJKwQpCq=WeLt57aaZA@mail.gmail.com Link: https://lore.kernel.org/r/20220905060714.22549-1-tiwai@suse.de ",
        "func_before": "static int snd_pcm_oss_sync(struct snd_pcm_oss_file *pcm_oss_file)\n{\n\tint err = 0;\n\tunsigned int saved_f_flags;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tsnd_pcm_format_t format;\n\tunsigned long width;\n\tsize_t size;\n\n\tsubstream = pcm_oss_file->streams[SNDRV_PCM_STREAM_PLAYBACK];\n\tif (substream != NULL) {\n\t\truntime = substream->runtime;\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\tgoto __direct;\n\t\terr = snd_pcm_oss_make_ready(substream);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tatomic_inc(&runtime->oss.rw_ref);\n\t\tif (mutex_lock_interruptible(&runtime->oss.params_lock)) {\n\t\t\tatomic_dec(&runtime->oss.rw_ref);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\t\tformat = snd_pcm_oss_format_from(runtime->oss.format);\n\t\twidth = snd_pcm_format_physical_width(format);\n\t\tif (runtime->oss.buffer_used > 0) {\n#ifdef OSS_DEBUG\n\t\t\tpcm_dbg(substream->pcm, \"sync: buffer_used\\n\");\n#endif\n\t\t\tsize = (8 * (runtime->oss.period_bytes - runtime->oss.buffer_used) + 7) / width;\n\t\t\tsnd_pcm_format_set_silence(format,\n\t\t\t\t\t\t   runtime->oss.buffer + runtime->oss.buffer_used,\n\t\t\t\t\t\t   size);\n\t\t\terr = snd_pcm_oss_sync1(substream, runtime->oss.period_bytes);\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\t\t} else if (runtime->oss.period_ptr > 0) {\n#ifdef OSS_DEBUG\n\t\t\tpcm_dbg(substream->pcm, \"sync: period_ptr\\n\");\n#endif\n\t\t\tsize = runtime->oss.period_bytes - runtime->oss.period_ptr;\n\t\t\tsnd_pcm_format_set_silence(format,\n\t\t\t\t\t\t   runtime->oss.buffer,\n\t\t\t\t\t\t   size * 8 / width);\n\t\t\terr = snd_pcm_oss_sync1(substream, size);\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\t\t}\n\t\t/*\n\t\t * The ALSA's period might be a bit large than OSS one.\n\t\t * Fill the remain portion of ALSA period with zeros.\n\t\t */\n\t\tsize = runtime->control->appl_ptr % runtime->period_size;\n\t\tif (size > 0) {\n\t\t\tsize = runtime->period_size - size;\n\t\t\tif (runtime->access == SNDRV_PCM_ACCESS_RW_INTERLEAVED)\n\t\t\t\tsnd_pcm_lib_write(substream, NULL, size);\n\t\t\telse if (runtime->access == SNDRV_PCM_ACCESS_RW_NONINTERLEAVED)\n\t\t\t\tsnd_pcm_lib_writev(substream, NULL, size);\n\t\t}\nunlock:\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t\tatomic_dec(&runtime->oss.rw_ref);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\t/*\n\t\t * finish sync: drain the buffer\n\t\t */\n\t      __direct:\n\t\tsaved_f_flags = substream->f_flags;\n\t\tsubstream->f_flags &= ~O_NONBLOCK;\n\t\terr = snd_pcm_kernel_ioctl(substream, SNDRV_PCM_IOCTL_DRAIN, NULL);\n\t\tsubstream->f_flags = saved_f_flags;\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tmutex_lock(&runtime->oss.params_lock);\n\t\truntime->oss.prepare = 1;\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t}\n\n\tsubstream = pcm_oss_file->streams[SNDRV_PCM_STREAM_CAPTURE];\n\tif (substream != NULL) {\n\t\terr = snd_pcm_oss_make_ready(substream);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\truntime = substream->runtime;\n\t\terr = snd_pcm_kernel_ioctl(substream, SNDRV_PCM_IOCTL_DROP, NULL);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tmutex_lock(&runtime->oss.params_lock);\n\t\truntime->oss.buffer_used = 0;\n\t\truntime->oss.prepare = 1;\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t}\n\treturn 0;\n}",
        "func": "static int snd_pcm_oss_sync(struct snd_pcm_oss_file *pcm_oss_file)\n{\n\tint err = 0;\n\tunsigned int saved_f_flags;\n\tstruct snd_pcm_substream *substream;\n\tstruct snd_pcm_runtime *runtime;\n\tsnd_pcm_format_t format;\n\tunsigned long width;\n\tsize_t size;\n\n\tsubstream = pcm_oss_file->streams[SNDRV_PCM_STREAM_PLAYBACK];\n\tif (substream != NULL) {\n\t\truntime = substream->runtime;\n\t\tif (atomic_read(&substream->mmap_count))\n\t\t\tgoto __direct;\n\t\tatomic_inc(&runtime->oss.rw_ref);\n\t\tif (mutex_lock_interruptible(&runtime->oss.params_lock)) {\n\t\t\tatomic_dec(&runtime->oss.rw_ref);\n\t\t\treturn -ERESTARTSYS;\n\t\t}\n\t\terr = snd_pcm_oss_make_ready_locked(substream);\n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\t\tformat = snd_pcm_oss_format_from(runtime->oss.format);\n\t\twidth = snd_pcm_format_physical_width(format);\n\t\tif (runtime->oss.buffer_used > 0) {\n#ifdef OSS_DEBUG\n\t\t\tpcm_dbg(substream->pcm, \"sync: buffer_used\\n\");\n#endif\n\t\t\tsize = (8 * (runtime->oss.period_bytes - runtime->oss.buffer_used) + 7) / width;\n\t\t\tsnd_pcm_format_set_silence(format,\n\t\t\t\t\t\t   runtime->oss.buffer + runtime->oss.buffer_used,\n\t\t\t\t\t\t   size);\n\t\t\terr = snd_pcm_oss_sync1(substream, runtime->oss.period_bytes);\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\t\t} else if (runtime->oss.period_ptr > 0) {\n#ifdef OSS_DEBUG\n\t\t\tpcm_dbg(substream->pcm, \"sync: period_ptr\\n\");\n#endif\n\t\t\tsize = runtime->oss.period_bytes - runtime->oss.period_ptr;\n\t\t\tsnd_pcm_format_set_silence(format,\n\t\t\t\t\t\t   runtime->oss.buffer,\n\t\t\t\t\t\t   size * 8 / width);\n\t\t\terr = snd_pcm_oss_sync1(substream, size);\n\t\t\tif (err < 0)\n\t\t\t\tgoto unlock;\n\t\t}\n\t\t/*\n\t\t * The ALSA's period might be a bit large than OSS one.\n\t\t * Fill the remain portion of ALSA period with zeros.\n\t\t */\n\t\tsize = runtime->control->appl_ptr % runtime->period_size;\n\t\tif (size > 0) {\n\t\t\tsize = runtime->period_size - size;\n\t\t\tif (runtime->access == SNDRV_PCM_ACCESS_RW_INTERLEAVED)\n\t\t\t\tsnd_pcm_lib_write(substream, NULL, size);\n\t\t\telse if (runtime->access == SNDRV_PCM_ACCESS_RW_NONINTERLEAVED)\n\t\t\t\tsnd_pcm_lib_writev(substream, NULL, size);\n\t\t}\nunlock:\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t\tatomic_dec(&runtime->oss.rw_ref);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\t/*\n\t\t * finish sync: drain the buffer\n\t\t */\n\t      __direct:\n\t\tsaved_f_flags = substream->f_flags;\n\t\tsubstream->f_flags &= ~O_NONBLOCK;\n\t\terr = snd_pcm_kernel_ioctl(substream, SNDRV_PCM_IOCTL_DRAIN, NULL);\n\t\tsubstream->f_flags = saved_f_flags;\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tmutex_lock(&runtime->oss.params_lock);\n\t\truntime->oss.prepare = 1;\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t}\n\n\tsubstream = pcm_oss_file->streams[SNDRV_PCM_STREAM_CAPTURE];\n\tif (substream != NULL) {\n\t\terr = snd_pcm_oss_make_ready(substream);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\truntime = substream->runtime;\n\t\terr = snd_pcm_kernel_ioctl(substream, SNDRV_PCM_IOCTL_DROP, NULL);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tmutex_lock(&runtime->oss.params_lock);\n\t\truntime->oss.buffer_used = 0;\n\t\truntime->oss.prepare = 1;\n\t\tmutex_unlock(&runtime->oss.params_lock);\n\t}\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -13,14 +13,14 @@\n \t\truntime = substream->runtime;\n \t\tif (atomic_read(&substream->mmap_count))\n \t\t\tgoto __direct;\n-\t\terr = snd_pcm_oss_make_ready(substream);\n-\t\tif (err < 0)\n-\t\t\treturn err;\n \t\tatomic_inc(&runtime->oss.rw_ref);\n \t\tif (mutex_lock_interruptible(&runtime->oss.params_lock)) {\n \t\t\tatomic_dec(&runtime->oss.rw_ref);\n \t\t\treturn -ERESTARTSYS;\n \t\t}\n+\t\terr = snd_pcm_oss_make_ready_locked(substream);\n+\t\tif (err < 0)\n+\t\t\tgoto unlock;\n \t\tformat = snd_pcm_oss_format_from(runtime->oss.format);\n \t\twidth = snd_pcm_format_physical_width(format);\n \t\tif (runtime->oss.buffer_used > 0) {",
        "diff_line_info": {
            "deleted_lines": [
                "\t\terr = snd_pcm_oss_make_ready(substream);",
                "\t\tif (err < 0)",
                "\t\t\treturn err;"
            ],
            "added_lines": [
                "\t\terr = snd_pcm_oss_make_ready_locked(substream);",
                "\t\tif (err < 0)",
                "\t\t\tgoto unlock;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0160",
        "func_name": "torvalds/linux/__sock_map_delete",
        "description": "A deadlock flaw was found in the Linux kernels BPF subsystem. This flaw allows a local user to potentially crash the system.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ed17aa92dc56",
        "commit_title": "When huang uses sched_switch tracepoint, the tracepoint",
        "commit_text": "does only one thing in the mounted ebpf program, which deletes the fixed elements in sockhash ([0])  It seems that elements in sockhash are rarely actively deleted by users or ebpf program. Therefore, we do not pay much attention to their deletion. Compared with hash maps, sockhash only provides spin_lock_bh protection. This causes it to appear to have self-locking behavior in the interrupt context.    [0]:https://lore.kernel.org/all/CABcoxUayum5oOqFMMqAeWuS8+EzojquSOSyDA3J_2omY=2EeAg@mail.gmail.com/  Link: https://lore.kernel.org/r/20230406122622.109978-1-liuxin350@huawei.com ",
        "func_before": "static int __sock_map_delete(struct bpf_stab *stab, struct sock *sk_test,\n\t\t\t     struct sock **psk)\n{\n\tstruct sock *sk;\n\tint err = 0;\n\n\traw_spin_lock_bh(&stab->lock);\n\tsk = *psk;\n\tif (!sk_test || sk_test == sk)\n\t\tsk = xchg(psk, NULL);\n\n\tif (likely(sk))\n\t\tsock_map_unref(sk, psk);\n\telse\n\t\terr = -EINVAL;\n\n\traw_spin_unlock_bh(&stab->lock);\n\treturn err;\n}",
        "func": "static int __sock_map_delete(struct bpf_stab *stab, struct sock *sk_test,\n\t\t\t     struct sock **psk)\n{\n\tstruct sock *sk;\n\tint err = 0;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&stab->lock, flags);\n\tsk = *psk;\n\tif (!sk_test || sk_test == sk)\n\t\tsk = xchg(psk, NULL);\n\n\tif (likely(sk))\n\t\tsock_map_unref(sk, psk);\n\telse\n\t\terr = -EINVAL;\n\n\traw_spin_unlock_irqrestore(&stab->lock, flags);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,8 +3,9 @@\n {\n \tstruct sock *sk;\n \tint err = 0;\n+\tunsigned long flags;\n \n-\traw_spin_lock_bh(&stab->lock);\n+\traw_spin_lock_irqsave(&stab->lock, flags);\n \tsk = *psk;\n \tif (!sk_test || sk_test == sk)\n \t\tsk = xchg(psk, NULL);\n@@ -14,6 +15,6 @@\n \telse\n \t\terr = -EINVAL;\n \n-\traw_spin_unlock_bh(&stab->lock);\n+\traw_spin_unlock_irqrestore(&stab->lock, flags);\n \treturn err;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\traw_spin_lock_bh(&stab->lock);",
                "\traw_spin_unlock_bh(&stab->lock);"
            ],
            "added_lines": [
                "\tunsigned long flags;",
                "\traw_spin_lock_irqsave(&stab->lock, flags);",
                "\traw_spin_unlock_irqrestore(&stab->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-0160",
        "func_name": "torvalds/linux/sock_hash_delete_elem",
        "description": "A deadlock flaw was found in the Linux kernels BPF subsystem. This flaw allows a local user to potentially crash the system.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=ed17aa92dc56",
        "commit_title": "When huang uses sched_switch tracepoint, the tracepoint",
        "commit_text": "does only one thing in the mounted ebpf program, which deletes the fixed elements in sockhash ([0])  It seems that elements in sockhash are rarely actively deleted by users or ebpf program. Therefore, we do not pay much attention to their deletion. Compared with hash maps, sockhash only provides spin_lock_bh protection. This causes it to appear to have self-locking behavior in the interrupt context.    [0]:https://lore.kernel.org/all/CABcoxUayum5oOqFMMqAeWuS8+EzojquSOSyDA3J_2omY=2EeAg@mail.gmail.com/  Link: https://lore.kernel.org/r/20230406122622.109978-1-liuxin350@huawei.com ",
        "func_before": "static long sock_hash_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu32 hash, key_size = map->key_size;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct bpf_shtab_elem *elem;\n\tint ret = -ENOENT;\n\n\thash = sock_hash_bucket_hash(key, key_size);\n\tbucket = sock_hash_select_bucket(htab, hash);\n\n\traw_spin_lock_bh(&bucket->lock);\n\telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n\tif (elem) {\n\t\thlist_del_rcu(&elem->node);\n\t\tsock_map_unref(elem->sk, elem);\n\t\tsock_hash_free_elem(htab, elem);\n\t\tret = 0;\n\t}\n\traw_spin_unlock_bh(&bucket->lock);\n\treturn ret;\n}",
        "func": "static long sock_hash_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_shtab *htab = container_of(map, struct bpf_shtab, map);\n\tu32 hash, key_size = map->key_size;\n\tstruct bpf_shtab_bucket *bucket;\n\tstruct bpf_shtab_elem *elem;\n\tint ret = -ENOENT;\n\tunsigned long flags;\n\n\thash = sock_hash_bucket_hash(key, key_size);\n\tbucket = sock_hash_select_bucket(htab, hash);\n\n\traw_spin_lock_irqsave(&bucket->lock, flags);\n\telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n\tif (elem) {\n\t\thlist_del_rcu(&elem->node);\n\t\tsock_map_unref(elem->sk, elem);\n\t\tsock_hash_free_elem(htab, elem);\n\t\tret = 0;\n\t}\n\traw_spin_unlock_irqrestore(&bucket->lock, flags);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,11 +5,12 @@\n \tstruct bpf_shtab_bucket *bucket;\n \tstruct bpf_shtab_elem *elem;\n \tint ret = -ENOENT;\n+\tunsigned long flags;\n \n \thash = sock_hash_bucket_hash(key, key_size);\n \tbucket = sock_hash_select_bucket(htab, hash);\n \n-\traw_spin_lock_bh(&bucket->lock);\n+\traw_spin_lock_irqsave(&bucket->lock, flags);\n \telem = sock_hash_lookup_elem_raw(&bucket->head, hash, key, key_size);\n \tif (elem) {\n \t\thlist_del_rcu(&elem->node);\n@@ -17,6 +18,6 @@\n \t\tsock_hash_free_elem(htab, elem);\n \t\tret = 0;\n \t}\n-\traw_spin_unlock_bh(&bucket->lock);\n+\traw_spin_unlock_irqrestore(&bucket->lock, flags);\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\traw_spin_lock_bh(&bucket->lock);",
                "\traw_spin_unlock_bh(&bucket->lock);"
            ],
            "added_lines": [
                "\tunsigned long flags;",
                "\traw_spin_lock_irqsave(&bucket->lock, flags);",
                "\traw_spin_unlock_irqrestore(&bucket->lock, flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2430",
        "func_name": "torvalds/linux/io_msg_ring",
        "description": "A vulnerability was found due to missing lock for IOPOLL flaw in io_cqring_event_overflow() in io_uring.c in Linux Kernel. This flaw allows a local attacker with user privilege to trigger a Denial of Service threat.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=e12d7a46f65ae4b7d58a5e0c1cbfa825cf8",
        "commit_title": "If the target ring is configured with IOPOLL, then we always need to hold",
        "commit_text": "the target ring uring_lock before posting CQEs. We could just grab it unconditionally, but since we don't expect many target rings to be of this type, make grabbing the uring_lock conditional on the ring type.  Link: https://lore.kernel.org/io-uring/Y8krlYa52%2F0YGqkg@ip-172-31-85-199.ec2.internal/ ",
        "func_before": "int io_msg_ring(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg);\n\tint ret;\n\n\tret = -EBADFD;\n\tif (!io_is_uring_fops(req->file))\n\t\tgoto done;\n\n\tswitch (msg->cmd) {\n\tcase IORING_MSG_DATA:\n\t\tret = io_msg_ring_data(req);\n\t\tbreak;\n\tcase IORING_MSG_SEND_FD:\n\t\tret = io_msg_send_fd(req, issue_flags);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\ndone:\n\tif (ret < 0) {\n\t\tif (ret == -EAGAIN || ret == IOU_ISSUE_SKIP_COMPLETE)\n\t\t\treturn ret;\n\t\treq_set_fail(req);\n\t}\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "func": "int io_msg_ring(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg);\n\tint ret;\n\n\tret = -EBADFD;\n\tif (!io_is_uring_fops(req->file))\n\t\tgoto done;\n\n\tswitch (msg->cmd) {\n\tcase IORING_MSG_DATA:\n\t\tret = io_msg_ring_data(req, issue_flags);\n\t\tbreak;\n\tcase IORING_MSG_SEND_FD:\n\t\tret = io_msg_send_fd(req, issue_flags);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\ndone:\n\tif (ret < 0) {\n\t\tif (ret == -EAGAIN || ret == IOU_ISSUE_SKIP_COMPLETE)\n\t\t\treturn ret;\n\t\treq_set_fail(req);\n\t}\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,7 @@\n \n \tswitch (msg->cmd) {\n \tcase IORING_MSG_DATA:\n-\t\tret = io_msg_ring_data(req);\n+\t\tret = io_msg_ring_data(req, issue_flags);\n \t\tbreak;\n \tcase IORING_MSG_SEND_FD:\n \t\tret = io_msg_send_fd(req, issue_flags);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tret = io_msg_ring_data(req);"
            ],
            "added_lines": [
                "\t\tret = io_msg_ring_data(req, issue_flags);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2430",
        "func_name": "torvalds/linux/io_msg_tw_complete",
        "description": "A vulnerability was found due to missing lock for IOPOLL flaw in io_cqring_event_overflow() in io_uring.c in Linux Kernel. This flaw allows a local attacker with user privilege to trigger a Denial of Service threat.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=e12d7a46f65ae4b7d58a5e0c1cbfa825cf8",
        "commit_title": "If the target ring is configured with IOPOLL, then we always need to hold",
        "commit_text": "the target ring uring_lock before posting CQEs. We could just grab it unconditionally, but since we don't expect many target rings to be of this type, make grabbing the uring_lock conditional on the ring type.  Link: https://lore.kernel.org/io-uring/Y8krlYa52%2F0YGqkg@ip-172-31-85-199.ec2.internal/ ",
        "func_before": "static void io_msg_tw_complete(struct callback_head *head)\n{\n\tstruct io_msg *msg = container_of(head, struct io_msg, tw);\n\tstruct io_kiocb *req = cmd_to_io_kiocb(msg);\n\tstruct io_ring_ctx *target_ctx = req->file->private_data;\n\tint ret = 0;\n\n\tif (current->flags & PF_EXITING)\n\t\tret = -EOWNERDEAD;\n\telse if (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n\t\tret = -EOVERFLOW;\n\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_queue_tw_complete(req, ret);\n}",
        "func": "static void io_msg_tw_complete(struct callback_head *head)\n{\n\tstruct io_msg *msg = container_of(head, struct io_msg, tw);\n\tstruct io_kiocb *req = cmd_to_io_kiocb(msg);\n\tstruct io_ring_ctx *target_ctx = req->file->private_data;\n\tint ret = 0;\n\n\tif (current->flags & PF_EXITING) {\n\t\tret = -EOWNERDEAD;\n\t} else {\n\t\t/*\n\t\t * If the target ring is using IOPOLL mode, then we need to be\n\t\t * holding the uring_lock for posting completions. Other ring\n\t\t * types rely on the regular completion locking, which is\n\t\t * handled while posting.\n\t\t */\n\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_lock(&target_ctx->uring_lock);\n\t\tif (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n\t\t\tret = -EOVERFLOW;\n\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)\n\t\t\tmutex_unlock(&target_ctx->uring_lock);\n\t}\n\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_queue_tw_complete(req, ret);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,10 +5,22 @@\n \tstruct io_ring_ctx *target_ctx = req->file->private_data;\n \tint ret = 0;\n \n-\tif (current->flags & PF_EXITING)\n+\tif (current->flags & PF_EXITING) {\n \t\tret = -EOWNERDEAD;\n-\telse if (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n-\t\tret = -EOVERFLOW;\n+\t} else {\n+\t\t/*\n+\t\t * If the target ring is using IOPOLL mode, then we need to be\n+\t\t * holding the uring_lock for posting completions. Other ring\n+\t\t * types rely on the regular completion locking, which is\n+\t\t * handled while posting.\n+\t\t */\n+\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)\n+\t\t\tmutex_lock(&target_ctx->uring_lock);\n+\t\tif (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n+\t\t\tret = -EOVERFLOW;\n+\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)\n+\t\t\tmutex_unlock(&target_ctx->uring_lock);\n+\t}\n \n \tif (ret < 0)\n \t\treq_set_fail(req);",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (current->flags & PF_EXITING)",
                "\telse if (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))",
                "\t\tret = -EOVERFLOW;"
            ],
            "added_lines": [
                "\tif (current->flags & PF_EXITING) {",
                "\t} else {",
                "\t\t/*",
                "\t\t * If the target ring is using IOPOLL mode, then we need to be",
                "\t\t * holding the uring_lock for posting completions. Other ring",
                "\t\t * types rely on the regular completion locking, which is",
                "\t\t * handled while posting.",
                "\t\t */",
                "\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)",
                "\t\t\tmutex_lock(&target_ctx->uring_lock);",
                "\t\tif (!io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))",
                "\t\t\tret = -EOVERFLOW;",
                "\t\tif (target_ctx->flags & IORING_SETUP_IOPOLL)",
                "\t\t\tmutex_unlock(&target_ctx->uring_lock);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-2430",
        "func_name": "torvalds/linux/io_msg_ring_data",
        "description": "A vulnerability was found due to missing lock for IOPOLL flaw in io_cqring_event_overflow() in io_uring.c in Linux Kernel. This flaw allows a local attacker with user privilege to trigger a Denial of Service threat.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=e12d7a46f65ae4b7d58a5e0c1cbfa825cf8",
        "commit_title": "If the target ring is configured with IOPOLL, then we always need to hold",
        "commit_text": "the target ring uring_lock before posting CQEs. We could just grab it unconditionally, but since we don't expect many target rings to be of this type, make grabbing the uring_lock conditional on the ring type.  Link: https://lore.kernel.org/io-uring/Y8krlYa52%2F0YGqkg@ip-172-31-85-199.ec2.internal/ ",
        "func_before": "static int io_msg_ring_data(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *target_ctx = req->file->private_data;\n\tstruct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg);\n\n\tif (msg->src_fd || msg->dst_fd || msg->flags)\n\t\treturn -EINVAL;\n\n\tif (target_ctx->task_complete && current != target_ctx->submitter_task) {\n\t\tinit_task_work(&msg->tw, io_msg_tw_complete);\n\t\tif (task_work_add(target_ctx->submitter_task, &msg->tw,\n\t\t\t\t  TWA_SIGNAL_NO_IPI))\n\t\t\treturn -EOWNERDEAD;\n\n\t\tatomic_or(IORING_SQ_TASKRUN, &target_ctx->rings->sq_flags);\n\t\treturn IOU_ISSUE_SKIP_COMPLETE;\n\t}\n\n\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n\t\treturn 0;\n\n\treturn -EOVERFLOW;\n}",
        "func": "static int io_msg_ring_data(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *target_ctx = req->file->private_data;\n\tstruct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg);\n\tint ret;\n\n\tif (msg->src_fd || msg->dst_fd || msg->flags)\n\t\treturn -EINVAL;\n\n\tif (target_ctx->task_complete && current != target_ctx->submitter_task) {\n\t\tinit_task_work(&msg->tw, io_msg_tw_complete);\n\t\tif (task_work_add(target_ctx->submitter_task, &msg->tw,\n\t\t\t\t  TWA_SIGNAL_NO_IPI))\n\t\t\treturn -EOWNERDEAD;\n\n\t\tatomic_or(IORING_SQ_TASKRUN, &target_ctx->rings->sq_flags);\n\t\treturn IOU_ISSUE_SKIP_COMPLETE;\n\t}\n\n\tret = -EOVERFLOW;\n\tif (target_ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (unlikely(io_double_lock_ctx(target_ctx, issue_flags)))\n\t\t\treturn -EAGAIN;\n\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n\t\t\tret = 0;\n\t\tio_double_unlock_ctx(target_ctx);\n\t} else {\n\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n\t\t\tret = 0;\n\t}\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,8 @@\n-static int io_msg_ring_data(struct io_kiocb *req)\n+static int io_msg_ring_data(struct io_kiocb *req, unsigned int issue_flags)\n {\n \tstruct io_ring_ctx *target_ctx = req->file->private_data;\n \tstruct io_msg *msg = io_kiocb_to_cmd(req, struct io_msg);\n+\tint ret;\n \n \tif (msg->src_fd || msg->dst_fd || msg->flags)\n \t\treturn -EINVAL;\n@@ -16,8 +17,16 @@\n \t\treturn IOU_ISSUE_SKIP_COMPLETE;\n \t}\n \n-\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n-\t\treturn 0;\n-\n-\treturn -EOVERFLOW;\n+\tret = -EOVERFLOW;\n+\tif (target_ctx->flags & IORING_SETUP_IOPOLL) {\n+\t\tif (unlikely(io_double_lock_ctx(target_ctx, issue_flags)))\n+\t\t\treturn -EAGAIN;\n+\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n+\t\t\tret = 0;\n+\t\tio_double_unlock_ctx(target_ctx);\n+\t} else {\n+\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))\n+\t\t\tret = 0;\n+\t}\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static int io_msg_ring_data(struct io_kiocb *req)",
                "\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))",
                "\t\treturn 0;",
                "",
                "\treturn -EOVERFLOW;"
            ],
            "added_lines": [
                "static int io_msg_ring_data(struct io_kiocb *req, unsigned int issue_flags)",
                "\tint ret;",
                "\tret = -EOVERFLOW;",
                "\tif (target_ctx->flags & IORING_SETUP_IOPOLL) {",
                "\t\tif (unlikely(io_double_lock_ctx(target_ctx, issue_flags)))",
                "\t\t\treturn -EAGAIN;",
                "\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))",
                "\t\t\tret = 0;",
                "\t\tio_double_unlock_ctx(target_ctx);",
                "\t} else {",
                "\t\tif (io_post_aux_cqe(target_ctx, msg->user_data, msg->len, 0))",
                "\t\t\tret = 0;",
                "\t}",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-38203",
        "func_name": "torvalds/linux/check_system_chunk",
        "description": "btrfs in the Linux kernel before 5.13.4 allows attackers to cause a denial of service (deadlock) via processes that trigger allocation of new system chunks during times when there is a shortage of free space in the system space_info.",
        "git_url": "https://github.com/torvalds/linux/commit/1cb3db1cf383a3c7dbda1aa0ce748b0958759947",
        "commit_title": "btrfs: fix deadlock with concurrent chunk allocations involving system chunks",
        "commit_text": " When a task attempting to allocate a new chunk verifies that there is not currently enough free space in the system space_info and there is another task that allocated a new system chunk but it did not finish yet the creation of the respective block group, it waits for that other task to finish creating the block group. This is to avoid exhaustion of the system chunk array in the superblock, which is limited, when we have a thundering herd of tasks allocating new chunks. This problem was described and fixed by commit eafa4fd0ad0607 (\"btrfs: fix exhaustion of the system chunk array due to concurrent allocations\").  However there are two very similar scenarios where this can lead to a deadlock:  1) Task B allocated a new system chunk and task A is waiting on task B    to finish creation of the respective system block group. However before    task B ends its transaction handle and finishes the creation of the    system block group, it attempts to allocate another chunk (like a data    chunk for an fallocate operation for a very large range). Task B will    be unable to progress and allocate the new chunk, because task A set    space_info->chunk_alloc to 1 and therefore it loops at    btrfs_chunk_alloc() waiting for task A to finish its chunk allocation    and set space_info->chunk_alloc to 0, but task A is waiting on task B    to finish creation of the new system block group, therefore resulting    in a deadlock;  2) Task B allocated a new system chunk and task A is waiting on task B to    finish creation of the respective system block group. By the time that    task B enter the final phase of block group allocation, which happens    at btrfs_create_pending_block_groups(), when it modifies the extent    tree, the device tree or the chunk tree to insert the items for some    new block group, it needs to allocate a new chunk, so it ends up at    btrfs_chunk_alloc() and keeps looping there because task A has set    space_info->chunk_alloc to 1, but task A is waiting for task B to    finish creation of the new system block group and release the reserved    system space, therefore resulting in a deadlock.  In short, the problem is if a task B needs to allocate a new chunk after it previously allocated a new system chunk and if another task A is currently waiting for task B to complete the allocation of the new system chunk.  Unfortunately this deadlock scenario introduced by the previous fix for the system chunk array exhaustion problem does not have a simple and short fix, and requires a big change to rework the chunk allocation code so that chunk btree updates are all made in the first phase of chunk allocation. And since this deadlock regression is being frequently hit on zoned filesystems and the system chunk array exhaustion problem is triggered in more extreme cases (originally observed on PowerPC with a node size of 64K when running the fallocate tests from stress-ng), revert the changes from that commit. The next patch in the series, with a subject of \"btrfs: rework chunk allocation to avoid exhaustion of the system chunk array\" does the necessary changes to fix the system chunk array exhaustion problem.  Link: https://lore.kernel.org/linux-btrfs/20210621015922.ewgbffxuawia7liz@naota-xeon/",
        "func_before": "void check_system_chunk(struct btrfs_trans_handle *trans, u64 type)\n{\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_space_info *info;\n\tu64 left;\n\tu64 thresh;\n\tint ret = 0;\n\tu64 num_devs;\n\n\t/*\n\t * Needed because we can end up allocating a system chunk and for an\n\t * atomic and race free space reservation in the chunk block reserve.\n\t */\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tinfo = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);\nagain:\n\tspin_lock(&info->lock);\n\tleft = info->total_bytes - btrfs_space_info_used(info, true);\n\tspin_unlock(&info->lock);\n\n\tnum_devs = get_profile_num_devs(fs_info, type);\n\n\t/* num_devs device items to update and 1 chunk item to add or remove */\n\tthresh = btrfs_calc_metadata_size(fs_info, num_devs) +\n\t\tbtrfs_calc_insert_metadata_size(fs_info, 1);\n\n\tif (left < thresh && btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n\t\tbtrfs_info(fs_info, \"left=%llu, need=%llu, flags=%llu\",\n\t\t\t   left, thresh, type);\n\t\tbtrfs_dump_space_info(fs_info, info, 0, 0);\n\t}\n\n\tif (left < thresh) {\n\t\tu64 flags = btrfs_system_alloc_profile(fs_info);\n\t\tu64 reserved = atomic64_read(&cur_trans->chunk_bytes_reserved);\n\n\t\t/*\n\t\t * If there's not available space for the chunk tree (system\n\t\t * space) and there are other tasks that reserved space for\n\t\t * creating a new system block group, wait for them to complete\n\t\t * the creation of their system block group and release excess\n\t\t * reserved space. We do this because:\n\t\t *\n\t\t * *) We can end up allocating more system chunks than necessary\n\t\t *    when there are multiple tasks that are concurrently\n\t\t *    allocating block groups, which can lead to exhaustion of\n\t\t *    the system array in the superblock;\n\t\t *\n\t\t * *) If we allocate extra and unnecessary system block groups,\n\t\t *    despite being empty for a long time, and possibly forever,\n\t\t *    they end not being added to the list of unused block groups\n\t\t *    because that typically happens only when deallocating the\n\t\t *    last extent from a block group - which never happens since\n\t\t *    we never allocate from them in the first place. The few\n\t\t *    exceptions are when mounting a filesystem or running scrub,\n\t\t *    which add unused block groups to the list of unused block\n\t\t *    groups, to be deleted by the cleaner kthread.\n\t\t *    And even when they are added to the list of unused block\n\t\t *    groups, it can take a long time until they get deleted,\n\t\t *    since the cleaner kthread might be sleeping or busy with\n\t\t *    other work (deleting subvolumes, running delayed iputs,\n\t\t *    defrag scheduling, etc);\n\t\t *\n\t\t * This is rare in practice, but can happen when too many tasks\n\t\t * are allocating blocks groups in parallel (via fallocate())\n\t\t * and before the one that reserved space for a new system block\n\t\t * group finishes the block group creation and releases the space\n\t\t * reserved in excess (at btrfs_create_pending_block_groups()),\n\t\t * other tasks end up here and see free system space temporarily\n\t\t * not enough for updating the chunk tree.\n\t\t *\n\t\t * We unlock the chunk mutex before waiting for such tasks and\n\t\t * lock it again after the wait, otherwise we would deadlock.\n\t\t * It is safe to do so because allocating a system chunk is the\n\t\t * first thing done while allocating a new block group.\n\t\t */\n\t\tif (reserved > trans->chunk_bytes_reserved) {\n\t\t\tconst u64 min_needed = reserved - thresh;\n\n\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n\t\t\twait_event(cur_trans->chunk_reserve_wait,\n\t\t\t   atomic64_read(&cur_trans->chunk_bytes_reserved) <=\n\t\t\t   min_needed);\n\t\t\tmutex_lock(&fs_info->chunk_mutex);\n\t\t\tgoto again;\n\t\t}\n\n\t\t/*\n\t\t * Ignore failure to create system chunk. We might end up not\n\t\t * needing it, as we might not need to COW all nodes/leafs from\n\t\t * the paths we visit in the chunk tree (they were already COWed\n\t\t * or created in the current transaction for example).\n\t\t */\n\t\tret = btrfs_alloc_chunk(trans, flags);\n\t}\n\n\tif (!ret) {\n\t\tret = btrfs_block_rsv_add(fs_info->chunk_root,\n\t\t\t\t\t  &fs_info->chunk_block_rsv,\n\t\t\t\t\t  thresh, BTRFS_RESERVE_NO_FLUSH);\n\t\tif (!ret) {\n\t\t\tatomic64_add(thresh, &cur_trans->chunk_bytes_reserved);\n\t\t\ttrans->chunk_bytes_reserved += thresh;\n\t\t}\n\t}\n}",
        "func": "void check_system_chunk(struct btrfs_trans_handle *trans, u64 type)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_space_info *info;\n\tu64 left;\n\tu64 thresh;\n\tint ret = 0;\n\tu64 num_devs;\n\n\t/*\n\t * Needed because we can end up allocating a system chunk and for an\n\t * atomic and race free space reservation in the chunk block reserve.\n\t */\n\tlockdep_assert_held(&fs_info->chunk_mutex);\n\n\tinfo = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);\n\tspin_lock(&info->lock);\n\tleft = info->total_bytes - btrfs_space_info_used(info, true);\n\tspin_unlock(&info->lock);\n\n\tnum_devs = get_profile_num_devs(fs_info, type);\n\n\t/* num_devs device items to update and 1 chunk item to add or remove */\n\tthresh = btrfs_calc_metadata_size(fs_info, num_devs) +\n\t\tbtrfs_calc_insert_metadata_size(fs_info, 1);\n\n\tif (left < thresh && btrfs_test_opt(fs_info, ENOSPC_DEBUG)) {\n\t\tbtrfs_info(fs_info, \"left=%llu, need=%llu, flags=%llu\",\n\t\t\t   left, thresh, type);\n\t\tbtrfs_dump_space_info(fs_info, info, 0, 0);\n\t}\n\n\tif (left < thresh) {\n\t\tu64 flags = btrfs_system_alloc_profile(fs_info);\n\n\t\t/*\n\t\t * Ignore failure to create system chunk. We might end up not\n\t\t * needing it, as we might not need to COW all nodes/leafs from\n\t\t * the paths we visit in the chunk tree (they were already COWed\n\t\t * or created in the current transaction for example).\n\t\t */\n\t\tret = btrfs_alloc_chunk(trans, flags);\n\t}\n\n\tif (!ret) {\n\t\tret = btrfs_block_rsv_add(fs_info->chunk_root,\n\t\t\t\t\t  &fs_info->chunk_block_rsv,\n\t\t\t\t\t  thresh, BTRFS_RESERVE_NO_FLUSH);\n\t\tif (!ret)\n\t\t\ttrans->chunk_bytes_reserved += thresh;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,6 +1,5 @@\n void check_system_chunk(struct btrfs_trans_handle *trans, u64 type)\n {\n-\tstruct btrfs_transaction *cur_trans = trans->transaction;\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n \tstruct btrfs_space_info *info;\n \tu64 left;\n@@ -15,7 +14,6 @@\n \tlockdep_assert_held(&fs_info->chunk_mutex);\n \n \tinfo = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);\n-again:\n \tspin_lock(&info->lock);\n \tleft = info->total_bytes - btrfs_space_info_used(info, true);\n \tspin_unlock(&info->lock);\n@@ -34,58 +32,6 @@\n \n \tif (left < thresh) {\n \t\tu64 flags = btrfs_system_alloc_profile(fs_info);\n-\t\tu64 reserved = atomic64_read(&cur_trans->chunk_bytes_reserved);\n-\n-\t\t/*\n-\t\t * If there's not available space for the chunk tree (system\n-\t\t * space) and there are other tasks that reserved space for\n-\t\t * creating a new system block group, wait for them to complete\n-\t\t * the creation of their system block group and release excess\n-\t\t * reserved space. We do this because:\n-\t\t *\n-\t\t * *) We can end up allocating more system chunks than necessary\n-\t\t *    when there are multiple tasks that are concurrently\n-\t\t *    allocating block groups, which can lead to exhaustion of\n-\t\t *    the system array in the superblock;\n-\t\t *\n-\t\t * *) If we allocate extra and unnecessary system block groups,\n-\t\t *    despite being empty for a long time, and possibly forever,\n-\t\t *    they end not being added to the list of unused block groups\n-\t\t *    because that typically happens only when deallocating the\n-\t\t *    last extent from a block group - which never happens since\n-\t\t *    we never allocate from them in the first place. The few\n-\t\t *    exceptions are when mounting a filesystem or running scrub,\n-\t\t *    which add unused block groups to the list of unused block\n-\t\t *    groups, to be deleted by the cleaner kthread.\n-\t\t *    And even when they are added to the list of unused block\n-\t\t *    groups, it can take a long time until they get deleted,\n-\t\t *    since the cleaner kthread might be sleeping or busy with\n-\t\t *    other work (deleting subvolumes, running delayed iputs,\n-\t\t *    defrag scheduling, etc);\n-\t\t *\n-\t\t * This is rare in practice, but can happen when too many tasks\n-\t\t * are allocating blocks groups in parallel (via fallocate())\n-\t\t * and before the one that reserved space for a new system block\n-\t\t * group finishes the block group creation and releases the space\n-\t\t * reserved in excess (at btrfs_create_pending_block_groups()),\n-\t\t * other tasks end up here and see free system space temporarily\n-\t\t * not enough for updating the chunk tree.\n-\t\t *\n-\t\t * We unlock the chunk mutex before waiting for such tasks and\n-\t\t * lock it again after the wait, otherwise we would deadlock.\n-\t\t * It is safe to do so because allocating a system chunk is the\n-\t\t * first thing done while allocating a new block group.\n-\t\t */\n-\t\tif (reserved > trans->chunk_bytes_reserved) {\n-\t\t\tconst u64 min_needed = reserved - thresh;\n-\n-\t\t\tmutex_unlock(&fs_info->chunk_mutex);\n-\t\t\twait_event(cur_trans->chunk_reserve_wait,\n-\t\t\t   atomic64_read(&cur_trans->chunk_bytes_reserved) <=\n-\t\t\t   min_needed);\n-\t\t\tmutex_lock(&fs_info->chunk_mutex);\n-\t\t\tgoto again;\n-\t\t}\n \n \t\t/*\n \t\t * Ignore failure to create system chunk. We might end up not\n@@ -100,9 +46,7 @@\n \t\tret = btrfs_block_rsv_add(fs_info->chunk_root,\n \t\t\t\t\t  &fs_info->chunk_block_rsv,\n \t\t\t\t\t  thresh, BTRFS_RESERVE_NO_FLUSH);\n-\t\tif (!ret) {\n-\t\t\tatomic64_add(thresh, &cur_trans->chunk_bytes_reserved);\n+\t\tif (!ret)\n \t\t\ttrans->chunk_bytes_reserved += thresh;\n-\t\t}\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct btrfs_transaction *cur_trans = trans->transaction;",
                "again:",
                "\t\tu64 reserved = atomic64_read(&cur_trans->chunk_bytes_reserved);",
                "",
                "\t\t/*",
                "\t\t * If there's not available space for the chunk tree (system",
                "\t\t * space) and there are other tasks that reserved space for",
                "\t\t * creating a new system block group, wait for them to complete",
                "\t\t * the creation of their system block group and release excess",
                "\t\t * reserved space. We do this because:",
                "\t\t *",
                "\t\t * *) We can end up allocating more system chunks than necessary",
                "\t\t *    when there are multiple tasks that are concurrently",
                "\t\t *    allocating block groups, which can lead to exhaustion of",
                "\t\t *    the system array in the superblock;",
                "\t\t *",
                "\t\t * *) If we allocate extra and unnecessary system block groups,",
                "\t\t *    despite being empty for a long time, and possibly forever,",
                "\t\t *    they end not being added to the list of unused block groups",
                "\t\t *    because that typically happens only when deallocating the",
                "\t\t *    last extent from a block group - which never happens since",
                "\t\t *    we never allocate from them in the first place. The few",
                "\t\t *    exceptions are when mounting a filesystem or running scrub,",
                "\t\t *    which add unused block groups to the list of unused block",
                "\t\t *    groups, to be deleted by the cleaner kthread.",
                "\t\t *    And even when they are added to the list of unused block",
                "\t\t *    groups, it can take a long time until they get deleted,",
                "\t\t *    since the cleaner kthread might be sleeping or busy with",
                "\t\t *    other work (deleting subvolumes, running delayed iputs,",
                "\t\t *    defrag scheduling, etc);",
                "\t\t *",
                "\t\t * This is rare in practice, but can happen when too many tasks",
                "\t\t * are allocating blocks groups in parallel (via fallocate())",
                "\t\t * and before the one that reserved space for a new system block",
                "\t\t * group finishes the block group creation and releases the space",
                "\t\t * reserved in excess (at btrfs_create_pending_block_groups()),",
                "\t\t * other tasks end up here and see free system space temporarily",
                "\t\t * not enough for updating the chunk tree.",
                "\t\t *",
                "\t\t * We unlock the chunk mutex before waiting for such tasks and",
                "\t\t * lock it again after the wait, otherwise we would deadlock.",
                "\t\t * It is safe to do so because allocating a system chunk is the",
                "\t\t * first thing done while allocating a new block group.",
                "\t\t */",
                "\t\tif (reserved > trans->chunk_bytes_reserved) {",
                "\t\t\tconst u64 min_needed = reserved - thresh;",
                "",
                "\t\t\tmutex_unlock(&fs_info->chunk_mutex);",
                "\t\t\twait_event(cur_trans->chunk_reserve_wait,",
                "\t\t\t   atomic64_read(&cur_trans->chunk_bytes_reserved) <=",
                "\t\t\t   min_needed);",
                "\t\t\tmutex_lock(&fs_info->chunk_mutex);",
                "\t\t\tgoto again;",
                "\t\t}",
                "\t\tif (!ret) {",
                "\t\t\tatomic64_add(thresh, &cur_trans->chunk_bytes_reserved);",
                "\t\t}"
            ],
            "added_lines": [
                "\t\tif (!ret)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-38203",
        "func_name": "torvalds/linux/join_transaction",
        "description": "btrfs in the Linux kernel before 5.13.4 allows attackers to cause a denial of service (deadlock) via processes that trigger allocation of new system chunks during times when there is a shortage of free space in the system space_info.",
        "git_url": "https://github.com/torvalds/linux/commit/1cb3db1cf383a3c7dbda1aa0ce748b0958759947",
        "commit_title": "btrfs: fix deadlock with concurrent chunk allocations involving system chunks",
        "commit_text": " When a task attempting to allocate a new chunk verifies that there is not currently enough free space in the system space_info and there is another task that allocated a new system chunk but it did not finish yet the creation of the respective block group, it waits for that other task to finish creating the block group. This is to avoid exhaustion of the system chunk array in the superblock, which is limited, when we have a thundering herd of tasks allocating new chunks. This problem was described and fixed by commit eafa4fd0ad0607 (\"btrfs: fix exhaustion of the system chunk array due to concurrent allocations\").  However there are two very similar scenarios where this can lead to a deadlock:  1) Task B allocated a new system chunk and task A is waiting on task B    to finish creation of the respective system block group. However before    task B ends its transaction handle and finishes the creation of the    system block group, it attempts to allocate another chunk (like a data    chunk for an fallocate operation for a very large range). Task B will    be unable to progress and allocate the new chunk, because task A set    space_info->chunk_alloc to 1 and therefore it loops at    btrfs_chunk_alloc() waiting for task A to finish its chunk allocation    and set space_info->chunk_alloc to 0, but task A is waiting on task B    to finish creation of the new system block group, therefore resulting    in a deadlock;  2) Task B allocated a new system chunk and task A is waiting on task B to    finish creation of the respective system block group. By the time that    task B enter the final phase of block group allocation, which happens    at btrfs_create_pending_block_groups(), when it modifies the extent    tree, the device tree or the chunk tree to insert the items for some    new block group, it needs to allocate a new chunk, so it ends up at    btrfs_chunk_alloc() and keeps looping there because task A has set    space_info->chunk_alloc to 1, but task A is waiting for task B to    finish creation of the new system block group and release the reserved    system space, therefore resulting in a deadlock.  In short, the problem is if a task B needs to allocate a new chunk after it previously allocated a new system chunk and if another task A is currently waiting for task B to complete the allocation of the new system chunk.  Unfortunately this deadlock scenario introduced by the previous fix for the system chunk array exhaustion problem does not have a simple and short fix, and requires a big change to rework the chunk allocation code so that chunk btree updates are all made in the first phase of chunk allocation. And since this deadlock regression is being frequently hit on zoned filesystems and the system chunk array exhaustion problem is triggered in more extreme cases (originally observed on PowerPC with a node size of 64K when running the fallocate tests from stress-ng), revert the changes from that commit. The next patch in the series, with a subject of \"btrfs: rework chunk allocation to avoid exhaustion of the system chunk array\" does the necessary changes to fix the system chunk array exhaustion problem.  Link: https://lore.kernel.org/linux-btrfs/20210621015922.ewgbffxuawia7liz@naota-xeon/",
        "func_before": "static noinline int join_transaction(struct btrfs_fs_info *fs_info,\n\t\t\t\t     unsigned int type)\n{\n\tstruct btrfs_transaction *cur_trans;\n\n\tspin_lock(&fs_info->trans_lock);\nloop:\n\t/* The file system has been taken offline. No new transactions. */\n\tif (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn -EROFS;\n\t}\n\n\tcur_trans = fs_info->running_transaction;\n\tif (cur_trans) {\n\t\tif (TRANS_ABORTED(cur_trans)) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn cur_trans->aborted;\n\t\t}\n\t\tif (btrfs_blocked_trans_types[cur_trans->state] & type) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\trefcount_inc(&cur_trans->use_count);\n\t\tatomic_inc(&cur_trans->num_writers);\n\t\textwriter_counter_inc(cur_trans, type);\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&fs_info->trans_lock);\n\n\t/*\n\t * If we are ATTACH, we just want to catch the current transaction,\n\t * and commit it. If there is no transaction, just return ENOENT.\n\t */\n\tif (type == TRANS_ATTACH)\n\t\treturn -ENOENT;\n\n\t/*\n\t * JOIN_NOLOCK only happens during the transaction commit, so\n\t * it is impossible that ->running_transaction is NULL\n\t */\n\tBUG_ON(type == TRANS_JOIN_NOLOCK);\n\n\tcur_trans = kmalloc(sizeof(*cur_trans), GFP_NOFS);\n\tif (!cur_trans)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&fs_info->trans_lock);\n\tif (fs_info->running_transaction) {\n\t\t/*\n\t\t * someone started a transaction after we unlocked.  Make sure\n\t\t * to redo the checks above\n\t\t */\n\t\tkfree(cur_trans);\n\t\tgoto loop;\n\t} else if (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\tkfree(cur_trans);\n\t\treturn -EROFS;\n\t}\n\n\tcur_trans->fs_info = fs_info;\n\tatomic_set(&cur_trans->pending_ordered, 0);\n\tinit_waitqueue_head(&cur_trans->pending_wait);\n\tatomic_set(&cur_trans->num_writers, 1);\n\textwriter_counter_init(cur_trans, type);\n\tinit_waitqueue_head(&cur_trans->writer_wait);\n\tinit_waitqueue_head(&cur_trans->commit_wait);\n\tcur_trans->state = TRANS_STATE_RUNNING;\n\t/*\n\t * One for this trans handle, one so it will live on until we\n\t * commit the transaction.\n\t */\n\trefcount_set(&cur_trans->use_count, 2);\n\tcur_trans->flags = 0;\n\tcur_trans->start_time = ktime_get_seconds();\n\n\tmemset(&cur_trans->delayed_refs, 0, sizeof(cur_trans->delayed_refs));\n\n\tcur_trans->delayed_refs.href_root = RB_ROOT_CACHED;\n\tcur_trans->delayed_refs.dirty_extent_root = RB_ROOT;\n\tatomic_set(&cur_trans->delayed_refs.num_entries, 0);\n\n\t/*\n\t * although the tree mod log is per file system and not per transaction,\n\t * the log must never go across transaction boundaries.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&fs_info->tree_mod_seq_list))\n\t\tWARN(1, KERN_ERR \"BTRFS: tree_mod_seq_list not empty when creating a fresh transaction\\n\");\n\tif (!RB_EMPTY_ROOT(&fs_info->tree_mod_log))\n\t\tWARN(1, KERN_ERR \"BTRFS: tree_mod_log rb tree not empty when creating a fresh transaction\\n\");\n\tatomic64_set(&fs_info->tree_mod_seq, 0);\n\n\tspin_lock_init(&cur_trans->delayed_refs.lock);\n\n\tINIT_LIST_HEAD(&cur_trans->pending_snapshots);\n\tINIT_LIST_HEAD(&cur_trans->dev_update_list);\n\tINIT_LIST_HEAD(&cur_trans->switch_commits);\n\tINIT_LIST_HEAD(&cur_trans->dirty_bgs);\n\tINIT_LIST_HEAD(&cur_trans->io_bgs);\n\tINIT_LIST_HEAD(&cur_trans->dropped_roots);\n\tmutex_init(&cur_trans->cache_write_mutex);\n\tspin_lock_init(&cur_trans->dirty_bgs_lock);\n\tINIT_LIST_HEAD(&cur_trans->deleted_bgs);\n\tspin_lock_init(&cur_trans->dropped_roots_lock);\n\tINIT_LIST_HEAD(&cur_trans->releasing_ebs);\n\tspin_lock_init(&cur_trans->releasing_ebs_lock);\n\tatomic64_set(&cur_trans->chunk_bytes_reserved, 0);\n\tinit_waitqueue_head(&cur_trans->chunk_reserve_wait);\n\tlist_add_tail(&cur_trans->list, &fs_info->trans_list);\n\textent_io_tree_init(fs_info, &cur_trans->dirty_pages,\n\t\t\tIO_TREE_TRANS_DIRTY_PAGES, fs_info->btree_inode);\n\textent_io_tree_init(fs_info, &cur_trans->pinned_extents,\n\t\t\tIO_TREE_FS_PINNED_EXTENTS, NULL);\n\tfs_info->generation++;\n\tcur_trans->transid = fs_info->generation;\n\tfs_info->running_transaction = cur_trans;\n\tcur_trans->aborted = 0;\n\tspin_unlock(&fs_info->trans_lock);\n\n\treturn 0;\n}",
        "func": "static noinline int join_transaction(struct btrfs_fs_info *fs_info,\n\t\t\t\t     unsigned int type)\n{\n\tstruct btrfs_transaction *cur_trans;\n\n\tspin_lock(&fs_info->trans_lock);\nloop:\n\t/* The file system has been taken offline. No new transactions. */\n\tif (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn -EROFS;\n\t}\n\n\tcur_trans = fs_info->running_transaction;\n\tif (cur_trans) {\n\t\tif (TRANS_ABORTED(cur_trans)) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn cur_trans->aborted;\n\t\t}\n\t\tif (btrfs_blocked_trans_types[cur_trans->state] & type) {\n\t\t\tspin_unlock(&fs_info->trans_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\trefcount_inc(&cur_trans->use_count);\n\t\tatomic_inc(&cur_trans->num_writers);\n\t\textwriter_counter_inc(cur_trans, type);\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&fs_info->trans_lock);\n\n\t/*\n\t * If we are ATTACH, we just want to catch the current transaction,\n\t * and commit it. If there is no transaction, just return ENOENT.\n\t */\n\tif (type == TRANS_ATTACH)\n\t\treturn -ENOENT;\n\n\t/*\n\t * JOIN_NOLOCK only happens during the transaction commit, so\n\t * it is impossible that ->running_transaction is NULL\n\t */\n\tBUG_ON(type == TRANS_JOIN_NOLOCK);\n\n\tcur_trans = kmalloc(sizeof(*cur_trans), GFP_NOFS);\n\tif (!cur_trans)\n\t\treturn -ENOMEM;\n\n\tspin_lock(&fs_info->trans_lock);\n\tif (fs_info->running_transaction) {\n\t\t/*\n\t\t * someone started a transaction after we unlocked.  Make sure\n\t\t * to redo the checks above\n\t\t */\n\t\tkfree(cur_trans);\n\t\tgoto loop;\n\t} else if (test_bit(BTRFS_FS_STATE_ERROR, &fs_info->fs_state)) {\n\t\tspin_unlock(&fs_info->trans_lock);\n\t\tkfree(cur_trans);\n\t\treturn -EROFS;\n\t}\n\n\tcur_trans->fs_info = fs_info;\n\tatomic_set(&cur_trans->pending_ordered, 0);\n\tinit_waitqueue_head(&cur_trans->pending_wait);\n\tatomic_set(&cur_trans->num_writers, 1);\n\textwriter_counter_init(cur_trans, type);\n\tinit_waitqueue_head(&cur_trans->writer_wait);\n\tinit_waitqueue_head(&cur_trans->commit_wait);\n\tcur_trans->state = TRANS_STATE_RUNNING;\n\t/*\n\t * One for this trans handle, one so it will live on until we\n\t * commit the transaction.\n\t */\n\trefcount_set(&cur_trans->use_count, 2);\n\tcur_trans->flags = 0;\n\tcur_trans->start_time = ktime_get_seconds();\n\n\tmemset(&cur_trans->delayed_refs, 0, sizeof(cur_trans->delayed_refs));\n\n\tcur_trans->delayed_refs.href_root = RB_ROOT_CACHED;\n\tcur_trans->delayed_refs.dirty_extent_root = RB_ROOT;\n\tatomic_set(&cur_trans->delayed_refs.num_entries, 0);\n\n\t/*\n\t * although the tree mod log is per file system and not per transaction,\n\t * the log must never go across transaction boundaries.\n\t */\n\tsmp_mb();\n\tif (!list_empty(&fs_info->tree_mod_seq_list))\n\t\tWARN(1, KERN_ERR \"BTRFS: tree_mod_seq_list not empty when creating a fresh transaction\\n\");\n\tif (!RB_EMPTY_ROOT(&fs_info->tree_mod_log))\n\t\tWARN(1, KERN_ERR \"BTRFS: tree_mod_log rb tree not empty when creating a fresh transaction\\n\");\n\tatomic64_set(&fs_info->tree_mod_seq, 0);\n\n\tspin_lock_init(&cur_trans->delayed_refs.lock);\n\n\tINIT_LIST_HEAD(&cur_trans->pending_snapshots);\n\tINIT_LIST_HEAD(&cur_trans->dev_update_list);\n\tINIT_LIST_HEAD(&cur_trans->switch_commits);\n\tINIT_LIST_HEAD(&cur_trans->dirty_bgs);\n\tINIT_LIST_HEAD(&cur_trans->io_bgs);\n\tINIT_LIST_HEAD(&cur_trans->dropped_roots);\n\tmutex_init(&cur_trans->cache_write_mutex);\n\tspin_lock_init(&cur_trans->dirty_bgs_lock);\n\tINIT_LIST_HEAD(&cur_trans->deleted_bgs);\n\tspin_lock_init(&cur_trans->dropped_roots_lock);\n\tINIT_LIST_HEAD(&cur_trans->releasing_ebs);\n\tspin_lock_init(&cur_trans->releasing_ebs_lock);\n\tlist_add_tail(&cur_trans->list, &fs_info->trans_list);\n\textent_io_tree_init(fs_info, &cur_trans->dirty_pages,\n\t\t\tIO_TREE_TRANS_DIRTY_PAGES, fs_info->btree_inode);\n\textent_io_tree_init(fs_info, &cur_trans->pinned_extents,\n\t\t\tIO_TREE_FS_PINNED_EXTENTS, NULL);\n\tfs_info->generation++;\n\tcur_trans->transid = fs_info->generation;\n\tfs_info->running_transaction = cur_trans;\n\tcur_trans->aborted = 0;\n\tspin_unlock(&fs_info->trans_lock);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -107,8 +107,6 @@\n \tspin_lock_init(&cur_trans->dropped_roots_lock);\n \tINIT_LIST_HEAD(&cur_trans->releasing_ebs);\n \tspin_lock_init(&cur_trans->releasing_ebs_lock);\n-\tatomic64_set(&cur_trans->chunk_bytes_reserved, 0);\n-\tinit_waitqueue_head(&cur_trans->chunk_reserve_wait);\n \tlist_add_tail(&cur_trans->list, &fs_info->trans_list);\n \textent_io_tree_init(fs_info, &cur_trans->dirty_pages,\n \t\t\tIO_TREE_TRANS_DIRTY_PAGES, fs_info->btree_inode);",
        "diff_line_info": {
            "deleted_lines": [
                "\tatomic64_set(&cur_trans->chunk_bytes_reserved, 0);",
                "\tinit_waitqueue_head(&cur_trans->chunk_reserve_wait);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-38203",
        "func_name": "torvalds/linux/btrfs_trans_release_chunk_metadata",
        "description": "btrfs in the Linux kernel before 5.13.4 allows attackers to cause a denial of service (deadlock) via processes that trigger allocation of new system chunks during times when there is a shortage of free space in the system space_info.",
        "git_url": "https://github.com/torvalds/linux/commit/1cb3db1cf383a3c7dbda1aa0ce748b0958759947",
        "commit_title": "btrfs: fix deadlock with concurrent chunk allocations involving system chunks",
        "commit_text": " When a task attempting to allocate a new chunk verifies that there is not currently enough free space in the system space_info and there is another task that allocated a new system chunk but it did not finish yet the creation of the respective block group, it waits for that other task to finish creating the block group. This is to avoid exhaustion of the system chunk array in the superblock, which is limited, when we have a thundering herd of tasks allocating new chunks. This problem was described and fixed by commit eafa4fd0ad0607 (\"btrfs: fix exhaustion of the system chunk array due to concurrent allocations\").  However there are two very similar scenarios where this can lead to a deadlock:  1) Task B allocated a new system chunk and task A is waiting on task B    to finish creation of the respective system block group. However before    task B ends its transaction handle and finishes the creation of the    system block group, it attempts to allocate another chunk (like a data    chunk for an fallocate operation for a very large range). Task B will    be unable to progress and allocate the new chunk, because task A set    space_info->chunk_alloc to 1 and therefore it loops at    btrfs_chunk_alloc() waiting for task A to finish its chunk allocation    and set space_info->chunk_alloc to 0, but task A is waiting on task B    to finish creation of the new system block group, therefore resulting    in a deadlock;  2) Task B allocated a new system chunk and task A is waiting on task B to    finish creation of the respective system block group. By the time that    task B enter the final phase of block group allocation, which happens    at btrfs_create_pending_block_groups(), when it modifies the extent    tree, the device tree or the chunk tree to insert the items for some    new block group, it needs to allocate a new chunk, so it ends up at    btrfs_chunk_alloc() and keeps looping there because task A has set    space_info->chunk_alloc to 1, but task A is waiting for task B to    finish creation of the new system block group and release the reserved    system space, therefore resulting in a deadlock.  In short, the problem is if a task B needs to allocate a new chunk after it previously allocated a new system chunk and if another task A is currently waiting for task B to complete the allocation of the new system chunk.  Unfortunately this deadlock scenario introduced by the previous fix for the system chunk array exhaustion problem does not have a simple and short fix, and requires a big change to rework the chunk allocation code so that chunk btree updates are all made in the first phase of chunk allocation. And since this deadlock regression is being frequently hit on zoned filesystems and the system chunk array exhaustion problem is triggered in more extreme cases (originally observed on PowerPC with a node size of 64K when running the fallocate tests from stress-ng), revert the changes from that commit. The next patch in the series, with a subject of \"btrfs: rework chunk allocation to avoid exhaustion of the system chunk array\" does the necessary changes to fix the system chunk array exhaustion problem.  Link: https://lore.kernel.org/linux-btrfs/20210621015922.ewgbffxuawia7liz@naota-xeon/",
        "func_before": "void btrfs_trans_release_chunk_metadata(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_transaction *cur_trans = trans->transaction;\n\n\tif (!trans->chunk_bytes_reserved)\n\t\treturn;\n\n\tWARN_ON_ONCE(!list_empty(&trans->new_bgs));\n\n\tbtrfs_block_rsv_release(fs_info, &fs_info->chunk_block_rsv,\n\t\t\t\ttrans->chunk_bytes_reserved, NULL);\n\tatomic64_sub(trans->chunk_bytes_reserved, &cur_trans->chunk_bytes_reserved);\n\tcond_wake_up(&cur_trans->chunk_reserve_wait);\n\ttrans->chunk_bytes_reserved = 0;\n}",
        "func": "void btrfs_trans_release_chunk_metadata(struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\n\tif (!trans->chunk_bytes_reserved)\n\t\treturn;\n\n\tWARN_ON_ONCE(!list_empty(&trans->new_bgs));\n\n\tbtrfs_block_rsv_release(fs_info, &fs_info->chunk_block_rsv,\n\t\t\t\ttrans->chunk_bytes_reserved, NULL);\n\ttrans->chunk_bytes_reserved = 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,6 @@\n void btrfs_trans_release_chunk_metadata(struct btrfs_trans_handle *trans)\n {\n \tstruct btrfs_fs_info *fs_info = trans->fs_info;\n-\tstruct btrfs_transaction *cur_trans = trans->transaction;\n \n \tif (!trans->chunk_bytes_reserved)\n \t\treturn;\n@@ -10,7 +9,5 @@\n \n \tbtrfs_block_rsv_release(fs_info, &fs_info->chunk_block_rsv,\n \t\t\t\ttrans->chunk_bytes_reserved, NULL);\n-\tatomic64_sub(trans->chunk_bytes_reserved, &cur_trans->chunk_bytes_reserved);\n-\tcond_wake_up(&cur_trans->chunk_reserve_wait);\n \ttrans->chunk_bytes_reserved = 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tstruct btrfs_transaction *cur_trans = trans->transaction;",
                "\tatomic64_sub(trans->chunk_bytes_reserved, &cur_trans->chunk_bytes_reserved);",
                "\tcond_wake_up(&cur_trans->chunk_reserve_wait);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-43395",
        "func_name": "illumos/illumos-gate/tdirenter",
        "description": "An issue was discovered in illumos before f859e7171bb5db34321e45585839c6c3200ebb90, OmniOS Community Edition r151038, OpenIndiana Hipster 2021.04, and SmartOS 20210923. A local unprivileged user can cause a deadlock and kernel panic via crafted rename and rmdir calls on tmpfs filesystems. Oracle Solaris 10 and 11 is also affected.",
        "git_url": "https://github.com/illumos/illumos-gate/commit/f859e7171bb5db34321e45585839c6c3200ebb90",
        "commit_title": "14424 tmpfs can be induced to deadlock",
        "commit_text": "Reviewed by: Robert Mustacchi <rm@fingolfin.org> Reviewed by: Andy Fiddaman <andy@omnios.org> Reviewed by: Mike Zeller <mike.zeller@joyent.com> Approved by: Robert Mustacchi <rm@fingolfin.org>",
        "func_before": "int\ntdirenter(\n\tstruct tmount\t*tm,\n\tstruct tmpnode\t*dir,\t\t/* target directory to make entry in */\n\tchar\t\t*name,\t\t/* name of entry */\n\tenum de_op\top,\t\t/* entry operation */\n\tstruct tmpnode\t*fromparent,\t/* source directory if rename */\n\tstruct tmpnode\t*tp,\t\t/* source tmpnode, if link/rename */\n\tstruct vattr\t*va,\n\tstruct tmpnode\t**tpp,\t\t/* return tmpnode, if create/mkdir */\n\tstruct cred\t*cred,\n\tcaller_context_t *ctp)\n{\n\tstruct tdirent *tdp;\n\tstruct tmpnode *found = NULL;\n\tint error = 0;\n\tchar *s;\n\n\t/*\n\t * tn_rwlock is held to serialize direnter and dirdeletes\n\t */\n\tASSERT(RW_WRITE_HELD(&dir->tn_rwlock));\n\tASSERT(dir->tn_type == VDIR);\n\n\t/*\n\t * Don't allow '/' characters in pathname component\n\t * (thus in ufs_direnter()).\n\t */\n\tfor (s = name; *s; s++)\n\t\tif (*s == '/')\n\t\t\treturn (EACCES);\n\n\tif (name[0] == '\\0')\n\t\tpanic(\"tdirenter: NULL name\");\n\n\t/*\n\t * For link and rename lock the source entry and check the link count\n\t * to see if it has been removed while it was unlocked.\n\t */\n\tif (op == DE_LINK || op == DE_RENAME) {\n\t\tif (tp != dir)\n\t\t\trw_enter(&tp->tn_rwlock, RW_WRITER);\n\t\tmutex_enter(&tp->tn_tlock);\n\t\tif (tp->tn_nlink == 0) {\n\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\tif (tp != dir)\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\treturn (ENOENT);\n\t\t}\n\n\t\tif (tp->tn_nlink == MAXLINK) {\n\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\tif (tp != dir)\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\treturn (EMLINK);\n\t\t}\n\t\ttp->tn_nlink++;\n\t\tgethrestime(&tp->tn_ctime);\n\t\tmutex_exit(&tp->tn_tlock);\n\t\tif (tp != dir)\n\t\t\trw_exit(&tp->tn_rwlock);\n\t}\n\n\t/*\n\t * This might be a \"dangling detached directory\".\n\t * it could have been removed, but a reference\n\t * to it kept in u_cwd.  don't bother searching\n\t * it, and with any luck the user will get tired\n\t * of dealing with us and cd to some absolute\n\t * pathway.  *sigh*, thus in ufs, too.\n\t */\n\tif (dir->tn_nlink == 0) {\n\t\terror = ENOENT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this is a rename of a directory and the parent is\n\t * different (\"..\" must be changed), then the source\n\t * directory must not be in the directory hierarchy\n\t * above the target, as this would orphan everything\n\t * below the source directory.\n\t */\n\tif (op == DE_RENAME) {\n\t\tif (tp == dir) {\n\t\t\terror = EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tp->tn_type == VDIR) {\n\t\t\tif ((fromparent != dir) &&\n\t\t\t    (error = tdircheckpath(tp, dir, cred))) {\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Search for the entry.  Return \"found\" if it exists.\n\t */\n\ttdp = tmpfs_hash_lookup(name, dir, 1, &found);\n\n\tif (tdp) {\n\t\tASSERT(found);\n\t\tswitch (op) {\n\t\tcase DE_CREATE:\n\t\tcase DE_MKDIR:\n\t\t\tif (tpp) {\n\t\t\t\t*tpp = found;\n\t\t\t\terror = EEXIST;\n\t\t\t} else {\n\t\t\t\ttmpnode_rele(found);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase DE_RENAME:\n\t\t\terror = tdirrename(fromparent, tp,\n\t\t\t    dir, name, found, tdp, cred);\n\t\t\tif (error == 0) {\n\t\t\t\tif (found != NULL) {\n\t\t\t\t\tvnevent_rename_dest(TNTOV(found),\n\t\t\t\t\t    TNTOV(dir), name, ctp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttmpnode_rele(found);\n\t\t\tbreak;\n\n\t\tcase DE_LINK:\n\t\t\t/*\n\t\t\t * Can't link to an existing file.\n\t\t\t */\n\t\t\terror = EEXIST;\n\t\t\ttmpnode_rele(found);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\n\t\t/*\n\t\t * The entry does not exist. Check write permission in\n\t\t * directory to see if entry can be created.\n\t\t */\n\t\tif (error = tmp_taccess(dir, VWRITE, cred))\n\t\t\tgoto out;\n\t\tif (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\t/*\n\t\t\t * Make new tmpnode and directory entry as required.\n\t\t\t */\n\t\t\terror = tdirmaketnode(dir, tm, va, op, &tp, cred);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (error = tdiraddentry(dir, tp, name, op, fromparent)) {\n\t\t\tif (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\t\t/*\n\t\t\t\t * Unmake the inode we just made.\n\t\t\t\t */\n\t\t\t\trw_enter(&tp->tn_rwlock, RW_WRITER);\n\t\t\t\tif ((tp->tn_type) == VDIR) {\n\t\t\t\t\tASSERT(tdp == NULL);\n\t\t\t\t\t/*\n\t\t\t\t\t * cleanup allocs made by tdirinit()\n\t\t\t\t\t */\n\t\t\t\t\ttdirtrunc(tp);\n\t\t\t\t}\n\t\t\t\tmutex_enter(&tp->tn_tlock);\n\t\t\t\ttp->tn_nlink = 0;\n\t\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\t\tgethrestime(&tp->tn_ctime);\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\t\ttmpnode_rele(tp);\n\t\t\t\ttp = NULL;\n\t\t\t}\n\t\t} else if (tpp) {\n\t\t\t*tpp = tp;\n\t\t} else if (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\ttmpnode_rele(tp);\n\t\t}\n\t}\n\nout:\n\tif (error && (op == DE_LINK || op == DE_RENAME)) {\n\t\t/*\n\t\t * Undo bumped link count.\n\t\t */\n\t\tDECR_COUNT(&tp->tn_nlink, &tp->tn_tlock);\n\t\tgethrestime(&tp->tn_ctime);\n\t}\n\treturn (error);\n}",
        "func": "int\ntdirenter(\n\tstruct tmount\t*tm,\n\tstruct tmpnode\t*dir,\t\t/* target directory to make entry in */\n\tchar\t\t*name,\t\t/* name of entry */\n\tenum de_op\top,\t\t/* entry operation */\n\tstruct tmpnode\t*fromparent,\t/* source directory if rename */\n\tstruct tmpnode\t*tp,\t\t/* source tmpnode, if link/rename */\n\tstruct vattr\t*va,\n\tstruct tmpnode\t**tpp,\t\t/* return tmpnode, if create/mkdir */\n\tstruct cred\t*cred,\n\tcaller_context_t *ctp)\n{\n\tstruct tdirent *tdp;\n\tstruct tmpnode *found = NULL;\n\tint error = 0;\n\tchar *s;\n\n\t/*\n\t * tn_rwlock is held to serialize direnter and dirdeletes\n\t */\n\tASSERT(RW_WRITE_HELD(&dir->tn_rwlock));\n\tASSERT(dir->tn_type == VDIR);\n\n\t/*\n\t * Don't allow '/' characters in pathname component\n\t * (thus in ufs_direnter()).\n\t */\n\tfor (s = name; *s; s++)\n\t\tif (*s == '/')\n\t\t\treturn (EACCES);\n\n\tif (name[0] == '\\0')\n\t\tpanic(\"tdirenter: NULL name\");\n\n\t/*\n\t * For link and rename lock the source entry and check the link count\n\t * to see if it has been removed while it was unlocked.\n\t */\n\tif (op == DE_LINK || op == DE_RENAME) {\n\t\tif (tp != dir) {\n\t\t\tunsigned int tries = 0;\n\n\t\t\t/*\n\t\t\t * If we are acquiring tp->tn_rwlock (for SOURCE)\n\t\t\t * inside here, we must consider the following:\n\t\t\t *\n\t\t\t * - dir->tn_rwlock (TARGET) is already HELD (see\n\t\t\t * above ASSERT()).\n\t\t\t *\n\t\t\t * - It is possible our SOURCE is a parent of our\n\t\t\t * TARGET. Yes it's unusual, but it will return an\n\t\t\t * error below via tdircheckpath().\n\t\t\t *\n\t\t\t * - It is also possible that another thread,\n\t\t\t * concurrent to this one, is performing\n\t\t\t * rmdir(TARGET), which means it will first acquire\n\t\t\t * SOURCE's lock, THEN acquire TARGET's lock, which\n\t\t\t * could result in this thread holding TARGET and\n\t\t\t * trying for SOURCE, but the other thread holding\n\t\t\t * SOURCE and trying for TARGET.  This is deadlock,\n\t\t\t * and it's inducible.\n\t\t\t *\n\t\t\t * To prevent this, we borrow some techniques from UFS\n\t\t\t * and rw_tryenter(), delaying if we fail, and\n\t\t\t * if someone tweaks the number of backoff tries to be\n\t\t\t * nonzero, return EBUSY after that number of tries.\n\t\t\t */\n\t\t\twhile (!rw_tryenter(&tp->tn_rwlock, RW_WRITER)) {\n\t\t\t\t/*\n\t\t\t\t * Sloppy, but this is a diagnostic so atomic\n\t\t\t\t * increment would be overkill.\n\t\t\t\t */\n\t\t\t\ttmpfs_rename_loops++;\n\n\t\t\t\tif (tmpfs_rename_backoff_tries != 0) {\n\t\t\t\t\tif (tries > tmpfs_rename_backoff_tries)\n\t\t\t\t\t\treturn (EBUSY);\n\t\t\t\t\ttries++;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * NOTE: We're still holding dir->tn_rwlock,\n\t\t\t\t * so drop it over the delay, so any other\n\t\t\t\t * thread can get its business done.\n\t\t\t\t *\n\t\t\t\t * No state change or state inspection happens\n\t\t\t\t * prior to here, so it is not wholly dangerous\n\t\t\t\t * to release-and-reacquire dir->tn_rwlock.\n\t\t\t\t *\n\t\t\t\t * Hold the vnode of dir in case it gets\n\t\t\t\t * released by another thread, though.\n\t\t\t\t */\n\t\t\t\tVN_HOLD(TNTOV(dir));\n\t\t\t\trw_exit(&dir->tn_rwlock);\n\t\t\t\tdelay(tmpfs_rename_backoff_delay);\n\t\t\t\trw_enter(&dir->tn_rwlock, RW_WRITER);\n\t\t\t\tVN_RELE(TNTOV(dir));\n\t\t\t}\n\t\t}\n\t\tmutex_enter(&tp->tn_tlock);\n\t\tif (tp->tn_nlink == 0) {\n\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\tif (tp != dir)\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\treturn (ENOENT);\n\t\t}\n\n\t\tif (tp->tn_nlink == MAXLINK) {\n\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\tif (tp != dir)\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\treturn (EMLINK);\n\t\t}\n\t\ttp->tn_nlink++;\n\t\tgethrestime(&tp->tn_ctime);\n\t\tmutex_exit(&tp->tn_tlock);\n\t\tif (tp != dir)\n\t\t\trw_exit(&tp->tn_rwlock);\n\t}\n\n\t/*\n\t * This might be a \"dangling detached directory\".\n\t * it could have been removed, but a reference\n\t * to it kept in u_cwd.  don't bother searching\n\t * it, and with any luck the user will get tired\n\t * of dealing with us and cd to some absolute\n\t * pathway.  *sigh*, thus in ufs, too.\n\t */\n\tif (dir->tn_nlink == 0) {\n\t\terror = ENOENT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this is a rename of a directory and the parent is\n\t * different (\"..\" must be changed), then the source\n\t * directory must not be in the directory hierarchy\n\t * above the target, as this would orphan everything\n\t * below the source directory.\n\t */\n\tif (op == DE_RENAME) {\n\t\tif (tp == dir) {\n\t\t\terror = EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tp->tn_type == VDIR) {\n\t\t\tif ((fromparent != dir) &&\n\t\t\t    (error = tdircheckpath(tp, dir, cred))) {\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Search for the entry.  Return \"found\" if it exists.\n\t */\n\ttdp = tmpfs_hash_lookup(name, dir, 1, &found);\n\n\tif (tdp) {\n\t\tASSERT(found);\n\t\tswitch (op) {\n\t\tcase DE_CREATE:\n\t\tcase DE_MKDIR:\n\t\t\tif (tpp) {\n\t\t\t\t*tpp = found;\n\t\t\t\terror = EEXIST;\n\t\t\t} else {\n\t\t\t\ttmpnode_rele(found);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase DE_RENAME:\n\t\t\terror = tdirrename(fromparent, tp,\n\t\t\t    dir, name, found, tdp, cred);\n\t\t\tif (error == 0) {\n\t\t\t\tif (found != NULL) {\n\t\t\t\t\tvnevent_rename_dest(TNTOV(found),\n\t\t\t\t\t    TNTOV(dir), name, ctp);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\ttmpnode_rele(found);\n\t\t\tbreak;\n\n\t\tcase DE_LINK:\n\t\t\t/*\n\t\t\t * Can't link to an existing file.\n\t\t\t */\n\t\t\terror = EEXIST;\n\t\t\ttmpnode_rele(found);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\n\t\t/*\n\t\t * The entry does not exist. Check write permission in\n\t\t * directory to see if entry can be created.\n\t\t */\n\t\tif (error = tmp_taccess(dir, VWRITE, cred))\n\t\t\tgoto out;\n\t\tif (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\t/*\n\t\t\t * Make new tmpnode and directory entry as required.\n\t\t\t */\n\t\t\terror = tdirmaketnode(dir, tm, va, op, &tp, cred);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t}\n\t\tif (error = tdiraddentry(dir, tp, name, op, fromparent)) {\n\t\t\tif (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\t\t/*\n\t\t\t\t * Unmake the inode we just made.\n\t\t\t\t */\n\t\t\t\trw_enter(&tp->tn_rwlock, RW_WRITER);\n\t\t\t\tif ((tp->tn_type) == VDIR) {\n\t\t\t\t\tASSERT(tdp == NULL);\n\t\t\t\t\t/*\n\t\t\t\t\t * cleanup allocs made by tdirinit()\n\t\t\t\t\t */\n\t\t\t\t\ttdirtrunc(tp);\n\t\t\t\t}\n\t\t\t\tmutex_enter(&tp->tn_tlock);\n\t\t\t\ttp->tn_nlink = 0;\n\t\t\t\tmutex_exit(&tp->tn_tlock);\n\t\t\t\tgethrestime(&tp->tn_ctime);\n\t\t\t\trw_exit(&tp->tn_rwlock);\n\t\t\t\ttmpnode_rele(tp);\n\t\t\t\ttp = NULL;\n\t\t\t}\n\t\t} else if (tpp) {\n\t\t\t*tpp = tp;\n\t\t} else if (op == DE_CREATE || op == DE_MKDIR) {\n\t\t\ttmpnode_rele(tp);\n\t\t}\n\t}\n\nout:\n\tif (error && (op == DE_LINK || op == DE_RENAME)) {\n\t\t/*\n\t\t * Undo bumped link count.\n\t\t */\n\t\tDECR_COUNT(&tp->tn_nlink, &tp->tn_tlock);\n\t\tgethrestime(&tp->tn_ctime);\n\t}\n\treturn (error);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,8 +38,65 @@\n \t * to see if it has been removed while it was unlocked.\n \t */\n \tif (op == DE_LINK || op == DE_RENAME) {\n-\t\tif (tp != dir)\n-\t\t\trw_enter(&tp->tn_rwlock, RW_WRITER);\n+\t\tif (tp != dir) {\n+\t\t\tunsigned int tries = 0;\n+\n+\t\t\t/*\n+\t\t\t * If we are acquiring tp->tn_rwlock (for SOURCE)\n+\t\t\t * inside here, we must consider the following:\n+\t\t\t *\n+\t\t\t * - dir->tn_rwlock (TARGET) is already HELD (see\n+\t\t\t * above ASSERT()).\n+\t\t\t *\n+\t\t\t * - It is possible our SOURCE is a parent of our\n+\t\t\t * TARGET. Yes it's unusual, but it will return an\n+\t\t\t * error below via tdircheckpath().\n+\t\t\t *\n+\t\t\t * - It is also possible that another thread,\n+\t\t\t * concurrent to this one, is performing\n+\t\t\t * rmdir(TARGET), which means it will first acquire\n+\t\t\t * SOURCE's lock, THEN acquire TARGET's lock, which\n+\t\t\t * could result in this thread holding TARGET and\n+\t\t\t * trying for SOURCE, but the other thread holding\n+\t\t\t * SOURCE and trying for TARGET.  This is deadlock,\n+\t\t\t * and it's inducible.\n+\t\t\t *\n+\t\t\t * To prevent this, we borrow some techniques from UFS\n+\t\t\t * and rw_tryenter(), delaying if we fail, and\n+\t\t\t * if someone tweaks the number of backoff tries to be\n+\t\t\t * nonzero, return EBUSY after that number of tries.\n+\t\t\t */\n+\t\t\twhile (!rw_tryenter(&tp->tn_rwlock, RW_WRITER)) {\n+\t\t\t\t/*\n+\t\t\t\t * Sloppy, but this is a diagnostic so atomic\n+\t\t\t\t * increment would be overkill.\n+\t\t\t\t */\n+\t\t\t\ttmpfs_rename_loops++;\n+\n+\t\t\t\tif (tmpfs_rename_backoff_tries != 0) {\n+\t\t\t\t\tif (tries > tmpfs_rename_backoff_tries)\n+\t\t\t\t\t\treturn (EBUSY);\n+\t\t\t\t\ttries++;\n+\t\t\t\t}\n+\t\t\t\t/*\n+\t\t\t\t * NOTE: We're still holding dir->tn_rwlock,\n+\t\t\t\t * so drop it over the delay, so any other\n+\t\t\t\t * thread can get its business done.\n+\t\t\t\t *\n+\t\t\t\t * No state change or state inspection happens\n+\t\t\t\t * prior to here, so it is not wholly dangerous\n+\t\t\t\t * to release-and-reacquire dir->tn_rwlock.\n+\t\t\t\t *\n+\t\t\t\t * Hold the vnode of dir in case it gets\n+\t\t\t\t * released by another thread, though.\n+\t\t\t\t */\n+\t\t\t\tVN_HOLD(TNTOV(dir));\n+\t\t\t\trw_exit(&dir->tn_rwlock);\n+\t\t\t\tdelay(tmpfs_rename_backoff_delay);\n+\t\t\t\trw_enter(&dir->tn_rwlock, RW_WRITER);\n+\t\t\t\tVN_RELE(TNTOV(dir));\n+\t\t\t}\n+\t\t}\n \t\tmutex_enter(&tp->tn_tlock);\n \t\tif (tp->tn_nlink == 0) {\n \t\t\tmutex_exit(&tp->tn_tlock);",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (tp != dir)",
                "\t\t\trw_enter(&tp->tn_rwlock, RW_WRITER);"
            ],
            "added_lines": [
                "\t\tif (tp != dir) {",
                "\t\t\tunsigned int tries = 0;",
                "",
                "\t\t\t/*",
                "\t\t\t * If we are acquiring tp->tn_rwlock (for SOURCE)",
                "\t\t\t * inside here, we must consider the following:",
                "\t\t\t *",
                "\t\t\t * - dir->tn_rwlock (TARGET) is already HELD (see",
                "\t\t\t * above ASSERT()).",
                "\t\t\t *",
                "\t\t\t * - It is possible our SOURCE is a parent of our",
                "\t\t\t * TARGET. Yes it's unusual, but it will return an",
                "\t\t\t * error below via tdircheckpath().",
                "\t\t\t *",
                "\t\t\t * - It is also possible that another thread,",
                "\t\t\t * concurrent to this one, is performing",
                "\t\t\t * rmdir(TARGET), which means it will first acquire",
                "\t\t\t * SOURCE's lock, THEN acquire TARGET's lock, which",
                "\t\t\t * could result in this thread holding TARGET and",
                "\t\t\t * trying for SOURCE, but the other thread holding",
                "\t\t\t * SOURCE and trying for TARGET.  This is deadlock,",
                "\t\t\t * and it's inducible.",
                "\t\t\t *",
                "\t\t\t * To prevent this, we borrow some techniques from UFS",
                "\t\t\t * and rw_tryenter(), delaying if we fail, and",
                "\t\t\t * if someone tweaks the number of backoff tries to be",
                "\t\t\t * nonzero, return EBUSY after that number of tries.",
                "\t\t\t */",
                "\t\t\twhile (!rw_tryenter(&tp->tn_rwlock, RW_WRITER)) {",
                "\t\t\t\t/*",
                "\t\t\t\t * Sloppy, but this is a diagnostic so atomic",
                "\t\t\t\t * increment would be overkill.",
                "\t\t\t\t */",
                "\t\t\t\ttmpfs_rename_loops++;",
                "",
                "\t\t\t\tif (tmpfs_rename_backoff_tries != 0) {",
                "\t\t\t\t\tif (tries > tmpfs_rename_backoff_tries)",
                "\t\t\t\t\t\treturn (EBUSY);",
                "\t\t\t\t\ttries++;",
                "\t\t\t\t}",
                "\t\t\t\t/*",
                "\t\t\t\t * NOTE: We're still holding dir->tn_rwlock,",
                "\t\t\t\t * so drop it over the delay, so any other",
                "\t\t\t\t * thread can get its business done.",
                "\t\t\t\t *",
                "\t\t\t\t * No state change or state inspection happens",
                "\t\t\t\t * prior to here, so it is not wholly dangerous",
                "\t\t\t\t * to release-and-reacquire dir->tn_rwlock.",
                "\t\t\t\t *",
                "\t\t\t\t * Hold the vnode of dir in case it gets",
                "\t\t\t\t * released by another thread, though.",
                "\t\t\t\t */",
                "\t\t\t\tVN_HOLD(TNTOV(dir));",
                "\t\t\t\trw_exit(&dir->tn_rwlock);",
                "\t\t\t\tdelay(tmpfs_rename_backoff_delay);",
                "\t\t\t\trw_enter(&dir->tn_rwlock, RW_WRITER);",
                "\t\t\t\tVN_RELE(TNTOV(dir));",
                "\t\t\t}",
                "\t\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-3667",
        "func_name": "libvirt/storagePoolLookupByTargetPath",
        "description": "An improper locking issue was found in the virStoragePoolLookupByTargetPath API of libvirt. It occurs in the storagePoolLookupByTargetPath function where a locked virStoragePoolObj object is not properly released on ACL permission failure. Clients connecting to the read-write socket with limited ACL permissions could use this flaw to acquire the lock and prevent other users from accessing storage pool/volume APIs, resulting in a denial of service condition. The highest threat from this vulnerability is to system availability.",
        "git_url": "https://gitlab.com/libvirt/libvirt/-/commit/447f69dec47e1b0bd15ecd7cd49a9fd3b050fb87",
        "commit_title": "storage_driver: Unlock object on ACL fail in storagePoolLookupByTargetPath",
        "commit_text": " 'virStoragePoolObjListSearch' returns a locked and refed object, thus we must release it on ACL permission failure.  Resolves: https://bugzilla.redhat.com/show_bug.cgi?id=1984318 ",
        "func_before": "virStoragePoolPtr\nstoragePoolLookupByTargetPath(virConnectPtr conn,\n                              const char *path)\n{\n    virStoragePoolObj *obj;\n    virStoragePoolDef *def;\n    virStoragePoolPtr pool = NULL;\n    g_autofree char *cleanpath = NULL;\n\n    cleanpath = virFileSanitizePath(path);\n    if (!cleanpath)\n        return NULL;\n\n    if ((obj = virStoragePoolObjListSearch(driver->pools,\n                                           storagePoolLookupByTargetPathCallback,\n                                           cleanpath))) {\n        def = virStoragePoolObjGetDef(obj);\n        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0)\n            return NULL;\n\n        pool = virGetStoragePool(conn, def->name, def->uuid, NULL, NULL);\n        virStoragePoolObjEndAPI(&obj);\n    }\n\n    if (!pool) {\n        if (STREQ(path, cleanpath)) {\n            virReportError(VIR_ERR_NO_STORAGE_POOL,\n                           _(\"no storage pool with matching target path '%s'\"),\n                           path);\n        } else {\n            virReportError(VIR_ERR_NO_STORAGE_POOL,\n                           _(\"no storage pool with matching target path '%s' (%s)\"),\n                           path, cleanpath);\n        }\n    }\n\n    return pool;\n}",
        "func": "virStoragePoolPtr\nstoragePoolLookupByTargetPath(virConnectPtr conn,\n                              const char *path)\n{\n    virStoragePoolObj *obj;\n    virStoragePoolDef *def;\n    virStoragePoolPtr pool = NULL;\n    g_autofree char *cleanpath = NULL;\n\n    cleanpath = virFileSanitizePath(path);\n    if (!cleanpath)\n        return NULL;\n\n    if ((obj = virStoragePoolObjListSearch(driver->pools,\n                                           storagePoolLookupByTargetPathCallback,\n                                           cleanpath))) {\n        def = virStoragePoolObjGetDef(obj);\n        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0) {\n            virStoragePoolObjEndAPI(&obj);\n            return NULL;\n        }\n\n        pool = virGetStoragePool(conn, def->name, def->uuid, NULL, NULL);\n        virStoragePoolObjEndAPI(&obj);\n    }\n\n    if (!pool) {\n        if (STREQ(path, cleanpath)) {\n            virReportError(VIR_ERR_NO_STORAGE_POOL,\n                           _(\"no storage pool with matching target path '%s'\"),\n                           path);\n        } else {\n            virReportError(VIR_ERR_NO_STORAGE_POOL,\n                           _(\"no storage pool with matching target path '%s' (%s)\"),\n                           path, cleanpath);\n        }\n    }\n\n    return pool;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,8 +15,10 @@\n                                            storagePoolLookupByTargetPathCallback,\n                                            cleanpath))) {\n         def = virStoragePoolObjGetDef(obj);\n-        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0)\n+        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0) {\n+            virStoragePoolObjEndAPI(&obj);\n             return NULL;\n+        }\n \n         pool = virGetStoragePool(conn, def->name, def->uuid, NULL, NULL);\n         virStoragePoolObjEndAPI(&obj);",
        "diff_line_info": {
            "deleted_lines": [
                "        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0)"
            ],
            "added_lines": [
                "        if (virStoragePoolLookupByTargetPathEnsureACL(conn, def) < 0) {",
                "            virStoragePoolObjEndAPI(&obj);",
                "        }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26356",
        "func_name": "xen-project/xen/paging_log_dirty_enable",
        "description": "Racy interactions between dirty vram tracking and paging log dirty hypercalls Activation of log dirty mode done by XEN_DMOP_track_dirty_vram (was named HVMOP_track_dirty_vram before Xen 4.9) is racy with ongoing log dirty hypercalls. A suitably timed call to XEN_DMOP_track_dirty_vram can enable log dirty while another CPU is still in the process of tearing down the structures related to a previously enabled log dirty mode (XEN_DOMCTL_SHADOW_OP_OFF). This is due to lack of mutually exclusive locking between both operations and can lead to entries being added in already freed slots, resulting in a memory leak.",
        "git_url": "https://github.com/xen-project/xen/commit/4f4db53784d912c4f409a451c36ebfd4754e0a42",
        "commit_title": "x86/hap: do not switch on log dirty for VRAM tracking",
        "commit_text": " XEN_DMOP_track_dirty_vram possibly calls into paging_log_dirty_enable when using HAP mode, and it can interact badly with other ongoing paging domctls, as XEN_DMOP_track_dirty_vram is not holding the domctl lock.  This was detected as a result of the following assert triggering when doing repeated migrations of a HAP HVM domain with a stubdom:  Assertion 'd->arch.paging.log_dirty.allocs == 0' failed at paging.c:198 ----[ Xen-4.17-unstable  x86_64  debug=y  Not tainted ]---- CPU:    34 RIP:    e008:[<ffff82d040314b3b>] arch/x86/mm/paging.c#paging_free_log_dirty_bitmap+0x606/0x6 RFLAGS: 0000000000010206   CONTEXT: hypervisor (d0v23) [...] Xen call trace:    [<ffff82d040314b3b>] R arch/x86/mm/paging.c#paging_free_log_dirty_bitmap+0x606/0x63a    [<ffff82d040279f96>] S xsm/flask/hooks.c#domain_has_perm+0x5a/0x67    [<ffff82d04031577f>] F paging_domctl+0x251/0xd41    [<ffff82d04031640c>] F paging_domctl_continuation+0x19d/0x202    [<ffff82d0403202fa>] F pv_hypercall+0x150/0x2a7    [<ffff82d0403a729d>] F lstar_enter+0x12d/0x140  Such assert triggered because the stubdom used XEN_DMOP_track_dirty_vram while dom0 was in the middle of executing XEN_DOMCTL_SHADOW_OP_OFF, and so log dirty become enabled while retiring the old structures, thus leading to new entries being populated in already clear slots.  Fix this by not enabling log dirty for VRAM tracking, similar to what is done when using shadow instead of HAP. Call p2m_enable_hardware_log_dirty when enabling VRAM tracking in order to get some hardware assistance if available. As a side effect the memory pressure on the p2m pool should go down if only VRAM tracking is enabled, as the dirty bitmap is no longer allocated.  Note that paging_log_dirty_range (used to get the dirty bitmap for VRAM tracking) doesn't use the log dirty bitmap, and instead relies on checking whether each gfn on the range has been switched from p2m_ram_logdirty to p2m_ram_rw in order to account for dirty pages.  This is CVE-2022-26356 / XSA-397. ",
        "func_before": "int paging_log_dirty_enable(struct domain *d, bool log_global)\n{\n    int ret;\n\n    if ( has_arch_pdevs(d) && log_global )\n    {\n        /*\n         * Refuse to turn on global log-dirty mode\n         * if the domain is sharing the P2M with the IOMMU.\n         */\n        return -EINVAL;\n    }\n\n    if ( paging_mode_log_dirty(d) )\n        return -EINVAL;\n\n    domain_pause(d);\n    ret = d->arch.paging.log_dirty.ops->enable(d, log_global);\n    domain_unpause(d);\n\n    return ret;\n}",
        "func": "static int paging_log_dirty_enable(struct domain *d, bool log_global)\n{\n    int ret;\n\n    if ( has_arch_pdevs(d) && log_global )\n    {\n        /*\n         * Refuse to turn on global log-dirty mode\n         * if the domain is sharing the P2M with the IOMMU.\n         */\n        return -EINVAL;\n    }\n\n    if ( paging_mode_log_dirty(d) )\n        return -EINVAL;\n\n    domain_pause(d);\n    ret = d->arch.paging.log_dirty.ops->enable(d, log_global);\n    domain_unpause(d);\n\n    return ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-int paging_log_dirty_enable(struct domain *d, bool log_global)\n+static int paging_log_dirty_enable(struct domain *d, bool log_global)\n {\n     int ret;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "int paging_log_dirty_enable(struct domain *d, bool log_global)"
            ],
            "added_lines": [
                "static int paging_log_dirty_enable(struct domain *d, bool log_global)"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26356",
        "func_name": "xen-project/xen/hap_track_dirty_vram",
        "description": "Racy interactions between dirty vram tracking and paging log dirty hypercalls Activation of log dirty mode done by XEN_DMOP_track_dirty_vram (was named HVMOP_track_dirty_vram before Xen 4.9) is racy with ongoing log dirty hypercalls. A suitably timed call to XEN_DMOP_track_dirty_vram can enable log dirty while another CPU is still in the process of tearing down the structures related to a previously enabled log dirty mode (XEN_DOMCTL_SHADOW_OP_OFF). This is due to lack of mutually exclusive locking between both operations and can lead to entries being added in already freed slots, resulting in a memory leak.",
        "git_url": "https://github.com/xen-project/xen/commit/4f4db53784d912c4f409a451c36ebfd4754e0a42",
        "commit_title": "x86/hap: do not switch on log dirty for VRAM tracking",
        "commit_text": " XEN_DMOP_track_dirty_vram possibly calls into paging_log_dirty_enable when using HAP mode, and it can interact badly with other ongoing paging domctls, as XEN_DMOP_track_dirty_vram is not holding the domctl lock.  This was detected as a result of the following assert triggering when doing repeated migrations of a HAP HVM domain with a stubdom:  Assertion 'd->arch.paging.log_dirty.allocs == 0' failed at paging.c:198 ----[ Xen-4.17-unstable  x86_64  debug=y  Not tainted ]---- CPU:    34 RIP:    e008:[<ffff82d040314b3b>] arch/x86/mm/paging.c#paging_free_log_dirty_bitmap+0x606/0x6 RFLAGS: 0000000000010206   CONTEXT: hypervisor (d0v23) [...] Xen call trace:    [<ffff82d040314b3b>] R arch/x86/mm/paging.c#paging_free_log_dirty_bitmap+0x606/0x63a    [<ffff82d040279f96>] S xsm/flask/hooks.c#domain_has_perm+0x5a/0x67    [<ffff82d04031577f>] F paging_domctl+0x251/0xd41    [<ffff82d04031640c>] F paging_domctl_continuation+0x19d/0x202    [<ffff82d0403202fa>] F pv_hypercall+0x150/0x2a7    [<ffff82d0403a729d>] F lstar_enter+0x12d/0x140  Such assert triggered because the stubdom used XEN_DMOP_track_dirty_vram while dom0 was in the middle of executing XEN_DOMCTL_SHADOW_OP_OFF, and so log dirty become enabled while retiring the old structures, thus leading to new entries being populated in already clear slots.  Fix this by not enabling log dirty for VRAM tracking, similar to what is done when using shadow instead of HAP. Call p2m_enable_hardware_log_dirty when enabling VRAM tracking in order to get some hardware assistance if available. As a side effect the memory pressure on the p2m pool should go down if only VRAM tracking is enabled, as the dirty bitmap is no longer allocated.  Note that paging_log_dirty_range (used to get the dirty bitmap for VRAM tracking) doesn't use the log dirty bitmap, and instead relies on checking whether each gfn on the range has been switched from p2m_ram_logdirty to p2m_ram_rw in order to account for dirty pages.  This is CVE-2022-26356 / XSA-397. ",
        "func_before": "int hap_track_dirty_vram(struct domain *d,\n                         unsigned long begin_pfn,\n                         unsigned int nr_frames,\n                         XEN_GUEST_HANDLE(void) guest_dirty_bitmap)\n{\n    long rc = 0;\n    struct sh_dirty_vram *dirty_vram;\n    uint8_t *dirty_bitmap = NULL;\n\n    if ( nr_frames )\n    {\n        unsigned int size = DIV_ROUND_UP(nr_frames, BITS_PER_BYTE);\n\n        if ( !paging_mode_log_dirty(d) )\n        {\n            rc = paging_log_dirty_enable(d, false);\n            if ( rc )\n                goto out;\n        }\n\n        rc = -ENOMEM;\n        dirty_bitmap = vzalloc(size);\n        if ( !dirty_bitmap )\n            goto out;\n\n        paging_lock(d);\n\n        dirty_vram = d->arch.hvm.dirty_vram;\n        if ( !dirty_vram )\n        {\n            rc = -ENOMEM;\n            if ( (dirty_vram = xzalloc(struct sh_dirty_vram)) == NULL )\n            {\n                paging_unlock(d);\n                goto out;\n            }\n\n            d->arch.hvm.dirty_vram = dirty_vram;\n        }\n\n        if ( begin_pfn != dirty_vram->begin_pfn ||\n             begin_pfn + nr_frames != dirty_vram->end_pfn )\n        {\n            unsigned long ostart = dirty_vram->begin_pfn;\n            unsigned long oend = dirty_vram->end_pfn;\n\n            dirty_vram->begin_pfn = begin_pfn;\n            dirty_vram->end_pfn = begin_pfn + nr_frames;\n\n            paging_unlock(d);\n\n            if ( oend > ostart )\n                p2m_change_type_range(d, ostart, oend,\n                                      p2m_ram_logdirty, p2m_ram_rw);\n\n            /*\n             * Switch vram to log dirty mode, either by setting l1e entries of\n             * P2M table to be read-only, or via hardware-assisted log-dirty.\n             */\n            p2m_change_type_range(d, begin_pfn, begin_pfn + nr_frames,\n                                  p2m_ram_rw, p2m_ram_logdirty);\n\n            guest_flush_tlb_mask(d, d->dirty_cpumask);\n\n            memset(dirty_bitmap, 0xff, size); /* consider all pages dirty */\n        }\n        else\n        {\n            paging_unlock(d);\n\n            domain_pause(d);\n\n            /* Flush dirty GFNs potentially cached by hardware. */\n            p2m_flush_hardware_cached_dirty(d);\n\n            /* get the bitmap */\n            paging_log_dirty_range(d, begin_pfn, nr_frames, dirty_bitmap);\n\n            domain_unpause(d);\n        }\n\n        rc = -EFAULT;\n        if ( copy_to_guest(guest_dirty_bitmap, dirty_bitmap, size) == 0 )\n            rc = 0;\n    }\n    else\n    {\n        paging_lock(d);\n\n        dirty_vram = d->arch.hvm.dirty_vram;\n        if ( dirty_vram )\n        {\n            /*\n             * If zero pages specified while tracking dirty vram\n             * then stop tracking\n             */\n            begin_pfn = dirty_vram->begin_pfn;\n            nr_frames = dirty_vram->end_pfn - dirty_vram->begin_pfn;\n            xfree(dirty_vram);\n            d->arch.hvm.dirty_vram = NULL;\n        }\n\n        paging_unlock(d);\n\n        if ( nr_frames )\n            p2m_change_type_range(d, begin_pfn, begin_pfn + nr_frames,\n                                  p2m_ram_logdirty, p2m_ram_rw);\n    }\nout:\n    vfree(dirty_bitmap);\n\n    return rc;\n}",
        "func": "int hap_track_dirty_vram(struct domain *d,\n                         unsigned long begin_pfn,\n                         unsigned int nr_frames,\n                         XEN_GUEST_HANDLE(void) guest_dirty_bitmap)\n{\n    long rc = 0;\n    struct sh_dirty_vram *dirty_vram;\n    uint8_t *dirty_bitmap = NULL;\n\n    if ( nr_frames )\n    {\n        unsigned int size = DIV_ROUND_UP(nr_frames, BITS_PER_BYTE);\n\n        rc = -ENOMEM;\n        dirty_bitmap = vzalloc(size);\n        if ( !dirty_bitmap )\n            goto out;\n\n        paging_lock(d);\n\n        dirty_vram = d->arch.hvm.dirty_vram;\n        if ( !dirty_vram )\n        {\n            rc = -ENOMEM;\n            if ( (dirty_vram = xzalloc(struct sh_dirty_vram)) == NULL )\n            {\n                paging_unlock(d);\n                goto out;\n            }\n\n            d->arch.hvm.dirty_vram = dirty_vram;\n        }\n\n        if ( begin_pfn != dirty_vram->begin_pfn ||\n             begin_pfn + nr_frames != dirty_vram->end_pfn )\n        {\n            unsigned long ostart = dirty_vram->begin_pfn;\n            unsigned long oend = dirty_vram->end_pfn;\n\n            dirty_vram->begin_pfn = begin_pfn;\n            dirty_vram->end_pfn = begin_pfn + nr_frames;\n\n            paging_unlock(d);\n\n            domain_pause(d);\n            p2m_enable_hardware_log_dirty(d);\n            domain_unpause(d);\n\n            if ( oend > ostart )\n                p2m_change_type_range(d, ostart, oend,\n                                      p2m_ram_logdirty, p2m_ram_rw);\n\n            /*\n             * Switch vram to log dirty mode, either by setting l1e entries of\n             * P2M table to be read-only, or via hardware-assisted log-dirty.\n             */\n            p2m_change_type_range(d, begin_pfn, begin_pfn + nr_frames,\n                                  p2m_ram_rw, p2m_ram_logdirty);\n\n            guest_flush_tlb_mask(d, d->dirty_cpumask);\n\n            memset(dirty_bitmap, 0xff, size); /* consider all pages dirty */\n        }\n        else\n        {\n            paging_unlock(d);\n\n            domain_pause(d);\n\n            /* Flush dirty GFNs potentially cached by hardware. */\n            p2m_flush_hardware_cached_dirty(d);\n\n            /* get the bitmap */\n            paging_log_dirty_range(d, begin_pfn, nr_frames, dirty_bitmap);\n\n            domain_unpause(d);\n        }\n\n        rc = -EFAULT;\n        if ( copy_to_guest(guest_dirty_bitmap, dirty_bitmap, size) == 0 )\n            rc = 0;\n    }\n    else\n    {\n        paging_lock(d);\n\n        dirty_vram = d->arch.hvm.dirty_vram;\n        if ( dirty_vram )\n        {\n            /*\n             * If zero pages specified while tracking dirty vram\n             * then stop tracking\n             */\n            begin_pfn = dirty_vram->begin_pfn;\n            nr_frames = dirty_vram->end_pfn - dirty_vram->begin_pfn;\n            xfree(dirty_vram);\n            d->arch.hvm.dirty_vram = NULL;\n        }\n\n        paging_unlock(d);\n\n        if ( nr_frames )\n            p2m_change_type_range(d, begin_pfn, begin_pfn + nr_frames,\n                                  p2m_ram_logdirty, p2m_ram_rw);\n    }\nout:\n    vfree(dirty_bitmap);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,13 +10,6 @@\n     if ( nr_frames )\n     {\n         unsigned int size = DIV_ROUND_UP(nr_frames, BITS_PER_BYTE);\n-\n-        if ( !paging_mode_log_dirty(d) )\n-        {\n-            rc = paging_log_dirty_enable(d, false);\n-            if ( rc )\n-                goto out;\n-        }\n \n         rc = -ENOMEM;\n         dirty_bitmap = vzalloc(size);\n@@ -48,6 +41,10 @@\n             dirty_vram->end_pfn = begin_pfn + nr_frames;\n \n             paging_unlock(d);\n+\n+            domain_pause(d);\n+            p2m_enable_hardware_log_dirty(d);\n+            domain_unpause(d);\n \n             if ( oend > ostart )\n                 p2m_change_type_range(d, ostart, oend,",
        "diff_line_info": {
            "deleted_lines": [
                "",
                "        if ( !paging_mode_log_dirty(d) )",
                "        {",
                "            rc = paging_log_dirty_enable(d, false);",
                "            if ( rc )",
                "                goto out;",
                "        }"
            ],
            "added_lines": [
                "",
                "            domain_pause(d);",
                "            p2m_enable_hardware_log_dirty(d);",
                "            domain_unpause(d);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-43429",
        "func_name": "Seagate/cortx-s3server/mempool_destroy",
        "description": "A Denial of Service vulnerability exists in CORTX-S3 Server as of 11/7/2021 via the mempool_destroy method due to a failture to release locks pool->lock.",
        "git_url": "https://github.com/Seagate/cortx-s3server/commit/23882c427837ee988a60201c5d09a5e91e19b2d7",
        "commit_title": "avoid the unrelesed lock after the method returns",
        "commit_text": " Update s3_memory_pool.c. Avoid the unreleased lock pool->lock after the method returns.",
        "func_before": "int mempool_destroy(MemoryPoolHandle *handle) {\n  struct mempool *pool = NULL;\n  struct memory_pool_element *pool_item;\n  char *log_msg_fmt = \"mempool(%p): free(%p) called for buffer size(%zu)\";\n  char log_msg[200];\n\n  if (handle == NULL) {\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  pool = (struct mempool *)*handle;\n  if (pool == NULL) {\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  if ((pool->flags & ENABLE_LOCKING) != 0) {\n    pthread_mutex_lock(&pool->lock);\n  }\n\n  if (*handle == NULL) {\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  /* reset the handle */\n  *handle = NULL;\n  /* Free the items in free list */\n  pool_item = pool->free_list;\n  while (pool_item != NULL) {\n    pool->free_list = pool_item->next;\n    /* Log message about free()'ed item */\n    if (pool->log_callback_func) {\n      snprintf(log_msg, sizeof(log_msg), log_msg_fmt, (void *)pool,\n               (void *)pool_item, pool->mempool_item_size);\n      pool->log_callback_func(MEMPOOL_LOG_DEBUG, log_msg);\n    }\n    free(pool_item);\n#if 0\n    /* Need this if below asserts are there */\n    pool->total_bufs_allocated_by_pool--;\n    pool->free_bufs_in_pool--;\n#endif\n    pool_item = pool->free_list;\n  }\n  pool->free_list = NULL;\n\n  /* TODO: libevhtp/libevent seems to hold some references and not release back\n   * to pool. Bug will be logged for this to investigate.\n   */\n  /* Assert if there are leaks */\n  /*\n    assert(pool->total_bufs_allocated_by_pool == 0);\n    assert(pool->number_of_bufs_shared == 0);\n    assert(pool->free_bufs_in_pool == 0);\n  */\n\n  if ((pool->flags & ENABLE_LOCKING) != 0) {\n    pthread_mutex_unlock(&pool->lock);\n    pthread_mutex_destroy(&pool->lock);\n  }\n\n  free(pool);\n  pool = NULL;\n  return 0;\n}",
        "func": "int mempool_destroy(MemoryPoolHandle *handle) {\n  struct mempool *pool = NULL;\n  struct memory_pool_element *pool_item;\n  char *log_msg_fmt = \"mempool(%p): free(%p) called for buffer size(%zu)\";\n  char log_msg[200];\n\n  if (handle == NULL) {\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  pool = (struct mempool *)*handle;\n  if (pool == NULL) {\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  if ((pool->flags & ENABLE_LOCKING) != 0) {\n    pthread_mutex_lock(&pool->lock);\n  }\n\n  if (*handle == NULL) {\n    if ((pool->flags & ENABLE_LOCKING) != 0) {\n      pthread_mutex_unlock(&pool->lock);\n    }\n    return S3_MEMPOOL_INVALID_ARG;\n  }\n\n  /* reset the handle */\n  *handle = NULL;\n  /* Free the items in free list */\n  pool_item = pool->free_list;\n  while (pool_item != NULL) {\n    pool->free_list = pool_item->next;\n    /* Log message about free()'ed item */\n    if (pool->log_callback_func) {\n      snprintf(log_msg, sizeof(log_msg), log_msg_fmt, (void *)pool,\n               (void *)pool_item, pool->mempool_item_size);\n      pool->log_callback_func(MEMPOOL_LOG_DEBUG, log_msg);\n    }\n    free(pool_item);\n#if 0\n    /* Need this if below asserts are there */\n    pool->total_bufs_allocated_by_pool--;\n    pool->free_bufs_in_pool--;\n#endif\n    pool_item = pool->free_list;\n  }\n  pool->free_list = NULL;\n\n  /* TODO: libevhtp/libevent seems to hold some references and not release back\n   * to pool. Bug will be logged for this to investigate.\n   */\n  /* Assert if there are leaks */\n  /*\n    assert(pool->total_bufs_allocated_by_pool == 0);\n    assert(pool->number_of_bufs_shared == 0);\n    assert(pool->free_bufs_in_pool == 0);\n  */\n\n  if ((pool->flags & ENABLE_LOCKING) != 0) {\n    pthread_mutex_unlock(&pool->lock);\n    pthread_mutex_destroy(&pool->lock);\n  }\n\n  free(pool);\n  pool = NULL;\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,6 +18,9 @@\n   }\n \n   if (*handle == NULL) {\n+    if ((pool->flags & ENABLE_LOCKING) != 0) {\n+      pthread_mutex_unlock(&pool->lock);\n+    }\n     return S3_MEMPOOL_INVALID_ARG;\n   }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    if ((pool->flags & ENABLE_LOCKING) != 0) {",
                "      pthread_mutex_unlock(&pool->lock);",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-31621",
        "func_name": "MariaDB/server/xbstream_open",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. In extra/mariabackup/ds_xbstream.cc, when an error occurs (stream_ctxt->dest_file == NULL) while executing the method xbstream_open, the held lock is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/b1351c15946349f9daa7e5297fb2ac6f3139e4a8",
        "commit_title": "MDEV-26574 An improper locking bug due to unreleased lock in the ds_xbstream.cc",
        "commit_text": " release lock in all as cases n xbstream_open, also fix the case where malloc would return NULL.",
        "func_before": "static\nds_file_t *\nxbstream_open(ds_ctxt_t *ctxt, const char *path, MY_STAT *mystat)\n{\n\tds_file_t\t\t*file;\n\tds_stream_file_t\t*stream_file;\n\tds_stream_ctxt_t\t*stream_ctxt;\n\tds_ctxt_t\t\t*dest_ctxt;\n\txb_wstream_t\t\t*xbstream;\n\txb_wstream_file_t\t*xbstream_file;\n\n\n\txb_ad(ctxt->pipe_ctxt != NULL);\n\tdest_ctxt = ctxt->pipe_ctxt;\n\n\tstream_ctxt = (ds_stream_ctxt_t *) ctxt->ptr;\n\n\tpthread_mutex_lock(&stream_ctxt->mutex);\n\tif (stream_ctxt->dest_file == NULL) {\n\t\tstream_ctxt->dest_file = ds_open(dest_ctxt, path, mystat);\n\t\tif (stream_ctxt->dest_file == NULL) {\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tpthread_mutex_unlock(&stream_ctxt->mutex);\n\n\tfile = (ds_file_t *) my_malloc(sizeof(ds_file_t) +\n\t\t\t\t       sizeof(ds_stream_file_t),\n\t\t\t\t       MYF(MY_FAE));\n\tstream_file = (ds_stream_file_t *) (file + 1);\n\n\txbstream = stream_ctxt->xbstream;\n\n\txbstream_file = xb_stream_write_open(xbstream, path, mystat,\n\t\t                             stream_ctxt,\n\t\t\t\t\t     my_xbstream_write_callback);\n\n\tif (xbstream_file == NULL) {\n\t\tmsg(\"xb_stream_write_open() failed.\");\n\t\tgoto err;\n\t}\n\n\tstream_file->xbstream_file = xbstream_file;\n\tstream_file->stream_ctxt = stream_ctxt;\n\tfile->ptr = stream_file;\n\tfile->path = stream_ctxt->dest_file->path;\n\n\treturn file;\n\nerr:\n\tif (stream_ctxt->dest_file) {\n\t\tds_close(stream_ctxt->dest_file);\n\t\tstream_ctxt->dest_file = NULL;\n\t}\n\tmy_free(file);\n\n\treturn NULL;\n}",
        "func": "static\nds_file_t *\nxbstream_open(ds_ctxt_t *ctxt, const char *path, MY_STAT *mystat)\n{\n\tds_file_t\t\t*file;\n\tds_stream_file_t\t*stream_file;\n\tds_stream_ctxt_t\t*stream_ctxt;\n\tds_ctxt_t\t\t*dest_ctxt;\n\txb_wstream_t\t\t*xbstream;\n\txb_wstream_file_t\t*xbstream_file;\n\n\n\txb_ad(ctxt->pipe_ctxt != NULL);\n\tdest_ctxt = ctxt->pipe_ctxt;\n\n\tstream_ctxt = (ds_stream_ctxt_t *) ctxt->ptr;\n\n\tpthread_mutex_lock(&stream_ctxt->mutex);\n\tif (stream_ctxt->dest_file == NULL) {\n\t\tstream_ctxt->dest_file = ds_open(dest_ctxt, path, mystat);\n\t}\n\tpthread_mutex_unlock(&stream_ctxt->mutex);\n\tif (stream_ctxt->dest_file == NULL) {\n\t\treturn NULL;\n\t}\n\n\tfile = (ds_file_t *) my_malloc(sizeof(ds_file_t) +\n\t\t\t\t       sizeof(ds_stream_file_t),\n\t\t\t\t       MYF(MY_FAE));\n\tif (!file) {\n\t\tmsg(\"my_malloc() failed.\");\n\t\tgoto err;\n\t}\n\tstream_file = (ds_stream_file_t *) (file + 1);\n\n\txbstream = stream_ctxt->xbstream;\n\n\txbstream_file = xb_stream_write_open(xbstream, path, mystat,\n\t\t                             stream_ctxt,\n\t\t\t\t\t     my_xbstream_write_callback);\n\n\tif (xbstream_file == NULL) {\n\t\tmsg(\"xb_stream_write_open() failed.\");\n\t\tgoto err;\n\t}\n\n\tstream_file->xbstream_file = xbstream_file;\n\tstream_file->stream_ctxt = stream_ctxt;\n\tfile->ptr = stream_file;\n\tfile->path = stream_ctxt->dest_file->path;\n\n\treturn file;\n\nerr:\n\tif (stream_ctxt->dest_file) {\n\t\tds_close(stream_ctxt->dest_file);\n\t\tstream_ctxt->dest_file = NULL;\n\t}\n\tmy_free(file);\n\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -18,15 +18,19 @@\n \tpthread_mutex_lock(&stream_ctxt->mutex);\n \tif (stream_ctxt->dest_file == NULL) {\n \t\tstream_ctxt->dest_file = ds_open(dest_ctxt, path, mystat);\n-\t\tif (stream_ctxt->dest_file == NULL) {\n-\t\t\treturn NULL;\n-\t\t}\n \t}\n \tpthread_mutex_unlock(&stream_ctxt->mutex);\n+\tif (stream_ctxt->dest_file == NULL) {\n+\t\treturn NULL;\n+\t}\n \n \tfile = (ds_file_t *) my_malloc(sizeof(ds_file_t) +\n \t\t\t\t       sizeof(ds_stream_file_t),\n \t\t\t\t       MYF(MY_FAE));\n+\tif (!file) {\n+\t\tmsg(\"my_malloc() failed.\");\n+\t\tgoto err;\n+\t}\n \tstream_file = (ds_stream_file_t *) (file + 1);\n \n \txbstream = stream_ctxt->xbstream;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tif (stream_ctxt->dest_file == NULL) {",
                "\t\t\treturn NULL;",
                "\t\t}"
            ],
            "added_lines": [
                "\tif (stream_ctxt->dest_file == NULL) {",
                "\t\treturn NULL;",
                "\t}",
                "\tif (!file) {",
                "\t\tmsg(\"my_malloc() failed.\");",
                "\t\tgoto err;",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-31622",
        "func_name": "MariaDB/server/create_worker_threads",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. In extra/mariabackup/ds_compress.cc, when an error occurs (pthread_create returns a nonzero value) while executing the method create_worker_threads, the held lock is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/e1eb39a446c30b8459c39fd7f2ee1c55a36e97d2",
        "commit_title": "MDEV-26561 Fix a bug due to unreleased lock",
        "commit_text": " Fix a bug of unreleased lock ctrl_mutex in the method create_worker_threads",
        "func_before": "static\ncomp_thread_ctxt_t *\ncreate_worker_threads(uint n)\n{\n\tcomp_thread_ctxt_t\t*threads;\n\tuint \t\t\ti;\n\n\tthreads = (comp_thread_ctxt_t *)\n\t\tmy_malloc(sizeof(comp_thread_ctxt_t) * n, MYF(MY_FAE));\n\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\tthd->num = i + 1;\n\t\tthd->started = FALSE;\n\t\tthd->cancelled = FALSE;\n\t\tthd->data_avail = FALSE;\n\n\t\tthd->to = (char *) my_malloc(COMPRESS_CHUNK_SIZE +\n\t\t\t\t\t\t   MY_QLZ_COMPRESS_OVERHEAD,\n\t\t\t\t\t\t   MYF(MY_FAE));\n\n\t\t/* Initialize the control mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->ctrl_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->ctrl_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* Initialize and data mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->data_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->data_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\tpthread_mutex_lock(&thd->ctrl_mutex);\n\n\t\tif (pthread_create(&thd->id, NULL, compress_worker_thread_func,\n\t\t\t\t   thd)) {\n\t\t\tmsg(\"compress: pthread_create() failed: \"\n\t\t\t    \"errno = %d\", errno);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* Wait for the threads to start */\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\twhile (thd->started == FALSE)\n\t\t\tpthread_cond_wait(&thd->ctrl_cond, &thd->ctrl_mutex);\n\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t}\n\n\treturn threads;\n\nerr:\n\tmy_free(threads);\n\treturn NULL;\n}",
        "func": "static\ncomp_thread_ctxt_t *\ncreate_worker_threads(uint n)\n{\n\tcomp_thread_ctxt_t\t*threads;\n\tuint \t\t\ti;\n\n\tthreads = (comp_thread_ctxt_t *)\n\t\tmy_malloc(sizeof(comp_thread_ctxt_t) * n, MYF(MY_FAE));\n\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\tthd->num = i + 1;\n\t\tthd->started = FALSE;\n\t\tthd->cancelled = FALSE;\n\t\tthd->data_avail = FALSE;\n\n\t\tthd->to = (char *) my_malloc(COMPRESS_CHUNK_SIZE +\n\t\t\t\t\t\t   MY_QLZ_COMPRESS_OVERHEAD,\n\t\t\t\t\t\t   MYF(MY_FAE));\n\n\t\t/* Initialize the control mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->ctrl_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->ctrl_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* Initialize and data mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->data_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->data_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\tpthread_mutex_lock(&thd->ctrl_mutex);\n\n\t\tif (pthread_create(&thd->id, NULL, compress_worker_thread_func,\n\t\t\t\t   thd)) {\n\t\t\tmsg(\"compress: pthread_create() failed: \"\n\t\t\t    \"errno = %d\", errno);\n\t\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* Wait for the threads to start */\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\twhile (thd->started == FALSE)\n\t\t\tpthread_cond_wait(&thd->ctrl_cond, &thd->ctrl_mutex);\n\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t}\n\n\treturn threads;\n\nerr:\n\tmy_free(threads);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -38,6 +38,7 @@\n \t\t\t\t   thd)) {\n \t\t\tmsg(\"compress: pthread_create() failed: \"\n \t\t\t    \"errno = %d\", errno);\n+\t\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n \t\t\tgoto err;\n \t\t}\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t\tpthread_mutex_unlock(&thd->ctrl_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-31623",
        "func_name": "MariaDB/server/create_worker_threads",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. In extra/mariabackup/ds_compress.cc, when an error occurs (i.e., going to the err label) while executing the method create_worker_threads, the held lock thd->ctrl_mutex is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/7c30bc38a588b22b01f11130cfe99e7f36accf94",
        "commit_title": "MDEV-26561 mariabackup release locks",
        "commit_text": " The previous threads locked need to be released too.  This occurs if the initialization of any of the non-first mutex/conditition variables errors occurs.",
        "func_before": "static\ncomp_thread_ctxt_t *\ncreate_worker_threads(uint n)\n{\n\tcomp_thread_ctxt_t\t*threads;\n\tuint \t\t\ti;\n\n\tthreads = (comp_thread_ctxt_t *)\n\t\tmy_malloc(sizeof(comp_thread_ctxt_t) * n, MYF(MY_FAE));\n\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\tthd->num = i + 1;\n\t\tthd->started = FALSE;\n\t\tthd->cancelled = FALSE;\n\t\tthd->data_avail = FALSE;\n\n\t\tthd->to = (char *) my_malloc(COMPRESS_CHUNK_SIZE +\n\t\t\t\t\t\t   MY_QLZ_COMPRESS_OVERHEAD,\n\t\t\t\t\t\t   MYF(MY_FAE));\n\n\t\t/* Initialize the control mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->ctrl_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->ctrl_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* Initialize and data mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->data_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->data_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\tpthread_mutex_lock(&thd->ctrl_mutex);\n\n\t\tif (pthread_create(&thd->id, NULL, compress_worker_thread_func,\n\t\t\t\t   thd)) {\n\t\t\tmsg(\"compress: pthread_create() failed: \"\n\t\t\t    \"errno = %d\", errno);\n\t\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* Wait for the threads to start */\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\twhile (thd->started == FALSE)\n\t\t\tpthread_cond_wait(&thd->ctrl_cond, &thd->ctrl_mutex);\n\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t}\n\n\treturn threads;\n\nerr:\n\tmy_free(threads);\n\treturn NULL;\n}",
        "func": "static\ncomp_thread_ctxt_t *\ncreate_worker_threads(uint n)\n{\n\tcomp_thread_ctxt_t\t*threads;\n\tuint \t\t\ti;\n\n\tthreads = (comp_thread_ctxt_t *)\n\t\tmy_malloc(sizeof(comp_thread_ctxt_t) * n, MYF(MY_FAE));\n\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\tthd->num = i + 1;\n\t\tthd->started = FALSE;\n\t\tthd->cancelled = FALSE;\n\t\tthd->data_avail = FALSE;\n\n\t\tthd->to = (char *) my_malloc(COMPRESS_CHUNK_SIZE +\n\t\t\t\t\t\t   MY_QLZ_COMPRESS_OVERHEAD,\n\t\t\t\t\t\t   MYF(MY_FAE));\n\n\t\t/* Initialize the control mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->ctrl_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->ctrl_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\t/* Initialize and data mutex and condition var */\n\t\tif (pthread_mutex_init(&thd->data_mutex, NULL) ||\n\t\t    pthread_cond_init(&thd->data_cond, NULL)) {\n\t\t\tgoto err;\n\t\t}\n\n\t\tpthread_mutex_lock(&thd->ctrl_mutex);\n\n\t\tif (pthread_create(&thd->id, NULL, compress_worker_thread_func,\n\t\t\t\t   thd)) {\n\t\t\tmsg(\"compress: pthread_create() failed: \"\n\t\t\t    \"errno = %d\", errno);\n\t\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t/* Wait for the threads to start */\n\tfor (i = 0; i < n; i++) {\n\t\tcomp_thread_ctxt_t *thd = threads + i;\n\n\t\twhile (thd->started == FALSE)\n\t\t\tpthread_cond_wait(&thd->ctrl_cond, &thd->ctrl_mutex);\n\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t}\n\n\treturn threads;\n\nerr:\n\twhile (i > 0) {\n\t\tcomp_thread_ctxt_t *thd;\n\t\ti--;\n\t\tthd = threads + i;\n\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n\t}\n\n\tmy_free(threads);\n\treturn NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -55,6 +55,13 @@\n \treturn threads;\n \n err:\n+\twhile (i > 0) {\n+\t\tcomp_thread_ctxt_t *thd;\n+\t\ti--;\n+\t\tthd = threads + i;\n+\t\tpthread_mutex_unlock(&thd->ctrl_mutex);\n+\t}\n+\n \tmy_free(threads);\n \treturn NULL;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\twhile (i > 0) {",
                "\t\tcomp_thread_ctxt_t *thd;",
                "\t\ti--;",
                "\t\tthd = threads + i;",
                "\t\tpthread_mutex_unlock(&thd->ctrl_mutex);",
                "\t}",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-31624",
        "func_name": "MariaDB/server/server_audit_init",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. While executing the plugin/server_audit/server_audit.c method log_statement_ex, the held lock lock_bigbuffer is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/d627d00b13ab2f2c0954ea7b77202470cb102944",
        "commit_title": "MDEV-26556 An improper locking bug(s) due to unreleased lock.",
        "commit_text": " Get rid of the global big_buffer.",
        "func_before": "static int server_audit_init(void *p __attribute__((unused)))\n{\n  if (!serv_ver)\n  {\n#ifdef _WIN32\n    serv_ver= (const char *) GetProcAddress(0, \"server_version\");\n#else\n    serv_ver= server_version;\n#endif /*_WIN32*/\n  }\n  if (!mysql_57_started)\n  {\n    const void *my_hash_init_ptr= dlsym(RTLD_DEFAULT, \"_my_hash_init\");\n    if (!my_hash_init_ptr)\n    {\n      maria_above_5= 1;\n      my_hash_init_ptr= dlsym(RTLD_DEFAULT, \"my_hash_init2\");\n    }\n    if (!my_hash_init_ptr)\n      return 1;\n  }\n\n  if(!(int_mysql_data_home= dlsym(RTLD_DEFAULT, \"mysql_data_home\")))\n  {\n    if(!(int_mysql_data_home= dlsym(RTLD_DEFAULT, \"?mysql_data_home@@3PADA\")))\n      int_mysql_data_home= &default_home;\n  }\n\n  if (!serv_ver)\n    return 1;\n\n  if (!started_mysql)\n  {\n    if (!maria_above_5 && serv_ver[4]=='3' && serv_ver[5]<'3')\n    {\n      mode= 1;\n      mode_readonly= 1;\n    }\n  }\n\n  if (gethostname(servhost, sizeof(servhost)))\n    strcpy(servhost, \"unknown\");\n\n  servhost_len= (uint)strlen(servhost);\n\n  logger_init_mutexes();\n#if defined(HAVE_PSI_INTERFACE) && !defined(FLOGGER_NO_PSI)\n  if (PSI_server)\n    PSI_server->register_mutex(\"server_audit\", mutex_key_list, 1);\n#endif\n  flogger_mutex_init(key_LOCK_operations, &lock_operations, MY_MUTEX_INIT_FAST);\n  flogger_mutex_init(key_LOCK_operations, &lock_atomic, MY_MUTEX_INIT_FAST);\n  flogger_mutex_init(key_LOCK_operations, &lock_bigbuffer, MY_MUTEX_INIT_FAST);\n\n  coll_init(&incl_user_coll);\n  coll_init(&excl_user_coll);\n\n  if (incl_users)\n  {\n    if (excl_users)\n    {\n      incl_users= excl_users= NULL;\n      error_header();\n      fprintf(stderr, \"INCL_DML_USERS and EXCL_DML_USERS specified\"\n                      \" simultaneously - both set to empty\\n\");\n    }\n    update_incl_users(NULL, NULL, NULL, &incl_users);\n  }\n  else if (excl_users)\n  {\n    update_excl_users(NULL, NULL, NULL, &excl_users);\n  }\n\n  error_header();\n  fprintf(stderr, \"MariaDB Audit Plugin version %s%s STARTED.\\n\",\n          PLUGIN_STR_VERSION, PLUGIN_DEBUG_VERSION);\n\n  /* The Query Cache shadows TABLE events if the result is taken from it */\n  /* so we warn users if both Query Cashe and TABLE events enabled.      */\n  if (!started_mysql && FILTER(EVENT_TABLE))\n  {\n    ulonglong *qc_size= (ulonglong *) dlsym(RTLD_DEFAULT, \"query_cache_size\");\n    if (qc_size == NULL || *qc_size != 0)\n    {\n      struct loc_system_variables *g_sys_var=\n        (struct loc_system_variables *) dlsym(RTLD_DEFAULT,\n                                          \"global_system_variables\");\n      if (g_sys_var && g_sys_var->query_cache_type != 0)\n      {\n        error_header();\n        fprintf(stderr, \"Query cache is enabled with the TABLE events.\"\n                        \" Some table reads can be veiled.\");\n      }\n    }\n  }\n\n  ci_disconnect_buffer.header= 10;\n  ci_disconnect_buffer.thread_id= 0;\n  ci_disconnect_buffer.query_id= 0;\n  ci_disconnect_buffer.db_length= 0;\n  ci_disconnect_buffer.user_length= 0;\n  ci_disconnect_buffer.host_length= 0;\n  ci_disconnect_buffer.ip_length= 0;\n  ci_disconnect_buffer.query= empty_str;\n  ci_disconnect_buffer.query_length= 0;\n\n  if (logging)\n    start_logging();\n\n  init_done= 1;\n  return 0;\n}",
        "func": "static int server_audit_init(void *p __attribute__((unused)))\n{\n  if (!serv_ver)\n  {\n#ifdef _WIN32\n    serv_ver= (const char *) GetProcAddress(0, \"server_version\");\n#else\n    serv_ver= server_version;\n#endif /*_WIN32*/\n  }\n  if (!mysql_57_started)\n  {\n    const void *my_hash_init_ptr= dlsym(RTLD_DEFAULT, \"_my_hash_init\");\n    if (!my_hash_init_ptr)\n    {\n      maria_above_5= 1;\n      my_hash_init_ptr= dlsym(RTLD_DEFAULT, \"my_hash_init2\");\n    }\n    if (!my_hash_init_ptr)\n      return 1;\n  }\n\n  if(!(int_mysql_data_home= dlsym(RTLD_DEFAULT, \"mysql_data_home\")))\n  {\n    if(!(int_mysql_data_home= dlsym(RTLD_DEFAULT, \"?mysql_data_home@@3PADA\")))\n      int_mysql_data_home= &default_home;\n  }\n\n  if (!serv_ver)\n    return 1;\n\n  if (!started_mysql)\n  {\n    if (!maria_above_5 && serv_ver[4]=='3' && serv_ver[5]<'3')\n    {\n      mode= 1;\n      mode_readonly= 1;\n    }\n  }\n\n  if (gethostname(servhost, sizeof(servhost)))\n    strcpy(servhost, \"unknown\");\n\n  servhost_len= (uint)strlen(servhost);\n\n  logger_init_mutexes();\n#if defined(HAVE_PSI_INTERFACE) && !defined(FLOGGER_NO_PSI)\n  if (PSI_server)\n    PSI_server->register_mutex(\"server_audit\", mutex_key_list, 1);\n#endif\n  flogger_mutex_init(key_LOCK_operations, &lock_operations, MY_MUTEX_INIT_FAST);\n  flogger_mutex_init(key_LOCK_operations, &lock_atomic, MY_MUTEX_INIT_FAST);\n\n  coll_init(&incl_user_coll);\n  coll_init(&excl_user_coll);\n\n  if (incl_users)\n  {\n    if (excl_users)\n    {\n      incl_users= excl_users= NULL;\n      error_header();\n      fprintf(stderr, \"INCL_DML_USERS and EXCL_DML_USERS specified\"\n                      \" simultaneously - both set to empty\\n\");\n    }\n    update_incl_users(NULL, NULL, NULL, &incl_users);\n  }\n  else if (excl_users)\n  {\n    update_excl_users(NULL, NULL, NULL, &excl_users);\n  }\n\n  error_header();\n  fprintf(stderr, \"MariaDB Audit Plugin version %s%s STARTED.\\n\",\n          PLUGIN_STR_VERSION, PLUGIN_DEBUG_VERSION);\n\n  /* The Query Cache shadows TABLE events if the result is taken from it */\n  /* so we warn users if both Query Cashe and TABLE events enabled.      */\n  if (!started_mysql && FILTER(EVENT_TABLE))\n  {\n    ulonglong *qc_size= (ulonglong *) dlsym(RTLD_DEFAULT, \"query_cache_size\");\n    if (qc_size == NULL || *qc_size != 0)\n    {\n      struct loc_system_variables *g_sys_var=\n        (struct loc_system_variables *) dlsym(RTLD_DEFAULT,\n                                          \"global_system_variables\");\n      if (g_sys_var && g_sys_var->query_cache_type != 0)\n      {\n        error_header();\n        fprintf(stderr, \"Query cache is enabled with the TABLE events.\"\n                        \" Some table reads can be veiled.\");\n      }\n    }\n  }\n\n  ci_disconnect_buffer.header= 10;\n  ci_disconnect_buffer.thread_id= 0;\n  ci_disconnect_buffer.query_id= 0;\n  ci_disconnect_buffer.db_length= 0;\n  ci_disconnect_buffer.user_length= 0;\n  ci_disconnect_buffer.host_length= 0;\n  ci_disconnect_buffer.ip_length= 0;\n  ci_disconnect_buffer.query= empty_str;\n  ci_disconnect_buffer.query_length= 0;\n\n  if (logging)\n    start_logging();\n\n  init_done= 1;\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -50,7 +50,6 @@\n #endif\n   flogger_mutex_init(key_LOCK_operations, &lock_operations, MY_MUTEX_INIT_FAST);\n   flogger_mutex_init(key_LOCK_operations, &lock_atomic, MY_MUTEX_INIT_FAST);\n-  flogger_mutex_init(key_LOCK_operations, &lock_bigbuffer, MY_MUTEX_INIT_FAST);\n \n   coll_init(&incl_user_coll);\n   coll_init(&excl_user_coll);",
        "diff_line_info": {
            "deleted_lines": [
                "  flogger_mutex_init(key_LOCK_operations, &lock_bigbuffer, MY_MUTEX_INIT_FAST);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-31624",
        "func_name": "MariaDB/server/log_statement_ex",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. While executing the plugin/server_audit/server_audit.c method log_statement_ex, the held lock lock_bigbuffer is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/d627d00b13ab2f2c0954ea7b77202470cb102944",
        "commit_title": "MDEV-26556 An improper locking bug(s) due to unreleased lock.",
        "commit_text": " Get rid of the global big_buffer.",
        "func_before": "static int log_statement_ex(const struct connection_info *cn,\n                            time_t ev_time, unsigned long thd_id,\n                            const char *query, unsigned int query_len,\n                            int error_code, const char *type, int take_lock)\n{\n  size_t csize;\n  char message_loc[1024];\n  char *message= message_loc;\n  size_t message_size= sizeof(message_loc);\n  char *uh_buffer;\n  size_t uh_buffer_size;\n  const char *db;\n  unsigned int db_length;\n  long long query_id;\n  int result;\n\n  if ((db= cn->db))\n    db_length= cn->db_length;\n  else\n  {\n    db= \"\";\n    db_length= 0;\n  }\n\n  if (!(query_id= cn->query_id))\n    query_id= query_counter++;\n\n  if (query == 0)\n  {\n    /* Can happen after the error in mysqld_prepare_stmt() */\n    query= cn->query;\n    query_len= cn->query_length;\n    if (query == 0 || query_len == 0)\n      return 0;\n  }\n\n  if (query && !(events & EVENT_QUERY_ALL) &&\n      (events & EVENT_QUERY && !cn->log_always))\n  {\n    const char *orig_query= query;\n\n    if (filter_query_type(query, keywords_to_skip))\n    {\n      char fword[MAX_KEYWORD + 1];\n      int len;\n      do\n      {\n        len= get_next_word(query, fword);\n        query+= len ? len : 1;\n        if (len == 3 && strncmp(fword, \"FOR\", 3) == 0)\n          break;\n      } while (*query);\n\n      if (*query == 0)\n        return 0;\n    }\n\n    if (events & EVENT_QUERY_DDL)\n    {\n      if (!filter_query_type(query, not_ddl_keywords) &&\n          filter_query_type(query, ddl_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DML)\n    {\n      if (filter_query_type(query, dml_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DML_NO_SELECT)\n    {\n      if (filter_query_type(query, dml_no_select_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DCL)\n    {\n      if (filter_query_type(query, dcl_keywords))\n        goto do_log_query;\n    }\n\n    return 0;\ndo_log_query:\n    query= orig_query;\n  }\n\n  csize= log_header(message, message_size-1, &ev_time,\n                    servhost, servhost_len,\n                    cn->user, cn->user_length,cn->host, cn->host_length,\n                    cn->ip, cn->ip_length, thd_id, query_id, type);\n\n  csize+= my_snprintf(message+csize, message_size - 1 - csize,\n      \",%.*s,\\'\", db_length, db);\n\n  if (query_log_limit > 0 && query_len > query_log_limit)\n    query_len= query_log_limit;\n\n  if (query_len > (message_size - csize)/2)\n  {\n    flogger_mutex_lock(&lock_bigbuffer);\n    if (big_buffer_alloced < (query_len * 2 + csize))\n    {\n      big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;\n      big_buffer= realloc(big_buffer, big_buffer_alloced);\n      if (big_buffer == NULL)\n      {\n        big_buffer_alloced= 0;\n        return 0;\n      }\n    }\n\n    memcpy(big_buffer, message, csize);\n    message= big_buffer;\n    message_size= big_buffer_alloced;\n  }\n\n  uh_buffer= message + csize;\n  uh_buffer_size= message_size - csize;\n  if (query_log_limit > 0 && uh_buffer_size > query_log_limit+2)\n    uh_buffer_size= query_log_limit+2;\n\n  switch (filter_query_type(query, passwd_keywords))\n  {\n    case SQLCOM_GRANT:\n    case SQLCOM_CREATE_USER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"IDENTIFIED\", 10, \"BY\", 2, 0);\n      break;\n    case SQLCOM_CHANGE_MASTER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"MASTER_PASSWORD\", 15, \"=\", 1, 0);\n      break;\n    case SQLCOM_CREATE_SERVER:\n    case SQLCOM_ALTER_SERVER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"PASSWORD\", 8, NULL, 0, 0);\n      break;\n    case SQLCOM_SET_OPTION:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"=\", 1, NULL, 0, 1);\n      break;\n    default:\n      csize+= escape_string(query, query_len,\n                               uh_buffer, uh_buffer_size); \n      break;\n  }\n  csize+= my_snprintf(message+csize, message_size - 1 - csize,\n                      \"\\',%d\", error_code);\n  message[csize]= '\\n';\n  result= write_log(message, csize + 1, take_lock);\n  if (message == big_buffer)\n    flogger_mutex_unlock(&lock_bigbuffer);\n\n  return result;\n}",
        "func": "static int log_statement_ex(const struct connection_info *cn,\n                            time_t ev_time, unsigned long thd_id,\n                            const char *query, unsigned int query_len,\n                            int error_code, const char *type, int take_lock)\n{\n  size_t csize;\n  char message_loc[2048];\n  char *message= message_loc;\n  size_t message_size= sizeof(message_loc);\n  char *uh_buffer;\n  size_t uh_buffer_size;\n  const char *db;\n  unsigned int db_length;\n  long long query_id;\n  int result;\n  char *big_buffer= NULL;\n\n  if ((db= cn->db))\n    db_length= cn->db_length;\n  else\n  {\n    db= \"\";\n    db_length= 0;\n  }\n\n  if (!(query_id= cn->query_id))\n    query_id= query_counter++;\n\n  if (query == 0)\n  {\n    /* Can happen after the error in mysqld_prepare_stmt() */\n    query= cn->query;\n    query_len= cn->query_length;\n    if (query == 0 || query_len == 0)\n      return 0;\n  }\n\n  if (query && !(events & EVENT_QUERY_ALL) &&\n      (events & EVENT_QUERY && !cn->log_always))\n  {\n    const char *orig_query= query;\n\n    if (filter_query_type(query, keywords_to_skip))\n    {\n      char fword[MAX_KEYWORD + 1];\n      int len;\n      do\n      {\n        len= get_next_word(query, fword);\n        query+= len ? len : 1;\n        if (len == 3 && strncmp(fword, \"FOR\", 3) == 0)\n          break;\n      } while (*query);\n\n      if (*query == 0)\n        return 0;\n    }\n\n    if (events & EVENT_QUERY_DDL)\n    {\n      if (!filter_query_type(query, not_ddl_keywords) &&\n          filter_query_type(query, ddl_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DML)\n    {\n      if (filter_query_type(query, dml_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DML_NO_SELECT)\n    {\n      if (filter_query_type(query, dml_no_select_keywords))\n        goto do_log_query;\n    }\n    if (events & EVENT_QUERY_DCL)\n    {\n      if (filter_query_type(query, dcl_keywords))\n        goto do_log_query;\n    }\n\n    return 0;\ndo_log_query:\n    query= orig_query;\n  }\n\n  csize= log_header(message, message_size-1, &ev_time,\n                    servhost, servhost_len,\n                    cn->user, cn->user_length,cn->host, cn->host_length,\n                    cn->ip, cn->ip_length, thd_id, query_id, type);\n\n  csize+= my_snprintf(message+csize, message_size - 1 - csize,\n      \",%.*s,\\'\", db_length, db);\n\n  if (query_log_limit > 0 && query_len > query_log_limit)\n    query_len= query_log_limit;\n\n  if (query_len > (message_size - csize)/2)\n  {\n    size_t big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;\n    if(!(big_buffer= malloc(big_buffer_alloced)))\n      return 0;\n\n    memcpy(big_buffer, message, csize);\n    message= big_buffer;\n    message_size= big_buffer_alloced;\n  }\n\n  uh_buffer= message + csize;\n  uh_buffer_size= message_size - csize;\n  if (query_log_limit > 0 && uh_buffer_size > query_log_limit+2)\n    uh_buffer_size= query_log_limit+2;\n\n  switch (filter_query_type(query, passwd_keywords))\n  {\n    case SQLCOM_GRANT:\n    case SQLCOM_CREATE_USER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"IDENTIFIED\", 10, \"BY\", 2, 0);\n      break;\n    case SQLCOM_CHANGE_MASTER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"MASTER_PASSWORD\", 15, \"=\", 1, 0);\n      break;\n    case SQLCOM_CREATE_SERVER:\n    case SQLCOM_ALTER_SERVER:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"PASSWORD\", 8, NULL, 0, 0);\n      break;\n    case SQLCOM_SET_OPTION:\n      csize+= escape_string_hide_passwords(query, query_len,\n                                           uh_buffer, uh_buffer_size,\n                                           \"=\", 1, NULL, 0, 1);\n      break;\n    default:\n      csize+= escape_string(query, query_len,\n                               uh_buffer, uh_buffer_size); \n      break;\n  }\n  csize+= my_snprintf(message+csize, message_size - 1 - csize,\n                      \"\\',%d\", error_code);\n  message[csize]= '\\n';\n  result= write_log(message, csize + 1, take_lock);\n  if (big_buffer)\n    free(big_buffer);\n\n  return result;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,7 @@\n                             int error_code, const char *type, int take_lock)\n {\n   size_t csize;\n-  char message_loc[1024];\n+  char message_loc[2048];\n   char *message= message_loc;\n   size_t message_size= sizeof(message_loc);\n   char *uh_buffer;\n@@ -13,6 +13,7 @@\n   unsigned int db_length;\n   long long query_id;\n   int result;\n+  char *big_buffer= NULL;\n \n   if ((db= cn->db))\n     db_length= cn->db_length;\n@@ -95,17 +96,9 @@\n \n   if (query_len > (message_size - csize)/2)\n   {\n-    flogger_mutex_lock(&lock_bigbuffer);\n-    if (big_buffer_alloced < (query_len * 2 + csize))\n-    {\n-      big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;\n-      big_buffer= realloc(big_buffer, big_buffer_alloced);\n-      if (big_buffer == NULL)\n-      {\n-        big_buffer_alloced= 0;\n-        return 0;\n-      }\n-    }\n+    size_t big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;\n+    if(!(big_buffer= malloc(big_buffer_alloced)))\n+      return 0;\n \n     memcpy(big_buffer, message, csize);\n     message= big_buffer;\n@@ -150,8 +143,8 @@\n                       \"\\',%d\", error_code);\n   message[csize]= '\\n';\n   result= write_log(message, csize + 1, take_lock);\n-  if (message == big_buffer)\n-    flogger_mutex_unlock(&lock_bigbuffer);\n+  if (big_buffer)\n+    free(big_buffer);\n \n   return result;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  char message_loc[1024];",
                "    flogger_mutex_lock(&lock_bigbuffer);",
                "    if (big_buffer_alloced < (query_len * 2 + csize))",
                "    {",
                "      big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;",
                "      big_buffer= realloc(big_buffer, big_buffer_alloced);",
                "      if (big_buffer == NULL)",
                "      {",
                "        big_buffer_alloced= 0;",
                "        return 0;",
                "      }",
                "    }",
                "  if (message == big_buffer)",
                "    flogger_mutex_unlock(&lock_bigbuffer);"
            ],
            "added_lines": [
                "  char message_loc[2048];",
                "  char *big_buffer= NULL;",
                "    size_t big_buffer_alloced= (query_len * 2 + csize + 4095) & ~4095L;",
                "    if(!(big_buffer= malloc(big_buffer_alloced)))",
                "      return 0;",
                "  if (big_buffer)",
                "    free(big_buffer);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-31624",
        "func_name": "MariaDB/server/server_audit_deinit",
        "description": "MariaDB Server before 10.7 is vulnerable to Denial of Service. While executing the plugin/server_audit/server_audit.c method log_statement_ex, the held lock lock_bigbuffer is not released correctly, which allows local users to trigger a denial of service due to the deadlock.",
        "git_url": "https://github.com/MariaDB/server/commit/d627d00b13ab2f2c0954ea7b77202470cb102944",
        "commit_title": "MDEV-26556 An improper locking bug(s) due to unreleased lock.",
        "commit_text": " Get rid of the global big_buffer.",
        "func_before": "static int server_audit_deinit(void *p __attribute__((unused)))\n{\n  if (!init_done)\n    return 0;\n\n  init_done= 0;\n  coll_free(&incl_user_coll);\n  coll_free(&excl_user_coll);\n\n  if (output_type == OUTPUT_FILE && logfile)\n    logger_close(logfile);\n  else if (output_type == OUTPUT_SYSLOG)\n    closelog();\n\n  (void) free(big_buffer);\n  flogger_mutex_destroy(&lock_operations);\n  flogger_mutex_destroy(&lock_atomic);\n  flogger_mutex_destroy(&lock_bigbuffer);\n\n  error_header();\n  fprintf(stderr, \"STOPPED\\n\");\n  return 0;\n}",
        "func": "static int server_audit_deinit(void *p __attribute__((unused)))\n{\n  if (!init_done)\n    return 0;\n\n  init_done= 0;\n  coll_free(&incl_user_coll);\n  coll_free(&excl_user_coll);\n\n  if (output_type == OUTPUT_FILE && logfile)\n    logger_close(logfile);\n  else if (output_type == OUTPUT_SYSLOG)\n    closelog();\n\n  flogger_mutex_destroy(&lock_operations);\n  flogger_mutex_destroy(&lock_atomic);\n\n  error_header();\n  fprintf(stderr, \"STOPPED\\n\");\n  return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,10 +12,8 @@\n   else if (output_type == OUTPUT_SYSLOG)\n     closelog();\n \n-  (void) free(big_buffer);\n   flogger_mutex_destroy(&lock_operations);\n   flogger_mutex_destroy(&lock_atomic);\n-  flogger_mutex_destroy(&lock_bigbuffer);\n \n   error_header();\n   fprintf(stderr, \"STOPPED\\n\");",
        "diff_line_info": {
            "deleted_lines": [
                "  (void) free(big_buffer);",
                "  flogger_mutex_destroy(&lock_bigbuffer);"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/ipp_alloc_codec",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "static pj_status_t ipp_alloc_codec( pjmedia_codec_factory *factory, \n\t\t\t\t    const pjmedia_codec_info *id,\n\t\t\t\t    pjmedia_codec **p_codec)\n{\n    ipp_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &ipp_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(ipp_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(ipp_codec); ++i) {\n\tpj_str_t name = pj_str((char*)ipp_codec[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)ipp_codec[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)ipp_codec[i].channel_count) &&\n\t    (ipp_codec[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(ipp_factory.endpt, \"IPPcodec\", 512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    PJ_ASSERT_RETURN(codec != NULL, PJ_ENOMEM);\n    codec->op = &ipp_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, ipp_private_t);\n    codec_data = (ipp_private_t*) codec->codec_data;\n\n    /* Create PLC if codec has no internal PLC */\n    if (!ipp_codec[idx].has_native_plc) {\n\tpj_status_t status;\n\tstatus = pjmedia_plc_create(pool, ipp_codec[idx].clock_rate, \n\t\t\t\t    ipp_codec[idx].samples_per_frame, 0,\n\t\t\t\t    &codec_data->plc);\n\tif (status != PJ_SUCCESS) {\n\t    pj_pool_release(pool);\n\t    pj_mutex_unlock(ipp_factory.mutex);\n\t    return status;\n\t}\n    }\n\n    /* Create silence detector if codec has no internal VAD */\n    if (!ipp_codec[idx].has_native_vad) {\n\tpj_status_t status;\n\tstatus = pjmedia_silence_det_create(pool,\n\t\t\t\t\t    ipp_codec[idx].clock_rate,\n\t\t\t\t\t    ipp_codec[idx].samples_per_frame,\n\t\t\t\t\t    &codec_data->vad);\n\tif (status != PJ_SUCCESS) {\n\t    pj_pool_release(pool);\n\t    pj_mutex_unlock(ipp_factory.mutex);\n\t    return status;\n\t}\n    }\n\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    pj_mutex_unlock(ipp_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n}",
        "func": "static pj_status_t ipp_alloc_codec( pjmedia_codec_factory *factory, \n\t\t\t\t    const pjmedia_codec_info *id,\n\t\t\t\t    pjmedia_codec **p_codec)\n{\n    ipp_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &ipp_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(ipp_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(ipp_codec); ++i) {\n\tpj_str_t name = pj_str((char*)ipp_codec[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)ipp_codec[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)ipp_codec[i].channel_count) &&\n\t    (ipp_codec[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\tpj_mutex_unlock(ipp_factory.mutex);\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(ipp_factory.endpt, \"IPPcodec\", 512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    PJ_ASSERT_RETURN(codec != NULL, PJ_ENOMEM);\n    codec->op = &ipp_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, ipp_private_t);\n    codec_data = (ipp_private_t*) codec->codec_data;\n\n    /* Create PLC if codec has no internal PLC */\n    if (!ipp_codec[idx].has_native_plc) {\n\tpj_status_t status;\n\tstatus = pjmedia_plc_create(pool, ipp_codec[idx].clock_rate, \n\t\t\t\t    ipp_codec[idx].samples_per_frame, 0,\n\t\t\t\t    &codec_data->plc);\n\tif (status != PJ_SUCCESS) {\n\t    pj_pool_release(pool);\n\t    pj_mutex_unlock(ipp_factory.mutex);\n\t    return status;\n\t}\n    }\n\n    /* Create silence detector if codec has no internal VAD */\n    if (!ipp_codec[idx].has_native_vad) {\n\tpj_status_t status;\n\tstatus = pjmedia_silence_det_create(pool,\n\t\t\t\t\t    ipp_codec[idx].clock_rate,\n\t\t\t\t\t    ipp_codec[idx].samples_per_frame,\n\t\t\t\t\t    &codec_data->vad);\n\tif (status != PJ_SUCCESS) {\n\t    pj_pool_release(pool);\n\t    pj_mutex_unlock(ipp_factory.mutex);\n\t    return status;\n\t}\n    }\n\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    pj_mutex_unlock(ipp_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,7 @@\n     }\n     if (idx == -1) {\n \t*p_codec = NULL;\n+\tpj_mutex_unlock(ipp_factory.mutex);\n \treturn PJMEDIA_CODEC_EFAILED;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpj_mutex_unlock(ipp_factory.mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/and_media_alloc_codec",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "static pj_status_t and_media_alloc_codec(pjmedia_codec_factory *factory,\n\t\t\t\t\t const pjmedia_codec_info *id,\n\t\t\t\t\t pjmedia_codec **p_codec)\n{\n    and_media_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &and_media_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(and_media_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(and_media_codec); ++i) {\n\tpj_str_t name = pj_str((char*)and_media_codec[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)and_media_codec[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)and_media_codec[i].channel_count) &&\n\t    (and_media_codec[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(and_media_factory.endpt, \"andmedaud%p\",\n                                     512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    PJ_ASSERT_RETURN(codec != NULL, PJ_ENOMEM);\n    codec->op = &and_media_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, and_media_private_t);\n    codec_data = (and_media_private_t*) codec->codec_data;\n\n    /* Create PLC if codec has no internal PLC */\n    if (!and_media_codec[idx].has_native_plc) {\n\tpj_status_t status;\n\tstatus = pjmedia_plc_create(pool, and_media_codec[idx].clock_rate,\n\t\t\t\t    and_media_codec[idx].samples_per_frame, 0,\n\t\t\t\t    &codec_data->plc);\n\tif (status != PJ_SUCCESS) {\n\t    goto on_error;\n\t}\n    }\n\n    /* Create silence detector if codec has no internal VAD */\n    if (!and_media_codec[idx].has_native_vad) {\n\tpj_status_t status;\n\tstatus = pjmedia_silence_det_create(pool,\n\t\t\t\t\tand_media_codec[idx].clock_rate,\n\t\t\t\t\tand_media_codec[idx].samples_per_frame,\n\t\t\t\t\t&codec_data->vad);\n\tif (status != PJ_SUCCESS) {\n\t    goto on_error;\n\t}\n    }\n\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    create_codec(codec_data);\n    if (!codec_data->enc || !codec_data->dec) {\n\tgoto on_error;\n    }\n    pj_mutex_unlock(and_media_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n\non_error:\n    pj_mutex_unlock(and_media_factory.mutex);\n    and_media_dealloc_codec(factory, codec);\n    return PJMEDIA_CODEC_EFAILED;\n}",
        "func": "static pj_status_t and_media_alloc_codec(pjmedia_codec_factory *factory,\n\t\t\t\t\t const pjmedia_codec_info *id,\n\t\t\t\t\t pjmedia_codec **p_codec)\n{\n    and_media_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &and_media_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(and_media_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(and_media_codec); ++i) {\n\tpj_str_t name = pj_str((char*)and_media_codec[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)and_media_codec[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)and_media_codec[i].channel_count) &&\n\t    (and_media_codec[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\tpj_mutex_unlock(and_media_factory.mutex);\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(and_media_factory.endpt, \"andmedaud%p\",\n                                     512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    PJ_ASSERT_RETURN(codec != NULL, PJ_ENOMEM);\n    codec->op = &and_media_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, and_media_private_t);\n    codec_data = (and_media_private_t*) codec->codec_data;\n\n    /* Create PLC if codec has no internal PLC */\n    if (!and_media_codec[idx].has_native_plc) {\n\tpj_status_t status;\n\tstatus = pjmedia_plc_create(pool, and_media_codec[idx].clock_rate,\n\t\t\t\t    and_media_codec[idx].samples_per_frame, 0,\n\t\t\t\t    &codec_data->plc);\n\tif (status != PJ_SUCCESS) {\n\t    goto on_error;\n\t}\n    }\n\n    /* Create silence detector if codec has no internal VAD */\n    if (!and_media_codec[idx].has_native_vad) {\n\tpj_status_t status;\n\tstatus = pjmedia_silence_det_create(pool,\n\t\t\t\t\tand_media_codec[idx].clock_rate,\n\t\t\t\t\tand_media_codec[idx].samples_per_frame,\n\t\t\t\t\t&codec_data->vad);\n\tif (status != PJ_SUCCESS) {\n\t    goto on_error;\n\t}\n    }\n\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    create_codec(codec_data);\n    if (!codec_data->enc || !codec_data->dec) {\n\tgoto on_error;\n    }\n    pj_mutex_unlock(and_media_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n\non_error:\n    pj_mutex_unlock(and_media_factory.mutex);\n    and_media_dealloc_codec(factory, codec);\n    return PJMEDIA_CODEC_EFAILED;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,7 @@\n     }\n     if (idx == -1) {\n \t*p_codec = NULL;\n+\tpj_mutex_unlock(and_media_factory.mutex);\n \treturn PJMEDIA_CODEC_EFAILED;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpj_mutex_unlock(and_media_factory.mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/alloc_codec",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "static pj_status_t alloc_codec( pjmedia_codec_factory *factory, \n\t\t\t\tconst pjmedia_codec_info *id,\n\t\t\t\tpjmedia_codec **p_codec)\n{\n    codec_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &codec_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(codec_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(codec_desc); ++i) {\n\tpj_str_t name = pj_str((char*)codec_desc[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)codec_desc[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)codec_desc[i].channel_count) &&\n\t    (codec_desc[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\treturn PJMEDIA_CODEC_EUNSUP;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(codec_factory.endpt, \"passthroughcodec\",\n\t\t\t\t     512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    codec->op = &codec_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, codec_private_t);\n    codec_data = (codec_private_t*) codec->codec_data;\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    pj_mutex_unlock(codec_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n}",
        "func": "static pj_status_t alloc_codec( pjmedia_codec_factory *factory, \n\t\t\t\tconst pjmedia_codec_info *id,\n\t\t\t\tpjmedia_codec **p_codec)\n{\n    codec_private_t *codec_data;\n    pjmedia_codec *codec;\n    int idx;\n    pj_pool_t *pool;\n    unsigned i;\n\n    PJ_ASSERT_RETURN(factory && id && p_codec, PJ_EINVAL);\n    PJ_ASSERT_RETURN(factory == &codec_factory.base, PJ_EINVAL);\n\n    pj_mutex_lock(codec_factory.mutex);\n\n    /* Find codec's index */\n    idx = -1;\n    for (i = 0; i < PJ_ARRAY_SIZE(codec_desc); ++i) {\n\tpj_str_t name = pj_str((char*)codec_desc[i].name);\n\tif ((pj_stricmp(&id->encoding_name, &name) == 0) &&\n\t    (id->clock_rate == (unsigned)codec_desc[i].clock_rate) &&\n\t    (id->channel_cnt == (unsigned)codec_desc[i].channel_count) &&\n\t    (codec_desc[i].enabled))\n\t{\n\t    idx = i;\n\t    break;\n\t}\n    }\n    if (idx == -1) {\n\t*p_codec = NULL;\n\tpj_mutex_unlock(codec_factory.mutex);\n\treturn PJMEDIA_CODEC_EUNSUP;\n    }\n\n    /* Create pool for codec instance */\n    pool = pjmedia_endpt_create_pool(codec_factory.endpt, \"passthroughcodec\",\n\t\t\t\t     512, 512);\n    codec = PJ_POOL_ZALLOC_T(pool, pjmedia_codec);\n    codec->op = &codec_op;\n    codec->factory = factory;\n    codec->codec_data = PJ_POOL_ZALLOC_T(pool, codec_private_t);\n    codec_data = (codec_private_t*) codec->codec_data;\n    codec_data->pool = pool;\n    codec_data->codec_idx = idx;\n\n    pj_mutex_unlock(codec_factory.mutex);\n\n    *p_codec = codec;\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,6 +28,7 @@\n     }\n     if (idx == -1) {\n \t*p_codec = NULL;\n+\tpj_mutex_unlock(codec_factory.mutex);\n \treturn PJMEDIA_CODEC_EUNSUP;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpj_mutex_unlock(codec_factory.mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/codec_open",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "static pj_status_t  codec_open( pjmedia_codec *codec,\n\t\t\t\tpjmedia_codec_param *attr )\n{\n    struct opus_data *opus_data = (struct opus_data *)codec->codec_data;\n    int idx, err;\n    pj_bool_t auto_bit_rate = PJ_TRUE;\n\n    PJ_ASSERT_RETURN(codec && attr && opus_data, PJ_EINVAL);\n\n    pj_mutex_lock (opus_data->mutex);\n\n    TRACE_((THIS_FILE, \"%s:%d: - TRACE\", __FUNCTION__, __LINE__));\n\n    opus_data->cfg.sample_rate = attr->info.clock_rate;\n    opus_data->cfg.channel_cnt = attr->info.channel_cnt;\n    opus_data->enc_ptime = opus_data->dec_ptime = attr->info.frm_ptime;\n\n    /* Allocate memory used by the codec */\n    if (!opus_data->enc) {\n\t/* Allocate memory for max 2 channels */\n\topus_data->enc = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\topus_encoder_get_size(2));\n    }\n    if (!opus_data->dec) {\n\t/* Allocate memory for max 2 channels */\n\topus_data->dec = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\topus_decoder_get_size(2));\n    }\n    if (!opus_data->enc_packer) {\n\topus_data->enc_packer = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\t       opus_repacketizer_get_size());\n    }\n    if (!opus_data->dec_packer) {\n\topus_data->dec_packer = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\t       opus_repacketizer_get_size());\n    }\n    if (!opus_data->enc || !opus_data->dec ||\n\t!opus_data->enc_packer || !opus_data->dec_packer)\n    {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to allocate memory for the codec\"));\n        pj_mutex_unlock (opus_data->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Check max average bit rate */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_MAX_BIT_RATE, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned rate;\n\tauto_bit_rate = PJ_FALSE;\n\trate = (unsigned)pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tif (rate < attr->info.avg_bps)\n\t    attr->info.avg_bps = rate;\n    }\n\n    /* Check plc */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_INBAND_FEC, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned plc;\n\tplc = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tattr->setting.plc = plc > 0? PJ_TRUE: PJ_FALSE;\n    }\n\n    /* Check vad */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_DTX, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned vad;\n\tvad = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tattr->setting.vad = vad > 0? PJ_TRUE: PJ_FALSE;\n    }\n\n    /* Check cbr */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_CBR, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned cbr;\n\tcbr = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\topus_data->cfg.cbr = cbr > 0? PJ_TRUE: PJ_FALSE;\n    }\n    \n    /* Check max average bit rate */\n    idx = find_fmtp(&attr->setting.dec_fmtp, &STR_MAX_BIT_RATE, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned rate;\n\trate = (unsigned) pj_strtoul(&attr->setting.dec_fmtp.param[idx].val);\n\tif (rate < attr->info.avg_bps)\n\t    attr->info.avg_bps = rate;\n    }\n\n    TRACE_((THIS_FILE, \"%s:%d: sample_rate: %u\",\n\t    __FUNCTION__, __LINE__, opus_data->cfg.sample_rate));\n\n    /* Initialize encoder */\n    err = opus_encoder_init(opus_data->enc,\n\t\t\t    opus_data->cfg.sample_rate,\n\t\t\t    attr->info.channel_cnt,\n\t\t\t    OPUS_APPLICATION_VOIP);\n    if (err != OPUS_OK) {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to create encoder\"));\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n    \n    /* Set signal type */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_SIGNAL(OPUS_SIGNAL_VOICE));\n    /* Set bitrate */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_BITRATE(auto_bit_rate?\n    \t\t\t\t\t\t      OPUS_AUTO:\n    \t\t\t\t\t\t      attr->info.avg_bps));\n    /* Set VAD */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_DTX(attr->setting.vad ? 1 : 0));\n    /* Set PLC */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_INBAND_FEC(attr->setting.plc ? 1 : 0));\n    /* Set bandwidth */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_MAX_BANDWIDTH(get_opus_bw_constant(\n    \t\t\t\t\t    opus_data->cfg.sample_rate)));\n    /* Set expected packet loss */\n    opus_encoder_ctl(opus_data->enc,\n    \t\tOPUS_SET_PACKET_LOSS_PERC(opus_data->cfg.packet_loss));\n    /* Set complexity */\n    opus_encoder_ctl(opus_data->enc,\n\t\t     OPUS_SET_COMPLEXITY(opus_data->cfg.complexity));\n    /* Set constant bit rate */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_VBR(opus_data->cfg.cbr ? 0 : 1));\n\n    PJ_LOG(5, (THIS_FILE, \"Initialize Opus encoder, sample rate: %d, \"\n    \t\t\t  \"avg bitrate: %d, vad: %d, plc: %d, pkt loss: %d, \"\n    \t\t\t  \"complexity: %d, constant bit rate: %d\",\n               \t\t  opus_data->cfg.sample_rate,\n               \t\t  attr->info.avg_bps, attr->setting.vad?1:0,\n               \t\t  attr->setting.plc?1:0,\n               \t\t  opus_data->cfg.packet_loss,\n               \t\t  opus_data->cfg.complexity,\n               \t\t  opus_data->cfg.cbr?1:0));\n\n    /* Initialize decoder */\n    err = opus_decoder_init (opus_data->dec,\n\t\t\t     opus_data->cfg.sample_rate,\n\t\t\t     attr->info.channel_cnt);\n    if (err != OPUS_OK) {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to initialize decoder\"));\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Initialize temporary decode frames used for FEC */\n    opus_data->dec_frame[0].type = PJMEDIA_FRAME_TYPE_NONE;\n    opus_data->dec_frame[0].buf  = pj_pool_zalloc(opus_data->pool,                                   \n        \t(opus_data->cfg.sample_rate / 1000)\n                * 60 * attr->info.channel_cnt * 2 /* bytes per sample */);\n    opus_data->dec_frame[1].type = PJMEDIA_FRAME_TYPE_NONE;\n    opus_data->dec_frame[1].buf  = pj_pool_zalloc(opus_data->pool,\n\t\t(opus_data->cfg.sample_rate / 1000)\n                * 60 * attr->info.channel_cnt * 2 /* bytes per sample */);\n    opus_data->dec_frame_index = -1;\n\n    /* Initialize the repacketizers */\n    opus_repacketizer_init(opus_data->enc_packer);\n    opus_repacketizer_init(opus_data->dec_packer);\n\n    pj_mutex_unlock (opus_data->mutex);\n    return PJ_SUCCESS;\n}",
        "func": "static pj_status_t  codec_open( pjmedia_codec *codec,\n\t\t\t\tpjmedia_codec_param *attr )\n{\n    struct opus_data *opus_data = (struct opus_data *)codec->codec_data;\n    int idx, err;\n    pj_bool_t auto_bit_rate = PJ_TRUE;\n\n    PJ_ASSERT_RETURN(codec && attr && opus_data, PJ_EINVAL);\n\n    pj_mutex_lock (opus_data->mutex);\n\n    TRACE_((THIS_FILE, \"%s:%d: - TRACE\", __FUNCTION__, __LINE__));\n\n    opus_data->cfg.sample_rate = attr->info.clock_rate;\n    opus_data->cfg.channel_cnt = attr->info.channel_cnt;\n    opus_data->enc_ptime = opus_data->dec_ptime = attr->info.frm_ptime;\n\n    /* Allocate memory used by the codec */\n    if (!opus_data->enc) {\n\t/* Allocate memory for max 2 channels */\n\topus_data->enc = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\topus_encoder_get_size(2));\n    }\n    if (!opus_data->dec) {\n\t/* Allocate memory for max 2 channels */\n\topus_data->dec = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\topus_decoder_get_size(2));\n    }\n    if (!opus_data->enc_packer) {\n\topus_data->enc_packer = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\t       opus_repacketizer_get_size());\n    }\n    if (!opus_data->dec_packer) {\n\topus_data->dec_packer = pj_pool_zalloc(opus_data->pool,\n\t\t\t\t\t       opus_repacketizer_get_size());\n    }\n    if (!opus_data->enc || !opus_data->dec ||\n\t!opus_data->enc_packer || !opus_data->dec_packer)\n    {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to allocate memory for the codec\"));\n        pj_mutex_unlock (opus_data->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Check max average bit rate */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_MAX_BIT_RATE, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned rate;\n\tauto_bit_rate = PJ_FALSE;\n\trate = (unsigned)pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tif (rate < attr->info.avg_bps)\n\t    attr->info.avg_bps = rate;\n    }\n\n    /* Check plc */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_INBAND_FEC, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned plc;\n\tplc = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tattr->setting.plc = plc > 0? PJ_TRUE: PJ_FALSE;\n    }\n\n    /* Check vad */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_DTX, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned vad;\n\tvad = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\tattr->setting.vad = vad > 0? PJ_TRUE: PJ_FALSE;\n    }\n\n    /* Check cbr */\n    idx = find_fmtp(&attr->setting.enc_fmtp, &STR_CBR, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned cbr;\n\tcbr = (unsigned) pj_strtoul(&attr->setting.enc_fmtp.param[idx].val);\n\topus_data->cfg.cbr = cbr > 0? PJ_TRUE: PJ_FALSE;\n    }\n    \n    /* Check max average bit rate */\n    idx = find_fmtp(&attr->setting.dec_fmtp, &STR_MAX_BIT_RATE, PJ_FALSE);\n    if (idx >= 0) {\n\tunsigned rate;\n\trate = (unsigned) pj_strtoul(&attr->setting.dec_fmtp.param[idx].val);\n\tif (rate < attr->info.avg_bps)\n\t    attr->info.avg_bps = rate;\n    }\n\n    TRACE_((THIS_FILE, \"%s:%d: sample_rate: %u\",\n\t    __FUNCTION__, __LINE__, opus_data->cfg.sample_rate));\n\n    /* Initialize encoder */\n    err = opus_encoder_init(opus_data->enc,\n\t\t\t    opus_data->cfg.sample_rate,\n\t\t\t    attr->info.channel_cnt,\n\t\t\t    OPUS_APPLICATION_VOIP);\n    if (err != OPUS_OK) {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to create encoder\"));\n\tpj_mutex_unlock (opus_data->mutex);\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n    \n    /* Set signal type */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_SIGNAL(OPUS_SIGNAL_VOICE));\n    /* Set bitrate */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_BITRATE(auto_bit_rate?\n    \t\t\t\t\t\t      OPUS_AUTO:\n    \t\t\t\t\t\t      attr->info.avg_bps));\n    /* Set VAD */\n    opus_encoder_ctl(opus_data->enc, OPUS_SET_DTX(attr->setting.vad ? 1 : 0));\n    /* Set PLC */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_INBAND_FEC(attr->setting.plc ? 1 : 0));\n    /* Set bandwidth */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_MAX_BANDWIDTH(get_opus_bw_constant(\n    \t\t\t\t\t    opus_data->cfg.sample_rate)));\n    /* Set expected packet loss */\n    opus_encoder_ctl(opus_data->enc,\n    \t\tOPUS_SET_PACKET_LOSS_PERC(opus_data->cfg.packet_loss));\n    /* Set complexity */\n    opus_encoder_ctl(opus_data->enc,\n\t\t     OPUS_SET_COMPLEXITY(opus_data->cfg.complexity));\n    /* Set constant bit rate */\n    opus_encoder_ctl(opus_data->enc,\n    \t\t     OPUS_SET_VBR(opus_data->cfg.cbr ? 0 : 1));\n\n    PJ_LOG(5, (THIS_FILE, \"Initialize Opus encoder, sample rate: %d, \"\n    \t\t\t  \"avg bitrate: %d, vad: %d, plc: %d, pkt loss: %d, \"\n    \t\t\t  \"complexity: %d, constant bit rate: %d\",\n               \t\t  opus_data->cfg.sample_rate,\n               \t\t  attr->info.avg_bps, attr->setting.vad?1:0,\n               \t\t  attr->setting.plc?1:0,\n               \t\t  opus_data->cfg.packet_loss,\n               \t\t  opus_data->cfg.complexity,\n               \t\t  opus_data->cfg.cbr?1:0));\n\n    /* Initialize decoder */\n    err = opus_decoder_init (opus_data->dec,\n\t\t\t     opus_data->cfg.sample_rate,\n\t\t\t     attr->info.channel_cnt);\n    if (err != OPUS_OK) {\n\tPJ_LOG(2, (THIS_FILE, \"Unable to initialize decoder\"));\n\tpj_mutex_unlock (opus_data->mutex);\n\treturn PJMEDIA_CODEC_EFAILED;\n    }\n\n    /* Initialize temporary decode frames used for FEC */\n    opus_data->dec_frame[0].type = PJMEDIA_FRAME_TYPE_NONE;\n    opus_data->dec_frame[0].buf  = pj_pool_zalloc(opus_data->pool,                                   \n        \t(opus_data->cfg.sample_rate / 1000)\n                * 60 * attr->info.channel_cnt * 2 /* bytes per sample */);\n    opus_data->dec_frame[1].type = PJMEDIA_FRAME_TYPE_NONE;\n    opus_data->dec_frame[1].buf  = pj_pool_zalloc(opus_data->pool,\n\t\t(opus_data->cfg.sample_rate / 1000)\n                * 60 * attr->info.channel_cnt * 2 /* bytes per sample */);\n    opus_data->dec_frame_index = -1;\n\n    /* Initialize the repacketizers */\n    opus_repacketizer_init(opus_data->enc_packer);\n    opus_repacketizer_init(opus_data->dec_packer);\n\n    pj_mutex_unlock (opus_data->mutex);\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -95,6 +95,7 @@\n \t\t\t    OPUS_APPLICATION_VOIP);\n     if (err != OPUS_OK) {\n \tPJ_LOG(2, (THIS_FILE, \"Unable to create encoder\"));\n+\tpj_mutex_unlock (opus_data->mutex);\n \treturn PJMEDIA_CODEC_EFAILED;\n     }\n     \n@@ -139,6 +140,7 @@\n \t\t\t     attr->info.channel_cnt);\n     if (err != OPUS_OK) {\n \tPJ_LOG(2, (THIS_FILE, \"Unable to initialize decoder\"));\n+\tpj_mutex_unlock (opus_data->mutex);\n \treturn PJMEDIA_CODEC_EFAILED;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpj_mutex_unlock (opus_data->mutex);",
                "\tpj_mutex_unlock (opus_data->mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/pjmedia_vid_conf_remove_port",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_vid_conf_remove_port( pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t\t  unsigned slot)\n{\n    vconf_port *cport;\n\n    PJ_ASSERT_RETURN(vid_conf && slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Port must be valid. */\n    cport = vid_conf->ports[slot];\n    if (cport == NULL) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Disconnect slot -> listeners */\n    while (cport->listener_cnt) {\n\tpjmedia_vid_conf_disconnect_port(vid_conf, slot,\n\t\t\t\t\t cport->listener_slots[0]);\n    }\n\n    /* Disconnect transmitters -> slot */\n    while (cport->transmitter_cnt) {\n\tpjmedia_vid_conf_disconnect_port(vid_conf,\n\t\t\t\t\t cport->transmitter_slots[0], slot);\n    }\n\n    /* Remove the port. */\n    vid_conf->ports[slot] = NULL;\n    --vid_conf->port_cnt;\n\n    PJ_LOG(4,(THIS_FILE,\"Removed port %d (%.*s)\",\n\t      slot, (int)cport->name.slen, cport->name.ptr));\n\n    /* Release pool */\n    pj_pool_safe_release(&cport->pool);\n\n    if (AUTO_STOP_CLOCK && vid_conf->connect_cnt == 0) {\n\tpj_status_t status;\n\n\t/* Warning: will stuck if this is called from the clock thread */\n\tstatus = pjmedia_clock_stop(vid_conf->clock);\n\tif (status != PJ_SUCCESS) {\n\t    PJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n\t    return status;\n\t}\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_vid_conf_remove_port( pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t\t  unsigned slot)\n{\n    vconf_port *cport;\n\n    PJ_ASSERT_RETURN(vid_conf && slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Port must be valid. */\n    cport = vid_conf->ports[slot];\n    if (cport == NULL) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Disconnect slot -> listeners */\n    while (cport->listener_cnt) {\n\tpjmedia_vid_conf_disconnect_port(vid_conf, slot,\n\t\t\t\t\t cport->listener_slots[0]);\n    }\n\n    /* Disconnect transmitters -> slot */\n    while (cport->transmitter_cnt) {\n\tpjmedia_vid_conf_disconnect_port(vid_conf,\n\t\t\t\t\t cport->transmitter_slots[0], slot);\n    }\n\n    /* Remove the port. */\n    vid_conf->ports[slot] = NULL;\n    --vid_conf->port_cnt;\n\n    PJ_LOG(4,(THIS_FILE,\"Removed port %d (%.*s)\",\n\t      slot, (int)cport->name.slen, cport->name.ptr));\n\n    /* Release pool */\n    pj_pool_safe_release(&cport->pool);\n\n    if (AUTO_STOP_CLOCK && vid_conf->connect_cnt == 0) {\n\tpj_status_t status;\n\n\t/* Warning: will stuck if this is called from the clock thread */\n\tstatus = pjmedia_clock_stop(vid_conf->clock);\n\tif (status != PJ_SUCCESS) {\n\t    PJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n\t    pj_mutex_unlock(vid_conf->mutex);\n\t    return status;\n\t}\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,6 +43,7 @@\n \tstatus = pjmedia_clock_stop(vid_conf->clock);\n \tif (status != PJ_SUCCESS) {\n \t    PJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n+\t    pj_mutex_unlock(vid_conf->mutex);\n \t    return status;\n \t}\n     }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t    pj_mutex_unlock(vid_conf->mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/pjmedia_vid_conf_connect_port",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_vid_conf_connect_port(\n\t\t\t\t\t    pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t    unsigned src_slot,\n\t\t\t\t\t    unsigned sink_slot,\n\t\t\t\t\t    void *opt)\n{\n    vconf_port *src_port, *dst_port;\n    unsigned i;\n\n    /* Check arguments */\n    PJ_ASSERT_RETURN(vid_conf &&\n\t\t     src_slot<vid_conf->opt.max_slot_cnt && \n\t\t     sink_slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n    PJ_UNUSED_ARG(opt);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Ports must be valid. */\n    src_port = vid_conf->ports[src_slot];\n    dst_port = vid_conf->ports[sink_slot];\n    if (!src_port || !src_port->port->get_frame ||\n\t!dst_port || !dst_port->port->put_frame)\n    {\n\tPJ_LOG(4,(THIS_FILE,\"Failed connecting video ports, make sure that \"\n\t\t\t    \"source has get_frame() & sink has put_frame()\"));\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Check if connection has been made */\n    for (i=0; i<src_port->listener_cnt; ++i) {\n\tif (src_port->listener_slots[i] == sink_slot)\n\t    break;\n    }\n\n    if (i == src_port->listener_cnt) {\n\tsrc_port->listener_slots[src_port->listener_cnt] = sink_slot;\n\tdst_port->transmitter_slots[dst_port->transmitter_cnt] = src_slot;\n\t++src_port->listener_cnt;\n\t++dst_port->transmitter_cnt;\n\n\tif (src_port->listener_cnt == 1) {\n    \t    /* If this is the first listener, initialize source's buffer\n    \t     * with black color.\n    \t     */\n\t    const pjmedia_video_format_info *vfi;\n\t    pjmedia_video_apply_fmt_param vafp;\n\n\t    vfi = pjmedia_get_video_format_info(NULL,\n\t    \t\t\t\t\tsrc_port->port->info.fmt.id);\n\t    pj_bzero(&vafp, sizeof(vafp));\n\t    vafp.size = src_port->port->info.fmt.det.vid.size;\n\t    (*vfi->apply_fmt)(vfi, &vafp);\n\n\t    if (vfi->color_model == PJMEDIA_COLOR_MODEL_RGB) {\n\t    \tpj_memset(src_port->get_buf, 0, vafp.framebytes);\n\t    } else if (src_port->port->info.fmt.id == PJMEDIA_FORMAT_I420 ||\n\t  \t       src_port->port->info.fmt.id == PJMEDIA_FORMAT_YV12)\n\t    {\t    \t\n\t    \tpj_memset(src_port->get_buf, 16, vafp.plane_bytes[0]);\n\t    \tpj_memset((pj_uint8_t*)src_port->get_buf + vafp.plane_bytes[0],\n\t\t      \t  0x80, vafp.plane_bytes[1] * 2);\n\t    }\n\t}\n\n\tupdate_render_state(vid_conf, dst_port);\n\n\t++vid_conf->connect_cnt;\n\tif (vid_conf->connect_cnt == 1) {\n\t    pj_status_t status;\n\t    status = pjmedia_clock_start(vid_conf->clock);\n\t    if (status != PJ_SUCCESS) {\n\t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to start clock\"));\n\t\treturn status;\n\t    }\n\t}\n\n\tPJ_LOG(4,(THIS_FILE,\"Port %d (%.*s) transmitting to port %d (%.*s)\",\n\t\t  src_slot,\n\t\t  (int)src_port->name.slen,\n\t\t  src_port->name.ptr,\n\t\t  sink_slot,\n\t\t  (int)dst_port->name.slen,\n\t\t  dst_port->name.ptr));\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_vid_conf_connect_port(\n\t\t\t\t\t    pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t    unsigned src_slot,\n\t\t\t\t\t    unsigned sink_slot,\n\t\t\t\t\t    void *opt)\n{\n    vconf_port *src_port, *dst_port;\n    unsigned i;\n\n    /* Check arguments */\n    PJ_ASSERT_RETURN(vid_conf &&\n\t\t     src_slot<vid_conf->opt.max_slot_cnt && \n\t\t     sink_slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n    PJ_UNUSED_ARG(opt);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Ports must be valid. */\n    src_port = vid_conf->ports[src_slot];\n    dst_port = vid_conf->ports[sink_slot];\n    if (!src_port || !src_port->port->get_frame ||\n\t!dst_port || !dst_port->port->put_frame)\n    {\n\tPJ_LOG(4,(THIS_FILE,\"Failed connecting video ports, make sure that \"\n\t\t\t    \"source has get_frame() & sink has put_frame()\"));\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Check if connection has been made */\n    for (i=0; i<src_port->listener_cnt; ++i) {\n\tif (src_port->listener_slots[i] == sink_slot)\n\t    break;\n    }\n\n    if (i == src_port->listener_cnt) {\n\tsrc_port->listener_slots[src_port->listener_cnt] = sink_slot;\n\tdst_port->transmitter_slots[dst_port->transmitter_cnt] = src_slot;\n\t++src_port->listener_cnt;\n\t++dst_port->transmitter_cnt;\n\n\tif (src_port->listener_cnt == 1) {\n    \t    /* If this is the first listener, initialize source's buffer\n    \t     * with black color.\n    \t     */\n\t    const pjmedia_video_format_info *vfi;\n\t    pjmedia_video_apply_fmt_param vafp;\n\n\t    vfi = pjmedia_get_video_format_info(NULL,\n\t    \t\t\t\t\tsrc_port->port->info.fmt.id);\n\t    pj_bzero(&vafp, sizeof(vafp));\n\t    vafp.size = src_port->port->info.fmt.det.vid.size;\n\t    (*vfi->apply_fmt)(vfi, &vafp);\n\n\t    if (vfi->color_model == PJMEDIA_COLOR_MODEL_RGB) {\n\t    \tpj_memset(src_port->get_buf, 0, vafp.framebytes);\n\t    } else if (src_port->port->info.fmt.id == PJMEDIA_FORMAT_I420 ||\n\t  \t       src_port->port->info.fmt.id == PJMEDIA_FORMAT_YV12)\n\t    {\t    \t\n\t    \tpj_memset(src_port->get_buf, 16, vafp.plane_bytes[0]);\n\t    \tpj_memset((pj_uint8_t*)src_port->get_buf + vafp.plane_bytes[0],\n\t\t      \t  0x80, vafp.plane_bytes[1] * 2);\n\t    }\n\t}\n\n\tupdate_render_state(vid_conf, dst_port);\n\n\t++vid_conf->connect_cnt;\n\tif (vid_conf->connect_cnt == 1) {\n\t    pj_status_t status;\n\t    status = pjmedia_clock_start(vid_conf->clock);\n\t    if (status != PJ_SUCCESS) {\n\t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to start clock\"));\n\t\tpj_mutex_unlock(vid_conf->mutex);\n\t\treturn status;\n\t    }\n\t}\n\n\tPJ_LOG(4,(THIS_FILE,\"Port %d (%.*s) transmitting to port %d (%.*s)\",\n\t\t  src_slot,\n\t\t  (int)src_port->name.slen,\n\t\t  src_port->name.ptr,\n\t\t  sink_slot,\n\t\t  (int)dst_port->name.slen,\n\t\t  dst_port->name.ptr));\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -71,6 +71,7 @@\n \t    status = pjmedia_clock_start(vid_conf->clock);\n \t    if (status != PJ_SUCCESS) {\n \t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to start clock\"));\n+\t\tpj_mutex_unlock(vid_conf->mutex);\n \t\treturn status;\n \t    }\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tpj_mutex_unlock(vid_conf->mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/pjmedia_vid_conf_disconnect_port",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_vid_conf_disconnect_port(\n\t\t\t\t\t    pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t    unsigned src_slot,\n\t\t\t\t\t    unsigned sink_slot)\n{\n    vconf_port *src_port, *dst_port;\n    unsigned i, j;\n\n    /* Check arguments */\n    PJ_ASSERT_RETURN(vid_conf &&\n\t\t     src_slot<vid_conf->opt.max_slot_cnt && \n\t\t     sink_slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Ports must be valid. */\n    src_port = vid_conf->ports[src_slot];\n    dst_port = vid_conf->ports[sink_slot];\n    if (!src_port || !dst_port) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Check if connection has been made */\n    for (i=0; i<src_port->listener_cnt; ++i) {\n\tif (src_port->listener_slots[i] == sink_slot)\n\t    break;\n    }\n    for (j=0; j<dst_port->transmitter_cnt; ++j) {\n\tif (dst_port->transmitter_slots[j] == src_slot)\n\t    break;\n    }\n\n    if (i != src_port->listener_cnt && j != dst_port->transmitter_cnt) {\n\tunsigned k;\n\n\tpj_assert(src_port->listener_cnt > 0 && \n\t\t  src_port->listener_cnt < vid_conf->opt.max_slot_cnt);\n\tpj_assert(dst_port->transmitter_cnt > 0 && \n\t\t  dst_port->transmitter_cnt < vid_conf->opt.max_slot_cnt);\n\n\t/* Cleanup all render states of the sink */\n\tfor (k=0; k<dst_port->transmitter_cnt; ++k)\n\t    cleanup_render_state(dst_port, k);\n\n\t/* Update listeners array of the source and transmitters array of\n\t * the sink.\n\t */\n\tpj_array_erase(src_port->listener_slots, sizeof(unsigned), \n\t\t       src_port->listener_cnt, i);\n\tpj_array_erase(dst_port->transmitter_slots, sizeof(unsigned), \n\t\t       dst_port->transmitter_cnt, j);\n\t--src_port->listener_cnt;\n\t--dst_port->transmitter_cnt;\n\n\t/* Update render states of the sink */\n\tupdate_render_state(vid_conf, dst_port);\n\n\t--vid_conf->connect_cnt;\n\n\tif (AUTO_STOP_CLOCK && vid_conf->connect_cnt == 0) {\n\t    pj_status_t status;\n\t    /* Warning: will stuck if this is called from the clock thread */\n\t    status = pjmedia_clock_stop(vid_conf->clock);\n\t    if (status != PJ_SUCCESS) {\n\t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n\t\treturn status;\n\t    }\n\t}\n\n\tPJ_LOG(4,(THIS_FILE,\n\t\t  \"Port %d (%.*s) stop transmitting to port %d (%.*s)\",\n\t\t  src_slot,\n\t\t  (int)src_port->name.slen,\n\t\t  src_port->name.ptr,\n\t\t  sink_slot,\n\t\t  (int)dst_port->name.slen,\n\t\t  dst_port->name.ptr));\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_vid_conf_disconnect_port(\n\t\t\t\t\t    pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t    unsigned src_slot,\n\t\t\t\t\t    unsigned sink_slot)\n{\n    vconf_port *src_port, *dst_port;\n    unsigned i, j;\n\n    /* Check arguments */\n    PJ_ASSERT_RETURN(vid_conf &&\n\t\t     src_slot<vid_conf->opt.max_slot_cnt && \n\t\t     sink_slot<vid_conf->opt.max_slot_cnt, PJ_EINVAL);\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    /* Ports must be valid. */\n    src_port = vid_conf->ports[src_slot];\n    dst_port = vid_conf->ports[sink_slot];\n    if (!src_port || !dst_port) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_EINVAL;\n    }\n\n    /* Check if connection has been made */\n    for (i=0; i<src_port->listener_cnt; ++i) {\n\tif (src_port->listener_slots[i] == sink_slot)\n\t    break;\n    }\n    for (j=0; j<dst_port->transmitter_cnt; ++j) {\n\tif (dst_port->transmitter_slots[j] == src_slot)\n\t    break;\n    }\n\n    if (i != src_port->listener_cnt && j != dst_port->transmitter_cnt) {\n\tunsigned k;\n\n\tpj_assert(src_port->listener_cnt > 0 && \n\t\t  src_port->listener_cnt < vid_conf->opt.max_slot_cnt);\n\tpj_assert(dst_port->transmitter_cnt > 0 && \n\t\t  dst_port->transmitter_cnt < vid_conf->opt.max_slot_cnt);\n\n\t/* Cleanup all render states of the sink */\n\tfor (k=0; k<dst_port->transmitter_cnt; ++k)\n\t    cleanup_render_state(dst_port, k);\n\n\t/* Update listeners array of the source and transmitters array of\n\t * the sink.\n\t */\n\tpj_array_erase(src_port->listener_slots, sizeof(unsigned), \n\t\t       src_port->listener_cnt, i);\n\tpj_array_erase(dst_port->transmitter_slots, sizeof(unsigned), \n\t\t       dst_port->transmitter_cnt, j);\n\t--src_port->listener_cnt;\n\t--dst_port->transmitter_cnt;\n\n\t/* Update render states of the sink */\n\tupdate_render_state(vid_conf, dst_port);\n\n\t--vid_conf->connect_cnt;\n\n\tif (AUTO_STOP_CLOCK && vid_conf->connect_cnt == 0) {\n\t    pj_status_t status;\n\t    /* Warning: will stuck if this is called from the clock thread */\n\t    status = pjmedia_clock_stop(vid_conf->clock);\n\t    if (status != PJ_SUCCESS) {\n\t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n\t\tpj_mutex_unlock(vid_conf->mutex);\n\t\treturn status;\n\t    }\n\t}\n\n\tPJ_LOG(4,(THIS_FILE,\n\t\t  \"Port %d (%.*s) stop transmitting to port %d (%.*s)\",\n\t\t  src_slot,\n\t\t  (int)src_port->name.slen,\n\t\t  src_port->name.ptr,\n\t\t  sink_slot,\n\t\t  (int)dst_port->name.slen,\n\t\t  dst_port->name.ptr));\n    }\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -64,6 +64,7 @@\n \t    status = pjmedia_clock_stop(vid_conf->clock);\n \t    if (status != PJ_SUCCESS) {\n \t\tPJ_PERROR(4, (THIS_FILE, status, \"Failed to stop clock\"));\n+\t\tpj_mutex_unlock(vid_conf->mutex);\n \t\treturn status;\n \t    }\n \t}",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tpj_mutex_unlock(vid_conf->mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/pjmedia_vid_conf_add_port",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_vid_conf_add_port( pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t       pj_pool_t *parent_pool,\n\t\t\t\t\t       pjmedia_port *port,\n\t\t\t\t\t       const pj_str_t *name,\n\t\t\t\t\t       void *opt,\n\t\t\t\t\t       unsigned *p_slot)\n{\n    pj_pool_t *pool;\n    vconf_port *cport;\n    unsigned index;\n\n    PJ_ASSERT_RETURN(vid_conf && parent_pool && port, PJ_EINVAL);\n    PJ_ASSERT_RETURN(port->info.fmt.type==PJMEDIA_TYPE_VIDEO &&\n\t\t     port->info.fmt.detail_type==PJMEDIA_FORMAT_DETAIL_VIDEO,\n\t\t     PJ_EINVAL);\n    PJ_UNUSED_ARG(opt);\n\n    /* If name is not specified, use the port's name */\n    if (!name)\n\tname = &port->info.name;\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    if (vid_conf->port_cnt >= vid_conf->opt.max_slot_cnt) {\n\tpj_assert(!\"Too many ports\");\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ETOOMANY;\n    }\n\n    /* Find empty port in the conference bridge. */\n    for (index=0; index < vid_conf->opt.max_slot_cnt; ++index) {\n\tif (vid_conf->ports[index] == NULL)\n\t    break;\n    }\n    pj_assert(index != vid_conf->opt.max_slot_cnt);\n\n    /* Create pool */\n    pool = pj_pool_create(parent_pool->factory, name->ptr, 500, 500, NULL);\n    PJ_ASSERT_RETURN(pool, PJ_ENOMEM);\n\n    /* Create port. */\n    cport = PJ_POOL_ZALLOC_T(pool, vconf_port);\n    PJ_ASSERT_RETURN(cport, PJ_ENOMEM);\n\n    /* Set pool, port, index, and name */\n    cport->pool = pool;\n    cport->port = port;\n    cport->format = port->info.fmt;\n    cport->idx  = index;\n    pj_strdup_with_null(pool, &cport->name, name);\n\n    /* Init put/get_frame() intervals */\n    {\n\tpjmedia_ratio *fps = &port->info.fmt.det.vid.fps;\n\tpj_uint32_t vconf_interval = (pj_uint32_t)\n\t\t\t\t     (TS_CLOCK_RATE * 1.0 /\n\t\t\t\t     vid_conf->opt.frame_rate);\n\tcport->ts_interval = (pj_uint32_t)(TS_CLOCK_RATE * 1.0 /\n\t\t\t\t\t   fps->num * fps->denum);\n\n\t/* Normalize the interval */\n\tif (cport->ts_interval < vconf_interval) {\n\t    cport->ts_interval = vconf_interval;\n\t    PJ_LOG(3,(THIS_FILE, \"Warning: frame rate of port %s is higher \"\n\t\t\t\t \"than video conference bridge (%d > %d)\",\n\t\t\t\t name->ptr, (int)(fps->num/fps->denum),\n\t\t\t\t vid_conf->opt.frame_rate));\n\t}\n    }\n\n    /* Allocate buffer for put/get_frame() */\n    {\n\tconst pjmedia_video_format_info *vfi;\n\tpjmedia_video_apply_fmt_param vafp;\n\tpj_status_t status;\n\n\tvfi = pjmedia_get_video_format_info(NULL, port->info.fmt.id);\n\tif (!vfi) {\n\t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n\t\t\t\t \"unrecognized format %04X\",\n\t\t\t\t port->info.fmt.id));\n\t    return PJMEDIA_EBADFMT;\n\t}\n\n\tpj_bzero(&vafp, sizeof(vafp));\n\tvafp.size = port->info.fmt.det.vid.size;\n\tstatus = (*vfi->apply_fmt)(vfi, &vafp);\n\tif (status != PJ_SUCCESS) {\n\t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n\t\t\t\t \"Failed to apply format %04X\",\n\t\t\t\t port->info.fmt.id));\n\t    return status;\n\t}\n\tif (port->put_frame) {\n\t    cport->put_buf_size = vafp.framebytes;\n\t    cport->put_buf = pj_pool_zalloc(cport->pool, cport->put_buf_size);\n\t}\n\tif (port->get_frame) {\n\t    cport->get_buf_size = vafp.framebytes;\n\t    cport->get_buf = pj_pool_zalloc(cport->pool, cport->get_buf_size);\n\t}\n    }\n\n    /* Create listener array */\n    cport->listener_slots = (unsigned*)\n\t\t\t    pj_pool_zalloc(pool,\n\t\t\t\t\t   vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t   sizeof(unsigned));\n    PJ_ASSERT_RETURN(cport->listener_slots, PJ_ENOMEM);\n\n    /* Create transmitter array */\n    cport->transmitter_slots = (unsigned*)\n\t\t\t       pj_pool_zalloc(pool,\n\t\t\t\t\t      vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t      sizeof(unsigned));\n    PJ_ASSERT_RETURN(cport->transmitter_slots, PJ_ENOMEM);\n\n    /* Create pointer-to-render_state array */\n    cport->render_states = (render_state**)\n\t\t\t   pj_pool_zalloc(pool,\n\t\t\t\t\t  vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t  sizeof(render_state*));\n    PJ_ASSERT_RETURN(cport->render_states, PJ_ENOMEM);\n\n    /* Create pointer-to-render-pool array */\n    cport->render_pool = (pj_pool_t**)\n\t\t\t pj_pool_zalloc(pool,\n\t\t\t\t\tvid_conf->opt.max_slot_cnt *\n\t\t\t\t\tsizeof(pj_pool_t*));\n    PJ_ASSERT_RETURN(cport->render_pool, PJ_ENOMEM);\n\n    /* Register the conf port. */\n    vid_conf->ports[index] = cport;\n    vid_conf->port_cnt++;\n\n    PJ_LOG(4,(THIS_FILE,\"Added port %d (%.*s)\",\n\t      index, (int)cport->name.slen, cport->name.ptr));\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    /* Done. */\n    if (p_slot) {\n\t*p_slot = index;\n    }\n\n    return PJ_SUCCESS;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_vid_conf_add_port( pjmedia_vid_conf *vid_conf,\n\t\t\t\t\t       pj_pool_t *parent_pool,\n\t\t\t\t\t       pjmedia_port *port,\n\t\t\t\t\t       const pj_str_t *name,\n\t\t\t\t\t       void *opt,\n\t\t\t\t\t       unsigned *p_slot)\n{\n    pj_pool_t *pool;\n    vconf_port *cport;\n    unsigned index;\n\n    PJ_ASSERT_RETURN(vid_conf && parent_pool && port, PJ_EINVAL);\n    PJ_ASSERT_RETURN(port->info.fmt.type==PJMEDIA_TYPE_VIDEO &&\n\t\t     port->info.fmt.detail_type==PJMEDIA_FORMAT_DETAIL_VIDEO,\n\t\t     PJ_EINVAL);\n    PJ_UNUSED_ARG(opt);\n\n    /* If name is not specified, use the port's name */\n    if (!name)\n\tname = &port->info.name;\n\n    pj_mutex_lock(vid_conf->mutex);\n\n    if (vid_conf->port_cnt >= vid_conf->opt.max_slot_cnt) {\n\tpj_assert(!\"Too many ports\");\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ETOOMANY;\n    }\n\n    /* Find empty port in the conference bridge. */\n    for (index=0; index < vid_conf->opt.max_slot_cnt; ++index) {\n\tif (vid_conf->ports[index] == NULL)\n\t    break;\n    }\n    pj_assert(index != vid_conf->opt.max_slot_cnt);\n\n    /* Create pool */\n    pool = pj_pool_create(parent_pool->factory, name->ptr, 500, 500, NULL);\n    PJ_ASSERT_RETURN(pool, PJ_ENOMEM);\n\n    /* Create port. */\n    cport = PJ_POOL_ZALLOC_T(pool, vconf_port);\n    PJ_ASSERT_RETURN(cport, PJ_ENOMEM);\n\n    /* Set pool, port, index, and name */\n    cport->pool = pool;\n    cport->port = port;\n    cport->format = port->info.fmt;\n    cport->idx  = index;\n    pj_strdup_with_null(pool, &cport->name, name);\n\n    /* Init put/get_frame() intervals */\n    {\n\tpjmedia_ratio *fps = &port->info.fmt.det.vid.fps;\n\tpj_uint32_t vconf_interval = (pj_uint32_t)\n\t\t\t\t     (TS_CLOCK_RATE * 1.0 /\n\t\t\t\t     vid_conf->opt.frame_rate);\n\tcport->ts_interval = (pj_uint32_t)(TS_CLOCK_RATE * 1.0 /\n\t\t\t\t\t   fps->num * fps->denum);\n\n\t/* Normalize the interval */\n\tif (cport->ts_interval < vconf_interval) {\n\t    cport->ts_interval = vconf_interval;\n\t    PJ_LOG(3,(THIS_FILE, \"Warning: frame rate of port %s is higher \"\n\t\t\t\t \"than video conference bridge (%d > %d)\",\n\t\t\t\t name->ptr, (int)(fps->num/fps->denum),\n\t\t\t\t vid_conf->opt.frame_rate));\n\t}\n    }\n\n    /* Allocate buffer for put/get_frame() */\n    {\n\tconst pjmedia_video_format_info *vfi;\n\tpjmedia_video_apply_fmt_param vafp;\n\tpj_status_t status;\n\n\tvfi = pjmedia_get_video_format_info(NULL, port->info.fmt.id);\n\tif (!vfi) {\n\t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n\t\t\t\t \"unrecognized format %04X\",\n\t\t\t\t port->info.fmt.id));\n\t    pj_mutex_unlock(vid_conf->mutex);\n\t    return PJMEDIA_EBADFMT;\n\t}\n\n\tpj_bzero(&vafp, sizeof(vafp));\n\tvafp.size = port->info.fmt.det.vid.size;\n\tstatus = (*vfi->apply_fmt)(vfi, &vafp);\n\tif (status != PJ_SUCCESS) {\n\t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n\t\t\t\t \"Failed to apply format %04X\",\n\t\t\t\t port->info.fmt.id));\n\t    pj_mutex_unlock(vid_conf->mutex);\n\t    return status;\n\t}\n\tif (port->put_frame) {\n\t    cport->put_buf_size = vafp.framebytes;\n\t    cport->put_buf = pj_pool_zalloc(cport->pool, cport->put_buf_size);\n\t}\n\tif (port->get_frame) {\n\t    cport->get_buf_size = vafp.framebytes;\n\t    cport->get_buf = pj_pool_zalloc(cport->pool, cport->get_buf_size);\n\t}\n    }\n\n    /* Create listener array */\n    cport->listener_slots = (unsigned*)\n\t\t\t    pj_pool_zalloc(pool,\n\t\t\t\t\t   vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t   sizeof(unsigned));\n    if (!cport->listener_slots) {\t\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Create transmitter array */\n    cport->transmitter_slots = (unsigned*)\n\t\t\t       pj_pool_zalloc(pool,\n\t\t\t\t\t      vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t      sizeof(unsigned));    \n    if (!cport->transmitter_slots) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Create pointer-to-render_state array */\n    cport->render_states = (render_state**)\n\t\t\t   pj_pool_zalloc(pool,\n\t\t\t\t\t  vid_conf->opt.max_slot_cnt *\n\t\t\t\t\t  sizeof(render_state*));\n\n    if (!cport->render_states) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Create pointer-to-render-pool array */\n    cport->render_pool = (pj_pool_t**)\n\t\t\t pj_pool_zalloc(pool,\n\t\t\t\t\tvid_conf->opt.max_slot_cnt *\n\t\t\t\t\tsizeof(pj_pool_t*));    \n    if (!cport->render_pool) {\n\tpj_mutex_unlock(vid_conf->mutex);\n\treturn PJ_ENOMEM;\n    }\n\n    /* Register the conf port. */\n    vid_conf->ports[index] = cport;\n    vid_conf->port_cnt++;\n\n    PJ_LOG(4,(THIS_FILE,\"Added port %d (%.*s)\",\n\t      index, (int)cport->name.slen, cport->name.ptr));\n\n    pj_mutex_unlock(vid_conf->mutex);\n\n    /* Done. */\n    if (p_slot) {\n\t*p_slot = index;\n    }\n\n    return PJ_SUCCESS;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -79,6 +79,7 @@\n \t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n \t\t\t\t \"unrecognized format %04X\",\n \t\t\t\t port->info.fmt.id));\n+\t    pj_mutex_unlock(vid_conf->mutex);\n \t    return PJMEDIA_EBADFMT;\n \t}\n \n@@ -89,6 +90,7 @@\n \t    PJ_LOG(4,(THIS_FILE, \"pjmedia_vid_conf_add_port(): \"\n \t\t\t\t \"Failed to apply format %04X\",\n \t\t\t\t port->info.fmt.id));\n+\t    pj_mutex_unlock(vid_conf->mutex);\n \t    return status;\n \t}\n \tif (port->put_frame) {\n@@ -106,28 +108,41 @@\n \t\t\t    pj_pool_zalloc(pool,\n \t\t\t\t\t   vid_conf->opt.max_slot_cnt *\n \t\t\t\t\t   sizeof(unsigned));\n-    PJ_ASSERT_RETURN(cport->listener_slots, PJ_ENOMEM);\n+    if (!cport->listener_slots) {\t\n+\tpj_mutex_unlock(vid_conf->mutex);\n+\treturn PJ_ENOMEM;\n+    }\n \n     /* Create transmitter array */\n     cport->transmitter_slots = (unsigned*)\n \t\t\t       pj_pool_zalloc(pool,\n \t\t\t\t\t      vid_conf->opt.max_slot_cnt *\n-\t\t\t\t\t      sizeof(unsigned));\n-    PJ_ASSERT_RETURN(cport->transmitter_slots, PJ_ENOMEM);\n+\t\t\t\t\t      sizeof(unsigned));    \n+    if (!cport->transmitter_slots) {\n+\tpj_mutex_unlock(vid_conf->mutex);\n+\treturn PJ_ENOMEM;\n+    }\n \n     /* Create pointer-to-render_state array */\n     cport->render_states = (render_state**)\n \t\t\t   pj_pool_zalloc(pool,\n \t\t\t\t\t  vid_conf->opt.max_slot_cnt *\n \t\t\t\t\t  sizeof(render_state*));\n-    PJ_ASSERT_RETURN(cport->render_states, PJ_ENOMEM);\n+\n+    if (!cport->render_states) {\n+\tpj_mutex_unlock(vid_conf->mutex);\n+\treturn PJ_ENOMEM;\n+    }\n \n     /* Create pointer-to-render-pool array */\n     cport->render_pool = (pj_pool_t**)\n \t\t\t pj_pool_zalloc(pool,\n \t\t\t\t\tvid_conf->opt.max_slot_cnt *\n-\t\t\t\t\tsizeof(pj_pool_t*));\n-    PJ_ASSERT_RETURN(cport->render_pool, PJ_ENOMEM);\n+\t\t\t\t\tsizeof(pj_pool_t*));    \n+    if (!cport->render_pool) {\n+\tpj_mutex_unlock(vid_conf->mutex);\n+\treturn PJ_ENOMEM;\n+    }\n \n     /* Register the conf port. */\n     vid_conf->ports[index] = cport;",
        "diff_line_info": {
            "deleted_lines": [
                "    PJ_ASSERT_RETURN(cport->listener_slots, PJ_ENOMEM);",
                "\t\t\t\t\t      sizeof(unsigned));",
                "    PJ_ASSERT_RETURN(cport->transmitter_slots, PJ_ENOMEM);",
                "    PJ_ASSERT_RETURN(cport->render_states, PJ_ENOMEM);",
                "\t\t\t\t\tsizeof(pj_pool_t*));",
                "    PJ_ASSERT_RETURN(cport->render_pool, PJ_ENOMEM);"
            ],
            "added_lines": [
                "\t    pj_mutex_unlock(vid_conf->mutex);",
                "\t    pj_mutex_unlock(vid_conf->mutex);",
                "    if (!cport->listener_slots) {\t",
                "\tpj_mutex_unlock(vid_conf->mutex);",
                "\treturn PJ_ENOMEM;",
                "    }",
                "\t\t\t\t\t      sizeof(unsigned));    ",
                "    if (!cport->transmitter_slots) {",
                "\tpj_mutex_unlock(vid_conf->mutex);",
                "\treturn PJ_ENOMEM;",
                "    }",
                "",
                "    if (!cport->render_states) {",
                "\tpj_mutex_unlock(vid_conf->mutex);",
                "\treturn PJ_ENOMEM;",
                "    }",
                "\t\t\t\t\tsizeof(pj_pool_t*));    ",
                "    if (!cport->render_pool) {",
                "\tpj_mutex_unlock(vid_conf->mutex);",
                "\treturn PJ_ENOMEM;",
                "    }"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41141",
        "func_name": "pjsip/pjproject/pjmedia_codec_speex_deinit",
        "description": "PJSIP is a free and open source multimedia communication library written in the C language implementing standard based protocols such as SIP, SDP, RTP, STUN, TURN, and ICE. In various parts of PJSIP, when error/failure occurs, it is found that the function returns without releasing the currently held locks. This could result in a system deadlock, which cause a denial of service for the users. No release has yet been made which contains the linked fix commit. All versions up to an including 2.11.1 are affected. Users may need to manually apply the patch.",
        "git_url": "https://github.com/pjsip/pjproject/commit/1aa2c0e0fb60a1b0bf793e0d834073ffe50fb196",
        "commit_title": "Merge pull request from GHSA-8fmx-hqw7-6gmc",
        "commit_text": "",
        "func_before": "PJ_DEF(pj_status_t) pjmedia_codec_speex_deinit(void)\n{\n    pjmedia_codec_mgr *codec_mgr;\n    pj_status_t status;\n\n    if (spx_factory.pool == NULL) {\n\t/* Already deinitialized */\n\treturn PJ_SUCCESS;\n    }\n\n    pj_mutex_lock(spx_factory.mutex);\n\n    /* We don't want to deinit if there's outstanding codec. */\n    /* This is silly, as we'll always have codec in the list if\n       we ever allocate a codec! A better behavior maybe is to \n       deallocate all codecs in the list.\n    if (!pj_list_empty(&spx_factory.codec_list)) {\n\tpj_mutex_unlock(spx_factory.mutex);\n\treturn PJ_EBUSY;\n    }\n    */\n\n    /* Get the codec manager. */\n    codec_mgr = pjmedia_endpt_get_codec_mgr(spx_factory.endpt);\n    if (!codec_mgr) {\n\tpj_pool_release(spx_factory.pool);\n\tspx_factory.pool = NULL;\n\treturn PJ_EINVALIDOP;\n    }\n\n    /* Unregister Speex codec factory. */\n    status = pjmedia_codec_mgr_unregister_factory(codec_mgr,\n\t\t\t\t\t\t  &spx_factory.base);\n    \n    /* Destroy mutex. */\n    pj_mutex_unlock(spx_factory.mutex);\n    pj_mutex_destroy(spx_factory.mutex);\n    spx_factory.mutex = NULL;\n\n    /* Destroy pool. */\n    pj_pool_release(spx_factory.pool);\n    spx_factory.pool = NULL;\n\n    return status;\n}",
        "func": "PJ_DEF(pj_status_t) pjmedia_codec_speex_deinit(void)\n{\n    pjmedia_codec_mgr *codec_mgr;\n    pj_status_t status;\n\n    if (spx_factory.pool == NULL) {\n\t/* Already deinitialized */\n\treturn PJ_SUCCESS;\n    }\n\n    pj_mutex_lock(spx_factory.mutex);\n\n    /* We don't want to deinit if there's outstanding codec. */\n    /* This is silly, as we'll always have codec in the list if\n       we ever allocate a codec! A better behavior maybe is to \n       deallocate all codecs in the list.\n    if (!pj_list_empty(&spx_factory.codec_list)) {\n\tpj_mutex_unlock(spx_factory.mutex);\n\treturn PJ_EBUSY;\n    }\n    */\n\n    /* Get the codec manager. */\n    codec_mgr = pjmedia_endpt_get_codec_mgr(spx_factory.endpt);\n    if (!codec_mgr) {\n\tpj_pool_release(spx_factory.pool);\n\tspx_factory.pool = NULL;\n\tpj_mutex_unlock(spx_factory.mutex);\n\treturn PJ_EINVALIDOP;\n    }\n\n    /* Unregister Speex codec factory. */\n    status = pjmedia_codec_mgr_unregister_factory(codec_mgr,\n\t\t\t\t\t\t  &spx_factory.base);\n    \n    /* Destroy mutex. */\n    pj_mutex_unlock(spx_factory.mutex);\n    pj_mutex_destroy(spx_factory.mutex);\n    spx_factory.mutex = NULL;\n\n    /* Destroy pool. */\n    pj_pool_release(spx_factory.pool);\n    spx_factory.pool = NULL;\n\n    return status;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,7 @@\n     if (!codec_mgr) {\n \tpj_pool_release(spx_factory.pool);\n \tspx_factory.pool = NULL;\n+\tpj_mutex_unlock(spx_factory.mutex);\n \treturn PJ_EINVALIDOP;\n     }\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpj_mutex_unlock(spx_factory.mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2006-2275",
        "func_name": "torvalds/linux/sctp_inq_pop",
        "description": "Linux SCTP (lksctp) before 2.6.17 allows remote attackers to cause a denial of service (deadlock) via a large number of small messages to a receiver application that cannot process the messages quickly enough, which leads to \"spillover of the receive buffer.\"",
        "git_url": "http://git.kernel.org/git/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=7c3ceb4fb9667f34f1599a062efecf4cdc4a4ce5",
        "commit_title": "This patch fixes a deadlock situation in the receive path by allowing",
        "commit_text": "temporary spillover of the receive buffer.  - If the chunk we receive has a tsn that immediately follows the ctsn,   accept it even if we run out of receive buffer space and renege data with   higher TSNs. - Once we accept one chunk in a packet, accept all the remaining chunks   even if we run out of receive buffer space.  ",
        "func_before": "struct sctp_chunk *sctp_inq_pop(struct sctp_inq *queue)\n{\n\tstruct sctp_chunk *chunk;\n\tsctp_chunkhdr_t *ch = NULL;\n\n\t/* The assumption is that we are safe to process the chunks\n\t * at this time.\n\t */\n\n\tif ((chunk = queue->in_progress)) {\n\t\t/* There is a packet that we have been working on.\n\t\t * Any post processing work to do before we move on?\n\t\t */\n\t\tif (chunk->singleton ||\n\t\t    chunk->end_of_packet ||\n\t\t    chunk->pdiscard) {\n\t\t\tsctp_chunk_free(chunk);\n\t\t\tchunk = queue->in_progress = NULL;\n\t\t} else {\n\t\t\t/* Nothing to do. Next chunk in the packet, please. */\n\t\t\tch = (sctp_chunkhdr_t *) chunk->chunk_end;\n\n\t\t\t/* Force chunk->skb->data to chunk->chunk_end.  */\n\t\t\tskb_pull(chunk->skb,\n\t\t\t\t chunk->chunk_end - chunk->skb->data);\n\t\t}\n\t}\n\n\t/* Do we need to take the next packet out of the queue to process? */\n\tif (!chunk) {\n\t\tstruct list_head *entry;\n\n\t\t/* Is the queue empty?  */\n\t\tif (list_empty(&queue->in_chunk_list))\n\t\t\treturn NULL;\n\n\t\tentry = queue->in_chunk_list.next;\n\t\tchunk = queue->in_progress =\n\t\t\tlist_entry(entry, struct sctp_chunk, list);\n\t\tlist_del_init(entry);\n\n\t\t/* This is the first chunk in the packet.  */\n\t\tchunk->singleton = 1;\n\t\tch = (sctp_chunkhdr_t *) chunk->skb->data;\n\t}\n\n        chunk->chunk_hdr = ch;\n        chunk->chunk_end = ((__u8 *)ch) + WORD_ROUND(ntohs(ch->length));\n\t/* In the unlikely case of an IP reassembly, the skb could be\n\t * non-linear. If so, update chunk_end so that it doesn't go past\n\t * the skb->tail.\n\t */\n\tif (unlikely(skb_is_nonlinear(chunk->skb))) {\n\t\tif (chunk->chunk_end > chunk->skb->tail)\n\t\t\tchunk->chunk_end = chunk->skb->tail;\n\t}\n\tskb_pull(chunk->skb, sizeof(sctp_chunkhdr_t));\n\tchunk->subh.v = NULL; /* Subheader is no longer valid.  */\n\n\tif (chunk->chunk_end < chunk->skb->tail) {\n\t\t/* This is not a singleton */\n\t\tchunk->singleton = 0;\n\t} else if (chunk->chunk_end > chunk->skb->tail) {\n                /* RFC 2960, Section 6.10  Bundling\n\t\t *\n\t\t * Partial chunks MUST NOT be placed in an SCTP packet.\n\t\t * If the receiver detects a partial chunk, it MUST drop\n\t\t * the chunk.  \n\t\t *\n\t\t * Since the end of the chunk is past the end of our buffer\n\t\t * (which contains the whole packet, we can freely discard\n\t\t * the whole packet.\n\t\t */\n\t\tsctp_chunk_free(chunk);\n\t\tchunk = queue->in_progress = NULL;\n\n\t\treturn NULL;\n\t} else {\n\t\t/* We are at the end of the packet, so mark the chunk\n\t\t * in case we need to send a SACK.\n\t\t */\n\t\tchunk->end_of_packet = 1;\n\t}\n\n\tSCTP_DEBUG_PRINTK(\"+++sctp_inq_pop+++ chunk %p[%s],\"\n\t\t\t  \" length %d, skb->len %d\\n\",chunk,\n\t\t\t  sctp_cname(SCTP_ST_CHUNK(chunk->chunk_hdr->type)),\n\t\t\t  ntohs(chunk->chunk_hdr->length), chunk->skb->len);\n\treturn chunk;\n}",
        "func": "struct sctp_chunk *sctp_inq_pop(struct sctp_inq *queue)\n{\n\tstruct sctp_chunk *chunk;\n\tsctp_chunkhdr_t *ch = NULL;\n\n\t/* The assumption is that we are safe to process the chunks\n\t * at this time.\n\t */\n\n\tif ((chunk = queue->in_progress)) {\n\t\t/* There is a packet that we have been working on.\n\t\t * Any post processing work to do before we move on?\n\t\t */\n\t\tif (chunk->singleton ||\n\t\t    chunk->end_of_packet ||\n\t\t    chunk->pdiscard) {\n\t\t\tsctp_chunk_free(chunk);\n\t\t\tchunk = queue->in_progress = NULL;\n\t\t} else {\n\t\t\t/* Nothing to do. Next chunk in the packet, please. */\n\t\t\tch = (sctp_chunkhdr_t *) chunk->chunk_end;\n\n\t\t\t/* Force chunk->skb->data to chunk->chunk_end.  */\n\t\t\tskb_pull(chunk->skb,\n\t\t\t\t chunk->chunk_end - chunk->skb->data);\n\t\t}\n\t}\n\n\t/* Do we need to take the next packet out of the queue to process? */\n\tif (!chunk) {\n\t\tstruct list_head *entry;\n\n\t\t/* Is the queue empty?  */\n\t\tif (list_empty(&queue->in_chunk_list))\n\t\t\treturn NULL;\n\n\t\tentry = queue->in_chunk_list.next;\n\t\tchunk = queue->in_progress =\n\t\t\tlist_entry(entry, struct sctp_chunk, list);\n\t\tlist_del_init(entry);\n\n\t\t/* This is the first chunk in the packet.  */\n\t\tchunk->singleton = 1;\n\t\tch = (sctp_chunkhdr_t *) chunk->skb->data;\n\t\tchunk->data_accepted = 0;\n\t}\n\n        chunk->chunk_hdr = ch;\n        chunk->chunk_end = ((__u8 *)ch) + WORD_ROUND(ntohs(ch->length));\n\t/* In the unlikely case of an IP reassembly, the skb could be\n\t * non-linear. If so, update chunk_end so that it doesn't go past\n\t * the skb->tail.\n\t */\n\tif (unlikely(skb_is_nonlinear(chunk->skb))) {\n\t\tif (chunk->chunk_end > chunk->skb->tail)\n\t\t\tchunk->chunk_end = chunk->skb->tail;\n\t}\n\tskb_pull(chunk->skb, sizeof(sctp_chunkhdr_t));\n\tchunk->subh.v = NULL; /* Subheader is no longer valid.  */\n\n\tif (chunk->chunk_end < chunk->skb->tail) {\n\t\t/* This is not a singleton */\n\t\tchunk->singleton = 0;\n\t} else if (chunk->chunk_end > chunk->skb->tail) {\n                /* RFC 2960, Section 6.10  Bundling\n\t\t *\n\t\t * Partial chunks MUST NOT be placed in an SCTP packet.\n\t\t * If the receiver detects a partial chunk, it MUST drop\n\t\t * the chunk.  \n\t\t *\n\t\t * Since the end of the chunk is past the end of our buffer\n\t\t * (which contains the whole packet, we can freely discard\n\t\t * the whole packet.\n\t\t */\n\t\tsctp_chunk_free(chunk);\n\t\tchunk = queue->in_progress = NULL;\n\n\t\treturn NULL;\n\t} else {\n\t\t/* We are at the end of the packet, so mark the chunk\n\t\t * in case we need to send a SACK.\n\t\t */\n\t\tchunk->end_of_packet = 1;\n\t}\n\n\tSCTP_DEBUG_PRINTK(\"+++sctp_inq_pop+++ chunk %p[%s],\"\n\t\t\t  \" length %d, skb->len %d\\n\",chunk,\n\t\t\t  sctp_cname(SCTP_ST_CHUNK(chunk->chunk_hdr->type)),\n\t\t\t  ntohs(chunk->chunk_hdr->length), chunk->skb->len);\n\treturn chunk;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -42,6 +42,7 @@\n \t\t/* This is the first chunk in the packet.  */\n \t\tchunk->singleton = 1;\n \t\tch = (sctp_chunkhdr_t *) chunk->skb->data;\n+\t\tchunk->data_accepted = 0;\n \t}\n \n         chunk->chunk_hdr = ch;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tchunk->data_accepted = 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2006-2275",
        "func_name": "torvalds/linux/sctp_eat_data",
        "description": "Linux SCTP (lksctp) before 2.6.17 allows remote attackers to cause a denial of service (deadlock) via a large number of small messages to a receiver application that cannot process the messages quickly enough, which leads to \"spillover of the receive buffer.\"",
        "git_url": "http://git.kernel.org/git/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=7c3ceb4fb9667f34f1599a062efecf4cdc4a4ce5",
        "commit_title": "This patch fixes a deadlock situation in the receive path by allowing",
        "commit_text": "temporary spillover of the receive buffer.  - If the chunk we receive has a tsn that immediately follows the ctsn,   accept it even if we run out of receive buffer space and renege data with   higher TSNs. - Once we accept one chunk in a packet, accept all the remaining chunks   even if we run out of receive buffer space.  ",
        "func_before": "static int sctp_eat_data(const struct sctp_association *asoc,\n\t\t\t struct sctp_chunk *chunk,\n\t\t\t sctp_cmd_seq_t *commands)\n{\n\tsctp_datahdr_t *data_hdr;\n\tstruct sctp_chunk *err;\n\tsize_t datalen;\n\tsctp_verb_t deliver;\n\tint tmp;\n\t__u32 tsn;\n\tint account_value;\n\tstruct sock *sk = asoc->base.sk;\n\n\tdata_hdr = chunk->subh.data_hdr = (sctp_datahdr_t *)chunk->skb->data;\n\tskb_pull(chunk->skb, sizeof(sctp_datahdr_t));\n\n\ttsn = ntohl(data_hdr->tsn);\n\tSCTP_DEBUG_PRINTK(\"eat_data: TSN 0x%x.\\n\", tsn);\n\n\t/* ASSERT:  Now skb->data is really the user data.  */\n\n\t/*\n\t * if we are established, and we have used up our receive\n\t * buffer memory, drop the frame\n\t */\n\tif (asoc->state == SCTP_STATE_ESTABLISHED) {\n\t\t/*\n\t\t * If the receive buffer policy is 1, then each\n\t\t * association can allocate up to sk_rcvbuf bytes\n\t\t * otherwise, all the associations in aggregate\n\t\t * may allocate up to sk_rcvbuf bytes\n\t\t */\n\t\tif (asoc->ep->rcvbuf_policy)\n\t\t\taccount_value = atomic_read(&asoc->rmem_alloc);\n\t\telse\n\t\t\taccount_value = atomic_read(&sk->sk_rmem_alloc);\n\n\t\tif (account_value > sk->sk_rcvbuf)\n\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n\t}\n\n\t/* Process ECN based congestion.\n\t *\n\t * Since the chunk structure is reused for all chunks within\n\t * a packet, we use ecn_ce_done to track if we've already\n\t * done CE processing for this packet.\n\t *\n\t * We need to do ECN processing even if we plan to discard the\n\t * chunk later.\n\t */\n\n\tif (!chunk->ecn_ce_done) {\n\t\tstruct sctp_af *af;\n\t\tchunk->ecn_ce_done = 1;\n\n\t\taf = sctp_get_af_specific(\n\t\t\tipver2af(chunk->skb->nh.iph->version));\n\n\t\tif (af && af->is_ce(chunk->skb) && asoc->peer.ecn_capable) {\n\t\t\t/* Do real work as sideffect. */\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_ECN_CE,\n\t\t\t\t\tSCTP_U32(tsn));\n\t\t}\n\t}\n\n\ttmp = sctp_tsnmap_check(&asoc->peer.tsn_map, tsn);\n\tif (tmp < 0) {\n\t\t/* The TSN is too high--silently discard the chunk and\n\t\t * count on it getting retransmitted later.\n\t\t */\n\t\treturn SCTP_IERROR_HIGH_TSN;\n\t} else if (tmp > 0) {\n\t\t/* This is a duplicate.  Record it.  */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPORT_DUP, SCTP_U32(tsn));\n\t\treturn SCTP_IERROR_DUP_TSN;\n\t}\n\n\t/* This is a new TSN.  */\n\n\t/* Discard if there is no room in the receive window.\n\t * Actually, allow a little bit of overflow (up to a MTU).\n\t */\n\tdatalen = ntohs(chunk->chunk_hdr->length);\n\tdatalen -= sizeof(sctp_data_chunk_t);\n\n\tdeliver = SCTP_CMD_CHUNK_ULP;\n\n\t/* Think about partial delivery. */\n\tif ((datalen >= asoc->rwnd) && (!asoc->ulpq.pd_mode)) {\n\n\t\t/* Even if we don't accept this chunk there is\n\t\t * memory pressure.\n\t\t */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_PART_DELIVER, SCTP_NULL());\n\t}\n\n        /* Spill over rwnd a little bit.  Note: While allowed, this spill over\n\t * seems a bit troublesome in that frag_point varies based on\n\t * PMTU.  In cases, such as loopback, this might be a rather\n\t * large spill over.\n\t */\n\tif (!asoc->rwnd || asoc->rwnd_over ||\n\t    (datalen > asoc->rwnd + asoc->frag_point)) {\n\n\t\t/* If this is the next TSN, consider reneging to make\n\t\t * room.   Note: Playing nice with a confused sender.  A\n\t\t * malicious sender can still eat up all our buffer\n\t\t * space and in the future we may want to detect and\n\t\t * do more drastic reneging.\n\t\t */\n\t\tif (sctp_tsnmap_has_gap(&asoc->peer.tsn_map) &&\n\t\t    (sctp_tsnmap_get_ctsn(&asoc->peer.tsn_map) + 1) == tsn) {\n\t\t\tSCTP_DEBUG_PRINTK(\"Reneging for tsn:%u\\n\", tsn);\n\t\t\tdeliver = SCTP_CMD_RENEGE;\n\t\t} else {\n\t\t\tSCTP_DEBUG_PRINTK(\"Discard tsn: %u len: %Zd, \"\n\t\t\t\t\t  \"rwnd: %d\\n\", tsn, datalen,\n\t\t\t\t\t  asoc->rwnd);\n\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n\t\t}\n\t}\n\n\t/*\n\t * Section 3.3.10.9 No User Data (9)\n\t *\n\t * Cause of error\n\t * ---------------\n\t * No User Data:  This error cause is returned to the originator of a\n\t * DATA chunk if a received DATA chunk has no user data.\n\t */\n\tif (unlikely(0 == datalen)) {\n\t\terr = sctp_make_abort_no_data(asoc, chunk, tsn);\n\t\tif (err) {\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY,\n\t\t\t\t\tSCTP_CHUNK(err));\n\t\t}\n\t\t/* We are going to ABORT, so we might as well stop\n\t\t * processing the rest of the chunks in the packet.\n\t\t */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_DISCARD_PACKET,SCTP_NULL());\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_ASSOC_FAILED,\n\t\t\t\tSCTP_U32(SCTP_ERROR_NO_DATA));\n\t\tSCTP_INC_STATS(SCTP_MIB_ABORTEDS);\n\t\tSCTP_DEC_STATS(SCTP_MIB_CURRESTAB);\n\t\treturn SCTP_IERROR_NO_DATA;\n\t}\n\n\t/* If definately accepting the DATA chunk, record its TSN, otherwise\n\t * wait for renege processing.\n\t */\n\tif (SCTP_CMD_CHUNK_ULP == deliver)\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPORT_TSN, SCTP_U32(tsn));\n\n\t/* Note: Some chunks may get overcounted (if we drop) or overcounted\n\t * if we renege and the chunk arrives again.\n\t */\n\tif (chunk->chunk_hdr->flags & SCTP_DATA_UNORDERED)\n\t\tSCTP_INC_STATS(SCTP_MIB_INUNORDERCHUNKS);\n\telse\n\t\tSCTP_INC_STATS(SCTP_MIB_INORDERCHUNKS);\n\n\t/* RFC 2960 6.5 Stream Identifier and Stream Sequence Number\n\t *\n\t * If an endpoint receive a DATA chunk with an invalid stream\n\t * identifier, it shall acknowledge the reception of the DATA chunk\n\t * following the normal procedure, immediately send an ERROR chunk\n\t * with cause set to \"Invalid Stream Identifier\" (See Section 3.3.10)\n\t * and discard the DATA chunk.\n\t */\n\tif (ntohs(data_hdr->stream) >= asoc->c.sinit_max_instreams) {\n\t\terr = sctp_make_op_error(asoc, chunk, SCTP_ERROR_INV_STRM,\n\t\t\t\t\t &data_hdr->stream,\n\t\t\t\t\t sizeof(data_hdr->stream));\n\t\tif (err)\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY,\n\t\t\t\t\tSCTP_CHUNK(err));\n\t\treturn SCTP_IERROR_BAD_STREAM;\n\t}\n\n\t/* Send the data up to the user.  Note:  Schedule  the\n\t * SCTP_CMD_CHUNK_ULP cmd before the SCTP_CMD_GEN_SACK, as the SACK\n\t * chunk needs the updated rwnd.\n\t */\n\tsctp_add_cmd_sf(commands, deliver, SCTP_CHUNK(chunk));\n\n\treturn SCTP_IERROR_NO_ERROR;\n}",
        "func": "static int sctp_eat_data(const struct sctp_association *asoc,\n\t\t\t struct sctp_chunk *chunk,\n\t\t\t sctp_cmd_seq_t *commands)\n{\n\tsctp_datahdr_t *data_hdr;\n\tstruct sctp_chunk *err;\n\tsize_t datalen;\n\tsctp_verb_t deliver;\n\tint tmp;\n\t__u32 tsn;\n\tint account_value;\n\tstruct sctp_tsnmap *map = (struct sctp_tsnmap *)&asoc->peer.tsn_map;\n\tstruct sock *sk = asoc->base.sk;\n\tint rcvbuf_over = 0;\n\n\tdata_hdr = chunk->subh.data_hdr = (sctp_datahdr_t *)chunk->skb->data;\n\tskb_pull(chunk->skb, sizeof(sctp_datahdr_t));\n\n\ttsn = ntohl(data_hdr->tsn);\n\tSCTP_DEBUG_PRINTK(\"eat_data: TSN 0x%x.\\n\", tsn);\n\n\t/* ASSERT:  Now skb->data is really the user data.  */\n\n\t/*\n\t * If we are established, and we have used up our receive buffer\n\t * memory, think about droping the frame.\n\t * Note that we have an opportunity to improve performance here.\n\t * If we accept one chunk from an skbuff, we have to keep all the\n\t * memory of that skbuff around until the chunk is read into user\n\t * space. Therefore, once we accept 1 chunk we may as well accept all\n\t * remaining chunks in the skbuff. The data_accepted flag helps us do\n\t * that.\n\t */\n\tif ((asoc->state == SCTP_STATE_ESTABLISHED) && (!chunk->data_accepted)) {\n\t\t/*\n\t\t * If the receive buffer policy is 1, then each\n\t\t * association can allocate up to sk_rcvbuf bytes\n\t\t * otherwise, all the associations in aggregate\n\t\t * may allocate up to sk_rcvbuf bytes\n\t\t */\n\t\tif (asoc->ep->rcvbuf_policy)\n\t\t\taccount_value = atomic_read(&asoc->rmem_alloc);\n\t\telse\n\t\t\taccount_value = atomic_read(&sk->sk_rmem_alloc);\n\t\tif (account_value > sk->sk_rcvbuf) {\n\t\t\t/*\n\t\t\t * We need to make forward progress, even when we are\n\t\t\t * under memory pressure, so we always allow the\n\t\t\t * next tsn after the ctsn ack point to be accepted.\n\t\t\t * This lets us avoid deadlocks in which we have to\n\t\t\t * drop frames that would otherwise let us drain the\n\t\t\t * receive queue.\n\t\t\t */\n\t\t\tif ((sctp_tsnmap_get_ctsn(map) + 1) != tsn)\n\t\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n\n\t\t\t/*\n\t\t\t * We're going to accept the frame but we should renege\n\t\t\t * to make space for it. This will send us down that\n\t\t\t * path later in this function.\n\t\t\t */\n\t\t\trcvbuf_over = 1;\n\t\t}\n\t}\n\n\t/* Process ECN based congestion.\n\t *\n\t * Since the chunk structure is reused for all chunks within\n\t * a packet, we use ecn_ce_done to track if we've already\n\t * done CE processing for this packet.\n\t *\n\t * We need to do ECN processing even if we plan to discard the\n\t * chunk later.\n\t */\n\n\tif (!chunk->ecn_ce_done) {\n\t\tstruct sctp_af *af;\n\t\tchunk->ecn_ce_done = 1;\n\n\t\taf = sctp_get_af_specific(\n\t\t\tipver2af(chunk->skb->nh.iph->version));\n\n\t\tif (af && af->is_ce(chunk->skb) && asoc->peer.ecn_capable) {\n\t\t\t/* Do real work as sideffect. */\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_ECN_CE,\n\t\t\t\t\tSCTP_U32(tsn));\n\t\t}\n\t}\n\n\ttmp = sctp_tsnmap_check(&asoc->peer.tsn_map, tsn);\n\tif (tmp < 0) {\n\t\t/* The TSN is too high--silently discard the chunk and\n\t\t * count on it getting retransmitted later.\n\t\t */\n\t\treturn SCTP_IERROR_HIGH_TSN;\n\t} else if (tmp > 0) {\n\t\t/* This is a duplicate.  Record it.  */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPORT_DUP, SCTP_U32(tsn));\n\t\treturn SCTP_IERROR_DUP_TSN;\n\t}\n\n\t/* This is a new TSN.  */\n\n\t/* Discard if there is no room in the receive window.\n\t * Actually, allow a little bit of overflow (up to a MTU).\n\t */\n\tdatalen = ntohs(chunk->chunk_hdr->length);\n\tdatalen -= sizeof(sctp_data_chunk_t);\n\n\tdeliver = SCTP_CMD_CHUNK_ULP;\n\tchunk->data_accepted = 1;\n\n\t/* Think about partial delivery. */\n\tif ((datalen >= asoc->rwnd) && (!asoc->ulpq.pd_mode)) {\n\n\t\t/* Even if we don't accept this chunk there is\n\t\t * memory pressure.\n\t\t */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_PART_DELIVER, SCTP_NULL());\n\t}\n\n        /* Spill over rwnd a little bit.  Note: While allowed, this spill over\n\t * seems a bit troublesome in that frag_point varies based on\n\t * PMTU.  In cases, such as loopback, this might be a rather\n\t * large spill over.\n\t */\n\tif (!asoc->rwnd || asoc->rwnd_over ||\n\t    (datalen > asoc->rwnd + asoc->frag_point) ||\n\t    rcvbuf_over) {\n\n\t\t/* If this is the next TSN, consider reneging to make\n\t\t * room.   Note: Playing nice with a confused sender.  A\n\t\t * malicious sender can still eat up all our buffer\n\t\t * space and in the future we may want to detect and\n\t\t * do more drastic reneging.\n\t\t */\n\t\tif (sctp_tsnmap_has_gap(map) &&\n\t\t    (sctp_tsnmap_get_ctsn(map) + 1) == tsn) {\n\t\t\tSCTP_DEBUG_PRINTK(\"Reneging for tsn:%u\\n\", tsn);\n\t\t\tdeliver = SCTP_CMD_RENEGE;\n\t\t} else {\n\t\t\tSCTP_DEBUG_PRINTK(\"Discard tsn: %u len: %Zd, \"\n\t\t\t\t\t  \"rwnd: %d\\n\", tsn, datalen,\n\t\t\t\t\t  asoc->rwnd);\n\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n\t\t}\n\t}\n\n\t/*\n\t * Section 3.3.10.9 No User Data (9)\n\t *\n\t * Cause of error\n\t * ---------------\n\t * No User Data:  This error cause is returned to the originator of a\n\t * DATA chunk if a received DATA chunk has no user data.\n\t */\n\tif (unlikely(0 == datalen)) {\n\t\terr = sctp_make_abort_no_data(asoc, chunk, tsn);\n\t\tif (err) {\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY,\n\t\t\t\t\tSCTP_CHUNK(err));\n\t\t}\n\t\t/* We are going to ABORT, so we might as well stop\n\t\t * processing the rest of the chunks in the packet.\n\t\t */\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_DISCARD_PACKET,SCTP_NULL());\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_ASSOC_FAILED,\n\t\t\t\tSCTP_U32(SCTP_ERROR_NO_DATA));\n\t\tSCTP_INC_STATS(SCTP_MIB_ABORTEDS);\n\t\tSCTP_DEC_STATS(SCTP_MIB_CURRESTAB);\n\t\treturn SCTP_IERROR_NO_DATA;\n\t}\n\n\t/* If definately accepting the DATA chunk, record its TSN, otherwise\n\t * wait for renege processing.\n\t */\n\tif (SCTP_CMD_CHUNK_ULP == deliver)\n\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPORT_TSN, SCTP_U32(tsn));\n\n\t/* Note: Some chunks may get overcounted (if we drop) or overcounted\n\t * if we renege and the chunk arrives again.\n\t */\n\tif (chunk->chunk_hdr->flags & SCTP_DATA_UNORDERED)\n\t\tSCTP_INC_STATS(SCTP_MIB_INUNORDERCHUNKS);\n\telse\n\t\tSCTP_INC_STATS(SCTP_MIB_INORDERCHUNKS);\n\n\t/* RFC 2960 6.5 Stream Identifier and Stream Sequence Number\n\t *\n\t * If an endpoint receive a DATA chunk with an invalid stream\n\t * identifier, it shall acknowledge the reception of the DATA chunk\n\t * following the normal procedure, immediately send an ERROR chunk\n\t * with cause set to \"Invalid Stream Identifier\" (See Section 3.3.10)\n\t * and discard the DATA chunk.\n\t */\n\tif (ntohs(data_hdr->stream) >= asoc->c.sinit_max_instreams) {\n\t\terr = sctp_make_op_error(asoc, chunk, SCTP_ERROR_INV_STRM,\n\t\t\t\t\t &data_hdr->stream,\n\t\t\t\t\t sizeof(data_hdr->stream));\n\t\tif (err)\n\t\t\tsctp_add_cmd_sf(commands, SCTP_CMD_REPLY,\n\t\t\t\t\tSCTP_CHUNK(err));\n\t\treturn SCTP_IERROR_BAD_STREAM;\n\t}\n\n\t/* Send the data up to the user.  Note:  Schedule  the\n\t * SCTP_CMD_CHUNK_ULP cmd before the SCTP_CMD_GEN_SACK, as the SACK\n\t * chunk needs the updated rwnd.\n\t */\n\tsctp_add_cmd_sf(commands, deliver, SCTP_CHUNK(chunk));\n\n\treturn SCTP_IERROR_NO_ERROR;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -9,7 +9,9 @@\n \tint tmp;\n \t__u32 tsn;\n \tint account_value;\n+\tstruct sctp_tsnmap *map = (struct sctp_tsnmap *)&asoc->peer.tsn_map;\n \tstruct sock *sk = asoc->base.sk;\n+\tint rcvbuf_over = 0;\n \n \tdata_hdr = chunk->subh.data_hdr = (sctp_datahdr_t *)chunk->skb->data;\n \tskb_pull(chunk->skb, sizeof(sctp_datahdr_t));\n@@ -20,10 +22,16 @@\n \t/* ASSERT:  Now skb->data is really the user data.  */\n \n \t/*\n-\t * if we are established, and we have used up our receive\n-\t * buffer memory, drop the frame\n-\t */\n-\tif (asoc->state == SCTP_STATE_ESTABLISHED) {\n+\t * If we are established, and we have used up our receive buffer\n+\t * memory, think about droping the frame.\n+\t * Note that we have an opportunity to improve performance here.\n+\t * If we accept one chunk from an skbuff, we have to keep all the\n+\t * memory of that skbuff around until the chunk is read into user\n+\t * space. Therefore, once we accept 1 chunk we may as well accept all\n+\t * remaining chunks in the skbuff. The data_accepted flag helps us do\n+\t * that.\n+\t */\n+\tif ((asoc->state == SCTP_STATE_ESTABLISHED) && (!chunk->data_accepted)) {\n \t\t/*\n \t\t * If the receive buffer policy is 1, then each\n \t\t * association can allocate up to sk_rcvbuf bytes\n@@ -34,9 +42,25 @@\n \t\t\taccount_value = atomic_read(&asoc->rmem_alloc);\n \t\telse\n \t\t\taccount_value = atomic_read(&sk->sk_rmem_alloc);\n-\n-\t\tif (account_value > sk->sk_rcvbuf)\n-\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n+\t\tif (account_value > sk->sk_rcvbuf) {\n+\t\t\t/*\n+\t\t\t * We need to make forward progress, even when we are\n+\t\t\t * under memory pressure, so we always allow the\n+\t\t\t * next tsn after the ctsn ack point to be accepted.\n+\t\t\t * This lets us avoid deadlocks in which we have to\n+\t\t\t * drop frames that would otherwise let us drain the\n+\t\t\t * receive queue.\n+\t\t\t */\n+\t\t\tif ((sctp_tsnmap_get_ctsn(map) + 1) != tsn)\n+\t\t\t\treturn SCTP_IERROR_IGNORE_TSN;\n+\n+\t\t\t/*\n+\t\t\t * We're going to accept the frame but we should renege\n+\t\t\t * to make space for it. This will send us down that\n+\t\t\t * path later in this function.\n+\t\t\t */\n+\t\t\trcvbuf_over = 1;\n+\t\t}\n \t}\n \n \t/* Process ECN based congestion.\n@@ -84,6 +108,7 @@\n \tdatalen -= sizeof(sctp_data_chunk_t);\n \n \tdeliver = SCTP_CMD_CHUNK_ULP;\n+\tchunk->data_accepted = 1;\n \n \t/* Think about partial delivery. */\n \tif ((datalen >= asoc->rwnd) && (!asoc->ulpq.pd_mode)) {\n@@ -100,7 +125,8 @@\n \t * large spill over.\n \t */\n \tif (!asoc->rwnd || asoc->rwnd_over ||\n-\t    (datalen > asoc->rwnd + asoc->frag_point)) {\n+\t    (datalen > asoc->rwnd + asoc->frag_point) ||\n+\t    rcvbuf_over) {\n \n \t\t/* If this is the next TSN, consider reneging to make\n \t\t * room.   Note: Playing nice with a confused sender.  A\n@@ -108,8 +134,8 @@\n \t\t * space and in the future we may want to detect and\n \t\t * do more drastic reneging.\n \t\t */\n-\t\tif (sctp_tsnmap_has_gap(&asoc->peer.tsn_map) &&\n-\t\t    (sctp_tsnmap_get_ctsn(&asoc->peer.tsn_map) + 1) == tsn) {\n+\t\tif (sctp_tsnmap_has_gap(map) &&\n+\t\t    (sctp_tsnmap_get_ctsn(map) + 1) == tsn) {\n \t\t\tSCTP_DEBUG_PRINTK(\"Reneging for tsn:%u\\n\", tsn);\n \t\t\tdeliver = SCTP_CMD_RENEGE;\n \t\t} else {",
        "diff_line_info": {
            "deleted_lines": [
                "\t * if we are established, and we have used up our receive",
                "\t * buffer memory, drop the frame",
                "\t */",
                "\tif (asoc->state == SCTP_STATE_ESTABLISHED) {",
                "",
                "\t\tif (account_value > sk->sk_rcvbuf)",
                "\t\t\treturn SCTP_IERROR_IGNORE_TSN;",
                "\t    (datalen > asoc->rwnd + asoc->frag_point)) {",
                "\t\tif (sctp_tsnmap_has_gap(&asoc->peer.tsn_map) &&",
                "\t\t    (sctp_tsnmap_get_ctsn(&asoc->peer.tsn_map) + 1) == tsn) {"
            ],
            "added_lines": [
                "\tstruct sctp_tsnmap *map = (struct sctp_tsnmap *)&asoc->peer.tsn_map;",
                "\tint rcvbuf_over = 0;",
                "\t * If we are established, and we have used up our receive buffer",
                "\t * memory, think about droping the frame.",
                "\t * Note that we have an opportunity to improve performance here.",
                "\t * If we accept one chunk from an skbuff, we have to keep all the",
                "\t * memory of that skbuff around until the chunk is read into user",
                "\t * space. Therefore, once we accept 1 chunk we may as well accept all",
                "\t * remaining chunks in the skbuff. The data_accepted flag helps us do",
                "\t * that.",
                "\t */",
                "\tif ((asoc->state == SCTP_STATE_ESTABLISHED) && (!chunk->data_accepted)) {",
                "\t\tif (account_value > sk->sk_rcvbuf) {",
                "\t\t\t/*",
                "\t\t\t * We need to make forward progress, even when we are",
                "\t\t\t * under memory pressure, so we always allow the",
                "\t\t\t * next tsn after the ctsn ack point to be accepted.",
                "\t\t\t * This lets us avoid deadlocks in which we have to",
                "\t\t\t * drop frames that would otherwise let us drain the",
                "\t\t\t * receive queue.",
                "\t\t\t */",
                "\t\t\tif ((sctp_tsnmap_get_ctsn(map) + 1) != tsn)",
                "\t\t\t\treturn SCTP_IERROR_IGNORE_TSN;",
                "",
                "\t\t\t/*",
                "\t\t\t * We're going to accept the frame but we should renege",
                "\t\t\t * to make space for it. This will send us down that",
                "\t\t\t * path later in this function.",
                "\t\t\t */",
                "\t\t\trcvbuf_over = 1;",
                "\t\t}",
                "\tchunk->data_accepted = 1;",
                "\t    (datalen > asoc->rwnd + asoc->frag_point) ||",
                "\t    rcvbuf_over) {",
                "\t\tif (sctp_tsnmap_has_gap(map) &&",
                "\t\t    (sctp_tsnmap_get_ctsn(map) + 1) == tsn) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2008-4302",
        "func_name": "torvalds/linux/pipe_to_file",
        "description": "fs/splice.c in the splice subsystem in the Linux kernel before 2.6.22.2 does not properly handle a failure of the add_to_page_cache_lru function, and subsequently attempts to unlock a page that was not locked, which allows local users to cause a denial of service (kernel BUG and system crash), as demonstrated by the fio I/O tool.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/stable/linux.git;a=commit;h=6a860c979b35469e4d77da781a96bdb2ca05ae64",
        "commit_title": "If add_to_page_cache_lru() fails, the page will not be locked. But",
        "commit_text": "splice jumps to an error path that does a page release and unlock, causing a BUG() in unlock_page().  Fix this by adding one more label that just releases the page. This bug was actually triggered on EL5 by gurudas pai <gurudas.pai@oracle.com> using fio.  ",
        "func_before": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tpage_cache_release(page);\n\tunlock_page(page);\nout_ret:\n\treturn ret;\n}",
        "func": "static int pipe_to_file(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tstruct address_space *mapping = file->f_mapping;\n\tunsigned int offset, this_len;\n\tstruct page *page;\n\tpgoff_t index;\n\tint ret;\n\n\t/*\n\t * make sure the data in this buffer is uptodate\n\t */\n\tret = buf->ops->confirm(pipe, buf);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tindex = sd->pos >> PAGE_CACHE_SHIFT;\n\toffset = sd->pos & ~PAGE_CACHE_MASK;\n\n\tthis_len = sd->len;\n\tif (this_len + offset > PAGE_CACHE_SIZE)\n\t\tthis_len = PAGE_CACHE_SIZE - offset;\n\nfind_page:\n\tpage = find_lock_page(mapping, index);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tpage = page_cache_alloc_cold(mapping);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_ret;\n\n\t\t/*\n\t\t * This will also lock the page\n\t\t */\n\t\tret = add_to_page_cache_lru(page, mapping, index,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (unlikely(ret))\n\t\t\tgoto out_release;\n\t}\n\n\tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n\tif (unlikely(ret)) {\n\t\tloff_t isize = i_size_read(mapping->host);\n\n\t\tif (ret != AOP_TRUNCATED_PAGE)\n\t\t\tunlock_page(page);\n\t\tpage_cache_release(page);\n\t\tif (ret == AOP_TRUNCATED_PAGE)\n\t\t\tgoto find_page;\n\n\t\t/*\n\t\t * prepare_write() may have instantiated a few blocks\n\t\t * outside i_size.  Trim these off again.\n\t\t */\n\t\tif (sd->pos + this_len > isize)\n\t\t\tvmtruncate(mapping->host, isize);\n\n\t\tgoto out_ret;\n\t}\n\n\tif (buf->page != page) {\n\t\t/*\n\t\t * Careful, ->map() uses KM_USER0!\n\t\t */\n\t\tchar *src = buf->ops->map(pipe, buf, 1);\n\t\tchar *dst = kmap_atomic(page, KM_USER1);\n\n\t\tmemcpy(dst + offset, src + buf->offset, this_len);\n\t\tflush_dcache_page(page);\n\t\tkunmap_atomic(dst, KM_USER1);\n\t\tbuf->ops->unmap(pipe, buf, src);\n\t}\n\n\tret = mapping->a_ops->commit_write(file, page, offset, offset+this_len);\n\tif (ret) {\n\t\tif (ret == AOP_TRUNCATED_PAGE) {\n\t\t\tpage_cache_release(page);\n\t\t\tgoto find_page;\n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\t/*\n\t\t * Partial write has happened, so 'ret' already initialized by\n\t\t * number of bytes written, Where is nothing we have to do here.\n\t\t */\n\t} else\n\t\tret = this_len;\n\t/*\n\t * Return the number of bytes written and mark page as\n\t * accessed, we are now done!\n\t */\n\tmark_page_accessed(page);\nout:\n\tunlock_page(page);\nout_release:\n\tpage_cache_release(page);\nout_ret:\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,7 +36,7 @@\n \t\tret = add_to_page_cache_lru(page, mapping, index,\n \t\t\t\t\t    GFP_KERNEL);\n \t\tif (unlikely(ret))\n-\t\t\tgoto out;\n+\t\t\tgoto out_release;\n \t}\n \n \tret = mapping->a_ops->prepare_write(file, page, offset, offset+this_len);\n@@ -92,8 +92,9 @@\n \t */\n \tmark_page_accessed(page);\n out:\n+\tunlock_page(page);\n+out_release:\n \tpage_cache_release(page);\n-\tunlock_page(page);\n out_ret:\n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\tgoto out;",
                "\tunlock_page(page);"
            ],
            "added_lines": [
                "\t\t\tgoto out_release;",
                "\tunlock_page(page);",
                "out_release:"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1243",
        "func_name": "torvalds/linux/udp_seq_start",
        "description": "net/ipv4/udp.c in the Linux kernel before 2.6.29.1 performs an unlocking step in certain incorrect circumstances, which allows local users to cause a denial of service (panic) by reading zero bytes from the /proc/net/udp file and unspecified other files, related to the \"udp seq_file infrastructure.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=30842f2989aacfaba3ccb39829b3417be9313dbe",
        "commit_title": "Reading zero bytes from /proc/net/udp or other similar files which use",
        "commit_text": "the same seq_file udp infrastructure panics kernel in that way:  ===================================== [ BUG: bad unlock balance detected! ] ------------------------------------- read/1985 is trying to release lock (&table->hash[i].lock) at: [<ffffffff81321d83>] udp_seq_stop+0x27/0x29 but there are no more locks to release!  other info that might help us debug this: 1 lock held by read/1985:  #0:  (&p->lock){--..}, at: [<ffffffff810eefb6>] seq_read+0x38/0x348  stack backtrace: Pid: 1985, comm: read Not tainted 2.6.29-rc8 #9 Call Trace:  [<ffffffff81321d83>] ? udp_seq_stop+0x27/0x29  [<ffffffff8106dab9>] print_unlock_inbalance_bug+0xd6/0xe1  [<ffffffff8106db62>] lock_release_non_nested+0x9e/0x1c6  [<ffffffff810ef030>] ? seq_read+0xb2/0x348  [<ffffffff8106bdba>] ? mark_held_locks+0x68/0x86  [<ffffffff81321d83>] ? udp_seq_stop+0x27/0x29  [<ffffffff8106dde7>] lock_release+0x15d/0x189  [<ffffffff8137163c>] _spin_unlock_bh+0x1e/0x34  [<ffffffff81321d83>] udp_seq_stop+0x27/0x29  [<ffffffff810ef239>] seq_read+0x2bb/0x348  [<ffffffff810eef7e>] ? seq_read+0x0/0x348  [<ffffffff8111aedd>] proc_reg_read+0x90/0xaf  [<ffffffff810d878f>] vfs_read+0xa6/0x103  [<ffffffff8106bfac>] ? trace_hardirqs_on_caller+0x12f/0x153  [<ffffffff810d88a2>] sys_read+0x45/0x69  [<ffffffff8101123a>] system_call_fastpath+0x16/0x1b BUG: scheduling while atomic: read/1985/0xffffff00 INFO: lockdep is turned off. Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table dm_multipath kvm ppdev snd_hda_codec_analog snd_hda_intel snd_hda_codec snd_hwdep snd_seq_dummy snd_seq_oss snd_seq_midi_event arc4 snd_s eq ecb thinkpad_acpi snd_seq_device iwl3945 hwmon sdhci_pci snd_pcm_oss sdhci rfkill mmc_core snd_mixer_oss i2c_i801 mac80211 yenta_socket ricoh_mmc i2c_core iTCO_wdt snd_pcm iTCO_vendor_support rs rc_nonstatic snd_timer snd lib80211 cfg80211 soundcore snd_page_alloc video parport_pc output parport e1000e [last unloaded: scsi_wait_scan] Pid: 1985, comm: read Not tainted 2.6.29-rc8 #9 Call Trace:  [<ffffffff8106b456>] ? __debug_show_held_locks+0x1b/0x24  [<ffffffff81043660>] __schedule_bug+0x7e/0x83  [<ffffffff8136ede9>] schedule+0xce/0x838  [<ffffffff810d7972>] ? fsnotify_access+0x5f/0x67  [<ffffffff810112d0>] ? sysret_careful+0xb/0x37  [<ffffffff8106be9c>] ? trace_hardirqs_on_caller+0x1f/0x153  [<ffffffff8137127b>] ? trace_hardirqs_on_thunk+0x3a/0x3f  [<ffffffff810112f6>] sysret_careful+0x31/0x37 read[1985]: segfault at 7fffc479bfe8 ip 0000003e7420a180 sp 00007fffc479bfa0 error 6 Kernel panic - not syncing: Aiee, killing interrupt handler!  udp_seq_stop() tries to unlock not yet locked spinlock. The lock was lost during splitting global udp_hash_lock to subsequent spinlocks.  Signed-off by: Vitaly Mayatskikh <v.mayatskih@gmail.com> ",
        "func_before": "static void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}",
        "func": "static void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstate->bucket = UDP_HTABLE_SIZE;\n\n\treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,7 @@\n static void *udp_seq_start(struct seq_file *seq, loff_t *pos)\n {\n+\tstruct udp_iter_state *state = seq->private;\n+\tstate->bucket = UDP_HTABLE_SIZE;\n+\n \treturn *pos ? udp_get_idx(seq, *pos-1) : SEQ_START_TOKEN;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tstruct udp_iter_state *state = seq->private;",
                "\tstate->bucket = UDP_HTABLE_SIZE;",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1243",
        "func_name": "torvalds/linux/udp_get_next",
        "description": "net/ipv4/udp.c in the Linux kernel before 2.6.29.1 performs an unlocking step in certain incorrect circumstances, which allows local users to cause a denial of service (panic) by reading zero bytes from the /proc/net/udp file and unspecified other files, related to the \"udp seq_file infrastructure.\"",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=30842f2989aacfaba3ccb39829b3417be9313dbe",
        "commit_title": "Reading zero bytes from /proc/net/udp or other similar files which use",
        "commit_text": "the same seq_file udp infrastructure panics kernel in that way:  ===================================== [ BUG: bad unlock balance detected! ] ------------------------------------- read/1985 is trying to release lock (&table->hash[i].lock) at: [<ffffffff81321d83>] udp_seq_stop+0x27/0x29 but there are no more locks to release!  other info that might help us debug this: 1 lock held by read/1985:  #0:  (&p->lock){--..}, at: [<ffffffff810eefb6>] seq_read+0x38/0x348  stack backtrace: Pid: 1985, comm: read Not tainted 2.6.29-rc8 #9 Call Trace:  [<ffffffff81321d83>] ? udp_seq_stop+0x27/0x29  [<ffffffff8106dab9>] print_unlock_inbalance_bug+0xd6/0xe1  [<ffffffff8106db62>] lock_release_non_nested+0x9e/0x1c6  [<ffffffff810ef030>] ? seq_read+0xb2/0x348  [<ffffffff8106bdba>] ? mark_held_locks+0x68/0x86  [<ffffffff81321d83>] ? udp_seq_stop+0x27/0x29  [<ffffffff8106dde7>] lock_release+0x15d/0x189  [<ffffffff8137163c>] _spin_unlock_bh+0x1e/0x34  [<ffffffff81321d83>] udp_seq_stop+0x27/0x29  [<ffffffff810ef239>] seq_read+0x2bb/0x348  [<ffffffff810eef7e>] ? seq_read+0x0/0x348  [<ffffffff8111aedd>] proc_reg_read+0x90/0xaf  [<ffffffff810d878f>] vfs_read+0xa6/0x103  [<ffffffff8106bfac>] ? trace_hardirqs_on_caller+0x12f/0x153  [<ffffffff810d88a2>] sys_read+0x45/0x69  [<ffffffff8101123a>] system_call_fastpath+0x16/0x1b BUG: scheduling while atomic: read/1985/0xffffff00 INFO: lockdep is turned off. Modules linked in: cpufreq_ondemand acpi_cpufreq freq_table dm_multipath kvm ppdev snd_hda_codec_analog snd_hda_intel snd_hda_codec snd_hwdep snd_seq_dummy snd_seq_oss snd_seq_midi_event arc4 snd_s eq ecb thinkpad_acpi snd_seq_device iwl3945 hwmon sdhci_pci snd_pcm_oss sdhci rfkill mmc_core snd_mixer_oss i2c_i801 mac80211 yenta_socket ricoh_mmc i2c_core iTCO_wdt snd_pcm iTCO_vendor_support rs rc_nonstatic snd_timer snd lib80211 cfg80211 soundcore snd_page_alloc video parport_pc output parport e1000e [last unloaded: scsi_wait_scan] Pid: 1985, comm: read Not tainted 2.6.29-rc8 #9 Call Trace:  [<ffffffff8106b456>] ? __debug_show_held_locks+0x1b/0x24  [<ffffffff81043660>] __schedule_bug+0x7e/0x83  [<ffffffff8136ede9>] schedule+0xce/0x838  [<ffffffff810d7972>] ? fsnotify_access+0x5f/0x67  [<ffffffff810112d0>] ? sysret_careful+0xb/0x37  [<ffffffff8106be9c>] ? trace_hardirqs_on_caller+0x1f/0x153  [<ffffffff8137127b>] ? trace_hardirqs_on_thunk+0x3a/0x3f  [<ffffffff810112f6>] sysret_careful+0x31/0x37 read[1985]: segfault at 7fffc479bfe8 ip 0000003e7420a180 sp 00007fffc479bfa0 error 6 Kernel panic - not syncing: Aiee, killing interrupt handler!  udp_seq_stop() tries to unlock not yet locked spinlock. The lock was lost during splitting global udp_hash_lock to subsequent spinlocks.  Signed-off by: Vitaly Mayatskikh <v.mayatskih@gmail.com> ",
        "func_before": "static struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}",
        "func": "static struct sock *udp_get_next(struct seq_file *seq, struct sock *sk)\n{\n\tstruct udp_iter_state *state = seq->private;\n\tstruct net *net = seq_file_net(seq);\n\n\tdo {\n\t\tsk = sk_nulls_next(sk);\n\t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n\n\tif (!sk) {\n\t\tif (state->bucket < UDP_HTABLE_SIZE)\n\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n\t\treturn udp_get_first(seq, state->bucket + 1);\n\t}\n\treturn sk;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,7 +8,8 @@\n \t} while (sk && (!net_eq(sock_net(sk), net) || sk->sk_family != state->family));\n \n \tif (!sk) {\n-\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n+\t\tif (state->bucket < UDP_HTABLE_SIZE)\n+\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);\n \t\treturn udp_get_first(seq, state->bucket + 1);\n \t}\n \treturn sk;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);"
            ],
            "added_lines": [
                "\t\tif (state->bucket < UDP_HTABLE_SIZE)",
                "\t\t\tspin_unlock_bh(&state->udp_table->hash[state->bucket].lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1961",
        "func_name": "torvalds/linux/splice_from_pipe",
        "description": "The inode double locking code in fs/ocfs2/file.c in the Linux kernel 2.6.30 before 2.6.30-rc3, 2.6.27 before 2.6.27.24, 2.6.29 before 2.6.29.4, and possibly other versions down to 2.6.19 allows local users to cause a denial of service (prevention of file creation and removal) via a series of splice system calls that trigger a deadlock between the generic_file_splice_write, splice_from_pipe, and ocfs2_file_splice_write functions.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff;h=7bfac9ecf0585962fe13584f5cf526d8c8e76f17",
        "commit_title": "There's a possible deadlock in generic_file_splice_write(),",
        "commit_text": "splice_from_pipe() and ocfs2_file_splice_write():   - task A calls generic_file_splice_write()  - this calls inode_double_lock(), which locks i_mutex on both    pipe->inode and target inode  - ordering depends on inode pointers, can happen that pipe->inode is    locked first  - __splice_from_pipe() needs more data, calls pipe_wait()  - this releases lock on pipe->inode, goes to interruptible sleep  - task B calls generic_file_splice_write(), similarly to the first  - this locks pipe->inode, then tries to lock inode, but that is    already held by task A  - task A is interrupted, it tries to lock pipe->inode, but fails, as    it is already held by task B  - ABBA deadlock  Fix this by explicitly ordering locks: the outer lock must be on target inode and the inner lock (which is later unlocked and relocked) must be on pipe->inode.  This is OK, pipe inodes and target inodes form two nonoverlapping sets, generic_file_splice_write() and friends are not called with a target which is a pipe.  Cc: stable@kernel.org ",
        "func_before": "ssize_t splice_from_pipe(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t loff_t *ppos, size_t len, unsigned int flags,\n\t\t\t splice_actor *actor)\n{\n\tssize_t ret;\n\tstruct inode *inode = out->f_mapping->host;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\n\t/*\n\t * The actor worker might be calling ->write_begin and\n\t * ->write_end. Most of the time, these expect i_mutex to\n\t * be held. Since this may result in an ABBA deadlock with\n\t * pipe->inode, we have to order lock acquiry here.\n\t */\n\tinode_double_lock(inode, pipe->inode);\n\tret = __splice_from_pipe(pipe, &sd, actor);\n\tinode_double_unlock(inode, pipe->inode);\n\n\treturn ret;\n}",
        "func": "ssize_t splice_from_pipe(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t loff_t *ppos, size_t len, unsigned int flags,\n\t\t\t splice_actor *actor)\n{\n\tssize_t ret;\n\tstruct inode *inode = out->f_mapping->host;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\n\t/*\n\t * The actor worker might be calling ->write_begin and\n\t * ->write_end. Most of the time, these expect i_mutex to\n\t * be held. Since this may result in an ABBA deadlock with\n\t * pipe->inode, we have to order lock acquiry here.\n\t *\n\t * Outer lock must be inode->i_mutex, as pipe_wait() will\n\t * release and reacquire pipe->inode->i_mutex, AND inode must\n\t * never be a pipe.\n\t */\n\tWARN_ON(S_ISFIFO(inode->i_mode));\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\tif (pipe->inode)\n\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n\tret = __splice_from_pipe(pipe, &sd, actor);\n\tif (pipe->inode)\n\t\tmutex_unlock(&pipe->inode->i_mutex);\n\tmutex_unlock(&inode->i_mutex);\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,10 +16,19 @@\n \t * ->write_end. Most of the time, these expect i_mutex to\n \t * be held. Since this may result in an ABBA deadlock with\n \t * pipe->inode, we have to order lock acquiry here.\n+\t *\n+\t * Outer lock must be inode->i_mutex, as pipe_wait() will\n+\t * release and reacquire pipe->inode->i_mutex, AND inode must\n+\t * never be a pipe.\n \t */\n-\tinode_double_lock(inode, pipe->inode);\n+\tWARN_ON(S_ISFIFO(inode->i_mode));\n+\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n+\tif (pipe->inode)\n+\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n \tret = __splice_from_pipe(pipe, &sd, actor);\n-\tinode_double_unlock(inode, pipe->inode);\n+\tif (pipe->inode)\n+\t\tmutex_unlock(&pipe->inode->i_mutex);\n+\tmutex_unlock(&inode->i_mutex);\n \n \treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tinode_double_lock(inode, pipe->inode);",
                "\tinode_double_unlock(inode, pipe->inode);"
            ],
            "added_lines": [
                "\t *",
                "\t * Outer lock must be inode->i_mutex, as pipe_wait() will",
                "\t * release and reacquire pipe->inode->i_mutex, AND inode must",
                "\t * never be a pipe.",
                "\tWARN_ON(S_ISFIFO(inode->i_mode));",
                "\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);",
                "\tif (pipe->inode)",
                "\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);",
                "\tif (pipe->inode)",
                "\t\tmutex_unlock(&pipe->inode->i_mutex);",
                "\tmutex_unlock(&inode->i_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1961",
        "func_name": "torvalds/linux/generic_file_splice_write",
        "description": "The inode double locking code in fs/ocfs2/file.c in the Linux kernel 2.6.30 before 2.6.30-rc3, 2.6.27 before 2.6.27.24, 2.6.29 before 2.6.29.4, and possibly other versions down to 2.6.19 allows local users to cause a denial of service (prevention of file creation and removal) via a series of splice system calls that trigger a deadlock between the generic_file_splice_write, splice_from_pipe, and ocfs2_file_splice_write functions.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff;h=7bfac9ecf0585962fe13584f5cf526d8c8e76f17",
        "commit_title": "There's a possible deadlock in generic_file_splice_write(),",
        "commit_text": "splice_from_pipe() and ocfs2_file_splice_write():   - task A calls generic_file_splice_write()  - this calls inode_double_lock(), which locks i_mutex on both    pipe->inode and target inode  - ordering depends on inode pointers, can happen that pipe->inode is    locked first  - __splice_from_pipe() needs more data, calls pipe_wait()  - this releases lock on pipe->inode, goes to interruptible sleep  - task B calls generic_file_splice_write(), similarly to the first  - this locks pipe->inode, then tries to lock inode, but that is    already held by task A  - task A is interrupted, it tries to lock pipe->inode, but fails, as    it is already held by task B  - ABBA deadlock  Fix this by explicitly ordering locks: the outer lock must be on target inode and the inner lock (which is later unlocked and relocked) must be on pipe->inode.  This is OK, pipe inodes and target inodes form two nonoverlapping sets, generic_file_splice_write() and friends are not called with a target which is a pipe.  Cc: stable@kernel.org ",
        "func_before": "ssize_t\ngeneric_file_splice_write(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t  loff_t *ppos, size_t len, unsigned int flags)\n{\n\tstruct address_space *mapping = out->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\tssize_t ret;\n\n\tinode_double_lock(inode, pipe->inode);\n\tret = file_remove_suid(out);\n\tif (likely(!ret))\n\t\tret = __splice_from_pipe(pipe, &sd, pipe_to_file);\n\tinode_double_unlock(inode, pipe->inode);\n\tif (ret > 0) {\n\t\tunsigned long nr_pages;\n\n\t\t*ppos += ret;\n\t\tnr_pages = (ret + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\n\t\t/*\n\t\t * If file or inode is SYNC and we actually wrote some data,\n\t\t * sync it.\n\t\t */\n\t\tif (unlikely((out->f_flags & O_SYNC) || IS_SYNC(inode))) {\n\t\t\tint err;\n\n\t\t\tmutex_lock(&inode->i_mutex);\n\t\t\terr = generic_osync_inode(inode, mapping,\n\t\t\t\t\t\t  OSYNC_METADATA|OSYNC_DATA);\n\t\t\tmutex_unlock(&inode->i_mutex);\n\n\t\t\tif (err)\n\t\t\t\tret = err;\n\t\t}\n\t\tbalance_dirty_pages_ratelimited_nr(mapping, nr_pages);\n\t}\n\n\treturn ret;\n}",
        "func": "ssize_t\ngeneric_file_splice_write(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t  loff_t *ppos, size_t len, unsigned int flags)\n{\n\tstruct address_space *mapping = out->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\tssize_t ret;\n\n\tWARN_ON(S_ISFIFO(inode->i_mode));\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\tret = file_remove_suid(out);\n\tif (likely(!ret)) {\n\t\tif (pipe->inode)\n\t\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n\t\tret = __splice_from_pipe(pipe, &sd, pipe_to_file);\n\t\tif (pipe->inode)\n\t\t\tmutex_unlock(&pipe->inode->i_mutex);\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\tif (ret > 0) {\n\t\tunsigned long nr_pages;\n\n\t\t*ppos += ret;\n\t\tnr_pages = (ret + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\n\n\t\t/*\n\t\t * If file or inode is SYNC and we actually wrote some data,\n\t\t * sync it.\n\t\t */\n\t\tif (unlikely((out->f_flags & O_SYNC) || IS_SYNC(inode))) {\n\t\t\tint err;\n\n\t\t\tmutex_lock(&inode->i_mutex);\n\t\t\terr = generic_osync_inode(inode, mapping,\n\t\t\t\t\t\t  OSYNC_METADATA|OSYNC_DATA);\n\t\t\tmutex_unlock(&inode->i_mutex);\n\n\t\t\tif (err)\n\t\t\t\tret = err;\n\t\t}\n\t\tbalance_dirty_pages_ratelimited_nr(mapping, nr_pages);\n\t}\n\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,11 +12,17 @@\n \t};\n \tssize_t ret;\n \n-\tinode_double_lock(inode, pipe->inode);\n+\tWARN_ON(S_ISFIFO(inode->i_mode));\n+\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n \tret = file_remove_suid(out);\n-\tif (likely(!ret))\n+\tif (likely(!ret)) {\n+\t\tif (pipe->inode)\n+\t\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n \t\tret = __splice_from_pipe(pipe, &sd, pipe_to_file);\n-\tinode_double_unlock(inode, pipe->inode);\n+\t\tif (pipe->inode)\n+\t\t\tmutex_unlock(&pipe->inode->i_mutex);\n+\t}\n+\tmutex_unlock(&inode->i_mutex);\n \tif (ret > 0) {\n \t\tunsigned long nr_pages;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tinode_double_lock(inode, pipe->inode);",
                "\tif (likely(!ret))",
                "\tinode_double_unlock(inode, pipe->inode);"
            ],
            "added_lines": [
                "\tWARN_ON(S_ISFIFO(inode->i_mode));",
                "\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);",
                "\tif (likely(!ret)) {",
                "\t\tif (pipe->inode)",
                "\t\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);",
                "\t\tif (pipe->inode)",
                "\t\t\tmutex_unlock(&pipe->inode->i_mutex);",
                "\t}",
                "\tmutex_unlock(&inode->i_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-1961",
        "func_name": "torvalds/linux/ocfs2_file_splice_write",
        "description": "The inode double locking code in fs/ocfs2/file.c in the Linux kernel 2.6.30 before 2.6.30-rc3, 2.6.27 before 2.6.27.24, 2.6.29 before 2.6.29.4, and possibly other versions down to 2.6.19 allows local users to cause a denial of service (prevention of file creation and removal) via a series of splice system calls that trigger a deadlock between the generic_file_splice_write, splice_from_pipe, and ocfs2_file_splice_write functions.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commitdiff;h=7bfac9ecf0585962fe13584f5cf526d8c8e76f17",
        "commit_title": "There's a possible deadlock in generic_file_splice_write(),",
        "commit_text": "splice_from_pipe() and ocfs2_file_splice_write():   - task A calls generic_file_splice_write()  - this calls inode_double_lock(), which locks i_mutex on both    pipe->inode and target inode  - ordering depends on inode pointers, can happen that pipe->inode is    locked first  - __splice_from_pipe() needs more data, calls pipe_wait()  - this releases lock on pipe->inode, goes to interruptible sleep  - task B calls generic_file_splice_write(), similarly to the first  - this locks pipe->inode, then tries to lock inode, but that is    already held by task A  - task A is interrupted, it tries to lock pipe->inode, but fails, as    it is already held by task B  - ABBA deadlock  Fix this by explicitly ordering locks: the outer lock must be on target inode and the inner lock (which is later unlocked and relocked) must be on pipe->inode.  This is OK, pipe inodes and target inodes form two nonoverlapping sets, generic_file_splice_write() and friends are not called with a target which is a pipe.  Cc: stable@kernel.org ",
        "func_before": "static ssize_t ocfs2_file_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t       struct file *out,\n\t\t\t\t       loff_t *ppos,\n\t\t\t\t       size_t len,\n\t\t\t\t       unsigned int flags)\n{\n\tint ret;\n\tstruct inode *inode = out->f_path.dentry->d_inode;\n\n\tmlog_entry(\"(0x%p, 0x%p, %u, '%.*s')\\n\", out, pipe,\n\t\t   (unsigned int)len,\n\t\t   out->f_path.dentry->d_name.len,\n\t\t   out->f_path.dentry->d_name.name);\n\n\tinode_double_lock(inode, pipe->inode);\n\n\tret = ocfs2_rw_lock(inode, 1);\n\tif (ret < 0) {\n\t\tmlog_errno(ret);\n\t\tgoto out;\n\t}\n\n\tret = ocfs2_prepare_inode_for_write(out->f_path.dentry, ppos, len, 0,\n\t\t\t\t\t    NULL);\n\tif (ret < 0) {\n\t\tmlog_errno(ret);\n\t\tgoto out_unlock;\n\t}\n\n\tret = generic_file_splice_write_nolock(pipe, out, ppos, len, flags);\n\nout_unlock:\n\tocfs2_rw_unlock(inode, 1);\nout:\n\tinode_double_unlock(inode, pipe->inode);\n\n\tmlog_exit(ret);\n\treturn ret;\n}",
        "func": "static ssize_t ocfs2_file_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t       struct file *out,\n\t\t\t\t       loff_t *ppos,\n\t\t\t\t       size_t len,\n\t\t\t\t       unsigned int flags)\n{\n\tint ret;\n\tstruct inode *inode = out->f_path.dentry->d_inode;\n\n\tmlog_entry(\"(0x%p, 0x%p, %u, '%.*s')\\n\", out, pipe,\n\t\t   (unsigned int)len,\n\t\t   out->f_path.dentry->d_name.len,\n\t\t   out->f_path.dentry->d_name.name);\n\n\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n\n\tret = ocfs2_rw_lock(inode, 1);\n\tif (ret < 0) {\n\t\tmlog_errno(ret);\n\t\tgoto out;\n\t}\n\n\tret = ocfs2_prepare_inode_for_write(out->f_path.dentry, ppos, len, 0,\n\t\t\t\t\t    NULL);\n\tif (ret < 0) {\n\t\tmlog_errno(ret);\n\t\tgoto out_unlock;\n\t}\n\n\tif (pipe->inode)\n\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n\tret = generic_file_splice_write_nolock(pipe, out, ppos, len, flags);\n\tif (pipe->inode)\n\t\tmutex_unlock(&pipe->inode->i_mutex);\n\nout_unlock:\n\tocfs2_rw_unlock(inode, 1);\nout:\n\tmutex_unlock(&inode->i_mutex);\n\n\tmlog_exit(ret);\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,7 @@\n \t\t   out->f_path.dentry->d_name.len,\n \t\t   out->f_path.dentry->d_name.name);\n \n-\tinode_double_lock(inode, pipe->inode);\n+\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);\n \n \tret = ocfs2_rw_lock(inode, 1);\n \tif (ret < 0) {\n@@ -27,12 +27,16 @@\n \t\tgoto out_unlock;\n \t}\n \n+\tif (pipe->inode)\n+\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);\n \tret = generic_file_splice_write_nolock(pipe, out, ppos, len, flags);\n+\tif (pipe->inode)\n+\t\tmutex_unlock(&pipe->inode->i_mutex);\n \n out_unlock:\n \tocfs2_rw_unlock(inode, 1);\n out:\n-\tinode_double_unlock(inode, pipe->inode);\n+\tmutex_unlock(&inode->i_mutex);\n \n \tmlog_exit(ret);\n \treturn ret;",
        "diff_line_info": {
            "deleted_lines": [
                "\tinode_double_lock(inode, pipe->inode);",
                "\tinode_double_unlock(inode, pipe->inode);"
            ],
            "added_lines": [
                "\tmutex_lock_nested(&inode->i_mutex, I_MUTEX_PARENT);",
                "\tif (pipe->inode)",
                "\t\tmutex_lock_nested(&pipe->inode->i_mutex, I_MUTEX_CHILD);",
                "\tif (pipe->inode)",
                "\t\tmutex_unlock(&pipe->inode->i_mutex);",
                "\tmutex_unlock(&inode->i_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4272",
        "func_name": "torvalds/linux/rt_intern_hash",
        "description": "A certain Red Hat patch for net/ipv4/route.c in the Linux kernel 2.6.18 on Red Hat Enterprise Linux (RHEL) 5 allows remote attackers to cause a denial of service (deadlock) via crafted packets that force collisions in the IPv4 routing hash table, and trigger a routing \"emergency\" in which a hash chain is too long.  NOTE: this is related to an issue in the Linux kernel before 2.6.31, when the kernel routing cache is disabled, involving an uninitialized pointer and a panic.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=b6280b47a7a42970d098a3059f4ebe7e55e90d8d",
        "commit_title": "When route caching is disabled (rt_caching returns false), We still use route",
        "commit_text": "cache entries that are created and passed into rt_intern_hash once.  These routes need to be made usable for the one call path that holds a reference to them, and they need to be reclaimed when they're finished with their use.  To be made usable, they need to be associated with a neighbor table entry (which they currently are not), otherwise iproute_finish2 just discards the packet, since we don't know which L2 peer to send the packet to.  To do this binding, we need to follow the path a bit higher up in rt_intern_hash, which calls arp_bind_neighbour, but not assign the route entry to the hash table. Currently, if caching is off, we simply assign the route to the rp pointer and are reutrn success.  This patch associates us with a neighbor entry first.  Secondly, we need to make sure that any single use routes like this are known to the garbage collector when caching is off.  If caching is off, and we try to hash in a route, it will leak when its refcount reaches zero.  To avoid this, this patch calls rt_free on the route cache entry passed into rt_intern_hash. This places us on the gc list for the route cache garbage collector, so that when its refcount reaches zero, it will be reclaimed (Thanks to Alexey for this suggestion).  I've tested this on a local system here, and with these patches in place, I'm able to maintain routed connectivity to remote systems, even if I set /proc/sys/net/ipv4/rt_cache_rebuild_count to -1, which forces rt_caching to return false.  ",
        "func_before": "static int rt_intern_hash(unsigned hash, struct rtable *rt,\n\t\t\t  struct rtable **rp, struct sk_buff *skb)\n{\n\tstruct rtable\t*rth, **rthp;\n\tunsigned long\tnow;\n\tstruct rtable *cand, **candp;\n\tu32 \t\tmin_score;\n\tint\t\tchain_length;\n\tint attempts = !in_softirq();\n\nrestart:\n\tchain_length = 0;\n\tmin_score = ~(u32)0;\n\tcand = NULL;\n\tcandp = NULL;\n\tnow = jiffies;\n\n\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t/*\n\t\t * If we're not caching, just tell the caller we\n\t\t * were successful and don't touch the route.  The\n\t\t * caller hold the sole reference to the cache entry, and\n\t\t * it will be released when the caller is done with it.\n\t\t * If we drop it here, the callers have no way to resolve routes\n\t\t * when we're not caching.  Instead, just point *rp at rt, so\n\t\t * the caller gets a single use out of the route\n\t\t */\n\t\tgoto report_and_exit;\n\t}\n\n\trthp = &rt_hash_table[hash].chain;\n\n\tspin_lock_bh(rt_hash_lock_addr(hash));\n\twhile ((rth = *rthp) != NULL) {\n\t\tif (rt_is_expired(rth)) {\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\trt_free(rth);\n\t\t\tcontinue;\n\t\t}\n\t\tif (compare_keys(&rth->fl, &rt->fl) && compare_netns(rth, rt)) {\n\t\t\t/* Put it first */\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the deletion\n\t\t\t * must be visible to another weakly ordered CPU before\n\t\t\t * the insertion at the start of the hash chain.\n\t\t\t */\n\t\t\trcu_assign_pointer(rth->u.dst.rt_next,\n\t\t\t\t\t   rt_hash_table[hash].chain);\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the update writes\n\t\t\t * must be ordered for consistency on SMP.\n\t\t\t */\n\t\t\trcu_assign_pointer(rt_hash_table[hash].chain, rth);\n\n\t\t\tdst_use(&rth->u.dst, now);\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\trt_drop(rt);\n\t\t\tif (rp)\n\t\t\t\t*rp = rth;\n\t\t\telse\n\t\t\t\tskb_dst_set(skb, &rth->u.dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!atomic_read(&rth->u.dst.__refcnt)) {\n\t\t\tu32 score = rt_score(rth);\n\n\t\t\tif (score <= min_score) {\n\t\t\t\tcand = rth;\n\t\t\t\tcandp = rthp;\n\t\t\t\tmin_score = score;\n\t\t\t}\n\t\t}\n\n\t\tchain_length++;\n\n\t\trthp = &rth->u.dst.rt_next;\n\t}\n\n\tif (cand) {\n\t\t/* ip_rt_gc_elasticity used to be average length of chain\n\t\t * length, when exceeded gc becomes really aggressive.\n\t\t *\n\t\t * The second limit is less certain. At the moment it allows\n\t\t * only 2 entries per bucket. We will see.\n\t\t */\n\t\tif (chain_length > ip_rt_gc_elasticity) {\n\t\t\t*candp = cand->u.dst.rt_next;\n\t\t\trt_free(cand);\n\t\t}\n\t} else {\n\t\tif (chain_length > rt_chain_length_max) {\n\t\t\tstruct net *net = dev_net(rt->u.dst.dev);\n\t\t\tint num = ++net->ipv4.current_rt_cache_rebuild_count;\n\t\t\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t\t\tprintk(KERN_WARNING \"%s: %d rebuilds is over limit, route caching disabled\\n\",\n\t\t\t\t\trt->u.dst.dev->name, num);\n\t\t\t}\n\t\t\trt_emergency_hash_rebuild(dev_net(rt->u.dst.dev));\n\t\t}\n\t}\n\n\t/* Try to bind route to arp only if it is output\n\t   route or unicast forwarding path.\n\t */\n\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n\t\tint err = arp_bind_neighbour(&rt->u.dst);\n\t\tif (err) {\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\tif (err != -ENOBUFS) {\n\t\t\t\trt_drop(rt);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Neighbour tables are full and nothing\n\t\t\t   can be released. Try to shrink route cache,\n\t\t\t   it is most likely it holds some neighbour records.\n\t\t\t */\n\t\t\tif (attempts-- > 0) {\n\t\t\t\tint saved_elasticity = ip_rt_gc_elasticity;\n\t\t\t\tint saved_int = ip_rt_gc_min_interval;\n\t\t\t\tip_rt_gc_elasticity\t= 1;\n\t\t\t\tip_rt_gc_min_interval\t= 0;\n\t\t\t\trt_garbage_collect(&ipv4_dst_ops);\n\t\t\t\tip_rt_gc_min_interval\t= saved_int;\n\t\t\t\tip_rt_gc_elasticity\t= saved_elasticity;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_WARNING \"Neighbour table overflow.\\n\");\n\t\t\trt_drop(rt);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t}\n\n\trt->u.dst.rt_next = rt_hash_table[hash].chain;\n\n#if RT_CACHE_DEBUG >= 2\n\tif (rt->u.dst.rt_next) {\n\t\tstruct rtable *trt;\n\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\", hash, &rt->rt_dst);\n\t\tfor (trt = rt->u.dst.rt_next; trt; trt = trt->u.dst.rt_next)\n\t\t\tprintk(\" . %pI4\", &trt->rt_dst);\n\t\tprintk(\"\\n\");\n\t}\n#endif\n\t/*\n\t * Since lookup is lockfree, we must make sure\n\t * previous writes to rt are comitted to memory\n\t * before making rt visible to other CPUS.\n\t */\n\trcu_assign_pointer(rt_hash_table[hash].chain, rt);\n\n\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\nreport_and_exit:\n\tif (rp)\n\t\t*rp = rt;\n\telse\n\t\tskb_dst_set(skb, &rt->u.dst);\n\treturn 0;\n}",
        "func": "static int rt_intern_hash(unsigned hash, struct rtable *rt,\n\t\t\t  struct rtable **rp, struct sk_buff *skb)\n{\n\tstruct rtable\t*rth, **rthp;\n\tunsigned long\tnow;\n\tstruct rtable *cand, **candp;\n\tu32 \t\tmin_score;\n\tint\t\tchain_length;\n\tint attempts = !in_softirq();\n\nrestart:\n\tchain_length = 0;\n\tmin_score = ~(u32)0;\n\tcand = NULL;\n\tcandp = NULL;\n\tnow = jiffies;\n\n\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t/*\n\t\t * If we're not caching, just tell the caller we\n\t\t * were successful and don't touch the route.  The\n\t\t * caller hold the sole reference to the cache entry, and\n\t\t * it will be released when the caller is done with it.\n\t\t * If we drop it here, the callers have no way to resolve routes\n\t\t * when we're not caching.  Instead, just point *rp at rt, so\n\t\t * the caller gets a single use out of the route\n\t\t * Note that we do rt_free on this new route entry, so that\n\t\t * once its refcount hits zero, we are still able to reap it\n\t\t * (Thanks Alexey)\n\t\t * Note also the rt_free uses call_rcu.  We don't actually\n\t\t * need rcu protection here, this is just our path to get\n\t\t * on the route gc list.\n\t\t */\n\n\t\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n\t\t\tint err = arp_bind_neighbour(&rt->u.dst);\n\t\t\tif (err) {\n\t\t\t\tif (net_ratelimit())\n\t\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t\t    \"Neighbour table failure & not caching routes.\\n\");\n\t\t\t\trt_drop(rt);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\trt_free(rt);\n\t\tgoto skip_hashing;\n\t}\n\n\trthp = &rt_hash_table[hash].chain;\n\n\tspin_lock_bh(rt_hash_lock_addr(hash));\n\twhile ((rth = *rthp) != NULL) {\n\t\tif (rt_is_expired(rth)) {\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\trt_free(rth);\n\t\t\tcontinue;\n\t\t}\n\t\tif (compare_keys(&rth->fl, &rt->fl) && compare_netns(rth, rt)) {\n\t\t\t/* Put it first */\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the deletion\n\t\t\t * must be visible to another weakly ordered CPU before\n\t\t\t * the insertion at the start of the hash chain.\n\t\t\t */\n\t\t\trcu_assign_pointer(rth->u.dst.rt_next,\n\t\t\t\t\t   rt_hash_table[hash].chain);\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the update writes\n\t\t\t * must be ordered for consistency on SMP.\n\t\t\t */\n\t\t\trcu_assign_pointer(rt_hash_table[hash].chain, rth);\n\n\t\t\tdst_use(&rth->u.dst, now);\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\trt_drop(rt);\n\t\t\tif (rp)\n\t\t\t\t*rp = rth;\n\t\t\telse\n\t\t\t\tskb_dst_set(skb, &rth->u.dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!atomic_read(&rth->u.dst.__refcnt)) {\n\t\t\tu32 score = rt_score(rth);\n\n\t\t\tif (score <= min_score) {\n\t\t\t\tcand = rth;\n\t\t\t\tcandp = rthp;\n\t\t\t\tmin_score = score;\n\t\t\t}\n\t\t}\n\n\t\tchain_length++;\n\n\t\trthp = &rth->u.dst.rt_next;\n\t}\n\n\tif (cand) {\n\t\t/* ip_rt_gc_elasticity used to be average length of chain\n\t\t * length, when exceeded gc becomes really aggressive.\n\t\t *\n\t\t * The second limit is less certain. At the moment it allows\n\t\t * only 2 entries per bucket. We will see.\n\t\t */\n\t\tif (chain_length > ip_rt_gc_elasticity) {\n\t\t\t*candp = cand->u.dst.rt_next;\n\t\t\trt_free(cand);\n\t\t}\n\t} else {\n\t\tif (chain_length > rt_chain_length_max) {\n\t\t\tstruct net *net = dev_net(rt->u.dst.dev);\n\t\t\tint num = ++net->ipv4.current_rt_cache_rebuild_count;\n\t\t\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t\t\tprintk(KERN_WARNING \"%s: %d rebuilds is over limit, route caching disabled\\n\",\n\t\t\t\t\trt->u.dst.dev->name, num);\n\t\t\t}\n\t\t\trt_emergency_hash_rebuild(dev_net(rt->u.dst.dev));\n\t\t}\n\t}\n\n\t/* Try to bind route to arp only if it is output\n\t   route or unicast forwarding path.\n\t */\n\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n\t\tint err = arp_bind_neighbour(&rt->u.dst);\n\t\tif (err) {\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\tif (err != -ENOBUFS) {\n\t\t\t\trt_drop(rt);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Neighbour tables are full and nothing\n\t\t\t   can be released. Try to shrink route cache,\n\t\t\t   it is most likely it holds some neighbour records.\n\t\t\t */\n\t\t\tif (attempts-- > 0) {\n\t\t\t\tint saved_elasticity = ip_rt_gc_elasticity;\n\t\t\t\tint saved_int = ip_rt_gc_min_interval;\n\t\t\t\tip_rt_gc_elasticity\t= 1;\n\t\t\t\tip_rt_gc_min_interval\t= 0;\n\t\t\t\trt_garbage_collect(&ipv4_dst_ops);\n\t\t\t\tip_rt_gc_min_interval\t= saved_int;\n\t\t\t\tip_rt_gc_elasticity\t= saved_elasticity;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_WARNING \"Neighbour table overflow.\\n\");\n\t\t\trt_drop(rt);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t}\n\n\trt->u.dst.rt_next = rt_hash_table[hash].chain;\n\n#if RT_CACHE_DEBUG >= 2\n\tif (rt->u.dst.rt_next) {\n\t\tstruct rtable *trt;\n\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\",\n\t\t       hash, &rt->rt_dst);\n\t\tfor (trt = rt->u.dst.rt_next; trt; trt = trt->u.dst.rt_next)\n\t\t\tprintk(\" . %pI4\", &trt->rt_dst);\n\t\tprintk(\"\\n\");\n\t}\n#endif\n\t/*\n\t * Since lookup is lockfree, we must make sure\n\t * previous writes to rt are comitted to memory\n\t * before making rt visible to other CPUS.\n\t */\n\trcu_assign_pointer(rt_hash_table[hash].chain, rt);\n\n\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\nskip_hashing:\n\tif (rp)\n\t\t*rp = rt;\n\telse\n\t\tskb_dst_set(skb, &rt->u.dst);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,8 +24,27 @@\n \t\t * If we drop it here, the callers have no way to resolve routes\n \t\t * when we're not caching.  Instead, just point *rp at rt, so\n \t\t * the caller gets a single use out of the route\n+\t\t * Note that we do rt_free on this new route entry, so that\n+\t\t * once its refcount hits zero, we are still able to reap it\n+\t\t * (Thanks Alexey)\n+\t\t * Note also the rt_free uses call_rcu.  We don't actually\n+\t\t * need rcu protection here, this is just our path to get\n+\t\t * on the route gc list.\n \t\t */\n-\t\tgoto report_and_exit;\n+\n+\t\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n+\t\t\tint err = arp_bind_neighbour(&rt->u.dst);\n+\t\t\tif (err) {\n+\t\t\t\tif (net_ratelimit())\n+\t\t\t\t\tprintk(KERN_WARNING\n+\t\t\t\t\t    \"Neighbour table failure & not caching routes.\\n\");\n+\t\t\t\trt_drop(rt);\n+\t\t\t\treturn err;\n+\t\t\t}\n+\t\t}\n+\n+\t\trt_free(rt);\n+\t\tgoto skip_hashing;\n \t}\n \n \trthp = &rt_hash_table[hash].chain;\n@@ -142,7 +161,8 @@\n #if RT_CACHE_DEBUG >= 2\n \tif (rt->u.dst.rt_next) {\n \t\tstruct rtable *trt;\n-\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\", hash, &rt->rt_dst);\n+\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\",\n+\t\t       hash, &rt->rt_dst);\n \t\tfor (trt = rt->u.dst.rt_next; trt; trt = trt->u.dst.rt_next)\n \t\t\tprintk(\" . %pI4\", &trt->rt_dst);\n \t\tprintk(\"\\n\");\n@@ -157,7 +177,7 @@\n \n \tspin_unlock_bh(rt_hash_lock_addr(hash));\n \n-report_and_exit:\n+skip_hashing:\n \tif (rp)\n \t\t*rp = rt;\n \telse",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgoto report_and_exit;",
                "\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\", hash, &rt->rt_dst);",
                "report_and_exit:"
            ],
            "added_lines": [
                "\t\t * Note that we do rt_free on this new route entry, so that",
                "\t\t * once its refcount hits zero, we are still able to reap it",
                "\t\t * (Thanks Alexey)",
                "\t\t * Note also the rt_free uses call_rcu.  We don't actually",
                "\t\t * need rcu protection here, this is just our path to get",
                "\t\t * on the route gc list.",
                "",
                "\t\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {",
                "\t\t\tint err = arp_bind_neighbour(&rt->u.dst);",
                "\t\t\tif (err) {",
                "\t\t\t\tif (net_ratelimit())",
                "\t\t\t\t\tprintk(KERN_WARNING",
                "\t\t\t\t\t    \"Neighbour table failure & not caching routes.\\n\");",
                "\t\t\t\trt_drop(rt);",
                "\t\t\t\treturn err;",
                "\t\t\t}",
                "\t\t}",
                "",
                "\t\trt_free(rt);",
                "\t\tgoto skip_hashing;",
                "\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\",",
                "\t\t       hash, &rt->rt_dst);",
                "skip_hashing:"
            ]
        }
    },
    {
        "cve_id": "CVE-2009-4272",
        "func_name": "torvalds/linux/rt_intern_hash",
        "description": "A certain Red Hat patch for net/ipv4/route.c in the Linux kernel 2.6.18 on Red Hat Enterprise Linux (RHEL) 5 allows remote attackers to cause a denial of service (deadlock) via crafted packets that force collisions in the IPv4 routing hash table, and trigger a routing \"emergency\" in which a hash chain is too long.  NOTE: this is related to an issue in the Linux kernel before 2.6.31, when the kernel routing cache is disabled, involving an uninitialized pointer and a panic.",
        "git_url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=commit;h=73e42897e8e5619eacb787d2ce69be12f47cfc21",
        "commit_title": "Don't drop route if we're not caching\t",
        "commit_text": " \tI recently got a report of an oops on a route lookup.  Maxime was testing what would happen if route caching was turned off (doing so by setting making rt_caching always return 0), and found that it triggered an oops.  I looked at it and found that the problem stemmed from the fact that the route lookup routines were returning success from their lookup paths (which is good), but never set the **rp pointer to anything (which is bad).  This happens because in rt_intern_hash, if rt_caching returns false, we call rt_drop and return 0. This almost emulates slient success.  What we should be doing is assigning *rp = rt and _not_ dropping the route.  This way, during slow path lookups, when we create a new route cache entry, we don't immediately discard it, rather we just don't add it into the cache hash table, but we let this one lookup use it for the purpose of this route request.  Maxime has tested and reports it prevents the oops.  There is still a subsequent routing issue that I'm looking into further, but I'm confident that, even if its related to this same path, this patch makes sense to take.  ",
        "func_before": "static int rt_intern_hash(unsigned hash, struct rtable *rt,\n\t\t\t  struct rtable **rp, struct sk_buff *skb)\n{\n\tstruct rtable\t*rth, **rthp;\n\tunsigned long\tnow;\n\tstruct rtable *cand, **candp;\n\tu32 \t\tmin_score;\n\tint\t\tchain_length;\n\tint attempts = !in_softirq();\n\nrestart:\n\tchain_length = 0;\n\tmin_score = ~(u32)0;\n\tcand = NULL;\n\tcandp = NULL;\n\tnow = jiffies;\n\n\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\trt_drop(rt);\n\t\treturn 0;\n\t}\n\n\trthp = &rt_hash_table[hash].chain;\n\n\tspin_lock_bh(rt_hash_lock_addr(hash));\n\twhile ((rth = *rthp) != NULL) {\n\t\tif (rt_is_expired(rth)) {\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\trt_free(rth);\n\t\t\tcontinue;\n\t\t}\n\t\tif (compare_keys(&rth->fl, &rt->fl) && compare_netns(rth, rt)) {\n\t\t\t/* Put it first */\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the deletion\n\t\t\t * must be visible to another weakly ordered CPU before\n\t\t\t * the insertion at the start of the hash chain.\n\t\t\t */\n\t\t\trcu_assign_pointer(rth->u.dst.rt_next,\n\t\t\t\t\t   rt_hash_table[hash].chain);\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the update writes\n\t\t\t * must be ordered for consistency on SMP.\n\t\t\t */\n\t\t\trcu_assign_pointer(rt_hash_table[hash].chain, rth);\n\n\t\t\tdst_use(&rth->u.dst, now);\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\trt_drop(rt);\n\t\t\tif (rp)\n\t\t\t\t*rp = rth;\n\t\t\telse\n\t\t\t\tskb_dst_set(skb, &rth->u.dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!atomic_read(&rth->u.dst.__refcnt)) {\n\t\t\tu32 score = rt_score(rth);\n\n\t\t\tif (score <= min_score) {\n\t\t\t\tcand = rth;\n\t\t\t\tcandp = rthp;\n\t\t\t\tmin_score = score;\n\t\t\t}\n\t\t}\n\n\t\tchain_length++;\n\n\t\trthp = &rth->u.dst.rt_next;\n\t}\n\n\tif (cand) {\n\t\t/* ip_rt_gc_elasticity used to be average length of chain\n\t\t * length, when exceeded gc becomes really aggressive.\n\t\t *\n\t\t * The second limit is less certain. At the moment it allows\n\t\t * only 2 entries per bucket. We will see.\n\t\t */\n\t\tif (chain_length > ip_rt_gc_elasticity) {\n\t\t\t*candp = cand->u.dst.rt_next;\n\t\t\trt_free(cand);\n\t\t}\n\t} else {\n\t\tif (chain_length > rt_chain_length_max) {\n\t\t\tstruct net *net = dev_net(rt->u.dst.dev);\n\t\t\tint num = ++net->ipv4.current_rt_cache_rebuild_count;\n\t\t\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t\t\tprintk(KERN_WARNING \"%s: %d rebuilds is over limit, route caching disabled\\n\",\n\t\t\t\t\trt->u.dst.dev->name, num);\n\t\t\t}\n\t\t\trt_emergency_hash_rebuild(dev_net(rt->u.dst.dev));\n\t\t}\n\t}\n\n\t/* Try to bind route to arp only if it is output\n\t   route or unicast forwarding path.\n\t */\n\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n\t\tint err = arp_bind_neighbour(&rt->u.dst);\n\t\tif (err) {\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\tif (err != -ENOBUFS) {\n\t\t\t\trt_drop(rt);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Neighbour tables are full and nothing\n\t\t\t   can be released. Try to shrink route cache,\n\t\t\t   it is most likely it holds some neighbour records.\n\t\t\t */\n\t\t\tif (attempts-- > 0) {\n\t\t\t\tint saved_elasticity = ip_rt_gc_elasticity;\n\t\t\t\tint saved_int = ip_rt_gc_min_interval;\n\t\t\t\tip_rt_gc_elasticity\t= 1;\n\t\t\t\tip_rt_gc_min_interval\t= 0;\n\t\t\t\trt_garbage_collect(&ipv4_dst_ops);\n\t\t\t\tip_rt_gc_min_interval\t= saved_int;\n\t\t\t\tip_rt_gc_elasticity\t= saved_elasticity;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_WARNING \"Neighbour table overflow.\\n\");\n\t\t\trt_drop(rt);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t}\n\n\trt->u.dst.rt_next = rt_hash_table[hash].chain;\n\n#if RT_CACHE_DEBUG >= 2\n\tif (rt->u.dst.rt_next) {\n\t\tstruct rtable *trt;\n\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\", hash, &rt->rt_dst);\n\t\tfor (trt = rt->u.dst.rt_next; trt; trt = trt->u.dst.rt_next)\n\t\t\tprintk(\" . %pI4\", &trt->rt_dst);\n\t\tprintk(\"\\n\");\n\t}\n#endif\n\t/*\n\t * Since lookup is lockfree, we must make sure\n\t * previous writes to rt are comitted to memory\n\t * before making rt visible to other CPUS.\n\t */\n\trcu_assign_pointer(rt_hash_table[hash].chain, rt);\n\n\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\tif (rp)\n\t\t*rp = rt;\n\telse\n\t\tskb_dst_set(skb, &rt->u.dst);\n\treturn 0;\n}",
        "func": "static int rt_intern_hash(unsigned hash, struct rtable *rt,\n\t\t\t  struct rtable **rp, struct sk_buff *skb)\n{\n\tstruct rtable\t*rth, **rthp;\n\tunsigned long\tnow;\n\tstruct rtable *cand, **candp;\n\tu32 \t\tmin_score;\n\tint\t\tchain_length;\n\tint attempts = !in_softirq();\n\nrestart:\n\tchain_length = 0;\n\tmin_score = ~(u32)0;\n\tcand = NULL;\n\tcandp = NULL;\n\tnow = jiffies;\n\n\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t/*\n\t\t * If we're not caching, just tell the caller we\n\t\t * were successful and don't touch the route.  The\n\t\t * caller hold the sole reference to the cache entry, and\n\t\t * it will be released when the caller is done with it.\n\t\t * If we drop it here, the callers have no way to resolve routes\n\t\t * when we're not caching.  Instead, just point *rp at rt, so\n\t\t * the caller gets a single use out of the route\n\t\t */\n\t\tgoto report_and_exit;\n\t}\n\n\trthp = &rt_hash_table[hash].chain;\n\n\tspin_lock_bh(rt_hash_lock_addr(hash));\n\twhile ((rth = *rthp) != NULL) {\n\t\tif (rt_is_expired(rth)) {\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\trt_free(rth);\n\t\t\tcontinue;\n\t\t}\n\t\tif (compare_keys(&rth->fl, &rt->fl) && compare_netns(rth, rt)) {\n\t\t\t/* Put it first */\n\t\t\t*rthp = rth->u.dst.rt_next;\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the deletion\n\t\t\t * must be visible to another weakly ordered CPU before\n\t\t\t * the insertion at the start of the hash chain.\n\t\t\t */\n\t\t\trcu_assign_pointer(rth->u.dst.rt_next,\n\t\t\t\t\t   rt_hash_table[hash].chain);\n\t\t\t/*\n\t\t\t * Since lookup is lockfree, the update writes\n\t\t\t * must be ordered for consistency on SMP.\n\t\t\t */\n\t\t\trcu_assign_pointer(rt_hash_table[hash].chain, rth);\n\n\t\t\tdst_use(&rth->u.dst, now);\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\trt_drop(rt);\n\t\t\tif (rp)\n\t\t\t\t*rp = rth;\n\t\t\telse\n\t\t\t\tskb_dst_set(skb, &rth->u.dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (!atomic_read(&rth->u.dst.__refcnt)) {\n\t\t\tu32 score = rt_score(rth);\n\n\t\t\tif (score <= min_score) {\n\t\t\t\tcand = rth;\n\t\t\t\tcandp = rthp;\n\t\t\t\tmin_score = score;\n\t\t\t}\n\t\t}\n\n\t\tchain_length++;\n\n\t\trthp = &rth->u.dst.rt_next;\n\t}\n\n\tif (cand) {\n\t\t/* ip_rt_gc_elasticity used to be average length of chain\n\t\t * length, when exceeded gc becomes really aggressive.\n\t\t *\n\t\t * The second limit is less certain. At the moment it allows\n\t\t * only 2 entries per bucket. We will see.\n\t\t */\n\t\tif (chain_length > ip_rt_gc_elasticity) {\n\t\t\t*candp = cand->u.dst.rt_next;\n\t\t\trt_free(cand);\n\t\t}\n\t} else {\n\t\tif (chain_length > rt_chain_length_max) {\n\t\t\tstruct net *net = dev_net(rt->u.dst.dev);\n\t\t\tint num = ++net->ipv4.current_rt_cache_rebuild_count;\n\t\t\tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n\t\t\t\tprintk(KERN_WARNING \"%s: %d rebuilds is over limit, route caching disabled\\n\",\n\t\t\t\t\trt->u.dst.dev->name, num);\n\t\t\t}\n\t\t\trt_emergency_hash_rebuild(dev_net(rt->u.dst.dev));\n\t\t}\n\t}\n\n\t/* Try to bind route to arp only if it is output\n\t   route or unicast forwarding path.\n\t */\n\tif (rt->rt_type == RTN_UNICAST || rt->fl.iif == 0) {\n\t\tint err = arp_bind_neighbour(&rt->u.dst);\n\t\tif (err) {\n\t\t\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\n\t\t\tif (err != -ENOBUFS) {\n\t\t\t\trt_drop(rt);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Neighbour tables are full and nothing\n\t\t\t   can be released. Try to shrink route cache,\n\t\t\t   it is most likely it holds some neighbour records.\n\t\t\t */\n\t\t\tif (attempts-- > 0) {\n\t\t\t\tint saved_elasticity = ip_rt_gc_elasticity;\n\t\t\t\tint saved_int = ip_rt_gc_min_interval;\n\t\t\t\tip_rt_gc_elasticity\t= 1;\n\t\t\t\tip_rt_gc_min_interval\t= 0;\n\t\t\t\trt_garbage_collect(&ipv4_dst_ops);\n\t\t\t\tip_rt_gc_min_interval\t= saved_int;\n\t\t\t\tip_rt_gc_elasticity\t= saved_elasticity;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\tif (net_ratelimit())\n\t\t\t\tprintk(KERN_WARNING \"Neighbour table overflow.\\n\");\n\t\t\trt_drop(rt);\n\t\t\treturn -ENOBUFS;\n\t\t}\n\t}\n\n\trt->u.dst.rt_next = rt_hash_table[hash].chain;\n\n#if RT_CACHE_DEBUG >= 2\n\tif (rt->u.dst.rt_next) {\n\t\tstruct rtable *trt;\n\t\tprintk(KERN_DEBUG \"rt_cache @%02x: %pI4\", hash, &rt->rt_dst);\n\t\tfor (trt = rt->u.dst.rt_next; trt; trt = trt->u.dst.rt_next)\n\t\t\tprintk(\" . %pI4\", &trt->rt_dst);\n\t\tprintk(\"\\n\");\n\t}\n#endif\n\t/*\n\t * Since lookup is lockfree, we must make sure\n\t * previous writes to rt are comitted to memory\n\t * before making rt visible to other CPUS.\n\t */\n\trcu_assign_pointer(rt_hash_table[hash].chain, rt);\n\n\tspin_unlock_bh(rt_hash_lock_addr(hash));\n\nreport_and_exit:\n\tif (rp)\n\t\t*rp = rt;\n\telse\n\t\tskb_dst_set(skb, &rt->u.dst);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,8 +16,16 @@\n \tnow = jiffies;\n \n \tif (!rt_caching(dev_net(rt->u.dst.dev))) {\n-\t\trt_drop(rt);\n-\t\treturn 0;\n+\t\t/*\n+\t\t * If we're not caching, just tell the caller we\n+\t\t * were successful and don't touch the route.  The\n+\t\t * caller hold the sole reference to the cache entry, and\n+\t\t * it will be released when the caller is done with it.\n+\t\t * If we drop it here, the callers have no way to resolve routes\n+\t\t * when we're not caching.  Instead, just point *rp at rt, so\n+\t\t * the caller gets a single use out of the route\n+\t\t */\n+\t\tgoto report_and_exit;\n \t}\n \n \trthp = &rt_hash_table[hash].chain;\n@@ -148,6 +156,8 @@\n \trcu_assign_pointer(rt_hash_table[hash].chain, rt);\n \n \tspin_unlock_bh(rt_hash_lock_addr(hash));\n+\n+report_and_exit:\n \tif (rp)\n \t\t*rp = rt;\n \telse",
        "diff_line_info": {
            "deleted_lines": [
                "\t\trt_drop(rt);",
                "\t\treturn 0;"
            ],
            "added_lines": [
                "\t\t/*",
                "\t\t * If we're not caching, just tell the caller we",
                "\t\t * were successful and don't touch the route.  The",
                "\t\t * caller hold the sole reference to the cache entry, and",
                "\t\t * it will be released when the caller is done with it.",
                "\t\t * If we drop it here, the callers have no way to resolve routes",
                "\t\t * when we're not caching.  Instead, just point *rp at rt, so",
                "\t\t * the caller gets a single use out of the route",
                "\t\t */",
                "\t\tgoto report_and_exit;",
                "",
                "report_and_exit:"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-0639",
        "func_name": "torvalds/linux/sctp_auto_asconf_init",
        "description": "A denial of service vulnerability due to a deadlock was found in sctp_auto_asconf_init in net/sctp/socket.c in the Linux kernels SCTP subsystem. This flaw allows guests with local user privileges to trigger a deadlock and potentially crash the system.",
        "git_url": "https://github.com/torvalds/linux/commit/6feb37b3b06e9049e20dcf7e23998f92c9c5be9a",
        "commit_title": "sctp: fix potential deadlock on &net->sctp.addr_wq_lock",
        "commit_text": " As &net->sctp.addr_wq_lock is also acquired by the timer sctp_addr_wq_timeout_handler() in protocal.c, the same lock acquisition at sctp_auto_asconf_init() seems should disable irq since it is called from sctp_accept() under process context.  Possible deadlock scenario: sctp_accept()     -> sctp_sock_migrate()     -> sctp_auto_asconf_init()     -> spin_lock(&net->sctp.addr_wq_lock)         <timer interrupt>         -> sctp_addr_wq_timeout_handler()         -> spin_lock_bh(&net->sctp.addr_wq_lock); (deadlock here)  This flaw was found using an experimental static analysis tool we are developing for irq-related deadlock.  The tentative patch fix the potential deadlock by spin_lock_bh().  Link: https://lore.kernel.org/r/20230627120340.19432-1-dg573847474@gmail.com",
        "func_before": "static void sctp_auto_asconf_init(struct sctp_sock *sp)\n{\n\tstruct net *net = sock_net(&sp->inet.sk);\n\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock(&net->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list, &net->sctp.auto_asconf_splist);\n\t\tspin_unlock(&net->sctp.addr_wq_lock);\n\t\tsp->do_auto_asconf = 1;\n\t}\n}",
        "func": "static void sctp_auto_asconf_init(struct sctp_sock *sp)\n{\n\tstruct net *net = sock_net(&sp->inet.sk);\n\n\tif (net->sctp.default_auto_asconf) {\n\t\tspin_lock_bh(&net->sctp.addr_wq_lock);\n\t\tlist_add_tail(&sp->auto_asconf_list, &net->sctp.auto_asconf_splist);\n\t\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n\t\tsp->do_auto_asconf = 1;\n\t}\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,9 +3,9 @@\n \tstruct net *net = sock_net(&sp->inet.sk);\n \n \tif (net->sctp.default_auto_asconf) {\n-\t\tspin_lock(&net->sctp.addr_wq_lock);\n+\t\tspin_lock_bh(&net->sctp.addr_wq_lock);\n \t\tlist_add_tail(&sp->auto_asconf_list, &net->sctp.auto_asconf_splist);\n-\t\tspin_unlock(&net->sctp.addr_wq_lock);\n+\t\tspin_unlock_bh(&net->sctp.addr_wq_lock);\n \t\tsp->do_auto_asconf = 1;\n \t}\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tspin_lock(&net->sctp.addr_wq_lock);",
                "\t\tspin_unlock(&net->sctp.addr_wq_lock);"
            ],
            "added_lines": [
                "\t\tspin_lock_bh(&net->sctp.addr_wq_lock);",
                "\t\tspin_unlock_bh(&net->sctp.addr_wq_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2024-0641",
        "func_name": "torvalds/linux/tipc_crypto_key_revoke",
        "description": "A denial of service vulnerability was found in tipc_crypto_key_revoke in net/tipc/crypto.c in the Linux kernels TIPC subsystem. This flaw allows guests with local user privileges to trigger a deadlock and potentially crash the system.",
        "git_url": "https://github.com/torvalds/linux/commit/08e50cf071847323414df0835109b6f3560d44f5",
        "commit_title": "tipc: fix a potential deadlock on &tx->lock",
        "commit_text": " It seems that tipc_crypto_key_revoke() could be be invoked by wokequeue tipc_crypto_work_rx() under process context and timer/rx callback under softirq context, thus the lock acquisition on &tx->lock seems better use spin_lock_bh() to prevent possible deadlock.  This flaw was found by an experimental static analysis tool I am developing for irq-related deadlock.  tipc_crypto_work_rx() <workqueue> --> tipc_crypto_key_distr() --> tipc_bcast_xmit() --> tipc_bcbase_xmit() --> tipc_bearer_bc_xmit() --> tipc_crypto_xmit() --> tipc_ehdr_build() --> tipc_crypto_key_revoke() --> spin_lock(&tx->lock) <timer interrupt>    --> tipc_disc_timeout()    --> tipc_bearer_xmit_skb()    --> tipc_crypto_xmit()    --> tipc_ehdr_build()    --> tipc_crypto_key_revoke()    --> spin_lock(&tx->lock) <deadlock here>  Link: https://lore.kernel.org/r/20230927181414.59928-1-dg573847474@gmail.com",
        "func_before": "static int tipc_crypto_key_revoke(struct net *net, u8 tx_key)\n{\n\tstruct tipc_crypto *tx = tipc_net(net)->crypto_tx;\n\tstruct tipc_key key;\n\n\tspin_lock(&tx->lock);\n\tkey = tx->key;\n\tWARN_ON(!key.active || tx_key != key.active);\n\n\t/* Free the active key */\n\ttipc_crypto_key_set_state(tx, key.passive, 0, key.pending);\n\ttipc_crypto_key_detach(tx->aead[key.active], &tx->lock);\n\tspin_unlock(&tx->lock);\n\n\tpr_warn(\"%s: key is revoked\\n\", tx->name);\n\treturn -EKEYREVOKED;\n}",
        "func": "static int tipc_crypto_key_revoke(struct net *net, u8 tx_key)\n{\n\tstruct tipc_crypto *tx = tipc_net(net)->crypto_tx;\n\tstruct tipc_key key;\n\n\tspin_lock_bh(&tx->lock);\n\tkey = tx->key;\n\tWARN_ON(!key.active || tx_key != key.active);\n\n\t/* Free the active key */\n\ttipc_crypto_key_set_state(tx, key.passive, 0, key.pending);\n\ttipc_crypto_key_detach(tx->aead[key.active], &tx->lock);\n\tspin_unlock_bh(&tx->lock);\n\n\tpr_warn(\"%s: key is revoked\\n\", tx->name);\n\treturn -EKEYREVOKED;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,14 +3,14 @@\n \tstruct tipc_crypto *tx = tipc_net(net)->crypto_tx;\n \tstruct tipc_key key;\n \n-\tspin_lock(&tx->lock);\n+\tspin_lock_bh(&tx->lock);\n \tkey = tx->key;\n \tWARN_ON(!key.active || tx_key != key.active);\n \n \t/* Free the active key */\n \ttipc_crypto_key_set_state(tx, key.passive, 0, key.pending);\n \ttipc_crypto_key_detach(tx->aead[key.active], &tx->lock);\n-\tspin_unlock(&tx->lock);\n+\tspin_unlock_bh(&tx->lock);\n \n \tpr_warn(\"%s: key is revoked\\n\", tx->name);\n \treturn -EKEYREVOKED;",
        "diff_line_info": {
            "deleted_lines": [
                "\tspin_lock(&tx->lock);",
                "\tspin_unlock(&tx->lock);"
            ],
            "added_lines": [
                "\tspin_lock_bh(&tx->lock);",
                "\tspin_unlock_bh(&tx->lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-46927",
        "func_name": "torvalds/linux/ne_set_user_memory_region_ioctl",
        "description": "In the Linux kernel, the following vulnerability has been resolved:\n\nnitro_enclaves: Use get_user_pages_unlocked() call to handle mmap assert\n\nAfter commit 5b78ed24e8ec (\"mm/pagemap: add mmap_assert_locked()\nannotations to find_vma*()\"), the call to get_user_pages() will trigger\nthe mmap assert.\n\nstatic inline void mmap_assert_locked(struct mm_struct *mm)\n{\n\tlockdep_assert_held(&mm->mmap_lock);\n\tVM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_lock), mm);\n}\n\n[   62.521410] kernel BUG at include/linux/mmap_lock.h:156!\n...........................................................\n[   62.538938] RIP: 0010:find_vma+0x32/0x80\n...........................................................\n[   62.605889] Call Trace:\n[   62.608502]  <TASK>\n[   62.610956]  ? lock_timer_base+0x61/0x80\n[   62.614106]  find_extend_vma+0x19/0x80\n[   62.617195]  __get_user_pages+0x9b/0x6a0\n[   62.620356]  __gup_longterm_locked+0x42d/0x450\n[   62.623721]  ? finish_wait+0x41/0x80\n[   62.626748]  ? __kmalloc+0x178/0x2f0\n[   62.629768]  ne_set_user_memory_region_ioctl.isra.0+0x225/0x6a0 [nitro_enclaves]\n[   62.635776]  ne_enclave_ioctl+0x1cf/0x6d7 [nitro_enclaves]\n[   62.639541]  __x64_sys_ioctl+0x82/0xb0\n[   62.642620]  do_syscall_64+0x3b/0x90\n[   62.645642]  entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nUse get_user_pages_unlocked() when setting the enclave memory regions.\nThat's a similar pattern as mmap_read_lock() used together with\nget_user_pages().",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3a0152b219523227c2a62a0a122cf99608287176",
        "commit_title": "After commit 5b78ed24e8ec (\"mm/pagemap: add mmap_assert_locked()",
        "commit_text": "annotations to find_vma*()\"), the call to get_user_pages() will trigger the mmap assert.  static inline void mmap_assert_locked(struct mm_struct *mm) { \tlockdep_assert_held(&mm->mmap_lock); \tVM_BUG_ON_MM(!rwsem_is_locked(&mm->mmap_lock), mm); }  [   62.521410] kernel BUG at include/linux/mmap_lock.h:156! ........................................................... [   62.538938] RIP: 0010:find_vma+0x32/0x80 ........................................................... [   62.605889] Call Trace: [   62.608502]  <TASK> [   62.610956]  ? lock_timer_base+0x61/0x80 [   62.614106]  find_extend_vma+0x19/0x80 [   62.617195]  __get_user_pages+0x9b/0x6a0 [   62.620356]  __gup_longterm_locked+0x42d/0x450 [   62.623721]  ? finish_wait+0x41/0x80 [   62.626748]  ? __kmalloc+0x178/0x2f0 [   62.629768]  ne_set_user_memory_region_ioctl.isra.0+0x225/0x6a0 [nitro_enclaves] [   62.635776]  ne_enclave_ioctl+0x1cf/0x6d7 [nitro_enclaves] [   62.639541]  __x64_sys_ioctl+0x82/0xb0 [   62.642620]  do_syscall_64+0x3b/0x90 [   62.645642]  entry_SYSCALL_64_after_hwframe+0x44/0xae  Use get_user_pages_unlocked() when setting the enclave memory regions. That's a similar pattern as mmap_read_lock() used together with get_user_pages().  Cc: stable@vger.kernel.org Link: https://lore.kernel.org/r/20211220195856.6549-1-andraprs@amazon.com ",
        "func_before": "static int ne_set_user_memory_region_ioctl(struct ne_enclave *ne_enclave,\n\t\t\t\t\t   struct ne_user_memory_region mem_region)\n{\n\tlong gup_rc = 0;\n\tunsigned long i = 0;\n\tunsigned long max_nr_pages = 0;\n\tunsigned long memory_size = 0;\n\tstruct ne_mem_region *ne_mem_region = NULL;\n\tunsigned long nr_phys_contig_mem_regions = 0;\n\tstruct pci_dev *pdev = ne_devs.ne_pci_dev->pdev;\n\tstruct page **phys_contig_mem_regions = NULL;\n\tint rc = -EINVAL;\n\n\trc = ne_sanity_check_user_mem_region(ne_enclave, mem_region);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tne_mem_region = kzalloc(sizeof(*ne_mem_region), GFP_KERNEL);\n\tif (!ne_mem_region)\n\t\treturn -ENOMEM;\n\n\tmax_nr_pages = mem_region.memory_size / NE_MIN_MEM_REGION_SIZE;\n\n\tne_mem_region->pages = kcalloc(max_nr_pages, sizeof(*ne_mem_region->pages),\n\t\t\t\t       GFP_KERNEL);\n\tif (!ne_mem_region->pages) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tphys_contig_mem_regions = kcalloc(max_nr_pages, sizeof(*phys_contig_mem_regions),\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!phys_contig_mem_regions) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tdo {\n\t\ti = ne_mem_region->nr_pages;\n\n\t\tif (i == max_nr_pages) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Reached max nr of pages in the pages data struct\\n\");\n\n\t\t\trc = -ENOMEM;\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\tgup_rc = get_user_pages(mem_region.userspace_addr + memory_size, 1, FOLL_GET,\n\t\t\t\t\tne_mem_region->pages + i, NULL);\n\t\tif (gup_rc < 0) {\n\t\t\trc = gup_rc;\n\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in get user pages [rc=%d]\\n\", rc);\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\trc = ne_sanity_check_user_mem_region_page(ne_enclave, ne_mem_region->pages[i]);\n\t\tif (rc < 0)\n\t\t\tgoto put_pages;\n\n\t\t/*\n\t\t * TODO: Update once handled non-contiguous memory regions\n\t\t * received from user space or contiguous physical memory regions\n\t\t * larger than 2 MiB e.g. 8 MiB.\n\t\t */\n\t\tphys_contig_mem_regions[i] = ne_mem_region->pages[i];\n\n\t\tmemory_size += page_size(ne_mem_region->pages[i]);\n\n\t\tne_mem_region->nr_pages++;\n\t} while (memory_size < mem_region.memory_size);\n\n\t/*\n\t * TODO: Update once handled non-contiguous memory regions received\n\t * from user space or contiguous physical memory regions larger than\n\t * 2 MiB e.g. 8 MiB.\n\t */\n\tnr_phys_contig_mem_regions = ne_mem_region->nr_pages;\n\n\tif ((ne_enclave->nr_mem_regions + nr_phys_contig_mem_regions) >\n\t    ne_enclave->max_mem_regions) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Reached max memory regions %lld\\n\",\n\t\t\t\t    ne_enclave->max_mem_regions);\n\n\t\trc = -NE_ERR_MEM_MAX_REGIONS;\n\n\t\tgoto put_pages;\n\t}\n\n\tfor (i = 0; i < nr_phys_contig_mem_regions; i++) {\n\t\tu64 phys_region_addr = page_to_phys(phys_contig_mem_regions[i]);\n\t\tu64 phys_region_size = page_size(phys_contig_mem_regions[i]);\n\n\t\tif (phys_region_size & (NE_MIN_MEM_REGION_SIZE - 1)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Physical mem region size is not multiple of 2 MiB\\n\");\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\tif (!IS_ALIGNED(phys_region_addr, NE_MIN_MEM_REGION_SIZE)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Physical mem region address is not 2 MiB aligned\\n\");\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto put_pages;\n\t\t}\n\t}\n\n\tne_mem_region->memory_size = mem_region.memory_size;\n\tne_mem_region->userspace_addr = mem_region.userspace_addr;\n\n\tlist_add(&ne_mem_region->mem_region_list_entry, &ne_enclave->mem_regions_list);\n\n\tfor (i = 0; i < nr_phys_contig_mem_regions; i++) {\n\t\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\t\tstruct slot_add_mem_req slot_add_mem_req = {};\n\n\t\tslot_add_mem_req.slot_uid = ne_enclave->slot_uid;\n\t\tslot_add_mem_req.paddr = page_to_phys(phys_contig_mem_regions[i]);\n\t\tslot_add_mem_req.size = page_size(phys_contig_mem_regions[i]);\n\n\t\trc = ne_do_request(pdev, SLOT_ADD_MEM,\n\t\t\t\t   &slot_add_mem_req, sizeof(slot_add_mem_req),\n\t\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\t\tif (rc < 0) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in slot add mem [rc=%d]\\n\", rc);\n\n\t\t\tkfree(phys_contig_mem_regions);\n\n\t\t\t/*\n\t\t\t * Exit here without put pages as memory regions may\n\t\t\t * already been added.\n\t\t\t */\n\t\t\treturn rc;\n\t\t}\n\n\t\tne_enclave->mem_size += slot_add_mem_req.size;\n\t\tne_enclave->nr_mem_regions++;\n\t}\n\n\tkfree(phys_contig_mem_regions);\n\n\treturn 0;\n\nput_pages:\n\tfor (i = 0; i < ne_mem_region->nr_pages; i++)\n\t\tput_page(ne_mem_region->pages[i]);\nfree_mem_region:\n\tkfree(phys_contig_mem_regions);\n\tkfree(ne_mem_region->pages);\n\tkfree(ne_mem_region);\n\n\treturn rc;\n}",
        "func": "static int ne_set_user_memory_region_ioctl(struct ne_enclave *ne_enclave,\n\t\t\t\t\t   struct ne_user_memory_region mem_region)\n{\n\tlong gup_rc = 0;\n\tunsigned long i = 0;\n\tunsigned long max_nr_pages = 0;\n\tunsigned long memory_size = 0;\n\tstruct ne_mem_region *ne_mem_region = NULL;\n\tunsigned long nr_phys_contig_mem_regions = 0;\n\tstruct pci_dev *pdev = ne_devs.ne_pci_dev->pdev;\n\tstruct page **phys_contig_mem_regions = NULL;\n\tint rc = -EINVAL;\n\n\trc = ne_sanity_check_user_mem_region(ne_enclave, mem_region);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tne_mem_region = kzalloc(sizeof(*ne_mem_region), GFP_KERNEL);\n\tif (!ne_mem_region)\n\t\treturn -ENOMEM;\n\n\tmax_nr_pages = mem_region.memory_size / NE_MIN_MEM_REGION_SIZE;\n\n\tne_mem_region->pages = kcalloc(max_nr_pages, sizeof(*ne_mem_region->pages),\n\t\t\t\t       GFP_KERNEL);\n\tif (!ne_mem_region->pages) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tphys_contig_mem_regions = kcalloc(max_nr_pages, sizeof(*phys_contig_mem_regions),\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!phys_contig_mem_regions) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tdo {\n\t\ti = ne_mem_region->nr_pages;\n\n\t\tif (i == max_nr_pages) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Reached max nr of pages in the pages data struct\\n\");\n\n\t\t\trc = -ENOMEM;\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\tgup_rc = get_user_pages_unlocked(mem_region.userspace_addr + memory_size, 1,\n\t\t\t\t\t\t ne_mem_region->pages + i, FOLL_GET);\n\n\t\tif (gup_rc < 0) {\n\t\t\trc = gup_rc;\n\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in get user pages [rc=%d]\\n\", rc);\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\trc = ne_sanity_check_user_mem_region_page(ne_enclave, ne_mem_region->pages[i]);\n\t\tif (rc < 0)\n\t\t\tgoto put_pages;\n\n\t\t/*\n\t\t * TODO: Update once handled non-contiguous memory regions\n\t\t * received from user space or contiguous physical memory regions\n\t\t * larger than 2 MiB e.g. 8 MiB.\n\t\t */\n\t\tphys_contig_mem_regions[i] = ne_mem_region->pages[i];\n\n\t\tmemory_size += page_size(ne_mem_region->pages[i]);\n\n\t\tne_mem_region->nr_pages++;\n\t} while (memory_size < mem_region.memory_size);\n\n\t/*\n\t * TODO: Update once handled non-contiguous memory regions received\n\t * from user space or contiguous physical memory regions larger than\n\t * 2 MiB e.g. 8 MiB.\n\t */\n\tnr_phys_contig_mem_regions = ne_mem_region->nr_pages;\n\n\tif ((ne_enclave->nr_mem_regions + nr_phys_contig_mem_regions) >\n\t    ne_enclave->max_mem_regions) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Reached max memory regions %lld\\n\",\n\t\t\t\t    ne_enclave->max_mem_regions);\n\n\t\trc = -NE_ERR_MEM_MAX_REGIONS;\n\n\t\tgoto put_pages;\n\t}\n\n\tfor (i = 0; i < nr_phys_contig_mem_regions; i++) {\n\t\tu64 phys_region_addr = page_to_phys(phys_contig_mem_regions[i]);\n\t\tu64 phys_region_size = page_size(phys_contig_mem_regions[i]);\n\n\t\tif (phys_region_size & (NE_MIN_MEM_REGION_SIZE - 1)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Physical mem region size is not multiple of 2 MiB\\n\");\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\tif (!IS_ALIGNED(phys_region_addr, NE_MIN_MEM_REGION_SIZE)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Physical mem region address is not 2 MiB aligned\\n\");\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto put_pages;\n\t\t}\n\t}\n\n\tne_mem_region->memory_size = mem_region.memory_size;\n\tne_mem_region->userspace_addr = mem_region.userspace_addr;\n\n\tlist_add(&ne_mem_region->mem_region_list_entry, &ne_enclave->mem_regions_list);\n\n\tfor (i = 0; i < nr_phys_contig_mem_regions; i++) {\n\t\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\t\tstruct slot_add_mem_req slot_add_mem_req = {};\n\n\t\tslot_add_mem_req.slot_uid = ne_enclave->slot_uid;\n\t\tslot_add_mem_req.paddr = page_to_phys(phys_contig_mem_regions[i]);\n\t\tslot_add_mem_req.size = page_size(phys_contig_mem_regions[i]);\n\n\t\trc = ne_do_request(pdev, SLOT_ADD_MEM,\n\t\t\t\t   &slot_add_mem_req, sizeof(slot_add_mem_req),\n\t\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\t\tif (rc < 0) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in slot add mem [rc=%d]\\n\", rc);\n\n\t\t\tkfree(phys_contig_mem_regions);\n\n\t\t\t/*\n\t\t\t * Exit here without put pages as memory regions may\n\t\t\t * already been added.\n\t\t\t */\n\t\t\treturn rc;\n\t\t}\n\n\t\tne_enclave->mem_size += slot_add_mem_req.size;\n\t\tne_enclave->nr_mem_regions++;\n\t}\n\n\tkfree(phys_contig_mem_regions);\n\n\treturn 0;\n\nput_pages:\n\tfor (i = 0; i < ne_mem_region->nr_pages; i++)\n\t\tput_page(ne_mem_region->pages[i]);\nfree_mem_region:\n\tkfree(phys_contig_mem_regions);\n\tkfree(ne_mem_region->pages);\n\tkfree(ne_mem_region);\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -49,8 +49,9 @@\n \t\t\tgoto put_pages;\n \t\t}\n \n-\t\tgup_rc = get_user_pages(mem_region.userspace_addr + memory_size, 1, FOLL_GET,\n-\t\t\t\t\tne_mem_region->pages + i, NULL);\n+\t\tgup_rc = get_user_pages_unlocked(mem_region.userspace_addr + memory_size, 1,\n+\t\t\t\t\t\t ne_mem_region->pages + i, FOLL_GET);\n+\n \t\tif (gup_rc < 0) {\n \t\t\trc = gup_rc;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tgup_rc = get_user_pages(mem_region.userspace_addr + memory_size, 1, FOLL_GET,",
                "\t\t\t\t\tne_mem_region->pages + i, NULL);"
            ],
            "added_lines": [
                "\t\tgup_rc = get_user_pages_unlocked(mem_region.userspace_addr + memory_size, 1,",
                "\t\t\t\t\t\t ne_mem_region->pages + i, FOLL_GET);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29660",
        "func_name": "torvalds/linux/__do_SAK",
        "description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9",
        "commit_title": "Currently, locking of ->session is very inconsistent; most places",
        "commit_text": "protect it using the legacy tty mutex, but disassociate_ctty(), __do_SAK(), tiocspgrp() and tiocgsid() don't. Two of the writers hold the ctrl_lock (because they already need it for ->pgrp), but __proc_set_tty() doesn't do that yet.  On a PREEMPT=y system, an unprivileged user can theoretically abuse this broken locking to read 4 bytes of freed memory via TIOCGSID if tiocgsid() is preempted long enough at the right point. (Other things might also go wrong, especially if root-only ioctls are involved; I'm not sure about that.)  Change the locking on ->session such that:   - tty_lock() is held by all writers: By making disassociate_ctty()    hold it. This should be fine because the same lock can already be    taken through the call to tty_vhangup_session().    The tricky part is that we need to shorten the area covered by    siglock to be able to take tty_lock() without ugly retry logic; as    far as I can tell, this should be fine, since nothing in the    signal_struct is touched in the `if (tty)` branch.  - ctrl_lock is held by all writers: By changing __proc_set_tty() to    hold the lock a little longer.  - All readers that aren't holding tty_lock() hold ctrl_lock: By    adding locking to tiocgsid() and __do_SAK(), and expanding the area    covered by ctrl_lock in tiocspgrp().  Cc: stable@kernel.org ",
        "func_before": "void __do_SAK(struct tty_struct *tty)\n{\n#ifdef TTY_SOFT_SAK\n\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\n\tif (!tty)\n\t\treturn;\n\tsession = tty->session;\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\tread_lock(&tasklist_lock);\n\t/* Kill the entire session */\n\tdo_each_pid_task(session, PIDTYPE_SID, p) {\n\t\ttty_notice(tty, \"SAK: killed process %d (%s): by session\\n\",\n\t\t\t   task_pid_nr(p), p->comm);\n\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t} while_each_pid_task(session, PIDTYPE_SID, p);\n\n\t/* Now kill any processes that happen to have the tty open */\n\tdo_each_thread(g, p) {\n\t\tif (p->signal->tty == tty) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by controlling tty\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_lock(p);\n\t\ti = iterate_fd(p->files, 0, this_tty, tty);\n\t\tif (i != 0) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by fd#%d\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm, i - 1);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n#endif\n}",
        "func": "void __do_SAK(struct tty_struct *tty)\n{\n#ifdef TTY_SOFT_SAK\n\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\tunsigned long flags;\n\n\tif (!tty)\n\t\treturn;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\tsession = get_pid(tty->session);\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\tread_lock(&tasklist_lock);\n\t/* Kill the entire session */\n\tdo_each_pid_task(session, PIDTYPE_SID, p) {\n\t\ttty_notice(tty, \"SAK: killed process %d (%s): by session\\n\",\n\t\t\t   task_pid_nr(p), p->comm);\n\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t} while_each_pid_task(session, PIDTYPE_SID, p);\n\n\t/* Now kill any processes that happen to have the tty open */\n\tdo_each_thread(g, p) {\n\t\tif (p->signal->tty == tty) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by controlling tty\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t\tcontinue;\n\t\t}\n\t\ttask_lock(p);\n\t\ti = iterate_fd(p->files, 0, this_tty, tty);\n\t\tif (i != 0) {\n\t\t\ttty_notice(tty, \"SAK: killed process %d (%s): by fd#%d\\n\",\n\t\t\t\t   task_pid_nr(p), p->comm, i - 1);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n\tput_pid(session);\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,10 +6,14 @@\n \tstruct task_struct *g, *p;\n \tstruct pid *session;\n \tint\t\ti;\n+\tunsigned long flags;\n \n \tif (!tty)\n \t\treturn;\n-\tsession = tty->session;\n+\n+\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n+\tsession = get_pid(tty->session);\n+\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n \n \ttty_ldisc_flush(tty);\n \n@@ -41,5 +45,6 @@\n \t\ttask_unlock(p);\n \t} while_each_thread(g, p);\n \tread_unlock(&tasklist_lock);\n+\tput_pid(session);\n #endif\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tsession = tty->session;"
            ],
            "added_lines": [
                "\tunsigned long flags;",
                "",
                "\tspin_lock_irqsave(&tty->ctrl_lock, flags);",
                "\tsession = get_pid(tty->session);",
                "\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);",
                "\tput_pid(session);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29660",
        "func_name": "torvalds/linux/tiocspgrp",
        "description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9",
        "commit_title": "Currently, locking of ->session is very inconsistent; most places",
        "commit_text": "protect it using the legacy tty mutex, but disassociate_ctty(), __do_SAK(), tiocspgrp() and tiocgsid() don't. Two of the writers hold the ctrl_lock (because they already need it for ->pgrp), but __proc_set_tty() doesn't do that yet.  On a PREEMPT=y system, an unprivileged user can theoretically abuse this broken locking to read 4 bytes of freed memory via TIOCGSID if tiocgsid() is preempted long enough at the right point. (Other things might also go wrong, especially if root-only ioctls are involved; I'm not sure about that.)  Change the locking on ->session such that:   - tty_lock() is held by all writers: By making disassociate_ctty()    hold it. This should be fine because the same lock can already be    taken through the call to tty_vhangup_session().    The tricky part is that we need to shorten the area covered by    siglock to be able to take tty_lock() without ugly retry logic; as    far as I can tell, this should be fine, since nothing in the    signal_struct is touched in the `if (tty)` branch.  - ctrl_lock is held by all writers: By changing __proc_set_tty() to    hold the lock a little longer.  - All readers that aren't holding tty_lock() hold ctrl_lock: By    adding locking to tiocgsid() and __do_SAK(), and expanding the area    covered by ctrl_lock in tiocspgrp().  Cc: stable@kernel.org ",
        "func_before": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current)))\n\t\treturn -ENOTTY;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\n\tspin_unlock_irq(&real_tty->ctrl_lock);\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}",
        "func": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current))) {\n\t\tretval = -ENOTTY;\n\t\tgoto out_unlock_ctrl;\n\t}\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\nout_unlock:\n\trcu_read_unlock();\nout_unlock_ctrl:\n\tspin_unlock_irq(&real_tty->ctrl_lock);\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,14 +8,19 @@\n \t\treturn -ENOTTY;\n \tif (retval)\n \t\treturn retval;\n-\tif (!current->signal->tty ||\n-\t    (current->signal->tty != real_tty) ||\n-\t    (real_tty->session != task_session(current)))\n-\t\treturn -ENOTTY;\n+\n \tif (get_user(pgrp_nr, p))\n \t\treturn -EFAULT;\n \tif (pgrp_nr < 0)\n \t\treturn -EINVAL;\n+\n+\tspin_lock_irq(&real_tty->ctrl_lock);\n+\tif (!current->signal->tty ||\n+\t    (current->signal->tty != real_tty) ||\n+\t    (real_tty->session != task_session(current))) {\n+\t\tretval = -ENOTTY;\n+\t\tgoto out_unlock_ctrl;\n+\t}\n \trcu_read_lock();\n \tpgrp = find_vpid(pgrp_nr);\n \tretval = -ESRCH;\n@@ -25,11 +30,11 @@\n \tif (session_of_pgrp(pgrp) != task_session(current))\n \t\tgoto out_unlock;\n \tretval = 0;\n-\tspin_lock_irq(&real_tty->ctrl_lock);\n \tput_pid(real_tty->pgrp);\n \treal_tty->pgrp = get_pid(pgrp);\n-\tspin_unlock_irq(&real_tty->ctrl_lock);\n out_unlock:\n \trcu_read_unlock();\n+out_unlock_ctrl:\n+\tspin_unlock_irq(&real_tty->ctrl_lock);\n \treturn retval;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (!current->signal->tty ||",
                "\t    (current->signal->tty != real_tty) ||",
                "\t    (real_tty->session != task_session(current)))",
                "\t\treturn -ENOTTY;",
                "\tspin_lock_irq(&real_tty->ctrl_lock);",
                "\tspin_unlock_irq(&real_tty->ctrl_lock);"
            ],
            "added_lines": [
                "",
                "",
                "\tspin_lock_irq(&real_tty->ctrl_lock);",
                "\tif (!current->signal->tty ||",
                "\t    (current->signal->tty != real_tty) ||",
                "\t    (real_tty->session != task_session(current))) {",
                "\t\tretval = -ENOTTY;",
                "\t\tgoto out_unlock_ctrl;",
                "\t}",
                "out_unlock_ctrl:",
                "\tspin_unlock_irq(&real_tty->ctrl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29660",
        "func_name": "torvalds/linux/tiocgsid",
        "description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9",
        "commit_title": "Currently, locking of ->session is very inconsistent; most places",
        "commit_text": "protect it using the legacy tty mutex, but disassociate_ctty(), __do_SAK(), tiocspgrp() and tiocgsid() don't. Two of the writers hold the ctrl_lock (because they already need it for ->pgrp), but __proc_set_tty() doesn't do that yet.  On a PREEMPT=y system, an unprivileged user can theoretically abuse this broken locking to read 4 bytes of freed memory via TIOCGSID if tiocgsid() is preempted long enough at the right point. (Other things might also go wrong, especially if root-only ioctls are involved; I'm not sure about that.)  Change the locking on ->session such that:   - tty_lock() is held by all writers: By making disassociate_ctty()    hold it. This should be fine because the same lock can already be    taken through the call to tty_vhangup_session().    The tricky part is that we need to shorten the area covered by    siglock to be able to take tty_lock() without ugly retry logic; as    far as I can tell, this should be fine, since nothing in the    signal_struct is touched in the `if (tty)` branch.  - ctrl_lock is held by all writers: By changing __proc_set_tty() to    hold the lock a little longer.  - All readers that aren't holding tty_lock() hold ctrl_lock: By    adding locking to tiocgsid() and __do_SAK(), and expanding the area    covered by ctrl_lock in tiocspgrp().  Cc: stable@kernel.org ",
        "func_before": "static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\t/*\n\t * (tty == real_tty) is a cheap way of\n\t * testing if the tty is NOT a master pty.\n\t*/\n\tif (tty == real_tty && current->signal->tty != real_tty)\n\t\treturn -ENOTTY;\n\tif (!real_tty->session)\n\t\treturn -ENOTTY;\n\treturn put_user(pid_vnr(real_tty->session), p);\n}",
        "func": "static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tunsigned long flags;\n\tpid_t sid;\n\n\t/*\n\t * (tty == real_tty) is a cheap way of\n\t * testing if the tty is NOT a master pty.\n\t*/\n\tif (tty == real_tty && current->signal->tty != real_tty)\n\t\treturn -ENOTTY;\n\n\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);\n\tif (!real_tty->session)\n\t\tgoto err;\n\tsid = pid_vnr(real_tty->session);\n\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n\n\treturn put_user(sid, p);\n\nerr:\n\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n\treturn -ENOTTY;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,12 +1,24 @@\n static int tiocgsid(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n {\n+\tunsigned long flags;\n+\tpid_t sid;\n+\n \t/*\n \t * (tty == real_tty) is a cheap way of\n \t * testing if the tty is NOT a master pty.\n \t*/\n \tif (tty == real_tty && current->signal->tty != real_tty)\n \t\treturn -ENOTTY;\n+\n+\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);\n \tif (!real_tty->session)\n-\t\treturn -ENOTTY;\n-\treturn put_user(pid_vnr(real_tty->session), p);\n+\t\tgoto err;\n+\tsid = pid_vnr(real_tty->session);\n+\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n+\n+\treturn put_user(sid, p);\n+\n+err:\n+\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);\n+\treturn -ENOTTY;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn -ENOTTY;",
                "\treturn put_user(pid_vnr(real_tty->session), p);"
            ],
            "added_lines": [
                "\tunsigned long flags;",
                "\tpid_t sid;",
                "",
                "",
                "\tspin_lock_irqsave(&real_tty->ctrl_lock, flags);",
                "\t\tgoto err;",
                "\tsid = pid_vnr(real_tty->session);",
                "\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);",
                "",
                "\treturn put_user(sid, p);",
                "",
                "err:",
                "\tspin_unlock_irqrestore(&real_tty->ctrl_lock, flags);",
                "\treturn -ENOTTY;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29660",
        "func_name": "torvalds/linux/__proc_set_tty",
        "description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9",
        "commit_title": "Currently, locking of ->session is very inconsistent; most places",
        "commit_text": "protect it using the legacy tty mutex, but disassociate_ctty(), __do_SAK(), tiocspgrp() and tiocgsid() don't. Two of the writers hold the ctrl_lock (because they already need it for ->pgrp), but __proc_set_tty() doesn't do that yet.  On a PREEMPT=y system, an unprivileged user can theoretically abuse this broken locking to read 4 bytes of freed memory via TIOCGSID if tiocgsid() is preempted long enough at the right point. (Other things might also go wrong, especially if root-only ioctls are involved; I'm not sure about that.)  Change the locking on ->session such that:   - tty_lock() is held by all writers: By making disassociate_ctty()    hold it. This should be fine because the same lock can already be    taken through the call to tty_vhangup_session().    The tricky part is that we need to shorten the area covered by    siglock to be able to take tty_lock() without ugly retry logic; as    far as I can tell, this should be fine, since nothing in the    signal_struct is touched in the `if (tty)` branch.  - ctrl_lock is held by all writers: By changing __proc_set_tty() to    hold the lock a little longer.  - All readers that aren't holding tty_lock() hold ctrl_lock: By    adding locking to tiocgsid() and __do_SAK(), and expanding the area    covered by ctrl_lock in tiocspgrp().  Cc: stable@kernel.org ",
        "func_before": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\ttty->session = get_pid(task_session(current));\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
        "func": "static void __proc_set_tty(struct tty_struct *tty)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t/*\n\t * The session and fg pgrp references will be non-NULL if\n\t * tiocsctty() is stealing the controlling tty\n\t */\n\tput_pid(tty->session);\n\tput_pid(tty->pgrp);\n\ttty->pgrp = get_pid(task_pgrp(current));\n\ttty->session = get_pid(task_session(current));\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\tif (current->signal->tty) {\n\t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n\t\t\t  current->signal->tty->name);\n\t\ttty_kref_put(current->signal->tty);\n\t}\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty = tty_kref_get(tty);\n\tcurrent->signal->tty_old_pgrp = NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,8 +10,8 @@\n \tput_pid(tty->session);\n \tput_pid(tty->pgrp);\n \ttty->pgrp = get_pid(task_pgrp(current));\n+\ttty->session = get_pid(task_session(current));\n \tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n-\ttty->session = get_pid(task_session(current));\n \tif (current->signal->tty) {\n \t\ttty_debug(tty, \"current tty %s not NULL!!\\n\",\n \t\t\t  current->signal->tty->name);",
        "diff_line_info": {
            "deleted_lines": [
                "\ttty->session = get_pid(task_session(current));"
            ],
            "added_lines": [
                "\ttty->session = get_pid(task_session(current));"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29660",
        "func_name": "torvalds/linux/disassociate_ctty",
        "description": "A locking inconsistency issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_io.c and drivers/tty/tty_jobctrl.c may allow a read-after-free attack against TIOCGSID, aka CID-c8bcd9c5be24.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9",
        "commit_title": "Currently, locking of ->session is very inconsistent; most places",
        "commit_text": "protect it using the legacy tty mutex, but disassociate_ctty(), __do_SAK(), tiocspgrp() and tiocgsid() don't. Two of the writers hold the ctrl_lock (because they already need it for ->pgrp), but __proc_set_tty() doesn't do that yet.  On a PREEMPT=y system, an unprivileged user can theoretically abuse this broken locking to read 4 bytes of freed memory via TIOCGSID if tiocgsid() is preempted long enough at the right point. (Other things might also go wrong, especially if root-only ioctls are involved; I'm not sure about that.)  Change the locking on ->session such that:   - tty_lock() is held by all writers: By making disassociate_ctty()    hold it. This should be fine because the same lock can already be    taken through the call to tty_vhangup_session().    The tricky part is that we need to shorten the area covered by    siglock to be able to take tty_lock() without ugly retry logic; as    far as I can tell, this should be fine, since nothing in the    signal_struct is touched in the `if (tty)` branch.  - ctrl_lock is held by all writers: By changing __proc_set_tty() to    hold the lock a little longer.  - All readers that aren't holding tty_lock() hold ctrl_lock: By    adding locking to tiocgsid() and __do_SAK(), and expanding the area    covered by ctrl_lock in tiocspgrp().  Cc: stable@kernel.org ",
        "func_before": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\n\ttty = tty_kref_get(current->signal->tty);\n\tif (tty) {\n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_kref_put(tty);\n\t}\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
        "func": "void disassociate_ctty(int on_exit)\n{\n\tstruct tty_struct *tty;\n\n\tif (!current->signal->leader)\n\t\treturn;\n\n\ttty = get_current_tty();\n\tif (tty) {\n\t\tif (on_exit && tty->driver->type != TTY_DRIVER_TYPE_PTY) {\n\t\t\ttty_vhangup_session(tty);\n\t\t} else {\n\t\t\tstruct pid *tty_pgrp = tty_get_pgrp(tty);\n\t\t\tif (tty_pgrp) {\n\t\t\t\tkill_pgrp(tty_pgrp, SIGHUP, on_exit);\n\t\t\t\tif (!on_exit)\n\t\t\t\t\tkill_pgrp(tty_pgrp, SIGCONT, on_exit);\n\t\t\t\tput_pid(tty_pgrp);\n\t\t\t}\n\t\t}\n\t\ttty_kref_put(tty);\n\n\t} else if (on_exit) {\n\t\tstruct pid *old_pgrp;\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\told_pgrp = current->signal->tty_old_pgrp;\n\t\tcurrent->signal->tty_old_pgrp = NULL;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\tif (old_pgrp) {\n\t\t\tkill_pgrp(old_pgrp, SIGHUP, on_exit);\n\t\t\tkill_pgrp(old_pgrp, SIGCONT, on_exit);\n\t\t\tput_pid(old_pgrp);\n\t\t}\n\t\treturn;\n\t}\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tput_pid(current->signal->tty_old_pgrp);\n\tcurrent->signal->tty_old_pgrp = NULL;\n\ttty = tty_kref_get(current->signal->tty);\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tif (tty) {\n\t\tunsigned long flags;\n\n\t\ttty_lock(tty);\n\t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\t\tput_pid(tty->session);\n\t\tput_pid(tty->pgrp);\n\t\ttty->session = NULL;\n\t\ttty->pgrp = NULL;\n\t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\t\ttty_unlock(tty);\n\t\ttty_kref_put(tty);\n\t}\n\n\t/* Now clear signal->tty under the lock */\n\tread_lock(&tasklist_lock);\n\tsession_clear_tty(task_session(current));\n\tread_unlock(&tasklist_lock);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -37,20 +37,23 @@\n \tspin_lock_irq(&current->sighand->siglock);\n \tput_pid(current->signal->tty_old_pgrp);\n \tcurrent->signal->tty_old_pgrp = NULL;\n+\ttty = tty_kref_get(current->signal->tty);\n+\tspin_unlock_irq(&current->sighand->siglock);\n \n-\ttty = tty_kref_get(current->signal->tty);\n \tif (tty) {\n \t\tunsigned long flags;\n+\n+\t\ttty_lock(tty);\n \t\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n \t\tput_pid(tty->session);\n \t\tput_pid(tty->pgrp);\n \t\ttty->session = NULL;\n \t\ttty->pgrp = NULL;\n \t\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n+\t\ttty_unlock(tty);\n \t\ttty_kref_put(tty);\n \t}\n \n-\tspin_unlock_irq(&current->sighand->siglock);\n \t/* Now clear signal->tty under the lock */\n \tread_lock(&tasklist_lock);\n \tsession_clear_tty(task_session(current));",
        "diff_line_info": {
            "deleted_lines": [
                "\ttty = tty_kref_get(current->signal->tty);",
                "\tspin_unlock_irq(&current->sighand->siglock);"
            ],
            "added_lines": [
                "\ttty = tty_kref_get(current->signal->tty);",
                "\tspin_unlock_irq(&current->sighand->siglock);",
                "",
                "\t\ttty_lock(tty);",
                "\t\ttty_unlock(tty);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-29661",
        "func_name": "torvalds/linux/tiocspgrp",
        "description": "A locking issue was discovered in the tty subsystem of the Linux kernel through 5.9.13. drivers/tty/tty_jobctrl.c allows a use-after-free attack against TIOCSPGRP, aka CID-54ffccbf053b.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=54ffccbf053b5b6ca4f6e45094b942fab92a25fc",
        "commit_title": "tiocspgrp() takes two tty_struct pointers: One to the tty that userspace",
        "commit_text": "passed to ioctl() (`tty`) and one to the TTY being changed (`real_tty`). These pointers are different when ioctl() is called with a master fd.  To properly lock real_tty->pgrp, we must take real_tty->ctrl_lock.  This bug makes it possible for racing ioctl(TIOCSPGRP, ...) calls on both sides of a PTY pair to corrupt the refcount of `struct pid`, leading to use-after-free errors.  ",
        "func_before": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current)))\n\t\treturn -ENOTTY;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tspin_lock_irq(&tty->ctrl_lock);\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\n\tspin_unlock_irq(&tty->ctrl_lock);\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}",
        "func": "static int tiocspgrp(struct tty_struct *tty, struct tty_struct *real_tty, pid_t __user *p)\n{\n\tstruct pid *pgrp;\n\tpid_t pgrp_nr;\n\tint retval = tty_check_change(real_tty);\n\n\tif (retval == -EIO)\n\t\treturn -ENOTTY;\n\tif (retval)\n\t\treturn retval;\n\tif (!current->signal->tty ||\n\t    (current->signal->tty != real_tty) ||\n\t    (real_tty->session != task_session(current)))\n\t\treturn -ENOTTY;\n\tif (get_user(pgrp_nr, p))\n\t\treturn -EFAULT;\n\tif (pgrp_nr < 0)\n\t\treturn -EINVAL;\n\trcu_read_lock();\n\tpgrp = find_vpid(pgrp_nr);\n\tretval = -ESRCH;\n\tif (!pgrp)\n\t\tgoto out_unlock;\n\tretval = -EPERM;\n\tif (session_of_pgrp(pgrp) != task_session(current))\n\t\tgoto out_unlock;\n\tretval = 0;\n\tspin_lock_irq(&real_tty->ctrl_lock);\n\tput_pid(real_tty->pgrp);\n\treal_tty->pgrp = get_pid(pgrp);\n\tspin_unlock_irq(&real_tty->ctrl_lock);\nout_unlock:\n\trcu_read_unlock();\n\treturn retval;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,10 +25,10 @@\n \tif (session_of_pgrp(pgrp) != task_session(current))\n \t\tgoto out_unlock;\n \tretval = 0;\n-\tspin_lock_irq(&tty->ctrl_lock);\n+\tspin_lock_irq(&real_tty->ctrl_lock);\n \tput_pid(real_tty->pgrp);\n \treal_tty->pgrp = get_pid(pgrp);\n-\tspin_unlock_irq(&tty->ctrl_lock);\n+\tspin_unlock_irq(&real_tty->ctrl_lock);\n out_unlock:\n \trcu_read_unlock();\n \treturn retval;",
        "diff_line_info": {
            "deleted_lines": [
                "\tspin_lock_irq(&tty->ctrl_lock);",
                "\tspin_unlock_irq(&tty->ctrl_lock);"
            ],
            "added_lines": [
                "\tspin_lock_irq(&real_tty->ctrl_lock);",
                "\tspin_unlock_irq(&real_tty->ctrl_lock);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-12658",
        "func_name": "gssapi/gssproxy/gp_worker_main",
        "description": "gssproxy (aka gss-proxy) before 0.8.3 does not unlock cond_mutex before pthread exit in gp_worker_main() in gp_workers.c. NOTE: An upstream comment states \"We are already on a shutdown path when running the code in question, so a DoS there doesn't make any sense, and there has been no additional information provided us (as upstream) to indicate why this would be a problem.",
        "git_url": "https://github.com/gssapi/gssproxy/commit/cb761412e299ef907f22cd7c4146d50c8a792003",
        "commit_title": "Unlock cond_mutex before pthread exit in gp_worker_main()",
        "commit_text": " [rharwood@redhat.com: whitespace, tweak commit message]",
        "func_before": "static void *gp_worker_main(void *pvt)\n{\n    struct gp_thread *t = (struct gp_thread *)pvt;\n    struct gp_query *q = NULL;\n    char dummy = 0;\n    int ret;\n\n    while (!t->pool->shutdown) {\n\n        /* initialize debug client id to 0 until work is scheduled */\n        gp_debug_set_conn_id(0);\n\n        /* ======> COND_MUTEX */\n        pthread_mutex_lock(&t->cond_mutex);\n        while (t->query == NULL) {\n            /* wait for next query */\n            pthread_cond_wait(&t->cond_wakeup, &t->cond_mutex);\n            if (t->pool->shutdown) {\n                pthread_exit(NULL);\n            }\n        }\n\n        /* grab the query off the shared pointer */\n        q = t->query;\n        t->query = NULL;\n\n        /* <====== COND_MUTEX */\n        pthread_mutex_unlock(&t->cond_mutex);\n\n        /* set client id before hndling requests */\n        gp_debug_set_conn_id(gp_conn_get_cid(q->conn));\n\n        /* handle the client request */\n        GPDEBUGN(3, \"[status] Handling query input: %p (%zu)\\n\", q->buffer,\n                 q->buflen);\n        gp_handle_query(t->pool, q);\n        GPDEBUGN(3 ,\"[status] Handling query output: %p (%zu)\\n\", q->buffer,\n                 q->buflen);\n\n        /* now get lock on main queue, to play with the reply list */\n        /* ======> POOL LOCK */\n        pthread_mutex_lock(&t->pool->lock);\n\n        /* put back query so that dispatcher can send reply */\n        q->next = t->pool->reply_list;\n        t->pool->reply_list = q;\n\n        /* add us back to the free list but only if we are not\n         * shutting down */\n        if (!t->pool->shutdown) {\n            LIST_DEL(t->pool->busy_list, t);\n            LIST_ADD(t->pool->free_list, t);\n        }\n\n        /* <====== POOL LOCK */\n        pthread_mutex_unlock(&t->pool->lock);\n\n        /* and wake up dispatcher so it will handle it */\n        ret = write(t->pool->sig_pipe[1], &dummy, 1);\n        if (ret == -1) {\n            GPERROR(\"Failed to signal dispatcher!\");\n        }\n    }\n\n    pthread_exit(NULL);\n}",
        "func": "static void *gp_worker_main(void *pvt)\n{\n    struct gp_thread *t = (struct gp_thread *)pvt;\n    struct gp_query *q = NULL;\n    char dummy = 0;\n    int ret;\n\n    while (!t->pool->shutdown) {\n\n        /* initialize debug client id to 0 until work is scheduled */\n        gp_debug_set_conn_id(0);\n\n        /* ======> COND_MUTEX */\n        pthread_mutex_lock(&t->cond_mutex);\n        while (t->query == NULL) {\n            /* wait for next query */\n            pthread_cond_wait(&t->cond_wakeup, &t->cond_mutex);\n            if (t->pool->shutdown) {\n                pthread_mutex_unlock(&t->cond_mutex);\n                pthread_exit(NULL);\n            }\n        }\n\n        /* grab the query off the shared pointer */\n        q = t->query;\n        t->query = NULL;\n\n        /* <====== COND_MUTEX */\n        pthread_mutex_unlock(&t->cond_mutex);\n\n        /* set client id before hndling requests */\n        gp_debug_set_conn_id(gp_conn_get_cid(q->conn));\n\n        /* handle the client request */\n        GPDEBUGN(3, \"[status] Handling query input: %p (%zu)\\n\", q->buffer,\n                 q->buflen);\n        gp_handle_query(t->pool, q);\n        GPDEBUGN(3 ,\"[status] Handling query output: %p (%zu)\\n\", q->buffer,\n                 q->buflen);\n\n        /* now get lock on main queue, to play with the reply list */\n        /* ======> POOL LOCK */\n        pthread_mutex_lock(&t->pool->lock);\n\n        /* put back query so that dispatcher can send reply */\n        q->next = t->pool->reply_list;\n        t->pool->reply_list = q;\n\n        /* add us back to the free list but only if we are not\n         * shutting down */\n        if (!t->pool->shutdown) {\n            LIST_DEL(t->pool->busy_list, t);\n            LIST_ADD(t->pool->free_list, t);\n        }\n\n        /* <====== POOL LOCK */\n        pthread_mutex_unlock(&t->pool->lock);\n\n        /* and wake up dispatcher so it will handle it */\n        ret = write(t->pool->sig_pipe[1], &dummy, 1);\n        if (ret == -1) {\n            GPERROR(\"Failed to signal dispatcher!\");\n        }\n    }\n\n    pthread_exit(NULL);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,6 +16,7 @@\n             /* wait for next query */\n             pthread_cond_wait(&t->cond_wakeup, &t->cond_mutex);\n             if (t->pool->shutdown) {\n+                pthread_mutex_unlock(&t->cond_mutex);\n                 pthread_exit(NULL);\n             }\n         }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "                pthread_mutex_unlock(&t->cond_mutex);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26708",
        "func_name": "torvalds/linux/vsock_stream_sendmsg",
        "description": "A local privilege escalation was discovered in the Linux kernel before 5.10.13. Multiple race conditions in the AF_VSOCK implementation are caused by wrong locking in net/vmw_vsock/af_vsock.c. The race conditions were implicitly introduced in the commits that added VSOCK multi-transport support.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c518adafa39f37858697ac9309c6cf1805581446",
        "commit_title": "There are multiple similar bugs implicitly introduced by the",
        "commit_text": "commit c0cfa2d8a788fcf4 (\"vsock: add multi-transports support\") and commit 6a2c0962105ae8ce (\"vsock: prevent transport modules unloading\").  The bug pattern:  [1] vsock_sock.transport pointer is copied to a local variable,  [2] lock_sock() is called,  [3] the local variable is used. VSOCK multi-transport support introduced the race condition: vsock_sock.transport value may change between [1] and [2].  Let's copy vsock_sock.transport pointer to local variables after the lock_sock() call.  Link: https://lore.kernel.org/r/20210201084719.2257066-1-alex.popov@linux.com ",
        "func_before": "static int vsock_stream_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\tsize_t len)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tssize_t total_written;\n\tlong timeout;\n\tint err;\n\tstruct vsock_transport_send_notify_data send_data;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\ttransport = vsk->transport;\n\ttotal_written = 0;\n\terr = 0;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tlock_sock(sk);\n\n\t/* Callers should not provide a destination with stream sockets. */\n\tif (msg->msg_namelen) {\n\t\terr = sk->sk_state == TCP_ESTABLISHED ? -EISCONN : -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* Send data only if both sides are not shutdown in the direction. */\n\tif (sk->sk_shutdown & SEND_SHUTDOWN ||\n\t    vsk->peer_shutdown & RCV_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tgoto out;\n\t}\n\n\tif (!transport || sk->sk_state != TCP_ESTABLISHED ||\n\t    !vsock_addr_bound(&vsk->local_addr)) {\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tif (!vsock_addr_bound(&vsk->remote_addr)) {\n\t\terr = -EDESTADDRREQ;\n\t\tgoto out;\n\t}\n\n\t/* Wait for room in the produce queue to enqueue our user's data. */\n\ttimeout = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\n\terr = transport->notify_send_init(vsk, &send_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\twhile (total_written < len) {\n\t\tssize_t written;\n\n\t\tadd_wait_queue(sk_sleep(sk), &wait);\n\t\twhile (vsock_stream_has_space(vsk) == 0 &&\n\t\t       sk->sk_err == 0 &&\n\t\t       !(sk->sk_shutdown & SEND_SHUTDOWN) &&\n\t\t       !(vsk->peer_shutdown & RCV_SHUTDOWN)) {\n\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\terr = transport->notify_send_pre_block(vsk, &send_data);\n\t\t\tif (err < 0) {\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = wait_woken(&wait, TASK_INTERRUPTIBLE, timeout);\n\t\t\tlock_sock(sk);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\n\t\t/* These checks occur both as part of and after the loop\n\t\t * conditional since we need to check before and after\n\t\t * sleeping.\n\t\t */\n\t\tif (sk->sk_err) {\n\t\t\terr = -sk->sk_err;\n\t\t\tgoto out_err;\n\t\t} else if ((sk->sk_shutdown & SEND_SHUTDOWN) ||\n\t\t\t   (vsk->peer_shutdown & RCV_SHUTDOWN)) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\terr = transport->notify_send_pre_enqueue(vsk, &send_data);\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\n\t\t/* Note that enqueue will only write as many bytes as are free\n\t\t * in the produce queue, so we don't need to ensure len is\n\t\t * smaller than the queue size.  It is the caller's\n\t\t * responsibility to check how many bytes we were able to send.\n\t\t */\n\n\t\twritten = transport->stream_enqueue(\n\t\t\t\tvsk, msg,\n\t\t\t\tlen - total_written);\n\t\tif (written < 0) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\ttotal_written += written;\n\n\t\terr = transport->notify_send_post_enqueue(\n\t\t\t\tvsk, written, &send_data);\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\n\t}\n\nout_err:\n\tif (total_written > 0)\n\t\terr = total_written;\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "func": "static int vsock_stream_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\tsize_t len)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tssize_t total_written;\n\tlong timeout;\n\tint err;\n\tstruct vsock_transport_send_notify_data send_data;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\ttotal_written = 0;\n\terr = 0;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tlock_sock(sk);\n\n\ttransport = vsk->transport;\n\n\t/* Callers should not provide a destination with stream sockets. */\n\tif (msg->msg_namelen) {\n\t\terr = sk->sk_state == TCP_ESTABLISHED ? -EISCONN : -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* Send data only if both sides are not shutdown in the direction. */\n\tif (sk->sk_shutdown & SEND_SHUTDOWN ||\n\t    vsk->peer_shutdown & RCV_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tgoto out;\n\t}\n\n\tif (!transport || sk->sk_state != TCP_ESTABLISHED ||\n\t    !vsock_addr_bound(&vsk->local_addr)) {\n\t\terr = -ENOTCONN;\n\t\tgoto out;\n\t}\n\n\tif (!vsock_addr_bound(&vsk->remote_addr)) {\n\t\terr = -EDESTADDRREQ;\n\t\tgoto out;\n\t}\n\n\t/* Wait for room in the produce queue to enqueue our user's data. */\n\ttimeout = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\n\terr = transport->notify_send_init(vsk, &send_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\twhile (total_written < len) {\n\t\tssize_t written;\n\n\t\tadd_wait_queue(sk_sleep(sk), &wait);\n\t\twhile (vsock_stream_has_space(vsk) == 0 &&\n\t\t       sk->sk_err == 0 &&\n\t\t       !(sk->sk_shutdown & SEND_SHUTDOWN) &&\n\t\t       !(vsk->peer_shutdown & RCV_SHUTDOWN)) {\n\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\terr = transport->notify_send_pre_block(vsk, &send_data);\n\t\t\tif (err < 0) {\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = wait_woken(&wait, TASK_INTERRUPTIBLE, timeout);\n\t\t\tlock_sock(sk);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\t\tremove_wait_queue(sk_sleep(sk), &wait);\n\n\t\t/* These checks occur both as part of and after the loop\n\t\t * conditional since we need to check before and after\n\t\t * sleeping.\n\t\t */\n\t\tif (sk->sk_err) {\n\t\t\terr = -sk->sk_err;\n\t\t\tgoto out_err;\n\t\t} else if ((sk->sk_shutdown & SEND_SHUTDOWN) ||\n\t\t\t   (vsk->peer_shutdown & RCV_SHUTDOWN)) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\terr = transport->notify_send_pre_enqueue(vsk, &send_data);\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\n\t\t/* Note that enqueue will only write as many bytes as are free\n\t\t * in the produce queue, so we don't need to ensure len is\n\t\t * smaller than the queue size.  It is the caller's\n\t\t * responsibility to check how many bytes we were able to send.\n\t\t */\n\n\t\twritten = transport->stream_enqueue(\n\t\t\t\tvsk, msg,\n\t\t\t\tlen - total_written);\n\t\tif (written < 0) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\ttotal_written += written;\n\n\t\terr = transport->notify_send_post_enqueue(\n\t\t\t\tvsk, written, &send_data);\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\n\t}\n\nout_err:\n\tif (total_written > 0)\n\t\terr = total_written;\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,7 +12,6 @@\n \n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n-\ttransport = vsk->transport;\n \ttotal_written = 0;\n \terr = 0;\n \n@@ -20,6 +19,8 @@\n \t\treturn -EOPNOTSUPP;\n \n \tlock_sock(sk);\n+\n+\ttransport = vsk->transport;\n \n \t/* Callers should not provide a destination with stream sockets. */\n \tif (msg->msg_namelen) {",
        "diff_line_info": {
            "deleted_lines": [
                "\ttransport = vsk->transport;"
            ],
            "added_lines": [
                "",
                "\ttransport = vsk->transport;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26708",
        "func_name": "torvalds/linux/vsock_stream_setsockopt",
        "description": "A local privilege escalation was discovered in the Linux kernel before 5.10.13. Multiple race conditions in the AF_VSOCK implementation are caused by wrong locking in net/vmw_vsock/af_vsock.c. The race conditions were implicitly introduced in the commits that added VSOCK multi-transport support.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c518adafa39f37858697ac9309c6cf1805581446",
        "commit_title": "There are multiple similar bugs implicitly introduced by the",
        "commit_text": "commit c0cfa2d8a788fcf4 (\"vsock: add multi-transports support\") and commit 6a2c0962105ae8ce (\"vsock: prevent transport modules unloading\").  The bug pattern:  [1] vsock_sock.transport pointer is copied to a local variable,  [2] lock_sock() is called,  [3] the local variable is used. VSOCK multi-transport support introduced the race condition: vsock_sock.transport value may change between [1] and [2].  Let's copy vsock_sock.transport pointer to local variables after the lock_sock() call.  Link: https://lore.kernel.org/r/20210201084719.2257066-1-alex.popov@linux.com ",
        "func_before": "static int vsock_stream_setsockopt(struct socket *sock,\n\t\t\t\t   int level,\n\t\t\t\t   int optname,\n\t\t\t\t   sockptr_t optval,\n\t\t\t\t   unsigned int optlen)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tu64 val;\n\n\tif (level != AF_VSOCK)\n\t\treturn -ENOPROTOOPT;\n\n#define COPY_IN(_v)                                       \\\n\tdo {\t\t\t\t\t\t  \\\n\t\tif (optlen < sizeof(_v)) {\t\t  \\\n\t\t\terr = -EINVAL;\t\t\t  \\\n\t\t\tgoto exit;\t\t\t  \\\n\t\t}\t\t\t\t\t  \\\n\t\tif (copy_from_sockptr(&_v, optval, sizeof(_v)) != 0) {\t\\\n\t\t\terr = -EFAULT;\t\t\t\t\t\\\n\t\t\tgoto exit;\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\ttransport = vsk->transport;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_VM_SOCKETS_BUFFER_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsock_update_buffer_size(vsk, transport, val);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_BUFFER_MAX_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsk->buffer_max_size = val;\n\t\tvsock_update_buffer_size(vsk, transport, vsk->buffer_size);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_BUFFER_MIN_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsk->buffer_min_size = val;\n\t\tvsock_update_buffer_size(vsk, transport, vsk->buffer_size);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_CONNECT_TIMEOUT: {\n\t\tstruct __kernel_old_timeval tv;\n\t\tCOPY_IN(tv);\n\t\tif (tv.tv_sec >= 0 && tv.tv_usec < USEC_PER_SEC &&\n\t\t    tv.tv_sec < (MAX_SCHEDULE_TIMEOUT / HZ - 1)) {\n\t\t\tvsk->connect_timeout = tv.tv_sec * HZ +\n\t\t\t    DIV_ROUND_UP(tv.tv_usec, (1000000 / HZ));\n\t\t\tif (vsk->connect_timeout == 0)\n\t\t\t\tvsk->connect_timeout =\n\t\t\t\t    VSOCK_DEFAULT_CONNECT_TIMEOUT;\n\n\t\t} else {\n\t\t\terr = -ERANGE;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n#undef COPY_IN\n\nexit:\n\trelease_sock(sk);\n\treturn err;\n}",
        "func": "static int vsock_stream_setsockopt(struct socket *sock,\n\t\t\t\t   int level,\n\t\t\t\t   int optname,\n\t\t\t\t   sockptr_t optval,\n\t\t\t\t   unsigned int optlen)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tu64 val;\n\n\tif (level != AF_VSOCK)\n\t\treturn -ENOPROTOOPT;\n\n#define COPY_IN(_v)                                       \\\n\tdo {\t\t\t\t\t\t  \\\n\t\tif (optlen < sizeof(_v)) {\t\t  \\\n\t\t\terr = -EINVAL;\t\t\t  \\\n\t\t\tgoto exit;\t\t\t  \\\n\t\t}\t\t\t\t\t  \\\n\t\tif (copy_from_sockptr(&_v, optval, sizeof(_v)) != 0) {\t\\\n\t\t\terr = -EFAULT;\t\t\t\t\t\\\n\t\t\tgoto exit;\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tlock_sock(sk);\n\n\ttransport = vsk->transport;\n\n\tswitch (optname) {\n\tcase SO_VM_SOCKETS_BUFFER_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsock_update_buffer_size(vsk, transport, val);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_BUFFER_MAX_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsk->buffer_max_size = val;\n\t\tvsock_update_buffer_size(vsk, transport, vsk->buffer_size);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_BUFFER_MIN_SIZE:\n\t\tCOPY_IN(val);\n\t\tvsk->buffer_min_size = val;\n\t\tvsock_update_buffer_size(vsk, transport, vsk->buffer_size);\n\t\tbreak;\n\n\tcase SO_VM_SOCKETS_CONNECT_TIMEOUT: {\n\t\tstruct __kernel_old_timeval tv;\n\t\tCOPY_IN(tv);\n\t\tif (tv.tv_sec >= 0 && tv.tv_usec < USEC_PER_SEC &&\n\t\t    tv.tv_sec < (MAX_SCHEDULE_TIMEOUT / HZ - 1)) {\n\t\t\tvsk->connect_timeout = tv.tv_sec * HZ +\n\t\t\t    DIV_ROUND_UP(tv.tv_usec, (1000000 / HZ));\n\t\t\tif (vsk->connect_timeout == 0)\n\t\t\t\tvsk->connect_timeout =\n\t\t\t\t    VSOCK_DEFAULT_CONNECT_TIMEOUT;\n\n\t\t} else {\n\t\t\terr = -ERANGE;\n\t\t}\n\t\tbreak;\n\t}\n\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n#undef COPY_IN\n\nexit:\n\trelease_sock(sk);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,9 +28,10 @@\n \terr = 0;\n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n-\ttransport = vsk->transport;\n \n \tlock_sock(sk);\n+\n+\ttransport = vsk->transport;\n \n \tswitch (optname) {\n \tcase SO_VM_SOCKETS_BUFFER_SIZE:",
        "diff_line_info": {
            "deleted_lines": [
                "\ttransport = vsk->transport;"
            ],
            "added_lines": [
                "",
                "\ttransport = vsk->transport;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26708",
        "func_name": "torvalds/linux/vsock_dgram_sendmsg",
        "description": "A local privilege escalation was discovered in the Linux kernel before 5.10.13. Multiple race conditions in the AF_VSOCK implementation are caused by wrong locking in net/vmw_vsock/af_vsock.c. The race conditions were implicitly introduced in the commits that added VSOCK multi-transport support.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c518adafa39f37858697ac9309c6cf1805581446",
        "commit_title": "There are multiple similar bugs implicitly introduced by the",
        "commit_text": "commit c0cfa2d8a788fcf4 (\"vsock: add multi-transports support\") and commit 6a2c0962105ae8ce (\"vsock: prevent transport modules unloading\").  The bug pattern:  [1] vsock_sock.transport pointer is copied to a local variable,  [2] lock_sock() is called,  [3] the local variable is used. VSOCK multi-transport support introduced the race condition: vsock_sock.transport value may change between [1] and [2].  Let's copy vsock_sock.transport pointer to local variables after the lock_sock() call.  Link: https://lore.kernel.org/r/20210201084719.2257066-1-alex.popov@linux.com ",
        "func_before": "static int vsock_dgram_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tstruct sockaddr_vm *remote_addr;\n\tconst struct vsock_transport *transport;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/* For now, MSG_DONTWAIT is always assumed... */\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\ttransport = vsk->transport;\n\n\tlock_sock(sk);\n\n\terr = vsock_auto_bind(vsk);\n\tif (err)\n\t\tgoto out;\n\n\n\t/* If the provided message contains an address, use that.  Otherwise\n\t * fall back on the socket's remote handle (if it has been connected).\n\t */\n\tif (msg->msg_name &&\n\t    vsock_addr_cast(msg->msg_name, msg->msg_namelen,\n\t\t\t    &remote_addr) == 0) {\n\t\t/* Ensure this address is of the right type and is a valid\n\t\t * destination.\n\t\t */\n\n\t\tif (remote_addr->svm_cid == VMADDR_CID_ANY)\n\t\t\tremote_addr->svm_cid = transport->get_local_cid();\n\n\t\tif (!vsock_addr_bound(remote_addr)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else if (sock->state == SS_CONNECTED) {\n\t\tremote_addr = &vsk->remote_addr;\n\n\t\tif (remote_addr->svm_cid == VMADDR_CID_ANY)\n\t\t\tremote_addr->svm_cid = transport->get_local_cid();\n\n\t\t/* XXX Should connect() or this function ensure remote_addr is\n\t\t * bound?\n\t\t */\n\t\tif (!vsock_addr_bound(&vsk->remote_addr)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!transport->dgram_allow(remote_addr->svm_cid,\n\t\t\t\t    remote_addr->svm_port)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = transport->dgram_enqueue(vsk, remote_addr, msg, len);\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "func": "static int vsock_dgram_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tint err;\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tstruct sockaddr_vm *remote_addr;\n\tconst struct vsock_transport *transport;\n\n\tif (msg->msg_flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\t/* For now, MSG_DONTWAIT is always assumed... */\n\terr = 0;\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tlock_sock(sk);\n\n\ttransport = vsk->transport;\n\n\terr = vsock_auto_bind(vsk);\n\tif (err)\n\t\tgoto out;\n\n\n\t/* If the provided message contains an address, use that.  Otherwise\n\t * fall back on the socket's remote handle (if it has been connected).\n\t */\n\tif (msg->msg_name &&\n\t    vsock_addr_cast(msg->msg_name, msg->msg_namelen,\n\t\t\t    &remote_addr) == 0) {\n\t\t/* Ensure this address is of the right type and is a valid\n\t\t * destination.\n\t\t */\n\n\t\tif (remote_addr->svm_cid == VMADDR_CID_ANY)\n\t\t\tremote_addr->svm_cid = transport->get_local_cid();\n\n\t\tif (!vsock_addr_bound(remote_addr)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else if (sock->state == SS_CONNECTED) {\n\t\tremote_addr = &vsk->remote_addr;\n\n\t\tif (remote_addr->svm_cid == VMADDR_CID_ANY)\n\t\t\tremote_addr->svm_cid = transport->get_local_cid();\n\n\t\t/* XXX Should connect() or this function ensure remote_addr is\n\t\t * bound?\n\t\t */\n\t\tif (!vsock_addr_bound(&vsk->remote_addr)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!transport->dgram_allow(remote_addr->svm_cid,\n\t\t\t\t    remote_addr->svm_port)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = transport->dgram_enqueue(vsk, remote_addr, msg, len);\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -14,9 +14,10 @@\n \terr = 0;\n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n-\ttransport = vsk->transport;\n \n \tlock_sock(sk);\n+\n+\ttransport = vsk->transport;\n \n \terr = vsock_auto_bind(vsk);\n \tif (err)",
        "diff_line_info": {
            "deleted_lines": [
                "\ttransport = vsk->transport;"
            ],
            "added_lines": [
                "",
                "\ttransport = vsk->transport;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26708",
        "func_name": "torvalds/linux/vsock_poll",
        "description": "A local privilege escalation was discovered in the Linux kernel before 5.10.13. Multiple race conditions in the AF_VSOCK implementation are caused by wrong locking in net/vmw_vsock/af_vsock.c. The race conditions were implicitly introduced in the commits that added VSOCK multi-transport support.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c518adafa39f37858697ac9309c6cf1805581446",
        "commit_title": "There are multiple similar bugs implicitly introduced by the",
        "commit_text": "commit c0cfa2d8a788fcf4 (\"vsock: add multi-transports support\") and commit 6a2c0962105ae8ce (\"vsock: prevent transport modules unloading\").  The bug pattern:  [1] vsock_sock.transport pointer is copied to a local variable,  [2] lock_sock() is called,  [3] the local variable is used. VSOCK multi-transport support introduced the race condition: vsock_sock.transport value may change between [1] and [2].  Let's copy vsock_sock.transport pointer to local variables after the lock_sock() call.  Link: https://lore.kernel.org/r/20210201084719.2257066-1-alex.popov@linux.com ",
        "func_before": "static __poll_t vsock_poll(struct file *file, struct socket *sock,\n\t\t\t       poll_table *wait)\n{\n\tstruct sock *sk;\n\t__poll_t mask;\n\tstruct vsock_sock *vsk;\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tpoll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\tif (sk->sk_err)\n\t\t/* Signify that there has been an error on this socket. */\n\t\tmask |= EPOLLERR;\n\n\t/* INET sockets treat local write shutdown and peer write shutdown as a\n\t * case of EPOLLHUP set.\n\t */\n\tif ((sk->sk_shutdown == SHUTDOWN_MASK) ||\n\t    ((sk->sk_shutdown & SEND_SHUTDOWN) &&\n\t     (vsk->peer_shutdown & SEND_SHUTDOWN))) {\n\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN ||\n\t    vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\tmask |= EPOLLRDHUP;\n\t}\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\t/* For datagram sockets we can read if there is something in\n\t\t * the queue and write as long as the socket isn't shutdown for\n\t\t * sending.\n\t\t */\n\t\tif (!skb_queue_empty_lockless(&sk->sk_receive_queue) ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t}\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN))\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;\n\n\t} else if (sock->type == SOCK_STREAM) {\n\t\tconst struct vsock_transport *transport = vsk->transport;\n\t\tlock_sock(sk);\n\n\t\t/* Listening sockets that have connections in their accept\n\t\t * queue can be read.\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN\n\t\t    && !vsock_is_accept_queue_empty(sk))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If there is something in the queue then we can read. */\n\t\tif (transport && transport->stream_is_active(vsk) &&\n\t\t    !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t\tbool data_ready_now = false;\n\t\t\tint ret = transport->notify_poll_in(\n\t\t\t\t\tvsk, 1, &data_ready_now);\n\t\t\tif (ret < 0) {\n\t\t\t\tmask |= EPOLLERR;\n\t\t\t} else {\n\t\t\t\tif (data_ready_now)\n\t\t\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t\t}\n\t\t}\n\n\t\t/* Sockets whose connections have been closed, reset, or\n\t\t * terminated should also be considered read, and we check the\n\t\t * shutdown flag for that.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN ||\n\t\t    vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t}\n\n\t\t/* Connected sockets that can produce data can be written. */\n\t\tif (transport && sk->sk_state == TCP_ESTABLISHED) {\n\t\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbool space_avail_now = false;\n\t\t\t\tint ret = transport->notify_poll_out(\n\t\t\t\t\t\tvsk, 1, &space_avail_now);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tmask |= EPOLLERR;\n\t\t\t\t} else {\n\t\t\t\t\tif (space_avail_now)\n\t\t\t\t\t\t/* Remove EPOLLWRBAND since INET\n\t\t\t\t\t\t * sockets are not setting it.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* Simulate INET socket poll behaviors, which sets\n\t\t * EPOLLOUT|EPOLLWRNORM when peer is closed and nothing to read,\n\t\t * but local send is not shutdown.\n\t\t */\n\t\tif (sk->sk_state == TCP_CLOSE || sk->sk_state == TCP_CLOSING) {\n\t\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN))\n\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t\t}\n\n\t\trelease_sock(sk);\n\t}\n\n\treturn mask;\n}",
        "func": "static __poll_t vsock_poll(struct file *file, struct socket *sock,\n\t\t\t       poll_table *wait)\n{\n\tstruct sock *sk;\n\t__poll_t mask;\n\tstruct vsock_sock *vsk;\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\n\tpoll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\tif (sk->sk_err)\n\t\t/* Signify that there has been an error on this socket. */\n\t\tmask |= EPOLLERR;\n\n\t/* INET sockets treat local write shutdown and peer write shutdown as a\n\t * case of EPOLLHUP set.\n\t */\n\tif ((sk->sk_shutdown == SHUTDOWN_MASK) ||\n\t    ((sk->sk_shutdown & SEND_SHUTDOWN) &&\n\t     (vsk->peer_shutdown & SEND_SHUTDOWN))) {\n\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN ||\n\t    vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\tmask |= EPOLLRDHUP;\n\t}\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\t/* For datagram sockets we can read if there is something in\n\t\t * the queue and write as long as the socket isn't shutdown for\n\t\t * sending.\n\t\t */\n\t\tif (!skb_queue_empty_lockless(&sk->sk_receive_queue) ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t}\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN))\n\t\t\tmask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;\n\n\t} else if (sock->type == SOCK_STREAM) {\n\t\tconst struct vsock_transport *transport;\n\n\t\tlock_sock(sk);\n\n\t\ttransport = vsk->transport;\n\n\t\t/* Listening sockets that have connections in their accept\n\t\t * queue can be read.\n\t\t */\n\t\tif (sk->sk_state == TCP_LISTEN\n\t\t    && !vsock_is_accept_queue_empty(sk))\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t/* If there is something in the queue then we can read. */\n\t\tif (transport && transport->stream_is_active(vsk) &&\n\t\t    !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t\tbool data_ready_now = false;\n\t\t\tint ret = transport->notify_poll_in(\n\t\t\t\t\tvsk, 1, &data_ready_now);\n\t\t\tif (ret < 0) {\n\t\t\t\tmask |= EPOLLERR;\n\t\t\t} else {\n\t\t\t\tif (data_ready_now)\n\t\t\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t\t}\n\t\t}\n\n\t\t/* Sockets whose connections have been closed, reset, or\n\t\t * terminated should also be considered read, and we check the\n\t\t * shutdown flag for that.\n\t\t */\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN ||\n\t\t    vsk->peer_shutdown & SEND_SHUTDOWN) {\n\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\t\t}\n\n\t\t/* Connected sockets that can produce data can be written. */\n\t\tif (transport && sk->sk_state == TCP_ESTABLISHED) {\n\t\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tbool space_avail_now = false;\n\t\t\t\tint ret = transport->notify_poll_out(\n\t\t\t\t\t\tvsk, 1, &space_avail_now);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tmask |= EPOLLERR;\n\t\t\t\t} else {\n\t\t\t\t\tif (space_avail_now)\n\t\t\t\t\t\t/* Remove EPOLLWRBAND since INET\n\t\t\t\t\t\t * sockets are not setting it.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/* Simulate INET socket poll behaviors, which sets\n\t\t * EPOLLOUT|EPOLLWRNORM when peer is closed and nothing to read,\n\t\t * but local send is not shutdown.\n\t\t */\n\t\tif (sk->sk_state == TCP_CLOSE || sk->sk_state == TCP_CLOSING) {\n\t\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN))\n\t\t\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t\t}\n\n\t\trelease_sock(sk);\n\t}\n\n\treturn mask;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -43,8 +43,11 @@\n \t\t\tmask |= EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND;\n \n \t} else if (sock->type == SOCK_STREAM) {\n-\t\tconst struct vsock_transport *transport = vsk->transport;\n+\t\tconst struct vsock_transport *transport;\n+\n \t\tlock_sock(sk);\n+\n+\t\ttransport = vsk->transport;\n \n \t\t/* Listening sockets that have connections in their accept\n \t\t * queue can be read.",
        "diff_line_info": {
            "deleted_lines": [
                "\t\tconst struct vsock_transport *transport = vsk->transport;"
            ],
            "added_lines": [
                "\t\tconst struct vsock_transport *transport;",
                "",
                "",
                "\t\ttransport = vsk->transport;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-26708",
        "func_name": "torvalds/linux/vsock_stream_recvmsg",
        "description": "A local privilege escalation was discovered in the Linux kernel before 5.10.13. Multiple race conditions in the AF_VSOCK implementation are caused by wrong locking in net/vmw_vsock/af_vsock.c. The race conditions were implicitly introduced in the commits that added VSOCK multi-transport support.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=c518adafa39f37858697ac9309c6cf1805581446",
        "commit_title": "There are multiple similar bugs implicitly introduced by the",
        "commit_text": "commit c0cfa2d8a788fcf4 (\"vsock: add multi-transports support\") and commit 6a2c0962105ae8ce (\"vsock: prevent transport modules unloading\").  The bug pattern:  [1] vsock_sock.transport pointer is copied to a local variable,  [2] lock_sock() is called,  [3] the local variable is used. VSOCK multi-transport support introduced the race condition: vsock_sock.transport value may change between [1] and [2].  Let's copy vsock_sock.transport pointer to local variables after the lock_sock() call.  Link: https://lore.kernel.org/r/20210201084719.2257066-1-alex.popov@linux.com ",
        "func_before": "static int\nvsock_stream_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t     int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\ttransport = vsk->transport;\n\terr = 0;\n\n\tlock_sock(sk);\n\n\tif (!transport || sk->sk_state != TCP_ESTABLISHED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\n\twhile (1) {\n\t\ts64 ready;\n\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tready = vsock_stream_has_data(vsk);\n\n\t\tif (ready == 0) {\n\t\t\tif (sk->sk_err != 0 ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0) {\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tssize_t read;\n\n\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\n\t\t\tif (ready < 0) {\n\t\t\t\t/* Invalid queue pair content. XXX This should\n\t\t\t\t* be changed to a connection reset in a later\n\t\t\t\t* change.\n\t\t\t\t*/\n\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0)\n\t\terr = copied;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "func": "static int\nvsock_stream_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t     int flags)\n{\n\tstruct sock *sk;\n\tstruct vsock_sock *vsk;\n\tconst struct vsock_transport *transport;\n\tint err;\n\tsize_t target;\n\tssize_t copied;\n\tlong timeout;\n\tstruct vsock_transport_recv_notify_data recv_data;\n\n\tDEFINE_WAIT(wait);\n\n\tsk = sock->sk;\n\tvsk = vsock_sk(sk);\n\terr = 0;\n\n\tlock_sock(sk);\n\n\ttransport = vsk->transport;\n\n\tif (!transport || sk->sk_state != TCP_ESTABLISHED) {\n\t\t/* Recvmsg is supposed to return 0 if a peer performs an\n\t\t * orderly shutdown. Differentiate between that case and when a\n\t\t * peer has not connected or a local shutdown occured with the\n\t\t * SOCK_DONE flag.\n\t\t */\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\terr = 0;\n\t\telse\n\t\t\terr = -ENOTCONN;\n\n\t\tgoto out;\n\t}\n\n\tif (flags & MSG_OOB) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\t/* We don't check peer_shutdown flag here since peer may actually shut\n\t * down, but there can be data in the queue that a local socket can\n\t * receive.\n\t */\n\tif (sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* It is valid on Linux to pass in a zero-length receive buffer.  This\n\t * is not an error.  We may as well bail out now.\n\t */\n\tif (!len) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\t/* We must not copy less than target bytes into the user's buffer\n\t * before returning successfully, so we wait for the consume queue to\n\t * have that much data to consume before dequeueing.  Note that this\n\t * makes it impossible to handle cases where target is greater than the\n\t * queue size.\n\t */\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\tif (target >= transport->stream_rcvhiwat(vsk)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ttimeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tcopied = 0;\n\n\terr = transport->notify_recv_init(vsk, target, &recv_data);\n\tif (err < 0)\n\t\tgoto out;\n\n\n\twhile (1) {\n\t\ts64 ready;\n\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tready = vsock_stream_has_data(vsk);\n\n\t\tif (ready == 0) {\n\t\t\tif (sk->sk_err != 0 ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    (vsk->peer_shutdown & SEND_SHUTDOWN)) {\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* Don't wait for non-blocking sockets. */\n\t\t\tif (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_block(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0) {\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\trelease_sock(sk);\n\t\t\ttimeout = schedule_timeout(timeout);\n\t\t\tlock_sock(sk);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeout);\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t} else if (timeout == 0) {\n\t\t\t\terr = -EAGAIN;\n\t\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tssize_t read;\n\n\t\t\tfinish_wait(sk_sleep(sk), &wait);\n\n\t\t\tif (ready < 0) {\n\t\t\t\t/* Invalid queue pair content. XXX This should\n\t\t\t\t* be changed to a connection reset in a later\n\t\t\t\t* change.\n\t\t\t\t*/\n\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\terr = transport->notify_recv_pre_dequeue(\n\t\t\t\t\tvsk, target, &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\n\t\t\tread = transport->stream_dequeue(\n\t\t\t\t\tvsk, msg,\n\t\t\t\t\tlen - copied, flags);\n\t\t\tif (read < 0) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcopied += read;\n\n\t\t\terr = transport->notify_recv_post_dequeue(\n\t\t\t\t\tvsk, target, read,\n\t\t\t\t\t!(flags & MSG_PEEK), &recv_data);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\n\t\t\tif (read >= target || flags & MSG_PEEK)\n\t\t\t\tbreak;\n\n\t\t\ttarget -= read;\n\t\t}\n\t}\n\n\tif (sk->sk_err)\n\t\terr = -sk->sk_err;\n\telse if (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\terr = 0;\n\n\tif (copied > 0)\n\t\terr = copied;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,10 +15,11 @@\n \n \tsk = sock->sk;\n \tvsk = vsock_sk(sk);\n-\ttransport = vsk->transport;\n \terr = 0;\n \n \tlock_sock(sk);\n+\n+\ttransport = vsk->transport;\n \n \tif (!transport || sk->sk_state != TCP_ESTABLISHED) {\n \t\t/* Recvmsg is supposed to return 0 if a peer performs an",
        "diff_line_info": {
            "deleted_lines": [
                "\ttransport = vsk->transport;"
            ],
            "added_lines": [
                "",
                "\ttransport = vsk->transport;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28951",
        "func_name": "torvalds/linux/io_sq_offload_start",
        "description": "An issue was discovered in fs/io_uring.c in the Linux kernel through 5.11.8. It allows attackers to cause a denial of service (deadlock) because exit may be waiting to park a SQPOLL thread, but concurrently that SQPOLL thread is waiting for a signal to start, aka CID-3ebba796fa25.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3ebba796fa251d042be42b929a2d916ee5c34a49",
        "commit_title": "If we create it in a disabled state because IORING_SETUP_R_DISABLED is",
        "commit_text": "set on ring creation, we need to ensure that we've kicked the thread if we're exiting before it's been explicitly disabled. Otherwise we can run into a deadlock where exit is waiting go park the SQPOLL thread, but the SQPOLL thread itself is waiting to get a signal to start.  That results in the below trace of both tasks hung, waiting on each other:  INFO: task syz-executor458:8401 blocked for more than 143 seconds.       Not tainted 5.11.0-next-20210226-syzkaller #0 \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message. task:syz-executor458 state:D stack:27536 pid: 8401 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread_park fs/io_uring.c:7115 [inline]  io_sq_thread_park+0xd5/0x130 fs/io_uring.c:7103  io_uring_cancel_task_requests+0x24c/0xd90 fs/io_uring.c:8745  __io_uring_files_cancel+0x110/0x230 fs/io_uring.c:8840  io_uring_files_cancel include/linux/io_uring.h:47 [inline]  do_exit+0x299/0x2a60 kernel/exit.c:780  do_group_exit+0x125/0x310 kernel/exit.c:922  __do_sys_exit_group kernel/exit.c:933 [inline]  __se_sys_exit_group kernel/exit.c:931 [inline]  __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:931  do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46  entry_SYSCALL_64_after_hwframe+0x44/0xae RIP: 0033:0x43e899 RSP: 002b:00007ffe89376d48 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7 RAX: ffffffffffffffda RBX: 00000000004af2f0 RCX: 000000000043e899 RDX: 000000000000003c RSI: 00000000000000e7 RDI: 0000000000000000 RBP: 0000000000000000 R08: ffffffffffffffc0 R09: 0000000010000000 R10: 0000000000008011 R11: 0000000000000246 R12: 00000000004af2f0 R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000001 INFO: task iou-sqp-8401:8402 can't die for more than 143 seconds. task:iou-sqp-8401    state:D stack:30272 pid: 8402 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread+0x27d/0x1ae0 fs/io_uring.c:6717  ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294 INFO: task iou-sqp-8401:8402 blocked for more than 143 seconds.  ",
        "func_before": "static void io_sq_offload_start(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL)\n\t\tcomplete(&sqd->startup);\n}",
        "func": "static void io_sq_offload_start(struct io_ring_ctx *ctx)\n{\n\tstruct io_sq_data *sqd = ctx->sq_data;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\tif (ctx->flags & IORING_SETUP_SQPOLL)\n\t\tcomplete(&sqd->startup);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n \tstruct io_sq_data *sqd = ctx->sq_data;\n \n+\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n \tif (ctx->flags & IORING_SETUP_SQPOLL)\n \t\tcomplete(&sqd->startup);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tctx->flags &= ~IORING_SETUP_R_DISABLED;"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28951",
        "func_name": "torvalds/linux/io_disable_sqo_submit",
        "description": "An issue was discovered in fs/io_uring.c in the Linux kernel through 5.11.8. It allows attackers to cause a denial of service (deadlock) because exit may be waiting to park a SQPOLL thread, but concurrently that SQPOLL thread is waiting for a signal to start, aka CID-3ebba796fa25.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3ebba796fa251d042be42b929a2d916ee5c34a49",
        "commit_title": "If we create it in a disabled state because IORING_SETUP_R_DISABLED is",
        "commit_text": "set on ring creation, we need to ensure that we've kicked the thread if we're exiting before it's been explicitly disabled. Otherwise we can run into a deadlock where exit is waiting go park the SQPOLL thread, but the SQPOLL thread itself is waiting to get a signal to start.  That results in the below trace of both tasks hung, waiting on each other:  INFO: task syz-executor458:8401 blocked for more than 143 seconds.       Not tainted 5.11.0-next-20210226-syzkaller #0 \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message. task:syz-executor458 state:D stack:27536 pid: 8401 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread_park fs/io_uring.c:7115 [inline]  io_sq_thread_park+0xd5/0x130 fs/io_uring.c:7103  io_uring_cancel_task_requests+0x24c/0xd90 fs/io_uring.c:8745  __io_uring_files_cancel+0x110/0x230 fs/io_uring.c:8840  io_uring_files_cancel include/linux/io_uring.h:47 [inline]  do_exit+0x299/0x2a60 kernel/exit.c:780  do_group_exit+0x125/0x310 kernel/exit.c:922  __do_sys_exit_group kernel/exit.c:933 [inline]  __se_sys_exit_group kernel/exit.c:931 [inline]  __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:931  do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46  entry_SYSCALL_64_after_hwframe+0x44/0xae RIP: 0033:0x43e899 RSP: 002b:00007ffe89376d48 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7 RAX: ffffffffffffffda RBX: 00000000004af2f0 RCX: 000000000043e899 RDX: 000000000000003c RSI: 00000000000000e7 RDI: 0000000000000000 RBP: 0000000000000000 R08: ffffffffffffffc0 R09: 0000000010000000 R10: 0000000000008011 R11: 0000000000000246 R12: 00000000004af2f0 R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000001 INFO: task iou-sqp-8401:8402 can't die for more than 143 seconds. task:iou-sqp-8401    state:D stack:30272 pid: 8402 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread+0x27d/0x1ae0 fs/io_uring.c:6717  ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294 INFO: task iou-sqp-8401:8402 blocked for more than 143 seconds.  ",
        "func_before": "static void io_disable_sqo_submit(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tctx->sqo_dead = 1;\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* make sure callers enter the ring to get error */\n\tif (ctx->rings)\n\t\tio_ring_set_wakeup_flag(ctx);\n}",
        "func": "static void io_disable_sqo_submit(struct io_ring_ctx *ctx)\n{\n\tmutex_lock(&ctx->uring_lock);\n\tctx->sqo_dead = 1;\n\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n\t\tio_sq_offload_start(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t/* make sure callers enter the ring to get error */\n\tif (ctx->rings)\n\t\tio_ring_set_wakeup_flag(ctx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,8 @@\n {\n \tmutex_lock(&ctx->uring_lock);\n \tctx->sqo_dead = 1;\n+\tif (ctx->flags & IORING_SETUP_R_DISABLED)\n+\t\tio_sq_offload_start(ctx);\n \tmutex_unlock(&ctx->uring_lock);\n \n \t/* make sure callers enter the ring to get error */",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tif (ctx->flags & IORING_SETUP_R_DISABLED)",
                "\t\tio_sq_offload_start(ctx);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-28951",
        "func_name": "torvalds/linux/io_register_enable_rings",
        "description": "An issue was discovered in fs/io_uring.c in the Linux kernel through 5.11.8. It allows attackers to cause a denial of service (deadlock) because exit may be waiting to park a SQPOLL thread, but concurrently that SQPOLL thread is waiting for a signal to start, aka CID-3ebba796fa25.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/?h=3ebba796fa251d042be42b929a2d916ee5c34a49",
        "commit_title": "If we create it in a disabled state because IORING_SETUP_R_DISABLED is",
        "commit_text": "set on ring creation, we need to ensure that we've kicked the thread if we're exiting before it's been explicitly disabled. Otherwise we can run into a deadlock where exit is waiting go park the SQPOLL thread, but the SQPOLL thread itself is waiting to get a signal to start.  That results in the below trace of both tasks hung, waiting on each other:  INFO: task syz-executor458:8401 blocked for more than 143 seconds.       Not tainted 5.11.0-next-20210226-syzkaller #0 \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message. task:syz-executor458 state:D stack:27536 pid: 8401 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread_park fs/io_uring.c:7115 [inline]  io_sq_thread_park+0xd5/0x130 fs/io_uring.c:7103  io_uring_cancel_task_requests+0x24c/0xd90 fs/io_uring.c:8745  __io_uring_files_cancel+0x110/0x230 fs/io_uring.c:8840  io_uring_files_cancel include/linux/io_uring.h:47 [inline]  do_exit+0x299/0x2a60 kernel/exit.c:780  do_group_exit+0x125/0x310 kernel/exit.c:922  __do_sys_exit_group kernel/exit.c:933 [inline]  __se_sys_exit_group kernel/exit.c:931 [inline]  __x64_sys_exit_group+0x3a/0x50 kernel/exit.c:931  do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46  entry_SYSCALL_64_after_hwframe+0x44/0xae RIP: 0033:0x43e899 RSP: 002b:00007ffe89376d48 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7 RAX: ffffffffffffffda RBX: 00000000004af2f0 RCX: 000000000043e899 RDX: 000000000000003c RSI: 00000000000000e7 RDI: 0000000000000000 RBP: 0000000000000000 R08: ffffffffffffffc0 R09: 0000000010000000 R10: 0000000000008011 R11: 0000000000000246 R12: 00000000004af2f0 R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000001 INFO: task iou-sqp-8401:8402 can't die for more than 143 seconds. task:iou-sqp-8401    state:D stack:30272 pid: 8402 ppid:  8400 flags:0x00004004 Call Trace:  context_switch kernel/sched/core.c:4324 [inline]  __schedule+0x90c/0x21a0 kernel/sched/core.c:5075  schedule+0xcf/0x270 kernel/sched/core.c:5154  schedule_timeout+0x1db/0x250 kernel/time/timer.c:1868  do_wait_for_common kernel/sched/completion.c:85 [inline]  __wait_for_common kernel/sched/completion.c:106 [inline]  wait_for_common kernel/sched/completion.c:117 [inline]  wait_for_completion+0x168/0x270 kernel/sched/completion.c:138  io_sq_thread+0x27d/0x1ae0 fs/io_uring.c:6717  ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:294 INFO: task iou-sqp-8401:8402 blocked for more than 143 seconds.  ",
        "func_before": "static int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\n\tio_sq_offload_start(ctx);\n\n\treturn 0;\n}",
        "func": "static int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tio_sq_offload_start(ctx);\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,9 +6,6 @@\n \tif (ctx->restrictions.registered)\n \t\tctx->restricted = 1;\n \n-\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n-\n \tio_sq_offload_start(ctx);\n-\n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tctx->flags &= ~IORING_SETUP_R_DISABLED;",
                "",
                ""
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2022-3996",
        "func_name": "openssl/ossl_policy_cache_set_mapping",
        "description": "If an X.509 certificate contains a malformed policy constraint and\npolicy processing is enabled, then a write lock will be taken twice\nrecursively.  On some operating systems (most widely: Windows) this\nresults in a denial of service when the affected process hangs.  Policy\nprocessing being enabled on a publicly facing server is not considered\nto be a common setup.\n\nPolicy processing is enabled by passing the `-policy'\nargument to the command line utilities or by calling the\n`X509_VERIFY_PARAM_set1_policies()' function.\n\nUpdate (31 March 2023): The description of the policy processing enablement\nwas corrected based on CVE-2023-0466.",
        "git_url": "https://github.com/openssl/openssl/commit/7725e7bfe6f2ce8146b6552b44e0d226be7638e7",
        "commit_title": "x509: fix double locking problem",
        "commit_text": " This reverts commit 9aa4be691f5c73eb3c68606d824c104550c053f7 and removed the redundant flag setting.  Fixes #19643  Fixes LOW CVE-2022-3996  (Merged from https://github.com/openssl/openssl/pull/19652)  (cherry picked from commit 4d0340a6d2f327700a059f0b8f954d6160f8eef5)",
        "func_before": "int ossl_policy_cache_set_mapping(X509 *x, POLICY_MAPPINGS *maps)\n{\n    POLICY_MAPPING *map;\n    X509_POLICY_DATA *data;\n    X509_POLICY_CACHE *cache = x->policy_cache;\n    int i;\n    int ret = 0;\n    if (sk_POLICY_MAPPING_num(maps) == 0) {\n        ret = -1;\n        goto bad_mapping;\n    }\n    for (i = 0; i < sk_POLICY_MAPPING_num(maps); i++) {\n        map = sk_POLICY_MAPPING_value(maps, i);\n        /* Reject if map to or from anyPolicy */\n        if ((OBJ_obj2nid(map->subjectDomainPolicy) == NID_any_policy)\n            || (OBJ_obj2nid(map->issuerDomainPolicy) == NID_any_policy)) {\n            ret = -1;\n            goto bad_mapping;\n        }\n\n        /* Attempt to find matching policy data */\n        data = ossl_policy_cache_find_data(cache, map->issuerDomainPolicy);\n        /* If we don't have anyPolicy can't map */\n        if (data == NULL && !cache->anyPolicy)\n            continue;\n\n        /* Create a NODE from anyPolicy */\n        if (data == NULL) {\n            data = ossl_policy_data_new(NULL, map->issuerDomainPolicy,\n                                        cache->anyPolicy->flags\n                                        & POLICY_DATA_FLAG_CRITICAL);\n            if (data == NULL)\n                goto bad_mapping;\n            data->qualifier_set = cache->anyPolicy->qualifier_set;\n            /*\n             * map->issuerDomainPolicy = NULL;\n             */\n            data->flags |= POLICY_DATA_FLAG_MAPPED_ANY;\n            data->flags |= POLICY_DATA_FLAG_SHARED_QUALIFIERS;\n            if (!sk_X509_POLICY_DATA_push(cache->data, data)) {\n                ossl_policy_data_free(data);\n                goto bad_mapping;\n            }\n        } else\n            data->flags |= POLICY_DATA_FLAG_MAPPED;\n        if (!sk_ASN1_OBJECT_push(data->expected_policy_set,\n                                 map->subjectDomainPolicy))\n            goto bad_mapping;\n        map->subjectDomainPolicy = NULL;\n\n    }\n\n    ret = 1;\n bad_mapping:\n    if (ret == -1 && CRYPTO_THREAD_write_lock(x->lock)) {\n        x->ex_flags |= EXFLAG_INVALID_POLICY;\n        CRYPTO_THREAD_unlock(x->lock);\n    }\n    sk_POLICY_MAPPING_pop_free(maps, POLICY_MAPPING_free);\n    return ret;\n\n}",
        "func": "int ossl_policy_cache_set_mapping(X509 *x, POLICY_MAPPINGS *maps)\n{\n    POLICY_MAPPING *map;\n    X509_POLICY_DATA *data;\n    X509_POLICY_CACHE *cache = x->policy_cache;\n    int i;\n    int ret = 0;\n    if (sk_POLICY_MAPPING_num(maps) == 0) {\n        ret = -1;\n        goto bad_mapping;\n    }\n    for (i = 0; i < sk_POLICY_MAPPING_num(maps); i++) {\n        map = sk_POLICY_MAPPING_value(maps, i);\n        /* Reject if map to or from anyPolicy */\n        if ((OBJ_obj2nid(map->subjectDomainPolicy) == NID_any_policy)\n            || (OBJ_obj2nid(map->issuerDomainPolicy) == NID_any_policy)) {\n            ret = -1;\n            goto bad_mapping;\n        }\n\n        /* Attempt to find matching policy data */\n        data = ossl_policy_cache_find_data(cache, map->issuerDomainPolicy);\n        /* If we don't have anyPolicy can't map */\n        if (data == NULL && !cache->anyPolicy)\n            continue;\n\n        /* Create a NODE from anyPolicy */\n        if (data == NULL) {\n            data = ossl_policy_data_new(NULL, map->issuerDomainPolicy,\n                                        cache->anyPolicy->flags\n                                        & POLICY_DATA_FLAG_CRITICAL);\n            if (data == NULL)\n                goto bad_mapping;\n            data->qualifier_set = cache->anyPolicy->qualifier_set;\n            /*\n             * map->issuerDomainPolicy = NULL;\n             */\n            data->flags |= POLICY_DATA_FLAG_MAPPED_ANY;\n            data->flags |= POLICY_DATA_FLAG_SHARED_QUALIFIERS;\n            if (!sk_X509_POLICY_DATA_push(cache->data, data)) {\n                ossl_policy_data_free(data);\n                goto bad_mapping;\n            }\n        } else\n            data->flags |= POLICY_DATA_FLAG_MAPPED;\n        if (!sk_ASN1_OBJECT_push(data->expected_policy_set,\n                                 map->subjectDomainPolicy))\n            goto bad_mapping;\n        map->subjectDomainPolicy = NULL;\n\n    }\n\n    ret = 1;\n bad_mapping:\n    sk_POLICY_MAPPING_pop_free(maps, POLICY_MAPPING_free);\n    return ret;\n\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -52,10 +52,6 @@\n \n     ret = 1;\n  bad_mapping:\n-    if (ret == -1 && CRYPTO_THREAD_write_lock(x->lock)) {\n-        x->ex_flags |= EXFLAG_INVALID_POLICY;\n-        CRYPTO_THREAD_unlock(x->lock);\n-    }\n     sk_POLICY_MAPPING_pop_free(maps, POLICY_MAPPING_free);\n     return ret;\n ",
        "diff_line_info": {
            "deleted_lines": [
                "    if (ret == -1 && CRYPTO_THREAD_write_lock(x->lock)) {",
                "        x->ex_flags |= EXFLAG_INVALID_POLICY;",
                "        CRYPTO_THREAD_unlock(x->lock);",
                "    }"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2018-1000127",
        "func_name": "memcached/process_get_command",
        "description": "memcached version prior to 1.4.37 contains an Integer Overflow vulnerability in items.c:item_free() that can result in data corruption and deadlocks due to items existing in hash table being reused from free list. This attack appear to be exploitable via network connectivity to the memcached service. This vulnerability appears to have been fixed in 1.4.37 and later.",
        "git_url": "https://github.com/memcached/memcached/commit/a8c4a82787b8b6c256d61bd5c42fb7f92d1bae00",
        "commit_title": "Don't overflow item refcount on get",
        "commit_text": " Counts as a miss if the refcount is too high. ASCII multigets are the only time refcounts can be held for so long.  doing a dirty read of refcount. is aligned.  trying to avoid adding an extra refcount branch for all calls of item_get due to performance. might be able to move it in there after logging refactoring simplifies some of the branches.",
        "func_before": "static inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas) {\n    char *key;\n    size_t nkey;\n    int i = 0;\n    item *it;\n    token_t *key_token = &tokens[KEY_TOKEN];\n    char *suffix;\n    assert(c != NULL);\n\n    do {\n        while(key_token->length != 0) {\n\n            key = key_token->value;\n            nkey = key_token->length;\n\n            if(nkey > KEY_MAX_LENGTH) {\n                out_string(c, \"CLIENT_ERROR bad command line format\");\n                while (i-- > 0) {\n                    item_remove(*(c->ilist + i));\n                }\n                return;\n            }\n\n            it = item_get(key, nkey, c, DO_UPDATE);\n            if (settings.detail_enabled) {\n                stats_prefix_record_get(key, nkey, NULL != it);\n            }\n            if (it) {\n                if (i >= c->isize) {\n                    item **new_list = realloc(c->ilist, sizeof(item *) * c->isize * 2);\n                    if (new_list) {\n                        c->isize *= 2;\n                        c->ilist = new_list;\n                    } else {\n                        STATS_LOCK();\n                        stats.malloc_fails++;\n                        STATS_UNLOCK();\n                        item_remove(it);\n                        break;\n                    }\n                }\n\n                /*\n                 * Construct the response. Each hit adds three elements to the\n                 * outgoing data list:\n                 *   \"VALUE \"\n                 *   key\n                 *   \" \" + flags + \" \" + data length + \"\\r\\n\" + data (with \\r\\n)\n                 */\n\n                if (return_cas || !settings.inline_ascii_response)\n                {\n                  MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                        it->nbytes, ITEM_get_cas(it));\n                  /* Goofy mid-flight realloc. */\n                  if (i >= c->suffixsize) {\n                    char **new_suffix_list = realloc(c->suffixlist,\n                                           sizeof(char *) * c->suffixsize * 2);\n                    if (new_suffix_list) {\n                        c->suffixsize *= 2;\n                        c->suffixlist  = new_suffix_list;\n                    } else {\n                        STATS_LOCK();\n                        stats.malloc_fails++;\n                        STATS_UNLOCK();\n                        item_remove(it);\n                        break;\n                    }\n                  }\n\n                  suffix = do_cache_alloc(c->thread->suffix_cache);\n                  if (suffix == NULL) {\n                      STATS_LOCK();\n                      stats.malloc_fails++;\n                      STATS_UNLOCK();\n                      out_of_memory(c, \"SERVER_ERROR out of memory making CAS suffix\");\n                      item_remove(it);\n                      while (i-- > 0) {\n                          item_remove(*(c->ilist + i));\n                      }\n                      return;\n                  }\n                  *(c->suffixlist + i) = suffix;\n                  int suffix_len = make_ascii_get_suffix(suffix, it, return_cas);\n                  if (add_iov(c, \"VALUE \", 6) != 0 ||\n                      add_iov(c, ITEM_key(it), it->nkey) != 0 ||\n                      (settings.inline_ascii_response && add_iov(c, ITEM_suffix(it), it->nsuffix - 2) != 0) ||\n                      add_iov(c, suffix, suffix_len) != 0)\n                      {\n                          item_remove(it);\n                          break;\n                      }\n                  if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                      add_iov(c, ITEM_data(it), it->nbytes);\n                  } else if (add_chunked_item_iovs(c, it, it->nbytes) != 0) {\n                      item_remove(it);\n                      break;\n                  }\n                }\n                else\n                {\n                  MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                        it->nbytes, ITEM_get_cas(it));\n                  if (add_iov(c, \"VALUE \", 6) != 0 ||\n                      add_iov(c, ITEM_key(it), it->nkey) != 0)\n                      {\n                          item_remove(it);\n                          break;\n                      }\n                  if ((it->it_flags & ITEM_CHUNKED) == 0)\n                      {\n                          if (add_iov(c, ITEM_suffix(it), it->nsuffix + it->nbytes) != 0)\n                          {\n                              item_remove(it);\n                              break;\n                          }\n                      } else if (add_iov(c, ITEM_suffix(it), it->nsuffix) != 0 ||\n                                 add_chunked_item_iovs(c, it, it->nbytes) != 0) {\n                          item_remove(it);\n                          break;\n                      }\n                }\n\n\n                if (settings.verbose > 1) {\n                    int ii;\n                    fprintf(stderr, \">%d sending key \", c->sfd);\n                    for (ii = 0; ii < it->nkey; ++ii) {\n                        fprintf(stderr, \"%c\", key[ii]);\n                    }\n                    fprintf(stderr, \"\\n\");\n                }\n\n                /* item_get() has incremented it->refcount for us */\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.slab_stats[ITEM_clsid(it)].get_hits++;\n                c->thread->stats.get_cmds++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                *(c->ilist + i) = it;\n                i++;\n\n            } else {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.get_misses++;\n                c->thread->stats.get_cmds++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                MEMCACHED_COMMAND_GET(c->sfd, key, nkey, -1, 0);\n            }\n\n            key_token++;\n        }\n\n        /*\n         * If the command string hasn't been fully processed, get the next set\n         * of tokens.\n         */\n        if(key_token->value != NULL) {\n            ntokens = tokenize_command(key_token->value, tokens, MAX_TOKENS);\n            key_token = tokens;\n        }\n\n    } while(key_token->value != NULL);\n\n    c->icurr = c->ilist;\n    c->ileft = i;\n    if (return_cas || !settings.inline_ascii_response) {\n        c->suffixcurr = c->suffixlist;\n        c->suffixleft = i;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d END\\n\", c->sfd);\n\n    /*\n        If the loop was terminated because of out-of-memory, it is not\n        reliable to add END\\r\\n to the buffer, because it might not end\n        in \\r\\n. So we send SERVER_ERROR instead.\n    */\n    if (key_token->value != NULL || add_iov(c, \"END\\r\\n\", 5) != 0\n        || (IS_UDP(c->transport) && build_udp_headers(c) != 0)) {\n        out_of_memory(c, \"SERVER_ERROR out of memory writing get response\");\n    }\n    else {\n        conn_set_state(c, conn_mwrite);\n        c->msgcurr = 0;\n    }\n}",
        "func": "static inline void process_get_command(conn *c, token_t *tokens, size_t ntokens, bool return_cas) {\n    char *key;\n    size_t nkey;\n    int i = 0;\n    item *it;\n    token_t *key_token = &tokens[KEY_TOKEN];\n    char *suffix;\n    assert(c != NULL);\n\n    do {\n        while(key_token->length != 0) {\n\n            key = key_token->value;\n            nkey = key_token->length;\n\n            if(nkey > KEY_MAX_LENGTH) {\n                out_string(c, \"CLIENT_ERROR bad command line format\");\n                while (i-- > 0) {\n                    item_remove(*(c->ilist + i));\n                }\n                return;\n            }\n\n            it = limited_get(key, nkey, c);\n            if (settings.detail_enabled) {\n                stats_prefix_record_get(key, nkey, NULL != it);\n            }\n            if (it) {\n                if (i >= c->isize) {\n                    item **new_list = realloc(c->ilist, sizeof(item *) * c->isize * 2);\n                    if (new_list) {\n                        c->isize *= 2;\n                        c->ilist = new_list;\n                    } else {\n                        STATS_LOCK();\n                        stats.malloc_fails++;\n                        STATS_UNLOCK();\n                        item_remove(it);\n                        break;\n                    }\n                }\n\n                /*\n                 * Construct the response. Each hit adds three elements to the\n                 * outgoing data list:\n                 *   \"VALUE \"\n                 *   key\n                 *   \" \" + flags + \" \" + data length + \"\\r\\n\" + data (with \\r\\n)\n                 */\n\n                if (return_cas || !settings.inline_ascii_response)\n                {\n                  MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                        it->nbytes, ITEM_get_cas(it));\n                  /* Goofy mid-flight realloc. */\n                  if (i >= c->suffixsize) {\n                    char **new_suffix_list = realloc(c->suffixlist,\n                                           sizeof(char *) * c->suffixsize * 2);\n                    if (new_suffix_list) {\n                        c->suffixsize *= 2;\n                        c->suffixlist  = new_suffix_list;\n                    } else {\n                        STATS_LOCK();\n                        stats.malloc_fails++;\n                        STATS_UNLOCK();\n                        item_remove(it);\n                        break;\n                    }\n                  }\n\n                  suffix = do_cache_alloc(c->thread->suffix_cache);\n                  if (suffix == NULL) {\n                      STATS_LOCK();\n                      stats.malloc_fails++;\n                      STATS_UNLOCK();\n                      out_of_memory(c, \"SERVER_ERROR out of memory making CAS suffix\");\n                      item_remove(it);\n                      while (i-- > 0) {\n                          item_remove(*(c->ilist + i));\n                      }\n                      return;\n                  }\n                  *(c->suffixlist + i) = suffix;\n                  int suffix_len = make_ascii_get_suffix(suffix, it, return_cas);\n                  if (add_iov(c, \"VALUE \", 6) != 0 ||\n                      add_iov(c, ITEM_key(it), it->nkey) != 0 ||\n                      (settings.inline_ascii_response && add_iov(c, ITEM_suffix(it), it->nsuffix - 2) != 0) ||\n                      add_iov(c, suffix, suffix_len) != 0)\n                      {\n                          item_remove(it);\n                          break;\n                      }\n                  if ((it->it_flags & ITEM_CHUNKED) == 0) {\n                      add_iov(c, ITEM_data(it), it->nbytes);\n                  } else if (add_chunked_item_iovs(c, it, it->nbytes) != 0) {\n                      item_remove(it);\n                      break;\n                  }\n                }\n                else\n                {\n                  MEMCACHED_COMMAND_GET(c->sfd, ITEM_key(it), it->nkey,\n                                        it->nbytes, ITEM_get_cas(it));\n                  if (add_iov(c, \"VALUE \", 6) != 0 ||\n                      add_iov(c, ITEM_key(it), it->nkey) != 0)\n                      {\n                          item_remove(it);\n                          break;\n                      }\n                  if ((it->it_flags & ITEM_CHUNKED) == 0)\n                      {\n                          if (add_iov(c, ITEM_suffix(it), it->nsuffix + it->nbytes) != 0)\n                          {\n                              item_remove(it);\n                              break;\n                          }\n                      } else if (add_iov(c, ITEM_suffix(it), it->nsuffix) != 0 ||\n                                 add_chunked_item_iovs(c, it, it->nbytes) != 0) {\n                          item_remove(it);\n                          break;\n                      }\n                }\n\n\n                if (settings.verbose > 1) {\n                    int ii;\n                    fprintf(stderr, \">%d sending key \", c->sfd);\n                    for (ii = 0; ii < it->nkey; ++ii) {\n                        fprintf(stderr, \"%c\", key[ii]);\n                    }\n                    fprintf(stderr, \"\\n\");\n                }\n\n                /* item_get() has incremented it->refcount for us */\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.slab_stats[ITEM_clsid(it)].get_hits++;\n                c->thread->stats.get_cmds++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                *(c->ilist + i) = it;\n                i++;\n\n            } else {\n                pthread_mutex_lock(&c->thread->stats.mutex);\n                c->thread->stats.get_misses++;\n                c->thread->stats.get_cmds++;\n                pthread_mutex_unlock(&c->thread->stats.mutex);\n                MEMCACHED_COMMAND_GET(c->sfd, key, nkey, -1, 0);\n            }\n\n            key_token++;\n        }\n\n        /*\n         * If the command string hasn't been fully processed, get the next set\n         * of tokens.\n         */\n        if(key_token->value != NULL) {\n            ntokens = tokenize_command(key_token->value, tokens, MAX_TOKENS);\n            key_token = tokens;\n        }\n\n    } while(key_token->value != NULL);\n\n    c->icurr = c->ilist;\n    c->ileft = i;\n    if (return_cas || !settings.inline_ascii_response) {\n        c->suffixcurr = c->suffixlist;\n        c->suffixleft = i;\n    }\n\n    if (settings.verbose > 1)\n        fprintf(stderr, \">%d END\\n\", c->sfd);\n\n    /*\n        If the loop was terminated because of out-of-memory, it is not\n        reliable to add END\\r\\n to the buffer, because it might not end\n        in \\r\\n. So we send SERVER_ERROR instead.\n    */\n    if (key_token->value != NULL || add_iov(c, \"END\\r\\n\", 5) != 0\n        || (IS_UDP(c->transport) && build_udp_headers(c) != 0)) {\n        out_of_memory(c, \"SERVER_ERROR out of memory writing get response\");\n    }\n    else {\n        conn_set_state(c, conn_mwrite);\n        c->msgcurr = 0;\n    }\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,7 +21,7 @@\n                 return;\n             }\n \n-            it = item_get(key, nkey, c, DO_UPDATE);\n+            it = limited_get(key, nkey, c);\n             if (settings.detail_enabled) {\n                 stats_prefix_record_get(key, nkey, NULL != it);\n             }",
        "diff_line_info": {
            "deleted_lines": [
                "            it = item_get(key, nkey, c, DO_UPDATE);"
            ],
            "added_lines": [
                "            it = limited_get(key, nkey, c);"
            ]
        }
    }
]