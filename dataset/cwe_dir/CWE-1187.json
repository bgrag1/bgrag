[
    {
        "cve_id": "CVE-2017-8891",
        "func_name": "dropbox/lepton/bounded_iostream::bounded_iostream",
        "description": "Dropbox Lepton 1.2.1 allows DoS (SEGV and application crash) via a malformed lepton file because the code does not ensure setup of a correct number of threads.",
        "git_url": "https://github.com/dropbox/lepton/commit/82167c144a322cc956da45407f6dce8d4303d346",
        "commit_title": "fix #87 : always check that threads_required set up the appropriate number of threads---fire off nop functions on unused threads for consistency",
        "commit_text": "",
        "func_before": "bounded_iostream::bounded_iostream(Sirikata::DecoderWriter *w,\n                                   const std::function<void(Sirikata::DecoderWriter*, size_t)> &size_callback,\n                                   const Sirikata::JpegAllocator<uint8_t> &alloc) \n    : parent(w), err(Sirikata::JpegError::nil()) {\n    this->size_callback = size_callback;\n    buffer_position = 0;\n    byte_position = 0;\n    num_bytes_attempted_to_write = 0;\n    set_bound(0);\n}",
        "func": "bounded_iostream::bounded_iostream(Sirikata::DecoderWriter *w,\n                                   const std::function<void(Sirikata::DecoderWriter*, size_t)> &size_callback,\n                                   const Sirikata::JpegAllocator<uint8_t> &alloc) \n    : parent(w), err(Sirikata::JpegError::nil()) {\n    this->size_callback = size_callback;\n    buffer_position = 0;\n    byte_position = 0;\n    byte_bound = 0x7FFFFFFF;\n    num_bytes_attempted_to_write = 0;\n    set_bound(0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,7 @@\n     this->size_callback = size_callback;\n     buffer_position = 0;\n     byte_position = 0;\n+    byte_bound = 0x7FFFFFFF;\n     num_bytes_attempted_to_write = 0;\n     set_bound(0);\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    byte_bound = 0x7FFFFFFF;"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-8891",
        "func_name": "dropbox/lepton/bounded_iostream::write_no_buffer",
        "description": "Dropbox Lepton 1.2.1 allows DoS (SEGV and application crash) via a malformed lepton file because the code does not ensure setup of a correct number of threads.",
        "git_url": "https://github.com/dropbox/lepton/commit/82167c144a322cc956da45407f6dce8d4303d346",
        "commit_title": "fix #87 : always check that threads_required set up the appropriate number of threads---fire off nop functions on unused threads for consistency",
        "commit_text": "",
        "func_before": "unsigned int bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {\n    //return iostream::write(from,tpsize,dtsize);\n    std::pair<unsigned int, Sirikata::JpegError> retval;\n    if (byte_bound != 0 && byte_position + bytes_to_write > byte_bound) {\n        size_t real_bytes_to_write = byte_bound - byte_position;\n        byte_position += real_bytes_to_write;\n        retval = parent->Write(reinterpret_cast<const unsigned char*>(from), real_bytes_to_write);\n        if (retval.first < real_bytes_to_write) {\n            err = retval.second;\n            return retval.first;\n        }\n        return bytes_to_write; // pretend we wrote it all\n    }\n    size_t total = bytes_to_write;\n    retval = parent->Write(reinterpret_cast<const unsigned char*>(from), total);\n    unsigned int written = retval.first;\n    byte_position += written;\n    if (written < total ) {\n        err = retval.second;\n        return written;\n    }\n    return bytes_to_write;\n}",
        "func": "uint32_t bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {\n    //return iostream::write(from,tpsize,dtsize);\n    std::pair<unsigned int, Sirikata::JpegError> retval;\n    if (byte_bound != 0 && byte_position + bytes_to_write > byte_bound) {\n        size_t real_bytes_to_write = byte_bound - byte_position;\n        byte_position += real_bytes_to_write;\n        retval = parent->Write(reinterpret_cast<const unsigned char*>(from), real_bytes_to_write);\n        if (retval.first < real_bytes_to_write) {\n            err = retval.second;\n            return retval.first;\n        }\n        return bytes_to_write; // pretend we wrote it all\n    }\n    size_t total = bytes_to_write;\n    retval = parent->Write(reinterpret_cast<const unsigned char*>(from), total);\n    unsigned int written = retval.first;\n    byte_position += written;\n    if (written < total ) {\n        err = retval.second;\n        return written;\n    }\n    return bytes_to_write;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,4 +1,4 @@\n-unsigned int bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {\n+uint32_t bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {\n     //return iostream::write(from,tpsize,dtsize);\n     std::pair<unsigned int, Sirikata::JpegError> retval;\n     if (byte_bound != 0 && byte_position + bytes_to_write > byte_bound) {",
        "diff_line_info": {
            "deleted_lines": [
                "unsigned int bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {"
            ],
            "added_lines": [
                "uint32_t bounded_iostream::write_no_buffer(const void *from, size_t bytes_to_write) {"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-8891",
        "func_name": "dropbox/lepton/VP8ComponentDecoder::decode_chunk",
        "description": "Dropbox Lepton 1.2.1 allows DoS (SEGV and application crash) via a malformed lepton file because the code does not ensure setup of a correct number of threads.",
        "git_url": "https://github.com/dropbox/lepton/commit/82167c144a322cc956da45407f6dce8d4303d346",
        "commit_title": "fix #87 : always check that threads_required set up the appropriate number of threads---fire off nop functions on unused threads for consistency",
        "commit_text": "",
        "func_before": "CodingReturnValue VP8ComponentDecoder::decode_chunk(UncompressedComponents * const colldata)\n{\n    mux_splicer.init(spin_workers_);\n    /* cmpc is a global variable with the component count */\n\n\n    /* construct 4x4 VP8 blocks to hold 8x8 JPEG blocks */\n    if ( thread_state_[0] == nullptr || thread_state_[0]->context_[0].isNil() ) {\n        /* first call */\n        BlockBasedImagePerChannel<false> framebuffer;\n        framebuffer.memset(0);\n        for (size_t i = 0; i < framebuffer.size() && int( i ) < colldata->get_num_components(); ++i) {\n            framebuffer[i] = &colldata->full_component_write((BlockType)i);\n        }\n        Sirikata::Array1d<BlockBasedImagePerChannel<false>, MAX_NUM_THREADS> all_framebuffers;\n        for (size_t i = 0; i < all_framebuffers.size(); ++i) {\n            all_framebuffers[i] = framebuffer;\n        }\n        size_t num_threads_needed = initialize_decoder_state(colldata,\n                                                             all_framebuffers).size();\n\n\n        for (size_t i = 0;i < num_threads_needed; ++i) {\n            map_logical_thread_to_physical_thread(i, i);\n        }\n        for (size_t i = 0;i < num_threads_needed; ++i) {\n            initialize_thread_id(i, i, framebuffer);\n            if (!do_threading_) {\n                break;\n            }\n        }\n        if (num_threads_needed > NUM_THREADS || num_threads_needed == 0) {\n            return CODING_ERROR;\n        }\n    }\n    if (do_threading_) {\n        for (unsigned int thread_id = 0; thread_id < NUM_THREADS; ++thread_id) {\n            unsigned int cur_spin_worker = thread_id;\n            spin_workers_[cur_spin_worker].work\n                = std::bind(worker_thread,\n                            thread_state_[thread_id],\n                            thread_id,\n                            colldata,\n                            mux_splicer.thread_target,\n                            getWorker(cur_spin_worker),\n                            &send_to_actual_thread_state);\n            spin_workers_[cur_spin_worker].activate_work();\n        }\n        flush();\n        for (unsigned int thread_id = 0; thread_id < NUM_THREADS; ++thread_id) {\n            unsigned int cur_spin_worker = thread_id;\n            TimingHarness::timing[thread_id][TimingHarness::TS_THREAD_WAIT_STARTED] = TimingHarness::get_time_us();\n            spin_workers_[cur_spin_worker].main_wait_for_done();\n            TimingHarness::timing[thread_id][TimingHarness::TS_THREAD_WAIT_FINISHED] = TimingHarness::get_time_us();\n        }\n        // join on all threads\n    } else {\n        if (virtual_thread_id_ != -1) {\n            TimingHarness::timing[0][TimingHarness::TS_ARITH_STARTED] = TimingHarness::get_time_us();\n            CodingReturnValue ret = thread_state_[0]->vp8_decode_thread(0, colldata);\n            if (ret == CODING_PARTIAL) {\n                return ret;\n            }\n            TimingHarness::timing[0][TimingHarness::TS_ARITH_FINISHED] = TimingHarness::get_time_us();\n        }\n        // wait for \"threads\"\n        virtual_thread_id_ += 1; // first time's a charm\n        for (unsigned int thread_id = virtual_thread_id_; thread_id < NUM_THREADS; ++thread_id, ++virtual_thread_id_) {\n            BlockBasedImagePerChannel<false> framebuffer;\n            framebuffer.memset(0);\n            for (size_t i = 0; i < framebuffer.size() && int( i ) < colldata->get_num_components(); ++i) {\n                framebuffer[i] = &colldata->full_component_write((BlockType)i);\n            }\n\n            initialize_thread_id(thread_id, 0, framebuffer);\n            thread_state_[0]->bool_decoder_.init(new VirtualThreadPacketReader(thread_id, &mux_reader_, &mux_splicer));\n            TimingHarness::timing[thread_id][TimingHarness::TS_ARITH_STARTED] = TimingHarness::get_time_us();\n            CodingReturnValue ret;\n            if ((ret = thread_state_[0]->vp8_decode_thread(0, colldata)) == CODING_PARTIAL) {\n                return ret;\n            }\n            TimingHarness::timing[thread_id][TimingHarness::TS_ARITH_FINISHED] = TimingHarness::get_time_us();\n        }\n    }\n    TimingHarness::timing[0][TimingHarness::TS_JPEG_RECODE_STARTED] = TimingHarness::get_time_us();\n    for (int component = 0; component < colldata->get_num_components(); ++component) {\n        colldata->worker_mark_cmp_finished((BlockType)component);\n    }\n    colldata->worker_update_coefficient_position_progress( 64 );\n    colldata->worker_update_bit_progress( 16 );\n    write_byte_bill(Billing::DELIMITERS, true, mux_reader_.getOverhead());\n    return CODING_DONE;\n}",
        "func": "CodingReturnValue VP8ComponentDecoder::decode_chunk(UncompressedComponents * const colldata)\n{\n    mux_splicer.init(spin_workers_);\n    /* cmpc is a global variable with the component count */\n\n\n    /* construct 4x4 VP8 blocks to hold 8x8 JPEG blocks */\n    if ( thread_state_[0] == nullptr || thread_state_[0]->context_[0].isNil() ) {\n        /* first call */\n        BlockBasedImagePerChannel<false> framebuffer;\n        framebuffer.memset(0);\n        for (size_t i = 0; i < framebuffer.size() && int( i ) < colldata->get_num_components(); ++i) {\n            framebuffer[i] = &colldata->full_component_write((BlockType)i);\n        }\n        Sirikata::Array1d<BlockBasedImagePerChannel<false>, MAX_NUM_THREADS> all_framebuffers;\n        for (size_t i = 0; i < all_framebuffers.size(); ++i) {\n            all_framebuffers[i] = framebuffer;\n        }\n        size_t num_threads_needed = initialize_decoder_state(colldata,\n                                                             all_framebuffers).size();\n\n\n        for (size_t i = 0;i < num_threads_needed; ++i) {\n            map_logical_thread_to_physical_thread(i, i);\n        }\n        for (size_t i = 0;i < num_threads_needed; ++i) {\n            initialize_thread_id(i, i, framebuffer);\n            if (!do_threading_) {\n                break;\n            }\n        }\n        if (num_threads_needed > NUM_THREADS || num_threads_needed == 0) {\n            return CODING_ERROR;\n        }\n    }\n    if (do_threading_) {\n        for (unsigned int thread_id = 0; thread_id < NUM_THREADS; ++thread_id) {\n            unsigned int cur_spin_worker = thread_id;\n            if (!thread_state_[thread_id]) {\n                spin_workers_[cur_spin_worker].work\n                    = &nop;\n            } else {\n                spin_workers_[cur_spin_worker].work\n                    = std::bind(worker_thread,\n                                thread_state_[thread_id],\n                                thread_id,\n                                colldata,\n                                mux_splicer.thread_target,\n                                getWorker(cur_spin_worker),\n                                &send_to_actual_thread_state);\n            }\n            spin_workers_[cur_spin_worker].activate_work();\n        }\n        flush();\n        for (unsigned int thread_id = 0; thread_id < NUM_THREADS; ++thread_id) {\n            unsigned int cur_spin_worker = thread_id;\n            TimingHarness::timing[thread_id][TimingHarness::TS_THREAD_WAIT_STARTED] = TimingHarness::get_time_us();\n            spin_workers_[cur_spin_worker].main_wait_for_done();\n            TimingHarness::timing[thread_id][TimingHarness::TS_THREAD_WAIT_FINISHED] = TimingHarness::get_time_us();\n        }\n        // join on all threads\n    } else {\n        if (virtual_thread_id_ != -1) {\n            TimingHarness::timing[0][TimingHarness::TS_ARITH_STARTED] = TimingHarness::get_time_us();\n            CodingReturnValue ret = thread_state_[0]->vp8_decode_thread(0, colldata);\n            if (ret == CODING_PARTIAL) {\n                return ret;\n            }\n            TimingHarness::timing[0][TimingHarness::TS_ARITH_FINISHED] = TimingHarness::get_time_us();\n        }\n        // wait for \"threads\"\n        virtual_thread_id_ += 1; // first time's a charm\n        for (unsigned int thread_id = virtual_thread_id_; thread_id < NUM_THREADS; ++thread_id, ++virtual_thread_id_) {\n            BlockBasedImagePerChannel<false> framebuffer;\n            framebuffer.memset(0);\n            for (size_t i = 0; i < framebuffer.size() && int( i ) < colldata->get_num_components(); ++i) {\n                framebuffer[i] = &colldata->full_component_write((BlockType)i);\n            }\n\n            initialize_thread_id(thread_id, 0, framebuffer);\n            thread_state_[0]->bool_decoder_.init(new VirtualThreadPacketReader(thread_id, &mux_reader_, &mux_splicer));\n            TimingHarness::timing[thread_id][TimingHarness::TS_ARITH_STARTED] = TimingHarness::get_time_us();\n            CodingReturnValue ret;\n            if ((ret = thread_state_[0]->vp8_decode_thread(0, colldata)) == CODING_PARTIAL) {\n                return ret;\n            }\n            TimingHarness::timing[thread_id][TimingHarness::TS_ARITH_FINISHED] = TimingHarness::get_time_us();\n        }\n    }\n    TimingHarness::timing[0][TimingHarness::TS_JPEG_RECODE_STARTED] = TimingHarness::get_time_us();\n    for (int component = 0; component < colldata->get_num_components(); ++component) {\n        colldata->worker_mark_cmp_finished((BlockType)component);\n    }\n    colldata->worker_update_coefficient_position_progress( 64 );\n    colldata->worker_update_bit_progress( 16 );\n    write_byte_bill(Billing::DELIMITERS, true, mux_reader_.getOverhead());\n    return CODING_DONE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -36,14 +36,19 @@\n     if (do_threading_) {\n         for (unsigned int thread_id = 0; thread_id < NUM_THREADS; ++thread_id) {\n             unsigned int cur_spin_worker = thread_id;\n-            spin_workers_[cur_spin_worker].work\n-                = std::bind(worker_thread,\n-                            thread_state_[thread_id],\n-                            thread_id,\n-                            colldata,\n-                            mux_splicer.thread_target,\n-                            getWorker(cur_spin_worker),\n-                            &send_to_actual_thread_state);\n+            if (!thread_state_[thread_id]) {\n+                spin_workers_[cur_spin_worker].work\n+                    = &nop;\n+            } else {\n+                spin_workers_[cur_spin_worker].work\n+                    = std::bind(worker_thread,\n+                                thread_state_[thread_id],\n+                                thread_id,\n+                                colldata,\n+                                mux_splicer.thread_target,\n+                                getWorker(cur_spin_worker),\n+                                &send_to_actual_thread_state);\n+            }\n             spin_workers_[cur_spin_worker].activate_work();\n         }\n         flush();",
        "diff_line_info": {
            "deleted_lines": [
                "            spin_workers_[cur_spin_worker].work",
                "                = std::bind(worker_thread,",
                "                            thread_state_[thread_id],",
                "                            thread_id,",
                "                            colldata,",
                "                            mux_splicer.thread_target,",
                "                            getWorker(cur_spin_worker),",
                "                            &send_to_actual_thread_state);"
            ],
            "added_lines": [
                "            if (!thread_state_[thread_id]) {",
                "                spin_workers_[cur_spin_worker].work",
                "                    = &nop;",
                "            } else {",
                "                spin_workers_[cur_spin_worker].work",
                "                    = std::bind(worker_thread,",
                "                                thread_state_[thread_id],",
                "                                thread_id,",
                "                                colldata,",
                "                                mux_splicer.thread_target,",
                "                                getWorker(cur_spin_worker),",
                "                                &send_to_actual_thread_state);",
                "            }"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-8891",
        "func_name": "dropbox/lepton/LeptonCodec::ThreadState::vp8_decode_thread",
        "description": "Dropbox Lepton 1.2.1 allows DoS (SEGV and application crash) via a malformed lepton file because the code does not ensure setup of a correct number of threads.",
        "git_url": "https://github.com/dropbox/lepton/commit/82167c144a322cc956da45407f6dce8d4303d346",
        "commit_title": "fix #87 : always check that threads_required set up the appropriate number of threads---fire off nop functions on unused threads for consistency",
        "commit_text": "",
        "func_before": "CodingReturnValue LeptonCodec::ThreadState::vp8_decode_thread(unsigned int thread_id,\n                                                              UncompressedComponents *const colldata) {\n    Sirikata::Array1d<uint32_t, (uint32_t)ColorChannel::NumBlockTypes> component_size_in_blocks;\n    BlockBasedImagePerChannel<false> image_data;\n    for (int i = 0; i < colldata->get_num_components(); ++i) {\n        component_size_in_blocks[i] = colldata->component_size_in_blocks(i);\n        image_data[i] = &colldata->full_component_write((BlockType)i);\n    }\n    Sirikata::Array1d<uint32_t,\n                      (size_t)ColorChannel::NumBlockTypes> max_coded_heights\n        = colldata->get_max_coded_heights();\n    /* deserialize each block in planar order */\n\n    dev_assert(luma_splits_.size() == 2); // not ready to do multiple work items on a thread yet\n    int min_y = luma_splits_[0];\n    int max_y = luma_splits_[1];\n    while(true) {\n        RowSpec cur_row = row_spec_from_index(decode_index_++, image_data, colldata->get_mcu_count_vertical(), max_coded_heights);\n        if (cur_row.done) {\n            break;\n        }\n        if (cur_row.luma_y >= max_y && thread_id + 1 != NUM_THREADS) {\n            break;\n        }\n        if (cur_row.skip) {\n            continue;\n        }\n        if (cur_row.luma_y < min_y) {\n            continue;\n        }\n        decode_rowf(image_data,\n                   component_size_in_blocks,\n                   cur_row.component,\n                   cur_row.curr_y);\n        if (thread_id == 0) {\n            colldata->worker_update_cmp_progress((BlockType)cur_row.component,\n                                                 image_data[cur_row.component]->block_width() );\n        }\n        return CODING_PARTIAL;\n    }\n    return CODING_DONE;\n}",
        "func": "CodingReturnValue LeptonCodec::ThreadState::vp8_decode_thread(unsigned int thread_id,\n                                                              UncompressedComponents *const colldata) {\n    Sirikata::Array1d<uint32_t, (uint32_t)ColorChannel::NumBlockTypes> component_size_in_blocks;\n    BlockBasedImagePerChannel<false> image_data;\n    for (int i = 0; i < colldata->get_num_components(); ++i) {\n        component_size_in_blocks[i] = colldata->component_size_in_blocks(i);\n        image_data[i] = &colldata->full_component_write((BlockType)i);\n    }\n    Sirikata::Array1d<uint32_t,\n                      (size_t)ColorChannel::NumBlockTypes> max_coded_heights\n        = colldata->get_max_coded_heights();\n    /* deserialize each block in planar order */\n\n    dev_assert(luma_splits_.size() == 2); // not ready to do multiple work items on a thread yet\n    always_assert(luma_splits_.size() >= 2);\n    int min_y = luma_splits_[0];\n    int max_y = luma_splits_[1];\n    while(true) {\n        RowSpec cur_row = row_spec_from_index(decode_index_++, image_data, colldata->get_mcu_count_vertical(), max_coded_heights);\n        if (cur_row.done) {\n            break;\n        }\n        if (cur_row.luma_y >= max_y && thread_id + 1 != NUM_THREADS) {\n            break;\n        }\n        if (cur_row.skip) {\n            continue;\n        }\n        if (cur_row.luma_y < min_y) {\n            continue;\n        }\n        decode_rowf(image_data,\n                   component_size_in_blocks,\n                   cur_row.component,\n                   cur_row.curr_y);\n        if (thread_id == 0) {\n            colldata->worker_update_cmp_progress((BlockType)cur_row.component,\n                                                 image_data[cur_row.component]->block_width() );\n        }\n        return CODING_PARTIAL;\n    }\n    return CODING_DONE;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,6 +12,7 @@\n     /* deserialize each block in planar order */\n \n     dev_assert(luma_splits_.size() == 2); // not ready to do multiple work items on a thread yet\n+    always_assert(luma_splits_.size() >= 2);\n     int min_y = luma_splits_[0];\n     int max_y = luma_splits_[1];\n     while(true) {",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    always_assert(luma_splits_.size() >= 2);"
            ]
        }
    },
    {
        "cve_id": "CVE-2017-8891",
        "func_name": "dropbox/lepton/memcpy_ro",
        "description": "Dropbox Lepton 1.2.1 allows DoS (SEGV and application crash) via a malformed lepton file because the code does not ensure setup of a correct number of threads.",
        "git_url": "https://github.com/dropbox/lepton/commit/82167c144a322cc956da45407f6dce8d4303d346",
        "commit_title": "fix #87 : always check that threads_required set up the appropriate number of threads---fire off nop functions on unused threads for consistency",
        "commit_text": "",
        "func_before": "void memcpy_ro(uint8_t *dest, size_t size) const {\n        if ((ptrdiff_t)size < rope[0].second-rope[0].first) {\n            memcpy(dest, rope[0].first, size);\n            return;\n        }\n        size_t del = rope[0].second-rope[0].first;\n        memcpy(dest, rope[0].first, del);\n        dest += del;\n        size -=del;\n        if (size) {\n            always_assert(rope[1].second - rope[1].first >= (ptrdiff_t)size);\n            memcpy(dest, rope[1].first, size);\n        }\n    }",
        "func": "void memcpy_ro(uint8_t *dest, size_t size) const {\n        if ((ptrdiff_t)size < rope[0].second-rope[0].first) {\n            memcpy(dest, rope[0].first, size);\n            return;\n        }\n        size_t del = rope[0].second-rope[0].first;\n        if (del) {\n            memcpy(dest, rope[0].first, del);\n        }\n        dest += del;\n        size -=del;\n        if (size) {\n            always_assert(rope[1].second - rope[1].first >= (ptrdiff_t)size);\n            memcpy(dest, rope[1].first, size);\n        }\n    }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -4,7 +4,9 @@\n             return;\n         }\n         size_t del = rope[0].second-rope[0].first;\n-        memcpy(dest, rope[0].first, del);\n+        if (del) {\n+            memcpy(dest, rope[0].first, del);\n+        }\n         dest += del;\n         size -=del;\n         if (size) {",
        "diff_line_info": {
            "deleted_lines": [
                "        memcpy(dest, rope[0].first, del);"
            ],
            "added_lines": [
                "        if (del) {",
                "            memcpy(dest, rope[0].first, del);",
                "        }"
            ]
        }
    }
]