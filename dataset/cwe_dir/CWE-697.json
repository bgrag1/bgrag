[
    {
        "cve_id": "CVE-2022-36148",
        "func_name": "nu774/fdkaac/wav_parse",
        "description": "fdkaac commit 53fe239 was discovered to contain a floating point exception (FPE) via wav_open at /src/wav_reader.c.",
        "git_url": "https://github.com/nu774/fdkaac/commit/4ec1422bd951a137225ffa4052da120e2ab0a0f4",
        "commit_title": "wav/caf parser: ensure fmt/desc chunk",
        "commit_text": " fixes https://github.com/nu774/fdkaac/issues/52",
        "func_before": "static\nint wav_parse(wav_reader_t *reader, int64_t *data_length)\n{\n    uint32_t container, fcc, chunk_size;\n\n    *data_length = 0;\n    container = riff_next_chunk(reader, &chunk_size);\n    ENSURE(container == RIFF_FOURCC('R','I','F','F') ||\n           container == RIFF_FOURCC('R','F','6','4'));\n    TRY_IO(pcm_read32le(&reader->io, &fcc));\n    ENSURE(fcc == RIFF_FOURCC('W','A','V','E'));\n\n    if (container == RIFF_FOURCC('R','F','6','4'))\n        riff_ds64(reader, data_length);\n    while ((fcc = riff_next_chunk(reader, &chunk_size)) != 0) {\n        if (fcc == RIFF_FOURCC('f','m','t',' ')) {\n            if (wav_fmt(reader, chunk_size) < 0)\n                goto FAIL;\n        } else if (fcc == RIFF_FOURCC('d','a','t','a')) {\n            if (container == RIFF_FOURCC('R','I','F','F'))\n                *data_length = chunk_size;\n            reader->data_offset = pcm_tell(&reader->io);\n            break;\n        } else {\n            TRY_IO(pcm_skip(&reader->io, (chunk_size + 1) & ~1));\n        }\n    }\n    if (fcc == RIFF_FOURCC('d','a','t','a'))\n        return 0;\nFAIL:\n    return -1;\n}",
        "func": "static\nint wav_parse(wav_reader_t *reader, int64_t *data_length)\n{\n    uint32_t container, fcc, chunk_size;\n    int fmt_seen = 0;\n\n    *data_length = 0;\n    container = riff_next_chunk(reader, &chunk_size);\n    ENSURE(container == RIFF_FOURCC('R','I','F','F') ||\n           container == RIFF_FOURCC('R','F','6','4'));\n    TRY_IO(pcm_read32le(&reader->io, &fcc));\n    ENSURE(fcc == RIFF_FOURCC('W','A','V','E'));\n\n    if (container == RIFF_FOURCC('R','F','6','4'))\n        riff_ds64(reader, data_length);\n    while ((fcc = riff_next_chunk(reader, &chunk_size)) != 0) {\n        if (fcc == RIFF_FOURCC('f','m','t',' ')) {\n            fmt_seen = 1;\n            if (wav_fmt(reader, chunk_size) < 0)\n                goto FAIL;\n        } else if (fcc == RIFF_FOURCC('d','a','t','a')) {\n            if (container == RIFF_FOURCC('R','I','F','F'))\n                *data_length = chunk_size;\n            reader->data_offset = pcm_tell(&reader->io);\n            break;\n        } else {\n            TRY_IO(pcm_skip(&reader->io, (chunk_size + 1) & ~1));\n        }\n    }\n    ENSURE(fmt_seen && fcc == RIFF_FOURCC('d', 'a', 't', 'a'));\n    return 0;\nFAIL:\n    return -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n int wav_parse(wav_reader_t *reader, int64_t *data_length)\n {\n     uint32_t container, fcc, chunk_size;\n+    int fmt_seen = 0;\n \n     *data_length = 0;\n     container = riff_next_chunk(reader, &chunk_size);\n@@ -14,6 +15,7 @@\n         riff_ds64(reader, data_length);\n     while ((fcc = riff_next_chunk(reader, &chunk_size)) != 0) {\n         if (fcc == RIFF_FOURCC('f','m','t',' ')) {\n+            fmt_seen = 1;\n             if (wav_fmt(reader, chunk_size) < 0)\n                 goto FAIL;\n         } else if (fcc == RIFF_FOURCC('d','a','t','a')) {\n@@ -25,8 +27,8 @@\n             TRY_IO(pcm_skip(&reader->io, (chunk_size + 1) & ~1));\n         }\n     }\n-    if (fcc == RIFF_FOURCC('d','a','t','a'))\n-        return 0;\n+    ENSURE(fmt_seen && fcc == RIFF_FOURCC('d', 'a', 't', 'a'));\n+    return 0;\n FAIL:\n     return -1;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    if (fcc == RIFF_FOURCC('d','a','t','a'))",
                "        return 0;"
            ],
            "added_lines": [
                "    int fmt_seen = 0;",
                "            fmt_seen = 1;",
                "    ENSURE(fmt_seen && fcc == RIFF_FOURCC('d', 'a', 't', 'a'));",
                "    return 0;"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-36148",
        "func_name": "nu774/fdkaac/caf_parse",
        "description": "fdkaac commit 53fe239 was discovered to contain a floating point exception (FPE) via wav_open at /src/wav_reader.c.",
        "git_url": "https://github.com/nu774/fdkaac/commit/4ec1422bd951a137225ffa4052da120e2ab0a0f4",
        "commit_title": "wav/caf parser: ensure fmt/desc chunk",
        "commit_text": " fixes https://github.com/nu774/fdkaac/issues/52",
        "func_before": "static\nint caf_parse(caf_reader_t *reader, int64_t *data_length)\n{\n    uint32_t fcc;\n    int64_t chunk_size;\n\n    *data_length = 0;\n\n    /* CAFFileHeader */\n    TRY_IO(pcm_read32be(&reader->io, &fcc));\n    ENSURE(fcc == M4AF_FOURCC('c','a','f','f'));\n    TRY_IO(pcm_skip(&reader->io, 4)); /* mFileVersion, mFileFlags */\n\n    while ((fcc = caf_next_chunk(reader, &chunk_size)) != 0) {\n        if (fcc == M4AF_FOURCC('d','e','s','c'))\n            TRY_IO(caf_desc(reader, chunk_size));\n        else if (fcc == M4AF_FOURCC('i','n','f','o'))\n            TRY_IO(caf_info(reader, chunk_size));\n        else if (fcc == M4AF_FOURCC('c','h','a','n')) {\n            ENSURE(reader->sample_format.channels_per_frame);\n            if (apple_chan_chunk(&reader->io, chunk_size,\n                                 &reader->sample_format, reader->chanmap) < 0)\n                goto FAIL;\n        } else if (fcc == M4AF_FOURCC('d','a','t','a')) {\n            TRY_IO(pcm_skip(&reader->io, 4)); /* mEditCount */\n            *data_length = (chunk_size == ~0ULL) ? chunk_size : chunk_size - 4;\n            reader->data_offset = pcm_tell(&reader->io);\n            break;\n        } else\n            TRY_IO(pcm_skip(&reader->io, chunk_size));\n    }\n    ENSURE(reader->sample_format.channels_per_frame);\n    ENSURE(fcc == M4AF_FOURCC('d','a','t','a'));\n    return 0;\nFAIL:\n    return -1;\n}",
        "func": "static\nint caf_parse(caf_reader_t *reader, int64_t *data_length)\n{\n    uint32_t fcc;\n    int64_t chunk_size;\n    int desc_seen = 0;\n\n    *data_length = 0;\n\n    /* CAFFileHeader */\n    TRY_IO(pcm_read32be(&reader->io, &fcc));\n    ENSURE(fcc == M4AF_FOURCC('c','a','f','f'));\n    TRY_IO(pcm_skip(&reader->io, 4)); /* mFileVersion, mFileFlags */\n\n    while ((fcc = caf_next_chunk(reader, &chunk_size)) != 0) {\n        if (fcc == M4AF_FOURCC('d','e','s','c')) {\n            desc_seen = 1;\n            TRY_IO(caf_desc(reader, chunk_size));\n        } else if (fcc == M4AF_FOURCC('i','n','f','o'))\n            TRY_IO(caf_info(reader, chunk_size));\n        else if (fcc == M4AF_FOURCC('c','h','a','n')) {\n            ENSURE(reader->sample_format.channels_per_frame);\n            if (apple_chan_chunk(&reader->io, chunk_size,\n                                 &reader->sample_format, reader->chanmap) < 0)\n                goto FAIL;\n        } else if (fcc == M4AF_FOURCC('d','a','t','a')) {\n            TRY_IO(pcm_skip(&reader->io, 4)); /* mEditCount */\n            *data_length = (chunk_size == ~0ULL) ? chunk_size : chunk_size - 4;\n            reader->data_offset = pcm_tell(&reader->io);\n            break;\n        } else\n            TRY_IO(pcm_skip(&reader->io, chunk_size));\n    }\n    ENSURE(reader->sample_format.channels_per_frame);\n    ENSURE(desc_seen && fcc == M4AF_FOURCC('d','a','t','a'));\n    return 0;\nFAIL:\n    return -1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,6 +3,7 @@\n {\n     uint32_t fcc;\n     int64_t chunk_size;\n+    int desc_seen = 0;\n \n     *data_length = 0;\n \n@@ -12,9 +13,10 @@\n     TRY_IO(pcm_skip(&reader->io, 4)); /* mFileVersion, mFileFlags */\n \n     while ((fcc = caf_next_chunk(reader, &chunk_size)) != 0) {\n-        if (fcc == M4AF_FOURCC('d','e','s','c'))\n+        if (fcc == M4AF_FOURCC('d','e','s','c')) {\n+            desc_seen = 1;\n             TRY_IO(caf_desc(reader, chunk_size));\n-        else if (fcc == M4AF_FOURCC('i','n','f','o'))\n+        } else if (fcc == M4AF_FOURCC('i','n','f','o'))\n             TRY_IO(caf_info(reader, chunk_size));\n         else if (fcc == M4AF_FOURCC('c','h','a','n')) {\n             ENSURE(reader->sample_format.channels_per_frame);\n@@ -30,7 +32,7 @@\n             TRY_IO(pcm_skip(&reader->io, chunk_size));\n     }\n     ENSURE(reader->sample_format.channels_per_frame);\n-    ENSURE(fcc == M4AF_FOURCC('d','a','t','a'));\n+    ENSURE(desc_seen && fcc == M4AF_FOURCC('d','a','t','a'));\n     return 0;\n FAIL:\n     return -1;",
        "diff_line_info": {
            "deleted_lines": [
                "        if (fcc == M4AF_FOURCC('d','e','s','c'))",
                "        else if (fcc == M4AF_FOURCC('i','n','f','o'))",
                "    ENSURE(fcc == M4AF_FOURCC('d','a','t','a'));"
            ],
            "added_lines": [
                "    int desc_seen = 0;",
                "        if (fcc == M4AF_FOURCC('d','e','s','c')) {",
                "            desc_seen = 1;",
                "        } else if (fcc == M4AF_FOURCC('i','n','f','o'))",
                "    ENSURE(desc_seen && fcc == M4AF_FOURCC('d','a','t','a'));"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26691",
        "func_name": "OpenPrinting/cups/ctcompare",
        "description": "A logic issue was addressed with improved state management. This issue is fixed in Security Update 2022-003 Catalina, macOS Monterey 12.3, macOS Big Sur 11.6.5. An application may be able to gain elevated privileges.",
        "git_url": "https://github.com/OpenPrinting/cups/commit/de4f8c196106033e4c372dce3e91b9d42b0b9444",
        "commit_title": "scheduler/cert.c: Fix string comparison (fixes CVE-2022-26691)",
        "commit_text": " The previous algorithm didn't expect the strings can have a different length, so one string can be a substring of the other and such substring was reported as equal to the longer string.",
        "func_before": "static int\t\t\t\t/* O - 0 on match, non-zero on non-match */\nctcompare(const char *a,\t\t/* I - First string */\n          const char *b)\t\t/* I - Second string */\n{\n  int\tresult = 0;\t\t\t/* Result */\n\n\n  while (*a && *b)\n  {\n    result |= *a ^ *b;\n    a ++;\n    b ++;\n  }\n\n  return (result);\n}",
        "func": "static int\t\t\t\t/* O - 0 on match, non-zero on non-match */\nctcompare(const char *a,\t\t/* I - First string */\n          const char *b)\t\t/* I - Second string */\n{\n  int\tresult = 0;\t\t\t/* Result */\n\n\n  while (*a && *b)\n  {\n    result |= *a ^ *b;\n    a ++;\n    b ++;\n  }\n\n /*\n  * The while loop finishes when *a == '\\0' or *b == '\\0'\n  * so after the while loop either both *a and *b == '\\0',\n  * or one points inside a string, so when we apply logical OR on *a,\n  * *b and result, we get a non-zero return value if the compared strings don't match.\n  */\n\n  return (result | *a | *b);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -12,5 +12,12 @@\n     b ++;\n   }\n \n-  return (result);\n+ /*\n+  * The while loop finishes when *a == '\\0' or *b == '\\0'\n+  * so after the while loop either both *a and *b == '\\0',\n+  * or one points inside a string, so when we apply logical OR on *a,\n+  * *b and result, we get a non-zero return value if the compared strings don't match.\n+  */\n+\n+  return (result | *a | *b);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "  return (result);"
            ],
            "added_lines": [
                " /*",
                "  * The while loop finishes when *a == '\\0' or *b == '\\0'",
                "  * so after the while loop either both *a and *b == '\\0',",
                "  * or one points inside a string, so when we apply logical OR on *a,",
                "  * *b and result, we get a non-zero return value if the compared strings don't match.",
                "  */",
                "",
                "  return (result | *a | *b);"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25666",
        "func_name": "tensorflow/SpectrogramShapeFn",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, there is a floating point exception in AudioSpectrogram. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/d0d4e779da0d0f56499c6fa5ba09f0a576cc6b14",
        "commit_title": "Fix audio spectrogram FPE.",
        "commit_text": " Do input validation in shape function.  PiperOrigin-RevId: 503481241",
        "func_before": "Status SpectrogramShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &input));\n  int32_t window_size;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"window_size\", &window_size));\n  int32_t stride;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"stride\", &stride));\n\n  DimensionHandle input_length = c->Dim(input, 0);\n  DimensionHandle input_channels = c->Dim(input, 1);\n\n  DimensionHandle output_length;\n  if (!c->ValueKnown(input_length)) {\n    output_length = c->UnknownDim();\n  } else {\n    const int64_t input_length_value = c->Value(input_length);\n    const int64_t length_minus_window = (input_length_value - window_size);\n    int64_t output_length_value;\n    if (length_minus_window < 0) {\n      output_length_value = 0;\n    } else {\n      output_length_value = 1 + (length_minus_window / stride);\n    }\n    output_length = c->MakeDim(output_length_value);\n  }\n\n  DimensionHandle output_channels =\n      c->MakeDim(1 + NextPowerOfTwo(window_size) / 2);\n  c->set_output(0,\n                c->MakeShape({input_channels, output_length, output_channels}));\n  return OkStatus();\n}",
        "func": "Status SpectrogramShapeFn(InferenceContext* c) {\n  ShapeHandle input;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &input));\n  int32_t window_size;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"window_size\", &window_size));\n  if (window_size <= 1) {\n    return errors::InvalidArgument(\"window size must be > 1, got \",\n                                   window_size);\n  }\n\n  int32_t stride;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"stride\", &stride));\n  if (stride <= 0) {\n    return errors::InvalidArgument(\"stride must be strictly positive, got \",\n                                   stride);\n  }\n\n  DimensionHandle input_length = c->Dim(input, 0);\n  DimensionHandle input_channels = c->Dim(input, 1);\n\n  DimensionHandle output_length;\n  if (!c->ValueKnown(input_length)) {\n    output_length = c->UnknownDim();\n  } else {\n    const int64_t input_length_value = c->Value(input_length);\n    const int64_t length_minus_window = (input_length_value - window_size);\n    int64_t output_length_value;\n    if (length_minus_window < 0) {\n      output_length_value = 0;\n    } else {\n      output_length_value = 1 + (length_minus_window / stride);\n    }\n    output_length = c->MakeDim(output_length_value);\n  }\n\n  DimensionHandle output_channels =\n      c->MakeDim(1 + NextPowerOfTwo(window_size) / 2);\n  c->set_output(0,\n                c->MakeShape({input_channels, output_length, output_channels}));\n  return OkStatus();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -3,8 +3,17 @@\n   TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &input));\n   int32_t window_size;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"window_size\", &window_size));\n+  if (window_size <= 1) {\n+    return errors::InvalidArgument(\"window size must be > 1, got \",\n+                                   window_size);\n+  }\n+\n   int32_t stride;\n   TF_RETURN_IF_ERROR(c->GetAttr(\"stride\", &stride));\n+  if (stride <= 0) {\n+    return errors::InvalidArgument(\"stride must be strictly positive, got \",\n+                                   stride);\n+  }\n \n   DimensionHandle input_length = c->Dim(input, 0);\n   DimensionHandle input_channels = c->Dim(input, 1);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  if (window_size <= 1) {",
                "    return errors::InvalidArgument(\"window size must be > 1, got \",",
                "                                   window_size);",
                "  }",
                "",
                "  if (stride <= 0) {",
                "    return errors::InvalidArgument(\"stride must be strictly positive, got \",",
                "                                   stride);",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25669",
        "func_name": "tensorflow/GetStride",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, if the stride and window size are not positive for `tf.raw_ops.AvgPoolGrad`, it can give a floating point exception. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
        "commit_title": "[tf2xla] Validate that stride and window size are positive",
        "commit_text": " PiperOrigin-RevId: 504866231",
        "func_before": "StatusOr<std::vector<int64_t>> GetStride(XlaOpKernelContext* ctx) {\n    if (ctx->num_inputs() == 1) {\n      return stride_;\n    }\n    const TensorShape stride_shape = ctx->InputShape(2);\n    // Validate input sizes.\n    if (!TensorShapeUtils::IsVector(stride_shape)) {\n      return errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                     stride_shape.DebugString());\n    }\n    if (stride_shape.num_elements() != num_dims()) {\n      return errors::InvalidArgument(\n          \"Sliding window stride field must \"\n          \"specify \",\n          num_dims(), \" dimensions\");\n    }\n    std::vector<int64_t> stride;\n    auto status = ctx->ConstantInputAsIntVector(2, &stride);\n    if (!status.ok()) {\n      return status;\n    }\n    return stride;\n  }",
        "func": "StatusOr<std::vector<int64_t>> GetStride(XlaOpKernelContext* ctx) {\n    std::vector<int64_t> stride;\n    if (ctx->num_inputs() == 1) {\n      stride = stride_;\n    } else {\n      const TensorShape stride_shape = ctx->InputShape(2);\n      // Validate input sizes.\n      if (!TensorShapeUtils::IsVector(stride_shape)) {\n        return errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                       stride_shape.DebugString());\n      }\n      if (stride_shape.num_elements() != num_dims()) {\n        return errors::InvalidArgument(\n            \"Sliding window stride field must \"\n            \"specify \",\n            num_dims(), \" dimensions\");\n      }\n      auto status = ctx->ConstantInputAsIntVector(2, &stride);\n      if (!status.ok()) {\n        return status;\n      }\n    }\n    TF_RETURN_IF_ERROR(ValidateStrides(stride));\n    return stride;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,23 +1,25 @@\n StatusOr<std::vector<int64_t>> GetStride(XlaOpKernelContext* ctx) {\n+    std::vector<int64_t> stride;\n     if (ctx->num_inputs() == 1) {\n-      return stride_;\n+      stride = stride_;\n+    } else {\n+      const TensorShape stride_shape = ctx->InputShape(2);\n+      // Validate input sizes.\n+      if (!TensorShapeUtils::IsVector(stride_shape)) {\n+        return errors::InvalidArgument(\"stride must be a vector, not shape \",\n+                                       stride_shape.DebugString());\n+      }\n+      if (stride_shape.num_elements() != num_dims()) {\n+        return errors::InvalidArgument(\n+            \"Sliding window stride field must \"\n+            \"specify \",\n+            num_dims(), \" dimensions\");\n+      }\n+      auto status = ctx->ConstantInputAsIntVector(2, &stride);\n+      if (!status.ok()) {\n+        return status;\n+      }\n     }\n-    const TensorShape stride_shape = ctx->InputShape(2);\n-    // Validate input sizes.\n-    if (!TensorShapeUtils::IsVector(stride_shape)) {\n-      return errors::InvalidArgument(\"stride must be a vector, not shape \",\n-                                     stride_shape.DebugString());\n-    }\n-    if (stride_shape.num_elements() != num_dims()) {\n-      return errors::InvalidArgument(\n-          \"Sliding window stride field must \"\n-          \"specify \",\n-          num_dims(), \" dimensions\");\n-    }\n-    std::vector<int64_t> stride;\n-    auto status = ctx->ConstantInputAsIntVector(2, &stride);\n-    if (!status.ok()) {\n-      return status;\n-    }\n+    TF_RETURN_IF_ERROR(ValidateStrides(stride));\n     return stride;\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "      return stride_;",
                "    const TensorShape stride_shape = ctx->InputShape(2);",
                "    // Validate input sizes.",
                "    if (!TensorShapeUtils::IsVector(stride_shape)) {",
                "      return errors::InvalidArgument(\"stride must be a vector, not shape \",",
                "                                     stride_shape.DebugString());",
                "    }",
                "    if (stride_shape.num_elements() != num_dims()) {",
                "      return errors::InvalidArgument(",
                "          \"Sliding window stride field must \"",
                "          \"specify \",",
                "          num_dims(), \" dimensions\");",
                "    }",
                "    std::vector<int64_t> stride;",
                "    auto status = ctx->ConstantInputAsIntVector(2, &stride);",
                "    if (!status.ok()) {",
                "      return status;",
                "    }"
            ],
            "added_lines": [
                "    std::vector<int64_t> stride;",
                "      stride = stride_;",
                "    } else {",
                "      const TensorShape stride_shape = ctx->InputShape(2);",
                "      // Validate input sizes.",
                "      if (!TensorShapeUtils::IsVector(stride_shape)) {",
                "        return errors::InvalidArgument(\"stride must be a vector, not shape \",",
                "                                       stride_shape.DebugString());",
                "      }",
                "      if (stride_shape.num_elements() != num_dims()) {",
                "        return errors::InvalidArgument(",
                "            \"Sliding window stride field must \"",
                "            \"specify \",",
                "            num_dims(), \" dimensions\");",
                "      }",
                "      auto status = ctx->ConstantInputAsIntVector(2, &stride);",
                "      if (!status.ok()) {",
                "        return status;",
                "      }",
                "    TF_RETURN_IF_ERROR(ValidateStrides(stride));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25669",
        "func_name": "tensorflow/Compile",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, if the stride and window size are not positive for `tf.raw_ops.AvgPoolGrad`, it can give a floating point exception. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
        "commit_title": "[tf2xla] Validate that stride and window size are positive",
        "commit_text": " PiperOrigin-RevId: 504866231",
        "func_before": "void Compile(XlaOpKernelContext* ctx) override {\n    if (ctx->num_inputs() != 3) {\n      OP_REQUIRES(\n          ctx, ctx->num_inputs() == 5,\n          errors::InvalidArgument(\"Must supply ksize and stride arguments.\"));\n      const TensorShape ksize_shape = ctx->InputShape(3);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(ksize_shape),\n                  errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                          ksize_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(3, &ksize_));\n\n      const TensorShape stride_shape = ctx->InputShape(4);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(stride_shape),\n                  errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                          stride_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(4, &stride_));\n    }\n\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n\n    const TensorShape tensor_in_shape = ctx->InputShape(0);\n    const TensorShape tensor_out_shape = ctx->InputShape(1);\n    const TensorShape out_backprop_shape = ctx->InputShape(2);\n\n    // For maxpooling, tensor_in should have num_dims() dimensions.\n    OP_REQUIRES(ctx, tensor_in_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_in must be \", num_dims(),\n                                        \"-dimensional\"));\n    OP_REQUIRES(ctx, tensor_out_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_out must be \", num_dims(),\n                                        \"-dimensional\"));\n    // For maxpooling, out_backprop should have num_dims() dimensions.\n    OP_REQUIRES(ctx, out_backprop_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"out_backprop must be \", num_dims(),\n                                        \"-dimensional\"));\n\n    // What we want to compute:\n    // Given y = MaxPool(x), and xs_grad = MaxPoolGrad(x, y, ys_grad)\n    // MaxPoolGradGrad computes {ys_grad}_grad given x, y, and {xs_grad}_grad.\n    //\n    // In the regular TF op, this amounts to selecting for each window the\n    // incoming backprop value from xs_grad_grad that corresponds to the maximal\n    // value in the corresponding window of x.\n    //\n    // TODO(b/73062247): What we really want is a ReduceWindow with different\n    // arrays for index selection vs return value selection--a select-to-gather.\n    //\n    // Here, we implement a bitwise hack: we use the hi 16 bits of input for\n    // separate max pooling alongside each of the hi and lo 16 bits of\n    // out_backprop packed into 16 lo bits, which we then glue back together at\n    // the end to get a full 32 bits of gradient.\n    //\n    // This could select the wrong backprop value for two x values that are\n    // equally maximal up to the first 16 bits, in which case we are taking the\n    // latter.\n    //\n    // Note that in principle we could use 32 separate maxpools to recover each\n    // of 32 bits of the gradient while preserving 31 bits of input for the max\n    // pooling criteria; here, we just truncate to the first 16 bits of input.\n\n    auto input = ctx->Input(0);\n    auto out_backprop = ctx->Input(2);\n\n    auto b = ctx->builder();\n\n    auto sixteen = xla::ConstantR0<uint32>(b, 16);\n    // in (f32) -> round to 7 mantissa bits (bf16)-> 16-high-bit u32.\n    //\n    // NOTE: Use a ReducePrecision operation instead of a cast to BF16 and back\n    // to F32 since the XLA compiler may ignore narrowing casts to floating\n    // point types if the debug option xla_allow_excess_precision is set.\n    auto in_hi = xla::BitcastConvertType(\n        xla::ReducePrecision(input, /*exponent_bits=*/8, /*mantissa_bits=*/7),\n        xla::U32);\n    auto bp_int = xla::BitcastConvertType(out_backprop, xla::U32);\n    auto bp_hi = xla::ShiftRightLogical(bp_int, sixteen);\n    auto bp_lo =\n        xla::ShiftRightLogical(xla::ShiftLeft(bp_int, sixteen), sixteen);\n    auto in_hi_bp_hi = xla::Add(in_hi, bp_hi);  // Want an unsigned add.\n    auto in_hi_bp_lo = xla::Add(in_hi, bp_lo);  // Want an unsigned add.\n\n    auto init_value = xla::MinValue(b, xla::F32);\n    // We will reduce by taking the maximal value up to 16 bits (ignoring the lo\n    // 16 bits of packed-in hi/lo backprop value).\n    auto rb = b->CreateSubBuilder(\"GreaterOrEqOf_ByFirst16Bits\");\n    {\n      // F32 parameters to satisfy lowering type restriction for reduce opcode.\n      const xla::Shape scalar = xla::ShapeUtil::MakeShape(xla::F32, {});\n      auto lhs = xla::Parameter(rb.get(), 0, scalar, \"lhs\");\n      auto rhs = xla::Parameter(rb.get(), 1, scalar, \"rhs\");\n      auto sixteen = xla::ConstantR0<int32>(rb.get(), 16);\n      auto lhs_criteria =\n          xla::ShiftLeft(xla::ShiftRightLogical(\n                             xla::BitcastConvertType(lhs, xla::S32), sixteen),\n                         sixteen);\n      auto rhs_criteria =\n          xla::ShiftLeft(xla::ShiftRightLogical(\n                             xla::BitcastConvertType(rhs, xla::S32), sixteen),\n                         sixteen);\n      // Must use a F32 comparison, because S32 would not work for negatives.\n      xla::Select(xla::Ge(xla::BitcastConvertType(lhs_criteria, xla::F32),\n                          xla::BitcastConvertType(rhs_criteria, xla::F32)),\n                  lhs, rhs);\n    }\n    auto reduce = rb->BuildAndNoteError();\n    xla::Padding xla_padding =\n        (padding_ == VALID) ? xla::Padding::kValid : xla::Padding::kSame;\n    auto pooled_hi =\n        xla::ReduceWindow(xla::BitcastConvertType(in_hi_bp_hi, xla::F32),\n                          init_value, reduce, ksize_, stride_, xla_padding);\n    auto pooled_lo =\n        xla::ReduceWindow(xla::BitcastConvertType(in_hi_bp_lo, xla::F32),\n                          init_value, reduce, ksize_, stride_, xla_padding);\n    auto grads_hi =\n        xla::ShiftLeft(xla::BitcastConvertType(pooled_hi, xla::U32), sixteen);\n    auto grads_lo = xla::ShiftRightLogical(\n        xla::ShiftLeft(xla::BitcastConvertType(pooled_lo, xla::U32), sixteen),\n        sixteen);\n    auto grads = xla::Add(grads_hi, grads_lo);  // Want an unsigned add.\n\n    xla::PrimitiveType element_type;\n    OP_REQUIRES_OK(ctx, DataTypeToPrimitiveType(input_type(2), &element_type));\n    ctx->SetOutput(0, xla::BitcastConvertType(grads, element_type));\n  }",
        "func": "void Compile(XlaOpKernelContext* ctx) override {\n    if (ctx->num_inputs() != 3) {\n      OP_REQUIRES(\n          ctx, ctx->num_inputs() == 5,\n          errors::InvalidArgument(\"Must supply ksize and stride arguments.\"));\n      const TensorShape ksize_shape = ctx->InputShape(3);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(ksize_shape),\n                  errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                          ksize_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(3, &ksize_));\n\n      const TensorShape stride_shape = ctx->InputShape(4);\n      // Validate input sizes.\n      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(stride_shape),\n                  errors::InvalidArgument(\"stride must be a vector, not shape \",\n                                          stride_shape.DebugString()));\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(4, &stride_));\n    }\n\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n\n    const TensorShape tensor_in_shape = ctx->InputShape(0);\n    const TensorShape tensor_out_shape = ctx->InputShape(1);\n    const TensorShape out_backprop_shape = ctx->InputShape(2);\n\n    // For maxpooling, tensor_in should have num_dims() dimensions.\n    OP_REQUIRES(ctx, tensor_in_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_in must be \", num_dims(),\n                                        \"-dimensional\"));\n    OP_REQUIRES(ctx, tensor_out_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"tensor_out must be \", num_dims(),\n                                        \"-dimensional\"));\n    // For maxpooling, out_backprop should have num_dims() dimensions.\n    OP_REQUIRES(ctx, out_backprop_shape.dims() == num_dims(),\n                errors::InvalidArgument(\"out_backprop must be \", num_dims(),\n                                        \"-dimensional\"));\n\n    // What we want to compute:\n    // Given y = MaxPool(x), and xs_grad = MaxPoolGrad(x, y, ys_grad)\n    // MaxPoolGradGrad computes {ys_grad}_grad given x, y, and {xs_grad}_grad.\n    //\n    // In the regular TF op, this amounts to selecting for each window the\n    // incoming backprop value from xs_grad_grad that corresponds to the maximal\n    // value in the corresponding window of x.\n    //\n    // TODO(b/73062247): What we really want is a ReduceWindow with different\n    // arrays for index selection vs return value selection--a select-to-gather.\n    //\n    // Here, we implement a bitwise hack: we use the hi 16 bits of input for\n    // separate max pooling alongside each of the hi and lo 16 bits of\n    // out_backprop packed into 16 lo bits, which we then glue back together at\n    // the end to get a full 32 bits of gradient.\n    //\n    // This could select the wrong backprop value for two x values that are\n    // equally maximal up to the first 16 bits, in which case we are taking the\n    // latter.\n    //\n    // Note that in principle we could use 32 separate maxpools to recover each\n    // of 32 bits of the gradient while preserving 31 bits of input for the max\n    // pooling criteria; here, we just truncate to the first 16 bits of input.\n\n    auto input = ctx->Input(0);\n    auto out_backprop = ctx->Input(2);\n\n    auto b = ctx->builder();\n\n    auto sixteen = xla::ConstantR0<uint32>(b, 16);\n    // in (f32) -> round to 7 mantissa bits (bf16)-> 16-high-bit u32.\n    //\n    // NOTE: Use a ReducePrecision operation instead of a cast to BF16 and back\n    // to F32 since the XLA compiler may ignore narrowing casts to floating\n    // point types if the debug option xla_allow_excess_precision is set.\n    auto in_hi = xla::BitcastConvertType(\n        xla::ReducePrecision(input, /*exponent_bits=*/8, /*mantissa_bits=*/7),\n        xla::U32);\n    auto bp_int = xla::BitcastConvertType(out_backprop, xla::U32);\n    auto bp_hi = xla::ShiftRightLogical(bp_int, sixteen);\n    auto bp_lo =\n        xla::ShiftRightLogical(xla::ShiftLeft(bp_int, sixteen), sixteen);\n    auto in_hi_bp_hi = xla::Add(in_hi, bp_hi);  // Want an unsigned add.\n    auto in_hi_bp_lo = xla::Add(in_hi, bp_lo);  // Want an unsigned add.\n\n    auto init_value = xla::MinValue(b, xla::F32);\n    // We will reduce by taking the maximal value up to 16 bits (ignoring the lo\n    // 16 bits of packed-in hi/lo backprop value).\n    auto rb = b->CreateSubBuilder(\"GreaterOrEqOf_ByFirst16Bits\");\n    {\n      // F32 parameters to satisfy lowering type restriction for reduce opcode.\n      const xla::Shape scalar = xla::ShapeUtil::MakeShape(xla::F32, {});\n      auto lhs = xla::Parameter(rb.get(), 0, scalar, \"lhs\");\n      auto rhs = xla::Parameter(rb.get(), 1, scalar, \"rhs\");\n      auto sixteen = xla::ConstantR0<int32>(rb.get(), 16);\n      auto lhs_criteria =\n          xla::ShiftLeft(xla::ShiftRightLogical(\n                             xla::BitcastConvertType(lhs, xla::S32), sixteen),\n                         sixteen);\n      auto rhs_criteria =\n          xla::ShiftLeft(xla::ShiftRightLogical(\n                             xla::BitcastConvertType(rhs, xla::S32), sixteen),\n                         sixteen);\n      // Must use a F32 comparison, because S32 would not work for negatives.\n      xla::Select(xla::Ge(xla::BitcastConvertType(lhs_criteria, xla::F32),\n                          xla::BitcastConvertType(rhs_criteria, xla::F32)),\n                  lhs, rhs);\n    }\n    auto reduce = rb->BuildAndNoteError();\n    xla::Padding xla_padding =\n        (padding_ == VALID) ? xla::Padding::kValid : xla::Padding::kSame;\n    auto pooled_hi =\n        xla::ReduceWindow(xla::BitcastConvertType(in_hi_bp_hi, xla::F32),\n                          init_value, reduce, ksize_, stride_, xla_padding);\n    auto pooled_lo =\n        xla::ReduceWindow(xla::BitcastConvertType(in_hi_bp_lo, xla::F32),\n                          init_value, reduce, ksize_, stride_, xla_padding);\n    auto grads_hi =\n        xla::ShiftLeft(xla::BitcastConvertType(pooled_hi, xla::U32), sixteen);\n    auto grads_lo = xla::ShiftRightLogical(\n        xla::ShiftLeft(xla::BitcastConvertType(pooled_lo, xla::U32), sixteen),\n        sixteen);\n    auto grads = xla::Add(grads_hi, grads_lo);  // Want an unsigned add.\n\n    xla::PrimitiveType element_type;\n    OP_REQUIRES_OK(ctx, DataTypeToPrimitiveType(input_type(2), &element_type));\n    ctx->SetOutput(0, xla::BitcastConvertType(grads, element_type));\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,10 +22,12 @@\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n     OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n \n     const TensorShape tensor_in_shape = ctx->InputShape(0);\n     const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
                "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25669",
        "func_name": "tensorflow/GetKernelSize",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, if the stride and window size are not positive for `tf.raw_ops.AvgPoolGrad`, it can give a floating point exception. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
        "commit_title": "[tf2xla] Validate that stride and window size are positive",
        "commit_text": " PiperOrigin-RevId: 504866231",
        "func_before": "StatusOr<std::vector<int64_t>> GetKernelSize(XlaOpKernelContext* ctx) {\n    if (ctx->num_inputs() == 1) {\n      return ksize_;\n    }\n    const TensorShape ksize_shape = ctx->InputShape(1);\n    // Validate input sizes.\n    if (!TensorShapeUtils::IsVector(ksize_shape)) {\n      return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                     ksize_shape.DebugString());\n    }\n    if (ksize_shape.num_elements() != num_dims()) {\n      return errors::InvalidArgument(\n          \"Sliding window ksize field must \"\n          \"specify \",\n          num_dims(), \" dimensions\");\n    }\n    std::vector<int64_t> ksize;\n    auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n    if (!status.ok()) {\n      return status;\n    }\n    return ksize;\n  }",
        "func": "StatusOr<std::vector<int64_t>> GetKernelSize(XlaOpKernelContext* ctx) {\n    std::vector<int64_t> ksize;\n    if (ctx->num_inputs() == 1) {\n      ksize = ksize_;\n    } else {\n      const TensorShape ksize_shape = ctx->InputShape(1);\n      // Validate input sizes.\n      if (!TensorShapeUtils::IsVector(ksize_shape)) {\n        return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n                                       ksize_shape.DebugString());\n      }\n      if (ksize_shape.num_elements() != num_dims()) {\n        return errors::InvalidArgument(\n            \"Sliding window ksize field must \"\n            \"specify \",\n            num_dims(), \" dimensions\");\n      }\n      auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n      if (!status.ok()) {\n        return status;\n      }\n    }\n    TF_RETURN_IF_ERROR(ValidateKernelSizes(ksize));\n    return ksize;\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,23 +1,25 @@\n StatusOr<std::vector<int64_t>> GetKernelSize(XlaOpKernelContext* ctx) {\n+    std::vector<int64_t> ksize;\n     if (ctx->num_inputs() == 1) {\n-      return ksize_;\n+      ksize = ksize_;\n+    } else {\n+      const TensorShape ksize_shape = ctx->InputShape(1);\n+      // Validate input sizes.\n+      if (!TensorShapeUtils::IsVector(ksize_shape)) {\n+        return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n+                                       ksize_shape.DebugString());\n+      }\n+      if (ksize_shape.num_elements() != num_dims()) {\n+        return errors::InvalidArgument(\n+            \"Sliding window ksize field must \"\n+            \"specify \",\n+            num_dims(), \" dimensions\");\n+      }\n+      auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n+      if (!status.ok()) {\n+        return status;\n+      }\n     }\n-    const TensorShape ksize_shape = ctx->InputShape(1);\n-    // Validate input sizes.\n-    if (!TensorShapeUtils::IsVector(ksize_shape)) {\n-      return errors::InvalidArgument(\"ksize must be a vector, not shape \",\n-                                     ksize_shape.DebugString());\n-    }\n-    if (ksize_shape.num_elements() != num_dims()) {\n-      return errors::InvalidArgument(\n-          \"Sliding window ksize field must \"\n-          \"specify \",\n-          num_dims(), \" dimensions\");\n-    }\n-    std::vector<int64_t> ksize;\n-    auto status = ctx->ConstantInputAsIntVector(1, &ksize);\n-    if (!status.ok()) {\n-      return status;\n-    }\n+    TF_RETURN_IF_ERROR(ValidateKernelSizes(ksize));\n     return ksize;\n   }",
        "diff_line_info": {
            "deleted_lines": [
                "      return ksize_;",
                "    const TensorShape ksize_shape = ctx->InputShape(1);",
                "    // Validate input sizes.",
                "    if (!TensorShapeUtils::IsVector(ksize_shape)) {",
                "      return errors::InvalidArgument(\"ksize must be a vector, not shape \",",
                "                                     ksize_shape.DebugString());",
                "    }",
                "    if (ksize_shape.num_elements() != num_dims()) {",
                "      return errors::InvalidArgument(",
                "          \"Sliding window ksize field must \"",
                "          \"specify \",",
                "          num_dims(), \" dimensions\");",
                "    }",
                "    std::vector<int64_t> ksize;",
                "    auto status = ctx->ConstantInputAsIntVector(1, &ksize);",
                "    if (!status.ok()) {",
                "      return status;",
                "    }"
            ],
            "added_lines": [
                "    std::vector<int64_t> ksize;",
                "      ksize = ksize_;",
                "    } else {",
                "      const TensorShape ksize_shape = ctx->InputShape(1);",
                "      // Validate input sizes.",
                "      if (!TensorShapeUtils::IsVector(ksize_shape)) {",
                "        return errors::InvalidArgument(\"ksize must be a vector, not shape \",",
                "                                       ksize_shape.DebugString());",
                "      }",
                "      if (ksize_shape.num_elements() != num_dims()) {",
                "        return errors::InvalidArgument(",
                "            \"Sliding window ksize field must \"",
                "            \"specify \",",
                "            num_dims(), \" dimensions\");",
                "      }",
                "      auto status = ctx->ConstantInputAsIntVector(1, &ksize);",
                "      if (!status.ok()) {",
                "        return status;",
                "      }",
                "    TF_RETURN_IF_ERROR(ValidateKernelSizes(ksize));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25669",
        "func_name": "tensorflow/AvgPoolGradOp",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, if the stride and window size are not positive for `tf.raw_ops.AvgPoolGrad`, it can give a floating point exception. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
        "commit_title": "[tf2xla] Validate that stride and window size are positive",
        "commit_text": " PiperOrigin-RevId: 504866231",
        "func_before": "AvgPoolGradOp(OpKernelConstruction* ctx, int num_spatial_dims)\n      : XlaOpKernel(ctx), num_spatial_dims_(num_spatial_dims) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(ctx, padding_ != EXPLICIT,\n                errors::Unimplemented(\n                    \"XLA does not support avgpoolgrad with explicit padding.\"));\n    OP_REQUIRES(ctx, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n\n    string data_format;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(ctx, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n  }",
        "func": "AvgPoolGradOp(OpKernelConstruction* ctx, int num_spatial_dims)\n      : XlaOpKernel(ctx), num_spatial_dims_(num_spatial_dims) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(ctx, ksize_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify \",\n                                        num_dims(), \" dimensions\"));\n    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(ctx, padding_ != EXPLICIT,\n                errors::Unimplemented(\n                    \"XLA does not support avgpoolgrad with explicit padding.\"));\n    OP_REQUIRES(ctx, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n\n    string data_format;\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(ctx, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,11 +5,13 @@\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"strides\", &stride_));\n     OP_REQUIRES(ctx, stride_.size() == num_dims(),\n                 errors::InvalidArgument(\"Sliding window strides field must \"\n                                         \"specify \",\n                                         num_dims(), \" dimensions\"));\n+    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"padding\", &padding_));\n     OP_REQUIRES(ctx, padding_ != EXPLICIT,\n                 errors::Unimplemented(",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
                "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25669",
        "func_name": "tensorflow/ValidatePaddingValues",
        "description": "TensorFlow is an open source platform for machine learning. Prior to versions 2.12.0 and 2.11.1, if the stride and window size are not positive for `tf.raw_ops.AvgPoolGrad`, it can give a floating point exception. A fix is included in TensorFlow version 2.12.0 and version 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
        "commit_title": "[tf2xla] Validate that stride and window size are positive",
        "commit_text": " PiperOrigin-RevId: 504866231",
        "func_before": "Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n                             absl::Span<const int64_t> window_dimensions,\n                             absl::Span<const int64_t> window_strides) {\n  bool ok = input_dimensions.size() == window_dimensions.size() &&\n            input_dimensions.size() == window_strides.size();\n  if (!ok) {\n    return InvalidArgument(\n        \"Want input dimensions size %u = window dimensions size %u = window \"\n        \"strides size %u\",\n        input_dimensions.size(), window_dimensions.size(),\n        window_strides.size());\n  }\n  return OkStatus();\n}",
        "func": "Status ValidatePaddingValues(absl::Span<const int64_t> input_dimensions,\n                             absl::Span<const int64_t> window_dimensions,\n                             absl::Span<const int64_t> window_strides) {\n  bool ok = input_dimensions.size() == window_dimensions.size() &&\n            input_dimensions.size() == window_strides.size();\n  if (!ok) {\n    return InvalidArgument(\n        \"Want input dimensions size %u = window dimensions size %u = window \"\n        \"strides size %u\",\n        input_dimensions.size(), window_dimensions.size(),\n        window_strides.size());\n  }\n  for (size_t i = 0; i < input_dimensions.size(); ++i) {\n    if (window_dimensions[i] <= 0) {\n      return InvalidArgument(\"Window dimension %u has non-positive size %d\", i,\n                             window_dimensions[i]);\n    }\n    if (window_strides[i] <= 0) {\n      return InvalidArgument(\"Window dimension %u has non-positive stride %d\",\n                             i, window_strides[i]);\n    }\n  }\n  return OkStatus();\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,5 +10,15 @@\n         input_dimensions.size(), window_dimensions.size(),\n         window_strides.size());\n   }\n+  for (size_t i = 0; i < input_dimensions.size(); ++i) {\n+    if (window_dimensions[i] <= 0) {\n+      return InvalidArgument(\"Window dimension %u has non-positive size %d\", i,\n+                             window_dimensions[i]);\n+    }\n+    if (window_strides[i] <= 0) {\n+      return InvalidArgument(\"Window dimension %u has non-positive stride %d\",\n+                             i, window_strides[i]);\n+    }\n+  }\n   return OkStatus();\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  for (size_t i = 0; i < input_dimensions.size(); ++i) {",
                "    if (window_dimensions[i] <= 0) {",
                "      return InvalidArgument(\"Window dimension %u has non-positive size %d\", i,",
                "                             window_dimensions[i]);",
                "    }",
                "    if (window_strides[i] <= 0) {",
                "      return InvalidArgument(\"Window dimension %u has non-positive stride %d\",",
                "                             i, window_strides[i]);",
                "    }",
                "  }"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-25675",
        "func_name": "tensorflow/Compile",
        "description": "TensorFlow is an open source machine learning platform. When running versions prior to 2.12.0 and 2.11.1 with XLA, `tf.raw_ops.Bincount` segfaults when given a parameter `weights` that is neither the same shape as parameter `arr` nor a length-0 tensor. A fix is included in TensorFlow 2.12.0 and 2.11.1.",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/8ae76cf085f4be26295d2ecf2081e759e04b8acf",
        "commit_title": "[Tensorflow] Fix security vulnerability with DenseBincountOp",
        "commit_text": " PiperOrigin-RevId: 506514542",
        "func_before": "void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.value();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.value();\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.value();\n    OP_REQUIRES(ctx,\n                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n                                                              input_shape) ||\n                    (weights_shape.dimensions_size() > 0 &&\n                     weights_shape.dimensions(0) == 0),\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    weights_shape.DebugString()));\n\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }",
        "func": "void Compile(XlaOpKernelContext* ctx) override {\n    int64_t output_size;\n    xla::XlaOp output_size_param = ctx->Input(\"size\");\n    StatusOr<xla::Shape> output_shape_or =\n        ctx->builder()->GetShape(output_size_param);\n    OP_REQUIRES_OK(ctx, output_shape_or.status());\n    auto output_shape_param = output_shape_or.value();\n    auto output_rank = output_shape_param.rank();\n    OP_REQUIRES(ctx, output_rank == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        output_rank));\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntScalar(\"size\", &output_size));\n    OP_REQUIRES(ctx, output_size >= 0,\n                errors::InvalidArgument(\"size (\", output_size,\n                                        \") must be non-negative\"));\n    xla::XlaOp idx, updates, output;\n    xla::XlaOp input = ctx->Input(0);\n    auto input_xla_type = ctx->input_xla_type(0);\n    xla::PrimitiveType dtype = ctx->InputXlaType(\"weights\");\n    auto zero = xla::Zero(ctx->builder(), dtype);\n    auto one = xla::One(ctx->builder(), dtype);\n    StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n    OP_REQUIRES_OK(ctx, input_shape_or.status());\n    auto input_shape = input_shape_or.value();\n\n    auto rank = input_shape.rank();\n\n    OP_REQUIRES(ctx, rank <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", rank));\n    xla::XlaOp weights = ctx->Input(2);\n    StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n\n    OP_REQUIRES_OK(ctx, weights_shape_or.status());\n\n    auto weights_shape = weights_shape_or.value();\n    OP_REQUIRES(ctx,\n                xla::ShapeUtil::CompatibleIgnoringElementType(weights_shape,\n                                                              input_shape) ||\n                    (weights_shape.dimensions_size() > 0 &&\n                     weights_shape.dimensions(0) == 0),\n                errors::InvalidArgument(\n                    \"`weights` must be the same shape as `arr` or a length-0 \"\n                    \"`Tensor`, in which case it acts as all weights equal to \"\n                    \"1. Received \",\n                    weights_shape.DebugString()));\n\n    auto size = input_shape.dimensions(0);\n\n    if (!size) {\n      output = xla::Broadcast(zero, {output_size});\n      ctx->SetOutput(0, output);\n      return;\n    }\n\n    auto weights_size = weights_shape.dimensions(0);\n    bool has_weights = false;\n    if (weights_size) {\n      has_weights = true;\n    }\n\n    xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n    xla::ScatterDimensionNumbers scatter_dnums;\n    scatter_dnums.set_index_vector_dim(1);\n    scatter_dnums.add_inserted_window_dims(0);\n    scatter_dnums.add_scatter_dims_to_operand_dims(0);\n\n    if (rank == 2) {\n      output_shape = xla::ShapeUtil::MakeShape(dtype, {size, output_size});\n      scatter_dnums.add_inserted_window_dims(1);\n      scatter_dnums.add_scatter_dims_to_operand_dims(1);\n      auto i_shape =\n          xla::ShapeUtil::MakeShape(input_xla_type, {input_shape.dimensions()});\n      auto i = xla::Iota(ctx->builder(), i_shape, 0);\n      i = xla::Reshape(\n          i, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      auto j = xla::Reshape(\n          input, {input_shape.dimensions(0) * input_shape.dimensions(1), 1});\n      std::vector<xla::XlaOp> iotas_to_concat;\n      iotas_to_concat.push_back(i);\n      iotas_to_concat.push_back(j);\n      idx = xla::ConcatInDim(ctx->builder(), iotas_to_concat, 1);\n      updates = xla::Broadcast(\n          one, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n      output = xla::Broadcast(\n          zero, {output_shape.dimensions(0), output_shape.dimensions(1)});\n      if (has_weights && !binary_output_) {\n        weights = xla::Reshape(\n            weights, {input_shape.dimensions(0) * input_shape.dimensions(1)});\n        updates = weights;\n      }\n    } else {\n      input = xla::Reshape(input, {size, 1});\n      idx = xla::Reshape(input, {size, 1});\n      updates = xla::Broadcast(one, {size});\n      output = xla::Broadcast(zero, {output_size});\n      if (has_weights && !binary_output_) {\n        updates = weights;\n      }\n    }\n\n    xla::XlaComputation assn_computation = [&] {\n      std::unique_ptr<xla::XlaBuilder> subb =\n          ctx->builder()->CreateSubBuilder(\"scatter_bincount\");\n      xla::Shape param_shape = xla::ShapeUtil::MakeShape(dtype, {});\n      auto p0 = xla::Parameter(subb.get(), 0, param_shape, \"p0\");\n      auto p1 = xla::Parameter(subb.get(), 1, param_shape, \"p1\");\n      if (!binary_output_) {\n        xla::Add(p0, p1);\n      }\n      return subb->BuildAndNoteError();\n    }();\n    output = xla::Scatter(output, idx, updates, assn_computation, scatter_dnums,\n                          false, false);\n    ctx->SetOutput(0, output);\n  }",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,21 +22,15 @@\n     StatusOr<xla::Shape> input_shape_or = ctx->builder()->GetShape(input);\n     OP_REQUIRES_OK(ctx, input_shape_or.status());\n     auto input_shape = input_shape_or.value();\n-    auto size = input_shape.dimensions(0);\n \n-    if (!size) {\n-      output = xla::Broadcast(zero, {output_size});\n-      ctx->SetOutput(0, output);\n-      return;\n-    }\n     auto rank = input_shape.rank();\n \n     OP_REQUIRES(ctx, rank <= 2,\n                 errors::InvalidArgument(\n                     \"Shape must be at most rank 2 but is rank \", rank));\n-\n     xla::XlaOp weights = ctx->Input(2);\n     StatusOr<xla::Shape> weights_shape_or = ctx->builder()->GetShape(weights);\n+\n     OP_REQUIRES_OK(ctx, weights_shape_or.status());\n \n     auto weights_shape = weights_shape_or.value();\n@@ -51,11 +45,20 @@\n                     \"1. Received \",\n                     weights_shape.DebugString()));\n \n+    auto size = input_shape.dimensions(0);\n+\n+    if (!size) {\n+      output = xla::Broadcast(zero, {output_size});\n+      ctx->SetOutput(0, output);\n+      return;\n+    }\n+\n     auto weights_size = weights_shape.dimensions(0);\n     bool has_weights = false;\n     if (weights_size) {\n       has_weights = true;\n     }\n+\n     xla::Shape output_shape = xla::ShapeUtil::MakeShape(dtype, {output_size});\n     xla::ScatterDimensionNumbers scatter_dnums;\n     scatter_dnums.set_index_vector_dim(1);",
        "diff_line_info": {
            "deleted_lines": [
                "    auto size = input_shape.dimensions(0);",
                "    if (!size) {",
                "      output = xla::Broadcast(zero, {output_size});",
                "      ctx->SetOutput(0, output);",
                "      return;",
                "    }",
                ""
            ],
            "added_lines": [
                "",
                "    auto size = input_shape.dimensions(0);",
                "",
                "    if (!size) {",
                "      output = xla::Broadcast(zero, {output_size});",
                "      ctx->SetOutput(0, output);",
                "      return;",
                "    }",
                "",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2023-27579",
        "func_name": "tensorflow/Prepare",
        "description": "TensorFlow is an end-to-end open source platform for machine learning. Constructing a tflite model with a paramater `filter_input_channel` of less than 1 gives a FPE. This issue has been patched in version 2.12. TensorFlow will also cherrypick the fix commit on TensorFlow 2.11.1.\n",
        "git_url": "https://github.com/tensorflow/tensorflow/commit/34f8368c535253f5c9cb3a303297743b62442aaa",
        "commit_title": "Check filter_input_channel > 0 in conv kernel.",
        "commit_text": " PiperOrigin-RevId: 503266928",
        "func_before": "TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n                     TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = node->inputs->size == 3;\n  // Check number of inputs/outputs\n  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n\n  // Check dimensionality of input, filter\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);\n  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);\n  // Check input channels matching filter\n  // Filter input channel can be a factor of channels of input (grouped conv)\n  // or equals (normal conv).\n  auto input_channel = input->dims->data[3];\n  auto filter_input_channel = filter->dims->data[3];\n  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);\n  data->groups = input_channel / filter_input_channel;\n\n  // Check types. (We assume that UINT8 refers to quantized tensors)\n  TfLiteType input_type = input->type;\n  TF_LITE_ENSURE(context,\n                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||\n                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);\n\n  if (input_type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  }\n  // Filter must have zero zero-points in per-channel quantization.\n  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {\n      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);\n    }\n  }\n\n  const TfLiteTensor* bias = nullptr;\n\n  // TODO(ahentz): At this point the optimized versions require 'bias'. We can\n  // either change that or document that convolution requires it.\n  TF_LITE_ENSURE(context, has_bias);\n\n  if (has_bias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));\n    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (input_type == kTfLiteInt16) {\n      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||\n                                  (bias->type == kTfLiteInt64));\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));\n  }\n\n  const bool is_hybrid =\n      (input->type == kTfLiteFloat32 &&\n       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));\n\n  if (is_hybrid && filter->type == kTfLiteInt8 &&\n      filter->quantization.type == kTfLiteAffineQuantization &&\n      filter->quantization.params &&\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)\n          ->scale &&\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)\n              ->scale->size > 1) {\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    const float scale = affine_quantization->scale->data[0];\n    for (int i = 1; i < affine_quantization->scale->size; i++) {\n      if (affine_quantization->scale->data[i] != scale) {\n        data->is_hybrid_per_channel = true;\n        break;\n      }\n    }\n  }\n\n  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and\n  // is incompatible with mutable input filters that might change between evals.\n  data->supports_multithreaded_kernel =\n      (kernel_type == kMultithreadOptimized) &&\n      (context->recommended_num_threads != 1) && !is_hybrid &&\n      (params->dilation_width_factor == 1) &&\n      (params->dilation_height_factor == 1) &&\n      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);\n\n  int channels_in = filter->dims->data[3];\n  int channels_out = filter->dims->data[0];\n  int width = input->dims->data[2];\n  int height = input->dims->data[1];\n  int filter_width = filter->dims->data[2];\n  int filter_height = filter->dims->data[1];\n  int batches = input->dims->data[0];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  size_t im2col_type_size;\n  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));\n  // Note that we intentionally promote the first multiplicand (i.e. 'batches')\n  // to 'size_t' to avoid integer overflow here.\n  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *\n                              out_width * channels_in * filter_height *\n                              filter_width * im2col_type_size;\n  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(\n      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,\n      im2col_bytes));\n\n  TF_LITE_ENSURE(context, has_bias);\n\n  // Note that full fixed-point inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (input_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, filter, bias, output, params->activation,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  auto output_status = context->ResizeTensor(context, output, output_size);\n\n  if (output_status != kTfLiteOk) return output_status;\n\n  if (data->need_im2col) {\n    node->temporaries->data[data->im2col_index] = data->im2col_id;\n\n    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);\n\n    auto filter_input_channel = filter->dims->data[3];\n    im2col_size->data[0] = output_size->data[0];\n    im2col_size->data[1] = output_size->data[1];\n    im2col_size->data[2] = output_size->data[2];\n    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;\n\n    TfLiteTensor* im2col =\n        &context->tensors[node->temporaries->data[data->im2col_index]];\n    im2col->type = input->type;\n    if (is_hybrid) {\n      im2col->type = filter->type;\n    }\n    im2col->allocation_type = kTfLiteArenaRw;\n    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);\n    if (im2col_status != kTfLiteOk) return im2col_status;\n  }\n\n  if (data->need_hwcn_weights) {\n    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;\n    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);\n\n    // Because we're treating the filter weights as a matrix when we do the\n    // transpose, we allocate the buffer with a two-dimensional shape, where one\n    // dimension is the number of elements in each filter, and the second is the\n    // total number of filters.\n    auto filter_input_channel = filter->dims->data[3];\n    hwcn_weights_size->data[0] =\n        (filter_height * filter_width * filter_input_channel);\n    hwcn_weights_size->data[1] = channels_out;\n\n    TfLiteTensor* hwcn_weights =\n        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];\n    hwcn_weights->type = input_type;\n    hwcn_weights->name = \"Conv_hwcn_weights\";\n    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;\n\n    auto hwcn_weights_status =\n        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);\n    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;\n\n    // TODO(petewarden): If Resize() is called when the size hasn't actually\n    // changed, this will do extra redundant work.\n    data->have_weights_been_transposed = false;\n  }\n\n  if (is_hybrid) {\n    node->temporaries->data[data->input_quantized_index] =\n        data->input_quantized_id;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->input_quantized_index,\n                                  &input_quantized));\n    input_quantized->type = kTfLiteInt8;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n\n    node->temporaries->data[data->scaling_factors_index] =\n        data->scaling_factors_id;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n                                  &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    // Only one scale factor per batch is typically necessary. See optimized\n    // implementation for why we need to allocate for the height of the inputs\n    // flattened to 2D.\n    TF_LITE_ENSURE(context, channels_in != 0);\n    const int height = NumElements(input) / channels_in;\n    int scaling_dims[1] = {height};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = height;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n\n    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;\n    TfLiteTensor* accum_scratch;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, data->accum_scratch_index,\n                                       &accum_scratch));\n    accum_scratch->type = kTfLiteInt32;\n    accum_scratch->allocation_type = kTfLiteArenaRw;\n    const int scratch_width = batches * out_height * out_width;\n    int accum_scratch_dims[2] = {channels_out, scratch_width};\n    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,\n                                   accum_scratch_dims)) {\n      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);\n      accum_scratch_size->data[0] = channels_out;\n      accum_scratch_size->data[1] = scratch_width;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,\n                                                       accum_scratch_size));\n    }\n\n    if (data->is_hybrid_per_channel) {\n      const auto* affine_quantization =\n          reinterpret_cast<TfLiteAffineQuantization*>(\n              filter->quantization.params);\n      TF_LITE_ENSURE(context, affine_quantization);\n      TF_LITE_ENSURE(context, affine_quantization->scale);\n      TF_LITE_ENSURE_EQ(\n          context, affine_quantization->scale->size,\n          filter->dims->data[affine_quantization->quantized_dimension]);\n      node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n      TfLiteTensor* input_offsets;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->input_offset_index,\n                                    &input_offsets));\n      input_offsets->type = kTfLiteInt32;\n      input_offsets->allocation_type = kTfLiteArenaRw;\n      // See above comment for the need to allocate for height of inputs.\n      TF_LITE_ENSURE(context, channels_in != 0);\n      const int height = NumElements(input) / channels_in;\n      const int input_offset_dims[1] = {height};\n      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,\n                                     input_offset_dims)) {\n        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);\n        input_offsets_size->data[0] = input_offset_dims[0];\n        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,\n                                                         input_offsets_size));\n      }\n      node->temporaries->data[data->row_sums_index] = data->row_sums_id;\n      TfLiteTensor* row_sums;\n      TF_LITE_ENSURE_OK(\n          context,\n          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));\n      row_sums->type = kTfLiteInt32;\n      row_sums->name = \"Conv_row_sums\";\n      row_sums->allocation_type = kTfLiteArenaRwPersistent;\n      // See above comment for the need to allocate for height of inputs.\n      const int row_sums_dims[1] = {channels_out};\n      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {\n        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n        row_sums_size->data[0] = row_sums_dims[0];\n        TF_LITE_ENSURE_OK(\n            context, context->ResizeTensor(context, row_sums, row_sums_size));\n      }\n    }\n  }\n  return kTfLiteOk;\n}",
        "func": "TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n                     TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = node->inputs->size == 3;\n  // Check number of inputs/outputs\n  TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n\n  // Check dimensionality of input, filter\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);\n  TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);\n  // Check input channels matching filter\n  // Filter input channel can be a factor of channels of input (grouped conv)\n  // or equals (normal conv).\n  auto input_channel = input->dims->data[3];\n  auto filter_input_channel = filter->dims->data[3];\n  TF_LITE_ENSURE(context, filter_input_channel > 0);\n  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);\n  data->groups = input_channel / filter_input_channel;\n\n  // Check types. (We assume that UINT8 refers to quantized tensors)\n  TfLiteType input_type = input->type;\n  TF_LITE_ENSURE(context,\n                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||\n                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);\n\n  if (input_type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  }\n  // Filter must have zero zero-points in per-channel quantization.\n  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {\n      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);\n    }\n  }\n\n  const TfLiteTensor* bias = nullptr;\n\n  // TODO(ahentz): At this point the optimized versions require 'bias'. We can\n  // either change that or document that convolution requires it.\n  TF_LITE_ENSURE(context, has_bias);\n\n  if (has_bias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));\n    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (input_type == kTfLiteInt16) {\n      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||\n                                  (bias->type == kTfLiteInt64));\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));\n  }\n\n  const bool is_hybrid =\n      (input->type == kTfLiteFloat32 &&\n       (filter->type == kTfLiteUInt8 || filter->type == kTfLiteInt8));\n\n  if (is_hybrid && filter->type == kTfLiteInt8 &&\n      filter->quantization.type == kTfLiteAffineQuantization &&\n      filter->quantization.params &&\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)\n          ->scale &&\n      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)\n              ->scale->size > 1) {\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    const float scale = affine_quantization->scale->data[0];\n    for (int i = 1; i < affine_quantization->scale->size; i++) {\n      if (affine_quantization->scale->data[i] != scale) {\n        data->is_hybrid_per_channel = true;\n        break;\n      }\n    }\n  }\n\n  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and\n  // is incompatible with mutable input filters that might change between evals.\n  data->supports_multithreaded_kernel =\n      (kernel_type == kMultithreadOptimized) &&\n      (context->recommended_num_threads != 1) && !is_hybrid &&\n      (params->dilation_width_factor == 1) &&\n      (params->dilation_height_factor == 1) &&\n      (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);\n\n  int channels_in = filter->dims->data[3];\n  int channels_out = filter->dims->data[0];\n  int width = input->dims->data[2];\n  int height = input->dims->data[1];\n  int filter_width = filter->dims->data[2];\n  int filter_height = filter->dims->data[1];\n  int batches = input->dims->data[0];\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  size_t im2col_type_size;\n  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));\n  // Note that we intentionally promote the first multiplicand (i.e. 'batches')\n  // to 'size_t' to avoid integer overflow here.\n  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *\n                              out_width * channels_in * filter_height *\n                              filter_width * im2col_type_size;\n  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(\n      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,\n      im2col_bytes));\n\n  TF_LITE_ENSURE(context, has_bias);\n\n  // Note that full fixed-point inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (input_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);\n    const auto* affine_quantization =\n        reinterpret_cast<TfLiteAffineQuantization*>(\n            filter->quantization.params);\n    TF_LITE_ENSURE(context, affine_quantization);\n    TF_LITE_ENSURE(context, affine_quantization->scale);\n    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||\n                             affine_quantization->scale->size == channels_out));\n\n    data->per_channel_output_multiplier.resize(channels_out);\n    data->per_channel_output_shift.resize(channels_out);\n    TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(\n        context, input, filter, bias, output, params->activation,\n        &data->output_multiplier, &data->output_shift,\n        &data->output_activation_min, &data->output_activation_max,\n        data->per_channel_output_multiplier.data(),\n        data->per_channel_output_shift.data(), channels_out));\n  }\n\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = batches;\n  output_size->data[1] = out_height;\n  output_size->data[2] = out_width;\n  output_size->data[3] = channels_out;\n  auto output_status = context->ResizeTensor(context, output, output_size);\n\n  if (output_status != kTfLiteOk) return output_status;\n\n  if (data->need_im2col) {\n    node->temporaries->data[data->im2col_index] = data->im2col_id;\n\n    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);\n\n    auto filter_input_channel = filter->dims->data[3];\n    im2col_size->data[0] = output_size->data[0];\n    im2col_size->data[1] = output_size->data[1];\n    im2col_size->data[2] = output_size->data[2];\n    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;\n\n    TfLiteTensor* im2col =\n        &context->tensors[node->temporaries->data[data->im2col_index]];\n    im2col->type = input->type;\n    if (is_hybrid) {\n      im2col->type = filter->type;\n    }\n    im2col->allocation_type = kTfLiteArenaRw;\n    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);\n    if (im2col_status != kTfLiteOk) return im2col_status;\n  }\n\n  if (data->need_hwcn_weights) {\n    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;\n    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);\n\n    // Because we're treating the filter weights as a matrix when we do the\n    // transpose, we allocate the buffer with a two-dimensional shape, where one\n    // dimension is the number of elements in each filter, and the second is the\n    // total number of filters.\n    auto filter_input_channel = filter->dims->data[3];\n    hwcn_weights_size->data[0] =\n        (filter_height * filter_width * filter_input_channel);\n    hwcn_weights_size->data[1] = channels_out;\n\n    TfLiteTensor* hwcn_weights =\n        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];\n    hwcn_weights->type = input_type;\n    hwcn_weights->name = \"Conv_hwcn_weights\";\n    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;\n\n    auto hwcn_weights_status =\n        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);\n    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;\n\n    // TODO(petewarden): If Resize() is called when the size hasn't actually\n    // changed, this will do extra redundant work.\n    data->have_weights_been_transposed = false;\n  }\n\n  if (is_hybrid) {\n    node->temporaries->data[data->input_quantized_index] =\n        data->input_quantized_id;\n    TfLiteTensor* input_quantized;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->input_quantized_index,\n                                  &input_quantized));\n    input_quantized->type = kTfLiteInt8;\n    input_quantized->allocation_type = kTfLiteArenaRw;\n    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {\n      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,\n                                                       input_quantized_size));\n    }\n\n    node->temporaries->data[data->scaling_factors_index] =\n        data->scaling_factors_id;\n    TfLiteTensor* scaling_factors;\n    TF_LITE_ENSURE_OK(\n        context, GetTemporarySafe(context, node, data->scaling_factors_index,\n                                  &scaling_factors));\n    scaling_factors->type = kTfLiteFloat32;\n    scaling_factors->allocation_type = kTfLiteArenaRw;\n    // Only one scale factor per batch is typically necessary. See optimized\n    // implementation for why we need to allocate for the height of the inputs\n    // flattened to 2D.\n    TF_LITE_ENSURE(context, channels_in != 0);\n    const int height = NumElements(input) / channels_in;\n    int scaling_dims[1] = {height};\n    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {\n      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);\n      scaling_factors_size->data[0] = height;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,\n                                                       scaling_factors_size));\n    }\n\n    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;\n    TfLiteTensor* accum_scratch;\n    TF_LITE_ENSURE_OK(context,\n                      GetTemporarySafe(context, node, data->accum_scratch_index,\n                                       &accum_scratch));\n    accum_scratch->type = kTfLiteInt32;\n    accum_scratch->allocation_type = kTfLiteArenaRw;\n    const int scratch_width = batches * out_height * out_width;\n    int accum_scratch_dims[2] = {channels_out, scratch_width};\n    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,\n                                   accum_scratch_dims)) {\n      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);\n      accum_scratch_size->data[0] = channels_out;\n      accum_scratch_size->data[1] = scratch_width;\n      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,\n                                                       accum_scratch_size));\n    }\n\n    if (data->is_hybrid_per_channel) {\n      const auto* affine_quantization =\n          reinterpret_cast<TfLiteAffineQuantization*>(\n              filter->quantization.params);\n      TF_LITE_ENSURE(context, affine_quantization);\n      TF_LITE_ENSURE(context, affine_quantization->scale);\n      TF_LITE_ENSURE_EQ(\n          context, affine_quantization->scale->size,\n          filter->dims->data[affine_quantization->quantized_dimension]);\n      node->temporaries->data[data->input_offset_index] = data->input_offset_id;\n      TfLiteTensor* input_offsets;\n      TF_LITE_ENSURE_OK(\n          context, GetTemporarySafe(context, node, data->input_offset_index,\n                                    &input_offsets));\n      input_offsets->type = kTfLiteInt32;\n      input_offsets->allocation_type = kTfLiteArenaRw;\n      // See above comment for the need to allocate for height of inputs.\n      TF_LITE_ENSURE(context, channels_in != 0);\n      const int height = NumElements(input) / channels_in;\n      const int input_offset_dims[1] = {height};\n      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,\n                                     input_offset_dims)) {\n        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);\n        input_offsets_size->data[0] = input_offset_dims[0];\n        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,\n                                                         input_offsets_size));\n      }\n      node->temporaries->data[data->row_sums_index] = data->row_sums_id;\n      TfLiteTensor* row_sums;\n      TF_LITE_ENSURE_OK(\n          context,\n          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));\n      row_sums->type = kTfLiteInt32;\n      row_sums->name = \"Conv_row_sums\";\n      row_sums->allocation_type = kTfLiteArenaRwPersistent;\n      // See above comment for the need to allocate for height of inputs.\n      const int row_sums_dims[1] = {channels_out};\n      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {\n        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);\n        row_sums_size->data[0] = row_sums_dims[0];\n        TF_LITE_ENSURE_OK(\n            context, context->ResizeTensor(context, row_sums, row_sums_size));\n      }\n    }\n  }\n  return kTfLiteOk;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -22,6 +22,7 @@\n   // or equals (normal conv).\n   auto input_channel = input->dims->data[3];\n   auto filter_input_channel = filter->dims->data[3];\n+  TF_LITE_ENSURE(context, filter_input_channel > 0);\n   TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);\n   data->groups = input_channel / filter_input_channel;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "  TF_LITE_ENSURE(context, filter_input_channel > 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-34141",
        "func_name": "numpy/_convert_from_str",
        "description": "An incomplete string comparison in the numpy.core component in NumPy before 1.22.0 allows attackers to trigger slightly incorrect copying by constructing specific string objects. NOTE: the vendor states that this reported code behavior is \"completely harmless.\"",
        "git_url": "https://github.com/numpy/numpy/commit/eeef9d4646103c3b1afd3085f1393f2b3f9575b2",
        "commit_title": "DEP: Remove deprecated numeric style dtype strings (#19539)",
        "commit_text": " Finishes the deprecation, and effectively closes gh-18993\r \r * Insecure String Comparison\r \r * Finished Deprecations\r \r * Breaks numpy types\r \r * Removed elements in dep_tps\r \r * Delete Typecode Comment\r \r * Deleted for loop\r \r * Fixed 80 characters or more issue\r \r * Expired Release Note\r \r * Updated Release Note\r \r * Update numpy/core/numerictypes.py\r \r * Update numpy/core/tests/test_deprecations.py\r \r Co-authored-by: Sebastian Berg <sebastian@sipsolutions.net>",
        "func_before": "static PyArray_Descr *\n_convert_from_str(PyObject *obj, int align)\n{\n    /* Check for a string typecode. */\n    Py_ssize_t len = 0;\n    char const *type = PyUnicode_AsUTF8AndSize(obj, &len);\n    if (type == NULL) {\n        return NULL;\n    }\n\n    /* Empty string is invalid */\n    if (len == 0) {\n        goto fail;\n    }\n\n    /* check for commas present or first (or second) element a digit */\n    if (_check_for_commastring(type, len)) {\n        return _convert_from_commastring(obj, align);\n    }\n\n    /* Process the endian character. '|' is replaced by '='*/\n    char endian = '=';\n    switch (type[0]) {\n        case '>':\n        case '<':\n        case '=':\n            endian = type[0];\n            ++type;\n            --len;\n            break;\n\n        case '|':\n            endian = '=';\n            ++type;\n            --len;\n            break;\n    }\n\n    /* Just an endian character is invalid */\n    if (len == 0) {\n        goto fail;\n    }\n\n    /* Check for datetime format */\n    if (is_datetime_typestr(type, len)) {\n        PyArray_Descr *ret = parse_dtype_from_datetime_typestr(type, len);\n        if (ret == NULL) {\n            return NULL;\n        }\n        /* ret has byte order '=' at this point */\n        if (!PyArray_ISNBO(endian)) {\n            ret->byteorder = endian;\n        }\n        return ret;\n    }\n\n    int check_num = NPY_NOTYPE + 10;\n    int elsize = 0;\n    /* A typecode like 'd' */\n    if (len == 1) {\n        /* Python byte string characters are unsigned */\n        check_num = (unsigned char) type[0];\n    }\n    /* A kind + size like 'f8' */\n    else {\n        char *typeend = NULL;\n        int kind;\n\n        /* Parse the integer, make sure it's the rest of the string */\n        elsize = (int)strtol(type + 1, &typeend, 10);\n        if (typeend - type == len) {\n\n            kind = type[0];\n            switch (kind) {\n                case NPY_STRINGLTR:\n                case NPY_STRINGLTR2:\n                    check_num = NPY_STRING;\n                    break;\n\n                /*\n                 * When specifying length of UNICODE\n                 * the number of characters is given to match\n                 * the STRING interface.  Each character can be\n                 * more than one byte and itemsize must be\n                 * the number of bytes.\n                 */\n                case NPY_UNICODELTR:\n                    check_num = NPY_UNICODE;\n                    elsize <<= 2;\n                    break;\n\n                case NPY_VOIDLTR:\n                    check_num = NPY_VOID;\n                    break;\n\n                default:\n                    if (elsize == 0) {\n                        check_num = NPY_NOTYPE+10;\n                    }\n                    /* Support for generic processing c8, i4, f8, etc...*/\n                    else {\n                        check_num = PyArray_TypestrConvert(elsize, kind);\n                        if (check_num == NPY_NOTYPE) {\n                            check_num += 10;\n                        }\n                        elsize = 0;\n                    }\n            }\n        }\n    }\n\n    if (PyErr_Occurred()) {\n        goto fail;\n    }\n\n    PyArray_Descr *ret;\n    if ((check_num == NPY_NOTYPE + 10) ||\n            (ret = PyArray_DescrFromType(check_num)) == NULL) {\n        PyErr_Clear();\n        /* Now check to see if the object is registered in typeDict */\n        if (typeDict == NULL) {\n            goto fail;\n        }\n        PyObject *item = PyDict_GetItemWithError(typeDict, obj);\n        if (item == NULL) {\n            if (PyErr_Occurred()) {\n                return NULL;\n            }\n            goto fail;\n        }\n\n        /* Check for a deprecated Numeric-style typecode */\n        /* `Uint` has deliberately weird uppercasing */\n        char *dep_tps[] = {\"Bytes\", \"Datetime64\", \"Str\", \"Uint\"};\n        int ndep_tps = sizeof(dep_tps) / sizeof(dep_tps[0]);\n        for (int i = 0; i < ndep_tps; ++i) {\n            char *dep_tp = dep_tps[i];\n\n            if (strncmp(type, dep_tp, strlen(dep_tp)) == 0) {\n                /* Deprecated 2020-06-09, NumPy 1.20 */\n                if (DEPRECATE(\"Numeric-style type codes are \"\n                              \"deprecated and will result in \"\n                              \"an error in the future.\") < 0) {\n                    goto fail;\n                }\n            }\n        }\n        /*\n         * Probably only ever dispatches to `_convert_from_type`, but who\n         * knows what users are injecting into `np.typeDict`.\n         */\n        return _convert_from_any(item, align);\n    }\n\n    if (PyDataType_ISUNSIZED(ret) && ret->elsize != elsize) {\n        PyArray_DESCR_REPLACE(ret);\n        if (ret == NULL) {\n            return NULL;\n        }\n        ret->elsize = elsize;\n    }\n    if (endian != '=' && PyArray_ISNBO(endian)) {\n        endian = '=';\n    }\n    if (endian != '=' && ret->byteorder != '|' && ret->byteorder != endian) {\n        PyArray_DESCR_REPLACE(ret);\n        if (ret == NULL) {\n            return NULL;\n        }\n        ret->byteorder = endian;\n    }\n    return ret;\n\nfail:\n    PyErr_Format(PyExc_TypeError, \"data type %R not understood\", obj);\n    return NULL;\n}",
        "func": "static PyArray_Descr *\n_convert_from_str(PyObject *obj, int align)\n{\n    /* Check for a string typecode. */\n    Py_ssize_t len = 0;\n    char const *type = PyUnicode_AsUTF8AndSize(obj, &len);\n    if (type == NULL) {\n        return NULL;\n    }\n\n    /* Empty string is invalid */\n    if (len == 0) {\n        goto fail;\n    }\n\n    /* check for commas present or first (or second) element a digit */\n    if (_check_for_commastring(type, len)) {\n        return _convert_from_commastring(obj, align);\n    }\n\n    /* Process the endian character. '|' is replaced by '='*/\n    char endian = '=';\n    switch (type[0]) {\n        case '>':\n        case '<':\n        case '=':\n            endian = type[0];\n            ++type;\n            --len;\n            break;\n\n        case '|':\n            endian = '=';\n            ++type;\n            --len;\n            break;\n    }\n\n    /* Just an endian character is invalid */\n    if (len == 0) {\n        goto fail;\n    }\n\n    /* Check for datetime format */\n    if (is_datetime_typestr(type, len)) {\n        PyArray_Descr *ret = parse_dtype_from_datetime_typestr(type, len);\n        if (ret == NULL) {\n            return NULL;\n        }\n        /* ret has byte order '=' at this point */\n        if (!PyArray_ISNBO(endian)) {\n            ret->byteorder = endian;\n        }\n        return ret;\n    }\n\n    int check_num = NPY_NOTYPE + 10;\n    int elsize = 0;\n    /* A typecode like 'd' */\n    if (len == 1) {\n        /* Python byte string characters are unsigned */\n        check_num = (unsigned char) type[0];\n    }\n    /* A kind + size like 'f8' */\n    else {\n        char *typeend = NULL;\n        int kind;\n\n        /* Parse the integer, make sure it's the rest of the string */\n        elsize = (int)strtol(type + 1, &typeend, 10);\n        if (typeend - type == len) {\n\n            kind = type[0];\n            switch (kind) {\n                case NPY_STRINGLTR:\n                case NPY_STRINGLTR2:\n                    check_num = NPY_STRING;\n                    break;\n\n                /*\n                 * When specifying length of UNICODE\n                 * the number of characters is given to match\n                 * the STRING interface.  Each character can be\n                 * more than one byte and itemsize must be\n                 * the number of bytes.\n                 */\n                case NPY_UNICODELTR:\n                    check_num = NPY_UNICODE;\n                    elsize <<= 2;\n                    break;\n\n                case NPY_VOIDLTR:\n                    check_num = NPY_VOID;\n                    break;\n\n                default:\n                    if (elsize == 0) {\n                        check_num = NPY_NOTYPE+10;\n                    }\n                    /* Support for generic processing c8, i4, f8, etc...*/\n                    else {\n                        check_num = PyArray_TypestrConvert(elsize, kind);\n                        if (check_num == NPY_NOTYPE) {\n                            check_num += 10;\n                        }\n                        elsize = 0;\n                    }\n            }\n        }\n    }\n\n    if (PyErr_Occurred()) {\n        goto fail;\n    }\n\n    PyArray_Descr *ret;\n    if ((check_num == NPY_NOTYPE + 10) ||\n            (ret = PyArray_DescrFromType(check_num)) == NULL) {\n        PyErr_Clear();\n        /* Now check to see if the object is registered in typeDict */\n        if (typeDict == NULL) {\n            goto fail;\n        }\n        PyObject *item = PyDict_GetItemWithError(typeDict, obj);\n        if (item == NULL) {\n            if (PyErr_Occurred()) {\n                return NULL;\n            }\n            goto fail;\n        }\n\n        /*\n         * Probably only ever dispatches to `_convert_from_type`, but who\n         * knows what users are injecting into `np.typeDict`.\n         */\n        return _convert_from_any(item, align);\n    }\n\n    if (PyDataType_ISUNSIZED(ret) && ret->elsize != elsize) {\n        PyArray_DESCR_REPLACE(ret);\n        if (ret == NULL) {\n            return NULL;\n        }\n        ret->elsize = elsize;\n    }\n    if (endian != '=' && PyArray_ISNBO(endian)) {\n        endian = '=';\n    }\n    if (endian != '=' && ret->byteorder != '|' && ret->byteorder != endian) {\n        PyArray_DESCR_REPLACE(ret);\n        if (ret == NULL) {\n            return NULL;\n        }\n        ret->byteorder = endian;\n    }\n    return ret;\n\nfail:\n    PyErr_Format(PyExc_TypeError, \"data type %R not understood\", obj);\n    return NULL;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -129,22 +129,6 @@\n             goto fail;\n         }\n \n-        /* Check for a deprecated Numeric-style typecode */\n-        /* `Uint` has deliberately weird uppercasing */\n-        char *dep_tps[] = {\"Bytes\", \"Datetime64\", \"Str\", \"Uint\"};\n-        int ndep_tps = sizeof(dep_tps) / sizeof(dep_tps[0]);\n-        for (int i = 0; i < ndep_tps; ++i) {\n-            char *dep_tp = dep_tps[i];\n-\n-            if (strncmp(type, dep_tp, strlen(dep_tp)) == 0) {\n-                /* Deprecated 2020-06-09, NumPy 1.20 */\n-                if (DEPRECATE(\"Numeric-style type codes are \"\n-                              \"deprecated and will result in \"\n-                              \"an error in the future.\") < 0) {\n-                    goto fail;\n-                }\n-            }\n-        }\n         /*\n          * Probably only ever dispatches to `_convert_from_type`, but who\n          * knows what users are injecting into `np.typeDict`.",
        "diff_line_info": {
            "deleted_lines": [
                "        /* Check for a deprecated Numeric-style typecode */",
                "        /* `Uint` has deliberately weird uppercasing */",
                "        char *dep_tps[] = {\"Bytes\", \"Datetime64\", \"Str\", \"Uint\"};",
                "        int ndep_tps = sizeof(dep_tps) / sizeof(dep_tps[0]);",
                "        for (int i = 0; i < ndep_tps; ++i) {",
                "            char *dep_tp = dep_tps[i];",
                "",
                "            if (strncmp(type, dep_tp, strlen(dep_tp)) == 0) {",
                "                /* Deprecated 2020-06-09, NumPy 1.20 */",
                "                if (DEPRECATE(\"Numeric-style type codes are \"",
                "                              \"deprecated and will result in \"",
                "                              \"an error in the future.\") < 0) {",
                "                    goto fail;",
                "                }",
                "            }",
                "        }"
            ],
            "added_lines": []
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/getfactor",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* getfactor(PyObject *self, PyObject *args)\n{\n    PyObject *F;\n    cholmod_factor *Lf;\n    cholmod_sparse *Ls;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n\n    if (!set_options()) return NULL;\n    if (!PyArg_ParseTuple(args, \"O\", &F)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    Lf = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    Lf = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n\n    /* Check factorization */\n    if (Lf->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"F must be a numeric Cholesky factor\");\n\n    if (!(Ls = CHOL(factor_to_sparse)(Lf, &Common)))\n        return PyErr_NoMemory();\n\n    spmatrix *ret = SpMatrix_New(Ls->nrow, Ls->ncol, Ls->nzmax,\n       (Ls->xtype == CHOLMOD_REAL ? DOUBLE : COMPLEX));\n    if (!ret) {\n        CHOL(free_sparse)(&Ls, &Common);\n        return NULL;\n    }\n\n    memcpy(SP_COL(ret), Ls->p, (Ls->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(ret), Ls->i, (Ls->nzmax)*sizeof(int_t));\n    memcpy(SP_VAL(ret), Ls->x, (Ls->nzmax)*E_SIZE[SP_ID(ret)]);\n    CHOL(free_sparse)(&Ls, &Common);\n\n    return (PyObject *)ret;\n}",
        "func": "static PyObject* getfactor(PyObject *self, PyObject *args)\n{\n    PyObject *F;\n    cholmod_factor *Lf;\n    cholmod_sparse *Ls;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n\n    if (!set_options()) return NULL;\n    if (!PyArg_ParseTuple(args, \"O\", &F)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    Lf = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    Lf = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n\n    /* Check factorization */\n    if (Lf->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"F must be a numeric Cholesky factor\");\n\n    if (!(Ls = CHOL(factor_to_sparse)(Lf, &Common)))\n        return PyErr_NoMemory();\n\n    spmatrix *ret = SpMatrix_New(Ls->nrow, Ls->ncol, Ls->nzmax,\n       (Ls->xtype == CHOLMOD_REAL ? DOUBLE : COMPLEX));\n    if (!ret) {\n        CHOL(free_sparse)(&Ls, &Common);\n        return NULL;\n    }\n\n    memcpy(SP_COL(ret), Ls->p, (Ls->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(ret), Ls->i, (Ls->nzmax)*sizeof(int_t));\n    memcpy(SP_VAL(ret), Ls->x, (Ls->nzmax)*E_SIZE[SP_ID(ret)]);\n    CHOL(free_sparse)(&Ls, &Common);\n\n    return (PyObject *)ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -15,13 +15,13 @@\n #if PY_MAJOR_VERSION >= 3\n     if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n         err_CO(\"F\");\n-    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     Lf = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n #else\n     if (!PyCObject_Check(F)) err_CO(\"F\");\n     descr = PyCObject_GetDesc(F);\n-    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     Lf = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n #endif",
        "diff_line_info": {
            "deleted_lines": [
                "    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))",
                "    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))"
            ],
            "added_lines": [
                "    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))",
                "    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/diag",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* diag(PyObject *self, PyObject *args)\n{\n    PyObject *F;\n    matrix *d=NULL;\n    cholmod_factor *L;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    int k, strt, incx=1, incy, nrows, ncols;\n\n    if (!set_options()) return NULL;\n    if (!PyArg_ParseTuple(args, \"O\", &F)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n\n    /* Check factorization */\n    if (L->xtype == CHOLMOD_PATTERN  || L->minor<L->n || !L->is_ll\n        || !L->is_super)\n        PY_ERR(PyExc_ValueError, \"F must be a nonsingular supernodal \"\n            \"Cholesky factor\");\n    if (!(d = Matrix_New(L->n,1,L->xtype == CHOLMOD_REAL ? DOUBLE :\n\t\t\t COMPLEX))) return NULL;\n\n    strt = 0;\n    for (k=0; k<L->nsuper; k++){\n\t/* x[L->px[k], .... ,L->px[k+1]-1] is a dense lower-triangular\n\t * nrowx times ncols matrix.  We copy its diagonal to\n\t * d[strt, ..., strt+ncols-1] */\n\n        ncols = (int)((int_t *) L->super)[k+1] -\n            ((int_t *) L->super)[k];\n        nrows = (int)((int_t *) L->pi)[k+1] - ((int_t *) L->pi)[k];\n        incy = nrows+1;\n        if (MAT_ID(d) == DOUBLE)\n\t    dcopy_(&ncols, ((double *) L->x) + ((int_t *) L->px)[k],\n                &incy, MAT_BUFD(d)+strt, &incx);\n        else\n\t    zcopy_(&ncols, ((complex_t *) L->x) + ((int_t *) L->px)[k],\n                &incy, MAT_BUFZ(d)+strt, &incx);\n        strt += ncols;\n    }\n    return (PyObject *)d;\n}",
        "func": "static PyObject* diag(PyObject *self, PyObject *args)\n{\n    PyObject *F;\n    matrix *d=NULL;\n    cholmod_factor *L;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    int k, strt, incx=1, incy, nrows, ncols;\n\n    if (!set_options()) return NULL;\n    if (!PyArg_ParseTuple(args, \"O\", &F)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n\n    /* Check factorization */\n    if (L->xtype == CHOLMOD_PATTERN  || L->minor<L->n || !L->is_ll\n        || !L->is_super)\n        PY_ERR(PyExc_ValueError, \"F must be a nonsingular supernodal \"\n            \"Cholesky factor\");\n    if (!(d = Matrix_New(L->n,1,L->xtype == CHOLMOD_REAL ? DOUBLE :\n\t\t\t COMPLEX))) return NULL;\n\n    strt = 0;\n    for (k=0; k<L->nsuper; k++){\n\t/* x[L->px[k], .... ,L->px[k+1]-1] is a dense lower-triangular\n\t * nrowx times ncols matrix.  We copy its diagonal to\n\t * d[strt, ..., strt+ncols-1] */\n\n        ncols = (int)((int_t *) L->super)[k+1] -\n            ((int_t *) L->super)[k];\n        nrows = (int)((int_t *) L->pi)[k+1] - ((int_t *) L->pi)[k];\n        incy = nrows+1;\n        if (MAT_ID(d) == DOUBLE)\n\t    dcopy_(&ncols, ((double *) L->x) + ((int_t *) L->px)[k],\n                &incy, MAT_BUFD(d)+strt, &incx);\n        else\n\t    zcopy_(&ncols, ((complex_t *) L->x) + ((int_t *) L->px)[k],\n                &incy, MAT_BUFZ(d)+strt, &incx);\n        strt += ncols;\n    }\n    return (PyObject *)d;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -16,13 +16,13 @@\n #if PY_MAJOR_VERSION >= 3\n     if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n         err_CO(\"F\");\n-    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n #else\n     if (!PyCObject_Check(F)) err_CO(\"F\");\n     descr = PyCObject_GetDesc(F);\n-    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n #endif",
        "diff_line_info": {
            "deleted_lines": [
                "    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))",
                "    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))"
            ],
            "added_lines": [
                "    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))",
                "    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/symbolic",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* symbolic(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *A;\n    cholmod_sparse *Ac = NULL;\n    cholmod_factor *L;\n    matrix *P=NULL;\n#if PY_MAJOR_VERSION >= 3\n    int uplo_='L';\n#endif\n    char uplo='L';\n    int n;\n    char *kwlist[] = {\"A\", \"p\", \"uplo\", NULL};\n\n    if (!set_options()) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|OC\", kwlist, &A,\n        &P, &uplo_)) return NULL;\n    uplo = (char) uplo_;\n#else\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|Oc\", kwlist, &A,\n        &P, &uplo)) return NULL;\n#endif\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a square sparse matrix\");\n    n = SP_NROWS(A);\n\n    if (P) {\n        if (!Matrix_Check(P) || MAT_ID(P) != INT) err_int_mtrx(\"p\");\n        if (MAT_LGT(P) != n) err_buf_len(\"p\");\n        if (!CHOL(check_perm)(P->buffer, n, n, &Common))\n            PY_ERR(PyExc_ValueError, \"p is not a valid permutation\");\n    }\n    if (uplo != 'U' && uplo != 'L') err_char(\"uplo\", \"'L', 'U'\");\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    L = CHOL(analyze_p)(Ac, P ? MAT_BUFI(P): NULL, NULL, 0, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status != CHOLMOD_OK){\n        if (Common.status == CHOLMOD_OUT_OF_MEMORY)\n            return PyErr_NoMemory();\n        else{\n            PyErr_SetString(PyExc_ValueError, \"symbolic factorization \"\n                \"failed\");\n            return NULL;\n        }\n    }\n#if PY_MAJOR_VERSION >= 3\n    return (PyObject *) PyCapsule_New((void *) L, SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n        (PyCapsule_Destructor) &cvxopt_free_cholmod_factor);\n#else\n    return (PyObject *) PyCObject_FromVoidPtrAndDesc((void *) L,\n        SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n\tcvxopt_free_cholmod_factor);\n#endif\n}",
        "func": "static PyObject* symbolic(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *A;\n    cholmod_sparse *Ac = NULL;\n    cholmod_factor *L;\n    matrix *P=NULL;\n#if PY_MAJOR_VERSION >= 3\n    int uplo_='L';\n#endif\n    char uplo='L';\n    int n;\n    char *kwlist[] = {\"A\", \"p\", \"uplo\", NULL};\n\n    if (!set_options()) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|OC\", kwlist, &A,\n        &P, &uplo_)) return NULL;\n    uplo = (char) uplo_;\n#else\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"O|Oc\", kwlist, &A,\n        &P, &uplo)) return NULL;\n#endif\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a square sparse matrix\");\n    n = SP_NROWS(A);\n\n    if (P) {\n        if (!Matrix_Check(P) || MAT_ID(P) != INT) err_int_mtrx(\"p\");\n        if (MAT_LGT(P) != n) err_buf_len(\"p\");\n        if (!CHOL(check_perm)(P->buffer, n, n, &Common))\n            PY_ERR(PyExc_ValueError, \"p is not a valid permutation\");\n    }\n    if (uplo != 'U' && uplo != 'L') err_char(\"uplo\", \"'L', 'U'\");\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    L = CHOL(analyze_p)(Ac, P ? MAT_BUFI(P): NULL, NULL, 0, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status != CHOLMOD_OK){\n        if (Common.status == CHOLMOD_OUT_OF_MEMORY)\n            return PyErr_NoMemory();\n        else{\n            PyErr_SetString(PyExc_ValueError, \"symbolic factorization \"\n                \"failed\");\n            return NULL;\n        }\n    }\n#if PY_MAJOR_VERSION >= 3\n    return (PyObject *) PyCapsule_New((void *) L, SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  strCFDL : strCFDU) :\n        (uplo == 'L' ?  strCFZL : strCFZU),\n        (PyCapsule_Destructor) &cvxopt_free_cholmod_factor);\n#else\n    return (PyObject *) PyCObject_FromVoidPtrAndDesc((void *) L,\n        SP_ID(A)==DOUBLE ?\n        (uplo == 'L' ?  strCFDL : strCFDU) :\n        (uplo == 'L' ?  strCFZL : strCFZU),\n\tcvxopt_free_cholmod_factor);\n#endif\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -48,14 +48,14 @@\n     }\n #if PY_MAJOR_VERSION >= 3\n     return (PyObject *) PyCapsule_New((void *) L, SP_ID(A)==DOUBLE ?\n-        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n-        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n+        (uplo == 'L' ?  strCFDL : strCFDU) :\n+        (uplo == 'L' ?  strCFZL : strCFZU),\n         (PyCapsule_Destructor) &cvxopt_free_cholmod_factor);\n #else\n     return (PyObject *) PyCObject_FromVoidPtrAndDesc((void *) L,\n         SP_ID(A)==DOUBLE ?\n-        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :\n-        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),\n+        (uplo == 'L' ?  strCFDL : strCFDU) :\n+        (uplo == 'L' ?  strCFZL : strCFZU),\n \tcvxopt_free_cholmod_factor);\n #endif\n }",
        "diff_line_info": {
            "deleted_lines": [
                "        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :",
                "        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),",
                "        (uplo == 'L' ?  \"CHOLMOD FACTOR D L\" : \"CHOLMOD FACTOR D U\") :",
                "        (uplo == 'L' ?  \"CHOLMOD FACTOR Z L\" : \"CHOLMOD FACTOR Z U\"),"
            ],
            "added_lines": [
                "        (uplo == 'L' ?  strCFDL : strCFDU) :",
                "        (uplo == 'L' ?  strCFZL : strCFZU),",
                "        (uplo == 'L' ?  strCFDL : strCFDU) :",
                "        (uplo == 'L' ?  strCFZL : strCFZU),"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/solve",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* solve(PyObject *self, PyObject *args, PyObject *kwrds)\n{\n    matrix *B;\n    PyObject *F;\n    int i, n, oB=0, ldB=0, nrhs=-1, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", \"nrhs\", \"ldB\", \"offsetB\", NULL};\n    int sysvalues[] = { CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|iiii\", kwlist,\n        &F, &B, &sys, &nrhs, &ldB, &oB)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    cholmod_factor *L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    cholmod_factor *L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!Matrix_Check(B) || MAT_ID(B) == INT ||\n        (MAT_ID(B) == DOUBLE && L->xtype == CHOLMOD_COMPLEX) ||\n        (MAT_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a dense matrix of the same numerical \"\n                \"type as F\");\n\n    if (nrhs < 0) nrhs = MAT_NCOLS(B);\n    if (n == 0 || nrhs == 0) return Py_BuildValue(\"\");\n    if (ldB == 0) ldB = MAX(1,MAT_NROWS(B));\n    if (ldB < MAX(1,n)) err_ld(\"ldB\");\n    if (oB < 0) err_nn_int(\"offsetB\");\n    if (oB + (nrhs-1)*ldB + n > MAT_LGT(B)) err_buf_len(\"B\");\n\n    cholmod_dense *x;\n    cholmod_dense *b = CHOL(allocate_dense)(n, 1, n,\n        (MAT_ID(B) == DOUBLE ? CHOLMOD_REAL : CHOLMOD_COMPLEX),\n        &Common);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n\n    void *b_old = b->x;\n    for (i=0; i<nrhs; i++){\n        b->x = (unsigned char*)MAT_BUF(B) + (i*ldB + oB)*E_SIZE[MAT_ID(B)];\n        x = CHOL(solve) (sysvalues[sys], L, b, &Common);\n        if (Common.status != CHOLMOD_OK){\n            PyErr_SetString(PyExc_ValueError, \"solve step failed\");\n            CHOL(free_dense)(&x, &Common);\n            CHOL(free_dense)(&b, &Common);\n\t    return NULL;\n\t}\n\tmemcpy(b->x, x->x, n*E_SIZE[MAT_ID(B)]);\n        CHOL(free_dense)(&x, &Common);\n    }\n    b->x = b_old;\n    CHOL(free_dense)(&b, &Common);\n\n    return Py_BuildValue(\"\");\n}",
        "func": "static PyObject* solve(PyObject *self, PyObject *args, PyObject *kwrds)\n{\n    matrix *B;\n    PyObject *F;\n    int i, n, oB=0, ldB=0, nrhs=-1, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", \"nrhs\", \"ldB\", \"offsetB\", NULL};\n    int sysvalues[] = { CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|iiii\", kwlist,\n        &F, &B, &sys, &nrhs, &ldB, &oB)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    cholmod_factor *L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    cholmod_factor *L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!Matrix_Check(B) || MAT_ID(B) == INT ||\n        (MAT_ID(B) == DOUBLE && L->xtype == CHOLMOD_COMPLEX) ||\n        (MAT_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a dense matrix of the same numerical \"\n                \"type as F\");\n\n    if (nrhs < 0) nrhs = MAT_NCOLS(B);\n    if (n == 0 || nrhs == 0) return Py_BuildValue(\"\");\n    if (ldB == 0) ldB = MAX(1,MAT_NROWS(B));\n    if (ldB < MAX(1,n)) err_ld(\"ldB\");\n    if (oB < 0) err_nn_int(\"offsetB\");\n    if (oB + (nrhs-1)*ldB + n > MAT_LGT(B)) err_buf_len(\"B\");\n\n    cholmod_dense *x;\n    cholmod_dense *b = CHOL(allocate_dense)(n, 1, n,\n        (MAT_ID(B) == DOUBLE ? CHOLMOD_REAL : CHOLMOD_COMPLEX),\n        &Common);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n\n    void *b_old = b->x;\n    for (i=0; i<nrhs; i++){\n        b->x = (unsigned char*)MAT_BUF(B) + (i*ldB + oB)*E_SIZE[MAT_ID(B)];\n        x = CHOL(solve) (sysvalues[sys], L, b, &Common);\n        if (Common.status != CHOLMOD_OK){\n            PyErr_SetString(PyExc_ValueError, \"solve step failed\");\n            CHOL(free_dense)(&x, &Common);\n            CHOL(free_dense)(&b, &Common);\n\t    return NULL;\n\t}\n\tmemcpy(b->x, x->x, n*E_SIZE[MAT_ID(B)]);\n        CHOL(free_dense)(&x, &Common);\n    }\n    b->x = b_old;\n    CHOL(free_dense)(&b, &Common);\n\n    return Py_BuildValue(\"\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,13 +21,13 @@\n #if PY_MAJOR_VERSION >= 3\n     if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n         err_CO(\"F\");\n-    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     cholmod_factor *L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n #else\n     if (!PyCObject_Check(F)) err_CO(\"F\");\n     descr = PyCObject_GetDesc(F);\n-    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     cholmod_factor *L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n #endif",
        "diff_line_info": {
            "deleted_lines": [
                "    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))",
                "    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))"
            ],
            "added_lines": [
                "    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))",
                "    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/numeric",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* numeric(PyObject *self, PyObject *args)\n{\n    spmatrix *A;\n    PyObject *F;\n    cholmod_factor *Lc;\n    cholmod_sparse *Ac = NULL;\n    char uplo;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTuple(args, \"OO\", &A, &F)) return NULL;\n\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a sparse matrix\");\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr) PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n#endif\n    if (SP_ID(A) == DOUBLE){\n        if (!strcmp(descr, \"CHOLMOD FACTOR D L\"))\n\t    uplo = 'L';\n\telse if (!strcmp(descr, \"CHOLMOD FACTOR D U\"))\n\t    uplo = 'U';\n        else\n\t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'd' matrix\");\n    } else {\n        if (!strcmp(descr, \"CHOLMOD FACTOR Z L\"))\n\t    uplo = 'L';\n\telse if (!strcmp(descr, \"CHOLMOD FACTOR Z U\"))\n\t    uplo = 'U';\n        else\n\t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'z' matrix\");\n    }\n#if PY_MAJOR_VERSION >= 3\n    Lc = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    Lc = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    CHOL(factorize) (Ac, Lc, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status < 0) switch (Common.status) {\n        case CHOLMOD_OUT_OF_MEMORY:\n            return PyErr_NoMemory();\n\n        default:\n            PyErr_SetString(PyExc_ValueError, \"factorization failed\");\n            return NULL;\n    }\n\n    if (Common.status > 0) switch (Common.status) {\n        case CHOLMOD_NOT_POSDEF:\n            PyErr_SetObject(PyExc_ArithmeticError, Py_BuildValue(\"i\",\n                Lc->minor));\n            return NULL;\n            break;\n\n        case CHOLMOD_DSMALL:\n            /* This never happens unless we change the default value\n             * of Common.dbound (0.0).  */\n            if (Lc->is_ll)\n                PyErr_Warn(PyExc_RuntimeWarning, \"tiny diagonal \"\\\n                    \"elements in L\");\n            else\n                PyErr_Warn(PyExc_RuntimeWarning, \"tiny diagonal \"\\\n                    \"elements in D\");\n            break;\n\n        default:\n            PyErr_Warn(PyExc_UserWarning, \"\");\n    }\n\n    return Py_BuildValue(\"\");\n}",
        "func": "static PyObject* numeric(PyObject *self, PyObject *args)\n{\n    spmatrix *A;\n    PyObject *F;\n    cholmod_factor *Lc;\n    cholmod_sparse *Ac = NULL;\n    char uplo;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTuple(args, \"OO\", &A, &F)) return NULL;\n\n    if (!SpMatrix_Check(A) || SP_NROWS(A) != SP_NCOLS(A))\n        PY_ERR_TYPE(\"A is not a sparse matrix\");\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr) PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n#endif\n    if (SP_ID(A) == DOUBLE){\n        if (!strcmp(descr, strCFDL))\n\t    uplo = 'L';\n\telse if (!strcmp(descr, strCFDU))\n\t    uplo = 'U';\n        else\n\t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'd' matrix\");\n    } else {\n        if (!strcmp(descr, strCFZL))\n\t    uplo = 'L';\n\telse if (!strcmp(descr, strCFZU))\n\t    uplo = 'U';\n        else\n\t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'z' matrix\");\n    }\n#if PY_MAJOR_VERSION >= 3\n    Lc = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    Lc = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (!(Ac = pack(A, uplo))) return PyErr_NoMemory();\n    CHOL(factorize) (Ac, Lc, &Common);\n    CHOL(free_sparse)(&Ac, &Common);\n\n    if (Common.status < 0) switch (Common.status) {\n        case CHOLMOD_OUT_OF_MEMORY:\n            return PyErr_NoMemory();\n\n        default:\n            PyErr_SetString(PyExc_ValueError, \"factorization failed\");\n            return NULL;\n    }\n\n    if (Common.status > 0) switch (Common.status) {\n        case CHOLMOD_NOT_POSDEF:\n            PyErr_SetObject(PyExc_ArithmeticError, Py_BuildValue(\"i\",\n                Lc->minor));\n            return NULL;\n            break;\n\n        case CHOLMOD_DSMALL:\n            /* This never happens unless we change the default value\n             * of Common.dbound (0.0).  */\n            if (Lc->is_ll)\n                PyErr_Warn(PyExc_RuntimeWarning, \"tiny diagonal \"\\\n                    \"elements in L\");\n            else\n                PyErr_Warn(PyExc_RuntimeWarning, \"tiny diagonal \"\\\n                    \"elements in D\");\n            break;\n\n        default:\n            PyErr_Warn(PyExc_UserWarning, \"\");\n    }\n\n    return Py_BuildValue(\"\");\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -27,16 +27,16 @@\n     if (!descr) PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n #endif\n     if (SP_ID(A) == DOUBLE){\n-        if (!strcmp(descr, \"CHOLMOD FACTOR D L\"))\n+        if (!strcmp(descr, strCFDL))\n \t    uplo = 'L';\n-\telse if (!strcmp(descr, \"CHOLMOD FACTOR D U\"))\n+\telse if (!strcmp(descr, strCFDU))\n \t    uplo = 'U';\n         else\n \t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'd' matrix\");\n     } else {\n-        if (!strcmp(descr, \"CHOLMOD FACTOR Z L\"))\n+        if (!strcmp(descr, strCFZL))\n \t    uplo = 'L';\n-\telse if (!strcmp(descr, \"CHOLMOD FACTOR Z U\"))\n+\telse if (!strcmp(descr, strCFZU))\n \t    uplo = 'U';\n         else\n \t    PY_ERR_TYPE(\"F is not the CHOLMOD factor of a 'z' matrix\");",
        "diff_line_info": {
            "deleted_lines": [
                "        if (!strcmp(descr, \"CHOLMOD FACTOR D L\"))",
                "\telse if (!strcmp(descr, \"CHOLMOD FACTOR D U\"))",
                "        if (!strcmp(descr, \"CHOLMOD FACTOR Z L\"))",
                "\telse if (!strcmp(descr, \"CHOLMOD FACTOR Z U\"))"
            ],
            "added_lines": [
                "        if (!strcmp(descr, strCFDL))",
                "\telse if (!strcmp(descr, strCFDU))",
                "        if (!strcmp(descr, strCFZL))",
                "\telse if (!strcmp(descr, strCFZU))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-41500",
        "func_name": "cvxopt/spsolve",
        "description": "Incomplete string comparison vulnerability exits in cvxopt.org cvxop <= 1.2.6 in APIs (cvxopt.cholmod.diag, cvxopt.cholmod.getfactor, cvxopt.cholmod.solve, cvxopt.cholmod.spsolve), which allows attackers to conduct Denial of Service attacks by construct fake Capsule objects.",
        "git_url": "https://github.com/cvxopt/cvxopt/commit/d5a21cf1da62e4269176384b1ff62edac5579f94",
        "commit_title": "Update to 1.2.7 (#199)",
        "commit_text": " * Resolves #193\r * Updated documentation\r * Changed version to 1.2.7",
        "func_before": "static PyObject* spsolve(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *B, *X=NULL;\n    cholmod_sparse *Bc=NULL, *Xc=NULL;\n    PyObject *F;\n    cholmod_factor *L;\n    int n, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", NULL};\n    int sysvalues[] = {CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|i\", kwlist, &F,\n        &B, &sys)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!SpMatrix_Check(B) ||\n        (SP_ID(B) == DOUBLE  && L->xtype == CHOLMOD_COMPLEX) ||\n        (SP_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a sparse matrix of the same \"\n                \"numerical type as F\");\n    if (SP_NROWS(B) != n)\n        PY_ERR(PyExc_ValueError, \"incompatible dimensions for B\");\n\n    if (!(Bc = create_matrix(B))) return PyErr_NoMemory();\n    Xc = CHOL(spsolve)(sysvalues[sys], L, Bc, &Common);\n    free_matrix(Bc);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n    if (Common.status != CHOLMOD_OK)\n        PY_ERR(PyExc_ValueError, \"solve step failed\");\n\n    if (!(X = SpMatrix_New(Xc->nrow, Xc->ncol,\n        ((int_t*)Xc->p)[Xc->ncol], (L->xtype == CHOLMOD_REAL ? DOUBLE :\n        COMPLEX)))) {\n        CHOL(free_sparse)(&Xc, &Common);\n        return NULL;\n    }\n    memcpy(SP_COL(X), Xc->p, (Xc->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(X), Xc->i, ((int_t *)Xc->p)[Xc->ncol]*sizeof(int_t));\n    memcpy(SP_VAL(X), Xc->x,\n        ((int_t *) Xc->p)[Xc->ncol]*E_SIZE[SP_ID(X)]);\n    CHOL(free_sparse)(&Xc, &Common);\n    return (PyObject *) X;\n}",
        "func": "static PyObject* spsolve(PyObject *self, PyObject *args,\n    PyObject *kwrds)\n{\n    spmatrix *B, *X=NULL;\n    cholmod_sparse *Bc=NULL, *Xc=NULL;\n    PyObject *F;\n    cholmod_factor *L;\n    int n, sys=0;\n#if PY_MAJOR_VERSION >= 3\n    const char *descr;\n#else\n    char *descr;\n#endif\n    char *kwlist[] = {\"F\", \"B\", \"sys\", NULL};\n    int sysvalues[] = {CHOLMOD_A, CHOLMOD_LDLt, CHOLMOD_LD,\n        CHOLMOD_DLt, CHOLMOD_L, CHOLMOD_Lt, CHOLMOD_D, CHOLMOD_P,\n        CHOLMOD_Pt };\n\n    if (!set_options()) return NULL;\n\n    if (!PyArg_ParseTupleAndKeywords(args, kwrds, \"OO|i\", kwlist, &F,\n        &B, &sys)) return NULL;\n\n#if PY_MAJOR_VERSION >= 3\n    if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n        err_CO(\"F\");\n    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n#else\n    if (!PyCObject_Check(F)) err_CO(\"F\");\n    descr = PyCObject_GetDesc(F);\n    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n        PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n    L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n#endif\n    if (L->xtype == CHOLMOD_PATTERN)\n        PY_ERR(PyExc_ValueError, \"called with symbolic factor\");\n    n = L->n;\n    if (L->minor<n) PY_ERR(PyExc_ArithmeticError, \"singular matrix\");\n\n    if (sys < 0 || sys > 8)\n         PY_ERR(PyExc_ValueError, \"invalid value for sys\");\n\n    if (!SpMatrix_Check(B) ||\n        (SP_ID(B) == DOUBLE  && L->xtype == CHOLMOD_COMPLEX) ||\n        (SP_ID(B) == COMPLEX && L->xtype == CHOLMOD_REAL))\n            PY_ERR_TYPE(\"B must a sparse matrix of the same \"\n                \"numerical type as F\");\n    if (SP_NROWS(B) != n)\n        PY_ERR(PyExc_ValueError, \"incompatible dimensions for B\");\n\n    if (!(Bc = create_matrix(B))) return PyErr_NoMemory();\n    Xc = CHOL(spsolve)(sysvalues[sys], L, Bc, &Common);\n    free_matrix(Bc);\n    if (Common.status == CHOLMOD_OUT_OF_MEMORY) return PyErr_NoMemory();\n    if (Common.status != CHOLMOD_OK)\n        PY_ERR(PyExc_ValueError, \"solve step failed\");\n\n    if (!(X = SpMatrix_New(Xc->nrow, Xc->ncol,\n        ((int_t*)Xc->p)[Xc->ncol], (L->xtype == CHOLMOD_REAL ? DOUBLE :\n        COMPLEX)))) {\n        CHOL(free_sparse)(&Xc, &Common);\n        return NULL;\n    }\n    memcpy(SP_COL(X), Xc->p, (Xc->ncol+1)*sizeof(int_t));\n    memcpy(SP_ROW(X), Xc->i, ((int_t *)Xc->p)[Xc->ncol]*sizeof(int_t));\n    memcpy(SP_VAL(X), Xc->x,\n        ((int_t *) Xc->p)[Xc->ncol]*E_SIZE[SP_ID(X)]);\n    CHOL(free_sparse)(&Xc, &Common);\n    return (PyObject *) X;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -24,13 +24,13 @@\n #if PY_MAJOR_VERSION >= 3\n     if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))\n         err_CO(\"F\");\n-    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     L = (cholmod_factor *) PyCapsule_GetPointer(F, descr);\n #else\n     if (!PyCObject_Check(F)) err_CO(\"F\");\n     descr = PyCObject_GetDesc(F);\n-    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))\n+    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))\n         PY_ERR_TYPE(\"F is not a CHOLMOD factor\");\n     L = (cholmod_factor *) PyCObject_AsVoidPtr(F);\n #endif",
        "diff_line_info": {
            "deleted_lines": [
                "    if (strncmp(descr, \"CHOLMOD FACTOR\", 14))",
                "    if (!descr || strncmp(descr, \"CHOLMOD FACTOR\", 14))"
            ],
            "added_lines": [
                "    if (!(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))",
                "    if (!descr || !(strcmp(descr, strCFDL) == 0 || strcmp(descr, strCFDU) == 0 || strcmp(descr, strCFZL) == 0 || strcmp(descr, strCFZU) == 0))"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44078",
        "func_name": "unicorn-engine/unicorn/uc_invalidate_tb",
        "description": "An issue was discovered in split_region in uc.c in Unicorn Engine before 2.0.0-rc5. It allows local attackers to escape the sandbox. An attacker must first obtain the ability to execute crafted code in the target sandbox in order to exploit this vulnerability. The specific flaw exists within the virtual memory manager. The issue results from the faulty comparison of GVA and GPA while calling uc_mem_map_ptr to free part of a claimed memory block. An attacker can leverage this vulnerability to escape the sandbox and execute arbitrary code on the host machine.",
        "git_url": "https://github.com/unicorn-engine/unicorn/commit/c733bbada356b0373fa8aa72c044574bb855fd24",
        "commit_title": "Fix wrong offset used in split_region",
        "commit_text": "",
        "func_before": "static void uc_invalidate_tb(struct uc_struct *uc, uint64_t start_addr, size_t len) \n{\n    tb_page_addr_t start, end;\n\n    // GVA to GPA (GPA -> HVA via page_find, HVA->HPA via host mmu)\n    start = get_page_addr_code(uc->cpu->env_ptr, start_addr) & (target_ulong)(-1);\n    \n    // For 32bit target.\n    end = (start + len) & (target_ulong)(-1);\n\n    // We get a wrap?\n    if (start > end) {\n        return;\n    }\n\n    tb_invalidate_phys_range(uc, start, end);\n}",
        "func": "static void uc_invalidate_tb(struct uc_struct *uc, uint64_t start_addr, size_t len) \n{\n    tb_page_addr_t start, end;\n\n    // GVA to GPA \n    // (GPA -> HVA via memory_region_get_ram_addr(mr) + GPA + block->host,\n    // HVA->HPA via host mmu)\n    start = get_page_addr_code(uc->cpu->env_ptr, start_addr) & (target_ulong)(-1);\n    \n    // For 32bit target.\n    end = (start + len) & (target_ulong)(-1);\n\n    // We get a wrap?\n    if (start > end) {\n        return;\n    }\n\n    tb_invalidate_phys_range(uc, start, end);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,7 +2,9 @@\n {\n     tb_page_addr_t start, end;\n \n-    // GVA to GPA (GPA -> HVA via page_find, HVA->HPA via host mmu)\n+    // GVA to GPA \n+    // (GPA -> HVA via memory_region_get_ram_addr(mr) + GPA + block->host,\n+    // HVA->HPA via host mmu)\n     start = get_page_addr_code(uc->cpu->env_ptr, start_addr) & (target_ulong)(-1);\n     \n     // For 32bit target.",
        "diff_line_info": {
            "deleted_lines": [
                "    // GVA to GPA (GPA -> HVA via page_find, HVA->HPA via host mmu)"
            ],
            "added_lines": [
                "    // GVA to GPA ",
                "    // (GPA -> HVA via memory_region_get_ram_addr(mr) + GPA + block->host,",
                "    // HVA->HPA via host mmu)"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-44078",
        "func_name": "unicorn-engine/unicorn/split_region",
        "description": "An issue was discovered in split_region in uc.c in Unicorn Engine before 2.0.0-rc5. It allows local attackers to escape the sandbox. An attacker must first obtain the ability to execute crafted code in the target sandbox in order to exploit this vulnerability. The specific flaw exists within the virtual memory manager. The issue results from the faulty comparison of GVA and GPA while calling uc_mem_map_ptr to free part of a claimed memory block. An attacker can leverage this vulnerability to escape the sandbox and execute arbitrary code on the host machine.",
        "git_url": "https://github.com/unicorn-engine/unicorn/commit/c733bbada356b0373fa8aa72c044574bb855fd24",
        "commit_title": "Fix wrong offset used in split_region",
        "commit_text": "",
        "func_before": "static bool split_region(struct uc_struct *uc, MemoryRegion *mr,\n                         uint64_t address, size_t size, bool do_delete)\n{\n    uint8_t *backup;\n    uint32_t perms;\n    uint64_t begin, end, chunk_end;\n    size_t l_size, m_size, r_size;\n    RAMBlock *block = NULL;\n    bool prealloc = false;\n\n    chunk_end = address + size;\n\n    // if this region belongs to area [address, address+size],\n    // then there is no work to do.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        // trivial case\n        return true;\n    }\n\n    if (address >= mr->end || chunk_end <= mr->addr) {\n        // impossible case\n        return false;\n    }\n\n    QLIST_FOREACH(block, &uc->ram_list.blocks, next)\n    {\n        if (block->offset <= mr->addr &&\n            block->used_length >= (mr->end - mr->addr)) {\n            break;\n        }\n    }\n\n    if (block == NULL) {\n        return false;\n    }\n\n    // RAM_PREALLOC is not defined outside exec.c and I didn't feel like\n    // moving it\n    prealloc = !!(block->flags & 1);\n\n    if (block->flags & 1) {\n        backup = block->host;\n    } else {\n        backup = copy_region(uc, mr);\n        if (backup == NULL) {\n            return false;\n        }\n    }\n\n    // save the essential information required for the split before mr gets\n    // deleted\n    perms = mr->perms;\n    begin = mr->addr;\n    end = mr->end;\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        goto error;\n    }\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n    m_size = (size_t)(chunk_end - address);\n\n    // If there are error in any of the below operations, things are too far\n    // gone at that point to recover. Could try to remap orignal region, but\n    // these smaller allocation just failed so no guarantee that we can recover\n    // the original allocation at this point\n    if (l_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, begin, l_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, begin, backup, l_size) != UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, begin, l_size, perms, backup) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (m_size > 0 && !do_delete) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, address, m_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, address, backup + l_size, m_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, address, m_size, perms, backup + l_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (r_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, chunk_end, r_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, chunk_end, backup + l_size + m_size, r_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, chunk_end, r_size, perms,\n                               backup + l_size + m_size) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (!prealloc) {\n        free(backup);\n    }\n    return true;\n\nerror:\n    if (!prealloc) {\n        free(backup);\n    }\n    return false;\n}",
        "func": "static bool split_region(struct uc_struct *uc, MemoryRegion *mr,\n                         uint64_t address, size_t size, bool do_delete)\n{\n    uint8_t *backup;\n    uint32_t perms;\n    uint64_t begin, end, chunk_end;\n    size_t l_size, m_size, r_size;\n    RAMBlock *block = NULL;\n    bool prealloc = false;\n\n    chunk_end = address + size;\n\n    // if this region belongs to area [address, address+size],\n    // then there is no work to do.\n    if (address <= mr->addr && chunk_end >= mr->end) {\n        return true;\n    }\n\n    if (size == 0) {\n        // trivial case\n        return true;\n    }\n\n    if (address >= mr->end || chunk_end <= mr->addr) {\n        // impossible case\n        return false;\n    }\n\n    QLIST_FOREACH(block, &uc->ram_list.blocks, next)\n    {\n        // block->offset is the offset within ram_addr_t, not GPA\n        if (block->mr->addr <= mr->addr &&\n            block->used_length >= (mr->end - mr->addr)) {\n            break;\n        }\n    }\n\n    if (block == NULL) {\n        return false;\n    }\n\n    // RAM_PREALLOC is not defined outside exec.c and I didn't feel like\n    // moving it\n    prealloc = !!(block->flags & 1);\n\n    if (block->flags & 1) {\n        backup = block->host;\n    } else {\n        backup = copy_region(uc, mr);\n        if (backup == NULL) {\n            return false;\n        }\n    }\n\n    // save the essential information required for the split before mr gets\n    // deleted\n    perms = mr->perms;\n    begin = mr->addr;\n    end = mr->end;\n\n    // unmap this region first, then do split it later\n    if (uc_mem_unmap(uc, mr->addr, (size_t)int128_get64(mr->size)) !=\n        UC_ERR_OK) {\n        goto error;\n    }\n\n    /* overlapping cases\n     *               |------mr------|\n     * case 1    |---size--|\n     * case 2           |--size--|\n     * case 3                  |---size--|\n     */\n\n    // adjust some things\n    if (address < begin) {\n        address = begin;\n    }\n    if (chunk_end > end) {\n        chunk_end = end;\n    }\n\n    // compute sub region sizes\n    l_size = (size_t)(address - begin);\n    r_size = (size_t)(end - chunk_end);\n    m_size = (size_t)(chunk_end - address);\n\n    // If there are error in any of the below operations, things are too far\n    // gone at that point to recover. Could try to remap orignal region, but\n    // these smaller allocation just failed so no guarantee that we can recover\n    // the original allocation at this point\n    if (l_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, begin, l_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, begin, backup, l_size) != UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, begin, l_size, perms, backup) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (m_size > 0 && !do_delete) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, address, m_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, address, backup + l_size, m_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, address, m_size, perms, backup + l_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (r_size > 0) {\n        if (!prealloc) {\n            if (uc_mem_map(uc, chunk_end, r_size, perms) != UC_ERR_OK) {\n                goto error;\n            }\n            if (uc_mem_write(uc, chunk_end, backup + l_size + m_size, r_size) !=\n                UC_ERR_OK) {\n                goto error;\n            }\n        } else {\n            if (uc_mem_map_ptr(uc, chunk_end, r_size, perms,\n                               backup + l_size + m_size) != UC_ERR_OK) {\n                goto error;\n            }\n        }\n    }\n\n    if (!prealloc) {\n        free(backup);\n    }\n    return true;\n\nerror:\n    if (!prealloc) {\n        free(backup);\n    }\n    return false;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -28,7 +28,8 @@\n \n     QLIST_FOREACH(block, &uc->ram_list.blocks, next)\n     {\n-        if (block->offset <= mr->addr &&\n+        // block->offset is the offset within ram_addr_t, not GPA\n+        if (block->mr->addr <= mr->addr &&\n             block->used_length >= (mr->end - mr->addr)) {\n             break;\n         }",
        "diff_line_info": {
            "deleted_lines": [
                "        if (block->offset <= mr->addr &&"
            ],
            "added_lines": [
                "        // block->offset is the offset within ram_addr_t, not GPA",
                "        if (block->mr->addr <= mr->addr &&"
            ]
        }
    },
    {
        "cve_id": "CVE-2021-40562",
        "func_name": "gpac/naludmx_create_avc_decoder_config",
        "description": "A Segmentation fault caused by a floating point exception exists in Gpac through 1.0.1 using mp4box via the naludmx_enqueue_or_dispatch function in reframe_nalu.c, which causes a denial of service.",
        "git_url": "https://github.com/gpac/gpac/commit/5dd71c7201a3e5cf40732d585bfb21c906c171d3",
        "commit_title": "fixed #1901",
        "commit_text": "",
        "func_before": "void naludmx_create_avc_decoder_config(GF_NALUDmxCtx *ctx, u8 **dsi, u32 *dsi_size, u8 **dsi_enh, u32 *dsi_enh_size, u32 *max_width, u32 *max_height, u32 *max_enh_width, u32 *max_enh_height, GF_Fraction *sar)\n{\n\tu32 i, count;\n\tBool first = GF_TRUE;\n\tBool first_svc = GF_TRUE;\n\tGF_AVCConfig *cfg;\n\tGF_AVCConfig *avcc;\n\tGF_AVCConfig *svcc;\n\tu32 max_w, max_h, max_ew, max_eh;\n\n\n\tmax_w = max_h = max_ew = max_eh = 0;\n\tsar->num = sar->den = 0;\n\n\tavcc = gf_odf_avc_cfg_new();\n\tsvcc = gf_odf_avc_cfg_new();\n\tavcc->nal_unit_size = ctx->nal_length;\n\tsvcc->nal_unit_size = ctx->nal_length;\n\n\tctx->is_mvc = GF_FALSE;\n\tcount = gf_list_count(ctx->sps);\n\tfor (i=0; i<count; i++) {\n\t\tBool is_svc = GF_FALSE;\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->sps, i);\n\t\tAVC_SPS *sps = &ctx->avc_state->sps[sl->id];\n\t\tu32 nal_type = sl->data[0] & 0x1F;\n\n\t\tif ((sps->profile_idc == 118) || (sps->profile_idc == 128)) {\n\t\t\tctx->is_mvc = GF_TRUE;\n\t\t}\n\n\t\tif (ctx->explicit) {\n\t\t\tcfg = svcc;\n\t\t} else if (nal_type == GF_AVC_NALU_SVC_SUBSEQ_PARAM) {\n\t\t\tcfg = svcc;\n\t\t\tis_svc = GF_TRUE;\n\t\t} else {\n\t\t\tcfg = avcc;\n\t\t}\n\n\t\tif (first || (is_svc && first_svc) ) {\n\t\t\tcfg->configurationVersion = 1;\n\t\t\tcfg->profile_compatibility = sps->prof_compat;\n\t\t\tcfg->AVCProfileIndication = sps->profile_idc;\n\t\t\tcfg->AVCLevelIndication = sps->level_idc;\n\t\t\tcfg->chroma_format = sps->chroma_format;\n\t\t\tcfg->luma_bit_depth = 8 + sps->luma_bit_depth_m8;\n\t\t\tcfg->chroma_bit_depth = 8 + sps->chroma_bit_depth_m8;\n\t\t\t/*try to patch ?*/\n\t\t\tif (!gf_avc_is_rext_profile(cfg->AVCProfileIndication)\n\t\t\t\t&& ((cfg->chroma_format>1) || (cfg->luma_bit_depth>8) || (cfg->chroma_bit_depth>8))\n\t\t\t) {\n\t\t\t\tif ((cfg->luma_bit_depth>8) || (cfg->chroma_bit_depth>8)) {\n\t\t\t\t\tcfg->AVCProfileIndication = 110;\n\t\t\t\t} else {\n\t\t\t\t\tcfg->AVCProfileIndication = (cfg->chroma_format==3) ? 244 : 122;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (sps->vui_parameters_present_flag && sps->vui.par_num && sps->vui.par_den) {\n\t\t\t\tsar->num = sps->vui.par_num;\n\t\t\t\tsar->den = sps->vui.par_den;\n\t\t\t}\n\t\t\tctx->interlaced = sps->frame_mbs_only_flag ? GF_FALSE : GF_TRUE;\n\n\n\t\t\t/*disable frame rate scan, most bitstreams have wrong values there*/\n\t\t\tif (first && (!ctx->fps.num || !ctx->fps.den) && sps->vui.timing_info_present_flag\n\t\t\t\t/*if detected FPS is greater than 1000, assume wrong timing info*/\n\t\t\t\t&& (sps->vui.time_scale <= 1000*sps->vui.num_units_in_tick)\n\t\t\t) {\n\t\t\t\t/*ISO/IEC 14496-10 n11084 Table E-6*/\n\t\t\t\t/* not used :\t\t\t\tu8 DeltaTfiDivisorTable[] = {1,1,1,2,2,2,2,3,3,4,6}; */\n\t\t\t\tu8 DeltaTfiDivisorIdx;\n\t\t\t\tif (!sps->vui.pic_struct_present_flag) {\n\t\t\t\t\tDeltaTfiDivisorIdx = 1 + (1 - ctx->avc_state->s_info.field_pic_flag);\n\t\t\t\t} else {\n\t\t\t\t\tif (!ctx->avc_state->sei.pic_timing.pic_struct)\n\t\t\t\t\t\tDeltaTfiDivisorIdx = 2;\n\t\t\t\t\telse if (ctx->avc_state->sei.pic_timing.pic_struct == 8)\n\t\t\t\t\t\tDeltaTfiDivisorIdx = 6;\n\t\t\t\t\telse\n\t\t\t\t\t\tDeltaTfiDivisorIdx = (ctx->avc_state->sei.pic_timing.pic_struct+1) / 2;\n\t\t\t\t}\n\t\t\t\tif (!ctx->timescale) {\n\t\t\t\t\tctx->cur_fps.num = 2 * sps->vui.time_scale;\n\t\t\t\t\tctx->cur_fps.den =  2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;\n\n\t\t\t\t\tif (!ctx->fps.num && ctx->dts==ctx->fps.den)\n\t\t\t\t\t\tctx->dts = ctx->cur_fps.den;\n\t\t\t\t}\n\t\t\t\tif (! sps->vui.fixed_frame_rate_flag)\n\t\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_PARSER, (\"[%s] Possible Variable Frame Rate: VUI \\\"fixed_frame_rate_flag\\\" absent\\n\", ctx->log_name));\n\t\t\t}\n\t\t\tctx->fps = ctx->cur_fps;\n\t\t}\n\t\tfirst = GF_FALSE;\n\t\tif (is_svc) {\n\t\t\tfirst_svc = GF_FALSE;\n\t\t\tif (sps->width > max_ew) max_ew = sps->width;\n\t\t\tif (sps->height > max_eh) max_eh = sps->height;\n\t\t} else {\n\t\t\tif (sps->width > max_w) max_w = sps->width;\n\t\t\tif (sps->height > max_h) max_h = sps->height;\n\t\t}\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->sequenceParameterSets, sl);\n\t}\n\n\tcfg = ctx->explicit ? svcc : avcc;\n\tcount = gf_list_count(ctx->sps_ext);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->sps_ext, i);\n\t\tif (!cfg->sequenceParameterSetExtensions) cfg->sequenceParameterSetExtensions = gf_list_new();\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->sequenceParameterSetExtensions, sl);\n\t}\n\n\tcfg = ctx->explicit ? svcc : avcc;\n\tcount = gf_list_count(ctx->pps);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->pps, i);\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->pictureParameterSets, sl);\n\t}\n\n\tcfg = svcc;\n\tcount = gf_list_count(ctx->pps_svc);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->pps_svc, i);\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->pictureParameterSets, sl);\n\t}\n\n\t*dsi = *dsi_enh = NULL;\n\t*dsi_size = *dsi_enh_size = 0;\n\n\tif (ctx->explicit) {\n\t\tgf_odf_avc_cfg_write(svcc, dsi, dsi_size);\n\t} else {\n\t\tgf_odf_avc_cfg_write(avcc, dsi, dsi_size);\n\t\tif (gf_list_count(svcc->sequenceParameterSets) || svcc->sequenceParameterSetExtensions) {\n\t\t\tgf_odf_avc_cfg_write(svcc, dsi_enh, dsi_enh_size);\n\t\t}\n\t}\n\tgf_list_reset(avcc->sequenceParameterSets);\n\tgf_list_reset(avcc->sequenceParameterSetExtensions);\n\tgf_list_reset(avcc->pictureParameterSets);\n\tgf_list_reset(svcc->sequenceParameterSets);\n\tgf_list_reset(svcc->sequenceParameterSetExtensions);\n\tgf_list_reset(svcc->pictureParameterSets);\n\tgf_odf_avc_cfg_del(avcc);\n\tgf_odf_avc_cfg_del(svcc);\n\t*max_width = max_w;\n\t*max_height = max_h;\n\t*max_enh_width = max_ew;\n\t*max_enh_height = max_eh;\n}",
        "func": "void naludmx_create_avc_decoder_config(GF_NALUDmxCtx *ctx, u8 **dsi, u32 *dsi_size, u8 **dsi_enh, u32 *dsi_enh_size, u32 *max_width, u32 *max_height, u32 *max_enh_width, u32 *max_enh_height, GF_Fraction *sar)\n{\n\tu32 i, count;\n\tBool first = GF_TRUE;\n\tBool first_svc = GF_TRUE;\n\tGF_AVCConfig *cfg;\n\tGF_AVCConfig *avcc;\n\tGF_AVCConfig *svcc;\n\tu32 max_w, max_h, max_ew, max_eh;\n\n\n\tmax_w = max_h = max_ew = max_eh = 0;\n\tsar->num = sar->den = 0;\n\n\tavcc = gf_odf_avc_cfg_new();\n\tsvcc = gf_odf_avc_cfg_new();\n\tavcc->nal_unit_size = ctx->nal_length;\n\tsvcc->nal_unit_size = ctx->nal_length;\n\n\tctx->is_mvc = GF_FALSE;\n\tcount = gf_list_count(ctx->sps);\n\tfor (i=0; i<count; i++) {\n\t\tBool is_svc = GF_FALSE;\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->sps, i);\n\t\tAVC_SPS *sps = &ctx->avc_state->sps[sl->id];\n\t\tu32 nal_type = sl->data[0] & 0x1F;\n\n\t\tif ((sps->profile_idc == 118) || (sps->profile_idc == 128)) {\n\t\t\tctx->is_mvc = GF_TRUE;\n\t\t}\n\n\t\tif (ctx->explicit) {\n\t\t\tcfg = svcc;\n\t\t} else if (nal_type == GF_AVC_NALU_SVC_SUBSEQ_PARAM) {\n\t\t\tcfg = svcc;\n\t\t\tis_svc = GF_TRUE;\n\t\t} else {\n\t\t\tcfg = avcc;\n\t\t}\n\n\t\tif (first || (is_svc && first_svc) ) {\n\t\t\tcfg->configurationVersion = 1;\n\t\t\tcfg->profile_compatibility = sps->prof_compat;\n\t\t\tcfg->AVCProfileIndication = sps->profile_idc;\n\t\t\tcfg->AVCLevelIndication = sps->level_idc;\n\t\t\tcfg->chroma_format = sps->chroma_format;\n\t\t\tcfg->luma_bit_depth = 8 + sps->luma_bit_depth_m8;\n\t\t\tcfg->chroma_bit_depth = 8 + sps->chroma_bit_depth_m8;\n\t\t\t/*try to patch ?*/\n\t\t\tif (!gf_avc_is_rext_profile(cfg->AVCProfileIndication)\n\t\t\t\t&& ((cfg->chroma_format>1) || (cfg->luma_bit_depth>8) || (cfg->chroma_bit_depth>8))\n\t\t\t) {\n\t\t\t\tif ((cfg->luma_bit_depth>8) || (cfg->chroma_bit_depth>8)) {\n\t\t\t\t\tcfg->AVCProfileIndication = 110;\n\t\t\t\t} else {\n\t\t\t\t\tcfg->AVCProfileIndication = (cfg->chroma_format==3) ? 244 : 122;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (sps->vui_parameters_present_flag && sps->vui.par_num && sps->vui.par_den) {\n\t\t\t\tsar->num = sps->vui.par_num;\n\t\t\t\tsar->den = sps->vui.par_den;\n\t\t\t}\n\t\t\tctx->interlaced = sps->frame_mbs_only_flag ? GF_FALSE : GF_TRUE;\n\n\n\t\t\t/*disable frame rate scan, most bitstreams have wrong values there*/\n\t\t\tif (first && (!ctx->fps.num || !ctx->fps.den) && sps->vui.timing_info_present_flag\n\t\t\t\t/*if detected FPS is greater than 1000, assume wrong timing info*/\n\t\t\t\t&& (sps->vui.time_scale <= 1000*sps->vui.num_units_in_tick)\n\t\t\t) {\n\t\t\t\t/*ISO/IEC 14496-10 n11084 Table E-6*/\n\t\t\t\t/* not used :\t\t\t\tu8 DeltaTfiDivisorTable[] = {1,1,1,2,2,2,2,3,3,4,6}; */\n\t\t\t\tu8 DeltaTfiDivisorIdx;\n\t\t\t\tif (!sps->vui.pic_struct_present_flag) {\n\t\t\t\t\tDeltaTfiDivisorIdx = 1 + (1 - ctx->avc_state->s_info.field_pic_flag);\n\t\t\t\t} else {\n\t\t\t\t\tif (!ctx->avc_state->sei.pic_timing.pic_struct)\n\t\t\t\t\t\tDeltaTfiDivisorIdx = 2;\n\t\t\t\t\telse if (ctx->avc_state->sei.pic_timing.pic_struct == 8)\n\t\t\t\t\t\tDeltaTfiDivisorIdx = 6;\n\t\t\t\t\telse\n\t\t\t\t\t\tDeltaTfiDivisorIdx = (ctx->avc_state->sei.pic_timing.pic_struct+1) / 2;\n\t\t\t\t}\n\t\t\t\tif (!ctx->timescale && sps->vui.time_scale && sps->vui.num_units_in_tick) {\n\t\t\t\t\tctx->cur_fps.num = 2 * sps->vui.time_scale;\n\t\t\t\t\tctx->cur_fps.den = 2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;\n\n\t\t\t\t\tif (!ctx->fps.num && ctx->dts==ctx->fps.den)\n\t\t\t\t\t\tctx->dts = ctx->cur_fps.den;\n\t\t\t\t}\n\t\t\t\tif (! sps->vui.fixed_frame_rate_flag)\n\t\t\t\t\tGF_LOG(GF_LOG_INFO, GF_LOG_PARSER, (\"[%s] Possible Variable Frame Rate: VUI \\\"fixed_frame_rate_flag\\\" absent\\n\", ctx->log_name));\n\t\t\t}\n\t\t\tctx->fps = ctx->cur_fps;\n\t\t}\n\t\tfirst = GF_FALSE;\n\t\tif (is_svc) {\n\t\t\tfirst_svc = GF_FALSE;\n\t\t\tif (sps->width > max_ew) max_ew = sps->width;\n\t\t\tif (sps->height > max_eh) max_eh = sps->height;\n\t\t} else {\n\t\t\tif (sps->width > max_w) max_w = sps->width;\n\t\t\tif (sps->height > max_h) max_h = sps->height;\n\t\t}\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->sequenceParameterSets, sl);\n\t}\n\n\tcfg = ctx->explicit ? svcc : avcc;\n\tcount = gf_list_count(ctx->sps_ext);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->sps_ext, i);\n\t\tif (!cfg->sequenceParameterSetExtensions) cfg->sequenceParameterSetExtensions = gf_list_new();\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->sequenceParameterSetExtensions, sl);\n\t}\n\n\tcfg = ctx->explicit ? svcc : avcc;\n\tcount = gf_list_count(ctx->pps);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->pps, i);\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->pictureParameterSets, sl);\n\t}\n\n\tcfg = svcc;\n\tcount = gf_list_count(ctx->pps_svc);\n\tfor (i=0; i<count; i++) {\n\t\tGF_NALUFFParam *sl = gf_list_get(ctx->pps_svc, i);\n\t\tif (!ctx->analyze)\n\t\t\tgf_list_add(cfg->pictureParameterSets, sl);\n\t}\n\n\t*dsi = *dsi_enh = NULL;\n\t*dsi_size = *dsi_enh_size = 0;\n\n\tif (ctx->explicit) {\n\t\tgf_odf_avc_cfg_write(svcc, dsi, dsi_size);\n\t} else {\n\t\tgf_odf_avc_cfg_write(avcc, dsi, dsi_size);\n\t\tif (gf_list_count(svcc->sequenceParameterSets) || svcc->sequenceParameterSetExtensions) {\n\t\t\tgf_odf_avc_cfg_write(svcc, dsi_enh, dsi_enh_size);\n\t\t}\n\t}\n\tgf_list_reset(avcc->sequenceParameterSets);\n\tgf_list_reset(avcc->sequenceParameterSetExtensions);\n\tgf_list_reset(avcc->pictureParameterSets);\n\tgf_list_reset(svcc->sequenceParameterSets);\n\tgf_list_reset(svcc->sequenceParameterSetExtensions);\n\tgf_list_reset(svcc->pictureParameterSets);\n\tgf_odf_avc_cfg_del(avcc);\n\tgf_odf_avc_cfg_del(svcc);\n\t*max_width = max_w;\n\t*max_height = max_h;\n\t*max_enh_width = max_ew;\n\t*max_enh_height = max_eh;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -81,9 +81,9 @@\n \t\t\t\t\telse\n \t\t\t\t\t\tDeltaTfiDivisorIdx = (ctx->avc_state->sei.pic_timing.pic_struct+1) / 2;\n \t\t\t\t}\n-\t\t\t\tif (!ctx->timescale) {\n+\t\t\t\tif (!ctx->timescale && sps->vui.time_scale && sps->vui.num_units_in_tick) {\n \t\t\t\t\tctx->cur_fps.num = 2 * sps->vui.time_scale;\n-\t\t\t\t\tctx->cur_fps.den =  2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;\n+\t\t\t\t\tctx->cur_fps.den = 2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;\n \n \t\t\t\t\tif (!ctx->fps.num && ctx->dts==ctx->fps.den)\n \t\t\t\t\t\tctx->dts = ctx->cur_fps.den;",
        "diff_line_info": {
            "deleted_lines": [
                "\t\t\t\tif (!ctx->timescale) {",
                "\t\t\t\t\tctx->cur_fps.den =  2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;"
            ],
            "added_lines": [
                "\t\t\t\tif (!ctx->timescale && sps->vui.time_scale && sps->vui.num_units_in_tick) {",
                "\t\t\t\t\tctx->cur_fps.den = 2 * sps->vui.num_units_in_tick * DeltaTfiDivisorIdx;"
            ]
        }
    }
]