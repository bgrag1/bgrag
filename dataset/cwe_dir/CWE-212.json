[
    {
        "cve_id": "CVE-2022-26365",
        "func_name": "xen-project/xen/parse_nic_config",
        "description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "git_url": "https://github.com/xen-project/xen/commit/54d8f27d0477937e1f99a414fc1ffd93d184b38a",
        "commit_title": "tools/libxl: report trusted backend status to frontends",
        "commit_text": " Allow administrators to notify a frontend driver that it's backend counterpart is not to be trusted, so the frontend can deploy whatever mitigations required in order to secure itself.  Allow such option for disk and network frontends only, as those are the only hardened ones currently supported.  This is part of XSA-403 ",
        "func_before": "int parse_nic_config(libxl_device_nic *nic, XLU_Config **config, char *token)\n{\n    char *endptr, *oparg;\n    int i;\n    unsigned int val;\n\n    if (MATCH_OPTION(\"type\", token, oparg)) {\n        if (!strcmp(\"vif\", oparg)) {\n            nic->nictype = LIBXL_NIC_TYPE_VIF;\n        } else if (!strcmp(\"ioemu\", oparg)) {\n            nic->nictype = LIBXL_NIC_TYPE_VIF_IOEMU;\n        } else {\n            fprintf(stderr, \"Invalid parameter `type'.\\n\");\n            return 1;\n        }\n    } else if (MATCH_OPTION(\"mac\", token, oparg)) {\n        for (i = 0; i < 6; i++) {\n            val = strtoul(oparg, &endptr, 16);\n            if ((oparg == endptr) || (val > 255)) {\n                fprintf(stderr, \"Invalid parameter `mac'.\\n\");\n                return 1;\n            }\n            nic->mac[i] = val;\n            oparg = endptr + 1;\n        }\n    } else if (MATCH_OPTION(\"bridge\", token, oparg)) {\n        replace_string(&nic->bridge, oparg);\n    } else if (MATCH_OPTION(\"netdev\", token, oparg)) {\n        fprintf(stderr, \"the netdev parameter is deprecated, \"\n                        \"please use gatewaydev instead\\n\");\n        replace_string(&nic->gatewaydev, oparg);\n    } else if (MATCH_OPTION(\"gatewaydev\", token, oparg)) {\n        replace_string(&nic->gatewaydev, oparg);\n    } else if (MATCH_OPTION(\"ip\", token, oparg)) {\n        replace_string(&nic->ip, oparg);\n    } else if (MATCH_OPTION(\"script\", token, oparg)) {\n        replace_string(&nic->script, oparg);\n    } else if (MATCH_OPTION(\"backend\", token, oparg)) {\n        replace_string(&nic->backend_domname, oparg);\n    } else if (MATCH_OPTION(\"vifname\", token, oparg)) {\n        replace_string(&nic->ifname, oparg);\n    } else if (MATCH_OPTION(\"model\", token, oparg)) {\n        replace_string(&nic->model, oparg);\n    } else if (MATCH_OPTION(\"rate\", token, oparg)) {\n        parse_vif_rate(config, oparg, nic);\n    } else if (MATCH_OPTION(\"forwarddev\", token, oparg)) {\n        replace_string(&nic->coloft_forwarddev, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_port, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_mirror_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_mirror_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_mirror_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_mirror_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_pri_in\", token, oparg)) {\n        replace_string(&nic->colo_compare_pri_in, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_sec_in\", token, oparg)) {\n        replace_string(&nic->colo_compare_sec_in, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_out\", token, oparg)) {\n        replace_string(&nic->colo_compare_out, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_notify_dev\", token, oparg)) {\n        replace_string(&nic->colo_compare_notify_dev, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_port, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_rewriter0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_rewriter0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_checkpoint_host\", token, oparg)) {\n        replace_string(&nic->colo_checkpoint_host, oparg);\n    } else if (MATCH_OPTION(\"colo_checkpoint_port\", token, oparg)) {\n        replace_string(&nic->colo_checkpoint_port, oparg);\n    } else if (MATCH_OPTION(\"accel\", token, oparg)) {\n        fprintf(stderr, \"the accel parameter for vifs is currently not supported\\n\");\n    } else if (MATCH_OPTION(\"devid\", token, oparg)) {\n        nic->devid = parse_ulong(oparg);\n    } else if (MATCH_OPTION(\"mtu\", token, oparg)) {\n        nic->mtu = parse_ulong(oparg);\n    } else {\n        fprintf(stderr, \"unrecognized argument `%s'\\n\", token);\n        return 1;\n    }\n    return 0;\n}",
        "func": "int parse_nic_config(libxl_device_nic *nic, XLU_Config **config, char *token)\n{\n    char *endptr, *oparg;\n    int i;\n    unsigned int val;\n\n    if (MATCH_OPTION(\"type\", token, oparg)) {\n        if (!strcmp(\"vif\", oparg)) {\n            nic->nictype = LIBXL_NIC_TYPE_VIF;\n        } else if (!strcmp(\"ioemu\", oparg)) {\n            nic->nictype = LIBXL_NIC_TYPE_VIF_IOEMU;\n        } else {\n            fprintf(stderr, \"Invalid parameter `type'.\\n\");\n            return 1;\n        }\n    } else if (MATCH_OPTION(\"mac\", token, oparg)) {\n        for (i = 0; i < 6; i++) {\n            val = strtoul(oparg, &endptr, 16);\n            if ((oparg == endptr) || (val > 255)) {\n                fprintf(stderr, \"Invalid parameter `mac'.\\n\");\n                return 1;\n            }\n            nic->mac[i] = val;\n            oparg = endptr + 1;\n        }\n    } else if (MATCH_OPTION(\"bridge\", token, oparg)) {\n        replace_string(&nic->bridge, oparg);\n    } else if (MATCH_OPTION(\"netdev\", token, oparg)) {\n        fprintf(stderr, \"the netdev parameter is deprecated, \"\n                        \"please use gatewaydev instead\\n\");\n        replace_string(&nic->gatewaydev, oparg);\n    } else if (MATCH_OPTION(\"gatewaydev\", token, oparg)) {\n        replace_string(&nic->gatewaydev, oparg);\n    } else if (MATCH_OPTION(\"ip\", token, oparg)) {\n        replace_string(&nic->ip, oparg);\n    } else if (MATCH_OPTION(\"script\", token, oparg)) {\n        replace_string(&nic->script, oparg);\n    } else if (MATCH_OPTION(\"backend\", token, oparg)) {\n        replace_string(&nic->backend_domname, oparg);\n    } else if (MATCH_OPTION(\"vifname\", token, oparg)) {\n        replace_string(&nic->ifname, oparg);\n    } else if (MATCH_OPTION(\"model\", token, oparg)) {\n        replace_string(&nic->model, oparg);\n    } else if (MATCH_OPTION(\"rate\", token, oparg)) {\n        parse_vif_rate(config, oparg, nic);\n    } else if (MATCH_OPTION(\"forwarddev\", token, oparg)) {\n        replace_string(&nic->coloft_forwarddev, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_mirror_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_mirror_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_sec_in_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_sec_in_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector0_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector0_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector1_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector1_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_redirector2_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_redirector2_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_pri_in_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_pri_in_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_compare_notify_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_compare_notify_port, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_mirror_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_mirror_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_mirror_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_mirror_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector0_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector0_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_redirector1_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_redirector1_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_pri_in\", token, oparg)) {\n        replace_string(&nic->colo_compare_pri_in, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_sec_in\", token, oparg)) {\n        replace_string(&nic->colo_compare_sec_in, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_out\", token, oparg)) {\n        replace_string(&nic->colo_compare_out, oparg);\n    } else if (MATCH_OPTION(\"colo_compare_notify_dev\", token, oparg)) {\n        replace_string(&nic->colo_compare_notify_dev, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector0_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector0_port, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_id\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_id, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_ip\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_ip, oparg);\n    } else if (MATCH_OPTION(\"colo_sock_sec_redirector1_port\", token, oparg)) {\n        replace_string(&nic->colo_sock_sec_redirector1_port, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector0_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector0_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_indev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_indev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_redirector1_outdev\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_redirector1_outdev, oparg);\n    } else if (MATCH_OPTION(\"colo_filter_sec_rewriter0_queue\", token, oparg)) {\n        replace_string(&nic->colo_filter_sec_rewriter0_queue, oparg);\n    } else if (MATCH_OPTION(\"colo_checkpoint_host\", token, oparg)) {\n        replace_string(&nic->colo_checkpoint_host, oparg);\n    } else if (MATCH_OPTION(\"colo_checkpoint_port\", token, oparg)) {\n        replace_string(&nic->colo_checkpoint_port, oparg);\n    } else if (MATCH_OPTION(\"accel\", token, oparg)) {\n        fprintf(stderr, \"the accel parameter for vifs is currently not supported\\n\");\n    } else if (MATCH_OPTION(\"devid\", token, oparg)) {\n        nic->devid = parse_ulong(oparg);\n    } else if (MATCH_OPTION(\"mtu\", token, oparg)) {\n        nic->mtu = parse_ulong(oparg);\n    } else if (!strcmp(\"trusted\", token)) {\n        libxl_defbool_set(&nic->trusted, true);\n    } else if (!strcmp(\"untrusted\", token)) {\n        libxl_defbool_set(&nic->trusted, false);\n    } else {\n        fprintf(stderr, \"unrecognized argument `%s'\\n\", token);\n        return 1;\n    }\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -147,6 +147,10 @@\n         nic->devid = parse_ulong(oparg);\n     } else if (MATCH_OPTION(\"mtu\", token, oparg)) {\n         nic->mtu = parse_ulong(oparg);\n+    } else if (!strcmp(\"trusted\", token)) {\n+        libxl_defbool_set(&nic->trusted, true);\n+    } else if (!strcmp(\"untrusted\", token)) {\n+        libxl_defbool_set(&nic->trusted, false);\n     } else {\n         fprintf(stderr, \"unrecognized argument `%s'\\n\", token);\n         return 1;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    } else if (!strcmp(\"trusted\", token)) {",
                "        libxl_defbool_set(&nic->trusted, true);",
                "    } else if (!strcmp(\"untrusted\", token)) {",
                "        libxl_defbool_set(&nic->trusted, false);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26365",
        "func_name": "xen-project/xen/libxl__device_disk_setdefault",
        "description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "git_url": "https://github.com/xen-project/xen/commit/54d8f27d0477937e1f99a414fc1ffd93d184b38a",
        "commit_title": "tools/libxl: report trusted backend status to frontends",
        "commit_text": " Allow administrators to notify a frontend driver that it's backend counterpart is not to be trusted, so the frontend can deploy whatever mitigations required in order to secure itself.  Allow such option for disk and network frontends only, as those are the only hardened ones currently supported.  This is part of XSA-403 ",
        "func_before": "static int libxl__device_disk_setdefault(libxl__gc *gc, uint32_t domid,\n                                         libxl_device_disk *disk, bool hotplug)\n{\n    int rc;\n\n    libxl_defbool_setdefault(&disk->discard_enable, !!disk->readwrite);\n    libxl_defbool_setdefault(&disk->colo_enable, false);\n    libxl_defbool_setdefault(&disk->colo_restore_enable, false);\n\n    rc = libxl__resolve_domid(gc, disk->backend_domname, &disk->backend_domid);\n    if (rc < 0) return rc;\n\n    /* Force Qdisk backend for CDROM devices of guests with a device model. */\n    if (disk->is_cdrom != 0 &&\n        libxl__domain_type(gc, domid) == LIBXL_DOMAIN_TYPE_HVM) {\n        if (!(disk->backend == LIBXL_DISK_BACKEND_QDISK ||\n              disk->backend == LIBXL_DISK_BACKEND_UNKNOWN)) {\n            LOGD(ERROR, domid, \"Backend for CD devices on HVM guests must be Qdisk\");\n            return ERROR_FAIL;\n        }\n        disk->backend = LIBXL_DISK_BACKEND_QDISK;\n    }\n\n    rc = libxl__device_disk_set_backend(gc, disk);\n    return rc;\n}",
        "func": "static int libxl__device_disk_setdefault(libxl__gc *gc, uint32_t domid,\n                                         libxl_device_disk *disk, bool hotplug)\n{\n    int rc;\n\n    libxl_defbool_setdefault(&disk->discard_enable, !!disk->readwrite);\n    libxl_defbool_setdefault(&disk->colo_enable, false);\n    libxl_defbool_setdefault(&disk->colo_restore_enable, false);\n    libxl_defbool_setdefault(&disk->trusted, true);\n\n    rc = libxl__resolve_domid(gc, disk->backend_domname, &disk->backend_domid);\n    if (rc < 0) return rc;\n\n    /* Force Qdisk backend for CDROM devices of guests with a device model. */\n    if (disk->is_cdrom != 0 &&\n        libxl__domain_type(gc, domid) == LIBXL_DOMAIN_TYPE_HVM) {\n        if (!(disk->backend == LIBXL_DISK_BACKEND_QDISK ||\n              disk->backend == LIBXL_DISK_BACKEND_UNKNOWN)) {\n            LOGD(ERROR, domid, \"Backend for CD devices on HVM guests must be Qdisk\");\n            return ERROR_FAIL;\n        }\n        disk->backend = LIBXL_DISK_BACKEND_QDISK;\n    }\n\n    rc = libxl__device_disk_set_backend(gc, disk);\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -6,6 +6,7 @@\n     libxl_defbool_setdefault(&disk->discard_enable, !!disk->readwrite);\n     libxl_defbool_setdefault(&disk->colo_enable, false);\n     libxl_defbool_setdefault(&disk->colo_restore_enable, false);\n+    libxl_defbool_setdefault(&disk->trusted, true);\n \n     rc = libxl__resolve_domid(gc, disk->backend_domname, &disk->backend_domid);\n     if (rc < 0) return rc;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    libxl_defbool_setdefault(&disk->trusted, true);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26365",
        "func_name": "xen-project/xen/device_disk_add",
        "description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "git_url": "https://github.com/xen-project/xen/commit/54d8f27d0477937e1f99a414fc1ffd93d184b38a",
        "commit_title": "tools/libxl: report trusted backend status to frontends",
        "commit_text": " Allow administrators to notify a frontend driver that it's backend counterpart is not to be trusted, so the frontend can deploy whatever mitigations required in order to secure itself.  Allow such option for disk and network frontends only, as those are the only hardened ones currently supported.  This is part of XSA-403 ",
        "func_before": "static void device_disk_add(libxl__egc *egc, uint32_t domid,\n                           libxl_device_disk *disk,\n                           libxl__ao_device *aodev,\n                           char *get_vdev(libxl__gc *, void *,\n                                          xs_transaction_t),\n                           void *get_vdev_user)\n{\n    STATE_AO_GC(aodev->ao);\n    flexarray_t *front = NULL;\n    flexarray_t *back = NULL;\n    char *dev = NULL, *script;\n    libxl__device *device;\n    int rc;\n    libxl_ctx *ctx = gc->owner;\n    xs_transaction_t t = XBT_NULL;\n    libxl_domain_config d_config;\n    libxl_device_disk disk_saved;\n    libxl__flock *lock = NULL;\n\n    libxl_domain_config_init(&d_config);\n    libxl_device_disk_init(&disk_saved);\n    libxl_device_disk_copy(ctx, &disk_saved, disk);\n\n    libxl_domain_type type = libxl__domain_type(gc, domid);\n    if (type == LIBXL_DOMAIN_TYPE_INVALID) {\n        rc = ERROR_FAIL;\n        goto out;\n    }\n\n    /*\n     * get_vdev != NULL -> local attach\n     * get_vdev == NULL -> block attach\n     *\n     * We don't care about local attach state because it's only\n     * intermediate state.\n     */\n    if (!get_vdev && aodev->update_json) {\n        lock = libxl__lock_domain_userdata(gc, domid);\n        if (!lock) {\n            rc = ERROR_LOCK_FAIL;\n            goto out;\n        }\n\n        rc = libxl__get_domain_configuration(gc, domid, &d_config);\n        if (rc) goto out;\n\n        device_add_domain_config(gc, &d_config, &libxl__disk_devtype,\n                                 &disk_saved);\n\n        rc = libxl__dm_check_start(gc, &d_config, domid);\n        if (rc) goto out;\n    }\n\n    for (;;) {\n        rc = libxl__xs_transaction_start(gc, &t);\n        if (rc) goto out;\n\n        if (get_vdev) {\n            assert(get_vdev_user);\n            disk->vdev = get_vdev(gc, get_vdev_user, t);\n            if (disk->vdev == NULL) {\n                rc = ERROR_FAIL;\n                goto out;\n            }\n        }\n\n        rc = libxl__device_disk_setdefault(gc, domid, disk, aodev->update_json);\n        if (rc) goto out;\n\n        front = flexarray_make(gc, 16, 1);\n        back = flexarray_make(gc, 16, 1);\n\n        GCNEW(device);\n        rc = libxl__device_from_disk(gc, domid, disk, device);\n        if (rc != 0) {\n            LOGD(ERROR, domid, \"Invalid or unsupported\"\" virtual disk identifier %s\",\n                 disk->vdev);\n            goto out;\n        }\n\n        rc = libxl__device_exists(gc, t, device);\n        if (rc < 0) goto out;\n        if (rc == 1) {              /* already exists in xenstore */\n            LOGD(ERROR, domid, \"device already exists in xenstore\");\n            aodev->action = LIBXL__DEVICE_ACTION_ADD; /* for error message */\n            rc = ERROR_DEVICE_EXISTS;\n            goto out;\n        }\n\n        switch (disk->backend) {\n            case LIBXL_DISK_BACKEND_PHY:\n                dev = disk->pdev_path;\n\n                flexarray_append(back, \"params\");\n                flexarray_append(back, dev);\n\n                script = libxl__abs_path(gc, disk->script?: \"block\",\n                                         libxl__xen_script_dir_path());\n                flexarray_append_pair(back, \"script\", script);\n\n                assert(device->backend_kind == LIBXL__DEVICE_KIND_VBD);\n                break;\n\n            case LIBXL_DISK_BACKEND_TAP:\n                LOG(ERROR, \"blktap is not supported\");\n                rc = ERROR_FAIL;\n                goto out;\n            case LIBXL_DISK_BACKEND_QDISK:\n                flexarray_append(back, \"params\");\n                flexarray_append(back, GCSPRINTF(\"%s:%s\",\n                              libxl__device_disk_string_of_format(disk->format),\n                              disk->pdev_path ? : \"\"));\n                if (libxl_defbool_val(disk->colo_enable)) {\n                    flexarray_append(back, \"colo-host\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->colo_host));\n                    flexarray_append(back, \"colo-port\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%d\", disk->colo_port));\n                    flexarray_append(back, \"colo-export\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->colo_export));\n                    flexarray_append(back, \"active-disk\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->active_disk));\n                    flexarray_append(back, \"hidden-disk\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->hidden_disk));\n                }\n                assert(device->backend_kind == LIBXL__DEVICE_KIND_QDISK);\n                break;\n            default:\n                LOGD(ERROR, domid, \"Unrecognized disk backend type: %d\",\n                     disk->backend);\n                rc = ERROR_INVAL;\n                goto out;\n        }\n\n        flexarray_append(back, \"frontend-id\");\n        flexarray_append(back, GCSPRINTF(\"%d\", domid));\n        flexarray_append(back, \"online\");\n        flexarray_append(back, \"1\");\n        flexarray_append(back, \"removable\");\n        flexarray_append(back, GCSPRINTF(\"%d\", (disk->removable) ? 1 : 0));\n        flexarray_append(back, \"bootable\");\n        flexarray_append(back, GCSPRINTF(\"%d\", 1));\n        flexarray_append(back, \"state\");\n        flexarray_append(back, GCSPRINTF(\"%d\", XenbusStateInitialising));\n        flexarray_append(back, \"dev\");\n        flexarray_append(back, disk->vdev);\n        flexarray_append(back, \"type\");\n        flexarray_append(back, libxl__device_disk_string_of_backend(disk->backend));\n        flexarray_append(back, \"mode\");\n        flexarray_append(back, disk->readwrite ? \"w\" : \"r\");\n        flexarray_append(back, \"device-type\");\n        flexarray_append(back, disk->is_cdrom ? \"cdrom\" : \"disk\");\n        if (disk->direct_io_safe) {\n            flexarray_append(back, \"direct-io-safe\");\n            flexarray_append(back, \"1\");\n        }\n        flexarray_append_pair(back, \"discard-enable\",\n                              libxl_defbool_val(disk->discard_enable) ?\n                              \"1\" : \"0\");\n\n        flexarray_append(front, \"backend-id\");\n        flexarray_append(front, GCSPRINTF(\"%d\", disk->backend_domid));\n        flexarray_append(front, \"state\");\n        flexarray_append(front, GCSPRINTF(\"%d\", XenbusStateInitialising));\n        flexarray_append(front, \"virtual-device\");\n        flexarray_append(front, GCSPRINTF(\"%d\", device->devid));\n        flexarray_append(front, \"device-type\");\n        flexarray_append(front, disk->is_cdrom ? \"cdrom\" : \"disk\");\n\n        /*\n         * Old PV kernel disk frontends before 2.6.26 rely on tool stack to\n         * write disk native protocol to frontend node. Xend does this, port\n         * this behaviour to xl.\n         *\n         * New kernels write this node themselves. In that case it just\n         * overwrites an existing node which is OK.\n         */\n        if (type == LIBXL_DOMAIN_TYPE_PV) {\n            const char *protocol =\n                xc_domain_get_native_protocol(ctx->xch, domid);\n            if (protocol) {\n                flexarray_append(front, \"protocol\");\n                flexarray_append(front, libxl__strdup(gc, protocol));\n            }\n        }\n\n        if (!get_vdev && aodev->update_json) {\n            rc = libxl__set_domain_configuration(gc, domid, &d_config);\n            if (rc) goto out;\n        }\n\n        libxl__device_generic_add(gc, t, device,\n                                  libxl__xs_kvs_of_flexarray(gc, back),\n                                  libxl__xs_kvs_of_flexarray(gc, front),\n                                  NULL);\n\n        rc = libxl__xs_transaction_commit(gc, &t);\n        if (!rc) break;\n        if (rc < 0) goto out;\n    }\n\n    aodev->dev = device;\n    aodev->action = LIBXL__DEVICE_ACTION_ADD;\n    libxl__wait_device_connection(egc, aodev);\n\n    rc = 0;\n\nout:\n    libxl__xs_transaction_abort(gc, &t);\n    if (lock) libxl__unlock_file(lock);\n    libxl_device_disk_dispose(&disk_saved);\n    libxl_domain_config_dispose(&d_config);\n    aodev->rc = rc;\n    if (rc) aodev->callback(egc, aodev);\n    return;\n}",
        "func": "static void device_disk_add(libxl__egc *egc, uint32_t domid,\n                           libxl_device_disk *disk,\n                           libxl__ao_device *aodev,\n                           char *get_vdev(libxl__gc *, void *,\n                                          xs_transaction_t),\n                           void *get_vdev_user)\n{\n    STATE_AO_GC(aodev->ao);\n    flexarray_t *front = NULL;\n    flexarray_t *back = NULL;\n    char *dev = NULL, *script;\n    libxl__device *device;\n    int rc;\n    libxl_ctx *ctx = gc->owner;\n    xs_transaction_t t = XBT_NULL;\n    libxl_domain_config d_config;\n    libxl_device_disk disk_saved;\n    libxl__flock *lock = NULL;\n\n    libxl_domain_config_init(&d_config);\n    libxl_device_disk_init(&disk_saved);\n    libxl_device_disk_copy(ctx, &disk_saved, disk);\n\n    libxl_domain_type type = libxl__domain_type(gc, domid);\n    if (type == LIBXL_DOMAIN_TYPE_INVALID) {\n        rc = ERROR_FAIL;\n        goto out;\n    }\n\n    /*\n     * get_vdev != NULL -> local attach\n     * get_vdev == NULL -> block attach\n     *\n     * We don't care about local attach state because it's only\n     * intermediate state.\n     */\n    if (!get_vdev && aodev->update_json) {\n        lock = libxl__lock_domain_userdata(gc, domid);\n        if (!lock) {\n            rc = ERROR_LOCK_FAIL;\n            goto out;\n        }\n\n        rc = libxl__get_domain_configuration(gc, domid, &d_config);\n        if (rc) goto out;\n\n        device_add_domain_config(gc, &d_config, &libxl__disk_devtype,\n                                 &disk_saved);\n\n        rc = libxl__dm_check_start(gc, &d_config, domid);\n        if (rc) goto out;\n    }\n\n    for (;;) {\n        rc = libxl__xs_transaction_start(gc, &t);\n        if (rc) goto out;\n\n        if (get_vdev) {\n            assert(get_vdev_user);\n            disk->vdev = get_vdev(gc, get_vdev_user, t);\n            if (disk->vdev == NULL) {\n                rc = ERROR_FAIL;\n                goto out;\n            }\n        }\n\n        rc = libxl__device_disk_setdefault(gc, domid, disk, aodev->update_json);\n        if (rc) goto out;\n\n        front = flexarray_make(gc, 16, 1);\n        back = flexarray_make(gc, 16, 1);\n\n        GCNEW(device);\n        rc = libxl__device_from_disk(gc, domid, disk, device);\n        if (rc != 0) {\n            LOGD(ERROR, domid, \"Invalid or unsupported\"\" virtual disk identifier %s\",\n                 disk->vdev);\n            goto out;\n        }\n\n        rc = libxl__device_exists(gc, t, device);\n        if (rc < 0) goto out;\n        if (rc == 1) {              /* already exists in xenstore */\n            LOGD(ERROR, domid, \"device already exists in xenstore\");\n            aodev->action = LIBXL__DEVICE_ACTION_ADD; /* for error message */\n            rc = ERROR_DEVICE_EXISTS;\n            goto out;\n        }\n\n        switch (disk->backend) {\n            case LIBXL_DISK_BACKEND_PHY:\n                dev = disk->pdev_path;\n\n                flexarray_append(back, \"params\");\n                flexarray_append(back, dev);\n\n                script = libxl__abs_path(gc, disk->script?: \"block\",\n                                         libxl__xen_script_dir_path());\n                flexarray_append_pair(back, \"script\", script);\n\n                assert(device->backend_kind == LIBXL__DEVICE_KIND_VBD);\n                break;\n\n            case LIBXL_DISK_BACKEND_TAP:\n                LOG(ERROR, \"blktap is not supported\");\n                rc = ERROR_FAIL;\n                goto out;\n            case LIBXL_DISK_BACKEND_QDISK:\n                flexarray_append(back, \"params\");\n                flexarray_append(back, GCSPRINTF(\"%s:%s\",\n                              libxl__device_disk_string_of_format(disk->format),\n                              disk->pdev_path ? : \"\"));\n                if (libxl_defbool_val(disk->colo_enable)) {\n                    flexarray_append(back, \"colo-host\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->colo_host));\n                    flexarray_append(back, \"colo-port\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%d\", disk->colo_port));\n                    flexarray_append(back, \"colo-export\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->colo_export));\n                    flexarray_append(back, \"active-disk\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->active_disk));\n                    flexarray_append(back, \"hidden-disk\");\n                    flexarray_append(back, libxl__sprintf(gc, \"%s\", disk->hidden_disk));\n                }\n                assert(device->backend_kind == LIBXL__DEVICE_KIND_QDISK);\n                break;\n            default:\n                LOGD(ERROR, domid, \"Unrecognized disk backend type: %d\",\n                     disk->backend);\n                rc = ERROR_INVAL;\n                goto out;\n        }\n\n        flexarray_append(back, \"frontend-id\");\n        flexarray_append(back, GCSPRINTF(\"%d\", domid));\n        flexarray_append(back, \"online\");\n        flexarray_append(back, \"1\");\n        flexarray_append(back, \"removable\");\n        flexarray_append(back, GCSPRINTF(\"%d\", (disk->removable) ? 1 : 0));\n        flexarray_append(back, \"bootable\");\n        flexarray_append(back, GCSPRINTF(\"%d\", 1));\n        flexarray_append(back, \"state\");\n        flexarray_append(back, GCSPRINTF(\"%d\", XenbusStateInitialising));\n        flexarray_append(back, \"dev\");\n        flexarray_append(back, disk->vdev);\n        flexarray_append(back, \"type\");\n        flexarray_append(back, libxl__device_disk_string_of_backend(disk->backend));\n        flexarray_append(back, \"mode\");\n        flexarray_append(back, disk->readwrite ? \"w\" : \"r\");\n        flexarray_append(back, \"device-type\");\n        flexarray_append(back, disk->is_cdrom ? \"cdrom\" : \"disk\");\n        if (disk->direct_io_safe) {\n            flexarray_append(back, \"direct-io-safe\");\n            flexarray_append(back, \"1\");\n        }\n        flexarray_append_pair(back, \"discard-enable\",\n                              libxl_defbool_val(disk->discard_enable) ?\n                              \"1\" : \"0\");\n\n        flexarray_append(front, \"backend-id\");\n        flexarray_append(front, GCSPRINTF(\"%d\", disk->backend_domid));\n        flexarray_append(front, \"state\");\n        flexarray_append(front, GCSPRINTF(\"%d\", XenbusStateInitialising));\n        flexarray_append(front, \"virtual-device\");\n        flexarray_append(front, GCSPRINTF(\"%d\", device->devid));\n        flexarray_append(front, \"device-type\");\n        flexarray_append(front, disk->is_cdrom ? \"cdrom\" : \"disk\");\n        flexarray_append(front, \"trusted\");\n        flexarray_append(front, libxl_defbool_val(disk->trusted) ? \"1\" : \"0\");\n\n        /*\n         * Old PV kernel disk frontends before 2.6.26 rely on tool stack to\n         * write disk native protocol to frontend node. Xend does this, port\n         * this behaviour to xl.\n         *\n         * New kernels write this node themselves. In that case it just\n         * overwrites an existing node which is OK.\n         */\n        if (type == LIBXL_DOMAIN_TYPE_PV) {\n            const char *protocol =\n                xc_domain_get_native_protocol(ctx->xch, domid);\n            if (protocol) {\n                flexarray_append(front, \"protocol\");\n                flexarray_append(front, libxl__strdup(gc, protocol));\n            }\n        }\n\n        if (!get_vdev && aodev->update_json) {\n            rc = libxl__set_domain_configuration(gc, domid, &d_config);\n            if (rc) goto out;\n        }\n\n        libxl__device_generic_add(gc, t, device,\n                                  libxl__xs_kvs_of_flexarray(gc, back),\n                                  libxl__xs_kvs_of_flexarray(gc, front),\n                                  NULL);\n\n        rc = libxl__xs_transaction_commit(gc, &t);\n        if (!rc) break;\n        if (rc < 0) goto out;\n    }\n\n    aodev->dev = device;\n    aodev->action = LIBXL__DEVICE_ACTION_ADD;\n    libxl__wait_device_connection(egc, aodev);\n\n    rc = 0;\n\nout:\n    libxl__xs_transaction_abort(gc, &t);\n    if (lock) libxl__unlock_file(lock);\n    libxl_device_disk_dispose(&disk_saved);\n    libxl_domain_config_dispose(&d_config);\n    aodev->rc = rc;\n    if (rc) aodev->callback(egc, aodev);\n    return;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -165,6 +165,8 @@\n         flexarray_append(front, GCSPRINTF(\"%d\", device->devid));\n         flexarray_append(front, \"device-type\");\n         flexarray_append(front, disk->is_cdrom ? \"cdrom\" : \"disk\");\n+        flexarray_append(front, \"trusted\");\n+        flexarray_append(front, libxl_defbool_val(disk->trusted) ? \"1\" : \"0\");\n \n         /*\n          * Old PV kernel disk frontends before 2.6.26 rely on tool stack to",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "        flexarray_append(front, \"trusted\");",
                "        flexarray_append(front, libxl_defbool_val(disk->trusted) ? \"1\" : \"0\");"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26365",
        "func_name": "xen-project/xen/libxl__set_xenstore_nic",
        "description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "git_url": "https://github.com/xen-project/xen/commit/54d8f27d0477937e1f99a414fc1ffd93d184b38a",
        "commit_title": "tools/libxl: report trusted backend status to frontends",
        "commit_text": " Allow administrators to notify a frontend driver that it's backend counterpart is not to be trusted, so the frontend can deploy whatever mitigations required in order to secure itself.  Allow such option for disk and network frontends only, as those are the only hardened ones currently supported.  This is part of XSA-403 ",
        "func_before": "static int libxl__set_xenstore_nic(libxl__gc *gc, uint32_t domid,\n                                   libxl_device_nic *nic,\n                                   flexarray_t *back, flexarray_t *front,\n                                   flexarray_t *ro_front)\n{\n    flexarray_grow(back, 2);\n\n    if (nic->script)\n        flexarray_append_pair(back, \"script\",\n                              libxl__abs_path(gc, nic->script,\n                                              libxl__xen_script_dir_path()));\n\n    if (nic->ifname) {\n        flexarray_append(back, \"vifname\");\n        flexarray_append(back, nic->ifname);\n    }\n\n    if (nic->coloft_forwarddev) {\n        flexarray_append(back, \"forwarddev\");\n        flexarray_append(back, nic->coloft_forwarddev);\n    }\n\n#define MAYBE_ADD_COLO_ARGS(arg) ({                                       \\\n    if (nic->colo_##arg) {                                                \\\n        flexarray_append(back, \"colo_\"#arg);                              \\\n        flexarray_append(back, nic->colo_##arg);                          \\\n    }                                                                     \\\n})\n\n    MAYBE_ADD_COLO_ARGS(sock_mirror_id);\n    MAYBE_ADD_COLO_ARGS(sock_mirror_ip);\n    MAYBE_ADD_COLO_ARGS(sock_mirror_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_port);\n    MAYBE_ADD_COLO_ARGS(filter_mirror_queue);\n    MAYBE_ADD_COLO_ARGS(filter_mirror_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_queue);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_indev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_queue);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_indev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_outdev);\n    MAYBE_ADD_COLO_ARGS(compare_pri_in);\n    MAYBE_ADD_COLO_ARGS(compare_sec_in);\n    MAYBE_ADD_COLO_ARGS(compare_out);\n    MAYBE_ADD_COLO_ARGS(compare_notify_dev);\n\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_id);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_ip);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_port);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_id);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_ip);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_port);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_queue);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_indev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_queue);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_indev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_rewriter0_queue);\n    MAYBE_ADD_COLO_ARGS(checkpoint_host);\n    MAYBE_ADD_COLO_ARGS(checkpoint_port);\n\n#undef MAYBE_ADD_COLO_ARGS\n\n    flexarray_append(back, \"mac\");\n    flexarray_append(back,GCSPRINTF(LIBXL_MAC_FMT, LIBXL_MAC_BYTES(nic->mac)));\n    if (nic->ip) {\n        flexarray_append(back, \"ip\");\n        flexarray_append(back, libxl__strdup(gc, nic->ip));\n    }\n    if (nic->gatewaydev) {\n        flexarray_append(back, \"gatewaydev\");\n        flexarray_append(back, libxl__strdup(gc, nic->gatewaydev));\n    }\n\n    if (nic->rate_interval_usecs > 0) {\n        flexarray_append(back, \"rate\");\n        flexarray_append(back, GCSPRINTF(\"%\"PRIu64\",%\"PRIu32\"\",\n                            nic->rate_bytes_per_interval,\n                            nic->rate_interval_usecs));\n    }\n\n    if (nic->mtu != LIBXL_DEVICE_NIC_MTU_DEFAULT) {\n        flexarray_append(back, \"mtu\");\n        flexarray_append(back, GCSPRINTF(\"%u\", nic->mtu));\n    }\n    \n    flexarray_append(back, \"bridge\");\n    flexarray_append(back, libxl__strdup(gc, nic->bridge));\n    flexarray_append(back, \"handle\");\n    flexarray_append(back, GCSPRINTF(\"%d\", nic->devid));\n    flexarray_append(back, \"type\");\n    flexarray_append(back, libxl__strdup(gc,\n                                     libxl_nic_type_to_string(nic->nictype)));\n\n    flexarray_append(front, \"handle\");\n    flexarray_append(front, GCSPRINTF(\"%d\", nic->devid));\n    flexarray_append(front, \"mac\");\n    flexarray_append(front, GCSPRINTF(\n                                    LIBXL_MAC_FMT, LIBXL_MAC_BYTES(nic->mac)));\n\n    flexarray_append(ro_front, \"mtu\");\n    flexarray_append(ro_front, GCSPRINTF(\"%u\", nic->mtu));\n\n    /*\n     * Force backend to wait for hotplug script execution before switching to\n     * connected state.\n     */\n    flexarray_append(back, \"hotplug-status\");\n    flexarray_append(back, \"\");\n\n    return 0;\n}",
        "func": "static int libxl__set_xenstore_nic(libxl__gc *gc, uint32_t domid,\n                                   libxl_device_nic *nic,\n                                   flexarray_t *back, flexarray_t *front,\n                                   flexarray_t *ro_front)\n{\n    flexarray_grow(back, 2);\n\n    if (nic->script)\n        flexarray_append_pair(back, \"script\",\n                              libxl__abs_path(gc, nic->script,\n                                              libxl__xen_script_dir_path()));\n\n    if (nic->ifname) {\n        flexarray_append(back, \"vifname\");\n        flexarray_append(back, nic->ifname);\n    }\n\n    if (nic->coloft_forwarddev) {\n        flexarray_append(back, \"forwarddev\");\n        flexarray_append(back, nic->coloft_forwarddev);\n    }\n\n#define MAYBE_ADD_COLO_ARGS(arg) ({                                       \\\n    if (nic->colo_##arg) {                                                \\\n        flexarray_append(back, \"colo_\"#arg);                              \\\n        flexarray_append(back, nic->colo_##arg);                          \\\n    }                                                                     \\\n})\n\n    MAYBE_ADD_COLO_ARGS(sock_mirror_id);\n    MAYBE_ADD_COLO_ARGS(sock_mirror_ip);\n    MAYBE_ADD_COLO_ARGS(sock_mirror_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_pri_in_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_sec_in_port);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_id);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_ip);\n    MAYBE_ADD_COLO_ARGS(sock_compare_notify_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector0_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector1_port);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_id);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_ip);\n    MAYBE_ADD_COLO_ARGS(sock_redirector2_port);\n    MAYBE_ADD_COLO_ARGS(filter_mirror_queue);\n    MAYBE_ADD_COLO_ARGS(filter_mirror_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_queue);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_indev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector0_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_queue);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_indev);\n    MAYBE_ADD_COLO_ARGS(filter_redirector1_outdev);\n    MAYBE_ADD_COLO_ARGS(compare_pri_in);\n    MAYBE_ADD_COLO_ARGS(compare_sec_in);\n    MAYBE_ADD_COLO_ARGS(compare_out);\n    MAYBE_ADD_COLO_ARGS(compare_notify_dev);\n\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_id);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_ip);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector0_port);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_id);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_ip);\n    MAYBE_ADD_COLO_ARGS(sock_sec_redirector1_port);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_queue);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_indev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector0_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_queue);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_indev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_redirector1_outdev);\n    MAYBE_ADD_COLO_ARGS(filter_sec_rewriter0_queue);\n    MAYBE_ADD_COLO_ARGS(checkpoint_host);\n    MAYBE_ADD_COLO_ARGS(checkpoint_port);\n\n#undef MAYBE_ADD_COLO_ARGS\n\n    flexarray_append(back, \"mac\");\n    flexarray_append(back,GCSPRINTF(LIBXL_MAC_FMT, LIBXL_MAC_BYTES(nic->mac)));\n    if (nic->ip) {\n        flexarray_append(back, \"ip\");\n        flexarray_append(back, libxl__strdup(gc, nic->ip));\n    }\n    if (nic->gatewaydev) {\n        flexarray_append(back, \"gatewaydev\");\n        flexarray_append(back, libxl__strdup(gc, nic->gatewaydev));\n    }\n\n    if (nic->rate_interval_usecs > 0) {\n        flexarray_append(back, \"rate\");\n        flexarray_append(back, GCSPRINTF(\"%\"PRIu64\",%\"PRIu32\"\",\n                            nic->rate_bytes_per_interval,\n                            nic->rate_interval_usecs));\n    }\n\n    if (nic->mtu != LIBXL_DEVICE_NIC_MTU_DEFAULT) {\n        flexarray_append(back, \"mtu\");\n        flexarray_append(back, GCSPRINTF(\"%u\", nic->mtu));\n    }\n    \n    flexarray_append(back, \"bridge\");\n    flexarray_append(back, libxl__strdup(gc, nic->bridge));\n    flexarray_append(back, \"handle\");\n    flexarray_append(back, GCSPRINTF(\"%d\", nic->devid));\n    flexarray_append(back, \"type\");\n    flexarray_append(back, libxl__strdup(gc,\n                                     libxl_nic_type_to_string(nic->nictype)));\n\n    flexarray_append(front, \"handle\");\n    flexarray_append(front, GCSPRINTF(\"%d\", nic->devid));\n    flexarray_append(front, \"mac\");\n    flexarray_append(front, GCSPRINTF(\n                                    LIBXL_MAC_FMT, LIBXL_MAC_BYTES(nic->mac)));\n\n    flexarray_append(ro_front, \"mtu\");\n    flexarray_append(ro_front, GCSPRINTF(\"%u\", nic->mtu));\n\n    /*\n     * Force backend to wait for hotplug script execution before switching to\n     * connected state.\n     */\n    flexarray_append(back, \"hotplug-status\");\n    flexarray_append(back, \"\");\n\n    flexarray_append(front, \"trusted\");\n    flexarray_append(front, libxl_defbool_val(nic->trusted) ? \"1\" : \"0\");\n\n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -126,5 +126,8 @@\n     flexarray_append(back, \"hotplug-status\");\n     flexarray_append(back, \"\");\n \n+    flexarray_append(front, \"trusted\");\n+    flexarray_append(front, libxl_defbool_val(nic->trusted) ? \"1\" : \"0\");\n+\n     return 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    flexarray_append(front, \"trusted\");",
                "    flexarray_append(front, libxl_defbool_val(nic->trusted) ? \"1\" : \"0\");",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-26365",
        "func_name": "xen-project/xen/libxl__device_nic_setdefault",
        "description": "Linux disk/nic frontends data leaks T[his CNA information record relates to multiple CVEs; the text explains which aspects/vulnerabilities correspond to which CVE.] Linux Block and Network PV device frontends don't zero memory regions before sharing them with the backend (CVE-2022-26365, CVE-2022-33740). Additionally the granularity of the grant table doesn't allow sharing less than a 4K page, leading to unrelated data residing in the same 4K page as data shared with a backend being accessible by such backend (CVE-2022-33741, CVE-2022-33742).",
        "git_url": "https://github.com/xen-project/xen/commit/54d8f27d0477937e1f99a414fc1ffd93d184b38a",
        "commit_title": "tools/libxl: report trusted backend status to frontends",
        "commit_text": " Allow administrators to notify a frontend driver that it's backend counterpart is not to be trusted, so the frontend can deploy whatever mitigations required in order to secure itself.  Allow such option for disk and network frontends only, as those are the only hardened ones currently supported.  This is part of XSA-403 ",
        "func_before": "static int libxl__device_nic_setdefault(libxl__gc *gc, uint32_t domid,\n                                        libxl_device_nic *nic, bool hotplug)\n{\n    int rc;\n\n    if (!nic->mtu)\n        nic->mtu = LIBXL_DEVICE_NIC_MTU_DEFAULT;\n    if (!nic->model) {\n        nic->model = strdup(\"rtl8139\");\n        if (!nic->model) return ERROR_NOMEM;\n    }\n    if (libxl__mac_is_default(&nic->mac)) {\n        const uint8_t *r;\n        libxl_uuid uuid;\n\n        libxl_uuid_generate(&uuid);\n        r = libxl_uuid_bytearray(&uuid);\n\n        /* Generate a random MAC address, with Xen's OUI (00:16:3e) */\n        nic->mac[0] = 0x00;\n        nic->mac[1] = 0x16;\n        nic->mac[2] = 0x3e;\n        nic->mac[3] = r[0] & 0x7f;\n        nic->mac[4] = r[1];\n        nic->mac[5] = r[2];\n    }\n    if (!nic->bridge) {\n        nic->bridge = strdup(\"xenbr0\");\n        if (!nic->bridge) return ERROR_NOMEM;\n    }\n    if ( !nic->script && asprintf(&nic->script, \"%s/vif-bridge\",\n                                  libxl__xen_script_dir_path()) < 0 )\n        return ERROR_FAIL;\n\n    rc = libxl__resolve_domid(gc, nic->backend_domname, &nic->backend_domid);\n    if (rc < 0) return rc;\n\n    switch (libxl__domain_type(gc, domid)) {\n    case LIBXL_DOMAIN_TYPE_HVM:\n        if (!nic->nictype) {\n            if (hotplug)\n                nic->nictype = LIBXL_NIC_TYPE_VIF;\n            else\n                nic->nictype = LIBXL_NIC_TYPE_VIF_IOEMU;\n        }\n        break;\n    case LIBXL_DOMAIN_TYPE_PVH:\n    case LIBXL_DOMAIN_TYPE_PV:\n        if (nic->nictype == LIBXL_NIC_TYPE_VIF_IOEMU) {\n            LOGD(ERROR, domid,\n            \"trying to create PV or PVH guest with an emulated interface\");\n            return ERROR_INVAL;\n        }\n        nic->nictype = LIBXL_NIC_TYPE_VIF;\n        break;\n    case LIBXL_DOMAIN_TYPE_INVALID:\n        return ERROR_FAIL;\n    default:\n        abort();\n    }\n\n    return rc;\n}",
        "func": "static int libxl__device_nic_setdefault(libxl__gc *gc, uint32_t domid,\n                                        libxl_device_nic *nic, bool hotplug)\n{\n    int rc;\n\n    if (!nic->mtu)\n        nic->mtu = LIBXL_DEVICE_NIC_MTU_DEFAULT;\n    if (!nic->model) {\n        nic->model = strdup(\"rtl8139\");\n        if (!nic->model) return ERROR_NOMEM;\n    }\n    if (libxl__mac_is_default(&nic->mac)) {\n        const uint8_t *r;\n        libxl_uuid uuid;\n\n        libxl_uuid_generate(&uuid);\n        r = libxl_uuid_bytearray(&uuid);\n\n        /* Generate a random MAC address, with Xen's OUI (00:16:3e) */\n        nic->mac[0] = 0x00;\n        nic->mac[1] = 0x16;\n        nic->mac[2] = 0x3e;\n        nic->mac[3] = r[0] & 0x7f;\n        nic->mac[4] = r[1];\n        nic->mac[5] = r[2];\n    }\n    if (!nic->bridge) {\n        nic->bridge = strdup(\"xenbr0\");\n        if (!nic->bridge) return ERROR_NOMEM;\n    }\n    if ( !nic->script && asprintf(&nic->script, \"%s/vif-bridge\",\n                                  libxl__xen_script_dir_path()) < 0 )\n        return ERROR_FAIL;\n\n    rc = libxl__resolve_domid(gc, nic->backend_domname, &nic->backend_domid);\n    if (rc < 0) return rc;\n\n    switch (libxl__domain_type(gc, domid)) {\n    case LIBXL_DOMAIN_TYPE_HVM:\n        if (!nic->nictype) {\n            if (hotplug)\n                nic->nictype = LIBXL_NIC_TYPE_VIF;\n            else\n                nic->nictype = LIBXL_NIC_TYPE_VIF_IOEMU;\n        }\n        break;\n    case LIBXL_DOMAIN_TYPE_PVH:\n    case LIBXL_DOMAIN_TYPE_PV:\n        if (nic->nictype == LIBXL_NIC_TYPE_VIF_IOEMU) {\n            LOGD(ERROR, domid,\n            \"trying to create PV or PVH guest with an emulated interface\");\n            return ERROR_INVAL;\n        }\n        nic->nictype = LIBXL_NIC_TYPE_VIF;\n        break;\n    case LIBXL_DOMAIN_TYPE_INVALID:\n        return ERROR_FAIL;\n    default:\n        abort();\n    }\n\n    libxl_defbool_setdefault(&nic->trusted, true);\n\n    return rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -59,5 +59,7 @@\n         abort();\n     }\n \n+    libxl_defbool_setdefault(&nic->trusted, true);\n+\n     return rc;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    libxl_defbool_setdefault(&nic->trusted, true);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_invalidate_memslot",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static void kvm_invalidate_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *old,\n\t\t\t\t   struct kvm_memory_slot *invalid_slot)\n{\n\t/*\n\t * Mark the current slot INVALID.  As with all memslot modifications,\n\t * this must be done on an unreachable slot to avoid modifying the\n\t * current slot in the active tree.\n\t */\n\tkvm_copy_memslot(invalid_slot, old);\n\tinvalid_slot->flags |= KVM_MEMSLOT_INVALID;\n\tkvm_replace_memslot(kvm, old, invalid_slot);\n\n\t/*\n\t * Activate the slot that is now marked INVALID, but don't propagate\n\t * the slot to the now inactive slots. The slot is either going to be\n\t * deleted or recreated as a new slot.\n\t */\n\tkvm_swap_active_memslots(kvm, old->as_id);\n\n\t/*\n\t * From this point no new shadow pages pointing to a deleted, or moved,\n\t * memslot will be created.  Validation of sp->gfn happens in:\n\t *\t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t *\t- kvm_is_visible_gfn (mmu_check_root)\n\t */\n\tkvm_arch_flush_shadow_memslot(kvm, old);\n\n\t/* Was released by kvm_swap_active_memslots, reacquire. */\n\tmutex_lock(&kvm->slots_arch_lock);\n\n\t/*\n\t * Copy the arch-specific field of the newly-installed slot back to the\n\t * old slot as the arch data could have changed between releasing\n\t * slots_arch_lock in install_new_memslots() and re-acquiring the lock\n\t * above.  Writers are required to retrieve memslots *after* acquiring\n\t * slots_arch_lock, thus the active slot's data is guaranteed to be fresh.\n\t */\n\told->arch = invalid_slot->arch;\n}",
        "func": "static void kvm_invalidate_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *old,\n\t\t\t\t   struct kvm_memory_slot *invalid_slot)\n{\n\t/*\n\t * Mark the current slot INVALID.  As with all memslot modifications,\n\t * this must be done on an unreachable slot to avoid modifying the\n\t * current slot in the active tree.\n\t */\n\tkvm_copy_memslot(invalid_slot, old);\n\tinvalid_slot->flags |= KVM_MEMSLOT_INVALID;\n\tkvm_replace_memslot(kvm, old, invalid_slot);\n\n\t/*\n\t * Activate the slot that is now marked INVALID, but don't propagate\n\t * the slot to the now inactive slots. The slot is either going to be\n\t * deleted or recreated as a new slot.\n\t */\n\tkvm_swap_active_memslots(kvm, old->as_id);\n\n\t/*\n\t * From this point no new shadow pages pointing to a deleted, or moved,\n\t * memslot will be created.  Validation of sp->gfn happens in:\n\t *\t- gfn_to_hva (kvm_read_guest, gfn_to_pfn)\n\t *\t- kvm_is_visible_gfn (mmu_check_root)\n\t */\n\tkvm_arch_flush_shadow_memslot(kvm, old);\n\tkvm_arch_guest_memory_reclaimed(kvm);\n\n\t/* Was released by kvm_swap_active_memslots, reacquire. */\n\tmutex_lock(&kvm->slots_arch_lock);\n\n\t/*\n\t * Copy the arch-specific field of the newly-installed slot back to the\n\t * old slot as the arch data could have changed between releasing\n\t * slots_arch_lock in install_new_memslots() and re-acquiring the lock\n\t * above.  Writers are required to retrieve memslots *after* acquiring\n\t * slots_arch_lock, thus the active slot's data is guaranteed to be fresh.\n\t */\n\told->arch = invalid_slot->arch;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -25,6 +25,7 @@\n \t *\t- kvm_is_visible_gfn (mmu_check_root)\n \t */\n \tkvm_arch_flush_shadow_memslot(kvm, old);\n+\tkvm_arch_guest_memory_reclaimed(kvm);\n \n \t/* Was released by kvm_swap_active_memslots, reacquire. */\n \tmutex_lock(&kvm->slots_arch_lock);",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tkvm_arch_guest_memory_reclaimed(kvm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_mmu_notifier_invalidate_range_end",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range hva_range = {\n\t\t.start\t\t= range->start,\n\t\t.end\t\t= range->end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= (void *)kvm_null_fn,\n\t\t.on_lock\t= kvm_dec_notifier_count,\n\t\t.flush_on_ret\t= false,\n\t\t.may_block\t= mmu_notifier_range_blockable(range),\n\t};\n\tbool wake;\n\n\t__kvm_handle_hva_range(kvm, &hva_range);\n\n\t/* Pairs with the increment in range_start(). */\n\tspin_lock(&kvm->mn_invalidate_lock);\n\twake = (--kvm->mn_active_invalidate_count == 0);\n\tspin_unlock(&kvm->mn_invalidate_lock);\n\n\t/*\n\t * There can only be one waiter, since the wait happens under\n\t * slots_lock.\n\t */\n\tif (wake)\n\t\trcuwait_wake_up(&kvm->mn_memslots_update_rcuwait);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}",
        "func": "static void kvm_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range hva_range = {\n\t\t.start\t\t= range->start,\n\t\t.end\t\t= range->end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= (void *)kvm_null_fn,\n\t\t.on_lock\t= kvm_dec_notifier_count,\n\t\t.on_unlock\t= (void *)kvm_null_fn,\n\t\t.flush_on_ret\t= false,\n\t\t.may_block\t= mmu_notifier_range_blockable(range),\n\t};\n\tbool wake;\n\n\t__kvm_handle_hva_range(kvm, &hva_range);\n\n\t/* Pairs with the increment in range_start(). */\n\tspin_lock(&kvm->mn_invalidate_lock);\n\twake = (--kvm->mn_active_invalidate_count == 0);\n\tspin_unlock(&kvm->mn_invalidate_lock);\n\n\t/*\n\t * There can only be one waiter, since the wait happens under\n\t * slots_lock.\n\t */\n\tif (wake)\n\t\trcuwait_wake_up(&kvm->mn_memslots_update_rcuwait);\n\n\tBUG_ON(kvm->mmu_notifier_count < 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n \t\t.pte\t\t= __pte(0),\n \t\t.handler\t= (void *)kvm_null_fn,\n \t\t.on_lock\t= kvm_dec_notifier_count,\n+\t\t.on_unlock\t= (void *)kvm_null_fn,\n \t\t.flush_on_ret\t= false,\n \t\t.may_block\t= mmu_notifier_range_blockable(range),\n \t};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t.on_unlock\t= (void *)kvm_null_fn,"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_destroy_vm",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_destroy_pm_notifier(kvm);\n\tkvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);\n\tkvm_destroy_vm_debugfs(kvm);\n\tkvm_arch_sync_events(kvm);\n\tmutex_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\tmutex_unlock(&kvm_lock);\n\tkvm_arch_pre_destroy_vm(kvm);\n\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tstruct kvm_io_bus *bus = kvm_get_bus(kvm, i);\n\n\t\tif (bus)\n\t\t\tkvm_io_bus_destroy(bus);\n\t\tkvm->buses[i] = NULL;\n\t}\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n\t/*\n\t * At this point, pending calls to invalidate_range_start()\n\t * have completed but no more MMU notifiers will run, so\n\t * mn_active_invalidate_count may remain unbalanced.\n\t * No threads can be waiting in install_new_memslots as the\n\t * last reference on KVM has been dropped, but freeing\n\t * memslots would deadlock without this manual intervention.\n\t */\n\tWARN_ON(rcuwait_active(&kvm->mn_memslots_update_rcuwait));\n\tkvm->mn_active_invalidate_count = 0;\n#else\n\tkvm_arch_flush_shadow_all(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_destroy_devices(kvm);\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tkvm_free_memslots(kvm, &kvm->__memslots[i][0]);\n\t\tkvm_free_memslots(kvm, &kvm->__memslots[i][1]);\n\t}\n\tcleanup_srcu_struct(&kvm->irq_srcu);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\tpreempt_notifier_dec();\n\thardware_disable_all();\n\tmmdrop(mm);\n\tmodule_put(kvm_chardev_ops.owner);\n}",
        "func": "static void kvm_destroy_vm(struct kvm *kvm)\n{\n\tint i;\n\tstruct mm_struct *mm = kvm->mm;\n\n\tkvm_destroy_pm_notifier(kvm);\n\tkvm_uevent_notify_change(KVM_EVENT_DESTROY_VM, kvm);\n\tkvm_destroy_vm_debugfs(kvm);\n\tkvm_arch_sync_events(kvm);\n\tmutex_lock(&kvm_lock);\n\tlist_del(&kvm->vm_list);\n\tmutex_unlock(&kvm_lock);\n\tkvm_arch_pre_destroy_vm(kvm);\n\n\tkvm_free_irq_routing(kvm);\n\tfor (i = 0; i < KVM_NR_BUSES; i++) {\n\t\tstruct kvm_io_bus *bus = kvm_get_bus(kvm, i);\n\n\t\tif (bus)\n\t\t\tkvm_io_bus_destroy(bus);\n\t\tkvm->buses[i] = NULL;\n\t}\n\tkvm_coalesced_mmio_free(kvm);\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tmmu_notifier_unregister(&kvm->mmu_notifier, kvm->mm);\n\t/*\n\t * At this point, pending calls to invalidate_range_start()\n\t * have completed but no more MMU notifiers will run, so\n\t * mn_active_invalidate_count may remain unbalanced.\n\t * No threads can be waiting in install_new_memslots as the\n\t * last reference on KVM has been dropped, but freeing\n\t * memslots would deadlock without this manual intervention.\n\t */\n\tWARN_ON(rcuwait_active(&kvm->mn_memslots_update_rcuwait));\n\tkvm->mn_active_invalidate_count = 0;\n#else\n\tkvm_flush_shadow_all(kvm);\n#endif\n\tkvm_arch_destroy_vm(kvm);\n\tkvm_destroy_devices(kvm);\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tkvm_free_memslots(kvm, &kvm->__memslots[i][0]);\n\t\tkvm_free_memslots(kvm, &kvm->__memslots[i][1]);\n\t}\n\tcleanup_srcu_struct(&kvm->irq_srcu);\n\tcleanup_srcu_struct(&kvm->srcu);\n\tkvm_arch_free_vm(kvm);\n\tpreempt_notifier_dec();\n\thardware_disable_all();\n\tmmdrop(mm);\n\tmodule_put(kvm_chardev_ops.owner);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -34,7 +34,7 @@\n \tWARN_ON(rcuwait_active(&kvm->mn_memslots_update_rcuwait));\n \tkvm->mn_active_invalidate_count = 0;\n #else\n-\tkvm_arch_flush_shadow_all(kvm);\n+\tkvm_flush_shadow_all(kvm);\n #endif\n \tkvm_arch_destroy_vm(kvm);\n \tkvm_destroy_devices(kvm);",
        "diff_line_info": {
            "deleted_lines": [
                "\tkvm_arch_flush_shadow_all(kvm);"
            ],
            "added_lines": [
                "\tkvm_flush_shadow_all(kvm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_handle_hva_range",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long end,\n\t\t\t\t\t\tpte_t pte,\n\t\t\t\t\t\thva_handler_t handler)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range range = {\n\t\t.start\t\t= start,\n\t\t.end\t\t= end,\n\t\t.pte\t\t= pte,\n\t\t.handler\t= handler,\n\t\t.on_lock\t= (void *)kvm_null_fn,\n\t\t.flush_on_ret\t= true,\n\t\t.may_block\t= false,\n\t};\n\n\treturn __kvm_handle_hva_range(kvm, &range);\n}",
        "func": "static __always_inline int kvm_handle_hva_range(struct mmu_notifier *mn,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long end,\n\t\t\t\t\t\tpte_t pte,\n\t\t\t\t\t\thva_handler_t handler)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range range = {\n\t\t.start\t\t= start,\n\t\t.end\t\t= end,\n\t\t.pte\t\t= pte,\n\t\t.handler\t= handler,\n\t\t.on_lock\t= (void *)kvm_null_fn,\n\t\t.on_unlock\t= (void *)kvm_null_fn,\n\t\t.flush_on_ret\t= true,\n\t\t.may_block\t= false,\n\t};\n\n\treturn __kvm_handle_hva_range(kvm, &range);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -11,6 +11,7 @@\n \t\t.pte\t\t= pte,\n \t\t.handler\t= handler,\n \t\t.on_lock\t= (void *)kvm_null_fn,\n+\t\t.on_unlock\t= (void *)kvm_null_fn,\n \t\t.flush_on_ret\t= true,\n \t\t.may_block\t= false,\n \t};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t.on_unlock\t= (void *)kvm_null_fn,"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_mmu_notifier_release",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_arch_flush_shadow_all(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}",
        "func": "static void kvm_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tint idx;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tkvm_flush_shadow_all(kvm);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -5,6 +5,6 @@\n \tint idx;\n \n \tidx = srcu_read_lock(&kvm->srcu);\n-\tkvm_arch_flush_shadow_all(kvm);\n+\tkvm_flush_shadow_all(kvm);\n \tsrcu_read_unlock(&kvm->srcu, idx);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\tkvm_arch_flush_shadow_all(kvm);"
            ],
            "added_lines": [
                "\tkvm_flush_shadow_all(kvm);"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_handle_hva_range_no_flush",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,\n\t\t\t\t\t\t\t unsigned long start,\n\t\t\t\t\t\t\t unsigned long end,\n\t\t\t\t\t\t\t hva_handler_t handler)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range range = {\n\t\t.start\t\t= start,\n\t\t.end\t\t= end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= handler,\n\t\t.on_lock\t= (void *)kvm_null_fn,\n\t\t.flush_on_ret\t= false,\n\t\t.may_block\t= false,\n\t};\n\n\treturn __kvm_handle_hva_range(kvm, &range);\n}",
        "func": "static __always_inline int kvm_handle_hva_range_no_flush(struct mmu_notifier *mn,\n\t\t\t\t\t\t\t unsigned long start,\n\t\t\t\t\t\t\t unsigned long end,\n\t\t\t\t\t\t\t hva_handler_t handler)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range range = {\n\t\t.start\t\t= start,\n\t\t.end\t\t= end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= handler,\n\t\t.on_lock\t= (void *)kvm_null_fn,\n\t\t.on_unlock\t= (void *)kvm_null_fn,\n\t\t.flush_on_ret\t= false,\n\t\t.may_block\t= false,\n\t};\n\n\treturn __kvm_handle_hva_range(kvm, &range);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -10,6 +10,7 @@\n \t\t.pte\t\t= __pte(0),\n \t\t.handler\t= handler,\n \t\t.on_lock\t= (void *)kvm_null_fn,\n+\t\t.on_unlock\t= (void *)kvm_null_fn,\n \t\t.flush_on_ret\t= false,\n \t\t.may_block\t= false,\n \t};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t.on_unlock\t= (void *)kvm_null_fn,"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/__kvm_handle_hva_range",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,\n\t\t\t\t\t\t  const struct kvm_hva_range *range)\n{\n\tbool ret = false, locked = false;\n\tstruct kvm_gfn_range gfn_range;\n\tstruct kvm_memory_slot *slot;\n\tstruct kvm_memslots *slots;\n\tint i, idx;\n\n\tif (WARN_ON_ONCE(range->end <= range->start))\n\t\treturn 0;\n\n\t/* A null handler is allowed if and only if on_lock() is provided. */\n\tif (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&\n\t\t\t IS_KVM_NULL_FN(range->handler)))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tstruct interval_tree_node *node;\n\n\t\tslots = __kvm_memslots(kvm, i);\n\t\tkvm_for_each_memslot_in_hva_range(node, slots,\n\t\t\t\t\t\t  range->start, range->end - 1) {\n\t\t\tunsigned long hva_start, hva_end;\n\n\t\t\tslot = container_of(node, struct kvm_memory_slot, hva_node[slots->node_idx]);\n\t\t\thva_start = max(range->start, slot->userspace_addr);\n\t\t\thva_end = min(range->end, slot->userspace_addr +\n\t\t\t\t\t\t  (slot->npages << PAGE_SHIFT));\n\n\t\t\t/*\n\t\t\t * To optimize for the likely case where the address\n\t\t\t * range is covered by zero or one memslots, don't\n\t\t\t * bother making these conditional (to avoid writes on\n\t\t\t * the second or later invocation of the handler).\n\t\t\t */\n\t\t\tgfn_range.pte = range->pte;\n\t\t\tgfn_range.may_block = range->may_block;\n\n\t\t\t/*\n\t\t\t * {gfn(page) | page intersects with [hva_start, hva_end)} =\n\t\t\t * {gfn_start, gfn_start+1, ..., gfn_end-1}.\n\t\t\t */\n\t\t\tgfn_range.start = hva_to_gfn_memslot(hva_start, slot);\n\t\t\tgfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);\n\t\t\tgfn_range.slot = slot;\n\n\t\t\tif (!locked) {\n\t\t\t\tlocked = true;\n\t\t\t\tKVM_MMU_LOCK(kvm);\n\t\t\t\tif (!IS_KVM_NULL_FN(range->on_lock))\n\t\t\t\t\trange->on_lock(kvm, range->start, range->end);\n\t\t\t\tif (IS_KVM_NULL_FN(range->handler))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret |= range->handler(kvm, &gfn_range);\n\t\t}\n\t}\n\n\tif (range->flush_on_ret && ret)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tif (locked)\n\t\tKVM_MMU_UNLOCK(kvm);\n\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* The notifiers are averse to booleans. :-( */\n\treturn (int)ret;\n}",
        "func": "static __always_inline int __kvm_handle_hva_range(struct kvm *kvm,\n\t\t\t\t\t\t  const struct kvm_hva_range *range)\n{\n\tbool ret = false, locked = false;\n\tstruct kvm_gfn_range gfn_range;\n\tstruct kvm_memory_slot *slot;\n\tstruct kvm_memslots *slots;\n\tint i, idx;\n\n\tif (WARN_ON_ONCE(range->end <= range->start))\n\t\treturn 0;\n\n\t/* A null handler is allowed if and only if on_lock() is provided. */\n\tif (WARN_ON_ONCE(IS_KVM_NULL_FN(range->on_lock) &&\n\t\t\t IS_KVM_NULL_FN(range->handler)))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tstruct interval_tree_node *node;\n\n\t\tslots = __kvm_memslots(kvm, i);\n\t\tkvm_for_each_memslot_in_hva_range(node, slots,\n\t\t\t\t\t\t  range->start, range->end - 1) {\n\t\t\tunsigned long hva_start, hva_end;\n\n\t\t\tslot = container_of(node, struct kvm_memory_slot, hva_node[slots->node_idx]);\n\t\t\thva_start = max(range->start, slot->userspace_addr);\n\t\t\thva_end = min(range->end, slot->userspace_addr +\n\t\t\t\t\t\t  (slot->npages << PAGE_SHIFT));\n\n\t\t\t/*\n\t\t\t * To optimize for the likely case where the address\n\t\t\t * range is covered by zero or one memslots, don't\n\t\t\t * bother making these conditional (to avoid writes on\n\t\t\t * the second or later invocation of the handler).\n\t\t\t */\n\t\t\tgfn_range.pte = range->pte;\n\t\t\tgfn_range.may_block = range->may_block;\n\n\t\t\t/*\n\t\t\t * {gfn(page) | page intersects with [hva_start, hva_end)} =\n\t\t\t * {gfn_start, gfn_start+1, ..., gfn_end-1}.\n\t\t\t */\n\t\t\tgfn_range.start = hva_to_gfn_memslot(hva_start, slot);\n\t\t\tgfn_range.end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, slot);\n\t\t\tgfn_range.slot = slot;\n\n\t\t\tif (!locked) {\n\t\t\t\tlocked = true;\n\t\t\t\tKVM_MMU_LOCK(kvm);\n\t\t\t\tif (!IS_KVM_NULL_FN(range->on_lock))\n\t\t\t\t\trange->on_lock(kvm, range->start, range->end);\n\t\t\t\tif (IS_KVM_NULL_FN(range->handler))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret |= range->handler(kvm, &gfn_range);\n\t\t}\n\t}\n\n\tif (range->flush_on_ret && ret)\n\t\tkvm_flush_remote_tlbs(kvm);\n\n\tif (locked) {\n\t\tKVM_MMU_UNLOCK(kvm);\n\t\tif (!IS_KVM_NULL_FN(range->on_unlock))\n\t\t\trange->on_unlock(kvm);\n\t}\n\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t/* The notifiers are averse to booleans. :-( */\n\treturn (int)ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -62,8 +62,11 @@\n \tif (range->flush_on_ret && ret)\n \t\tkvm_flush_remote_tlbs(kvm);\n \n-\tif (locked)\n+\tif (locked) {\n \t\tKVM_MMU_UNLOCK(kvm);\n+\t\tif (!IS_KVM_NULL_FN(range->on_unlock))\n+\t\t\trange->on_unlock(kvm);\n+\t}\n \n \tsrcu_read_unlock(&kvm->srcu, idx);\n ",
        "diff_line_info": {
            "deleted_lines": [
                "\tif (locked)"
            ],
            "added_lines": [
                "\tif (locked) {",
                "\t\tif (!IS_KVM_NULL_FN(range->on_unlock))",
                "\t\t\trange->on_unlock(kvm);",
                "\t}"
            ]
        }
    },
    {
        "cve_id": "CVE-2022-0171",
        "func_name": "torvalds/linux/kvm_mmu_notifier_invalidate_range_start",
        "description": "A flaw was found in the Linux kernel. The existing KVM SEV API has a vulnerability that allows a non-root (host) user-level application to crash the host kernel by creating a confidential guest VM instance in AMD CPU that supports Secure Encrypted Virtualization (SEV).",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=683412ccf61294d727ead4a73d97397396e69a6b",
        "commit_title": "Flush the CPU caches when memory is reclaimed from an SEV guest (where",
        "commit_text": "reclaim also includes it being unmapped from KVM's memslots).  Due to lack of coherency for SEV encrypted memory, failure to flush results in silent data corruption if userspace is malicious/broken and doesn't ensure SEV guest memory is properly pinned and unpinned.  Cache coherency is not enforced across the VM boundary in SEV (AMD APM vol.2 Section 15.34.7). Confidential cachelines, generated by confidential VM guests have to be explicitly flushed on the host side. If a memory page containing dirty confidential cachelines was released by VM and reallocated to another user, the cachelines may corrupt the new user at a later time.  KVM takes a shortcut by assuming all confidential memory remain pinned until the end of VM lifetime. Therefore, KVM does not flush cache at mmu_notifier invalidation events. Because of this incorrect assumption and the lack of cache flushing, malicous userspace can crash the host kernel: creating a malicious VM and continuously allocates/releases unpinned confidential memory pages when the VM is running.  Add cache flush operations to mmu_notifier operations to ensure that any physical memory leaving the guest VM get flushed. In particular, hook mmu_notifier_invalidate_range_start and mmu_notifier_release events and flush cache accordingly. The hook after releasing the mmu lock to avoid contention with other vCPUs.  Cc: stable@vger.kernel.org Suggested-by: Sean Christpherson <seanjc@google.com> Message-Id: <20220421031407.2516575-4-mizhang@google.com> ",
        "func_before": "static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range hva_range = {\n\t\t.start\t\t= range->start,\n\t\t.end\t\t= range->end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= kvm_unmap_gfn_range,\n\t\t.on_lock\t= kvm_inc_notifier_count,\n\t\t.flush_on_ret\t= true,\n\t\t.may_block\t= mmu_notifier_range_blockable(range),\n\t};\n\n\ttrace_kvm_unmap_hva_range(range->start, range->end);\n\n\t/*\n\t * Prevent memslot modification between range_start() and range_end()\n\t * so that conditionally locking provides the same result in both\n\t * functions.  Without that guarantee, the mmu_notifier_count\n\t * adjustments will be imbalanced.\n\t *\n\t * Pairs with the decrement in range_end().\n\t */\n\tspin_lock(&kvm->mn_invalidate_lock);\n\tkvm->mn_active_invalidate_count++;\n\tspin_unlock(&kvm->mn_invalidate_lock);\n\n\tgfn_to_pfn_cache_invalidate_start(kvm, range->start, range->end,\n\t\t\t\t\t  hva_range.may_block);\n\n\t__kvm_handle_hva_range(kvm, &hva_range);\n\n\treturn 0;\n}",
        "func": "static int kvm_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct kvm *kvm = mmu_notifier_to_kvm(mn);\n\tconst struct kvm_hva_range hva_range = {\n\t\t.start\t\t= range->start,\n\t\t.end\t\t= range->end,\n\t\t.pte\t\t= __pte(0),\n\t\t.handler\t= kvm_unmap_gfn_range,\n\t\t.on_lock\t= kvm_inc_notifier_count,\n\t\t.on_unlock\t= kvm_arch_guest_memory_reclaimed,\n\t\t.flush_on_ret\t= true,\n\t\t.may_block\t= mmu_notifier_range_blockable(range),\n\t};\n\n\ttrace_kvm_unmap_hva_range(range->start, range->end);\n\n\t/*\n\t * Prevent memslot modification between range_start() and range_end()\n\t * so that conditionally locking provides the same result in both\n\t * functions.  Without that guarantee, the mmu_notifier_count\n\t * adjustments will be imbalanced.\n\t *\n\t * Pairs with the decrement in range_end().\n\t */\n\tspin_lock(&kvm->mn_invalidate_lock);\n\tkvm->mn_active_invalidate_count++;\n\tspin_unlock(&kvm->mn_invalidate_lock);\n\n\tgfn_to_pfn_cache_invalidate_start(kvm, range->start, range->end,\n\t\t\t\t\t  hva_range.may_block);\n\n\t__kvm_handle_hva_range(kvm, &hva_range);\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -8,6 +8,7 @@\n \t\t.pte\t\t= __pte(0),\n \t\t.handler\t= kvm_unmap_gfn_range,\n \t\t.on_lock\t= kvm_inc_notifier_count,\n+\t\t.on_unlock\t= kvm_arch_guest_memory_reclaimed,\n \t\t.flush_on_ret\t= true,\n \t\t.may_block\t= mmu_notifier_range_blockable(range),\n \t};",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\t.on_unlock\t= kvm_arch_guest_memory_reclaimed,"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11684",
        "func_name": "linux4sam/at91bootstrap/secure_decrypt",
        "description": "AT91bootstrap before 3.9.2 does not properly wipe encryption and authentication keys from memory before passing control to a less privileged software component. This can be exploited to disclose these keys and subsequently encrypt and sign the next boot stage (such as the bootloader).",
        "git_url": "https://github.com/linux4sam/at91bootstrap/commit/45419497309ffbf27c17ea7938499aca99168927",
        "commit_title": "driver: secure: move keys into static arrays",
        "commit_text": " Move the keys into static arrays. This will avoid copying the keys from the code section to the stack on the previous init_keys implementation. Like this, the keys are hardcoded into data section at compile time, and can be completely wiped after use. ",
        "func_before": "int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n{\n\tat91_aes_key_size_t key_size;\n\tunsigned int cmac_key[8], cipher_key[8];\n\tunsigned int iv[AT91_AES_IV_SIZE_WORD];\n\tunsigned int computed_cmac[AT91_AES_BLOCK_SIZE_WORD];\n\tunsigned int fixed_length;\n\tconst unsigned int *cmac;\n\tint rc = -1;\n\n\t/* Init keys */\n\tinit_keys(&key_size, cipher_key, cmac_key, iv);\n\n\t/* Init periph */\n\tat91_aes_init();\n\n\t/* Check signature if required */\n\tif (is_signed) {\n\t\t/* Compute the CMAC */\n\t\tif (at91_aes_cmac(data_length, data, computed_cmac,\n\t\t\t\t  key_size, cmac_key))\n\t\t\tgoto exit;\n\n\t\t/* Check the CMAC */\n\t\tfixed_length = at91_aes_roundup(data_length);\n\t\tcmac = (const unsigned int *)((char *)data + fixed_length);\n\t\tif (!consttime_memequal(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n\t\t\tgoto exit;\n\t}\n\n\t/* Decrypt the whole file */\n\tif (at91_aes_cbc(data_length, data, data, 0,\n\t\t\t key_size, cipher_key, iv))\n\t\tgoto exit;\n\n\trc = 0;\nexit:\n\t/* Reset periph */\n\tat91_aes_cleanup();\n\n\t/* Reset keys */\n\tmemset(cmac_key, 0, sizeof(cmac_key));\n\tmemset(cipher_key, 0, sizeof(cipher_key));\n\tmemset(iv, 0, sizeof(iv));\n\n\treturn rc;\n}",
        "func": "static int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n{\n\tat91_aes_key_size_t key_size;\n\tunsigned int computed_cmac[AT91_AES_BLOCK_SIZE_WORD];\n\tunsigned int fixed_length;\n\tconst unsigned int *cmac;\n\tint rc = -1;\n\n#if defined(CONFIG_AES_KEY_SIZE_128)\n\tkey_size = AT91_AES_KEY_SIZE_128;\n#elif defined(CONFIG_AES_KEY_SIZE_192)\n\tkey_size = AT91_AES_KEY_SIZE_192;\n#elif defined(CONFIG_AES_KEY_SIZE_256)\n\tkey_size = AT91_AES_KEY_SIZE_256;\n#else\n#error \"bad AES key size\"\n#endif\n\n\t/* Init periph */\n\tat91_aes_init();\n\n\t/* Check signature if required */\n\tif (is_signed) {\n\t\t/* Compute the CMAC */\n\t\tif (at91_aes_cmac(data_length, data, computed_cmac,\n\t\t\t\t  key_size, cmac_key))\n\t\t\tgoto exit;\n\n\t\t/* Check the CMAC */\n\t\tfixed_length = at91_aes_roundup(data_length);\n\t\tcmac = (const unsigned int *)((char *)data + fixed_length);\n\t\tif (!consttime_memequal(cmac, computed_cmac, AT91_AES_BLOCK_SIZE_BYTE))\n\t\t\tgoto exit;\n\t}\n\n\t/* Decrypt the whole file */\n\tif (at91_aes_cbc(data_length, data, data, 0,\n\t\t\t key_size, cipher_key, iv))\n\t\tgoto exit;\n\n\trc = 0;\nexit:\n\t/* Reset periph */\n\tat91_aes_cleanup();\n\n\treturn rc;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,15 +1,20 @@\n-int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n+static int secure_decrypt(void *data, unsigned int data_length, int is_signed)\n {\n \tat91_aes_key_size_t key_size;\n-\tunsigned int cmac_key[8], cipher_key[8];\n-\tunsigned int iv[AT91_AES_IV_SIZE_WORD];\n \tunsigned int computed_cmac[AT91_AES_BLOCK_SIZE_WORD];\n \tunsigned int fixed_length;\n \tconst unsigned int *cmac;\n \tint rc = -1;\n \n-\t/* Init keys */\n-\tinit_keys(&key_size, cipher_key, cmac_key, iv);\n+#if defined(CONFIG_AES_KEY_SIZE_128)\n+\tkey_size = AT91_AES_KEY_SIZE_128;\n+#elif defined(CONFIG_AES_KEY_SIZE_192)\n+\tkey_size = AT91_AES_KEY_SIZE_192;\n+#elif defined(CONFIG_AES_KEY_SIZE_256)\n+\tkey_size = AT91_AES_KEY_SIZE_256;\n+#else\n+#error \"bad AES key size\"\n+#endif\n \n \t/* Init periph */\n \tat91_aes_init();\n@@ -38,10 +43,5 @@\n \t/* Reset periph */\n \tat91_aes_cleanup();\n \n-\t/* Reset keys */\n-\tmemset(cmac_key, 0, sizeof(cmac_key));\n-\tmemset(cipher_key, 0, sizeof(cipher_key));\n-\tmemset(iv, 0, sizeof(iv));\n-\n \treturn rc;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "int secure_decrypt(void *data, unsigned int data_length, int is_signed)",
                "\tunsigned int cmac_key[8], cipher_key[8];",
                "\tunsigned int iv[AT91_AES_IV_SIZE_WORD];",
                "\t/* Init keys */",
                "\tinit_keys(&key_size, cipher_key, cmac_key, iv);",
                "\t/* Reset keys */",
                "\tmemset(cmac_key, 0, sizeof(cmac_key));",
                "\tmemset(cipher_key, 0, sizeof(cipher_key));",
                "\tmemset(iv, 0, sizeof(iv));",
                ""
            ],
            "added_lines": [
                "static int secure_decrypt(void *data, unsigned int data_length, int is_signed)",
                "#if defined(CONFIG_AES_KEY_SIZE_128)",
                "\tkey_size = AT91_AES_KEY_SIZE_128;",
                "#elif defined(CONFIG_AES_KEY_SIZE_192)",
                "\tkey_size = AT91_AES_KEY_SIZE_192;",
                "#elif defined(CONFIG_AES_KEY_SIZE_256)",
                "\tkey_size = AT91_AES_KEY_SIZE_256;",
                "#else",
                "#error \"bad AES key size\"",
                "#endif"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11684",
        "func_name": "linux4sam/at91bootstrap/secure_check",
        "description": "AT91bootstrap before 3.9.2 does not properly wipe encryption and authentication keys from memory before passing control to a less privileged software component. This can be exploited to disclose these keys and subsequently encrypt and sign the next boot stage (such as the bootloader).",
        "git_url": "https://github.com/linux4sam/at91bootstrap/commit/45419497309ffbf27c17ea7938499aca99168927",
        "commit_title": "driver: secure: move keys into static arrays",
        "commit_text": " Move the keys into static arrays. This will avoid copying the keys from the code section to the stack on the previous init_keys implementation. Like this, the keys are hardcoded into data section at compile time, and can be completely wiped after use. ",
        "func_before": "int secure_check(void *data)\n{\n\tconst at91_secure_header_t *header;\n\tvoid *file;\n\n\tif (secure_decrypt(data, sizeof(*header), 0))\n\t\treturn -1;\n\n\theader = (const at91_secure_header_t *)data;\n\tif (header->magic != AT91_SECURE_MAGIC)\n\t\treturn -1;\n\n\tfile = (unsigned char *)data + sizeof(*header);\n\treturn secure_decrypt(file, header->file_size, 1);\n}",
        "func": "int secure_check(void *data)\n{\n\tconst at91_secure_header_t *header;\n\tvoid *file;\n\tint ret = -1;\n\n\tif (secure_decrypt(data, sizeof(*header), 0))\n\t\tgoto secure_wipe_keys;\n\n\theader = (const at91_secure_header_t *)data;\n\tif (header->magic != AT91_SECURE_MAGIC)\n\t\tgoto secure_wipe_keys;\n\n\tfile = (unsigned char *)data + sizeof(*header);\n\tret = secure_decrypt(file, header->file_size, 1);\n\nsecure_wipe_keys:\n\twipe_keys();\n\treturn ret;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,14 +2,19 @@\n {\n \tconst at91_secure_header_t *header;\n \tvoid *file;\n+\tint ret = -1;\n \n \tif (secure_decrypt(data, sizeof(*header), 0))\n-\t\treturn -1;\n+\t\tgoto secure_wipe_keys;\n \n \theader = (const at91_secure_header_t *)data;\n \tif (header->magic != AT91_SECURE_MAGIC)\n-\t\treturn -1;\n+\t\tgoto secure_wipe_keys;\n \n \tfile = (unsigned char *)data + sizeof(*header);\n-\treturn secure_decrypt(file, header->file_size, 1);\n+\tret = secure_decrypt(file, header->file_size, 1);\n+\n+secure_wipe_keys:\n+\twipe_keys();\n+\treturn ret;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "\t\treturn -1;",
                "\t\treturn -1;",
                "\treturn secure_decrypt(file, header->file_size, 1);"
            ],
            "added_lines": [
                "\tint ret = -1;",
                "\t\tgoto secure_wipe_keys;",
                "\t\tgoto secure_wipe_keys;",
                "\tret = secure_decrypt(file, header->file_size, 1);",
                "",
                "secure_wipe_keys:",
                "\twipe_keys();",
                "\treturn ret;"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-1637",
        "func_name": "torvalds/linux/pm_check_save_msr",
        "description": "A flaw that boot CPU could be vulnerable for the speculative execution behavior kind of attacks in the Linux kernel X86 CPU Power management options functionality was found in the way user resuming CPU from suspend-to-RAM. A local user could use this flaw to potentially get unauthorized access to some memory of the CPU similar to the speculative execution behavior kind of attacks.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=e2a1256b17b16f9b9adf1b6fea56819e7b68e463",
        "commit_title": "After resuming from suspend-to-RAM, the MSRs that control CPU's",
        "commit_text": "speculative execution behavior are not being restored on the boot CPU.  These MSRs are used to mitigate speculative execution vulnerabilities. Not restoring them correctly may leave the CPU vulnerable.  Secondary CPU's MSRs are correctly being restored at S3 resume by identify_secondary_cpu().  During S3 resume, restore these MSRs for boot CPU when restoring its processor state.  Cc: stable@vger.kernel.org ",
        "func_before": "static int pm_check_save_msr(void)\n{\n\tdmi_check_system(msr_save_dmi_table);\n\tpm_cpu_check(msr_save_cpu_table);\n\n\treturn 0;\n}",
        "func": "static int pm_check_save_msr(void)\n{\n\tdmi_check_system(msr_save_dmi_table);\n\tpm_cpu_check(msr_save_cpu_table);\n\tpm_save_spec_msr();\n\n\treturn 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -2,6 +2,7 @@\n {\n \tdmi_check_system(msr_save_dmi_table);\n \tpm_cpu_check(msr_save_cpu_table);\n+\tpm_save_spec_msr();\n \n \treturn 0;\n }",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\tpm_save_spec_msr();"
            ]
        }
    },
    {
        "cve_id": "CVE-2023-3006",
        "func_name": "kernel/git/arm64/linux/spectre_bhb_loop_affected",
        "description": "A known cache speculation vulnerability, known as Branch History Injection (BHI) or Spectre-BHB, becomes actual again for the new hw AmpereOne. Spectre-BHB is similar to Spectre v2, except that malicious code uses the shared branch history (stored in the CPU Branch History Buffer, or BHB) to influence mispredicted branches within the victim's hardware context. Once that occurs, speculation caused by the mispredicted branches can cause cache allocation. This issue leads to obtaining information that should not be accessible.",
        "git_url": "https://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux.git/commit/?h=0e5d5ae837c8",
        "commit_title": "Per AmpereOne erratum AC03_CPU_12, \"Branch history may allow control of",
        "commit_text": "speculative execution across software contexts,\" the AMPERE1 core needs the bhb clearing loop to mitigate Spectre-BHB, with a loop iteration count of 11.  Link: https://lore.kernel.org/r/20221011022140.432370-1-scott@os.amperecomputing.com ",
        "func_before": "u8 spectre_bhb_loop_affected(int scope)\n{\n\tu8 k = 0;\n\tstatic u8 max_bhb_k;\n\n\tif (scope == SCOPE_LOCAL_CPU) {\n\t\tstatic const struct midr_range spectre_bhb_k32_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78AE),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78C),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_X1),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A710),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_X2),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_N2),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_V1),\n\t\t\t{},\n\t\t};\n\t\tstatic const struct midr_range spectre_bhb_k24_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A76),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A77),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_N1),\n\t\t\t{},\n\t\t};\n\t\tstatic const struct midr_range spectre_bhb_k8_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A72),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A57),\n\t\t\t{},\n\t\t};\n\n\t\tif (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k32_list))\n\t\t\tk = 32;\n\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k24_list))\n\t\t\tk = 24;\n\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k8_list))\n\t\t\tk =  8;\n\n\t\tmax_bhb_k = max(max_bhb_k, k);\n\t} else {\n\t\tk = max_bhb_k;\n\t}\n\n\treturn k;\n}",
        "func": "u8 spectre_bhb_loop_affected(int scope)\n{\n\tu8 k = 0;\n\tstatic u8 max_bhb_k;\n\n\tif (scope == SCOPE_LOCAL_CPU) {\n\t\tstatic const struct midr_range spectre_bhb_k32_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78AE),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A78C),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_X1),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A710),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_X2),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_N2),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_V1),\n\t\t\t{},\n\t\t};\n\t\tstatic const struct midr_range spectre_bhb_k24_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A76),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A77),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_N1),\n\t\t\t{},\n\t\t};\n\t\tstatic const struct midr_range spectre_bhb_k11_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_AMPERE1),\n\t\t\t{},\n\t\t};\n\t\tstatic const struct midr_range spectre_bhb_k8_list[] = {\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A72),\n\t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A57),\n\t\t\t{},\n\t\t};\n\n\t\tif (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k32_list))\n\t\t\tk = 32;\n\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k24_list))\n\t\t\tk = 24;\n\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k11_list))\n\t\t\tk = 11;\n\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k8_list))\n\t\t\tk =  8;\n\n\t\tmax_bhb_k = max(max_bhb_k, k);\n\t} else {\n\t\tk = max_bhb_k;\n\t}\n\n\treturn k;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -21,6 +21,10 @@\n \t\t\tMIDR_ALL_VERSIONS(MIDR_NEOVERSE_N1),\n \t\t\t{},\n \t\t};\n+\t\tstatic const struct midr_range spectre_bhb_k11_list[] = {\n+\t\t\tMIDR_ALL_VERSIONS(MIDR_AMPERE1),\n+\t\t\t{},\n+\t\t};\n \t\tstatic const struct midr_range spectre_bhb_k8_list[] = {\n \t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A72),\n \t\t\tMIDR_ALL_VERSIONS(MIDR_CORTEX_A57),\n@@ -31,6 +35,8 @@\n \t\t\tk = 32;\n \t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k24_list))\n \t\t\tk = 24;\n+\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k11_list))\n+\t\t\tk = 11;\n \t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k8_list))\n \t\t\tk =  8;\n ",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "\t\tstatic const struct midr_range spectre_bhb_k11_list[] = {",
                "\t\t\tMIDR_ALL_VERSIONS(MIDR_AMPERE1),",
                "\t\t\t{},",
                "\t\t};",
                "\t\telse if (is_midr_in_range_list(read_cpuid_id(), spectre_bhb_k11_list))",
                "\t\t\tk = 11;"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11740",
        "func_name": "xen-project/xen/alloc_xenoprof_struct",
        "description": "An issue was discovered in xenoprof in Xen through 4.13.x, allowing guest OS users (without active profiling) to obtain sensitive information about other guests. Unprivileged guests can request to map xenoprof buffers, even if profiling has not been enabled for those guests. These buffers were not scrubbed.",
        "git_url": "https://github.com/xen-project/xen/commit/0763a7ebfcdad66cf9e5475a1301eefb29bae9ed",
        "commit_title": "xenoprof: clear buffer intended to be shared with guests",
        "commit_text": " alloc_xenheap_pages() making use of MEMF_no_scrub is fine for Xen internally used allocations, but buffers allocated to be shared with (unpriviliged) guests need to be zapped of their prior content.  This is part of XSA-313. ",
        "func_before": "static int alloc_xenoprof_struct(\n    struct domain *d, int max_samples, int is_passive)\n{\n    struct vcpu *v;\n    int nvcpu, npages, bufsize, max_bufsize;\n    unsigned max_max_samples;\n    int i;\n\n    nvcpu = 0;\n    for_each_vcpu ( d, v )\n        nvcpu++;\n\n    if ( !nvcpu )\n        return -EINVAL;\n\n    d->xenoprof = xzalloc(struct xenoprof);\n    if ( d->xenoprof == NULL )\n    {\n        printk(\"alloc_xenoprof_struct(): memory allocation failed\\n\");\n        return -ENOMEM;\n    }\n\n    d->xenoprof->vcpu = xzalloc_array(struct xenoprof_vcpu, d->max_vcpus);\n    if ( d->xenoprof->vcpu == NULL )\n    {\n        xfree(d->xenoprof);\n        d->xenoprof = NULL;\n        printk(\"alloc_xenoprof_struct(): vcpu array allocation failed\\n\");\n        return -ENOMEM;\n    }\n\n    bufsize = sizeof(struct xenoprof_buf);\n    i = sizeof(struct event_log);\n#ifdef CONFIG_COMPAT\n    d->xenoprof->is_compat = is_pv_32bit_domain(is_passive ? hardware_domain : d);\n    if ( XENOPROF_COMPAT(d->xenoprof) )\n    {\n        bufsize = sizeof(struct compat_oprof_buf);\n        i = sizeof(struct compat_event_log);\n    }\n#endif\n\n    /* reduce max_samples if necessary to limit pages allocated */\n    max_bufsize = (MAX_OPROF_SHARED_PAGES * PAGE_SIZE) / nvcpu;\n    max_max_samples = ( (max_bufsize - bufsize) / i ) + 1;\n    if ( (unsigned)max_samples > max_max_samples )\n        max_samples = max_max_samples;\n\n    bufsize += (max_samples - 1) * i;\n    npages = (nvcpu * bufsize - 1) / PAGE_SIZE + 1;\n\n    d->xenoprof->rawbuf = alloc_xenheap_pages(get_order_from_pages(npages), 0);\n    if ( d->xenoprof->rawbuf == NULL )\n    {\n        xfree(d->xenoprof->vcpu);\n        xfree(d->xenoprof);\n        d->xenoprof = NULL;\n        return -ENOMEM;\n    }\n\n    d->xenoprof->npages = npages;\n    d->xenoprof->nbuf = nvcpu;\n    d->xenoprof->bufsize = bufsize;\n    d->xenoprof->domain_ready = 0;\n    d->xenoprof->domain_type = XENOPROF_DOMAIN_IGNORED;\n\n    /* Update buffer pointers for active vcpus */\n    i = 0;\n    for_each_vcpu ( d, v )\n    {\n        xenoprof_buf_t *buf = (xenoprof_buf_t *)\n            &d->xenoprof->rawbuf[i * bufsize];\n\n        d->xenoprof->vcpu[v->vcpu_id].event_size = max_samples;\n        d->xenoprof->vcpu[v->vcpu_id].buffer = buf;\n        xenoprof_buf(d, buf, event_size) = max_samples;\n        xenoprof_buf(d, buf, vcpu_id) = v->vcpu_id;\n\n        i++;\n        /* in the unlikely case that the number of active vcpus changes */\n        if ( i >= nvcpu )\n            break;\n    }\n    \n    return 0;\n}",
        "func": "static int alloc_xenoprof_struct(\n    struct domain *d, int max_samples, int is_passive)\n{\n    struct vcpu *v;\n    int nvcpu, npages, bufsize, max_bufsize;\n    unsigned max_max_samples;\n    int i;\n\n    nvcpu = 0;\n    for_each_vcpu ( d, v )\n        nvcpu++;\n\n    if ( !nvcpu )\n        return -EINVAL;\n\n    d->xenoprof = xzalloc(struct xenoprof);\n    if ( d->xenoprof == NULL )\n    {\n        printk(\"alloc_xenoprof_struct(): memory allocation failed\\n\");\n        return -ENOMEM;\n    }\n\n    d->xenoprof->vcpu = xzalloc_array(struct xenoprof_vcpu, d->max_vcpus);\n    if ( d->xenoprof->vcpu == NULL )\n    {\n        xfree(d->xenoprof);\n        d->xenoprof = NULL;\n        printk(\"alloc_xenoprof_struct(): vcpu array allocation failed\\n\");\n        return -ENOMEM;\n    }\n\n    bufsize = sizeof(struct xenoprof_buf);\n    i = sizeof(struct event_log);\n#ifdef CONFIG_COMPAT\n    d->xenoprof->is_compat = is_pv_32bit_domain(is_passive ? hardware_domain : d);\n    if ( XENOPROF_COMPAT(d->xenoprof) )\n    {\n        bufsize = sizeof(struct compat_oprof_buf);\n        i = sizeof(struct compat_event_log);\n    }\n#endif\n\n    /* reduce max_samples if necessary to limit pages allocated */\n    max_bufsize = (MAX_OPROF_SHARED_PAGES * PAGE_SIZE) / nvcpu;\n    max_max_samples = ( (max_bufsize - bufsize) / i ) + 1;\n    if ( (unsigned)max_samples > max_max_samples )\n        max_samples = max_max_samples;\n\n    bufsize += (max_samples - 1) * i;\n    npages = (nvcpu * bufsize - 1) / PAGE_SIZE + 1;\n\n    d->xenoprof->rawbuf = alloc_xenheap_pages(get_order_from_pages(npages), 0);\n    if ( d->xenoprof->rawbuf == NULL )\n    {\n        xfree(d->xenoprof->vcpu);\n        xfree(d->xenoprof);\n        d->xenoprof = NULL;\n        return -ENOMEM;\n    }\n\n    for ( i = 0; i < npages; ++i )\n        clear_page(d->xenoprof->rawbuf + i * PAGE_SIZE);\n\n    d->xenoprof->npages = npages;\n    d->xenoprof->nbuf = nvcpu;\n    d->xenoprof->bufsize = bufsize;\n    d->xenoprof->domain_ready = 0;\n    d->xenoprof->domain_type = XENOPROF_DOMAIN_IGNORED;\n\n    /* Update buffer pointers for active vcpus */\n    i = 0;\n    for_each_vcpu ( d, v )\n    {\n        xenoprof_buf_t *buf = (xenoprof_buf_t *)\n            &d->xenoprof->rawbuf[i * bufsize];\n\n        d->xenoprof->vcpu[v->vcpu_id].event_size = max_samples;\n        d->xenoprof->vcpu[v->vcpu_id].buffer = buf;\n        xenoprof_buf(d, buf, event_size) = max_samples;\n        xenoprof_buf(d, buf, vcpu_id) = v->vcpu_id;\n\n        i++;\n        /* in the unlikely case that the number of active vcpus changes */\n        if ( i >= nvcpu )\n            break;\n    }\n    \n    return 0;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -58,6 +58,9 @@\n         return -ENOMEM;\n     }\n \n+    for ( i = 0; i < npages; ++i )\n+        clear_page(d->xenoprof->rawbuf + i * PAGE_SIZE);\n+\n     d->xenoprof->npages = npages;\n     d->xenoprof->nbuf = nvcpu;\n     d->xenoprof->bufsize = bufsize;",
        "diff_line_info": {
            "deleted_lines": [],
            "added_lines": [
                "    for ( i = 0; i < npages; ++i )",
                "        clear_page(d->xenoprof->rawbuf + i * PAGE_SIZE);",
                ""
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11740",
        "func_name": "xen-project/xen/xenoprof_buf_space",
        "description": "An issue was discovered in xenoprof in Xen through 4.13.x, allowing guest OS users (without active profiling) to obtain sensitive information about other guests. Unprivileged guests can request to map xenoprof buffers, even if profiling has not been enabled for those guests. These buffers were not scrubbed.",
        "git_url": "https://github.com/xen-project/xen/commit/50ef9a3cb26e2f9383f6fdfbed361d8f174bae9f",
        "commit_title": "xenoprof: limit consumption of shared buffer data",
        "commit_text": " Since a shared buffer can be written to by the guest, we may only read the head and tail pointers from there (all other fields should only ever be written to). Furthermore, for any particular operation the two values must be read exactly once, with both checks and consumption happening with the thus read values. (The backtrace related xenoprof_buf_space() use in xenoprof_log_event() is an exception: The values used there get re-checked by every subsequent xenoprof_add_sample().)  Since that code needed touching, also fix the double increment of the lost samples count in case the backtrace related xenoprof_add_sample() invocation in xenoprof_log_event() fails.  Where code is being touched anyway, add const as appropriate, but take the opportunity to entirely drop the now unused domain parameter of xenoprof_buf_space().  This is part of XSA-313. ",
        "func_before": "static int xenoprof_buf_space(struct domain *d, xenoprof_buf_t * buf, int size)\n{\n    int head, tail;\n\n    head = xenoprof_buf(d, buf, event_head);\n    tail = xenoprof_buf(d, buf, event_tail);\n\n    return ((tail > head) ? 0 : size) + tail - head - 1;\n}",
        "func": "static int xenoprof_buf_space(int head, int tail, int size)\n{\n    return ((tail > head) ? 0 : size) + tail - head - 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,9 +1,4 @@\n-static int xenoprof_buf_space(struct domain *d, xenoprof_buf_t * buf, int size)\n+static int xenoprof_buf_space(int head, int tail, int size)\n {\n-    int head, tail;\n-\n-    head = xenoprof_buf(d, buf, event_head);\n-    tail = xenoprof_buf(d, buf, event_tail);\n-\n     return ((tail > head) ? 0 : size) + tail - head - 1;\n }",
        "diff_line_info": {
            "deleted_lines": [
                "static int xenoprof_buf_space(struct domain *d, xenoprof_buf_t * buf, int size)",
                "    int head, tail;",
                "",
                "    head = xenoprof_buf(d, buf, event_head);",
                "    tail = xenoprof_buf(d, buf, event_tail);",
                ""
            ],
            "added_lines": [
                "static int xenoprof_buf_space(int head, int tail, int size)"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11740",
        "func_name": "xen-project/xen/xenoprof_log_event",
        "description": "An issue was discovered in xenoprof in Xen through 4.13.x, allowing guest OS users (without active profiling) to obtain sensitive information about other guests. Unprivileged guests can request to map xenoprof buffers, even if profiling has not been enabled for those guests. These buffers were not scrubbed.",
        "git_url": "https://github.com/xen-project/xen/commit/50ef9a3cb26e2f9383f6fdfbed361d8f174bae9f",
        "commit_title": "xenoprof: limit consumption of shared buffer data",
        "commit_text": " Since a shared buffer can be written to by the guest, we may only read the head and tail pointers from there (all other fields should only ever be written to). Furthermore, for any particular operation the two values must be read exactly once, with both checks and consumption happening with the thus read values. (The backtrace related xenoprof_buf_space() use in xenoprof_log_event() is an exception: The values used there get re-checked by every subsequent xenoprof_add_sample().)  Since that code needed touching, also fix the double increment of the lost samples count in case the backtrace related xenoprof_add_sample() invocation in xenoprof_log_event() fails.  Where code is being touched anyway, add const as appropriate, but take the opportunity to entirely drop the now unused domain parameter of xenoprof_buf_space().  This is part of XSA-313. ",
        "func_before": "void xenoprof_log_event(struct vcpu *vcpu, const struct cpu_user_regs *regs,\n                        uint64_t pc, int mode, int event)\n{\n    struct domain *d = vcpu->domain;\n    struct xenoprof_vcpu *v;\n    xenoprof_buf_t *buf;\n\n    total_samples++;\n\n    /* Ignore samples of un-monitored domains. */\n    if ( !is_profiled(d) )\n    {\n        others_samples++;\n        return;\n    }\n\n    v = &d->xenoprof->vcpu[vcpu->vcpu_id];\n    if ( v->buffer == NULL )\n    {\n        invalid_buffer_samples++;\n        return;\n    }\n    \n    buf = v->buffer;\n\n    /* Provide backtrace if requested. */\n    if ( backtrace_depth > 0 )\n    {\n        if ( (xenoprof_buf_space(d, buf, v->event_size) < 2) ||\n             !xenoprof_add_sample(d, buf, XENOPROF_ESCAPE_CODE, mode, \n                                  XENOPROF_TRACE_BEGIN) )\n        {\n            xenoprof_buf(d, buf, lost_samples)++;\n            lost_samples++;\n            return;\n        }\n    }\n\n    if ( xenoprof_add_sample(d, buf, pc, mode, event) )\n    {\n        if ( is_active(vcpu->domain) )\n            active_samples++;\n        else\n            passive_samples++;\n        if ( mode == 0 )\n            xenoprof_buf(d, buf, user_samples)++;\n        else if ( mode == 1 )\n            xenoprof_buf(d, buf, kernel_samples)++;\n        else\n            xenoprof_buf(d, buf, xen_samples)++;\n    \n    }\n\n    if ( backtrace_depth > 0 )\n        xenoprof_backtrace(vcpu, regs, backtrace_depth, mode);\n}",
        "func": "void xenoprof_log_event(struct vcpu *vcpu, const struct cpu_user_regs *regs,\n                        uint64_t pc, int mode, int event)\n{\n    struct domain *d = vcpu->domain;\n    struct xenoprof_vcpu *v;\n    xenoprof_buf_t *buf;\n\n    total_samples++;\n\n    /* Ignore samples of un-monitored domains. */\n    if ( !is_profiled(d) )\n    {\n        others_samples++;\n        return;\n    }\n\n    v = &d->xenoprof->vcpu[vcpu->vcpu_id];\n    if ( v->buffer == NULL )\n    {\n        invalid_buffer_samples++;\n        return;\n    }\n    \n    buf = v->buffer;\n\n    /* Provide backtrace if requested. */\n    if ( backtrace_depth > 0 )\n    {\n        if ( xenoprof_buf_space(xenoprof_buf(d, buf, event_head),\n                                xenoprof_buf(d, buf, event_tail),\n                                v->event_size) < 2 )\n        {\n            xenoprof_buf(d, buf, lost_samples)++;\n            lost_samples++;\n            return;\n        }\n\n        /* xenoprof_add_sample() will increment lost_samples on failure */\n        if ( !xenoprof_add_sample(d, v, XENOPROF_ESCAPE_CODE, mode,\n                                  XENOPROF_TRACE_BEGIN) )\n            return;\n    }\n\n    if ( xenoprof_add_sample(d, v, pc, mode, event) )\n    {\n        if ( is_active(vcpu->domain) )\n            active_samples++;\n        else\n            passive_samples++;\n        if ( mode == 0 )\n            xenoprof_buf(d, buf, user_samples)++;\n        else if ( mode == 1 )\n            xenoprof_buf(d, buf, kernel_samples)++;\n        else\n            xenoprof_buf(d, buf, xen_samples)++;\n    \n    }\n\n    if ( backtrace_depth > 0 )\n        xenoprof_backtrace(vcpu, regs, backtrace_depth, mode);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -26,17 +26,22 @@\n     /* Provide backtrace if requested. */\n     if ( backtrace_depth > 0 )\n     {\n-        if ( (xenoprof_buf_space(d, buf, v->event_size) < 2) ||\n-             !xenoprof_add_sample(d, buf, XENOPROF_ESCAPE_CODE, mode, \n-                                  XENOPROF_TRACE_BEGIN) )\n+        if ( xenoprof_buf_space(xenoprof_buf(d, buf, event_head),\n+                                xenoprof_buf(d, buf, event_tail),\n+                                v->event_size) < 2 )\n         {\n             xenoprof_buf(d, buf, lost_samples)++;\n             lost_samples++;\n             return;\n         }\n+\n+        /* xenoprof_add_sample() will increment lost_samples on failure */\n+        if ( !xenoprof_add_sample(d, v, XENOPROF_ESCAPE_CODE, mode,\n+                                  XENOPROF_TRACE_BEGIN) )\n+            return;\n     }\n \n-    if ( xenoprof_add_sample(d, buf, pc, mode, event) )\n+    if ( xenoprof_add_sample(d, v, pc, mode, event) )\n     {\n         if ( is_active(vcpu->domain) )\n             active_samples++;",
        "diff_line_info": {
            "deleted_lines": [
                "        if ( (xenoprof_buf_space(d, buf, v->event_size) < 2) ||",
                "             !xenoprof_add_sample(d, buf, XENOPROF_ESCAPE_CODE, mode, ",
                "                                  XENOPROF_TRACE_BEGIN) )",
                "    if ( xenoprof_add_sample(d, buf, pc, mode, event) )"
            ],
            "added_lines": [
                "        if ( xenoprof_buf_space(xenoprof_buf(d, buf, event_head),",
                "                                xenoprof_buf(d, buf, event_tail),",
                "                                v->event_size) < 2 )",
                "",
                "        /* xenoprof_add_sample() will increment lost_samples on failure */",
                "        if ( !xenoprof_add_sample(d, v, XENOPROF_ESCAPE_CODE, mode,",
                "                                  XENOPROF_TRACE_BEGIN) )",
                "            return;",
                "    if ( xenoprof_add_sample(d, v, pc, mode, event) )"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11740",
        "func_name": "xen-project/xen/xenoprof_add_trace",
        "description": "An issue was discovered in xenoprof in Xen through 4.13.x, allowing guest OS users (without active profiling) to obtain sensitive information about other guests. Unprivileged guests can request to map xenoprof buffers, even if profiling has not been enabled for those guests. These buffers were not scrubbed.",
        "git_url": "https://github.com/xen-project/xen/commit/50ef9a3cb26e2f9383f6fdfbed361d8f174bae9f",
        "commit_title": "xenoprof: limit consumption of shared buffer data",
        "commit_text": " Since a shared buffer can be written to by the guest, we may only read the head and tail pointers from there (all other fields should only ever be written to). Furthermore, for any particular operation the two values must be read exactly once, with both checks and consumption happening with the thus read values. (The backtrace related xenoprof_buf_space() use in xenoprof_log_event() is an exception: The values used there get re-checked by every subsequent xenoprof_add_sample().)  Since that code needed touching, also fix the double increment of the lost samples count in case the backtrace related xenoprof_add_sample() invocation in xenoprof_log_event() fails.  Where code is being touched anyway, add const as appropriate, but take the opportunity to entirely drop the now unused domain parameter of xenoprof_buf_space().  This is part of XSA-313. ",
        "func_before": "int xenoprof_add_trace(struct vcpu *vcpu, uint64_t pc, int mode)\n{\n    struct domain *d = vcpu->domain;\n    xenoprof_buf_t *buf = d->xenoprof->vcpu[vcpu->vcpu_id].buffer;\n\n    /* Do not accidentally write an escape code due to a broken frame. */\n    if ( pc == XENOPROF_ESCAPE_CODE )\n    {\n        invalid_buffer_samples++;\n        return 0;\n    }\n\n    return xenoprof_add_sample(d, buf, pc, mode, 0);\n}",
        "func": "int xenoprof_add_trace(struct vcpu *vcpu, uint64_t pc, int mode)\n{\n    struct domain *d = vcpu->domain;\n\n    /* Do not accidentally write an escape code due to a broken frame. */\n    if ( pc == XENOPROF_ESCAPE_CODE )\n    {\n        invalid_buffer_samples++;\n        return 0;\n    }\n\n    return xenoprof_add_sample(d, &d->xenoprof->vcpu[vcpu->vcpu_id],\n                               pc, mode, 0);\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,7 +1,6 @@\n int xenoprof_add_trace(struct vcpu *vcpu, uint64_t pc, int mode)\n {\n     struct domain *d = vcpu->domain;\n-    xenoprof_buf_t *buf = d->xenoprof->vcpu[vcpu->vcpu_id].buffer;\n \n     /* Do not accidentally write an escape code due to a broken frame. */\n     if ( pc == XENOPROF_ESCAPE_CODE )\n@@ -10,5 +9,6 @@\n         return 0;\n     }\n \n-    return xenoprof_add_sample(d, buf, pc, mode, 0);\n+    return xenoprof_add_sample(d, &d->xenoprof->vcpu[vcpu->vcpu_id],\n+                               pc, mode, 0);\n }",
        "diff_line_info": {
            "deleted_lines": [
                "    xenoprof_buf_t *buf = d->xenoprof->vcpu[vcpu->vcpu_id].buffer;",
                "    return xenoprof_add_sample(d, buf, pc, mode, 0);"
            ],
            "added_lines": [
                "    return xenoprof_add_sample(d, &d->xenoprof->vcpu[vcpu->vcpu_id],",
                "                               pc, mode, 0);"
            ]
        }
    },
    {
        "cve_id": "CVE-2020-11740",
        "func_name": "xen-project/xen/xenoprof_add_sample",
        "description": "An issue was discovered in xenoprof in Xen through 4.13.x, allowing guest OS users (without active profiling) to obtain sensitive information about other guests. Unprivileged guests can request to map xenoprof buffers, even if profiling has not been enabled for those guests. These buffers were not scrubbed.",
        "git_url": "https://github.com/xen-project/xen/commit/50ef9a3cb26e2f9383f6fdfbed361d8f174bae9f",
        "commit_title": "xenoprof: limit consumption of shared buffer data",
        "commit_text": " Since a shared buffer can be written to by the guest, we may only read the head and tail pointers from there (all other fields should only ever be written to). Furthermore, for any particular operation the two values must be read exactly once, with both checks and consumption happening with the thus read values. (The backtrace related xenoprof_buf_space() use in xenoprof_log_event() is an exception: The values used there get re-checked by every subsequent xenoprof_add_sample().)  Since that code needed touching, also fix the double increment of the lost samples count in case the backtrace related xenoprof_add_sample() invocation in xenoprof_log_event() fails.  Where code is being touched anyway, add const as appropriate, but take the opportunity to entirely drop the now unused domain parameter of xenoprof_buf_space().  This is part of XSA-313. ",
        "func_before": "static int xenoprof_add_sample(struct domain *d, xenoprof_buf_t *buf,\n                               uint64_t eip, int mode, int event)\n{\n    int head, tail, size;\n\n    head = xenoprof_buf(d, buf, event_head);\n    tail = xenoprof_buf(d, buf, event_tail);\n    size = xenoprof_buf(d, buf, event_size);\n    \n    /* make sure indexes in shared buffer are sane */\n    if ( (head < 0) || (head >= size) || (tail < 0) || (tail >= size) )\n    {\n        corrupted_buffer_samples++;\n        return 0;\n    }\n\n    if ( xenoprof_buf_space(d, buf, size) > 0 )\n    {\n        xenoprof_buf(d, buf, event_log[head].eip) = eip;\n        xenoprof_buf(d, buf, event_log[head].mode) = mode;\n        xenoprof_buf(d, buf, event_log[head].event) = event;\n        head++;\n        if ( head >= size )\n            head = 0;\n        \n        xenoprof_buf(d, buf, event_head) = head;\n    }\n    else\n    {\n        xenoprof_buf(d, buf, lost_samples)++;\n        lost_samples++;\n        return 0;\n    }\n\n    return 1;\n}",
        "func": "static int xenoprof_add_sample(const struct domain *d,\n                               const struct xenoprof_vcpu *v,\n                               uint64_t eip, int mode, int event)\n{\n    xenoprof_buf_t *buf = v->buffer;\n    int head, tail, size;\n\n    head = xenoprof_buf(d, buf, event_head);\n    tail = xenoprof_buf(d, buf, event_tail);\n    size = v->event_size;\n    \n    /* make sure indexes in shared buffer are sane */\n    if ( (head < 0) || (head >= size) || (tail < 0) || (tail >= size) )\n    {\n        corrupted_buffer_samples++;\n        return 0;\n    }\n\n    if ( xenoprof_buf_space(head, tail, size) > 0 )\n    {\n        xenoprof_buf(d, buf, event_log[head].eip) = eip;\n        xenoprof_buf(d, buf, event_log[head].mode) = mode;\n        xenoprof_buf(d, buf, event_log[head].event) = event;\n        head++;\n        if ( head >= size )\n            head = 0;\n        \n        xenoprof_buf(d, buf, event_head) = head;\n    }\n    else\n    {\n        xenoprof_buf(d, buf, lost_samples)++;\n        lost_samples++;\n        return 0;\n    }\n\n    return 1;\n}",
        "diff_func": "--- func_before\n+++ func_after\n@@ -1,11 +1,13 @@\n-static int xenoprof_add_sample(struct domain *d, xenoprof_buf_t *buf,\n+static int xenoprof_add_sample(const struct domain *d,\n+                               const struct xenoprof_vcpu *v,\n                                uint64_t eip, int mode, int event)\n {\n+    xenoprof_buf_t *buf = v->buffer;\n     int head, tail, size;\n \n     head = xenoprof_buf(d, buf, event_head);\n     tail = xenoprof_buf(d, buf, event_tail);\n-    size = xenoprof_buf(d, buf, event_size);\n+    size = v->event_size;\n     \n     /* make sure indexes in shared buffer are sane */\n     if ( (head < 0) || (head >= size) || (tail < 0) || (tail >= size) )\n@@ -14,7 +16,7 @@\n         return 0;\n     }\n \n-    if ( xenoprof_buf_space(d, buf, size) > 0 )\n+    if ( xenoprof_buf_space(head, tail, size) > 0 )\n     {\n         xenoprof_buf(d, buf, event_log[head].eip) = eip;\n         xenoprof_buf(d, buf, event_log[head].mode) = mode;",
        "diff_line_info": {
            "deleted_lines": [
                "static int xenoprof_add_sample(struct domain *d, xenoprof_buf_t *buf,",
                "    size = xenoprof_buf(d, buf, event_size);",
                "    if ( xenoprof_buf_space(d, buf, size) > 0 )"
            ],
            "added_lines": [
                "static int xenoprof_add_sample(const struct domain *d,",
                "                               const struct xenoprof_vcpu *v,",
                "    xenoprof_buf_t *buf = v->buffer;",
                "    size = v->event_size;",
                "    if ( xenoprof_buf_space(head, tail, size) > 0 )"
            ]
        }
    }
]